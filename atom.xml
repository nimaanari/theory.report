<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>Theory of Computing Report</title>
  <link rel="self" href=""/>
  <link href=""/>
  <id></id>
  <updated></updated>
  <generator uri="http://feedreader.github.io/">Pluto 1.6.2 on Ruby 3.0.6 (2023-03-30) [x86_64-linux]</generator>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: On the complexity of isomorphism problems for tensors, groups, and polynomials III: actions by classical groups</title>
    <link href="http://arxiv.org/abs/2306.03135"/>
    <id>http://arxiv.org/abs/2306.03135</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhili Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grochow_J/0/1/0/all/0/1&quot;&gt;Joshua A. Grochow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Youming Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1&quot;&gt;Gang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chuanqi Zhang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the complexity of isomorphism problems for d-way arrays, or tensors,
under natural actions by classical groups such as orthogonal, unitary, and
symplectic groups. Such problems arise naturally in statistical data analysis
and quantum information. We study two types of complexity-theoretic questions.
First, for a fixed action type (isomorphism, conjugacy, etc.), we relate the
complexity of the isomorphism problem over a classical group to that over the
general linear group. Second, for a fixed group type (orthogonal, unitary, or
symplectic), we compare the complexity of the decision problems for different
actions.
&lt;/p&gt;
&lt;p&gt;Our main results are as follows. First, for orthogonal and symplectic groups
acting on 3-way arrays, the isomorphism problems reduce to the corresponding
problem over the general linear group. Second, for orthogonal and unitary
groups, the isomorphism problems of five natural actions on 3-way arrays are
polynomial-time equivalent, and the d-tensor isomorphism problem reduces to the
3-tensor isomorphism problem for any fixed d&amp;gt;3. For unitary groups, the
preceding result implies that LOCC classification of tripartite quantum states
is at least as difficult as LOCC classification of d-partite quantum states for
any d. Lastly, we also show that the graph isomorphism problem reduces to the
tensor isomorphism problem over orthogonal and unitary groups.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: On the Role of Entanglement and Statistics in Learning</title>
    <link href="http://arxiv.org/abs/2306.03161"/>
    <id>http://arxiv.org/abs/2306.03161</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Arunachalam_S/0/1/0/all/0/1&quot;&gt;Srinivasan Arunachalam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Havlicek_V/0/1/0/all/0/1&quot;&gt;Vojtech Havlicek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Schatzki_L/0/1/0/all/0/1&quot;&gt;Louis Schatzki&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this work we make progress in understanding the relationship between
learning models with access to entangled, separable and statistical
measurements in the quantum statistical query (QSQ) model. To this end, we show
the following results.
&lt;/p&gt;
&lt;p&gt;$\textbf{Entangled versus separable measurements.}$ The goal here is to learn
an unknown $f$ from the concept class $C\subseteq \{f:\{0,1\}^n\rightarrow
[k]\}$ given copies of $\frac{1}{\sqrt{2^n}}\sum_x \vert x,f(x)\rangle$. We
show that, if $T$ copies suffice to learn $f$ using entangled measurements,
then $O(nT^2)$ copies suffice to learn $f$ using just separable measurements.
&lt;/p&gt;
&lt;p&gt;$\textbf{Entangled versus statistical measurements}$ The goal here is to
learn a function $f \in C$ given access to separable measurements and
statistical measurements. We exhibit a class $C$ that gives an exponential
separation between QSQ learning and quantum learning with entangled
measurements (even in the presence of noise). This proves the &quot;quantum
analogue&quot; of the seminal result of Blum et al. [BKW&#39;03]. that separates
classical SQ and PAC learning with classification noise.
&lt;/p&gt;
&lt;p&gt;$\textbf{QSQ lower bounds for learning states.}$ We introduce a quantum
statistical query dimension (QSD), which we use to give lower bounds on the QSQ
learning. With this we prove superpolynomial QSQ lower bounds for testing
purity, shadow tomography, Abelian hidden subgroup problem, degree-$2$
functions, planted bi-clique states and output states of Clifford circuits of
depth $\textsf{polylog}(n)$.
&lt;/p&gt;
&lt;p&gt;$\textbf{Further applications.}$ We give and $\textit{unconditional}$
separation between weak and strong error mitigation and prove lower bounds for
learning distributions in the QSQ model. Prior works by Quek et al. [QFK+&#39;22],
Hinsche et al. [HIN+&#39;22], and Nietner et al. [NIS+&#39;23] proved the analogous
results $\textit{assuming}$ diagonal measurements and our work removes this
assumption.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Three Candidate Plurality is Stablest for Correlations at most 1/11</title>
    <link href="http://arxiv.org/abs/2306.03312"/>
    <id>http://arxiv.org/abs/2306.03312</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Heilman_S/0/1/0/all/0/1&quot;&gt;Steven Heilman&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We prove the three candidate Plurality is Stablest Conjecture of
Khot-Kindler-Mossel-O&#39;Donnell from 2005 for correlations $\rho$ satisfying
$-1/36&amp;lt;\rho&amp;lt;1/11$: the Plurality function is the most noise stable three
candidate election method with small influences, when the corrupted votes have
correlation $-1/36&amp;lt;\rho&amp;lt;1/11$ with the original votes. The previous best result
of this type only achieved positive correlations at most $10^{-10^{10}}$. Our
result follows by solving the three set Standard Simplex Conjecture of
Isaksson-Mossel from 2011 for all correlations $-1/36&amp;lt;\rho&amp;lt;1/11$.
&lt;/p&gt;
&lt;p&gt;The Gaussian Double Bubble Problem corresponds to the case $\rho\to1^{-}$, so
in some sense, our result is a generalization of the Gaussian Double Bubble
Problem. Our result is also notable since it is the first result for any
$\rho&amp;lt;0$, which is the only relevant case for computational hardness of
MAX-3-CUT. As an additional corollary, we conclude that three candidate Borda
Count is stablest for all $-1/36&amp;lt;\rho&amp;lt;1/11$.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: Complexity of Anchored Crossing Number and Crossing Number of Almost Planar Graphs</title>
    <link href="http://arxiv.org/abs/2306.03490"/>
    <id>http://arxiv.org/abs/2306.03490</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hlineny_P/0/1/0/all/0/1&quot;&gt;Petr Hlin&amp;#x11b;n&amp;#xfd;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this paper we deal with the problem of computing the exact crossing number
of almost planar graphs and the closely related problem of computing the exact
anchored crossing number of a pair of planar graphs. It was shown by [Cabello
and Mohar, 2013] that both problems are NP-hard; although they required an
unbounded number of high-degree vertices (in the first problem) or an unbounded
number of anchors (in the second problem) to prove their result. Somehow
surprisingly, only three vertices of degree greater than 3, or only three
anchors, are sufficient to maintain hardness of these problems, as we prove
here. The new result also improves the previous result on hardness of joint
crossing number on surfaces by [Hlin\v{e}n\&#39;y and Salazar, 2015]. Our result is
best possible in the anchored case since the anchored crossing number of a pair
of planar graphs with two anchors each is trivial, and close to being best
possible in the almost planar case since the crossing number is efficiently
computable for almost planar graphs of maximum degree 3 [Riskin 1996, Cabello
and Mohar 2011].
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Tight Complexity Bounds for Counting Generalized Dominating Sets in Bounded-Treewidth Graphs Part II: Hardness Results</title>
    <link href="http://arxiv.org/abs/2306.03640"/>
    <id>http://arxiv.org/abs/2306.03640</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Focke_J/0/1/0/all/0/1&quot;&gt;Jacob Focke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marx_D/0/1/0/all/0/1&quot;&gt;D&amp;#xe1;niel Marx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inerney_F/0/1/0/all/0/1&quot;&gt;Fionn Mc Inerney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neuen_D/0/1/0/all/0/1&quot;&gt;Daniel Neuen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankar_G/0/1/0/all/0/1&quot;&gt;Govind S. Sankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schepper_P/0/1/0/all/0/1&quot;&gt;Philipp Schepper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wellnitz_P/0/1/0/all/0/1&quot;&gt;Philip Wellnitz&lt;/a&gt;&lt;/p&gt;&lt;p&gt;For a well-studied family of domination-type problems, in bounded-treewidth
graphs, we investigate whether it is possible to find faster algorithms. For
sets $\sigma,\rho$ of non-negative integers, a $(\sigma,\rho)$-set of a graph
$G$ is a set $S$ of vertices such that $|N(u)\cap S|\in \sigma$ for every $u\in
S$, and $|N(v)\cap S|\in \rho$ for every $v\not\in S$. The problem of finding a
$(\sigma,\rho)$-set (of a certain size) unifies common problems like
$\text{Independent Set}$, $\text{Dominating Set}$, $\text{Independent
Dominating Set}$, and many others.
&lt;/p&gt;
&lt;p&gt;In an accompanying paper, it is proven that, for all pairs of finite or
cofinite sets $(\sigma,\rho)$, there is an algorithm that counts
$(\sigma,\rho)$-sets in time $(c_{\sigma,\rho})^{\text{tw}}\cdot n^{O(1)}$ (if
a tree decomposition of width $\text{tw}$ is given in the input). Here,
$c_{\sigma,\rho}$ is a constant with an intricate dependency on $\sigma$ and
$\rho$. Despite this intricacy, we show that the algorithms in the accompanying
paper are most likely optimal, i.e., for any pair $(\sigma, \rho)$ of finite or
cofinite sets where the problem is non-trivial, and any $\varepsilon&amp;gt;0$, a
$(c_{\sigma,\rho}-\varepsilon)^{\text{tw}}\cdot n^{O(1)}$-algorithm counting
the number of $(\sigma,\rho)$-sets would violate the Counting Strong
Exponential-Time Hypothesis ($\#$SETH). For finite sets $\sigma$ and $\rho$,
our lower bounds also extend to the decision version, showing that those
algorithms are optimal in this setting as well.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: On the Parameterized Complexity of Computing $st$-Orientations with Few Transitive Edges</title>
    <link href="http://arxiv.org/abs/2306.03196"/>
    <id>http://arxiv.org/abs/2306.03196</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Binucci_C/0/1/0/all/0/1&quot;&gt;Carla Binucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liotta_G/0/1/0/all/0/1&quot;&gt;Giuseppe Liotta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montecchiani_F/0/1/0/all/0/1&quot;&gt;Fabrizio Montecchiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortali_G/0/1/0/all/0/1&quot;&gt;Giacomo Ortali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piselli_T/0/1/0/all/0/1&quot;&gt;Tommaso Piselli&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Orienting the edges of an undirected graph such that the resulting digraph
satisfies some given constraints is a classical problem in graph theory, with
multiple algorithmic applications. In particular, an $st$-orientation orients
each edge of the input graph such that the resulting digraph is acyclic, and it
contains a single source $s$ and a single sink $t$. Computing an
$st$-orientation of a graph can be done efficiently, and it finds notable
applications in graph algorithms and in particular in graph drawing. On the
other hand, finding an $st$-orientation with at most $k$ transitive edges is
more challenging and it was recently proven to be NP-hard already when $k=0$.
We strengthen this result by showing that the problem remains NP-hard even for
graphs of bounded diameter, and for graphs of bounded vertex degree. These
computational lower bounds naturally raise the question about which structural
parameters can lead to tractable parameterizations of the problem. Our main
result is a fixed-parameter tractable algorithm parameterized by treewidth.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Accelerating Range Minimum Queries with Ray Tracing Cores</title>
    <link href="http://arxiv.org/abs/2306.03282"/>
    <id>http://arxiv.org/abs/2306.03282</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meneses_E/0/1/0/all/0/1&quot;&gt;Enzo Meneses&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navarro_C/0/1/0/all/0/1&quot;&gt;Crist&amp;#xf3;bal A. Navarro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrada_H/0/1/0/all/0/1&quot;&gt;H&amp;#xe9;ctor Ferrada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quezada_F/0/1/0/all/0/1&quot;&gt;Felipe A. Quezada&lt;/a&gt;&lt;/p&gt;&lt;p&gt;During the last decade GPU technology has shifted from pure general purpose
computation to the inclusion of application specific integrated circuits
(ASICs), such as Tensor Cores and Ray Tracing (RT) cores. Although these
special purpose GPU cores were designed to further accelerate specific fields
such as AI and real-time rendering, recent research has managed to exploit them
to further accelerate other tasks that typically used regular GPU computing. In
this work we present RTXRMQ, a new approach that can compute range minimum
queries (RMQs) with RT cores. The main contribution is the proposal of a
geometric solution for RMQ, where elements become triangles that are placed and
shaped according to the element&#39;s value and position in the array,
respectively, such that the closest hit of a ray launched from a point given by
the query parameters corresponds to the result of that query. Experimental
results show that RTXRMQ is currently best suited for small query ranges
relative to the problem size, achieving up to $5\times$ and $2.3\times$ of
speedup over state of the art CPU (HRMQ) and GPU (LCA) approaches,
respectively. Although for medium and large query ranges RTXRMQ is currently
surpassed by LCA, it is still competitive by being $2.5\times$ and $4\times$
faster than HRMQ which is a highly parallel CPU approach. Furthermore,
performance scaling experiments across the latest RTX GPU architectures show
that if the current RT scaling trend continues, then RTXRMQ&#39;s performance would
scale at a higher rate than HRMQ and LCA, making the approach even more
relevant for future high performance applications that employ batches of RMQs.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Tracking Evolving labels using Cone based Oracles</title>
    <link href="http://arxiv.org/abs/2306.03306"/>
    <id>http://arxiv.org/abs/2306.03306</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1&quot;&gt;Aditya Acharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mount_D/0/1/0/all/0/1&quot;&gt;David Mount&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The evolving data framework was first proposed by Anagnostopoulos et al.,
where an evolver makes small changes to a structure behind the scenes. Instead
of taking a single input and producing a single output, an algorithm
judiciously probes the current state of the structure and attempts to
continuously maintain a sketch of the structure that is as close as possible to
its actual state. There have been a number of problems that have been studied
in the evolving framework including our own work on labeled trees. We were
motivated by the problem of maintaining a labeling in the plane, where updating
the labels require physically moving them. Applications involve tracking
evolving disease hot-spots via mobile testing units , and tracking unmanned
aerial vehicles. To be specific, we consider the problem of tracking labeled
nodes in the plane, where an evolver continuously swaps labels of any two
nearby nodes in the background unknown to us. We are tasked with maintaining a
hypothesis, an approximate sketch of the locations of these labels, which we
can only update by physically moving them over a sparse graph. We assume the
existence of an Oracle, which when suitably probed, guides us in fixing our
hypothesis.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: A Combinatorial Certifying Algorithm for Linear Programming Problems with Gainfree Leontief Substitution Systems</title>
    <link href="http://arxiv.org/abs/2306.03368"/>
    <id>http://arxiv.org/abs/2306.03368</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kimura_K/0/1/0/all/0/1&quot;&gt;Kei Kimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makino_K/0/1/0/all/0/1&quot;&gt;Kazuhisa Makino&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Linear programming (LP) problems with gainfree Leontief substitution systems
have been intensively studied in economics and operations research, and include
the feasibility problem of a class of Horn systems, which arises in, e.g.,
polyhedral combinatorics and logic. This subclass of LP problems admits a
strongly polynomial time algorithm, where devising such an algorithm for
general LP problems is one of the major theoretical open questions in
mathematical optimization and computer science. Recently, much attention has
been paid to devising certifying algorithms in software engineering, since
those algorithms enable one to confirm the correctness of outputs of programs
with simple computations. In this paper, we provide the first combinatorial
(and strongly polynomial time) certifying algorithm for LP problems with
gainfree Leontief substitution systems. As a by-product, we answer
affirmatively an open question whether the feasibility problem of the class of
Horn systems admits a combinatorial certifying algorithm.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Rigorous Runtime Analysis of MOEA/D for Solving Multi-Objective Minimum Weight Base Problems</title>
    <link href="http://arxiv.org/abs/2306.03409"/>
    <id>http://arxiv.org/abs/2306.03409</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_A/0/1/0/all/0/1&quot;&gt;Anh Viet Do&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_A/0/1/0/all/0/1&quot;&gt;Aneta Neumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_F/0/1/0/all/0/1&quot;&gt;Frank Neumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutton_A/0/1/0/all/0/1&quot;&gt;Andrew M. Sutton&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the multi-objective minimum weight base problem, an abstraction of
classical NP-hard combinatorial problems such as the multi-objective minimum
spanning tree problem. We prove some important properties of the convex hull of
the non-dominated front, such as its approximation quality and an upper bound
on the number of extreme points. Using these properties, we give the first
run-time analysis of the MOEA/D algorithm for this problem, an evolutionary
algorithm that effectively optimizes by decomposing the objectives into
single-objective components. We show that the MOEA/D, given an appropriate
decomposition setting, finds all extreme points within expected fixed-parameter
polynomial time in the oracle model, the parameter being the number of
objectives. Experiments are conducted on random bi-objective minimum spanning
tree instances, and the results agree with our theoretical findings.
Furthermore, compared with a previously studied evolutionary algorithm for the
problem GSEMO, MOEA/D finds all extreme points much faster across all
instances.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Minimizing Hitting Time between Disparate Groups with Shortcut Edges</title>
    <link href="http://arxiv.org/abs/2306.03571"/>
    <id>http://arxiv.org/abs/2306.03571</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adriaens_F/0/1/0/all/0/1&quot;&gt;Florian Adriaens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Honglian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gionis_A/0/1/0/all/0/1&quot;&gt;Aristides Gionis&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Structural bias or segregation of networks refers to situations where two or
more disparate groups are present in the network, so that the groups are highly
connected internally, but loosely connected to each other. In many cases it is
of interest to increase the connectivity of disparate groups so as to, e.g.,
minimize social friction, or expose individuals to diverse viewpoints. A
commonly-used mechanism for increasing the network connectivity is to add edge
shortcuts between pairs of nodes. In many applications of interest, edge
shortcuts typically translate to recommendations, e.g., what video to watch, or
what news article to read next. The problem of reducing structural bias or
segregation via edge shortcuts has recently been studied in the literature, and
random walks have been an essential tool for modeling navigation and
connectivity in the underlying networks. Existing methods, however, either do
not offer approximation guarantees, or engineer the objective so that it
satisfies certain desirable properties that simplify the optimization~task. In
this paper we address the problem of adding a given number of shortcut edges in
the network so as to directly minimize the average hitting time and the maximum
hitting time between two disparate groups. Our algorithm for minimizing average
hitting time is a greedy bicriteria that relies on supermodularity. In
contrast, maximum hitting time is not supermodular. Despite, we develop an
approximation algorithm for that objective as well, by leveraging connections
with average hitting time and the asymmetric k-center problem.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Representative set statements for delta-matroids and the Mader delta-matroid</title>
    <link href="http://arxiv.org/abs/2306.03605"/>
    <id>http://arxiv.org/abs/2306.03605</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wahlstrom_M/0/1/0/all/0/1&quot;&gt;Magnus Wahlstr&amp;#xf6;m&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We present representative sets-style statements for linear delta-matroids,
which are set systems that generalize matroids, with important connections to
matching theory and graph embeddings. Furthermore, our proof uses a new
approach of sieving polynomial families, which generalizes the linear algebra
approach of the representative sets lemma to a setting of bounded-degree
polynomials. The representative sets statements for linear delta-matroids then
follow by analyzing the Pfaffian of the skew-symmetric matrix representing the
delta-matroid. Applying the same framework to the determinant instead of the
Pfaffian recovers the representative sets lemma for linear matroids.
Altogether, this significantly extends the toolbox available for kernelization.
&lt;/p&gt;
&lt;p&gt;As an application, we show an exact sparsification result for Mader networks:
Let $G=(V,E)$ be a graph and $\mathcal{T}$ a partition of a set of terminals $T
\subseteq V(G)$, $|T|=k$. A $\mathcal{T}$-path in $G$ is a path with endpoints
in distinct parts of $\mathcal{T}$ and internal vertices disjoint from $T$. In
polynomial time, we can derive a graph $G&#39;=(V&#39;,E&#39;)$ with $T \subseteq V(G&#39;)$,
such that for every subset $S \subseteq T$ there is a packing of
$\mathcal{T}$-paths with endpoints $S$ in $G$ if and only if there is one in
$G&#39;$, and $|V(G&#39;)|=O(k^3)$. This generalizes the (undirected version of the)
cut-covering lemma, which corresponds to the case that $\mathcal{T}$ contains
only two blocks.
&lt;/p&gt;
&lt;p&gt;To prove the Mader network sparsification result, we furthermore define the
class of Mader delta-matroids, and show that they have linear representations.
This should be of independent interest.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Buying Information for Stochastic Optimization</title>
    <link href="http://arxiv.org/abs/2306.03607"/>
    <id>http://arxiv.org/abs/2306.03607</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Mingchen Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1&quot;&gt;Christos Tzamos&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Stochastic optimization is one of the central problems in Machine Learning
and Theoretical Computer Science. In the standard model, the algorithm is given
a fixed distribution known in advance. In practice though, one may acquire at a
cost extra information to make better decisions. In this paper, we study how to
buy information for stochastic optimization and formulate this question as an
online learning problem. Assuming the learner has an oracle for the original
optimization problem, we design a $2$-competitive deterministic algorithm and a
$e/(e-1)$-competitive randomized algorithm for buying information. We show that
this ratio is tight as the problem is equivalent to a robust generalization of
the ski-rental problem, which we call super-martingale stopping.
&lt;/p&gt;
&lt;p&gt;We also consider an adaptive setting where the learner can choose to buy
information after taking some actions for the underlying optimization problem.
We focus on the classic optimization problem, Min-Sum Set Cover, where the goal
is to quickly find an action that covers a given request drawn from a known
distribution. We provide an $8$-competitive algorithm running in polynomial
time that chooses actions and decides when to buy information about the
underlying request.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Constant Sequence Extension for Fast Search Using Weighted Hamming Distance</title>
    <link href="http://arxiv.org/abs/2306.03612"/>
    <id>http://arxiv.org/abs/2306.03612</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1&quot;&gt;Huiping Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haizhou Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhiping Lin&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Representing visual data using compact binary codes is attracting increasing
attention as binary codes are used as direct indices into hash table(s) for
fast non-exhaustive search. Recent methods show that ranking binary codes using
weighted Hamming distance (WHD) rather than Hamming distance (HD) by generating
query-adaptive weights for each bit can better retrieve query-related items.
However, search using WHD is slower than that using HD. One main challenge is
that the complexity of extending a monotone increasing sequence using WHD to
probe buckets in hash table(s) for existing methods is at least proportional to
the square of the sequence length, while that using HD is proportional to the
sequence length. To overcome this challenge, we propose a novel fast
non-exhaustive search method using WHD. The key idea is to design a constant
sequence extension algorithm to perform each sequence extension in constant
computational complexity and the total complexity is proportional to the
sequence length, which is justified by theoretical analysis. Experimental
results show that our method is faster than other WHD-based search methods.
Also, compared with the HD-based non-exhaustive search method, our method has
comparable efficiency but retrieves more query-related items for the dataset of
up to one billion items.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Efficient Centrality Maximization with Rademacher Averages</title>
    <link href="http://arxiv.org/abs/2306.03651"/>
    <id>http://arxiv.org/abs/2306.03651</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellegrina_L/0/1/0/all/0/1&quot;&gt;Leonardo Pellegrina&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The identification of the set of k most central nodes of a graph, or
centrality maximization, is a key task in network analysis, with various
applications ranging from finding communities in social and biological networks
to understanding which seed nodes are important to diffuse information in a
graph. As the exact computation of centrality measures does not scale to
modern-sized networks, the most practical solution is to resort to rigorous,
but efficiently computable, randomized approximations. In this work we present
CentRA, the first algorithm based on progressive sampling to compute
high-quality approximations of the set of k most central nodes. CentRA is based
on a novel approach to efficiently estimate Monte Carlo Rademacher Averages, a
powerful tool from statistical learning theory to compute sharp data-dependent
approximation bounds. Then, we study the sample complexity of centrality
maximization using the VC-dimension, a key concept from statistical learning
theory. We show that the number of random samples required to compute
high-quality approximations scales with finer characteristics of the graph,
such as its vertex diameter, or of the centrality of interest, significantly
improving looser bounds derived from standard techniques. We apply CentRA to
analyze large real-world networks, showing that it significantly outperforms
the state-of-the-art approximation algorithm in terms of number of samples,
running times, and accuracy.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: A survey of approximation algorithms for capacitated vehicle routing problems</title>
    <link href="http://arxiv.org/abs/2306.01826"/>
    <id>http://arxiv.org/abs/2306.01826</id>
    <updated>2023-06-06T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yongyu Chen&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Finding the shortest travelling tour of vehicles with capacity k from the
depot to the customers is called the Capacity vehicle routing problem (CVRP).
CVRP plays an essential position in logistics systems, and it is the most
intensively studied problem in combinatorial optimization. In complexity, CVRP
with k $\ge$ 3 is an NP-hard problem, and it is APX-hard as well. We already
knew that it could not be approximated in metric space. Moreover, it is the
first problem resisting Arora&#39;s famous approximation framework. So, whether
there is, a polynomial-time (1+$\epsilon$)-approximation for the Euclidean CVRP
for any $\epsilon&amp;gt;0$ is still an open problem. This paper will summarize the
research progress from history to up-to-date developments. The survey will be
updated periodically.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Revisiting Garg&#39;s 2-Approximation Algorithm for the k-MST Problem in Graphs</title>
    <link href="http://arxiv.org/abs/2306.01867"/>
    <id>http://arxiv.org/abs/2306.01867</id>
    <updated>2023-06-06T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breen_E/0/1/0/all/0/1&quot;&gt;Emmett Breen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirka_R/0/1/0/all/0/1&quot;&gt;Renee Mirka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zichen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williamson_D/0/1/0/all/0/1&quot;&gt;David P. Williamson&lt;/a&gt;&lt;/p&gt;&lt;p&gt;This paper revisits the 2-approximation algorithm for $k$-MST presented by
Garg in light of a recent paper of Paul et al.. In the $k$-MST problem, the
goal is to return a tree spanning $k$ vertices of minimum total edge cost. Paul
et al. extend Garg&#39;s primal-dual subroutine to improve the approximation ratios
for the budgeted prize-collecting traveling salesman and minimum spanning tree
problems. We follow their algorithm and analysis to provide a cleaner version
of Garg&#39;s result. Additionally, we introduce the novel concept of a kernel
which allows an easier visualization of the stages of the algorithm and a
clearer understanding of the pruning phase. Other notable updates include
presenting a linear programming formulation of the $k$-MST problem, including
pseudocode, replacing the coloring scheme used by Garg with the simpler concept
of neutral sets, and providing an explicit potential function.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Fast $(1+\varepsilon)$-Approximation Algorithms for Binary Matrix Factorization</title>
    <link href="http://arxiv.org/abs/2306.01869"/>
    <id>http://arxiv.org/abs/2306.01869</id>
    <updated>2023-06-06T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velingker_A/0/1/0/all/0/1&quot;&gt;Ameya Velingker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Votsch_M/0/1/0/all/0/1&quot;&gt;Maximilian V&amp;#xf6;tsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1&quot;&gt;David P. Woodruff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Samson Zhou&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We introduce efficient $(1+\varepsilon)$-approximation algorithms for the
binary matrix factorization (BMF) problem, where the inputs are a matrix
$\mathbf{A}\in\{0,1\}^{n\times d}$, a rank parameter $k&amp;gt;0$, as well as an
accuracy parameter $\varepsilon&amp;gt;0$, and the goal is to approximate $\mathbf{A}$
as a product of low-rank factors $\mathbf{U}\in\{0,1\}^{n\times k}$ and
$\mathbf{V}\in\{0,1\}^{k\times d}$. Equivalently, we want to find $\mathbf{U}$
and $\mathbf{V}$ that minimize the Frobenius loss $\|\mathbf{U}\mathbf{V} -
\mathbf{A}\|_F^2$. Before this work, the state-of-the-art for this problem was
the approximation algorithm of Kumar et. al. [ICML 2019], which achieves a
$C$-approximation for some constant $C\ge 576$. We give the first
$(1+\varepsilon)$-approximation algorithm using running time singly exponential
in $k$, where $k$ is typically a small integer. Our techniques generalize to
other common variants of the BMF problem, admitting bicriteria
$(1+\varepsilon)$-approximation algorithms for $L_p$ loss functions and the
setting where matrix operations are performed in $\mathbb{F}_2$. Our approach
can be implemented in standard big data models, such as the streaming or
distributed models.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Auditable data structures: theory and applications</title>
    <link href="http://arxiv.org/abs/2306.01886"/>
    <id>http://arxiv.org/abs/2306.01886</id>
    <updated>2023-06-06T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canciani_A/0/1/0/all/0/1&quot;&gt;Andrea Canciani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felicioli_C/0/1/0/all/0/1&quot;&gt;Claudio Felicioli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Severino_F/0/1/0/all/0/1&quot;&gt;Fabio Severino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tortola_D/0/1/0/all/0/1&quot;&gt;Domenico Tortola&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Every digital process needs to consume some data in order to work properly.
It is very common for applications to use some external data in their
processes, getting them by sources such as external APIs. Therefore, trusting
the received data becomes crucial in such scenarios, considering that if the
data are not self-produced by the consumer, the trust in the external data
source, or in the data that the source produces, can not always be taken for
granted. The most used approach to generate trust in the external source is
based on authenticated data structures, that are able to authenticate the
source when queried through the generation of proofs. Such proofs are useful to
assess authenticity or integrity, however, an external user could also be
interested in verifying the data history and its consistency. This problem
seems to be unaddressed by current literature, which proposes some approaches
aimed at executing audits by internal actors with prior knowledge about the
data structures. In this paper, we address the scenario of an external auditor
with no data knowledge that wants to verify the data history consistency. We
analyze the terminology and the current state of the art of the auditable data
structures, then we will propose a general framework to support external audits
from both internal and external users.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Windows on Theory: The (local) unit of intelligence is FLOPs</title>
    <link href="https://windowsontheory.org/2023/06/05/the-local-unit-of-intelligence-is-flops/"/>
    <id>http://windowsontheory.org/?p=8630</id>
    <updated>2023-06-05T18:22:58+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;&lt;em&gt;[Crossposting again on&amp;nbsp;&lt;/em&gt;&lt;a href=&quot;https://www.lesswrong.com/&quot;&gt;&lt;em&gt;&lt;u&gt;Lesswrong&lt;/u&gt;&lt;/em&gt;&lt;/a&gt;&lt;em&gt; and&amp;nbsp;&lt;/em&gt;&lt;a href=&quot;https://windowsontheory.org/&quot;&gt;&lt;em&gt;&lt;u&gt;Windowsontheory&lt;/u&gt;&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, with the hope I am not overstaying my welcome in LW.]&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;&lt;br&gt;Wealth can be measured by &lt;em&gt;dollars&lt;/em&gt;. This is not a perfect measurement: it’s hard to account for purchasing power and circumstances when comparing people across varying countries or time periods. However, within a particular place and time, one can measure wealth in the local currency. It still does not capture everything (e.g., future earnings, social connections). But generally, all else being roughly equal, the more dollars one has, the wealthier one is.&lt;/p&gt;



&lt;p&gt;How do we measure intelligence? I am not interested in measuring the intelligence of individual humans or individual animals. Nor am I looking for a universal absolute scale of intelligence on which we could rank humans, elephants, and GPT4. (Indeed, it doesn’t seem that a one-dimensional comparison can be made; for example, we seem to be more intelligent than elephants on most dimensions, but they do have an&amp;nbsp;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S014976340700070X&quot;&gt;&lt;u&gt;impressive memory&lt;/u&gt;&lt;/a&gt;.)&amp;nbsp; Rather, I want to compare different&amp;nbsp;&lt;em&gt;species&lt;/em&gt; within the same genus or different&amp;nbsp;&lt;em&gt;models&lt;/em&gt; within the same general architecture (e.g., Transformers).&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I think it’s fair to say that the local unit of intelligence for animal species is&amp;nbsp;&lt;em&gt;neurons&lt;/em&gt;. While elephants have larger brains than humans, within the genus&amp;nbsp;&lt;em&gt;Homo&lt;/em&gt;, to a first approximation, the bigger the brain, the more intelligent the species.&amp;nbsp;&lt;/p&gt;



&lt;figure class=&quot;wp-block-image&quot;&gt;&lt;img src=&quot;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e5d88d991fc175c7676c6ada658142cd44b9887c69c15521.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;



&lt;p&gt;(Figure from&amp;nbsp;&lt;a href=&quot;https://chomsky.info/20140826/&quot;&gt;&lt;u&gt;Bolihus et al.&lt;/u&gt;&lt;/a&gt;)&lt;/p&gt;



&lt;p&gt;I claim that within the current architectures and training frameworks of large language models,&amp;nbsp;&lt;strong&gt;the local unit of intelligence is FLOPs&lt;/strong&gt;. That is, as long as we follow the current paradigm of training transformer-based architectures within best practices of scaling compute and data, the more compute resources (FLOPs) invested in training the model, the more intelligent it is. This is an imperfect measurement, but probably one that is better than trying to give models “IQ exams” that were designed for humans (and even there have&amp;nbsp;&lt;a href=&quot;https://erikhoel.substack.com/p/your-iq-isnt-160-no-ones-is&quot;&gt;&lt;u&gt;dubious value&lt;/u&gt;&lt;/a&gt;).&amp;nbsp; Another way to say this is that the intelligence of the model scales with the number of&amp;nbsp;&lt;strong&gt;“load-bearing gradient steps”&lt;/strong&gt; that have gone into training it.&lt;/p&gt;



&lt;p&gt;So far, it might seem like a tautology, but as I claimed in the&amp;nbsp;&lt;a href=&quot;https://windowsontheory.org/2023/05/19/gpt-as-an-intelligence-forklift/&quot;&gt;&lt;u&gt;“intelligence forklift” post&lt;/u&gt;&lt;/a&gt;, this does have some implications. In particular, current general-purpose models such as ChatGPT are built in two phases. The first phase is a&amp;nbsp;&lt;strong&gt;pretraining phase&lt;/strong&gt;, in which the model is trained in a Trillion or more gradient steps on the next-token prediction task. The second phase is the&amp;nbsp;&lt;strong&gt;adaptation/fine-tuning phase&lt;/strong&gt;, in which, whether through instruction-tuning, reinforcement learning on human feedback (RLHF) or other methods, the model is “fine tuned” using fewer than a million gradient steps to be a better instruction-following or chatting agent. In other words, more than 99.9% (maybe as much as 99.9999%) of the FLOPs / gradient steps in training the model are invested during its pretraining phase. (One reason that the fine-tuning phase involves much fewer gradient steps is that, while the first phase can use any static data grabbed from the Internet, the second phase requires data that was especially collected for this task and often needs human labeling as well.)&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The adaptation phase can make a huge difference in the usefulness of the model. The&amp;nbsp;&lt;a href=&quot;https://chat.lmsys.org/?arena&quot;&gt;&lt;u&gt;chatbot arena&lt;/u&gt;&lt;/a&gt; doesn’t even contain non-fine-tuned models, and we can see that smaller but well-tuned models can put up a decent fight against ones that have at least 10 times the parameters (and so roughly at least 100 times the training compute). Unlike sashimi, language models should not be consumed raw. &amp;nbsp;&lt;/p&gt;



&lt;figure class=&quot;wp-block-image&quot;&gt;&lt;img src=&quot;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/fb47b643997a81bbcc3b8b7ef043ec132b28ff4071de246f.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;



&lt;p&gt;&lt;br&gt;However, their “intelligence” is ultimately derived from the FLOPs invested in the base models. (See also &lt;a href=&quot;https://arxiv.org/abs/2305.15717&quot;&gt;&lt;u&gt;this paper &lt;/u&gt;&lt;/a&gt;on the limitations of fine-tuning to close capability gaps.) Fine-tuning, whether using RL or not, is the proverbial “&lt;a href=&quot;https://medium.com/syncedreview/yann-lecun-cake-analogy-2-0-a361da560dae&quot;&gt;&lt;u&gt;cherry on the cake&lt;/u&gt;&lt;/a&gt;” and the pre-trained model captures more than 99.9% of the intelligence of the model.  That pretrained model is &lt;a href=&quot;https://www.lesswrong.com/s/N7nDePaNabJdnbXeE/p/vJFdjigzmcXMhNTsx&quot;&gt;&lt;u&gt;not an agent&lt;/u&gt;&lt;/a&gt; and &lt;a href=&quot;https://astralcodexten.substack.com/p/janus-simulators&quot;&gt;&lt;u&gt;does not have goals&lt;/u&gt;&lt;/a&gt; though it can “play one on TV” in the sense of coming up with plans and proposed actions if prompted to do so. (In LW language, a &lt;a href=&quot;https://www.lesswrong.com/tag/simulator-theory&quot;&gt;simulator&lt;/a&gt;.) This is why a pretrained model can be modeled as an &lt;a href=&quot;https://www.lesswrong.com/posts/wDL6wiqg3c6WFisHq/gpt-as-an-intelligence-forklift&quot;&gt;&lt;u&gt;“intelligence forklift”&lt;/u&gt;&lt;/a&gt;. Just like a forklift supplies strength but is useless without someone driving it, so does the pretrained model supply intelligence, but that intelligence needs to be directed via fine-tuning, conditioning on prompts, etc.  Another way to think of the pre-trained model is as the bee colony and the adapter as the queen. (That is, if the queen bee was actually telling bees what to do rather than just &lt;a href=&quot;https://www.perfectbee.com/learn-about-bees/the-life-of-bees/role-queen-bee&quot;&gt;&lt;u&gt;laying eggs&lt;/u&gt;&lt;/a&gt;.)&lt;/p&gt;



&lt;figure class=&quot;wp-block-image&quot;&gt;&lt;img src=&quot;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/da45c0196d68d4c0a178bc2d54360510fa6ca80b2caa40ec.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;



&lt;p&gt;In that sense, while I agree with&amp;nbsp;&lt;a href=&quot;https://gwern.net/tool-ai&quot;&gt;&lt;u&gt;Gwern&lt;/u&gt;&lt;/a&gt; that agentic models are more&amp;nbsp;&lt;em&gt;useful&lt;/em&gt; and that&lt;em&gt; “we don’t want low log-loss error on ImageNet, we want to refind a particular personal photo”&lt;/em&gt; , I disagree that&amp;nbsp;&lt;em&gt;“Agent AIs [will be] more intelligent than Tool AIs.”&amp;nbsp;&lt;/em&gt;Intelligence and usefulness are not the same thing.&lt;/p&gt;



&lt;h2 class=&quot;wp-block-heading&quot;&gt;Implications for alignment&lt;/h2&gt;



&lt;p&gt;If the pre-trained model does not have goals, then there is no sense in “aligning” it. Rather, there is a separation of concerns, with a highly intelligent but goal-less pre-trained model (“forklift”) and a not-so-intelligent but goal-directed adaptor (“driver”). It is the latter one that we need to align:&lt;/p&gt;



&lt;blockquote class=&quot;wp-block-quote&quot;&gt;
&lt;p&gt;&lt;strong&gt;The component of an AI system that needs to be aligned is not the component that accounts for its intelligence.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;



&lt;p&gt;That is a hopeful lesson since the adaptor can be a much smaller (e.g. have drastically&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot;&gt;&lt;u&gt;fewer parameters&lt;/u&gt;&lt;/a&gt;) and tractable object. However, it does not mean that the alignment problem is easy and that we are insulated from the complexities of the pretrained model:&lt;/p&gt;



&lt;blockquote class=&quot;wp-block-quote&quot;&gt;
&lt;p&gt;&lt;strong&gt;A forklift with a speed of 1000mph might not be actively trying to kill you, but this could still be the end result.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;



&lt;p&gt;In particular, we don’t understand the biases the pre-trained model inherits from the data, nor the way that these may play out when we use the model in applications. However, it does seem that for a pretrained model to be as good at its job as possible, it should learn all the biases in its data but not be constrained to any of them. It should be able to adapt to any context real or imagined and be the “perfect actor” that can take on any character’s personality.&lt;/p&gt;



&lt;p&gt;The traditional “anthropomorphic” view of intelligence is as something that “belongs” to an individual or&amp;nbsp;&lt;em&gt;agent&amp;nbsp;&lt;/em&gt;and that this agent has some sort of preferences or goals (a.k.a a&amp;nbsp;&lt;em&gt;utility function&lt;/em&gt;). Hence a potential future super-intelligent AI was thought of as an “alien” that pursues some goals. Under this viewpoint, we want to either “box” the alien to control its impact or “align” its goals to ours. Both of these options treat the AI system as a single component encompassing both goals and intelligence. However, if goals and intelligence parts correspond to different components, we may be able to&amp;nbsp;&lt;strong&gt;“take the alien’s brain for a ride”&lt;/strong&gt; and build a variety of systems that share the same&amp;nbsp;&lt;em&gt;capabilities&lt;/em&gt; but have very different objectives and profiles.&lt;/p&gt;



&lt;p&gt;To be clear, the “intelligence forklift” view does not preclude building an “anti-aligned” agent on top of a pre-trained model that is&amp;nbsp;&lt;em&gt;malicious&lt;/em&gt;,&amp;nbsp;&lt;em&gt;dishonest&lt;/em&gt;, and&amp;nbsp;&lt;em&gt;harmful&lt;/em&gt;. It just means that such an agent would not have an automatic intelligence advantage over other agents (including humans) since all of them can have access to a shared “intelligence engine” provided by the goal-less pretrained models. This is what I illustrated as “scenario 2” in this figure (taken from my&amp;nbsp;&lt;a href=&quot;https://www.lesswrong.com/posts/wDL6wiqg3c6WFisHq/gpt-as-an-intelligence-forklift&quot;&gt;&lt;u&gt;previous post&lt;/u&gt;&lt;/a&gt;):&lt;/p&gt;



&lt;figure class=&quot;wp-block-image&quot;&gt;&lt;img src=&quot;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/67505f5594ae360d8e9d1949c3e201489116d2d8c1ee48ba.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;



&lt;h2 class=&quot;wp-block-heading&quot;&gt;&lt;strong&gt;What about “self play”?&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;The above assumes that the intelligence component of a model is obtained by executing gradient steps on static data, but what if this data is itself generated by the model? This is what happened with games such as Go and Chess. Originally models were trained by predicting the next move of human games scraped from the Internet, but to improve beyond the quality of these data, models needed to play against themselves and generate new games. They could then filter out only the most successful ones and hence generate data that is of higher quality than the original games they trained on. (Eventually, it turned out that with this approach you don’t need to start with&amp;nbsp;&lt;em&gt;any&lt;/em&gt; data for games such as Chess and Go, hence the “Zero” in AlphaZero.)&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Self-play makes a lot of sense in games where there is a very clear notion of winning and losing, but what would be the analog for language models? I don’t know the answer to this in general, but in the realm of scientific literature, there is an analogous process. The model could play the roles of authors and reviewers alike, generate new papers, subject them to peer review, revise and resubmit, etc. At least in fields that don’t require “wet labs”, this could lead to the model simulating the scientific literature of 2024, then 2025, and so on and so forth. Models that manage to do this would be amazing and would speed up scientific progress tremendously. However, I believe they could still be (just more powerful) “intelligence forklifts”. Model outputs influencing its inputs can lead to a &amp;#8220;positive feedback loop,&amp;#8221; and so this is not certain. But I do not see an inherent reason why models could not be arbitrarily intelligent and still completely without goals. In the &lt;a href=&quot;https://astralcodexten.substack.com/p/janus-simulators&quot;&gt;&lt;u&gt;words of Scott Alexander&lt;/u&gt;&lt;/a&gt;, no matter how intelligent they are, models could still be “enlightened” and realize that&lt;/p&gt;



&lt;blockquote class=&quot;wp-block-quote&quot;&gt;
&lt;p&gt;&lt;strong&gt;“once you stop obsessing over the character you’re playing, you notice the GIANT SUPER-ACCURATE WORLD MODEL TAKING UP 99.99% OF YOUR BRAIN.”&lt;/strong&gt;&lt;br&gt;&amp;nbsp;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p class=&quot;authors&quot;&gt;By Boaz Barak&lt;/p&gt;
  </content>
    <author>
      <name>Windows on Theory</name>
      <uri>https://windowsontheory.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Emanuele Viola: Mathematics of the impossible, draft of a book</title>
    <link href="https://emanueleviola.wordpress.com/2023/06/05/mathematics-of-the-impossible-draft-of-a-book/"/>
    <id>http://emanueleviola.wordpress.com/?p=1259</id>
    <updated>2023-06-05T17:23:16+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;I posted a first draft of the book, &lt;a href=&quot;https://www.ccs.neu.edu/home/viola/papers/moti.pdf&quot;&gt;here&lt;/a&gt;. It has more material than the previous blog posts, including a chapter on communication complexity. I plan a major revision, including adding several chapters, but it seems that won&amp;#8217;t happen right away, so I am releasing what I have for now. Any comments are appreciated, either on this blog or via email.&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Manu&lt;/p&gt;
  </content>
    <author>
      <name>Emanuele Viola</name>
      <uri>https://emanueleviola.wordpress.com</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">CCI: jobs: Tenure-track faculty at The Australian National University (apply by May 31, 2024)</title>
    <link href="https://cstheory-jobs.org/2023/06/05/tenure-track-faculty-at-the-australian-national-university-apply-by-may-31-2024/"/>
    <id>http://cstheory-jobs.org/2023/06/05/tenure-track-faculty-at-the-australian-national-university-apply-by-may-31-2024/</id>
    <updated>2023-06-05T06:20:52+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;Tenure-track faculty members in the School of Computing at the Australian National University. Please see the link below for more information.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://jobs.anu.edu.au/jobs/tenure-track-lecturer-senior-lecturer-associate-professor-school-of-computing-canberra-act-act-australia&quot;&gt;https://jobs.anu.edu.au/jobs/tenure-track-lecturer-senior-lecturer-associate-professor-school-of-computing-canberra-act-act-australia&lt;/a&gt;&lt;br /&gt;
Email: ahadn.zehmakan@anu.edu.au&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </content>
    <author>
      <name>CCI: jobs</name>
      <uri>https://cstheory-jobs.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">ECCC Papers: TR23-085 |  Average-Case PAC-Learning from Nisan&amp;#39;s Natural Proofs | 

	Ari Karchmer</title>
    <link href="https://eccc.weizmann.ac.il/report/2023/085"/>
    <id>https://eccc.weizmann.ac.il/report/2023/085</id>
    <updated>2023-06-05T05:39:55+00:00</updated>
    <content type="html" xml:lang="en">
    Carmosino et al. (2016) demonstrated that natural proofs of circuit lower bounds imply algorithms for learning circuits with membership queries over the uniform distribution. Indeed, they exercised this implication to obtain a quasi-polynomial time learning algorithm for ${AC}^0[p]$ circuits, for any prime $p$, by leveraging the existing natural proofs from Razborov (1987) and Smolensky (1987). This achievement raises a logical question: can existing natural proofs be adapted into learning algorithms that utilize random examples and learn over unknown, arbitrary example distributions? 

In this work, we show that natural circuit lower bounds proven by specific communication complexity arguments (e.g., Nisan (1994)) witness a ``yes&amp;#39;&amp;#39; answer to this question, under the one limitation of average-case learning. Our primary technical contribution demonstrates a connection between the complexity of learning a concept class in the average-case, and the randomized communication complexity of an evaluation game associated with the class.  We apply this finding to derive polynomial time average-case PAC-learning algorithms that use only random examples from arbitrary and unknown distributions, for any concept class that may be evaluated by (for instance) a majority vote of linear threshold functions.

Additionally, our work contributes to a better understanding of the optimal parameters in XOR lemmas for communication complexity. We address a question posed by Viola and Wigderson (2007) by demonstrating that certain enhancements of parameters in their XOR lemmas are false, assuming the existence of one-way functions.
  </content>
    <author>
      <name>ECCC Papers</name>
      <uri>https://eccc.weizmann.ac.il/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Computational Complexity: Quantifiers: To Parenthesize or not to Parenthesize?  Matrix of Formula: To Bracket or not to Bracket?</title>
    <link href="https://blog.computationalcomplexity.org/2023/06/quantifiers-to-parenthesize-or-not-to.html"/>
    <id>tag:blogger.com,1999:blog-3722233.post-8658886384920717972</id>
    <updated>2023-06-05T02:42:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;&amp;nbsp;For the book&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Computational&amp;nbsp; Intractability: A Guide to Algorithmic Lower Bounds&lt;/b&gt;&lt;/p&gt;&lt;p&gt;by Demaine-Gasarch-Hajiaghayi&amp;nbsp;&lt;/p&gt;&lt;p&gt;(See&amp;nbsp; &lt;a href=&quot;https://hardness.mit.edu/&quot;&gt;here&lt;/a&gt;&amp;nbsp;for a link to a first draft.)&amp;nbsp;&lt;/p&gt;&lt;p&gt;we had to make some choices about which notation to use. One of the least important ones was the following:&amp;nbsp;&lt;/p&gt;&lt;p&gt;When defining NP, and in a few other places should we use:&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; (\exists y)(\forall y)[B(x,y)]&lt;/p&gt;&lt;p&gt;or&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp;\exists x : \forall y : B(x,y)&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;or&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; something else.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;We ended up doing it the second way.&amp;nbsp; But I wondered, which, if either, is standard. So I looked in many math and theoretical CS books looking for places they used quantifiers. Here is what I found&lt;/p&gt;&lt;p&gt;a) Most papers and books really don&#39;t use quantifiers at all!&amp;nbsp; This surprised me.&amp;nbsp;&lt;/p&gt;&lt;p&gt;b) When quantifiers are used, they are used in definitions, not theorems.&amp;nbsp;&lt;/p&gt;&lt;p&gt;c) One exception is in logic when they deal with formulas as objects onto themselves.&amp;nbsp; For example, the inductive definition of a formula will have a step:&lt;/p&gt;&lt;p&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;If f(x_1,...,x_n) is a formula then (\exists x_i)[f(x_1,...,x_n)] is a formula.&amp;nbsp;&lt;/p&gt;&lt;p&gt;d) Here is a list of the few places I saw quantifiers used and if they used parenthesis or not. I say if it has parenthesis (abbreviated Parens)&amp;nbsp; or not, and if the matrix of the formula is in square brackets, no brackets, or something ese.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/800157.805047&quot;&gt;Cook&#39;s classic paper&lt;/a&gt;&amp;nbsp;.&amp;nbsp;&lt;/i&gt;Page 154 Parens, no Brackets (1971)&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/030439757690061X?via%3Dihub&quot;&gt;&lt;i&gt;Stockmeyer&#39;s paper where he defines PH&lt;/i&gt;&lt;/a&gt;.&amp;nbsp; Page 6 Parens and Brackets&amp;nbsp; (1976)&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Computers and Intractability&lt;/i&gt; by Garey &amp;amp; Johnson. Page 164. Parens and Brackets (1979)&lt;/p&gt;&lt;p&gt;&lt;i&gt;Morass-like construction of aleph_2 trees in L&lt;/i&gt; by Devlin.&amp;nbsp; Page 2 Parens and matrix in Parens (1979)&lt;/p&gt;&lt;p&gt;&lt;i&gt;Descriptive Complexity by Immerman.&lt;/i&gt;&amp;nbsp;Page 38 Parens no Brackets (1999)&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Bounded Queries in Recursion Theory &lt;/i&gt;by Gasarch and Martin. Parens and Brackets&amp;nbsp; Throughout the book.&amp;nbsp; (1999)&lt;/p&gt;&lt;p&gt;&lt;i&gt;Complexity Theory from Godel to Feynman&lt;/i&gt;&amp;nbsp;by Rudich. No Parens, No Brackets in Def of PH. (2003)&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://yaroslavvb.com/upload/flum.pdf&quot;&gt;Parameterized Complexity Theory&lt;/a&gt;&amp;nbsp;by Flum &amp;amp; Grohe. Page 81 no Parens and no Brackets.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;a href=&quot;https://theory.cs.princeton.edu/complexity/book.pdf&quot;&gt;Computational Complexity: A Modern Approach&lt;/a&gt;&amp;nbsp;&lt;/i&gt;by Arora &amp;amp; Barak. Page 40. No Parens No Brackets.(2007)&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://theswissbay.ch/pdf/Gentoomen%20Library/Theory%20Of%20Computation/Oded_Goldreich-Computational_Complexity__A_Conceptual_Perspective%282008%29.pdf&quot;&gt;Computational Complexity: A Conceptual Prospective&lt;/a&gt;&amp;nbsp;by Goldreich.&amp;nbsp; Page 114 no parents, no brackets (2008)&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0890540109002338&quot;&gt;On Quantifer Rank Equivalence between linear orders by Siders&lt;/a&gt;.&amp;nbsp;&lt;/i&gt;On page 417 they use quantifiers to state a theorem, which is unusual. Parens no brackets.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1211.0020&quot;&gt;&lt;i&gt;Presburger arithmetic, Rational Generating Functions, and quasi polynomials&lt;/i&gt;&lt;/a&gt;&amp;nbsp;by Woods. Parens no&amp;nbsp; Brackets. (2012)&amp;nbsp;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://erikdemaine.org/papers/Witness_TCS/paper.pdf#page=69&quot;&gt;Who witness&#39;s the Witness by Abel et al.&lt;/a&gt;&amp;nbsp;On Page 69 (which the pointer takes you to) No Parens, no brackets. Colons between quantifiers (2018).&lt;/p&gt;&lt;p&gt;e) What to make of all this?&lt;/p&gt;&lt;p&gt;First off- the RARITY of the use of quantifiers really surprised me. The only place I saw them used a lot was my book, co-authored with Georgie Martin,&amp;nbsp;&amp;nbsp;&lt;i&gt;Bounded Queries in Recursion Theory. &lt;/i&gt;Perhaps it would have sold better if I didn&#39;t use so many quantifiers. Oh well.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Second off- Later works don&#39;t use parens and brackets. This is most clear if you just look at Complexity Theory Books&amp;nbsp;&lt;/p&gt;&lt;p&gt;Garey &amp;amp; Johnson - 1979- parens and brackets&lt;/p&gt;&lt;p&gt;Flun &amp;amp; Grohe- 1998- no parens and no brackts&lt;/p&gt;&lt;p&gt;Immerman- 1999 - parens but no brackets (this is the one exception)&amp;nbsp;&lt;/p&gt;&lt;p&gt;Arora &amp;amp; Barack- 2007 no parens and&amp;nbsp; no brackets&lt;/p&gt;&lt;p&gt;Goldreich-2008- no parens and no brackets&lt;/p&gt;&lt;p&gt;If you have a complexity theory book around that is not on this list, look up the definition of NP and the definition of the Poly Hierarchy and see (a) if they use parens around the quantifiers, and (b) if they use square brackets or no brackets of something else. Please leave a comment about it so I test the conjecture that parenthesis are just so 1979.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By gasarch&lt;/p&gt;
  </content>
    <author>
      <name>Computational Complexity</name>
      <uri>http://blog.computationalcomplexity.org/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Complexity of Motion Planning of Arbitrarily Many Robots: Gadgets, Petri Nets, and Counter Machines</title>
    <link href="http://arxiv.org/abs/2306.01193"/>
    <id>http://arxiv.org/abs/2306.01193</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ani_J/0/1/0/all/0/1&quot;&gt;Joshua Ani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coulombe_M/0/1/0/all/0/1&quot;&gt;Michael Coulombe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demaine_E/0/1/0/all/0/1&quot;&gt;Erik D. Demaine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diomidov_Y/0/1/0/all/0/1&quot;&gt;Yevhenii Diomidov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_T/0/1/0/all/0/1&quot;&gt;Timothy Gomez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendrickson_D/0/1/0/all/0/1&quot;&gt;Dylan Hendrickson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lynch_J/0/1/0/all/0/1&quot;&gt;Jayson Lynch&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We extend the motion-planning-through-gadgets framework to several new
scenarios involving various numbers of robots/agents, and analyze the
complexity of the resulting motion-planning problems. While past work considers
just one robot or one robot per player, most of our models allow for one or
more locations to spawn new robots in each time step, leading to arbitrarily
many robots. In the 0-player context, where all motion is deterministically
forced, we prove that deciding whether any robot ever reaches a specified
location is undecidable, by representing a counter machine. In the 1-player
context, where the player can choose how to move the robots, we prove
equivalence to Petri nets, EXPSPACE-completeness for reaching a specified
location, PSPACE-completeness for reconfiguration, and ACKERMANN-completeness
for reconfiguration when robots can be destroyed in addition to spawned.
Finally, we consider a variation on the standard 2-player context where,
instead of one robot per player, we have one robot shared by the players, along
with a ko rule to prevent immediately undoing the previous move. We prove this
impartial 2-player game EXPTIME-complete.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Trade-offs between Entanglement and Communication</title>
    <link href="http://arxiv.org/abs/2306.01233"/>
    <id>http://arxiv.org/abs/2306.01233</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Arunachalam_S/0/1/0/all/0/1&quot;&gt;Srinivasan Arunachalam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Girish_U/0/1/0/all/0/1&quot;&gt;Uma Girish&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the advantages of quantum communication models over classical
communication models that are equipped with a limited number of qubits of
entanglement. In this direction, we give explicit partial functions on $n$ bits
for which reducing the entanglement increases the classical communication
complexity exponentially. Our separations are as follows. For every $k\ge 1$:
&lt;/p&gt;
&lt;p&gt;$Q\|^*$ versus $R2^*$: We show that quantum simultaneous protocols with
$\tilde{\Theta}(k^5 \log^3 n)$ qubits of entanglement can exponentially
outperform two-way randomized protocols with $O(k)$ qubits of entanglement.
This resolves an open problem from [Gav08] and improves the state-of-the-art
separations between quantum simultaneous protocols with entanglement and
two-way randomized protocols without entanglement [Gav19, GRT22].
&lt;/p&gt;
&lt;p&gt;$R\|^*$ versus $Q\|^*$: We show that classical simultaneous protocols with
$\tilde{\Theta}(k \log n)$ qubits of entanglement can exponentially outperform
quantum simultaneous protocols with $O(k)$ qubits of entanglement, resolving an
open question from [GKRW06, Gav19]. The best result prior to our work was a
relational separation against protocols without entanglement [GKRW06].
&lt;/p&gt;
&lt;p&gt;$R\|^*$ versus $R1^*$: We show that classical simultaneous protocols with
$\tilde{\Theta}(k\log n)$ qubits of entanglement can exponentially outperform
randomized one-way protocols with $O(k)$ qubits of entanglement. Prior to our
work, only a relational separation was known [Gav08].
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Discreteness of asymptotic tensor ranks</title>
    <link href="http://arxiv.org/abs/2306.01718"/>
    <id>http://arxiv.org/abs/2306.01718</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briet_J/0/1/0/all/0/1&quot;&gt;Jop Bri&amp;#xeb;t&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christandl_M/0/1/0/all/0/1&quot;&gt;Matthias Christandl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leigh_I/0/1/0/all/0/1&quot;&gt;Itai Leigh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shpilka_A/0/1/0/all/0/1&quot;&gt;Amir Shpilka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuiddam_J/0/1/0/all/0/1&quot;&gt;Jeroen Zuiddam&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Tensor parameters that are amortized or regularized over large tensor powers,
often called &quot;asymptotic&quot; tensor parameters, play a central role in several
areas including algebraic complexity theory (constructing fast matrix
multiplication algorithms), quantum information (entanglement cost and
distillable entanglement), and additive combinatorics (bounds on cap sets,
sunflower-free sets, etc.). Examples are the asymptotic tensor rank, asymptotic
slice rank and asymptotic subrank. Recent works (Costa-Dalai,
Blatter-Draisma-Rupniewski, Christandl-Gesmundo-Zuiddam) have investigated
notions of discreteness (no accumulation points) or &quot;gaps&quot; in the values of
such tensor parameters.
&lt;/p&gt;
&lt;p&gt;We prove a general discreteness theorem for asymptotic tensor parameters of
order-three tensors and use this to prove that (1) over any finite field, the
asymptotic subrank and the asymptotic slice rank have no accumulation points,
and (2) over the complex numbers, the asymptotic slice rank has no accumulation
points.
&lt;/p&gt;
&lt;p&gt;Central to our approach are two new general lower bounds on the asymptotic
subrank of tensors, which measures how much a tensor can be diagonalized. The
first lower bound says that the asymptotic subrank of any concise three-tensor
is at least the cube-root of the smallest dimension. The second lower bound
says that any three-tensor that is &quot;narrow enough&quot; (has one dimension much
smaller than the other two) has maximal asymptotic subrank.
&lt;/p&gt;
&lt;p&gt;Our proofs rely on new lower bounds on the maximum rank in matrix subspaces
that are obtained by slicing a three-tensor in the three different directions.
We prove that for any concise tensor the product of any two such maximum ranks
must be large, and as a consequence there are always two distinct directions
with large max-rank.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Efficient Quantum State Synthesis with One Query</title>
    <link href="http://arxiv.org/abs/2306.01723"/>
    <id>http://arxiv.org/abs/2306.01723</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Rosenthal_G/0/1/0/all/0/1&quot;&gt;Gregory Rosenthal&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We present a polynomial-time quantum algorithm making a single query (in
superposition) to a classical oracle, such that for every state $|\psi\rangle$
there exists a choice of oracle that makes the algorithm construct an
exponentially close approximation of $|\psi\rangle$. Previous algorithms for
this problem either used a linear number of queries and polynomial time
[&lt;a href=&quot;/abs/1607.05256&quot;&gt;arXiv:1607.05256&lt;/a&gt;], or a constant number of queries and polynomially many
ancillae but no nontrivial bound on the runtime [&lt;a href=&quot;/abs/2111.02999&quot;&gt;arXiv:2111.02999&lt;/a&gt;]. As
corollaries we do the following:
&lt;/p&gt;
&lt;p&gt;- We simplify the proof that statePSPACE $\subseteq$ stateQIP
[&lt;a href=&quot;/abs/2108.07192&quot;&gt;arXiv:2108.07192&lt;/a&gt;] (a quantum state analogue of PSPACE $\subseteq$ IP) and show
that a constant number of rounds of interaction suffices.
&lt;/p&gt;
&lt;p&gt;- We show that QAC$\mathsf{_f^0}$ lower bounds for constructing explicit
states would imply breakthrough circuit lower bounds for computing explicit
boolean functions.
&lt;/p&gt;
&lt;p&gt;- We prove that every $n$-qubit state can be constructed to within 0.01 error
by an $O(2^n/n)$-size circuit over an appropriate finite gate set. More
generally we give a size-error tradeoff which, by a counting argument, is
optimal for any finite gate set.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: Does it pay to optimize AUC?</title>
    <link href="http://arxiv.org/abs/2306.01528"/>
    <id>http://arxiv.org/abs/2306.01528</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Baojian Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skiena_S/0/1/0/all/0/1&quot;&gt;Steven Skiena&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The Area Under the ROC Curve (AUC) is an important model metric for
evaluating binary classifiers, and many algorithms have been proposed to
optimize AUC approximately. It raises the question of whether the generally
insignificant gains observed by previous studies are due to inherent
limitations of the metric or the inadequate quality of optimization.
&lt;/p&gt;
&lt;p&gt;To better understand the value of optimizing for AUC, we present an efficient
algorithm, namely AUC-opt, to find the provably optimal AUC linear classifier
in $\mathbb{R}^2$, which runs in $\mathcal{O}(n_+ n_- \log (n_+ n_-))$ where
$n_+$ and $n_-$ are the number of positive and negative samples respectively.
Furthermore, it can be naturally extended to $\mathbb{R}^d$ in
$\mathcal{O}((n_+n_-)^{d-1}\log (n_+n_-))$ by calling AUC-opt in
lower-dimensional spaces recursively. We prove the problem is NP-complete when
$d$ is not fixed, reducing from the \textit{open hemisphere problem}.
&lt;/p&gt;
&lt;p&gt;Experiments show that compared with other methods, AUC-opt achieves
statistically significant improvements on between 17 to 40 in $\mathbb{R}^2$
and between 4 to 42 in $\mathbb{R}^3$ of 50 t-SNE training datasets. However,
generally the gain proves insignificant on most testing datasets compared to
the best standard classifiers. Similar observations are found for nonlinear AUC
methods under real-world datasets.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: No-dimensional Tverberg Partitions Revisited</title>
    <link href="http://arxiv.org/abs/2306.01678"/>
    <id>http://arxiv.org/abs/2306.01678</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Har_Peled_S/0/1/0/all/0/1&quot;&gt;Sariel Har-Peled&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robson_E/0/1/0/all/0/1&quot;&gt;Eliot W. Robson&lt;/a&gt;&lt;/p&gt;&lt;p&gt;$ \newcommand{\epsA}{\Mh{\delta}} \newcommand{\Re}{\mathbb{R}}
\newcommand{\reals}{\mathbb{R}} \newcommand{\SetX}{\mathsf{X}}
\newcommand{\diam}{\Delta} \newcommand{\Mh}[1]{#1} \newcommand{\query}{q}
\newcommand{\eps}{\varepsilon} \newcommand{\VorX}[1]{\mathcal{V} \pth{#1}}
\newcommand{\IntRange}[1]{[ #1 ]} \newcommand{\Space}{\overline{\mathsf{m}}}
\newcommand{\pth}[2][\!]{#1\left({#2}\right)}
\newcommand{\polylog}{\mathrm{polylog}} \newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z} \newcommand{\pt}{p} \newcommand{\distY}[2]{\left\|
{#1} - {#2} \right\|} \newcommand{\PP}{P} \newcommand{\ptq}{q}
\newcommand{\pts}{s}$ Given a set $\PP \subset \Re^d$ of $n$ points, with
diameter $\diam$, and a parameter $\epsA \in (0,1)$, it is known that there is
a partition of $\PP$ into sets $\PP_1, \ldots, \PP_t$, each of size
$O(1/\epsA^2)$, such that their convex-hulls all intersect a common ball of
radius $\epsA \diam$. We prove that a random partition, with a simple
alteration step, yields the desired partition, resulting in a linear time
algorithm. Previous proofs were either existential (i.e., at least exponential
time), or required much bigger sets. In addition, the algorithm and its proof
of correctness are significantly simpler than previous work, and the constants
are slightly better.
&lt;/p&gt;
&lt;p&gt;In addition, we provide a linear time algorithm for computing a ``fuzzy&#39;&#39;
centerpoint. We also prove a no-dimensional weak $\eps$-net theorem with an
improved constant.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: The Maximum Matrix Contraction Problem</title>
    <link href="http://arxiv.org/abs/2306.01349"/>
    <id>http://arxiv.org/abs/2306.01349</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watel_D/0/1/0/all/0/1&quot;&gt;Dimitri Watel&lt;/a&gt; (ENSIIE, CEDRIC - OC), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poirion_P/0/1/0/all/0/1&quot;&gt;Pierre-Louis Poirion&lt;/a&gt; (CEDRIC - OC)&lt;/p&gt;&lt;p&gt;In this paper, we introduce the Maximum Matrix Contraction problem, where we
aim to contract as much as possible a binary matrix in order to maximize its
density. We study the complexity and the polynomial approximability of the
problem. Especially, we prove this problem to be NP-Complete and that every
algorithm solving this problem is at most a $2\sqrt{n}$-approximation algorithm
where n is the number of ones in the matrix. We then focus on efficient
algorithms to solve the problem: an integer linear program and three
heuristics.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Improved Algorithms for Distance Selection and Related Problems</title>
    <link href="http://arxiv.org/abs/2306.01073"/>
    <id>http://arxiv.org/abs/2306.01073</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haitao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yiming Zhao&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this paper, we propose new techniques for solving geometric optimization
problems involving interpoint distances of a point set in the plane. Given a
set $P$ of $n$ points in the plane and an integer $1 \leq k \leq \binom{n}{2}$,
the distance selection problem is to find the $k$-th smallest interpoint
distance among all pairs of points of $P$. The previously best deterministic
algorithm solves the problem in $O(n^{4/3} \log^2 n)$ time [Katz and Sharir,
SIAM J. Comput. 1997 and SoCG 1993]. In this paper, we improve their algorithm
to $O(n^{4/3} \log n)$ time. Using similar techniques, we also give improved
algorithms on both the two-sided and the one-sided discrete Fr\&#39;{e}chet
distance with shortcuts problem for two point sets in the plane. For the
two-sided problem (resp., one-sided problem), we improve the previous work
[Avraham, Filtser, Kaplan, Katz, and Sharir, ACM Trans. Algorithms 2015 and
SoCG 2014] by a factor of roughly $\log^2(m+n)$ (resp., $(m+n)^{\epsilon}$),
where $m$ and $n$ are the sizes of the two input point sets, respectively.
Other problems whose solutions can be improved by our techniques include the
reverse shortest path problems for unit-disk graphs. Our techniques are quite
general and we believe they will find many other applications in future.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Labeled Interleaving Distance for Reeb Graphs</title>
    <link href="http://arxiv.org/abs/2306.01186"/>
    <id>http://arxiv.org/abs/2306.01186</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_F/0/1/0/all/0/1&quot;&gt;Fangfei Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parsa_S/0/1/0/all/0/1&quot;&gt;Salman Parsa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bei Wang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Merge trees, contour trees, and Reeb graphs are graph-based topological
descriptors that capture topological changes of (sub)level sets of scalar
fields. Comparing scalar fields using their topological descriptors has many
applications in topological data analysis and visualization of scientific data.
Recently, Munch and Stefanou introduced a labeled interleaving distance for
comparing two labeled merge trees, which enjoys a number of theoretical and
algorithmic properties. In particular, the labeled interleaving distance
between merge trees can be computed in polynomial time. In this work, we define
the labeled interleaving distance for labeled Reeb graphs. We then prove that
the (ordinary) interleaving distance between Reeb graphs equals the minimum of
the labeled interleaving distance over all labelings. We also provide an
efficient algorithm for computing the labeled interleaving distance between two
labeled contour trees (which are special types of Reeb graphs that arise from
simply-connected domains). In the case of merge trees, the notion of the
labeled interleaving distance was used by Gasparovic et al. to prove that the
(ordinary) interleaving distance on the set of (unlabeled) merge trees is
intrinsic. As our final contribution, we present counterexamples showing that,
on the contrary, the (ordinary) interleaving distance on (unlabeled) Reeb
graphs (and contour trees) is not intrinsic. It turns out that, under mild
conditions on the labelings, the labeled interleaving distance is a metric on
isomorphism classes of Reeb graphs, analogous to the ordinary interleaving
distance. This provides new metrics on large classes of Reeb graphs.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Fast Matrix Multiplication Without Tears: A Constraint Programming Approach</title>
    <link href="http://arxiv.org/abs/2306.01097"/>
    <id>http://arxiv.org/abs/2306.01097</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deza_A/0/1/0/all/0/1&quot;&gt;Arnaud Deza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaezipoor_P/0/1/0/all/0/1&quot;&gt;Pashootan Vaezipoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalil_E/0/1/0/all/0/1&quot;&gt;Elias B. Khalil&lt;/a&gt;&lt;/p&gt;&lt;p&gt;It is known that the multiplication of an $N \times M$ matrix with an $M
\times P$ matrix can be performed using fewer multiplications than what the
naive $NMP$ approach suggests. The most famous instance of this is Strassen&#39;s
algorithm for multiplying two $2\times 2$ matrices in 7 instead of 8
multiplications. This gives rise to the constraint satisfaction problem of fast
matrix multiplication, where a set of $R &amp;lt; NMP$ multiplication terms must be
chosen and combined such that they satisfy correctness constraints on the
output matrix. Despite its highly combinatorial nature, this problem has not
been exhaustively examined from that perspective, as evidenced for example by
the recent deep reinforcement learning approach of AlphaTensor. In this work,
we propose a simple yet novel Constraint Programming approach to find
non-commutative algorithms for fast matrix multiplication or provide proof of
infeasibility otherwise. We propose a set of symmetry-breaking constraints and
valid inequalities that are particularly helpful in proving infeasibility. On
the feasible side, we find that exploiting solver performance variability in
conjunction with a sparsity-based problem decomposition enables finding
solutions for larger (feasible) instances of fast matrix multiplication. Our
experimental results using CP Optimizer demonstrate that we can find fast
matrix multiplication algorithms for matrices up to $3\times 3$ in a short
amount of time.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Parameterized Complexity of Broadcasting in Graphs</title>
    <link href="http://arxiv.org/abs/2306.01536"/>
    <id>http://arxiv.org/abs/2306.01536</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fomin_F/0/1/0/all/0/1&quot;&gt;Fedor V. Fomin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fraigniaud_P/0/1/0/all/0/1&quot;&gt;Pierre Fraigniaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golovach_P/0/1/0/all/0/1&quot;&gt;Petr A. Golovach&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The task of the broadcast problem is, given a graph G and a source vertex s,
to compute the minimum number of rounds required to disseminate a piece of
information from s to all vertices in the graph. It is assumed that, at each
round, an informed vertex can transmit the information to at most one of its
neighbors. The broadcast problem is known to NP-hard. We show that the problem
is FPT when parametrized by the size k of a feedback edge-set, or by the size k
of a vertex-cover, or by k=n-t where t is the input deadline for the broadcast
protocol to complete.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">David Eppstein: Soddy’s quadlet</title>
    <link href="https://11011110.github.io/blog/2023/06/04/soddys-quadlet.html"/>
    <id>https://11011110.github.io/blog/2023/06/04/soddys-quadlet</id>
    <updated>2023-06-04T17:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;Soddy’s hexlet is a famous system of nine spheres in three-dimensional Euclidean space, consisting of a ring of six spheres, tangent in consecutive pairs, and a ring of three spheres, tangent in pairs, with every sphere in one ring tangent to every sphere of the other ring. The easy way to construct it is to observe that its properties are invariant under Möbius transformations, as long as you count planes as spheres, and parallel planes as tangent spheres. If you take two of the three spheres in the three-sphere ring to be parallel planes, the rest have to form seven congruent spheres, six of them in a ring around the seventh. Any other form of the hexlet can be obtained by a Möbius transformation from this one.&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/blog/assets/2023/hexlet.gif&quot; alt=&quot;Soddy&#39;s hexlet, in the form of seven congruent spheres between two parallel planes&quot; title=&quot;CC-BY-SA 3.0 image File:Hexlet annular opt.gif by WillowW from Wikimedia commons&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But there’s another way of constructing the same shape, coming from four-dimensional polyhedral geometry, which can also be used to construct another related pair of interlocking rings of spheres.&lt;/p&gt;

&lt;p&gt;By analogy, consider a cube in 3d, and start growing a sphere with its center at the center of the cube. As you grow the sphere, it will start to bulge out through the faces of the cube, which cut it in six circles. Initially, those circles will be small and disjoint from each other, but as they grow larger they will eventually touch at the midpoint of a cube edge, and then cross each other. At the time when any two of the growing circles touch each other, all six will touch four others, by the symmetries of the cube. In this way, we have generated a configuration of six circles, on the surface of a sphere, each touching four others with the same touching pattern as the square faces of a cube.&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/blog/assets/2023/cube-midsphere.png&quot; alt=&quot;A cube and its midsphere&quot; title=&quot;CC-BY-SA 4.0 image File:Skeleton 6, size m, sphere.png by Watchduck from Wikimedia commons&quot; style=&quot;width:100%;max-width:480px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now do the same thing in 4d with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Duoprism&quot;&gt;(3,6)-duoprism&lt;/a&gt;, the Cartesian product of an equilateral triangle and a regular hexagon. The resulting 4-dimensional polytope has facets of two type: six triangular prisms (attached to each other on their triangle faces) and three hexagonal prisms (attached to each other on their hexagon faces). Rectangular faces connect triangular prisms to a hexagonal prism. There is a choice for how big to make the triangle edges relative to the hexagon edges. You want them to be in the proportion \(\sqrt3:1\), so that both kinds of prism have an inscribed sphere touching all their faces. Instead, the only figures I could find of the duoprism show it with edges in the wrong proportion \(1:1\), generating square faces. But I want the rectangular one.&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/blog/assets/2023/6-3-duoprism.png&quot; alt=&quot;Skeleton of the square (3,6)-duoprism&quot; title=&quot;CC-BY image File:6-3 duoprism.png by Tomruen from Wikimedia commons, created using Robert Webb&#39;s Stella software, http://www.software3d.com/Stella.php&quot; style=&quot;width:100%;max-width:540px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Grow a hypersphere from the center of the duoprism. As it grows, it will intersect the prism facets of the duosphere in spheres, centered at the centers of the prisms. Initially small and disjoint, these spheres will grow until they become the inscribed spheres of the prisms, touching each other at the center of each triangle, hexagon, or rectangle of the duoprism. You have created a hexlet, simultaneously drawn on a sphere and inscribed in the faces of a duoprism! You can map it into the usual hexlet of three-dimensional Euclidean geometry (instead of three-dimensional spherical geometry) by a stereographic projection from the hypersphere to a flat three-dimensional space.&lt;/p&gt;

&lt;p&gt;Almost all the other duoprisms do not have this coincidence, that when you adjust the proportions to make one kind of prism have inscribed spheres, the other one does the same thing with the same proportions. There’s only one other duoprism for which this works: the (4,4)-duoprism, better known as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hypercube&quot;&gt;4-hypercube&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Hypercube&quot;&gt;tesseract&lt;/a&gt;. In this case, all the facets are the same, and all the 2-faces are the same. If we grow a hypersphere, centered at the center of the hypercube, it will cross the hypercube facets (which are cubes) in spheres. When these spheres grow to the size where they are inscribed in each cube facet, they will be tangent to each other at the centers of the square two-dimensional faces of the hypercube. At this point, you will have formed two rings of four tangent spheres, tangent in consecutive pairs, with every sphere in one ring tangent to every sphere of the other ring. We could call it the “quadlet”.&lt;/p&gt;

&lt;p&gt;Now that you’ve constructed a quadlet on a hypersphere in 4d, you can apply a stereographic projection to get the same quadlet as a collection of ordinary spheres in 3-dimensional Euclidean space. One of the more symmetric ways of doing this projection takes one of the two 4-sphere rings to two unit spheres sandwiched between two parallel planes at distance 4 from each other.  The four spheres of the other ring all have radius 2, and wrap around the central two unit spheres. It’s not obvious to me that these two parallel planes, two unit spheres, and four radius-2 spheres can all be tangent in this pattern, unless we calculate the coordinates of their tangencies or use reasoning based on the spheres inscribed on the facets of a hypercube, for which the same pattern of tangencies is obvious.&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/blog/assets/2023/quadlet.svg&quot; alt=&quot;Soddy&#39;s quadlet, in the form of four radius-2 spheres and two radius-1 spheres between two parallel planes, top and side view&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can rotate the four radius-2 spheres around the axis formed by the central unit spheres arbitrarily without changing the pattern of tangencies. This is more or less analogous to the fact that with the hexlet, you can start the ring of six tangent spheres at any sphere tangent to all three spheres of the other ring, and then add spheres to the ring one by one, keeping each sphere tangent to its predecessor in the same ring and to all the spheres of the other ring. It will always close up after six spheres to form a hexlet. You can also fix in place the six-sphere ring, choose any sphere tangent to all of them to start the three-sphere ring, and it will always close up after three spheres to form a hexlet. And once you have one of the four-sphere rings of a quadlet, you can choose any sphere tangent to all four to start the other ring, and it will always close up after four spheres to form a quadlet. For the hexlet, this becomes obvious after we do a Möbius transformation to take it into the form with two parallel planes and seven congruent spheres. For the quadlet, it is similarly obvious by doing a Möbius transformation to take it into a form with two parallel planes in one ring and four congruent spheres in the other. The only way for the remaining two spheres to complete the first ring is for them to fill the hole between the four congruent spheres, one directly on top of each other. They might not be the same size as each other but one more Möbius transformation makes them so. So just like the hexlet, all quadlets are Möbius-equivalent.&lt;/p&gt;

&lt;p&gt;Incidentally, the fact that you can get systems of three-dimensional tangent spheres from four-dimensional polytopes is not particularly new. I used it long ago in my paper with Kuperberg and Ziegler, “&lt;a href=&quot;https://arxiv.org/abs/math.CO/0204007&quot;&gt;Fat 4-polytopes and fatter 3-spheres&lt;/a&gt;”, to get a finite set of spheres with high kissing number from the &lt;a href=&quot;https://en.wikipedia.org/wiki/120-cell&quot;&gt;120-cell&lt;/a&gt; and its relatives. For more on the connection between sphere packings and 4-polytopes, including the construction of the hexlet from the duoprism, see &lt;a href=&quot;https://dr-how.github.io/&quot;&gt;Hao Chen’s papers&lt;/a&gt; and especially “&lt;a href=&quot;https://arxiv.org/abs/1306.2515&quot;&gt;Apollonian ball packings and stacked polytopes&lt;/a&gt;”.&lt;/p&gt;

&lt;p&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/110488812675802711&quot;&gt;Discuss on Mastodon&lt;/a&gt;)&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By David Eppstein&lt;/p&gt;
  </content>
    <author>
      <name>David Eppstein</name>
      <uri>https://11011110.github.io/blog/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">ECCC Papers: TR23-084 |  Time-Space Lower Bounds for Bounded-Error Computation in the Random-Query Model | 

	Itai Dinur</title>
    <link href="https://eccc.weizmann.ac.il/report/2023/084"/>
    <id>https://eccc.weizmann.ac.il/report/2023/084</id>
    <updated>2023-06-04T02:29:40+00:00</updated>
    <content type="html" xml:lang="en">
    The random-query model was introduced by Raz and Zhan at ITCS 2020 as a new model of space-bounded computation. In this model, a branching program of length $T$ and width $2^{S}$ attempts to compute a function $f:\{0,1\}^n \rightarrow \{0,1 \}$. However, instead of receiving direct access to the input bits $(x_1,\ldots,x_n)$, the input is given in pairs of the form $(i_j, x_{i_j}) \in \{1,\ldots,n\} \times \{0,1\}$ for $j = 1,2,\ldots,T$, where the indices $i_1,\ldots,i_T$ are chosen at random from a pre-fixed distribution. 

Raz and Zhan proved that any branching program in the random-query model with the independent distribution (where $\{i_j\}_{j = 1,\ldots,T}$ are uniform and independent) that computes a function $f$ with  sensitivity $k$ satisfies $T \cdot (S + \log n) \geq \Omega(n \cdot k)$. 
This gives a quadratic time-space lower bound for many natural functions which have sensitivity $\Omega(n)$, such as XOR and Majority. The bound was proved in the zero-error regime, where for each input, the branching program is required to output a value with high probability, and given that a value is output, it must be correct with probability $1$. 

Furthermore, Raz and Zhan conjectured that (up to logarithmic factors in $n$) a quadratic time-space lower bound still holds for the XOR function in the more conventional bounded-error regime, where for each input, the output must be correct with high probability.

In this paper, we prove this conjecture. More generally, let $f:\{0,1\}^n \rightarrow \{0,1 \}$ have average sensitivity (or total influence) $\mathrm{I}[f]$. We prove that any branching program in the random-query model with the independent distribution that computes $f$ in the bounded-error regime satisfies $T \cdot S  \geq \tilde{\Omega}(n) \cdot \mathrm{I}[f]$ (where $\tilde{\Omega}$ hides logarithmic factors in $n$). Moreover, we prove a quadratic time-space lower bound for the Majority function, even though its total influence is $\Theta(\sqrt{n})$.

Our proof is based on a reduction from a communication complexity problem.
  </content>
    <author>
      <name>ECCC Papers</name>
      <uri>https://eccc.weizmann.ac.il/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">ECCC Papers: TR23-083 |  Trade-offs between Entanglement and Communication | 

	Srinivasan A, 

	Uma Girish</title>
    <link href="https://eccc.weizmann.ac.il/report/2023/083"/>
    <id>https://eccc.weizmann.ac.il/report/2023/083</id>
    <updated>2023-06-04T02:27:27+00:00</updated>
    <content type="html" xml:lang="en">
    We study the advantages of quantum communication models over classical communication models that are equipped with a limited number of qubits of entanglement. In this direction, we give explicit partial functions on $n$ bits for which reducing the entanglement increases the classical communication complexity exponentially. Our separations are as follows. For every $k\ge 1$:
  $Q\|^*$ versus $R2^*$: We show that quantum simultaneous protocols with $\tilde{\Theta}(k^5 \log^3 n)$ qubits of entanglement can exponentially outperform two-way randomized protocols with $O(k)$ qubits of entanglement. This resolves an open problem from [Gav08] and improves the state-of-the-art separations between quantum simultaneous protocols with entanglement and two-way randomized protocols without entanglement [Gav19, GRT22].
  $R\|^*$ versus $Q\|^*$: We show that classical simultaneous protocols with $\tilde{\Theta}(k \log n)$ qubits of entanglement can exponentially outperform quantum simultaneous protocols with $O(k)$ qubits of entanglement, resolving an open question from [GKRW06, Gav19]. The best result prior to our work was a relational separation against protocols without entanglement [GKRW06].
  $R\|^*$ versus $R1^*$: We show that classical simultaneous protocols with $\tilde{\Theta}(k\log n)$ qubits of entanglement can exponentially outperform randomized one-way protocols with $O(k)$ qubits of entanglement. Prior to our work, only a relational separation was known [Gav08].
  </content>
    <author>
      <name>ECCC Papers</name>
      <uri>https://eccc.weizmann.ac.il/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Richard Lipton: Topping the Hat</title>
    <link href="https://rjlipton.wpcomstaging.com/2023/06/03/topping-the-hat/"/>
    <id>https://rjlipton.wpcomstaging.com/?p=21685</id>
    <updated>2023-06-03T18:56:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;
&lt;font color=&quot;#0044cc&quot;&gt;&lt;br /&gt;
&lt;em&gt;An &amp;#8220;einstein&amp;#8221; that doesn&amp;#8217;t need flipping&lt;/em&gt;&lt;br /&gt;
&lt;font color=&quot;#000000&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/03/topping-the-hat/sr2/&quot; rel=&quot;attachment wp-att-21687&quot;&gt;&lt;img data-attachment-id=&quot;21687&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/03/topping-the-hat/sr2/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/sr2.jpeg?fit=182%2C249&amp;amp;ssl=1&quot; data-orig-size=&quot;182,249&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;1&amp;quot;}&quot; data-image-title=&quot;sr2&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/sr2.jpeg?fit=182%2C249&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/sr2.jpeg?fit=182%2C249&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/sr2.jpeg?resize=121%2C166&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;121&quot; height=&quot;166&quot; class=&quot;alignright wp-image-21687&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
Siobhan Roberts is a Canadian science journalist, biographer, and historian of mathematics. She has an &lt;a href=&quot;https://www.nytimes.com/2023/06/01/science/puzzles-mathematics-tiling.html?action=click&amp;#038;module=Well&amp;#038;pgtype=Homepage&amp;#038;section=Science&quot;&gt;article&lt;/a&gt; that appeared in print in yesterday&amp;#8217;s New York Times. It is on a second breakthrough by a team of mathematicians, improving their solution to a famous problem on &lt;a href=&quot;https://en.wikipedia.org/wiki/Aperiodic_tiling&quot;&gt;tiling&lt;/a&gt; as we &lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/03/31/a-new-tiling/&quot;&gt;covered&lt;/a&gt; last March.&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
Yesterday, while reading the Times, I must admit that I was surprised. I was reading Section A, with tons of stuff on the war in Ukraine and the debt ceiling deal and on the Republican primaries for president. I was not expecting to see a math theorem given such prominence. &lt;/p&gt;
&lt;p&gt;
I knew Roberts&amp;#8217;s previous work on math such as a terrific &lt;a href=&quot;https://www.amazon.com/Genius-At-Play-Curious-Horton/dp/1620405954/ref=tmm_pap_swatch_0?_encoding=UTF8&amp;#038;qid=&amp;#038;sr=&quot;&gt;book&lt;/a&gt; a while ago on John Horton Conway. He discovered the Conway groups in mathematical symmetry and invented the aptly named &lt;em&gt;surreal numbers&lt;/em&gt; as well as the cult classic &lt;a href=&quot;https://en.wikipedia.org/wiki/Conway&#39;s_Game_of_Life&quot;&gt;Game of Life&lt;/a&gt;. Moving to Princeton in 1987, he deployed cards, ropes, dice, coat hangers, and even the odd Slinky as props to improve his lectures; see our &lt;a href=&quot;https://rjlipton.wpcomstaging.com/2020/04/14/john-horton-conway-1937-2020/&quot;&gt;memorial&lt;/a&gt; for more. &lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/03/topping-the-hat/conwaybook/&quot; rel=&quot;attachment wp-att-21688&quot;&gt;&lt;img data-attachment-id=&quot;21688&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/03/topping-the-hat/conwaybook/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/conwaybook.jpg?fit=329%2C499&amp;amp;ssl=1&quot; data-orig-size=&quot;329,499&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;conwaybook&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/conwaybook.jpg?fit=198%2C300&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/conwaybook.jpg?fit=329%2C499&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/conwaybook.jpg?resize=110%2C166&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;110&quot; height=&quot;166&quot; class=&quot;aligncenter wp-image-21688&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/conwaybook.jpg?w=329&amp;amp;ssl=1 329w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/conwaybook.jpg?resize=198%2C300&amp;amp;ssl=1 198w&quot; sizes=&quot;(max-width: 110px) 100vw, 110px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;H2&gt; The Problem &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
Roberts begins by recapping the original solution:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; In March, a team of mathematical tilers announced their solution to a storied problem: They had discovered an elusive &amp;#8220;Einstein&amp;#8221;&amp;#8212; a single shape that tiles a plane, or an infinite two-dimensional flat surface, but only in a non-repeating pattern. &amp;#8220;I&amp;#8217;ve always wanted to make a discovery,&amp;#8221; David Smith, the shape hobbyist whose original find spurred the research, said at the time. &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
The other members of the team are Craig Kaplan and Chaim Goodman-Strauss (to the left of Smith below) and Joseph Myers:&lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;/p&gt;
&lt;table style=&quot;margin:auto;&quot;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/03/topping-the-hat/fourtilers2/&quot; rel=&quot;attachment wp-att-21707&quot;&gt;&lt;img data-attachment-id=&quot;21707&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/03/topping-the-hat/fourtilers2/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/FourTilers2.png?fit=861%2C250&amp;amp;ssl=1&quot; data-orig-size=&quot;861,250&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;FourTilers2&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/FourTilers2.png?fit=300%2C87&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/FourTilers2.png?fit=600%2C174&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/FourTilers2.png?resize=574%2C167&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;574&quot; height=&quot;167&quot; class=&quot;aligncenter wp-image-21707&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/FourTilers2.png?w=861&amp;amp;ssl=1 861w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/FourTilers2.png?resize=300%2C87&amp;amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/FourTilers2.png?resize=768%2C223&amp;amp;ssl=1 768w&quot; sizes=&quot;(max-width: 574px) 100vw, 574px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;caption alignright&quot;&gt;&lt;FONT size=&quot;-2&quot;&gt;Composite crop of several sources&lt;/FONT&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;
The rub with the tile that the collaborators named &amp;#8220;the hat&amp;#8221; is that it needed to be flipped into its mirror image to create a set that can tile the plane. This is evident in the following grid&amp;#8212;note that the blue hats have their upper notch on the left not right: &lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;br /&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/03/topping-the-hat/grid2/&quot; rel=&quot;attachment wp-att-21709&quot;&gt;&lt;img data-attachment-id=&quot;21709&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/03/topping-the-hat/grid2/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/grid2.jpg?fit=3273%2C1961&amp;amp;ssl=1&quot; data-orig-size=&quot;3273,1961&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;1&amp;quot;}&quot; data-image-title=&quot;grid2&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/grid2.jpg?fit=300%2C180&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/grid2.jpg?fit=600%2C360&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/grid2.jpg?resize=545%2C327&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;545&quot; height=&quot;327&quot; class=&quot;aligncenter wp-image-21709&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/grid2.jpg?w=3273&amp;amp;ssl=1 3273w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/grid2.jpg?resize=300%2C180&amp;amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/grid2.jpg?resize=1024%2C614&amp;amp;ssl=1 1024w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/grid2.jpg?resize=768%2C460&amp;amp;ssl=1 768w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/grid2.jpg?resize=1536%2C920&amp;amp;ssl=1 1536w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/grid2.jpg?resize=2048%2C1227&amp;amp;ssl=1 2048w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/grid2.jpg?resize=1200%2C719&amp;amp;ssl=1 1200w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/grid2.jpg?w=1800&amp;amp;ssl=1 1800w&quot; sizes=&quot;(max-width: 545px) 100vw, 545px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;/p&gt;
&lt;p&gt;&lt;H2&gt; The New Solution &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
To remove this hitch and obtain the strongest possible result, the team needed to achieve two objectives:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
Create a single shape that can tile the plane without being flipped, but only aperiodically. &lt;/p&gt;
&lt;li&gt;
Show that the shape &lt;em&gt;plus its flip&lt;/em&gt; cannot tile the plane periodically.
&lt;/ol&gt;
&lt;p&gt;
Note that the second clause makes the &amp;#8220;but only aperiodically&amp;#8221; part of the first clause redundant. The key with the second clause is that the team could begin with work already done for their original result, which included the flip. Indeed, they had created a whole continuum of tiles &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BT%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{T}&quot; class=&quot;latex&quot; /&gt; with the same properties as the original &amp;#8220;hat.&amp;#8221; &lt;/p&gt;
&lt;p&gt;
There were two other dimensions of freedom for the team to explore:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
Modify the edges of the tiles in ways other than the continuum. &lt;/p&gt;
&lt;li&gt;
Combine basic tiles &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BT%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{T}&quot; class=&quot;latex&quot; /&gt; into &amp;#8220;supertiles&amp;#8221; &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BS%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{S}&quot; class=&quot;latex&quot; /&gt;. One possible idea is that would be OK for &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BS%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{S}&quot; class=&quot;latex&quot; /&gt; to include copies of both &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BT%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{T}&quot; class=&quot;latex&quot; /&gt; and flips &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BT%27%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{T&amp;#039;}&quot; class=&quot;latex&quot; /&gt; so long as &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BS%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{S}&quot; class=&quot;latex&quot; /&gt; can tile the plane without &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BS%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{S}&quot; class=&quot;latex&quot; /&gt; itself being flipped.
&lt;/ul&gt;
&lt;p&gt;
It appears from the new &lt;a href=&quot;https://arxiv.org/abs/2305.17743&quot;&gt;paper&lt;/a&gt; that the second dimension is &lt;b&gt;not&lt;/b&gt; used&amp;#8212;or rather, ideas like it are used in the proofs but not in the definition of the new &lt;em&gt;einstein&lt;/em&gt; tiles. The former dimension, however, made profit out of a step that at first seemed to &lt;em&gt;undo&lt;/em&gt; the work they had done before. They had already observed the key point in their original March &lt;a href=&quot;https://arxiv.org/abs/2303.10798&quot;&gt;paper&lt;/a&gt;, from which we reproduce this snippet:&lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;br /&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/03/topping-the-hat/tile11snippet/&quot; rel=&quot;attachment wp-att-21697&quot;&gt;&lt;img data-attachment-id=&quot;21697&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/03/topping-the-hat/tile11snippet/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/Tile11snippet.png?fit=762%2C514&amp;amp;ssl=1&quot; data-orig-size=&quot;762,514&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;Tile11snippet&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/Tile11snippet.png?fit=300%2C202&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/Tile11snippet.png?fit=600%2C405&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/Tile11snippet.png?resize=574%2C386&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;574&quot; height=&quot;386&quot; class=&quot;aligncenter wp-image-21697&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/Tile11snippet.png?w=762&amp;amp;ssl=1 762w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/Tile11snippet.png?resize=300%2C202&amp;amp;ssl=1 300w&quot; sizes=&quot;(max-width: 574px) 100vw, 574px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;br /&gt;
That is to say, &amp;#8220;&lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Br%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{r}&quot; class=&quot;latex&quot; /&gt;&amp;#8221; defines the dimension of their original continuum which changes the relative lengths of edges of the polygon. The case &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Br%3D1%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{r=1}&quot; class=&quot;latex&quot; /&gt; is an isolated point at which aperiodicity breaks down&amp;#8212;because it makes equal-length edges that permit sudden new ways tiles can fit. This is exploited by the simple columns of what they call &amp;#8220;Tile(1,1)&amp;#8221; and its flip in the figure. The key property of Tile(1,1) proved in the new paper is:&lt;/p&gt;
&lt;p&gt;&lt;P align=center&gt; Tile(1,1) &lt;em&gt;can&lt;/em&gt; tile the plane &lt;em&gt;without&lt;/em&gt; being flipped&amp;#8212;&lt;em&gt;but only aperiodically&lt;/em&gt;. &lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;br /&gt;
Further and most important, the new paper shows that by altering the edges of Tile(1,1) in any of a whole spectrum of ways represented in the figure below (which is rotated from ones in the paper and NYT article), one can rule out the flips and preserve the aperiodic tilings without flips:&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/03/topping-the-hat/spectres/&quot; rel=&quot;attachment wp-att-21694&quot;&gt;&lt;img data-attachment-id=&quot;21694&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/03/topping-the-hat/spectres/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/spectres.png?fit=1142%2C714&amp;amp;ssl=1&quot; data-orig-size=&quot;1142,714&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;spectres&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/spectres.png?fit=300%2C188&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/spectres.png?fit=600%2C375&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/spectres.png?resize=571%2C357&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;571&quot; height=&quot;357&quot; class=&quot;aligncenter wp-image-21694&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/spectres.png?w=1142&amp;amp;ssl=1 1142w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/spectres.png?resize=300%2C188&amp;amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/spectres.png?resize=1024%2C640&amp;amp;ssl=1 1024w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/spectres.png?resize=768%2C480&amp;amp;ssl=1 768w&quot; sizes=&quot;(max-width: 571px) 100vw, 571px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;br /&gt;
These tiles, which they call &amp;#8220;Spectres,&amp;#8221; are the new objection-free einsteins. They prove properties of a whole space of these tiles, fed by two main lines of hierarchical supertile constructions. Thus the case &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Br%3D1%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{r=1}&quot; class=&quot;latex&quot; /&gt; is like a Grand Central Station for connecting to the other dimensions.&lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; Open Problems &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
What could be the payoff of this wonderful discovery? Perhaps a new drug? A new solution to a potential Nobel level problem? Or something else?&lt;/p&gt;
&lt;p&gt;
Dan Shechtman was awarded the 2011 Nobel Prize in Chemistry for the discovery of natural &lt;a href=&quot;https://en.wikipedia.org/wiki/Quasicrystal&quot;&gt;quasicrystals&lt;/a&gt;, making him one of six Israelis who have won the Nobel Prize in Chemistry.&lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;/p&gt;
&lt;table style=&quot;margin:auto;&quot;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/03/topping-the-hat/dans/&quot; rel=&quot;attachment wp-att-21695&quot;&gt;&lt;img data-attachment-id=&quot;21695&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/03/topping-the-hat/dans/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/dans.jpg?fit=340%2C357&amp;amp;ssl=1&quot; data-orig-size=&quot;340,357&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;dans&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/dans.jpg?fit=286%2C300&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/dans.jpg?fit=340%2C357&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/dans.jpg?resize=340%2C357&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;340&quot; height=&quot;357&quot; class=&quot;aligncenter size-full wp-image-21695&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/dans.jpg?w=340&amp;amp;ssl=1 340w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/dans.jpg?resize=286%2C300&amp;amp;ssl=1 286w&quot; sizes=&quot;(max-width: 340px) 100vw, 340px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;caption alignright&quot;&gt;&lt;FONT size=&quot;-2&quot;&gt;Meeting at NIST in 1985 where Shechtman (on left) explains the atomic structure of quasicrystals: NIST &lt;a href=&quot;https://www.nist.gov/nist-and-nobel/dan-shechtman/nobel-moment-dan-shechtman&quot;&gt;source&lt;/a&gt;&lt;/FONT&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;
Quasicrystals are defined by being aperiodic&amp;#8212;the connection to the tiling problem is express in the Wikipedia page linked above. Will the new tiling results reported this spring lead to an insight worthy of a major award outside mathematics&amp;#8212;a Nobel perhaps?&lt;/p&gt;
&lt;p&gt;
&lt;p class=&quot;authors&quot;&gt;By RJLipton+KWRegan&lt;/p&gt;
  </content>
    <author>
      <name>Richard Lipton</name>
      <uri>https://rjlipton.wpcomstaging.com</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">CCI: jobs: Postdoc at Hamburg University of Technology (apply by June 22, 2023)</title>
    <link href="https://cstheory-jobs.org/2023/06/02/postdoc-at-hamburg-university-of-technology-apply-by-june-22-2023/"/>
    <id>http://cstheory-jobs.org/2023/06/02/postdoc-at-hamburg-university-of-technology-apply-by-june-22-2023/</id>
    <updated>2023-06-02T11:50:09+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;A postdoctoral research position (3 years, salary grade E-14) in theoretical computer science is available at The Institute for Algorithms and Complexity at Hamburg University of Technology. The successful candidate is expected to work on algorithms, discrete/combinatorial optimization and operations research with applications in data science (in collaboration with DASHH &amp;#8211; Data Science in Hamburg)&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://www.tuhh.de/algo/jobs/postdoc/senior-positions&quot;&gt;https://www.tuhh.de/algo/jobs/postdoc/senior-positions&lt;/a&gt;&lt;br /&gt;
Email: algo@tuhh.de&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </content>
    <author>
      <name>CCI: jobs</name>
      <uri>https://cstheory-jobs.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Richard Lipton: Combinatorics or Logic?</title>
    <link href="https://rjlipton.wpcomstaging.com/2023/06/01/combinatorics-or-logic/"/>
    <id>https://rjlipton.wpcomstaging.com/?p=21664</id>
    <updated>2023-06-02T02:13:18+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;
&lt;font color=&quot;#0044cc&quot;&gt;&lt;br /&gt;
&lt;em&gt;Or is it number theory?&lt;/em&gt;&lt;br /&gt;
&lt;font color=&quot;#000000&quot;&gt;&lt;/p&gt;
&lt;p&gt;
Julius B&amp;uuml;chi was a Swiss mathematician who taught at Purdue University for many years. His is arguably a case of influence&amp;#8212;in multiple fields&amp;#8212;far exceeding a modest number of publications. A number of those, both at &lt;a href=&quot;https://dblp.org/pid/05/1329.html&quot;&gt;DBLP&lt;/a&gt; and his &lt;a href=&quot;https://link.springer.com/book/10.1007/978-1-4613-8928-6?page=3#toc&quot;&gt;collected works&lt;/a&gt;, came after his untimely passing in 1984. His influence in logic was furthered by his association with Saunders Mac Lane; in computer science, through his PhD student Lawrence Landweber and a group of computer science logicians that included Paul Young. &lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;br /&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/01/combinatorics-or-logic/jbuchi/&quot; rel=&quot;attachment wp-att-21667&quot;&gt;&lt;img data-attachment-id=&quot;21667&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/01/combinatorics-or-logic/jbuchi/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/jbuchi.jpeg?fit=200%2C200&amp;amp;ssl=1&quot; data-orig-size=&quot;200,200&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;jbuchi&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/jbuchi.jpeg?fit=200%2C200&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/jbuchi.jpeg?fit=200%2C200&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/jbuchi.jpeg?resize=200%2C200&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;200&quot; height=&quot;200&quot; class=&quot;aligncenter size-full wp-image-21667&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/jbuchi.jpeg?w=200&amp;amp;ssl=1 200w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/jbuchi.jpeg?resize=150%2C150&amp;amp;ssl=1 150w&quot; sizes=&quot;(max-width: 200px) 100vw, 200px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;br /&gt;
Today we talk about a new book by Jeffrey Shallit that may extend this influence further.&lt;br /&gt;
&lt;span id=&quot;more-21664&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;
We &lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/01/19/rabin-scott-time/&quot;&gt;recently&lt;/a&gt; wrote about the seminal 1958 paper on finite automata by Michael Rabin and Dana Scott and its place in the early history of theory in the decade before and after. B&amp;uuml;chi contributed by associating to a finite automaton &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BM%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{M}&quot; class=&quot;latex&quot; /&gt; the language of &lt;em&gt;infinite&lt;/em&gt; words over the alphabet of &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BM%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{M}&quot; class=&quot;latex&quot; /&gt; that cause &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BM%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{M}&quot; class=&quot;latex&quot; /&gt; to be in an accepting state infinitely often. Another of B&amp;uuml;chi&amp;#8217;s close associates, Dirk Siefkes, wrote in his intro to B&amp;uuml;chi&amp;#8217;s collected works:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; He is probably best known for using finite automata as combinatorial devices to obtain strong results on decidability and definability in monadic second-order theories, and extending the method to infinite combinatorial tools. &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
Siefkes goes on to observe how Rabin extended B&amp;uuml;chi&amp;#8217;s method to general infinite binary trees, modeled via a logical theory with two successor functions, and how melding this with work by Robert McNaughton yielded rich decidability and determinacy results for monadic second-order formulas over this logic.&lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; Into Number Theory &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
B&amp;uuml;chi posed a question that at first seems innocuous:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; Are there five non-consecutive natural numbers &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Ba_1%2Ca_2%2Ca_3%2Ca_4%2Ca_5%7D&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{a_1,a_2,a_3,a_4,a_5}&quot; class=&quot;latex&quot; /&gt; that satisfy the recurrence &lt;a name=&quot;recur&quot;&gt;&lt;/p&gt;
&lt;p align=center&gt;&lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++a_i%5E2+%3D+2a_%7Bi-1%7D%5E2+-+a_%7Bi-2%7D%5E2+%2B+2+%5C+%5C+%5C+%5C+%5C+%281%29&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;&amp;#92;displaystyle  a_i^2 = 2a_{i-1}^2 - a_{i-2}^2 + 2 &amp;#92; &amp;#92; &amp;#92; &amp;#92; &amp;#92; (1)&quot; class=&quot;latex&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;/a&gt; for &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Bi%3D3%2C4%2C5%7D&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{i=3,4,5}&quot; class=&quot;latex&quot; /&gt;? &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
The entire infinite sequence of integers satisfies the recurrence since &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Bi%5E2+%2B+%28i-2%29%5E2+-+2%28i-1%29%5E2%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{i^2 + (i-2)^2 - 2(i-1)^2}&quot; class=&quot;latex&quot; /&gt; always equals 2. An example of four non-consecutive integers that do so is &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B6%2C23%2C32%2C39%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{6,23,32,39}&quot; class=&quot;latex&quot; /&gt;. But this sequence does not extend because &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B2%5Ccdot+39%5E2+-+32%5E2+%2B+2+%3D+2020%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{2&amp;#92;cdot 39^2 - 32^2 + 2 = 2020}&quot; class=&quot;latex&quot; /&gt; is not a perfect square. &lt;/p&gt;
&lt;p&gt;
No example of five is known. However, no one has ruled out that such sequences of length &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{k}&quot; class=&quot;latex&quot; /&gt; exist for every finite &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{k}&quot; class=&quot;latex&quot; /&gt;. What B&amp;uuml;chi proved is:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt;Theorem 1&lt;/b&gt; &lt;em&gt; If there exists &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{k}&quot; class=&quot;latex&quot; /&gt; such that the only length-&lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{k}&quot; class=&quot;latex&quot; /&gt; sequences of natural numbers satisfying the recurrence (&lt;a href=&quot;#recur&quot;&gt;1&lt;/a&gt;) are consecutive integers, then the undecidability of Hilbert&amp;#8217;s Tenth Problem applies all the way down to &lt;b&gt;quadratic&lt;/b&gt; equations, indeed ones of the form &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BAy+%3D+B%7D&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{Ay = B}&quot; class=&quot;latex&quot; /&gt; where &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BA%7D&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{A}&quot; class=&quot;latex&quot; /&gt; is a matrix of integers, &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BB%7D&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{B}&quot; class=&quot;latex&quot; /&gt; is a vector of integers, and &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7By%7D&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{y}&quot; class=&quot;latex&quot; /&gt; is a vector of squared variables. Moreover, existential sentences over Presburger arithmetic plus the predicate &amp;#8220;&lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{z}&quot; class=&quot;latex&quot; /&gt; is a perfect square&amp;#8221; would become undecidable. &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
The technical &lt;em&gt;tour-de-force&lt;/em&gt; is how B&amp;uuml;chi makes a length-&lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{k}&quot; class=&quot;latex&quot; /&gt; sequence of nonconsecutive natural numbers that satisfy the recurrence become the only obstacle to a scheme for defining the graph of multiplication &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Bz+%3D+x+%5Ccdot+y%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{z = x &amp;#92;cdot y}&quot; class=&quot;latex&quot; /&gt; from predicates about perfect squares. The theory of addition and multiplication is of course undecidable&amp;#8212;thanks to Kurt G&amp;ouml;del and Alan Turing. &lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; B&amp;uuml;chi&amp;#8217;s Own Arithmetic &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
B&amp;uuml;chi added a different predicate to Presburger arithmetic. Let &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BV_k%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{V_k}&quot; class=&quot;latex&quot; /&gt; denote the largest power of &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{k}&quot; class=&quot;latex&quot; /&gt; dividing &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{x}&quot; class=&quot;latex&quot; /&gt;. The resulting first-order theory &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BFO%28N%2C+%2B%2C+V_k%29%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{FO(N, +, V_k)}&quot; class=&quot;latex&quot; /&gt; is named in honor of B&amp;uuml;chi.&lt;/p&gt;
&lt;p&gt;
This theory is decidable. And the big observation is that this is more powerful than we perhaps realized. Ken and I find this quite interesting. &lt;/p&gt;
&lt;p&gt;
The theory is decidable, but just barely. The addition of &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BV_k%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{V_k}&quot; class=&quot;latex&quot; /&gt; to Presburger makes the theory much stronger. So strong that adding two independent &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BV_k%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{V_k}&quot; class=&quot;latex&quot; /&gt;&amp;#8216;s, that is &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BFO%28N%2C+%2B%2C+V_k%2C+V_%5Cell%29%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{FO(N, +, V_k, V_&amp;#92;ell)}&quot; class=&quot;latex&quot; /&gt; where &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{k}&quot; class=&quot;latex&quot; /&gt; and &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{&amp;#92;ell}&quot; class=&quot;latex&quot; /&gt; are relatively prime, is dangerous, since &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BFO%28N%2C+%2B%2C+V_k%2C+V_%5Cell%29%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{FO(N, +, V_k, V_&amp;#92;ell)}&quot; class=&quot;latex&quot; /&gt; becomes undecidable. This is neat. Indeed, &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BFO%28N%2C+%2B%2C+V_k%2C+V_%5Cell%29%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{FO(N, +, V_k, V_&amp;#92;ell)}&quot; class=&quot;latex&quot; /&gt; has the power of all of &lt;a href=&quot;https://en.wikipedia.org/wiki/Peano_axioms&quot;&gt;Peano arithmetic&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;
The power possessed by &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BFO%28N%2C+%2B%2C+V_k%29%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{FO(N, +, V_k)}&quot; class=&quot;latex&quot; /&gt; with a single &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BV_k%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{V_k}&quot; class=&quot;latex&quot; /&gt; is to describe finite automata over a &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{k}&quot; class=&quot;latex&quot; /&gt;-letter alphabet. This plays both into why &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BFO%28N%2C+%2B%2C+V_k%29%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{FO(N, +, V_k)}&quot; class=&quot;latex&quot; /&gt; is decidable and into the applications collected by Shallit.&lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; A New Book &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
Jeffrey Shallit recently sent a private message about his new book: &lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;br /&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/01/combinatorics-or-logic/walnut-book-cover/&quot; rel=&quot;attachment wp-att-21668&quot;&gt;&lt;img data-attachment-id=&quot;21668&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/01/combinatorics-or-logic/walnut-book-cover/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/walnut-book-cover.jpg?fit=1700%2C2560&amp;amp;ssl=1&quot; data-orig-size=&quot;1700,2560&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;walnut-book-cover&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/walnut-book-cover.jpg?fit=199%2C300&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/walnut-book-cover.jpg?fit=600%2C904&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/walnut-book-cover.jpg?resize=199%2C300&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;199&quot; height=&quot;300&quot; class=&quot;aligncenter size-medium wp-image-21668&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/walnut-book-cover.jpg?resize=199%2C300&amp;amp;ssl=1 199w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/walnut-book-cover.jpg?resize=680%2C1024&amp;amp;ssl=1 680w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/walnut-book-cover.jpg?resize=768%2C1157&amp;amp;ssl=1 768w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/walnut-book-cover.jpg?resize=1020%2C1536&amp;amp;ssl=1 1020w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/walnut-book-cover.jpg?resize=1360%2C2048&amp;amp;ssl=1 1360w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/walnut-book-cover.jpg?resize=1200%2C1807&amp;amp;ssl=1 1200w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/walnut-book-cover.jpg?w=1700&amp;amp;ssl=1 1700w&quot; sizes=&quot;(max-width: 199px) 100vw, 199px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;br /&gt;
He wrote: &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; I thought maybe you might possibly be interested in my new &lt;a href=&quot;https://cs.uwaterloo.ca/~shallit/walnut-book.html&quot;&gt;book&lt;/a&gt;, which is about how a decision procedure for &lt;a href=&quot;https://en.wikipedia.org/wiki/Buchi_arithmetic&quot;&gt;B&amp;uuml;chi arithmetic&lt;/a&gt;&amp;#8211;namely, &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BFO%28N%2C+%2B%2C+V_k%29%7D&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{FO(N, +, V_k)}&quot; class=&quot;latex&quot; /&gt;&amp;#8211;can be used to prove hundreds of old and new results in number theory, combinatorics on words, and automatic sequences. There is probably no big theoretical advance in the book, just a recognition that existing results might be more useful than we suspected. &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/01/combinatorics-or-logic/jshallit/&quot; rel=&quot;attachment wp-att-21669&quot;&gt;&lt;img data-attachment-id=&quot;21669&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/01/combinatorics-or-logic/jshallit/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/jshallit.jpeg?fit=214%2C125&amp;amp;ssl=1&quot; data-orig-size=&quot;214,125&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;jshallit&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/jshallit.jpeg?fit=214%2C125&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/jshallit.jpeg?fit=214%2C125&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/jshallit.jpeg?resize=214%2C125&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;214&quot; height=&quot;125&quot; class=&quot;aligncenter size-full wp-image-21669&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
The book comes with a software package called &lt;a href=&quot;https://cs.uwaterloo.ca/~shallit/walnut.html&quot;&gt;&lt;em&gt;Walnut&lt;/em&gt;&lt;/a&gt;, which was written by Hamoon Mousavi while a student in Shallit&amp;#8217;s department, and has been added to by other Waterloo students since. It executes a &lt;a href=&quot;https://projecteuclid.org/journals/bulletin-of-the-belgian-mathematical-society-simon-stevin/volume-1/issue-2/Logic-and-p-recognizable-sets-of-integers/10.36045/bbms/1103408547.full&quot;&gt;rendition&lt;/a&gt; of B&amp;uuml;chi&amp;#8217;s and others&amp;#8217; automata-based decision strategy. &lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; An Example &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
Here is a simple example that begins the number theory chapter of the book. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt;Proposition.&lt;/b&gt; &lt;em&gt; The numbers &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B2%5E%7B2n%2B1%7D+-+1%7D&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{2^{2n+1} - 1}&quot; class=&quot;latex&quot; /&gt; for &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Bn+%5Cgeq+0%7D&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{n &amp;#92;geq 0}&quot; class=&quot;latex&quot; /&gt; cannot be written as the sum of two natural numbers, each having an even number of &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B1%7D&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{1}&quot; class=&quot;latex&quot; /&gt;s in its binary representation. Every other number greater than &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B4%7D&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{4}&quot; class=&quot;latex&quot; /&gt; can be so written. &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
Via a finite automaton that distinguishes &amp;#8220;even&amp;#8221; from &amp;#8220;odd&amp;#8221; and its use of &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B%2B%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{+}&quot; class=&quot;latex&quot; /&gt; but not multiplication, this can be formulated in &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BFO%28N%2C+%2B%2C+V_2%29%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{FO(N, +, V_2)}&quot; class=&quot;latex&quot; /&gt;. The software then cranks out a proof. &lt;/p&gt;
&lt;p&gt;
Further examples require absorbing the notion of &lt;a href=&quot;https://en.wikipedia.org/wiki/Automatic_sequence&quot;&gt;automatic sequence&lt;/a&gt; in the book&amp;#8217;s title, and we&amp;#8217;ll stop here. Incidentally, the worst-case running time of &lt;em&gt;Walnut&lt;/em&gt; on a formula with &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Br%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{r}&quot; class=&quot;latex&quot; /&gt;-many alternating quantifiers built via an &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7BN%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{N}&quot; class=&quot;latex&quot; /&gt;-state finite automaton with output is &lt;/p&gt;
&lt;p align=center&gt;&lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5E%7B2%5E%7B%5Ccdot%5E%7B%5Ccdot%5E%7B%5Ccdot%5E%7B2%5E%7BN%5E%7BO%281%29%7D%7D%7D%7D%7D%7D%7D+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;&amp;#92;displaystyle  2^{2^{&amp;#92;cdot^{&amp;#92;cdot^{&amp;#92;cdot^{2^{N^{O(1)}}}}}}} &quot; class=&quot;latex&quot; /&gt;&lt;/p&gt;
&lt;p&gt;where there are &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Br%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{r}&quot; class=&quot;latex&quot; /&gt;-many &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B2%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{2}&quot; class=&quot;latex&quot; /&gt;s. The examples in the book&amp;#8212;and maybe almost all examples one would devise using the approaches in the book&amp;#8212;all halt within reasonable time. Why this is observed to hold might be a topic for another post.&lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; What Is Combinatorics? &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
Igor Pak, who is a professor in the Mathematics Department at UCLA and works in the Combinatorics Group, has this to say on his &lt;a href=&quot;https://www.math.ucla.edu/~pak/&quot;&gt;webpages&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/01/combinatorics-or-logic/ipak/&quot; rel=&quot;attachment wp-att-21670&quot;&gt;&lt;img data-attachment-id=&quot;21670&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/01/combinatorics-or-logic/ipak/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ipak.jpeg?fit=262%2C192&amp;amp;ssl=1&quot; data-orig-size=&quot;262,192&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;ipak&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ipak.jpeg?fit=262%2C192&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ipak.jpeg?fit=262%2C192&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ipak.jpeg?resize=262%2C192&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;262&quot; height=&quot;192&quot; class=&quot;aligncenter size-full wp-image-21670&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
He says: &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; Combinatorics is a pretty diverse area and hard to characterize. Of course, combinatorics is about counting, but it is a lot more than just that. See &lt;a href=&quot;https://www.math.ucla.edu/~pak/hidden/papers/Quotes/Combinatorics-quotes.htm&quot;&gt;here&lt;/a&gt; for more quotes about it.&lt;/p&gt;
&lt;p&gt;
As you all know, my field is Combinatorics. I care about it. I blog about it endlessly. I want to see it blossom. I am happy to see it accepted by the broad mathematical community. It&amp;#8217;s a joy to see it represented at (most) top universities and recognized with major awards. It&amp;#8217;s all mostly good.&lt;/p&gt;
&lt;p&gt;
Of course, not everyone is on board. This is normal. Changing views is hard. Some people and institutions continue insisting that Combinatorics is mostly a trivial nonsense (or at least large parts of it). This is an old fight best not rehashed again. &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; Open Problems &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
Enjoy Jeffrey&amp;#8217;s book&amp;#8212;thanks. See &lt;a href=&quot;https://www.cambridge.org/core/books/logical-approach-to-automatic-sequences/FF19494217F62C05003B28EDFC83020C&quot;&gt;here&lt;/a&gt; to buy it. Ken&amp;#8217;s current student Chen Xu already owns it and is referencing it for some of his work.&lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;br /&gt;
[&amp;#8220;integers&amp;#8221; -&gt; &amp;#8220;natural numbers&amp;#8221; in a few places; inserted &amp;#8220;of length k&amp;#8221; in sentence before B&amp;uuml;chi&amp;#8217;s theorem]]&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By RJLipton+KWRegan&lt;/p&gt;
  </content>
    <author>
      <name>Richard Lipton</name>
      <uri>https://rjlipton.wpcomstaging.com</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Bell sampling from quantum circuits</title>
    <link href="http://arxiv.org/abs/2306.00083"/>
    <id>http://arxiv.org/abs/2306.00083</id>
    <updated>2023-06-02T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Hangleiter_D/0/1/0/all/0/1&quot;&gt;Dominik Hangleiter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Gullans_M/0/1/0/all/0/1&quot;&gt;Michael J. Gullans&lt;/a&gt;&lt;/p&gt;&lt;p&gt;A central challenge in the verification of quantum computers is benchmarking
their performance as a whole and demonstrating their computational
capabilities. In this work, we find a model of quantum computation, Bell
sampling, that can be used for both of those tasks and thus provides an ideal
stepping stone towards fault-tolerance. In Bell sampling, we measure two copies
of a state prepared by a quantum circuit in the transversal Bell basis. We show
that the Bell samples are classically intractable to produce and at the same
time constitute what we call a circuit shadow: from the Bell samples we can
efficiently extract information about the quantum circuit preparing the state,
as well as diagnose circuit errors. In addition to known properties that can be
efficiently extracted from Bell samples, we give two new and efficient
protocols, a test for the depth of the circuit and an algorithm to estimate a
lower bound to the number of T gates in the circuit. With some additional
measurements, our algorithm learns a full description of states prepared by
circuits with low T -count.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Graph Colouring is Hard for Algorithms Based on Hilbert&#39;s Nullstellensatz and Gr\&quot;{o}bner Bases</title>
    <link href="http://arxiv.org/abs/2306.00125"/>
    <id>http://arxiv.org/abs/2306.00125</id>
    <updated>2023-06-02T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lauria_M/0/1/0/all/0/1&quot;&gt;Massimo Lauria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nordstrom_J/0/1/0/all/0/1&quot;&gt;Jakob Nordstr&amp;#xf6;m&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We consider the graph $k$-colouring problem encoded as a set of polynomial
equations in the standard way over $0/1$-valued variables. We prove that there
are bounded-degree graphs that do not have legal $k$-colourings but for which
the polynomial calculus proof system defined in [Clegg et al &#39;96, Alekhnovich
et al &#39;02] requires linear degree, and hence exponential size, to establish
this fact. This implies a linear degree lower bound for any algorithms based on
Gr\&quot;{o}bner bases solving graph $k$-colouring using this encoding. The same
bound applies also for the algorithm studied in a sequence of papers [De Loera
et al &#39;08,&#39;09,&#39;11,&#39;15] based on Hilbert&#39;s Nullstellensatz proofs for a slightly
different encoding, thus resolving an open problem mentioned in [De Loera et al
&#39;08,&#39;09,&#39;11] and [Li &#39;16]. We obtain our results by combining the polynomial
calculus degree lower bound for functional pigeonhole principle (FPHP) formulas
over bounded-degree bipartite graphs in [Mik\v{s}a and Nordstr\&quot;{o}m &#39;15] with
a reduction from FPHP to $k$-colouring derivable by polynomial calculus in
constant degree.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Logics with probabilistic team semantics and the Boolean negation</title>
    <link href="http://arxiv.org/abs/2306.00420"/>
    <id>http://arxiv.org/abs/2306.00420</id>
    <updated>2023-06-02T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hannula_M/0/1/0/all/0/1&quot;&gt;Miika Hannula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirvonen_M/0/1/0/all/0/1&quot;&gt;Minna Hirvonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kontinen_J/0/1/0/all/0/1&quot;&gt;Juha Kontinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmood_Y/0/1/0/all/0/1&quot;&gt;Yasir Mahmood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meier_A/0/1/0/all/0/1&quot;&gt;Arne Meier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Virtema_J/0/1/0/all/0/1&quot;&gt;Jonni Virtema&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the expressivity and the complexity of various logics in
probabilistic team semantics with the Boolean negation. In particular, we study
the extension of probabilistic independence logic with the Boolean negation,
and a recently introduced logic FOPT. We give a comprehensive picture of the
relative expressivity of these logics together with the most studied logics in
probabilistic team semantics setting, as well as relating their expressivity to
a numerical variant of second-order logic. In addition, we introduce novel
entropy atoms and show that the extension of first-order logic by entropy atoms
subsumes probabilistic independence logic. Finally, we obtain some results on
the complexity of model checking, validity, and satisfiability of our logics.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: Robust Estimation of Surface Curvature Information from Point Cloud Data</title>
    <link href="http://arxiv.org/abs/2306.00299"/>
    <id>http://arxiv.org/abs/2306.00299</id>
    <updated>2023-06-02T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spang_J/0/1/0/all/0/1&quot;&gt;Jared Spang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;This paper surveys and evaluates some popular state of the art methods for
algorithmic curvature and normal estimation. In addition to surveying existing
methods we also propose a new method for robust curvature estimation and
evaluate it against existing methods thus demonstrating its superiority to
existing methods in the case of significant data noise. Throughout this paper
we are concerned with computation in low dimensional spaces (N &amp;lt; 10) and
primarily focus on the computation of the Weingarten map and quantities that
may be derived from this; however, the algorithms discussed are theoretically
applicable in any dimension. One thing that is common to all these methods is
their basis in an estimated graph structure. For any of these methods to work
the local geometry of the manifold must be exploited; however, in the case of
point cloud data it is often difficult to discover a robust manifold structure
underlying the data, even in simple cases, which can greatly influence the
results of these algorithms. We hope that in pushing these algorithms to their
limits we are able to discover, and perhaps resolve, many major pitfalls that
may affect potential users and future researchers hoping to improve these
methods
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: A polynomial-time iterative algorithm for random graph matching with non-vanishing correlation</title>
    <link href="http://arxiv.org/abs/2306.00266"/>
    <id>http://arxiv.org/abs/2306.00266</id>
    <updated>2023-06-02T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1&quot;&gt;Jian Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhangsong Li&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We propose an efficient algorithm for matching two correlated
Erd\H{o}s--R\&#39;enyi graphs with $n$ vertices whose edges are correlated through
a latent vertex correspondence. When the edge density $q= n^{- \alpha+o(1)}$
for a constant $\alpha \in [0,1)$, we show that our algorithm has polynomial
running time and succeeds to recover the latent matching as long as the edge
correlation is non-vanishing. This is closely related to our previous work on a
polynomial-time algorithm that matches two Gaussian Wigner matrices with
non-vanishing correlation, and provides the first polynomial-time random graph
matching algorithm (regardless of the regime of $q$) when the edge correlation
is below the square root of the Otter&#39;s constant (which is $\approx 0.338$).
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Best $L_p$ Isotonic Regressions, $p \in \{0, 1, \infty\}$</title>
    <link href="http://arxiv.org/abs/2306.00269"/>
    <id>http://arxiv.org/abs/2306.00269</id>
    <updated>2023-06-02T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stout_Q/0/1/0/all/0/1&quot;&gt;Quentin F.Stout&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Given a real-valued weighted function $f$ on a finite dag, the $L_p$ isotonic
regression of $f$, $p \in [0,\infty]$, is unique except when $p \in [0,1] \cup
\{\infty\}$. We are interested in determining a ``best&#39;&#39; isotonic regression
for $p \in \{0, 1, \infty\}$, where by best we mean a regression satisfying
stronger properties than merely having minimal norm. One approach is to use
strict $L_p$ regression, which is the limit of the best $L_q$ approximation as
$q$ approaches $p$, and another is lex regression, which is based on lexical
ordering of regression errors. For $L_\infty$ the strict and lex regressions
are unique and the same. For $L_1$, strict $q \scriptstyle\searrow 1$ is
unique, but we show that $q \scriptstyle\nearrow 1$ may not be, and even when
it is unique the two limits may not be the same. For $L_0$, in general neither
of the strict and lex regressions are unique, nor do they always have the same
set of optimal regressions, but by expanding the objectives of $L_p$
optimization to $p &amp;lt; 0$ we show $p{ \scriptstyle \nearrow} 0$ is the same as
lex regression. We also give algorithms for computing the best $L_p$ isotonic
regression in certain situations.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Last Switch Dependent Bandits with Monotone Payoff Functions</title>
    <link href="http://arxiv.org/abs/2306.00338"/>
    <id>http://arxiv.org/abs/2306.00338</id>
    <updated>2023-06-02T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foussoul_A/0/1/0/all/0/1&quot;&gt;Ayoub Foussoul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1&quot;&gt;Vineet Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadigenopoulos_O/0/1/0/all/0/1&quot;&gt;Orestis Papadigenopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeevi_A/0/1/0/all/0/1&quot;&gt;Assaf Zeevi&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In a recent work, Laforgue et al. introduce the model of last switch
dependent (LSD) bandits, in an attempt to capture nonstationary phenomena
induced by the interaction between the player and the environment. Examples
include satiation, where consecutive plays of the same action lead to decreased
performance, or deprivation, where the payoff of an action increases after an
interval of inactivity. In this work, we take a step towards understanding the
approximability of planning LSD bandits, namely, the (NP-hard) problem of
computing an optimal arm-pulling strategy under complete knowledge of the
model. In particular, we design the first efficient constant approximation
algorithm for the problem and show that, under a natural monotonicity
assumption on the payoffs, its approximation guarantee (almost) matches the
state-of-the-art for the special and well-studied class of recharging bandits
(also known as delay-dependent). In this attempt, we develop new tools and
insights for this class of problems, including a novel higher-dimensional
relaxation and the technique of mirroring the evolution of virtual states. We
believe that these novel elements could potentially be used for approaching
richer classes of action-induced nonstationary bandits (e.g., special instances
of restless bandits). In the case where the model parameters are initially
unknown, we develop an online learning adaptation of our algorithm for which we
provide sublinear regret guarantees against its full-information counterpart.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Time and Space Optimal Massively Parallel Algorithm for the 2-Ruling Set Problem</title>
    <link href="http://arxiv.org/abs/2306.00432"/>
    <id>http://arxiv.org/abs/2306.00432</id>
    <updated>2023-06-02T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cambus_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe9;lanie Cambus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhn_F/0/1/0/all/0/1&quot;&gt;Fabian Kuhn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pai_S/0/1/0/all/0/1&quot;&gt;Shreyas Pai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uitto_J/0/1/0/all/0/1&quot;&gt;Jara Uitto&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this work, we present a constant-round algorithm for the $2$-ruling set
problem in the Congested Clique model. As a direct consequence, we obtain a
constant round algorithm in the MPC model with linear space-per-machine and
optimal total space. Our results improve on the $O(\log \log \log n)$-round
algorithm by [HPS, DISC&#39;14] and the $O(\log \log \Delta)$-round algorithm by
[GGKMR, PODC&#39;18]. Our techniques can also be applied to the semi-streaming
model to obtain an $O(1)$-pass algorithm. Our main technical contribution is a
novel sampling procedure that returns a small subgraph such that almost all
nodes in the input graph are adjacent to the sampled subgraph. An MIS on the
sampled subgraph provides a $2$-ruling set for a large fraction of the input
graph. As a technical challenge, we must handle the remaining part of the
graph, which might still be relatively large. We overcome this challenge by
showing useful structural properties of the remaining graph and show that
running our process twice yields a $2$-ruling set of the original input graph
with high probability.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">ECCC Papers: TR23-082 |  Self-Improvement for Circuit-Analysis Problems | 

	Ryan Williams</title>
    <link href="https://eccc.weizmann.ac.il/report/2023/082"/>
    <id>https://eccc.weizmann.ac.il/report/2023/082</id>
    <updated>2023-06-01T14:41:12+00:00</updated>
    <content type="html" xml:lang="en">
    Many results in fine-grained complexity reveal intriguing consequences from solving various SAT problems even slightly faster than exhaustive search. We prove a ``self-improving&amp;#39;&amp;#39; (or ``bootstrapping&amp;#39;&amp;#39;) theorem for Circuit-SAT, $\#$Circuit-SAT, and its fully-quantified version: solving one of these problems faster for ``large&amp;#39;&amp;#39; circuit sizes implies a significant speed-up for ``smaller&amp;#39;&amp;#39; circuit sizes. Our general arguments work for a variety of models solving circuit-analysis problems, including non-uniform circuits and randomized models of computation.

We derive striking consequences for the complexities of these problems. For example, we show that certain fine-grained improvements on the runtime exponents of polynomial-time versions of Circuit-SAT would imply *subexponential-time* algorithms for Circuit-SAT on $2^{o(n)}$-size circuits, refuting the Exponential Time Hypothesis. We also show how slightly faster $\#$Circuit-SAT algorithms on large circuits can be used to prove lower bounds against uniform circuits with symmetric gates for functions in deterministic linear time. Our result suggests an ``algorithmic method&amp;#39;&amp;#39; approach for uniform circuit lower bounds, which trades non-uniformity for a substantial reduction in the complexity of the hard function.
  </content>
    <author>
      <name>ECCC Papers</name>
      <uri>https://eccc.weizmann.ac.il/</uri>
    </author>
  </entry>


</feed>
