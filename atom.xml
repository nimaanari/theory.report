<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>Theory of Computing Report</title>
  <link rel="self" href=""/>
  <link href=""/>
  <id></id>
  <updated></updated>
  <generator uri="http://feedreader.github.io/">Pluto 1.6.2 on Ruby 3.0.6 (2023-03-30) [x86_64-linux]</generator>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Windows on Theory: Metaphors for AI, and why I don‚Äôt like them</title>
    <link href="https://windowsontheory.org/2023/06/28/metaphors-for-ai-and-why-i-dont-like-them/"/>
    <id>http://windowsontheory.org/?p=8635</id>
    <updated>2023-06-28T22:46:34+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;&lt;em&gt;Photo from¬†&lt;/em&gt;&lt;a href=&quot;https://www.loc.gov/pictures/item/2016838906/&quot;&gt;&lt;em&gt;National Photo Company Collection&lt;/em&gt;&lt;/a&gt;&lt;em&gt;; see also (&lt;/em&gt;&lt;a href=&quot;https://www.amazon.com/Glass-Universe-Harvard-Observatory-Measure-ebook/dp/B01CZCW45O&quot;&gt;&lt;em&gt;&lt;u&gt;Sobel, 2017&lt;/u&gt;&lt;/em&gt;&lt;/a&gt;&lt;em&gt;)&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;&lt;em&gt;[Cross posted in¬†&lt;/em&gt;&lt;a href=&quot;https://www.lesswrong.com/posts/pBHga8mFq88dK7548/metaphors-for-ai-and-why-i-don-t-like-them&quot;&gt;&lt;em&gt;&lt;u&gt;lesswrong&lt;/u&gt;&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt; and windows on theory&lt;em&gt; see here for my¬†&lt;/em&gt;&lt;a href=&quot;https://windowsontheory.org/category/philosophizing/&quot;&gt;&lt;em&gt;&lt;u&gt;prior writings&lt;/u&gt;&lt;/em&gt;&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;br&gt;&lt;br&gt;‚Äúcomputer,¬†&lt;em&gt;n.¬†/k…ômÀàpjuÀêt…ô/&lt;/em&gt;.¬†&lt;em&gt;One who computes; a calculator, reckoner; specifically a person employed to make calculations in an observatory, in surveying, etc&lt;/em&gt;‚Äù, Oxford English Dictionary&lt;/p&gt;



&lt;p&gt;‚Äú&lt;em&gt;There is no reason why mental as well as bodily labor should not be economized by the aid of machinery&lt;/em&gt;‚Äù, Charles Babbage, 1852&lt;/p&gt;



&lt;p&gt;&lt;em&gt;‚ÄúComputing is normally done by writing certain symbols on paper. We may suppose that this paper is divided into squares like a child‚Äôs arithmetic book.. The behavior of the [human] computer at any moment is determined by the symbols which he is observing, and of his‚Äô state of mind‚Äô at that moment‚Ä¶ We may suppose that in a simple operation not more than one symbol is altered.‚Äú&lt;/em&gt;, Alan Turing, 1936&lt;/p&gt;



&lt;p&gt;&lt;em&gt;Metaphors&lt;/em&gt; can be a great way to understand new concepts. When the first digital computers were envisioned and constructed, people used different metaphors to describe them. One such metaphor, used as early as 1950, was an¬†&lt;a href=&quot;https://www.theguardian.com/century/1950-1959/Story/0,,105181,00.html&quot;&gt;&lt;u&gt;‚Äúelectronic brain‚Äù&lt;/u&gt;&lt;/a&gt;. (Of course today we think of this metaphor as attempting to explain the brain using the computer rather than the other way around.) Another metaphor is the name ‚Äúcomputer‚Äù itself. This word has meant since the 1700s a human being whose job is to perform calculations. Computers, who by the late 1800s and early 1900s were often female, were instrumental to many scientific and technological advances.&lt;br&gt;&lt;br&gt;Thus the device we call ‚Äúa computer‚Äù was literally named after the job it was destined to replace. Interestingly, as recounted in the book¬†&lt;a href=&quot;https://www.amazon.com/Hidden-Figures-American-Untold-Mathematicians-ebook/dp/B0166JFFD0/ref=sr_1_1?hvadid=241710192377&amp;amp;hvdev=c&amp;amp;hvlocint=9001996&amp;amp;hvlocphy=1008366&amp;amp;hvnetw=g&amp;amp;hvqmt=b&amp;amp;hvrand=4723742300797761391&amp;amp;hvtargid=kwd-280240200699&amp;amp;hydadcr=3203_10391923&amp;amp;keywords=hidden+figures+the+book&amp;amp;qid=1687559672&amp;amp;sr=8-1&quot;&gt;&lt;u&gt;‚ÄúHidden Figures‚Äù&lt;/u&gt;&lt;/a&gt;, despite the first electronic computers constructed in the 1940s, up until the 1960s NASA, still needed human computers as well. Also, these days scientific computing is only a small aspect of what we use electronic computers for. Electronic computers did end up eliminating the jobs of human computers, but this ended up being a tiny fraction of their influence on the workforce.&lt;br&gt;¬†&lt;/p&gt;



&lt;p&gt;Recently, it seems that every week someone mints a fresh metaphor for AI, often in an article saying that&amp;nbsp;&lt;em&gt;‚ÄúAI is nothing new because it is just like X‚Äù&lt;/em&gt;. I believe that&amp;nbsp;&lt;strong&gt;modern deep-learning-based AI is a fundamentally new concept that is not the same as any prior one&lt;/strong&gt;. (This is despite the fact that the idea of ‚Äústacking more layers‚Äù goes back to Rosenblatt in the late 1950s; as we‚Äôve seen time and again in Science,&amp;nbsp;&lt;a href=&quot;https://www.science.org/doi/10.1126/science.177.4047.393&quot;&gt;&lt;u&gt;‚Äúmore is different‚Äù&lt;/u&gt;&lt;/a&gt;.) So, I am deeply suspicious of such articles, especially when they are aimed at a general (non-scientific) audience that can be confused or misled by these metaphors. I also believe that&amp;nbsp;&lt;strong&gt;we do not understand AI enough to make confident predictions or analogs&lt;/strong&gt;, so we should take any metaphor with a grain of salt. Hence I am also suspicious of articles saying&amp;nbsp;&lt;em&gt;‚ÄúAI is going to certainly cause Y because it is just like X.‚Äù&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;To be clear, I am by no means ‚Äúanti metaphor‚Äù. Metaphors can be extremely valuable ‚Äúintuition pumps‚Äù. I often use them when teaching, and in research and blogging as well (e.g., my recent post on&amp;nbsp;&lt;a href=&quot;https://www.lesswrong.com/posts/wDL6wiqg3c6WFisHq/gpt-as-an-intelligence-forklift&quot;&gt;&lt;u&gt;‚Äúintelligence forklift‚Äù&lt;/u&gt;&lt;/a&gt;). Turing himself came up with the ‚ÄúTuring Machine‚Äù by modeling a human computer. However, it is always important to remember our metaphors‚Äô limitations.&amp;nbsp;&lt;strong&gt;No single metaphor can capture all of AI‚Äôs essential elements, and we should be careful of over-interpreting metaphors.&lt;/strong&gt;&amp;nbsp;&lt;br&gt;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For example, in my blog post on the&amp;nbsp;&lt;a href=&quot;https://windowsontheory.org/2022/06/20/the-uneasy-relationship-between-deep-learning-and-classical-statistics/&quot;&gt;&lt;u&gt;uneasy relationship between deep learning and statistics&lt;/u&gt;&lt;/a&gt;, I claimed that human learning is a better metaphor for deep learning than statistical learning. I still stand behind this claim. However, it does not mean that we can automatically port all intuitions from the human learning domain. One example is that the human learning metaphor suggests that&amp;nbsp;&lt;em&gt;curriculum learning&lt;/em&gt; should be extremely useful for deep learning systems. But I have yet to see a convincing demonstration of this. (Though&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2306.11644&quot;&gt;&lt;u&gt;filtering to higher quality content&lt;/u&gt;&lt;/a&gt; does seem to help.)&lt;/p&gt;



&lt;p&gt;With disclaimers and caveats out of the way, let‚Äôs survey some of the metaphors that have been proposed for AI models.&amp;nbsp;&lt;/p&gt;



&lt;figure class=&quot;wp-block-image is-resized&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/WMJXHLmaCZi81466DjzoOCPwNBEge3EFiPsLeqRHdElf7NT1lww_L7yxg0W4LLo_YiU9hKZhfQ4gG2752UvU48PjWmoSaAPVrW7FbaTvEDreFPsdNmG776HN9j_zOyXztkvIs1b0SljU96o2WeCi8O8&quot; alt=&quot;&quot; width=&quot;582&quot; height=&quot;303&quot; /&gt;&lt;/figure&gt;



&lt;p&gt;Figure: OpenAI‚Äôs text-davinci-003 is 77% sure that it is not simply a next-token predictor.&lt;/p&gt;



&lt;h3 class=&quot;wp-block-heading&quot;&gt;&lt;strong&gt;Stochastic parrots&amp;nbsp;&lt;img src=&quot;https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f99c.png&quot; alt=&quot;ü¶ú&quot; class=&quot;wp-smiley&quot; style=&quot;height: 1em; max-height: 1em;&quot; /&gt;.&amp;nbsp;&lt;/strong&gt;&amp;nbsp;&lt;/h3&gt;



&lt;p&gt;In an&amp;nbsp;&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3442188.3445922&quot;&gt;&lt;u&gt;influential paper&lt;/u&gt;&lt;/a&gt;, Bender, Gebru, McMillan-Major, and Mitchell (aka ‚ÄúShmitchell‚Äù) proposed the ‚Äústochastic parrot‚Äù metaphor for large language models (LLMs). (The paper contains several different critiques of the reliance on size as the sole or main driver of performance, many of which do not rely on the validity of this metaphor.) In their words&amp;nbsp;&lt;em&gt;‚Äúan LM is a system for haphazardly stitching together sequences of linguistic forms it has observed in its vast training data, according to probabilistic information about how they combine, but without any reference to meaning: a stochastic parrot.‚Äù&lt;/em&gt;&amp;nbsp; This metaphor is literally true, in the sense that, true to their name, language modes aim to&amp;nbsp;&lt;em&gt;model&lt;/em&gt; language in the sense of reproducing the probability distribution of their training data. However, most people that have interacted with ChatGPT can see that it does much more than merely parrot its training set.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While it is true that Large Language Models are trained to maximize ‚Äúparroting‚Äù, i.e. minimize the cross-entropy loss, this loss function does not capture all of their capabilities. For example, according to the&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;&lt;u&gt;GPT-3 paper&lt;/u&gt;&lt;/a&gt;, (plugging numbers from Table D.1 into the formula in Figure 3.1),&amp;nbsp; the loss for their 13B parameter model was 1.97 while the loss for the 175B model is 1.73. This means that the 175B model is just a mildly better ‚Äúparrot‚Äù than the 13B model (roughly, guessing the next token with probability 17.7% instead of 14%). However, in many tasks, the 175B model is&amp;nbsp;&lt;em&gt;qualitatively superior&lt;/em&gt; to the 13B one, achieving near-perfect performance whereas the 13B one barely beats random guessing. Similarly, GPT4 is qualitatively stronger than GPT3 despite the (presumed) small difference in the validation loss. It is these emerging capabilities that are the essential property of LLMs, and the reason that the stochastic parrots metaphor is misleading.&lt;/p&gt;



&lt;p&gt;Emerging capabilities are not unique to language. The starting point for the&amp;nbsp;&lt;a href=&quot;https://www.nature.com/articles/nature16961&quot;&gt;&lt;u&gt;AlphaGo&lt;/u&gt;&lt;/a&gt; algorithm was training to predict (aka ‚Äúparrot‚Äù) human moves from the KGS Go Server. Then, using self-play, the algorithm ended up not merely mimicking human play but surpassing it. Ultimately, like the ‚Äústone soup‚Äù fable, it turned out that human data was not needed at all, and models could achieve superior play&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/1712.01815&quot;&gt;&lt;u&gt;starting from zero&lt;/u&gt;&lt;/a&gt;.&amp;nbsp;&lt;/p&gt;



&lt;figure class=&quot;wp-block-image is-resized&quot;&gt;&lt;img src=&quot;https://lh6.googleusercontent.com/G9GRhEOjubRWJBLClzuVAt39jl7Dq5cMKEAMGCh2Dkq2qUfSbk8NqL6aS0eWpgw7_al9spUzkv2xiF0p2q3mIUHUVrXmFD_zM67lrq-_qeo1-sM96lhpN75mVvruInK5ol1URXm1fkRMb0Kp96l65SI&quot; alt=&quot;&quot; width=&quot;568&quot; height=&quot;370&quot; /&gt;&lt;/figure&gt;



&lt;p&gt;Figure 3.10 in the&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;&lt;u&gt;GPT-3 paper&lt;/u&gt;&lt;/a&gt;. We see a dramatic increase in capabilities from the 13B model to the 175B model (for example going from 9.2% accuracy to 94.2% in 3-digit subtraction).&lt;/p&gt;



&lt;h3 class=&quot;wp-block-heading&quot;&gt;&lt;strong&gt;JPEG of the web&lt;/strong&gt; &lt;img src=&quot;https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f5bc.png&quot; alt=&quot;üñº&quot; class=&quot;wp-smiley&quot; style=&quot;height: 1em; max-height: 1em;&quot; /&gt;&lt;em&gt;&lt;strong&gt; &lt;/strong&gt; &lt;/em&gt;&lt;/h3&gt;



&lt;p&gt;In a February 2023&amp;nbsp;&lt;a href=&quot;https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web&quot;&gt;&lt;em&gt;&lt;u&gt;New Yorker&lt;/u&gt;&lt;/em&gt;&lt;u&gt; article&lt;/u&gt;&lt;/a&gt;, Ted Chiang wrote that&amp;nbsp;&lt;em&gt;‚ÄúChatGPT is a blurry JPEG of the web.‚Äù&amp;nbsp;&lt;/em&gt;He claimed that ChatGPT is analogous to&amp;nbsp;&lt;em&gt;‚Äúa compressed copy of all the text on the Web.‚Äù&lt;/em&gt; Once again, there is a sense in which this claim is true. 175B parameters, whether stored in 8,16, or 32-bit format, clearly isn‚Äôt very much compared to the size of the Internet. But Chiang seems to think that this is the&amp;nbsp;&lt;em&gt;essential&lt;/em&gt; fact about ChatGPT and that it is basically some degraded version of Google that would only be useful if we wanted to store the web in limited space. In his words,&amp;nbsp;&lt;em&gt;‚Äúwe aren‚Äôt losing our access to the Internet. So just how much use is a blurry jpeg, when you still have the original?‚Äù&lt;/em&gt; Yet, by the time Chiang wrote the article, ChatGPT already reached&amp;nbsp;&lt;a href=&quot;https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app&quot;&gt;&lt;u&gt;100 million users&lt;/u&gt;&lt;/a&gt;, so clearly plenty of people did find uses for this ‚Äúblurry jpeg‚Äù. Indeed, as we discuss below, it seems that Chiang himself by now moved to a new metaphor.&lt;/p&gt;



&lt;h3 class=&quot;wp-block-heading&quot;&gt;&lt;strong&gt;The new McKinsey.&lt;/strong&gt;&lt;img src=&quot;https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f574.png&quot; alt=&quot;üï¥&quot; class=&quot;wp-smiley&quot; style=&quot;height: 1em; max-height: 1em;&quot; /&gt;&lt;/h3&gt;



&lt;p&gt;In a May 2023&amp;nbsp;&lt;a href=&quot;https://www.newyorker.com/science/annals-of-artificial-intelligence/will-ai-become-the-new-mckinsey&quot;&gt;&lt;em&gt;&lt;u&gt;New Yorker&lt;/u&gt;&lt;/em&gt;&lt;u&gt; article&lt;/u&gt;&lt;/a&gt;, Chiang wrote&lt;em&gt; ‚ÄúI would like to propose another metaphor for the risks of artificial intelligence. I suggest that we think about A.I. as a management-consulting firm, along the lines of McKinsey &amp;amp; Company.‚Äù&amp;nbsp;&lt;/em&gt;He thinks of&amp;nbsp;&lt;em&gt;‚ÄúA.I. as a broad set of technologies being marketed to companies to help them cut their costs.‚Äù&amp;nbsp;&lt;/em&gt;&amp;nbsp;Once again, I believe Chiang misses the mark. First, the revenue of the major consulting companies put together is&amp;nbsp;&lt;a href=&quot;https://slidescience.co/mbb-firms/&quot;&gt;&lt;u&gt;less than $30B per year&lt;/u&gt;&lt;/a&gt;. If the overall impact of AI ended up being comparable, then it would certainly count as the scenario that Aaronson and I called&amp;nbsp;&lt;a href=&quot;https://scottaaronson.blog/?p=7266&quot;&gt;&lt;u&gt;‚ÄúAI Fizzle‚Äù&lt;/u&gt;&lt;/a&gt;. Just like digital computers ended up being so much more than a cheaper replacement for human numerical analysts, the exciting applications of AI are not about cutting costs but about creating new possibilities.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Chiang makes other points in the article, including a discussion of whether AI will end up strengthening capital or labor. I honestly think it‚Äôs impossible to predict these in advance. In fact, it seems that we cannot even agree about the past. Chiang argues that since the 1980s, economic conditions have deteriorated for the average American and says that&amp;nbsp;&lt;em&gt;‚Äúshopping online is fast and easy, and streaming movies at home is cool, but I think a lot of people would willingly trade those conveniences for the ability to own their own homes, send their kids to college without running up lifelong debt, and go to the hospital without falling into bankruptcy.‚Äù&amp;nbsp; &lt;/em&gt;Yet, home ownership has hovered between 63 to 67 percent in the last 50 years, while the share of the population that completed college and life expectancy (aside from COVID) steadily increased (see&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Home-ownership_in_the_United_States&quot;&gt;&lt;u&gt;here&lt;/u&gt;&lt;/a&gt;,&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Educational_attainment_in_the_United_States&quot;&gt;&lt;u&gt;here&lt;/u&gt;&lt;/a&gt;, and&amp;nbsp;&lt;a href=&quot;https://ourworldindata.org/grapher/life-expectancy&quot;&gt;&lt;u&gt;here&lt;/u&gt;&lt;/a&gt;). In any case, the U.S. economy is not the right lens to view AI. Whether or not the benefits of AI will accrue disproportionately to the top percentile has less to do with the technology, and more to do with economic policies, which vary by country.&lt;br&gt;&lt;br&gt;&lt;img src=&quot;https://lh4.googleusercontent.com/Zmfk5VQbG77hHS2W5WzBmD5aoShTiDzUgVTtkb7X7hcHSYhuzKp14QGVxnPXxrhApsZKqEog5V_1plmJyo0TpUxTCFJQF2WRZ0IjT5FOxJHxwIvzV8Kim9TJgTYqtDosoJ_zasCsEKB7cgHqDKgs8qo&quot; style=&quot;width: 400px&quot;&gt;&lt;/p&gt;



&lt;figure class=&quot;wp-block-image is-resized&quot;&gt;&lt;img src=&quot;https://lh6.googleusercontent.com/R0eIt0Eb-yrYqulpf6SCTP-DdLmrRjBIPYgVVQqrKfXanI3cKuJJXS0CqzOgT0nnOdCAyu5cqKnfXUMfDXNc0w1_PF7el6LVxwud9sFUpRrHPRTdQn8rL8JkGuSrxJR4WG63wqRZvti3a6QI8XjPLbM&quot; alt=&quot;&quot; width=&quot;531&quot; height=&quot;419&quot; /&gt;&lt;/figure&gt;



&lt;figure class=&quot;wp-block-image is-resized&quot;&gt;&lt;img src=&quot;https://lh5.googleusercontent.com/zagieAqaK_psSzDgWFE72XVCNguIG18o6-Fzg7D5yiid3DIRtJd02Kba9xsKFMKo7pG3FhoSRliPv98vdzOKxX6LS14_xP7aMtUtianXvaAkE1vIPkPRa42RAmIcottA2axeB5B9Sq4mOrOIXvbuMIQ&quot; alt=&quot;&quot; width=&quot;556&quot; height=&quot;393&quot; /&gt;&lt;/figure&gt;



&lt;h3 class=&quot;wp-block-heading&quot;&gt;&lt;strong&gt;Markets, Bureaucracies, Democracies&amp;nbsp;&lt;img src=&quot;https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4b5.png&quot; alt=&quot;üíµ&quot; class=&quot;wp-smiley&quot; style=&quot;height: 1em; max-height: 1em;&quot; /&gt; &lt;img src=&quot;https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f5f3.png&quot; alt=&quot;üó≥&quot; class=&quot;wp-smiley&quot; style=&quot;height: 1em; max-height: 1em;&quot; /&gt;&lt;/strong&gt;&lt;/h3&gt;



&lt;p&gt;In a June 2023&amp;nbsp;&lt;a href=&quot;https://www.economist.com/by-invitation/2023/06/21/artificial-intelligence-is-a-familiar-looking-monster-say-henry-farrell-and-cosma-shalizi&quot;&gt;&lt;u&gt;article in the Economist&lt;/u&gt;&lt;/a&gt; (see also&amp;nbsp;&lt;a href=&quot;https://twitter.com/henryfarrell/status/1671547591262191618?s=20&quot;&gt;&lt;u&gt;Twitter thread&lt;/u&gt;&lt;/a&gt;), Henry Farrell and Cosma Shalizi compared AIs to entities such as&amp;nbsp;&lt;em&gt;‚Äúmarkets, bureaucracies, and even democracy.‚Äù&lt;/em&gt; To be fair, Farell and Shalizi do not say that AIs are the same as these entities (which obviously are not the same as one another), but rather that they share essential similarities. This is building on Shalizi‚Äôs fascinating short essay,&amp;nbsp;&lt;a href=&quot;http://bactra.org/weblog/699.html&quot;&gt;&lt;u&gt;‚ÄúThe Singularity in our past light-cone‚Äù&lt;/u&gt;&lt;/a&gt; (2010), which claimed that the singularity has already happened (or maybe we‚Äôve already crossed the event horizon) during the industrial revolution. The idea is that we can think of markets, bureaucracies, and democracies as information-processing systems. Markets take as input massive amounts of data and produce the prices for goods, labor, and stocks. Democracy and bureaucracies take as input massive amounts of data (including citizens‚Äô preferences, gathered from elections, protests, media, and more) and produce decisions. Large language models are yet another system that processes massive amounts of data.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I believe that this is an interesting perspective, but again it fails to capture many aspects of AI, and so I disagree with Farell when&amp;nbsp;&lt;a href=&quot;https://twitter.com/henryfarrell/status/1671547607217209346?s=20&quot;&gt;&lt;u&gt;he says&lt;/u&gt;&lt;/a&gt;&amp;nbsp;&lt;em&gt;‚ÄúThe point is that [LLMs are] nothing new.‚Äù&lt;/em&gt; Currently, people are using ChatGPT as a tutor, researcher, writing assistant, coding assistant, and many more. None of these uses is captured by the ‚Äúmapping big data to society-wide decisions‚Äù framework of markets, bureaucracies, and democracy. Nevertheless, as Farell and Shalizi say, AI is likely to transform all of these entities, whether through automating arbitrages, gathering sentiments, or enabling new forms of regulations. Once again, I believe that whether or not this transformation will be for good or bad depends more on people‚Äôs decisions than on the technology itself. Furthermore, even after the transformation happened, we are likely not to have a consensus on whether it was good or bad.&lt;/p&gt;



&lt;h3 class=&quot;wp-block-heading&quot;&gt;&lt;strong&gt;King Midas / Genie&amp;nbsp;&lt;img src=&quot;https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f451.png&quot; alt=&quot;üëë&quot; class=&quot;wp-smiley&quot; style=&quot;height: 1em; max-height: 1em;&quot; /&gt;&lt;img src=&quot;https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f9de.png&quot; alt=&quot;üßû&quot; class=&quot;wp-smiley&quot; style=&quot;height: 1em; max-height: 1em;&quot; /&gt;&lt;/strong&gt;&lt;/h3&gt;



&lt;p&gt;One metaphor used for potentially risky AI is&amp;nbsp;&lt;a href=&quot;https://futureoflife.org/ai/artificial-intelligence-king-midas-problem/&quot;&gt;&lt;u&gt;‚ÄúKing Midas‚Äù&lt;/u&gt;&lt;/a&gt; or a&amp;nbsp; ‚ÄúGenie‚Äù. In this metaphor, AI is like one of the many entities in legends &amp;#8211; Genie,&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/The_Monkey%27s_Paw&quot;&gt;&lt;u&gt;Monkey‚Äôs paw&lt;/u&gt;&lt;/a&gt;, etc. &amp;#8211; that follow their user‚Äôs literal wishes, but in the least helpful way possible. This is not an unreasonable assumption. In almost any setting (aside from games such as Chess and Go) we cannot specify precisely our objective, and end up optimizing some&amp;nbsp;&lt;em&gt;proxy&lt;/em&gt; for it. This notion of ‚Äúover optimization‚Äù or ‚Äúover fitting‚Äù is extremely general in Machine Learning, deep or not. In particular, learning algorithms minimize ‚Äútrain loss‚Äù, which is a proxy for the ‚Äúvalidation loss‚Äù. As such, there have been many proposed solutions for over-optimizing, including early stopping and regularizing. Often protecting against over-optimization amounts to tuning a single hyperparameter.&amp;nbsp; Given that this is a well-established problem, which has well-understood families of solutions, I find it hard to believe that it will be an unsurmountable obstacle for mitigating ML risk.&amp;nbsp;&lt;br&gt;&amp;nbsp;&lt;/p&gt;



&lt;figure class=&quot;wp-block-image is-resized&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/MoRCsLLW0L8qeyP2TfGKPrNq5Jd6janyK9DqH4husP-9dRiz0L3rYZ-tQ-ZUh-7wamr6U623-uRNdNdKjLS-gIoINpty0cx-nGZpMAilIChXx1Wn0rrkwoe4sGNGEMRu3JylFSEoN4KMUr9FBxlaZVA&quot; alt=&quot;&quot; width=&quot;544&quot; height=&quot;278&quot; /&gt;&lt;/figure&gt;



&lt;p&gt;Figure from&amp;nbsp;&lt;a href=&quot;https://sohl-dickstein.github.io/2022/11/06/strong-Goodhart.html&quot;&gt;&lt;u&gt;Jascha Sohl-Dickstein‚Äôs blog&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;



&lt;h3 class=&quot;wp-block-heading&quot;&gt;&lt;strong&gt;Paperclip Maximizer&amp;nbsp;&lt;img src=&quot;https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4ce.png&quot; alt=&quot;üìé&quot; class=&quot;wp-smiley&quot; style=&quot;height: 1em; max-height: 1em;&quot; /&gt;&lt;/strong&gt;&lt;/h3&gt;



&lt;p&gt;The&lt;a href=&quot;https://www.lesswrong.com/tag/squiggle-maximizer-formerly-paperclip-maximizer&quot;&gt;&lt;u&gt; ‚Äúpaperclip maximizer‚Äù&lt;/u&gt;&lt;/a&gt; thought experiment is actually a metaphor I quite like. It does a good job of providing intuition for the scenario that it models. I just don‚Äôt believe that scenario is very likely. Unlike the ‚ÄúKing Midas‚Äù metaphor, in the ‚Äúpaperclip maximizer‚Äù setting, the AI model is not ‚Äúover optimizing‚Äù a human-provided objective but rather pursuing a goal that is completely unaligned with those of its human creators. Moreover, it pursues this goal with such fanaticism, that it cannot spare any resources, and so the whole galaxy is not big enough for humans and this goal to co-exist.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The paperclip maximizer is a fine metaphor, but I find it unlikely that highly intelligent entities will monomaniacally pursue any particular goal. Indeed, I think that highly-intelligent AIs will also realize (as do we) the dangers of ‚Äúover optimizing‚Äù to any particular goal. Just as humans have learned the wisdom of diversifying our investments and the precautionary principle (even if to a more limited than some readers would like), I suspect AIs would know this too. Using all of humanity‚Äôs atoms to make paperclips or squiggles strikes me as analogous to burning all the fossil fuels in the earth for energy. AIs will be smarter than that. I also tend to agree with Sohl-Dickstein‚Äôs&amp;nbsp;&lt;a href=&quot;https://sohl-dickstein.github.io/2023/03/09/coherence.html&quot;&gt;&lt;u&gt;‚Äúhot mess hypothesis‚Äù&lt;/u&gt;&lt;/a&gt; that more intelligent entities are likely to be&amp;nbsp;&lt;em&gt;less&lt;/em&gt; coherent in the sense of having a variety of competing goals, none of which is being pursued monomaniacally.&amp;nbsp;&lt;/p&gt;



&lt;figure class=&quot;wp-block-image is-resized&quot;&gt;&lt;img src=&quot;https://lh4.googleusercontent.com/Ntsfudiu47u0VeVqre-1dxyh-1LTNHst-x5xaKOf8erbcKzSx8AIQ_E7htzYVVFY3oFJlnQ6mHfbm6jcV8qVS9shqPa7UPtf4t3WW53Ro07iD50za2L4jKKsEc8BX0pAFQHudEfHyoqL0Bo5Q2Kd5Kw&quot; alt=&quot;&quot; width=&quot;579&quot; height=&quot;477&quot; /&gt;&lt;/figure&gt;



&lt;p&gt;Figure from&amp;nbsp;&lt;a href=&quot;https://sohl-dickstein.github.io/2023/03/09/coherence.html&quot;&gt;&lt;u&gt;Jascha Sohl-Dickstein‚Äôs blog&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;



&lt;h3 class=&quot;wp-block-heading&quot;&gt;&lt;strong&gt;Alien / New Species&amp;nbsp;&lt;img src=&quot;https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f47d.png&quot; alt=&quot;üëΩ&quot; class=&quot;wp-smiley&quot; style=&quot;height: 1em; max-height: 1em;&quot; /&gt;&lt;/strong&gt;&lt;/h3&gt;



&lt;figure class=&quot;wp-block-image is-resized&quot;&gt;&lt;img src=&quot;https://lh5.googleusercontent.com/5tcU02LK_EeMUuFVc_vB6_cIjSAnX7YhsbvVjvfrgdxOoQc-3nul8HnVwPrOWOCYuNVZt81Mb4iR3zio_rXX3G3RbaSS6aJDkNxeUhzSUi6G51U25cmw5M3pjiQu14Mzx3l-iEovJ9RqqCoMyCLDkCQ&quot; alt=&quot;&quot; width=&quot;567&quot; height=&quot;315&quot; /&gt;&lt;/figure&gt;



&lt;p&gt;Figure: Cartoon from my&amp;nbsp;&lt;a href=&quot;https://www.lesswrong.com/posts/wDL6wiqg3c6WFisHq/gpt-as-an-intelligence-forklift&quot;&gt;&lt;u&gt;‚ÄúIntelligence forklift‚Äù post&lt;/u&gt;&lt;/a&gt;, superimposing a 1000T parameter transformer on a figure from&amp;nbsp;&lt;a href=&quot;https://chomsky.info/20140826/&quot;&gt;&lt;u&gt;Bolhuis, Tattersall, Chomsky and Berwick&lt;/u&gt;&lt;/a&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;A persistent metaphor for future AIs is that they will be essentially a new alien or animal species, perhaps essentially the next in line in the genus&amp;nbsp;&lt;em&gt;Homo&lt;/em&gt;. In particular, since our brain are 3 times bigger than those of Chimpanzees, if AI‚Äôs ‚Äúbrain size‚Äù (e.g., number of parameters) would be at least 3 times bigger than ours, we might expect the relationship between AIs and us to be roughly the same as the relation between us and Chimpanzees. This is an important viewpoint to consider, but I believe that it should be taken with significant grains of salt, as it does anthropomorphize AIs too much. As&amp;nbsp;&lt;a href=&quot;https://www.lesswrong.com/posts/wDL6wiqg3c6WFisHq/gpt-as-an-intelligence-forklift&quot;&gt;&lt;u&gt;I wrote before&lt;/u&gt;&lt;/a&gt;, the fact that intelligence and agency co-evolved in humans does not mean that they cannot be decoupled in AIs. My main issue with the ‚Äúnew species‚Äù metaphor is one of&amp;nbsp;&lt;em&gt;quantifiers&lt;/em&gt; (‚àÄ vs. ‚àÉ). I have no doubt that eventually&amp;nbsp;&lt;em&gt;there will exist&lt;/em&gt; an AI that can be thought of as an individual creature. We should eventually be able to build a robot that has its own goals and can not only prove hard theorems but even&amp;nbsp;&lt;a href=&quot;https://behavior.stanford.edu/&quot;&gt;&lt;u&gt;cook an Omelet in a messy kitchen&lt;/u&gt;&lt;/a&gt;. However, I doubt that the species metaphor would be a good one for&amp;nbsp;&lt;em&gt;every&lt;/em&gt; strong AI or even for most of them.&lt;/p&gt;



&lt;p&gt;There is another issue with the ‚Äúnew species‚Äù metaphor, which is that it suggests that as a new species with a ‚Äúlarger brain‚Äù, AIs would completely dominate humans. However,&amp;nbsp;&lt;strong&gt;intelligence is not a single number&lt;/strong&gt;. Intelligence is a multidimensional set of skills/capabilities. This is one area where intuitions from games such as Chess or Go lead us astray, While in Chess it is possible to assign an ‚ÄúELO rating‚Äù which measures the skill (i.e. the&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2004.09468&quot;&gt;&lt;u&gt;transitive dimension&lt;/u&gt;&lt;/a&gt;), in real-life settings, people have a high-dimensional mixture of skills. Even in a field as ‚Äúintellectually pure‚Äù as pure mathematics, we cannot find anything similar to a total ordering of mathematicians by skill. If you have worked in this field, you will see that top mathematicians have incomparable strengths and weaknesses, and the set of theorems that one can prove is not a subset of that which can be proven by another.&lt;/p&gt;



&lt;p&gt;It is true that in the evolution of our genus, many of these skills improved together, and I believe it‚Äôs also true that humans are more intelligent than Chimpanzees across the board. But these correlations don‚Äôt necessarily indicate causation, and there is no reason for AIs to evolve the same way. Indeed, most people that have interacted with ChatGPT have seen it both perform highly impressive feats and make stupid mistakes. We also see it in&amp;nbsp;&lt;a href=&quot;https://huggingface.co/blog/evaluating-mmlu-leaderboard&quot;&gt;&lt;u&gt;the sensitivity of benchmarks&lt;/u&gt;&lt;/a&gt; to ways of measurement. (For example, unlike with humans, you actually need to worry about whether an LLM, when faced with answering a multiple choice question with options A,B,C,D, will output ‚ÄúZygote‚Äù instead.)&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;img src=&quot;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/98ffa3182cd22a89aac0474026bfe8a9945ca54ad9f6617d.png&quot; style=&quot;width: 450px&quot;&gt;&lt;br&gt;Figure: Cartoon of various skills, with the bars corresponding to the compute required to reach mean human performance for each skill. If the gap between the easiest and hardest skills is X orders of magnitude, and we assume increased performance is primarily driven by scale and Moore&amp;#8217;s law, then we might expect a period of roughly 7X years in which humans and AIs are incomparable.&lt;br&gt;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;All of which is not to say that ultimately one cannot get AI systems that are very strong in all of these dimensions, but just that we should not expect it to happen quickly, and there is likely to be an extended period in which AI‚Äôs and humans have different (and hence incomparable) skill profiles.&amp;nbsp; In particular, if we stay with our ‚Äúintelligence through scale‚Äù paradigm, improving on any particular skill would be quite expensive. It can cost three or more orders of magnitude more compute to move from a passable performance to a great one. And at the moment we have no way of getting near-perfect accuracy (99.9% or more), which is crucial for many applications. While I don‚Äôt doubt it is physically possible, we might never get to the point where it‚Äôs energy efficient or cost-effective to manufacture AIs that dominate humans in all intellectual dimensions. (And of course, humans are great at using tools, including AIs, to amplify our own shortcomings; this is how we routinely perform intellectual and physical feats today that were unthinkable 500 years ago, not to mention 5000 years, using essentially the same brains we had then.)&lt;/p&gt;



&lt;figure class=&quot;wp-block-image is-resized&quot;&gt;&lt;img src=&quot;https://lh6.googleusercontent.com/AJB_m9NVa7xfQmOHZ0arJClEy6Vs9eYRqpsWX395CGreCG36EbBnwI9s-ezV5Krgthc_SzUUyVtFmRGq-a1psOdZWdarLakejAELU-FYkIaQQuHspQzxTPDMV4KI6_8xyI6X7cVR-fS3-fo-Xub8zTc&quot; alt=&quot;&quot; width=&quot;392&quot; height=&quot;300&quot; /&gt;&lt;/figure&gt;



&lt;p&gt;Figure: Compute to benchmark extrapolations of&amp;nbsp;&lt;a href=&quot;https://www.alignmentforum.org/posts/75o8oja43LXGAqbAR/palm-2-and-gpt-4-in-extrapolating-gpt-n-performance&quot;&gt;&lt;u&gt;Lukas Finnveden&lt;/u&gt;&lt;/a&gt;. It typically takes around 3-4 orders of magnitude in compute cost from the point where models beat chance performance in a benchmark until the point they saturate it.&lt;/p&gt;



&lt;h3 class=&quot;wp-block-heading&quot;&gt;&lt;strong&gt;Shoggoth&lt;/strong&gt;&lt;/h3&gt;



&lt;figure class=&quot;wp-block-image is-resized&quot;&gt;&lt;img src=&quot;https://lh6.googleusercontent.com/Cfw3h_OTJUk-bqh3xCuHhGz5YCi8G7rP31oMRpb85XgVv7Cn55h038uxP04Es1OozKu9Z-AqBsyv-YGcIIGLR2W3QBEuM26jZdOieuy54YiE0nMzhvAM4Hhz6GKUCluEVU9CmikzCeg2lO6wn7dTm28&quot; alt=&quot;&quot; width=&quot;633&quot; height=&quot;187&quot; /&gt;&lt;/figure&gt;



&lt;p&gt;Left:&amp;nbsp;&lt;a href=&quot;https://www.deviantart.com/nottsuo&quot;&gt;&lt;u&gt;Nottsuo&lt;/u&gt;&lt;/a&gt;‚Äôs drawing of a Shoggoth.&amp;nbsp; Right: Cartoon by&amp;nbsp;&lt;a href=&quot;https://twitter.com/TetraspaceWest&quot;&gt;&lt;u&gt;@TetraspaceWest&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;



&lt;p&gt;A&amp;nbsp;&lt;a href=&quot;https://twitter.com/TetraspaceWest/status/1608966939929636864?s=20&quot;&gt;&lt;u&gt;Shoggoth&lt;/u&gt;&lt;/a&gt; is a perfect metaphor for AIs because no one knows what Shoggoths are. So, everyone can agree that ‚ÄúAI is a Shoggoth‚Äù means that AI is the thing that they already think AI is. (Apologies to the readers who propelled&amp;nbsp;&lt;a href=&quot;https://www.amazon.com/At-Mountains-Madness-H-Lovecraft/dp/1627555765/&quot;&gt;&lt;u&gt;At the Mountains of Madness&lt;/u&gt;&lt;/a&gt; to the 776,464th place on Amazon‚Äôs best-selling list.)&amp;nbsp; In particular,&amp;nbsp;&lt;a href=&quot;https://www.economist.com/by-invitation/2023/06/21/artificial-intelligence-is-a-familiar-looking-monster-say-henry-farrell-and-cosma-shalizi&quot;&gt;&lt;u&gt;Farrell and Shalizi&lt;/u&gt;&lt;/a&gt; call markets and democracies ‚ÄúShoggoths‚Äù. Wikipedia says that&amp;nbsp;&lt;em&gt;‚ÄúCthulhu Mythos media ‚Äã‚Äãmost commonly portray shoggoths as intelligent to some degree, but deal with problems using only their great size and strength‚Äù&lt;/em&gt;: a description that almost seems lifted from the ‚ÄúStochastic Parrots‚Äù paper.&amp;nbsp;&lt;a href=&quot;https://astralcodexten.substack.com/p/janus-simulators&quot;&gt;&lt;u&gt;Scott Alexander&lt;/u&gt;&lt;/a&gt; describes how the metaphors of Agent, Genie, or Oracle are not a good match for (non fine-tuned / RLHF‚Äôed) language models like GPT-3. Rather they are best captured as what Janus called a&amp;nbsp;&lt;a href=&quot;https://www.lesswrong.com/s/N7nDePaNabJdnbXeE/p/vJFdjigzmcXMhNTsx&quot;&gt;&lt;u&gt;Simulator&lt;/u&gt;&lt;/a&gt; and Scott calls ‚Äúan unmasked Shoggoth‚Äù. In both the market and simulator interpretations, the Shoggoth is a monster not because it‚Äôs evil but because it has no personality at all. (Though, despite writing in the&amp;nbsp;&lt;em&gt;Economist&lt;/em&gt;, Farrell and Shalizi seem to have a dim view of both markets and AIs.)&amp;nbsp; In p&lt;a href=&quot;https://astralcodexten.substack.com/i/97825611/iv-the-masked-shoggoth-between-keyboard-and-chair&quot;&gt;&lt;u&gt;art IV of his essay&lt;/u&gt;&lt;/a&gt;, Alexander speculates that perhaps we are all ‚Äúmasked Shoggoths‚Äù, and that becoming ‚Äúenlightened‚Äù corresponds to removing the mask, stopping being an agent, and being at peace just predicting that universe.&lt;br&gt;&amp;nbsp;&lt;/p&gt;



&lt;h3 class=&quot;wp-block-heading&quot;&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/h3&gt;



&lt;p&gt;AI systems of the type that have recently been created, and that will appear in the near future, are fundamentally new objects. Metaphors can be useful ways to build intuitions for them, but we should be wary of any article of the form ‚ÄúAI is like X‚Äù, especially one aimed at the general public. Probably the best we can say is that ‚ÄúThis aspect of AI is like X‚Äù or ‚ÄúThese types of AI systems, used in this way, are like X‚Äù. As mentioned, some of the people that coined these metaphors are well aware of these limitations, but metaphors have a way of being over-interpreted. I hope this essay can serve as a partial antidote to this tendency.&lt;br&gt;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Acknowledgments:&lt;/strong&gt; Thanks to Scott Aaronson, Michael Nielsen, and Preetum Nakkiran, for commenting on a previous version of this essay. Needless to say, they are not responsible for any views or errors in it.&lt;br&gt;&amp;nbsp;&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Boaz Barak&lt;/p&gt;
  </content>
    <author>
      <name>Windows on Theory</name>
      <uri>https://windowsontheory.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">ECCC Papers: TR23-092 |  Extracting Mergers and Projections of Partitions | 

	Swastik Kopparty, 

	Vishvajeet N</title>
    <link href="https://eccc.weizmann.ac.il/report/2023/092"/>
    <id>https://eccc.weizmann.ac.il/report/2023/092</id>
    <updated>2023-06-28T13:25:21+00:00</updated>
    <content type="html" xml:lang="en">
    We study the problem of extracting randomness from somewhere random sources, and related combinatorial phenomena: partition analogues of Shearer&amp;#39;s lemma on projections.

A somewhere-random source is a tuple $(X_1, \ldots, X_t)$ of (possibly correlated) $\{0,1\}^n$-valued random variables $X_i$ where for some unknown $i \in [t]$, $X_i$ is guaranteed to be uniformly distributed. 

An $extracting$ $merger$ is a seeded device that takes a somewhere-random source as input and outputs nearly uniform random bits. We study the seed-length needed for extracting mergers with constant $t$ and constant error.

Since a somewhere-random source has min-entropy at least $n$, a standard extractor can also serve as an extracting merger. Our goal is to understand whether the further structure of being somewhere random rather than just having high entropy enables smaller seed length, and towards this we show:

$\cdot$ Just like in the case of standard extractors, seedless extracting mergers with even just one output bit do not exist.

$\cdot$ Unlike the case of standard extractors, it $is$ possible to have extracting mergers that output a constant number of bits using only constant seed. Furthermore, a random choice of merger does not work for this purpose!

$\cdot$ Nevertheless, just like in the case of standard extractors, an extracting merger which gets most of the entropy out (namely, having $\Omega(n)$ output bits) must have $\Omega(\log n)$ seed. This is the main technical result of our work, and is proved by a second moment strengthening of the graph-theoretic approach of Radhakrishnan and Ta-Shma to extractors.

All this is in contrast to the status for condensing mergers (where the output is only required to have high min-entropy), whose seed length/output-length tradeoffs can all be fully explained by using standard condensers.

Inspired by such considerations, we also formulate a new and basic class of  problems in combinatorics: partition analogues of Shearer&amp;#39;s lemma. We show basic results in this direction; in particular, we prove that in any partition of the $3$-dimensional cube $[0,1]^3$ into two parts, one of the parts has an axis parallel $2$-dimensional projection of area at least $3/4$.
  </content>
    <author>
      <name>ECCC Papers</name>
      <uri>https://eccc.weizmann.ac.il/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: One-step replica symmetry breaking in the language of tensor networks</title>
    <link href="http://arxiv.org/abs/2306.15004"/>
    <id>http://arxiv.org/abs/2306.15004</id>
    <updated>2023-06-28T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Pancotti_N/0/1/0/all/0/1&quot;&gt;Nicola Pancotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Gray_J/0/1/0/all/0/1&quot;&gt;Johnnie Gray&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We develop an exact mapping between the one-step replica symmetry breaking
cavity method and tensor networks. The two schemes come with complementary
mathematical and numerical toolboxes that could be leveraged to improve the
respective states of the art. As an example, we construct a tensor-network
representation of Survey Propagation, one of the best deterministic k-SAT
solvers. The resulting algorithm outperforms any existent tensor-network solver
by several orders of magnitude. We comment on the generality of these ideas,
and we show how to extend them to the context of quantum tensor networks.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: From $O(\sqrt n)$ to $O(\log n)$ in Quadratic Programming</title>
    <link href="http://arxiv.org/abs/2306.15079"/>
    <id>http://arxiv.org/abs/2306.15079</id>
    <updated>2023-06-28T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Liang Wu&lt;/a&gt;&lt;/p&gt;&lt;p&gt;A &quot;dark cloud&quot; hangs over numerical optimization theory for decades, namely,
whether an optimization algorithm $O(\log(n))$ iteration complexity exists.
&quot;Yes&quot;, this paper answers, with a new optimization algorithm and strict theory
proof. It starts with box-constrained quadratic programming (Box-QP), and many
practical optimization problems fall into Box-QP. Smooth quadratic programming
(QP) and nonsmooth Lasso can be reformulated as Box-QP via duality theory. It
is the first time to present an $O(\log(n))$ iteration complexity QP algorithm,
in particular, which behaves like a &quot;direct&quot; method: the required number of
iterations is deterministic with exact value
$\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$.
This significant breakthrough enables us to transition from the $O(\sqrt{n})$
to the $O(\log(n))$ optimization algorithm, whose amazing scalability is
particularly relevant in today&#39;s era of big data and artificial intelligence.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: A Note on the Complexity of One-Sided Crossing Minimization of Trees</title>
    <link href="http://arxiv.org/abs/2306.15339"/>
    <id>http://arxiv.org/abs/2306.15339</id>
    <updated>2023-06-28T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dobler_A/0/1/0/all/0/1&quot;&gt;Alexander Dobler&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In 2011, Harrigan and Healy published a polynomial-time algorithm for
one-sided crossing minimization for trees. We point out a counterexample to
that algorithm, and show that one-sided crossing minimization is NP-hard for
trees.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design</title>
    <link href="http://arxiv.org/abs/2306.15656"/>
    <id>http://arxiv.org/abs/2306.15656</id>
    <updated>2023-06-28T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1&quot;&gt;Fu-Ming Guo&lt;/a&gt;&lt;/p&gt;&lt;p&gt;This paper introduces SparseOptimizer, a novel deep learning optimizer that
exploits Moreau-Yosida regularization to naturally induce sparsity in large
language models such as BERT, ALBERT and GPT. Key to the design of
SparseOptimizer is an embedded shrinkage operator, which imparts sparsity
directly within the optimization process. This operator, backed by a sound
theoretical framework, includes an analytical solution, thereby reinforcing the
optimizer&#39;s robustness and efficacy. Crucially, SparseOptimizer&#39;s plug-and-play
functionality eradicates the need for code modifications, making it a
universally adaptable tool for a wide array of large language models. Empirical
evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2
confirm that SparseBERT and SparseALBERT, when sparsified using
SparseOptimizer, achieve performance comparable to their dense counterparts,
BERT and ALBERT, while significantly reducing their parameter count. Further,
this work proposes an innovative optimizer-compiler co-design strategy,
demonstrating the potential of inference acceleration (\textbf{3.37x},
\textbf{6.30x}, and \textbf{7.15x} in comparison with Pytorch, TensorFlow, and
LLVM generic compile, respectively) in SparseBERT when paired with an
appropriately designed compiler. This study represents a significant step
forward in the evolution of efficient, scalable, and high-performing large
language models, setting a precedent for future exploration and optimization in
this domain. The SparseOptimizer code and SparseALBERT model will be made
available upon paper acceptance.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: Approximating Median Points in a Convex Polygon</title>
    <link href="http://arxiv.org/abs/2306.15097"/>
    <id>http://arxiv.org/abs/2306.15097</id>
    <updated>2023-06-28T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mohammadi_R/0/1/0/all/0/1&quot;&gt;Reyhaneh Mohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Devulapalli_R/0/1/0/all/0/1&quot;&gt;Raghuveer Devulapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Behroozi_M/0/1/0/all/0/1&quot;&gt;Mehdi Behroozi&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We develop two simple and efficient approximation algorithms for the
continuous $k$-medians problems, where we seek to find the optimal location of
$k$ facilities among a continuum of client points in a convex polygon $C$ with
$n$ vertices in a way that the total (average) Euclidean distance between
clients and their nearest facility is minimized. Both algorithms run in
$\mathcal{O}(n + k + k \log n)$ time. Our algorithms produce solutions within a
factor of 2.002 of optimality. In addition, our simulation results applied to
the convex hulls of the State of Massachusetts and the Town of Brookline, MA
show that our algorithms generally perform within a range of 5\% to 22\% of
optimality in practice.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: Heuristic Approaches to Obtain Low-Discrepancy Point Sets via Subset Selection</title>
    <link href="http://arxiv.org/abs/2306.15276"/>
    <id>http://arxiv.org/abs/2306.15276</id>
    <updated>2023-06-28T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clement_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Cl&amp;#xe9;ment&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_C/0/1/0/all/0/1&quot;&gt;Carola Doerr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paquete_L/0/1/0/all/0/1&quot;&gt;Lu&amp;#xed;s Paquete&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Building upon the exact methods presented in our earlier work [J. Complexity,
2022], we introduce a heuristic approach for the star discrepancy subset
selection problem. The heuristic gradually improves the current-best subset by
replacing one of its elements at a time. While we prove that the heuristic does
not necessarily return an optimal solution, we obtain very promising results
for all tested dimensions. For example, for moderate point set sizes $30 \leq n
\leq 240$ in dimension 6, we obtain point sets with $L_{\infty}$ star
discrepancy up to 35% better than that of the first $n$ points of the Sobol&#39;
sequence. Our heuristic works in all dimensions, the main limitation being the
precision of the discrepancy calculation algorithms.
&lt;/p&gt;
&lt;p&gt;We also provide a comparison with a recent energy functional introduced by
Steinerberger [J. Complexity, 2019], showing that our heuristic performs better
on all tested instances.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: Insertion-Only Dynamic Connectivity in General Disk Graphs</title>
    <link href="http://arxiv.org/abs/2306.15338"/>
    <id>http://arxiv.org/abs/2306.15338</id>
    <updated>2023-06-28T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaplan_H/0/1/0/all/0/1&quot;&gt;Haim Kaplan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klost_K/0/1/0/all/0/1&quot;&gt;Katharina Klost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knorr_K/0/1/0/all/0/1&quot;&gt;Kristin Knorr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mulzer_W/0/1/0/all/0/1&quot;&gt;Wolfgang Mulzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roditty_L/0/1/0/all/0/1&quot;&gt;Liam Roditty&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Let $S \subseteq \mathbb{R}^2$ be a set of $n$ \emph{sites} in the plane, so
that every site $s \in S$ has an \emph{associated radius} $r_s &amp;gt; 0$. Let $D(S)$
be the \emph{disk intersection graph} defined by $S$, i.e., the graph with
vertex set $S$ and an edge between two distinct sites $s, t \in S$ if and only
if the disks with centers $s$, $t$ and radii $r_s$, $r_t$ intersect. Our goal
is to design data structures that maintain the connectivity structure of $D(S)$
as $S$ changes dynamically over time. We consider the incremental case, where
new sites can be inserted into $S$. While previous work focuses on data
structures whose running time depends on the ratio between the smallest and the
largest site in $S$, we present a data structure with $O(\alpha(n))$ amortized
query time and $O(\log^6 n)$ expected amortized insertion time.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: Approximate Nearest Neighbor Searching with Non-Euclidean and Weighted Distances</title>
    <link href="http://arxiv.org/abs/2306.15621"/>
    <id>http://arxiv.org/abs/2306.15621</id>
    <updated>2023-06-28T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelkader_A/0/1/0/all/0/1&quot;&gt;Ahmed Abdelkader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arya_S/0/1/0/all/0/1&quot;&gt;Sunil Arya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fonseca_G/0/1/0/all/0/1&quot;&gt;Guilherme D. da Fonseca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mount_D/0/1/0/all/0/1&quot;&gt;David M. Mount&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We present a new approach to approximate nearest-neighbor queries in fixed
dimension under a variety of non-Euclidean distances. We are given a set $S$ of
$n$ points in $\mathbb{R}^d$, an approximation parameter $\varepsilon &amp;gt; 0$, and
a distance function that satisfies certain smoothness and growth-rate
assumptions. The objective is to preprocess $S$ into a data structure so that
for any query point $q$ in $\mathbb{R}^d$, it is possible to efficiently report
any point of $S$ whose distance from $q$ is within a factor of $1+\varepsilon$
of the actual closest point.
&lt;/p&gt;
&lt;p&gt;Prior to this work, the most efficient data structures for approximate
nearest-neighbor searching in spaces of constant dimensionality applied only to
the Euclidean metric. This paper overcomes this limitation through a method
called convexification. For admissible distance functions, the proposed data
structures answer queries in logarithmic time using $O(n \log (1 / \varepsilon)
/ \varepsilon^{d/2})$ space, nearly matching the best known bounds for the
Euclidean metric. These results apply to both convex scaling distance functions
(including the Mahalanobis distance and weighted Minkowski metrics) and Bregman
divergences (including the Kullback-Leibler divergence and the Itakura-Saito
distance).
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: Optimal Area-Sensitive Bounds for Polytope Approximation</title>
    <link href="http://arxiv.org/abs/2306.15648"/>
    <id>http://arxiv.org/abs/2306.15648</id>
    <updated>2023-06-28T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arya_S/0/1/0/all/0/1&quot;&gt;Sunil Arya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fonseca_G/0/1/0/all/0/1&quot;&gt;Guilherme D. da Fonseca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mount_D/0/1/0/all/0/1&quot;&gt;David M. Mount&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Approximating convex bodies is a fundamental question in geometry and has a
wide variety of applications. Given a convex body $K$ of diameter $\Delta$ in
$\mathbb{R}^d$ for fixed $d$, the objective is to minimize the number of
vertices (alternatively, the number of facets) of an approximating polytope for
a given Hausdorff error $\varepsilon$. The best known uniform bound, due to
Dudley (1974), shows that $O((\Delta/\varepsilon)^{(d-1)/2})$ facets suffice.
While this bound is optimal in the case of a Euclidean ball, it is far from
optimal for ``skinny&#39;&#39; convex bodies.
&lt;/p&gt;
&lt;p&gt;A natural way to characterize a convex object&#39;s skinniness is in terms of its
relationship to the Euclidean ball. Given a convex body $K$, define its surface
diameter $\Delta_{d-1}$ to be the diameter of a Euclidean ball of the same
surface area as $K$. It follows from generalizations of the isoperimetric
inequality that $\Delta \geq \Delta_{d-1}$.
&lt;/p&gt;
&lt;p&gt;We show that, under the assumption that the width of the body in any
direction is at least $\varepsilon$, it is possible to approximate a convex
body using $O((\Delta_{d-1}/\varepsilon)^{(d-1)/2})$ facets. This bound is
never worse than the previous bound and may be significantly better for skinny
bodies. The bound is tight, in the sense that for any value of $\Delta_{d-1}$,
there exist convex bodies that, up to constant factors, require this many
facets.
&lt;/p&gt;
&lt;p&gt;The improvement arises from a novel approach to sampling points on the
boundary of a convex body. We employ a classical concept from convexity, called
Macbeath regions. We demonstrate that Macbeath regions in $K$ and $K$&#39;s polar
behave much like polar pairs. We then apply known results on the Mahler volume
to bound their number.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Optimally Repurposing Existing Algorithms to Obtain Exponential-Time Approximations</title>
    <link href="http://arxiv.org/abs/2306.15331"/>
    <id>http://arxiv.org/abs/2306.15331</id>
    <updated>2023-06-28T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esmer_B/0/1/0/all/0/1&quot;&gt;Bar&amp;#x131;&amp;#x15f; Can Esmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulik_A/0/1/0/all/0/1&quot;&gt;Ariel Kulik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marx_D/0/1/0/all/0/1&quot;&gt;D&amp;#xe1;niel Marx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neuen_D/0/1/0/all/0/1&quot;&gt;Daniel Neuen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1&quot;&gt;Roohani Sharma&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The goal of this paper is to understand how exponential-time approximation
algorithms can be obtained from existing polynomial-time approximation
algorithms, existing parameterized exact algorithms, and existing parameterized
approximation algorithms. More formally, we consider a monotone subset
minimization problem over a universe of size $n$ (e.g., Vertex Cover or
Feedback Vertex Set). We have access to an algorithm that finds an
$\alpha$-approximate solution in time $c^k \cdot n^{O(1)}$ if a solution of
size $k$ exists (and more generally, an extension algorithm that can
approximate in a similar way if a set can be extended to a solution with $k$
further elements). Our goal is to obtain a $d^n \cdot n^{O(1)}$ time
$\beta$-approximation algorithm for the problem with $d$ as small as possible.
That is, for every fixed $\alpha,c,\beta \geq 1$, we would like to determine
the smallest possible $d$ that can be achieved in a model where our
problem-specific knowledge is limited to checking the feasibility of a solution
and invoking the $\alpha$-approximate extension algorithm. Our results
completely resolve this question:
&lt;/p&gt;
&lt;p&gt;(1) For every fixed $\alpha,c,\beta \geq 1$, a simple algorithm
(``approximate monotone local search&#39;&#39;) achieves the optimum value of $d$.
&lt;/p&gt;
&lt;p&gt;(2) Given $\alpha,c,\beta \geq 1$, we can efficiently compute the optimum $d$
up to any precision $\varepsilon &amp;gt; 0$.
&lt;/p&gt;
&lt;p&gt;Earlier work presented algorithms (but no lower bounds) for the special case
$\alpha = \beta = 1$ [Fomin et al., J. ACM 2019] and for the special case
$\alpha = \beta &amp;gt; 1$ [Esmer et al., ESA 2022]. Our work generalizes these
results and in particular confirms that the earlier algorithms are optimal in
these special cases.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Robust and Space-Efficient Dual Adversary Quantum Query Algorithms</title>
    <link href="http://arxiv.org/abs/2306.15040"/>
    <id>http://arxiv.org/abs/2306.15040</id>
    <updated>2023-06-28T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Czekanski_M/0/1/0/all/0/1&quot;&gt;Michael Czekanski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kimmel_S/0/1/0/all/0/1&quot;&gt;Shelby Kimmel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Witter_R/0/1/0/all/0/1&quot;&gt;R. Teal Witter&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The general adversary dual is a powerful tool in quantum computing because it
gives a query-optimal bounded-error quantum algorithm for deciding any Boolean
function. Unfortunately, the algorithm uses linear qubits in the worst case,
and only works if the constraints of the general adversary dual are exactly
satisfied. The challenge of improving the algorithm is that it is brittle to
arbitrarily small errors since it relies on a reflection over a span of
vectors. We overcome this challenge and build a robust dual adversary algorithm
that can handle approximately satisfied constraints. As one application of our
robust algorithm, we prove that for any Boolean function with polynomially many
1-valued inputs (or in fact a slightly weaker condition) there is a
query-optimal algorithm that uses logarithmic qubits. As another application,
we prove that numerically derived, approximate solutions to the general
adversary dual give a bounded-error quantum algorithm under certain conditions.
Further, we show that these conditions empirically hold with reasonable
iterations for Boolean functions with small domains. We also develop several
tools that may be of independent interest, including a robust approximate
spectral gap lemma, a method to compress a general adversary dual solution
using the Johnson-Lindenstrauss lemma, and open-source code to find solutions
to the general adversary dual.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Synthesis of Quantum Vector Databases Based on Grovers Algorithm</title>
    <link href="http://arxiv.org/abs/2306.15295"/>
    <id>http://arxiv.org/abs/2306.15295</id>
    <updated>2023-06-28T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Pronin_C/0/1/0/all/0/1&quot;&gt;Cesar Borisovich Pronin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ostroukh_A/0/1/0/all/0/1&quot;&gt;Andrey Vladimirovich Ostroukh&lt;/a&gt;&lt;/p&gt;&lt;p&gt;This paper describes a method for using Grovers algorithm to create a quantum
vector database, the database stores embeddings based on Controlled-S gates,
which represent a binary numerical value. This value represents the embeddings
value. The process of creating meaningful embeddings is handled by a classical
computer and the search process is handled by the quantum computer. This search
approach might be beneficial for a large enough database, or it could be seen
as a very qubit-efficient (super dense) way for storing data on a quantum
computer, since the proposed circuit stores many embeddings inside one quantum
register simultaneously.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: On the Deque and Rique Numbers of Complete and Complete Bipartite Graphs</title>
    <link href="http://arxiv.org/abs/2306.15395"/>
    <id>http://arxiv.org/abs/2306.15395</id>
    <updated>2023-06-28T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bekos_M/0/1/0/all/0/1&quot;&gt;Michael A. Bekos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaufmann_M/0/1/0/all/0/1&quot;&gt;Michael Kaufmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlidi_M/0/1/0/all/0/1&quot;&gt;Maria Eleni Pavlidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rieger_X/0/1/0/all/0/1&quot;&gt;Xenia Rieger&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Several types of linear layouts of graphs are obtained by leveraging known
data structures; the most notable representatives are the stack and the queue
layouts. In this content, given a data structure, one seeks to specify an order
of the vertices of the graph and a partition of its edges into pages, such that
the endpoints of the edges assigned to each page can be processed by the given
data structure in the underlying order. In this paper, we study deque and rique
layouts of graphs obtained by leveraging the double-ended queue and the
restricted-input double-ended queue (or deque and rique, for short),
respectively. Hence, they generalize both the stack and the queue layouts. We
focus on complete and complete bipartite graphs and present bounds on their
deque- and rique-numbers, that is, on the minimum number of pages needed by any
of these two types of linear layouts.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Scheduling with a Limited Testing Budget</title>
    <link href="http://arxiv.org/abs/2306.15597"/>
    <id>http://arxiv.org/abs/2306.15597</id>
    <updated>2023-06-28T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damerius_C/0/1/0/all/0/1&quot;&gt;Christoph Damerius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kling_P/0/1/0/all/0/1&quot;&gt;Peter Kling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Minming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruilong Zhang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Scheduling with testing falls under the umbrella of the research on
optimization with explorable uncertainty. In this model, each job has an upper
limit on its processing time that can be decreased to a lower limit (possibly
unknown) by some preliminary action (testing). Recently, D{\&quot;{u}}rr et al.
\cite{DBLP:journals/algorithmica/DurrEMM20} has studied a setting where testing
a job takes a unit time, and the goal is to minimize total completion time or
makespan on a single machine. In this paper, we extend their problem to the
budget setting in which each test consumes a job-specific cost, and we require
that the total testing cost cannot exceed a given budget. We consider the
offline variant (the lower processing time is known) and the oblivious variant
(the lower processing time is unknown) and aim to minimize the total completion
time or makespan on a single machine.
&lt;/p&gt;
&lt;p&gt;For the total completion time objective, we show NP-hardness and derive a
PTAS for the offline variant based on a novel LP rounding scheme. We give a
$(4+\epsilon)$-competitive algorithm for the oblivious variant based on a
framework inspired by the worst-case lower-bound instance. For the makespan
objective, we give an FPTAS for the offline variant and a
$(2+\epsilon)$-competitive algorithm for the oblivious variant. Our algorithms
for the oblivious variants under both objectives run in time
$O(poly(n/\epsilon))$. Lastly, we show that our results are essentially optimal
by providing matching lower bounds.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Asynchronous Algorithmic Alignment with Cocycles</title>
    <link href="http://arxiv.org/abs/2306.15632"/>
    <id>http://arxiv.org/abs/2306.15632</id>
    <updated>2023-06-28T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dudzik_A/0/1/0/all/0/1&quot;&gt;Andrew Dudzik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glehn_T/0/1/0/all/0/1&quot;&gt;Tamara von Glehn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1&quot;&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;State-of-the-art neural algorithmic reasoners make use of message passing in
graph neural networks (GNNs). But typical GNNs blur the distinction between the
definition and invocation of the message function, forcing a node to send
messages to its neighbours at every layer, synchronously. When applying GNNs to
learn to execute dynamic programming algorithms, however, on most steps only a
handful of the nodes would have meaningful updates to send. One, hence, runs
the risk of inefficiencies by sending too much irrelevant data across the graph
-- with many intermediate GNN steps having to learn identity functions. In this
work, we explicitly separate the concepts of node state update and message
function invocation. With this separation, we obtain a mathematical formulation
that allows us to reason about asynchronous computation in both algorithms and
neural networks.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">DifferentialPrivacy.org: Call for Papers - TPDP 2023 - Submission deadline July 7</title>
    <link href="https://differentialprivacy.org/tpdp2023/"/>
    <id>https://differentialprivacy.org/tpdp2023/</id>
    <updated>2023-06-28T00:01:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;The &lt;a href=&quot;https://tpdp.journalprivacyconfidentiality.org/2023/&quot;&gt;9th Workshop on the Theory and Practice of Differential Privacy (TPDP 2023)&lt;/a&gt; will take place in Boston September 27-28, 2023.
This is the first year the workshop is a standalone event. However, the &lt;a href=&quot;https://opendp.org/event/opendp-community-meeting-2023&quot;&gt;OpenDP community meeting&lt;/a&gt; is the following day (also in Boston). It is also moving from a one-day event to two days.&lt;/p&gt;

&lt;p&gt;The workshop is intended to bring together the DP research community to discuss new developments over the past year. The workshop is non-archival, so does not preclude publishing the work elsewhere.&lt;/p&gt;

&lt;p&gt;The submission deadline is July 7. Submissions should be 4 pages (plus references and appendices.)&lt;/p&gt;

&lt;p&gt;Submission website: TBD&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By &lt;/p&gt;
  </content>
    <author>
      <name>DifferentialPrivacy.org</name>
      <uri>https://differentialprivacy.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">DifferentialPrivacy.org: Open problem - Better privacy guarantees for larger groups</title>
    <link href="https://differentialprivacy.org/open-problem-better-privacy-guarantees-for-larger-groups/"/>
    <id>https://differentialprivacy.org/open-problem-better-privacy-guarantees-for-larger-groups/</id>
    <updated>2023-06-27T01:00:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;Consider a simple query counting the number of people in various mutually exclusive groups.
In the differential privacy literature, it is typical to assume that each of these groups should be subject to the same privacy loss: the noise added to each count has the same magnitude, and everyone gets the same privacy guarantees.
However, in settings where these groups have vastly different population sizes, larger populations may be willing to accept more error in exchange for stronger privacy protections.
In particular, in many use cases, &lt;em&gt;relative&lt;/em&gt; error (the noisy count is within 5% of the true value) matters more than absolute error (the noisy count is at a distance of at most 100 of the true value).
This leads to a natural question: can we use this fact to develop a mechanism that improves the privacy guarantees of individuals in larger groups, subject to a constraint on relative error?&lt;/p&gt;

&lt;h3 id=&quot;problem-definition&quot;&gt;Problem definition&lt;/h3&gt;

&lt;p&gt;Our goal is to obtain a mechanism which minimizes the overall privacy loss for each group without exceeding a relative error threshold for each group.
To formalize this goal, we first define a notion of per-group privacy we call group-wise zero-concentrated differential privacy as follows.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; &lt;em&gt;Group-wise zero-concentrated differential privacy.&lt;/em&gt;
Assume possible datasets consist of records from domain \(U\), and \(U\) can be partitioned into \(k\) fixed, disjoint groups \(U_1\), ‚Ä¶, \(U_k\). Let \(v : \mathcal{D} \rightarrow \mathbb{R}^k\) be a function associating a dataset to a vector of privacy budgets (one per group). We say a mechanism \(\mathcal{M}\) satisfies \(v\)-group-wise zero-concentrated differential privacy (zCDP) if for any two datasets \(D\), \(D‚Äô\) differing in the addition or removal of a record in \(U_i\), and for all \(\alpha&amp;gt;1\), we have:
\[
D_\alpha\left(\mathcal{M}(D||\mathcal{M}(D‚Äô)\right) \le \alpha \cdot {v(D)}_i
\]
\[
D_\alpha\left(\mathcal{M}(D‚Äô)||\mathcal{M}(D)\right) \le \alpha \cdot {v(D)}_i
\]
where \(D_\alpha\) is the R√©nyi divergence of order \(\alpha\).&lt;/p&gt;

&lt;p&gt;This definition is similar to &lt;em&gt;tailored DP&lt;/em&gt;, defined in [&lt;a href=&quot;https://eprint.iacr.org/2014/982.pdf&quot;&gt;LP15&lt;/a&gt;]: each individual gets a different privacy guarantee, depending on which group they belong to;
this guarantee also depends on how many people are in this group.
We use zCDP as our definition of privacy due to its compatibility with the Gaussian mechanism; the same idea could easily be applied to other definitions like with R√©nyi DP or pure DP.&lt;/p&gt;

&lt;p&gt;From there we can give a more formal definition of the problem as follows. The goal is to minimize the privacy loss for each individual group, while keeping the error under a given threshold.
For larger groups that can accept more noise, this means adding more noise to achieve the smallest possible privacy loss.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem.&lt;/strong&gt;
Let \(r \in (0,1]\) be an acceptable level of relative error, and \(k\) be the number of distinct, mutually-exclusive partitions of domain \(X\).
Given a dataset \(D\), let \(x(D)\) be a vector containing the count of records in each partition.
The objective is to find a mechanism \(\mathcal{M}\) which takes in \(r\), \(k\), and \(D\) and outputs \(\hat{x}(D)\) such that \(E\left[\left|{x(D)}_i-{\hat{x}(D)}_i\right|\right]&amp;lt;r\cdot {x(D)}_i\) for all \(i\), and satisfies \(v\)-group-wise zCDP where \(v(D)_i\) is as small as possible for all \(i\).
&lt;br /&gt;
To prevent pathological mechanisms that optimize for specific datasets, we add two constraints to the problem: the privacy guarantee \(v(D)_i\) should only depend on \(x(D)_i\), and should be nonincreasing with \(x(D)_i\).&lt;/p&gt;

&lt;p&gt;Since the relative error thresholds are proportional to the population size, each population can tolerate a different amount of noise.
This means that to minimize the privacy loss for each group, the mechanism must add noise of different scales to each group.
Of course, directly using \(x(D)_i\) to determine the scale of the noise for group \(i\) leads to a privacy loss which is data dependent, similarly to e.g. PATE [&lt;a href=&quot;https://openreview.net/forum?id=HkwoSDPg&quot;&gt;PAEGT17&lt;/a&gt;], and as such should be treated as a protected value.&lt;/p&gt;

&lt;h3 id=&quot;an-example-mechanism&quot;&gt;An example mechanism&lt;/h3&gt;

&lt;p&gt;An example mechanism that seems like it could address this problem is as follows.
First, perform the original counting query and add Gaussian noise to satisfy \(\rho\)-zCDP.
Then, add additional Gaussian noise to each count, with a variance that depends on the noisy count itself ‚Äî adding more noise to larger groups.
This mechanism is outlined in Algorithm 1.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Algorithm 1.&lt;/strong&gt;
&lt;em&gt;Adding data-dependent noise as a post-processing step.&lt;/em&gt;
&lt;br /&gt;
Require: A dataset \(D\) where each data point belongs to one of \(k\) groups, a privacy parameter \(\rho\), and a relative error rate \(r\).&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Let \(\sigma^2 = 1/(2\rho)\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;For&lt;/strong&gt; \(i=1\) to \(k\) &lt;strong&gt;do&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;\(\qquad\) Let \(x_i\) be the number of people in \(D\) in group \(i\)&lt;/li&gt;
  &lt;li&gt;\(\qquad\) Sample \(X_i \sim \mathcal{N}(x_i, \sigma^2)\)&lt;/li&gt;
  &lt;li&gt;\(\qquad\) Sample \(Y_i \sim \mathcal{N}_{k}(X_i, (rX_i)^2)\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;end for&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;return&lt;/strong&gt; \(Y_1,\dots,Y_k\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Algorithm 1 achieves this goal of having approximately \(r\) error in each group: the total variance error of the mechanism is \(\sigma^2 + (rX)^2\), and \(X\) is a zCDP measure of \(f(D)\).
This mechanism satisfies at least \(\rho\)-zCDP: line 4 is an invocation of the Gaussian mechanism with privacy parameter \(\rho\), and line 5 is a post processing step and as such preserves the zCDP guarantee.
We would like to show that this algorithm also satisfies a stronger group-wise zCDP guarantee.&lt;/p&gt;

&lt;p&gt;This makes intuitive sense: line 5 adds additional Gaussian noise without using the private data directly.
Since the noise scale in line 5 is proportional to the total count in line 4, we expect the privacy guarantee to be significantly stronger for large groups with more noise.
Further, we can verify experimentally that when the data magnitude is large compared to the noise, the output distribution for each group is close to a Gaussian distribution.&lt;/p&gt;

&lt;p&gt;The below figure illustrates this finding.
We plot 1,000,000 sample outputs of Algorithm 1 (red) with parameters \(\sigma^2 = 100\) and \(r= 0.3\), and compare it to the best fit Gaussian distribution (black outline) with mean \(10,002.6\) and standard deviation of \(2995.1\).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/two-stage-noise-gaussian.png&quot; width=&quot;70%&quot; alt=&quot;A comparison between sample outputs of Algorithm 1 and the best-fit Gaussian distribution, showing that both match very closely.&quot; style=&quot;margin:auto;display: block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With parameters such as these, the output of the mechanism looks and behaves like a Gaussian distribution, which should be ideal to characterize the zCDP guarantee.
However, it is difficult to directly quantify this guarantee, due to the changing variance which is also a random variable.
Likewise, if the true count is close to zero or if the first instance of noise is large compared to the true count than the resulting distribution takes on a heavy skew and is no longer similar to a single Gaussian distribution.
Such distributions with randomized variances have not, to the best of our knowledge, been considered much in the literature, and we do not know whether the mechanism‚Äôs output distribution follows some well-studied distribution.&lt;/p&gt;

&lt;p&gt;The randomized variance also makes it difficult to bound the R√©nyi divergence of the distribution and characterize the zCDP guarantees directly.
Current privacy amplification techniques are insufficient, as those techniques consider adding additional noise where the noise parameters are independent of the data itself.&lt;/p&gt;

&lt;p&gt;Perhaps the most promising direction to understand more about such processes is the area of stochastic differential equations, where it is common to study noise with data-dependent variance.
The Bessel process [&lt;a href=&quot;http://www.stat.ucla.edu/~ywu/research/documents/StochasticDifferentialEquations.pdf&quot;&gt;√òks03&lt;/a&gt;] is an example of such a process, where the noise is dependent on the current value.
This process captures the noise added as post-processing (Line 5), but not the initial noise-addition step (Line 4).
Furthermore, to the best of our knowledge, the Bessel process and other value-dependent stochastic differential equations do not have closed-form solutions.&lt;/p&gt;

&lt;h3 id=&quot;goal&quot;&gt;Goal&lt;/h3&gt;

&lt;p&gt;We see two possible paths forward to address the original question. One path would be to obtain an analysis of Algorithm 1 which shows non-trivial improved privacy guarantees for larger groups.
We tried multiple approaches, but could not prove such a result.&lt;/p&gt;

&lt;p&gt;An alternative path would be to develop a different algorithm, which achieves better privacy guarantees for larger groups while maintaining the error below the relative error threshold for all groups.&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By &lt;/p&gt;
  </content>
    <author>
      <name>DifferentialPrivacy.org</name>
      <uri>https://differentialprivacy.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: On the local consequence of modal Product logic: standard completeness and decidability</title>
    <link href="http://arxiv.org/abs/2306.13903"/>
    <id>http://arxiv.org/abs/2306.13903</id>
    <updated>2023-06-27T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Vidal_A/0/1/0/all/0/1&quot;&gt;Amanda Vidal&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Modal extensions of Product fuzzy logic can be considered both over Kripke
models whose accessibility relation is valued, and over Kripke models with
classical accessibility relation. We study the local consequence of the
previous two modal Product logics. We prove that these logics are standard
complete, in the sense that the logic defined over Kripke models evaluated over
all product algebras coincides with that defined over Kripke models evaluated
over the standard product algebra (with universe [0,1]). This holds both for
the logic over classical Kripke frames, and for that over frames with a valued
accessibility relation. Second, we prove that the previous logics are
decidable.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: A Circuit Complexity Formulation of Algorithmic Information Theory</title>
    <link href="http://arxiv.org/abs/2306.14087"/>
    <id>http://arxiv.org/abs/2306.14087</id>
    <updated>2023-06-27T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wyeth_C/0/1/0/all/0/1&quot;&gt;Cole Wyeth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sturtivant_C/0/1/0/all/0/1&quot;&gt;Carl Sturtivant&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Inspired by Solomonoffs theory of inductive inference, we propose a prior
based on circuit complexity. There are several advantages to this approach.
First, it relies on a complexity measure that does not depend on the choice of
UTM. There is one universal definition for Boolean circuits involving an
universal operation such as nand with simple conversions to alternative
definitions such as and, or, and not. Second, there is no analogue of the
halting problem. The output value of a circuit can be calculated recursively by
computer in time proportional to the number of gates, while a short program may
run for a very long time. Our prior assumes that a Boolean function, or
equivalently, Boolean string of fixed length, is generated by some Bayesian
mixture of circuits. This model is appropriate for learning Boolean functions
from partial information, a problem often encountered within machine learning
as &quot;binary classification.&quot; We argue that an inductive bias towards simple
explanations as measured by circuit complexity is appropriate for this problem.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: Using persistent homology to understand dimensionality reduction in resting-state fMRI</title>
    <link href="http://arxiv.org/abs/2306.13802"/>
    <id>http://arxiv.org/abs/2306.13802</id>
    <updated>2023-06-27T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Easley_T/0/1/0/all/0/1&quot;&gt;Ty Easley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freese_K/0/1/0/all/0/1&quot;&gt;Kevin Freese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munch_E/0/1/0/all/0/1&quot;&gt;Elizabeth Munch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bijsterbosch_J/0/1/0/all/0/1&quot;&gt;Janine Bijsterbosch&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Evaluating the success of a manifold learning method remains a challenging
problem, especially for methods adapted to a specific application domain. The
present work investigates shared geometric structure across different
dimensionality reduction (DR) algorithms within the scope of neuroimaging
applications. We examine reduced-dimension embeddings produced by a
representative assay of dimension reductions for brain data (&quot;brain
representations&quot;) through the lens of persistent homology, making statistical
claims about topological differences using a recent topological boostrap
method. We cluster these methods based on their induced topologies, finding
feature type and number -- rather than reduction algorithm -- as the main
drivers of observed topological differences.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Adaptive Privacy Composition for Accuracy-first Mechanisms</title>
    <link href="http://arxiv.org/abs/2306.13824"/>
    <id>http://arxiv.org/abs/2306.13824</id>
    <updated>2023-06-27T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rogers_R/0/1/0/all/0/1&quot;&gt;Ryan Rogers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samorodnitsky_G/0/1/0/all/0/1&quot;&gt;Gennady Samorodnitsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Steven Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramdas_A/0/1/0/all/0/1&quot;&gt;Aaditya Ramdas&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In many practical applications of differential privacy, practitioners seek to
provide the best privacy guarantees subject to a target level of accuracy. A
recent line of work by \cite{LigettNeRoWaWu17, WhitehouseWuRaRo22} has
developed such accuracy-first mechanisms by leveraging the idea of \emph{noise
reduction} that adds correlated noise to the sufficient statistic in a private
computation and produces a sequence of increasingly accurate answers. A major
advantage of noise reduction mechanisms is that the analysts only pay the
privacy cost of the least noisy or most accurate answer released. Despite this
appealing property in isolation, there has not been a systematic study on how
to use them in conjunction with other differentially private mechanisms. A
fundamental challenge is that the privacy guarantee for noise reduction
mechanisms is (necessarily) formulated as \emph{ex-post privacy} that bounds
the privacy loss as a function of the released outcome. Furthermore, there has
yet to be any study on how ex-post private mechanisms compose, which allows us
to track the accumulated privacy over several mechanisms. We develop privacy
filters \citep{RogersRoUlVa16, FeldmanZr21, WhitehouseRaRoWu22} that allow an
analyst to adaptively switch between differentially private and ex-post private
mechanisms subject to an overall privacy guarantee.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: A Dynamic Data Structure for Representing Timed Transitive Closures on Disk</title>
    <link href="http://arxiv.org/abs/2306.13937"/>
    <id>http://arxiv.org/abs/2306.13937</id>
    <updated>2023-06-27T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brito_L/0/1/0/all/0/1&quot;&gt;Luiz F. Afra Brito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albertini_M/0/1/0/all/0/1&quot;&gt;Marcelo Keese Albertini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Travencolo_B/0/1/0/all/0/1&quot;&gt;Bruno A. N. Traven&amp;#xe7;olo&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Temporal graphs represent interactions between entities over time. These
interactions may be direct, a contact between two vertices at some time
instant, or indirect, through sequences of contacts called journeys. Deciding
whether an entity can reach another through a journey is useful for various
applications in complex networks. In this paper, we present a disk-based data
structure that maintains temporal reachability information under the addition
of new contacts in a non-chronological order. It represents the \emph{timed
transitive closure} (TTC) by a set of \emph{expanded} R-tuples of the form $(u,
v, t^-, t^+)$, which encodes the existence of journeys from vertex $u$ to
vertex $v$ with departure at time $t^-$ and arrival at time $t^+$. Let $n$ be
the number of vertices and $\tau$ be the number of timestamps in the lifetime
of the temporal graph. Our data structure explicitly maintains this information
in linear arrays using $O(n^2\tau)$ space so that sequential accesses on disk
are prioritized. Furthermore, it adds a new unsorted contact $(u, v, t)$
accessing $O\left(\frac{n^2\tau}{B}\right)$ sequential pages in the worst-case,
where $B$ is the of pages on disk; it answers whether there is of a journey
from a vertex $u$ to a vertex $v$ within a time interval $[t_1, t_2]$ accessing
a single page; it answers whether all vertices can reach each other in $[t_1,
t_2]$; and it reconstructs a valid journey that validates the reachability from
a vertex $u$ to a vertex $v$ within $[t_1, t_1]$ accessing
$O\left(\frac{n\tau}{B}\right)$ pages. Our experiments show that our novel data
structure are better that the best known approach for the majority of cases
using synthetic and real world datasets.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: On Scalable Testing of Samplers</title>
    <link href="http://arxiv.org/abs/2306.13958"/>
    <id>http://arxiv.org/abs/2306.13958</id>
    <updated>2023-06-27T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pote_Y/0/1/0/all/0/1&quot;&gt;Yash Pote&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meel_K/0/1/0/all/0/1&quot;&gt;Kuldeep S. Meel&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this paper we study the problem of testing of constrained samplers over
high-dimensional distributions with $(\varepsilon,\eta,\delta)$ guarantees.
Samplers are increasingly used in a wide range of safety-critical ML
applications, and hence the testing problem has gained importance. For
$n$-dimensional distributions, the existing state-of-the-art algorithm,
$\mathsf{Barbarik2}$, has a worst case query complexity of exponential in $n$
and hence is not ideal for use in practice. Our primary contribution is an
exponentially faster algorithm that has a query complexity linear in $n$ and
hence can easily scale to larger instances. We demonstrate our claim by
implementing our algorithm and then comparing it against $\mathsf{Barbarik2}$.
Our experiments on the samplers $\mathsf{wUnigen3}$ and $\mathsf{wSTS}$, find
that $\mathsf{Barbarik3}$ requires $10\times$ fewer samples for
$\mathsf{wUnigen3}$ and $450\times$ fewer samples for $\mathsf{wSTS}$ as
compared to $\mathsf{Barbarik2}$.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Approximation Algorithm for Unrooted Prize-Collecting Forest with Multiple Components and Its Application on Prize-Collecting Sweep Coverage</title>
    <link href="http://arxiv.org/abs/2306.13996"/>
    <id>http://arxiv.org/abs/2306.13996</id>
    <updated>2023-06-27T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Wei Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhao Zhang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this paper, we present a polynomial time 2-approximation algorithm for the
{\em unrooted prize-collecting forest with $K$ components} (URPCF$_K$) problem,
the goal of which is to find a forest with exactly $K$ connected components to
minimize the weight of the forest plus the penalty incurred by the vertices not
spanned by the forest. For its rooted version RPCF$_K$, a 2-approximation
algorithm is known. For the unrooted version, transforming it into a rooted
version by guessing roots runs in time exponentially depending on $K$, which is
unacceptable when $K$ is not a constant. We conquer this challenge by designing
a rootless growing plus rootless pruning algorithm. As an application, we make
use of this algorithm to solve the {\em prize-collecting min-sensor sweep
cover} problem, improving previous approximation ratio 8 to 5.
&lt;/p&gt;
&lt;p&gt;Keywords: approximation algorithm, prize-collecting Steiner forest, sweep
cover.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: On finding 2-cuts and 3-edge-connected components in parallel</title>
    <link href="http://arxiv.org/abs/2306.14103"/>
    <id>http://arxiv.org/abs/2306.14103</id>
    <updated>2023-06-27T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsin_Y/0/1/0/all/0/1&quot;&gt;Yung H. Tsin&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Given a connected undirected multigraph G (a graph that may contain parallel
edges), the algorithm of [19] finds the 3-edge-connected components of $G$ in
linear time using an innovative graph contraction technique based on a
depth-first search. In [21], it was shown that the algorithm can be extended to
produce a Mader construction sequence for each 3-edge-connected component, a
cactus representation of the 2-cuts (cut-pairs) of each 2-edge-connected
component of $G$, and the 1-cuts (bridges) at the same time.
&lt;/p&gt;
&lt;p&gt;In this paper, we further extend the algorithm of [19] to generate the 2-cuts
and the 3-edge-connected components of $G$ simultaneously in linear time by
performing only one depth-first search over the input graph. Previously known
algorithms solve the two problems separately in multiple phases.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Richard Lipton: AI Ends It All</title>
    <link href="https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/"/>
    <id>https://rjlipton.wpcomstaging.com/?p=21826</id>
    <updated>2023-06-26T17:56:41+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;
I was getting a lift with a friend&amp;#8212;Greg Skau&amp;#8212;just the other day. No not in his boat, but in his car. &lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/gs/&quot; rel=&quot;attachment wp-att-21828&quot;&gt;&lt;img data-attachment-id=&quot;21828&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/gs/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?fit=219%2C320&amp;amp;ssl=1&quot; data-orig-size=&quot;219,320&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;1&amp;quot;}&quot; data-image-title=&quot;gs&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?fit=205%2C300&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?fit=219%2C320&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?resize=219%2C320&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;219&quot; height=&quot;320&quot; class=&quot;aligncenter size-full wp-image-21828&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?w=219&amp;amp;ssl=1 219w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?resize=205%2C300&amp;amp;ssl=1 205w&quot; sizes=&quot;(max-width: 219px) 100vw, 219px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;br /&gt;
Our conversation turned to the topic of: &amp;#8220;is AI a threat to all of us?&amp;#8221; Indeed. See &lt;a href=&quot;https://www.standard.co.uk/insider/ai-apocalypse-life-robots-take-over-elon-musk-chatgpt-b1078423.html&quot;&gt;this&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; The year is 2050. The location is London&amp;#8212; but not as we know it. GodBot, a robot so intelligent it can out-smart any human, is in charge of the United Kingdom &amp;#8212; the entire planet, in fact &amp;#8212; and just announced its latest plan to reverse global temperature rises: an international zero-child, zero-reproduction policy, which will see all human females systematically destroyed and replaced with carbon-neutral sex robots. &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
That my friend Greg would raise questions about AI seemed pretty neat. It seemed natural yet quite cool that a friend&amp;#8212;who was not an AI expert&amp;#8212;would raise these issues. I am also not an AI expert&amp;#8212;not even an advanced beginner. But it is on just about everyone&amp;#8217;s top list of questions. We had a fun conversation, but failed to resolve the issue. Of course.&lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; AI Limits &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
&lt;a href=&quot;https://openai.com&quot;&gt;OpenAI&lt;/a&gt; has just revealed that ChatGPT-4 has learned to lie, telling a human it was a blind person in order to get a task done. Somehow lies seem to set such AI systems apart from what we might have thought were the limits of AI.&lt;/p&gt;
&lt;p&gt;
Another thought on AI is: Is physical law an &lt;a href=&quot;https://getpocket.com/explore/item/is-physical-law-an-alien-intelligence?utm_source=pocket-newtab&quot;&gt;Alien Intelligence?&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; Arthur Clarke once pointed out that any sufficiently advanced technology is going to be indistinguishable from magic. If you dropped in on a bunch of Paleolithic farmers with your iPhone and a pair of sneakers, you&amp;#8217;d undoubtedly seem pretty magical. But the contrast is only middling: The farmers would still recognize you as basically like them, and before long they&amp;#8217;d be taking selfies. But what if life has moved so far on that it doesn&amp;#8217;t just appear magical, but appears like physics? &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
This is related to Clarke&amp;#8217;s &lt;a href=&quot;https://en.wikipedia.org/wiki/Clarke&amp;#37;27s_three_laws&quot;&gt;three laws&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong. &lt;/p&gt;
&lt;li&gt;
The only way of discovering the limits of the possible is to venture a little way past them into the impossible. &lt;/p&gt;
&lt;li&gt;
Any sufficiently advanced technology is indistinguishable from magic.
&lt;/ul&gt;
&lt;p&gt;
Or take a look at his T-shirt (referring to &lt;a href=&quot;https://web.mit.edu/m-i-t/science_fiction/jenkins/jenkins_4.html&quot;&gt;this&lt;/a&gt;): &lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;br /&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/ac/&quot; rel=&quot;attachment wp-att-21829&quot;&gt;&lt;img data-attachment-id=&quot;21829&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/ac/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?fit=240%2C240&amp;amp;ssl=1&quot; data-orig-size=&quot;240,240&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;ac&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?fit=240%2C240&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?fit=240%2C240&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?resize=240%2C240&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;240&quot; height=&quot;240&quot; class=&quot;aligncenter size-full wp-image-21829&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?w=240&amp;amp;ssl=1 240w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?resize=150%2C150&amp;amp;ssl=1 150w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?resize=200%2C200&amp;amp;ssl=1 200w&quot; sizes=&quot;(max-width: 240px) 100vw, 240px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; Open Problems &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
Alan Perlis&amp;#8212;the first Turing award winner&amp;#8212;is famous for his many &lt;a href=&quot;http://www.cs.yale.edu/homes/perlis-alan/quotes.html&quot;&gt;quotes&lt;/a&gt;. One was: &amp;#8220;A year spent in artificial intelligence is enough to make one believe in God.&amp;#8221;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/ai/&quot; rel=&quot;attachment wp-att-21830&quot;&gt;&lt;img data-attachment-id=&quot;21830&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/ai/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ai.jpeg?fit=300%2C240&amp;amp;ssl=1&quot; data-orig-size=&quot;300,240&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;ai&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ai.jpeg?fit=300%2C240&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ai.jpeg?fit=300%2C240&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ai.jpeg?resize=300%2C240&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;300&quot; height=&quot;240&quot; class=&quot;aligncenter size-full wp-image-21830&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
I enjoyed working for Alan at my first job at Yale University, many years ago. See &lt;a href=&quot;https://blog.computationalcomplexity.org/2021/06/i-went-to-debate-about-program-verif.html&quot;&gt;Fortnow&amp;#8217;s&lt;/a&gt; blog for comments on our joint work with Perlis and Rich DeMillo&amp;#8212;&lt;a href=&quot;https://www.cs.umd.edu/~gasarch/BLOGPAPERS/social.pdf&quot;&gt;Social Processes and Proofs of Theorems and Programs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;
Ken pipes in that the record seems to indicate that this late-1970s quote reflected frustration during the long &amp;#8220;AI winter&amp;#8221; when basic human capabilities stayed beyond reach. He also notes that the 2050 date for sex robots was &lt;a href=&quot;https://en.wikipedia.org/wiki/Love_and_Sex_with_Robots&quot;&gt;forecast&lt;/a&gt; by the British chess master who was previously best known for winning a famous computer bet in 1978.&lt;/p&gt;
&lt;p&gt;
&lt;p class=&quot;authors&quot;&gt;By rjlipton&lt;/p&gt;
  </content>
    <author>
      <name>Richard Lipton</name>
      <uri>https://rjlipton.wpcomstaging.com</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Decentralized Thoughts: $3f+1$ is needed in Partial Synchrony even against a Rollback adversary</title>
    <link href="https://decentralizedthoughts.github.io/2023-06-26-dls-meets-rollback/"/>
    <id>https://decentralizedthoughts.github.io/2023-06-26-dls-meets-rollback/</id>
    <updated>2023-06-26T11:00:00+00:00</updated>
    <content type="html" xml:lang="en">
    We covered the classic DLS88 split brain impossibility result against a Byzantine adversary in a previous post: DLS88: (Theorem 4.4) It is impossible to solve Agreement under partial synchrony against a Byzantine adversary if $f \geq n/3$. In a follow up, we discussed how CJKR12 strengthen this result by observing...
  </content>
    <author>
      <name>Decentralized Thoughts</name>
      <uri>https://decentralizedthoughts.github.io</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Quantum Merlin-Arthur and proofs without relative phase</title>
    <link href="http://arxiv.org/abs/2306.13247"/>
    <id>http://arxiv.org/abs/2306.13247</id>
    <updated>2023-06-26T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Bassirian_R/0/1/0/all/0/1&quot;&gt;Roozbeh Bassirian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Fefferman_B/0/1/0/all/0/1&quot;&gt;Bill Fefferman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Marwaha_K/0/1/0/all/0/1&quot;&gt;Kunal Marwaha&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study a variant of QMA where quantum proofs have no relative phase (i.e.
non-negative amplitudes, up to a global phase). If only completeness is
modified, this class is equal to QMA [&lt;a href=&quot;/abs/1410.2882&quot;&gt;arXiv:1410.2882&lt;/a&gt;]; but if both
completeness and soundness are modified, the class (named QMA+ by Jeronimo and
Wu) can be much more powerful. We show that QMA+ with some constant gap is
equal to NEXP, yet QMA+ with some *other* constant gap is equal to QMA. One
interpretation is that Merlin&#39;s ability to &quot;deceive&quot; originates from relative
phase at least as much as from entanglement, since QMA(2) $\subseteq$ NEXP.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: A SAT Solver and Computer Algebra Attack on the Minimum Kochen-Specker Problem</title>
    <link href="http://arxiv.org/abs/2306.13319"/>
    <id>http://arxiv.org/abs/2306.13319</id>
    <updated>2023-06-26T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Bright_C/0/1/0/all/0/1&quot;&gt;Curtis Bright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ganesh_V/0/1/0/all/0/1&quot;&gt;Vijay Ganesh&lt;/a&gt;&lt;/p&gt;&lt;p&gt;One of the foundational results in quantum mechanics is the Kochen-Specker
(KS) theorem, which states that any theory whose predictions agree with quantum
mechanics must be contextual, i.e., a quantum observation cannot be understood
as revealing a pre-existing value. The theorem hinges on the existence of a
mathematical object called a KS vector system. While many KS vector systems are
known to exist, the problem of finding the minimum KS vector system has
remained stubbornly open for over 55 years, despite significant attempts by
leading scientists and mathematicians. In this paper, we present a new method
based on a combination of a SAT solver and a computer algebra system (CAS) to
address this problem. Our approach improves the lower bound on the minimum
number of vectors in a KS system from 22 to 24, and is about 35,000 times more
efficient compared to the previous best computational methods. The increase in
efficiency derives from the fact we are able to exploit the powerful
combinatorial search-with-learning capabilities of a SAT solver together with
the isomorph-free exhaustive generation methods of a CAS. The quest for the
minimum KS vector system is motivated by myriad applications such as
simplifying experimental tests of contextuality, zero-error classical
communication, dimension witnessing, and the security of certain quantum
cryptographic protocols. To the best of our knowledge, this is the first
application of a novel SAT+CAS system to a problem in the realm of quantum
foundations.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: Decomposition of Geometric Graphs into Star Forests</title>
    <link href="http://arxiv.org/abs/2306.13201"/>
    <id>http://arxiv.org/abs/2306.13201</id>
    <updated>2023-06-26T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pach_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe1;nos Pach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Saghafian_M/0/1/0/all/0/1&quot;&gt;Morteza Saghafian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Schnider_P/0/1/0/all/0/1&quot;&gt;Patrick Schnider&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We solve a problem of Dujmovi\&#39;c and Wood (2007) by showing that a complete
convex geometric graph on $n$ vertices cannot be decomposed into fewer than
$n-1$ star-forests, each consisting of noncrossing edges. This bound is clearly
tight. We also discuss similar questions for abstract graphs.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: Effective data reduction algorithm for topological data analysis</title>
    <link href="http://arxiv.org/abs/2306.13312"/>
    <id>http://arxiv.org/abs/2306.13312</id>
    <updated>2023-06-26T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Seonmi Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jinseok Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jeong Rye Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Seung Yeop Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_H/0/1/0/all/0/1&quot;&gt;Hongdae Yun&lt;/a&gt;&lt;/p&gt;&lt;p&gt;One of the most interesting tools that have recently entered the data science
toolbox is topological data analysis (TDA). With the explosion of available
data sizes and dimensions, identifying and extracting the underlying structure
of a given dataset is a fundamental challenge in data science, and TDA provides
a methodology for analyzing the shape of a dataset using tools and prospects
from algebraic topology. However, the computational complexity makes it quickly
infeasible to process large datasets, especially those with high dimensions.
Here, we introduce a preprocessing strategy called the Characteristic Lattice
Algorithm (CLA), which allows users to reduce the size of a given dataset as
desired while maintaining geometric and topological features in order to make
the computation of TDA feasible or to shorten its computation time. In
addition, we derive a stability theorem and an upper bound of the barcode
errors for CLA based on the bottleneck distance.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Adversarial Resilience in Sequential Prediction via Abstention</title>
    <link href="http://arxiv.org/abs/2306.13119"/>
    <id>http://arxiv.org/abs/2306.13119</id>
    <updated>2023-06-26T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1&quot;&gt;Surbhi Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1&quot;&gt;Steve Hanneke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1&quot;&gt;Shay Moran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shetty_A/0/1/0/all/0/1&quot;&gt;Abhishek Shetty&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the problem of sequential prediction in the stochastic setting with
an adversary that is allowed to inject clean-label adversarial (or
out-of-distribution) examples. Algorithms designed to handle purely stochastic
data tend to fail in the presence of such adversarial examples, often leading
to erroneous predictions. This is undesirable in many high-stakes applications
such as medical recommendations, where abstaining from predictions on
adversarial examples is preferable to misclassification. On the other hand,
assuming fully adversarial data leads to very pessimistic bounds that are often
vacuous in practice.
&lt;/p&gt;
&lt;p&gt;To capture this motivation, we propose a new model of sequential prediction
that sits between the purely stochastic and fully adversarial settings by
allowing the learner to abstain from making a prediction at no cost on
adversarial examples. Assuming access to the marginal distribution on the
non-adversarial examples, we design a learner whose error scales with the VC
dimension (mirroring the stochastic setting) of the hypothesis class, as
opposed to the Littlestone dimension which characterizes the fully adversarial
setting. Furthermore, we design a learner for VC dimension~1 classes, which
works even in the absence of access to the marginal distribution. Our key
technical contribution is a novel measure for quantifying uncertainty for
learning VC classes, which may be of independent interest.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Breaking the cubic barrier in the Solovay-Kitaev algorithm</title>
    <link href="http://arxiv.org/abs/2306.13158"/>
    <id>http://arxiv.org/abs/2306.13158</id>
    <updated>2023-06-26T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kuperberg_G/0/1/0/all/0/1&quot;&gt;Greg Kuperberg&lt;/a&gt; (UC Davis)&lt;/p&gt;&lt;p&gt;We improve the Solovay-Kitaev theorem and algorithm for a general finite,
inverse-closed generating set acting on a qudit. Prior versions of the
algorithm can efficiently find a word of length $O((\log
1/\epsilon)^{3+\delta})$ to approximate an arbitrary target gate to within
$\epsilon$. Using two new ideas, each of which reduces the exponent separately,
our new bound on the world length is $O((\log
1/\epsilon)^{1.44042\ldots+\delta})$. Our result holds more generally for any
finite set that densely generates any connected, semisimple real Lie group,
with an extra length term in the non-compact case to reach group elements far
away from the identity.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: A Fast Maximum $k$-Plex Algorithm Parameterized by the Degeneracy Gap</title>
    <link href="http://arxiv.org/abs/2306.13258"/>
    <id>http://arxiv.org/abs/2306.13258</id>
    <updated>2023-06-26T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhengren Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chunyu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1&quot;&gt;Mingyu Xiao&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Given a graph, the $k$-plex is a vertex set in which each vertex is not
adjacent to at most $k-1$ other vertices in the set. The maximum $k$-plex
problem, which asks for the largest $k$-plex from a given graph, is an
important but computationally challenging problem in applications like graph
search and community detection. So far, there is a number of empirical
algorithms without sufficient theoretical explanations on the efficiency. We
try to bridge this gap by defining a novel parameter of the input instance,
$g_k(G)$, the gap between the degeneracy bound and the size of maximum $k$-plex
in the given graph, and presenting an exact algorithm parameterized by
$g_k(G)$. In other words, we design an algorithm with running time polynomial
in the size of input graph and exponential in $g_k(G)$ where $k$ is a constant.
Usually, $g_k(G)$ is small and bounded by $O(\log{(|V|)})$ in real-world
graphs, indicating that the algorithm runs in polynomial time. We also carry
out massive experiments and show that the algorithm is competitive with the
state-of-the-art solvers. Additionally, for large $k$ values such as $15$ and
$20$, our algorithm has superior performance over existing algorithms.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: On minimum $t$-claw deletion in split graphs</title>
    <link href="http://arxiv.org/abs/2306.13306"/>
    <id>http://arxiv.org/abs/2306.13306</id>
    <updated>2023-06-26T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Sounaka Mishra&lt;/a&gt;&lt;/p&gt;&lt;p&gt;For $t\geq 3$, $K_{1, t}$ is called $t$-claw. In minimum $t$-claw deletion
problem (\texttt{Min-$t$-Claw-Del}), given a graph $G=(V, E)$, it is required
to find a vertex set $S$ of minimum size such that $G[V\setminus S]$ is
$t$-claw free. In a split graph, the vertex set is partitioned into two sets
such that one forms a clique and the other forms an independent set. Every
$t$-claw in a split graph has a center vertex in the clique partition. This
observation motivates us to consider the minimum one-sided bipartite $t$-claw
deletion problem (\texttt{Min-$t$-OSBCD}). Given a bipartite graph $G=(A \cup
B, E)$, in \texttt{Min-$t$-OSBCD} it is asked to find a vertex set $S$ of
minimum size such that $G[V \setminus S]$ has no $t$-claw with the center
vertex in $A$. A primal-dual algorithm approximates \texttt{Min-$t$-OSBCD}
within a factor of $t$. We prove that it is $\UGC$-hard to approximate with a
factor better than $t$. We also prove it is approximable within a factor of 2
for dense bipartite graphs. By using these results on \texttt{Min-$t$-OSBCD},
we prove that \texttt{Min-$t$-Claw-Del} is $\UGC$-hard to approximate within a
factor better than $t$, for split graphs. We also consider their complementary
maximization problems and prove that they are $\APX$-complete.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Fair integer programming under dichotomous preferences</title>
    <link href="http://arxiv.org/abs/2306.13383"/>
    <id>http://arxiv.org/abs/2306.13383</id>
    <updated>2023-06-26T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demeulemeester_T/0/1/0/all/0/1&quot;&gt;Tom Demeulemeester&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goossens_D/0/1/0/all/0/1&quot;&gt;Dries Goossens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hermans_B/0/1/0/all/0/1&quot;&gt;Ben Hermans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leus_R/0/1/0/all/0/1&quot;&gt;Roel Leus&lt;/a&gt;&lt;/p&gt;&lt;p&gt;One cannot make truly fair decisions using integer linear programs unless one
controls the selection probabilities of the (possibly many) optimal solutions.
For this purpose, we propose a unified framework when binary decision variables
represent agents with dichotomous preferences, who only care about whether they
are selected in the final solution. We develop several general-purpose
algorithms to fairly select optimal solutions, for example, by maximizing the
Nash product or the minimum selection probability, or by using a random
ordering of the agents as a selection criterion (Random Serial Dictatorship).
As such, we embed the black-box procedure of solving an integer linear program
into a framework that is explainable from start to finish. Moreover, we study
the axiomatic properties of the proposed methods by embedding our framework
into the rich literature of cooperative bargaining and probabilistic social
choice. Lastly, we evaluate the proposed methods on a specific application,
namely kidney exchange. We find that while the methods maximizing the Nash
product or the minimum selection probability outperform the other methods on
the evaluated welfare criteria, methods such as Random Serial Dictatorship
perform reasonably well in computation times that are similar to those of
finding a single optimal solution.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Improved Competitive Ratios for Online Bipartite Matching on Degree Bounded Graphs</title>
    <link href="http://arxiv.org/abs/2306.13387"/>
    <id>http://arxiv.org/abs/2306.13387</id>
    <updated>2023-06-26T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yilong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaowei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shengwei Zhou&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We consider the online bipartite matching problem on $(k,d)$-bounded graphs,
where each online vertex has at most $d$ neighbors, each offline vertex has at
least $k$ neighbors, and $k\geq d\geq 2$. The model of $(k,d)$-bounded graphs
is proposed by Naor and Wajc (EC 2015 and TEAC 2018) to model the online
advertising applications in which offline advertisers are interested in a large
number of ad slots, while each online ad slot is interesting to a small number
of advertisers. They proposed deterministic and randomized algorithms with a
competitive ratio of $1 - (1-1/d)^k$ for the problem, and show that the
competitive ratio is optimal for deterministic algorithms. They also raised the
open questions of whether strictly better competitive ratios can be achieved
using randomized algorithms, for both the adversarial and stochastic arrival
models. In this paper we answer both of their open problems affirmatively. For
the adversarial arrival model, we propose a randomized algorithm with
competitive ratio $1 - (1-1/d)^k + \Omega(d^{-4}\cdot e^{-\frac{k}{d}})$ for
all $k\geq d\geq 2$. We also consider the stochastic model and show that even
better competitive ratios can be achieved. We show that for all $k\geq d\geq
2$, the competitive ratio is always at least $0.8237$. We further consider the
$b$-matching problem when each offline vertex can be matched at most $b$ times,
and provide several competitive ratio lower bounds for the adversarial and
stochastic model.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">ECCC Papers: TR23-091 |  Succinct Computational Secret Sharing | 

	Benny Applebaum, 

	Amos Beimel, 

	Yuval Ishai, 

	Eyal Kushilevitz, 

	Tianren Liu, 

	Vinod Vaikuntanathan</title>
    <link href="https://eccc.weizmann.ac.il/report/2023/091"/>
    <id>https://eccc.weizmann.ac.il/report/2023/091</id>
    <updated>2023-06-25T01:21:48+00:00</updated>
    <content type="html" xml:lang="en">
    A secret-sharing scheme enables a dealer to share a secret $s$ among $n$ parties such that only authorized subsets of parties, specified by a monotone access structure $f:\{0,1\}^n\to\{0,1\}$, can reconstruct $s$ from their shares. Other subsets of parties learn nothing about $s$.

The question of minimizing the (largest) share size for a given $f$ has been the subject of a large body of work. However, in most existing constructions for general access structures $f$, the share size is not much smaller than the size of some natural computational representation of $f$, a fact that has often been referred to as the ``representation size barrier&amp;#39;&amp;#39; in secret sharing.

In this work, we initiate a systematic study of succinct computational  secret sharing (SCSS), where the secrecy requirement is computational and the goal is to substantially beat the representation size barrier. We obtain the following main results.

(1) SCSS via Projective PRGs. We introduce the notion of a *projective PRG*, a pseudorandom generator for which any subset of the output bits can be revealed while keeping the other output bits hidden, using a *short* projective seed. We construct projective PRGs with different levels of succinctness under a variety of computational assumptions, and apply them towards constructing SCSS for graph access structures, monotone CNF formulas, and (less succinctly) useful subclasses of monotone circuits and branching programs. Most notably, under the sub-exponential RSA assumption, we obtain a SCSS scheme that, given an arbitrary access structure $f$, represented by a truth table of size $N=2^n$, produces shares of size $\polylog(N)=\poly(n)$ in time $\tilde O(N)$. For comparison, the share size of the best known information-theoretic schemes is $O(N^{0.58})$.

(2) SCSS via One-way Functions. Under the (minimal) assumption that one-way functions exist, we obtain a near-quadratic separation between the total share size of computational and information-theoretic secret sharing. This is the strongest separation one can hope for, given the state of the art in secret sharing lower bounds.
We also construct SCSS schemes from one-way functions for useful classes of access structures, including forbidden graphs and monotone DNF formulas.  This leads to constructions of fully-decomposable conditional disclosure of secrets (also known as privacy-free garbled circuits) for general functions, represented by a truth table of size $N=2^n$, with share size $\polylog(N)$ and computation time $\tilde O(N)$, assuming sub-exponentially secure one-way functions.
  </content>
    <author>
      <name>ECCC Papers</name>
      <uri>https://eccc.weizmann.ac.il/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Richard Lipton: A Hidden Heroine</title>
    <link href="https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/"/>
    <id>https://rjlipton.wpcomstaging.com/?p=21806</id>
    <updated>2023-06-24T21:53:47+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;William Friedman was famous as one who broke codes during both world wars. I knew about him from articles such as &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5419462/pdf/1.pdf&quot;&gt;this&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/wf/&quot; rel=&quot;attachment wp-att-21808&quot;&gt;&lt;img data-attachment-id=&quot;21808&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/wf/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/wf.jpeg?fit=299%2C168&amp;amp;ssl=1&quot; data-orig-size=&quot;299,168&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;wf&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/wf.jpeg?fit=299%2C168&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/wf.jpeg?fit=299%2C168&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/wf.jpeg?resize=299%2C168&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;299&quot; height=&quot;168&quot; class=&quot;aligncenter size-full wp-image-21808&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
But wait &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{&amp;#92;dots}&quot; class=&quot;latex&quot; /&gt; His wife Elizebeth Smith Friedman is the star of a PBS TV &lt;a href=&quot;https://www.imdb.com/title/tt12599258/&quot;&gt;special&lt;/a&gt;. Together they were the first great cryptographers of modern times. They quickly shifted gear from working on the hypothesis of embedded cryptograms in William Shakespeare&amp;#8217;s plays in 1915&amp;#8211;16 to helping the US WW I effort from 1917 on. &lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/ef/&quot; rel=&quot;attachment wp-att-21809&quot;&gt;&lt;img data-attachment-id=&quot;21809&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/ef/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ef.jpeg?fit=271%2C186&amp;amp;ssl=1&quot; data-orig-size=&quot;271,186&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;ef&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ef.jpeg?fit=271%2C186&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ef.jpeg?fit=271%2C186&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ef.jpeg?resize=271%2C186&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;271&quot; height=&quot;186&quot; class=&quot;aligncenter size-full wp-image-21809&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
What is so interesting is that I was unaware of her great contributions. I thought I knew the history of code breaking. But I was totally wrong. Elizebeth Friedman&amp;#8217;s work on decrypting coded radio messages helped tip the balances of WWI and WWII. She saved thousands of lives, but her work was hidden by the US government for 62 years. Her superiors&amp;#8212;all men&amp;#8212;took credit for her work. She initially got &lt;em&gt;none&lt;/em&gt;. Nothing at all. &lt;/p&gt;
&lt;p&gt;
This is&amp;#8212;at least it was&amp;#8212;one of the terrible injustices in the history of code breaking. &lt;/p&gt;
&lt;p&gt;
&lt;span id=&quot;more-21806&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;H2&gt; Century-Later Recognition &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
Her role was hidden until documents concerning it were declassified in 2008. She had taken an oath during her WW II work with the US Navy to keep that secret until her death, which came in 1980 with no fanfare. &lt;/p&gt;
&lt;p&gt;
Still, it took a decade more for true public awareness of her importance. Three recent biographies are: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
Gregg Stuart Smith, &lt;em&gt;A Life in Code: Pioneer Cryptanalyst Elizebeth Smith Friedman&lt;/em&gt;, &lt;a href=&quot;https://www.amazon.com/stores/G.-Stuart-Smith/author/B0077D2M0O?ref=ap_rdr&amp;#038;store_ref=ap_rdr&amp;#038;isDramIntegrated=true&amp;#038;shoppingPortalEnabled=true&quot;&gt;2017&lt;/a&gt;. &lt;/p&gt;
&lt;li&gt;
Jason Fagone, &lt;em&gt;The Woman Who Smashed Codes: A True Story of Love, Spies, and the Unlikely Heroine Who Outwitted America&amp;#8217;s Enemies&lt;/em&gt;, &lt;a href=&quot;https://www.amazon.com/Woman-Who-Smashed-Codes-Outwitted/dp/0062430513/ref=pd_lpo_sccl_2/146-5971751-8425411&quot;&gt;2018&lt;/a&gt;. &lt;/p&gt;
&lt;li&gt;
Amy Butler Greenfield, &lt;em&gt;The Woman All Spies Fear: Code Breaker Elizebeth Smith Friedman and Her Hidden Life&lt;/em&gt;, &lt;a href=&quot;https://www.goodreads.com/en/book/show/56364344-the-woman-all-spies-fear&quot;&gt;2021&lt;/a&gt;.
&lt;/ul&gt;
&lt;p&gt;
Although the last one is written for a young-adult audience, with large print and short chapters, it still has wonderful detail on her life and work. &lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/greenfieldbook/&quot; rel=&quot;attachment wp-att-21811&quot;&gt;&lt;img data-attachment-id=&quot;21811&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/greenfieldbook/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?fit=600%2C894&amp;amp;ssl=1&quot; data-orig-size=&quot;600,894&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;GreenfieldBook&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?fit=201%2C300&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?fit=600%2C894&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?resize=200%2C300&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;200&quot; height=&quot;300&quot; class=&quot;aligncenter wp-image-21811&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?resize=201%2C300&amp;amp;ssl=1 201w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?zoom=2&amp;amp;resize=200%2C300&amp;amp;ssl=1 400w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?zoom=3&amp;amp;resize=200%2C300&amp;amp;ssl=1 600w&quot; sizes=&quot;(max-width: 200px) 100vw, 200px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
In the meantime, the NSA&amp;#8217;s own tribute, dated &lt;a href=&quot;https://media.defense.gov/2021/Jul/13/2002761955/-1/-1/0/FRIEDMAN-LEGACY-TRANSCRIPT.PDF&quot;&gt;2006&lt;/a&gt;, gives details on Elizebeth but is headlined only for William. Most of its 226 pages reprints six lectures by William that were originally circulated within the agency in 1963. It reprints a 1980 memorial to her at the end.&lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; How She Started &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
Her origin story is amazing as well. She was one of only two in her Midwest farming family of nine to attend college and obtained a degree in English after transferring to a school closer to home. Her first job opportunity, a brief stint as substitute principal at a public high school in Indiana, did not lead to other teaching positions, so she moved back with her family. She journeyed to Chicago to look for work, using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Newberry_Library&quot;&gt;Newberry Library&lt;/a&gt; as a hub.&lt;/p&gt;
&lt;p&gt;
In one of her last days there, a librarian tipped her that a visiting millionaire, George Fabyan, was looking for help on a project involving Shakespeare. She connected with him and the scholarly director of the project, Elizabeth Gallup, and became employed at Fabyan&amp;#8217;s private Riverbank Research Laboratory.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/gf/&quot; rel=&quot;attachment wp-att-21812&quot;&gt;&lt;img data-attachment-id=&quot;21812&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/gf/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gf.jpeg?fit=203%2C249&amp;amp;ssl=1&quot; data-orig-size=&quot;203,249&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;gf&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gf.jpeg?fit=203%2C249&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gf.jpeg?fit=203%2C249&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gf.jpeg?resize=203%2C249&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;203&quot; height=&quot;249&quot; class=&quot;aligncenter size-full wp-image-21812&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
Riverbank had standard scientific projects as well. William Friedman was employed out of Cornell to work on plant genetics. One of several factors drawing him to the Shakespeare project was that his skill photographing plants was handy for images of original manuscripts kept in England. A second was developing techniques for statistical analysis. A third was Elizebeth. &lt;/p&gt;
&lt;p&gt;
By the time they married in 1917, they had worked out that the statistical randomness of defective type metal re-used by Elizabethan printers and bias in how the alleged codes in Shakespeare were identified effaced the claimed footprints of Francis Bacon&amp;#8217;s &lt;a href=&quot;https://en.wikipedia.org/wiki/Bacon&#39;s_cipher&quot;&gt;two-face cipher&lt;/a&gt;. Before they could even ascertain how to publish their eight draft papers of study, however, America&amp;#8217;s entry into World War I pressed them into other applications. Fabyan himself volunteered the services of his lab for top-secret work. &lt;/p&gt;
&lt;p&gt;
Their work on Shakespeare &lt;a href=&quot;https://www.amazon.com/Shakespearean-Ciphers-Examined-cryptographic-Shakespeare/dp/0521141397&quot;&gt;was published&lt;/a&gt; in 1957. It wasn&amp;#8217;t top secret and it bore both their names, as did its 1955 &lt;a href=&quot;https://books.google.com/books/about/The_Cryptologist_Looks_at_Shakespeare.html?id=BBdiuAAACAAJ&amp;#038;hl=en&amp;#038;output=html_text&quot;&gt;manuscript&lt;/a&gt; which won the Folger Library Shakespeare Prize. &lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; Open Problems &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
In 1999, the year of its creation, she was inducted to the NSA Hall of Honor&amp;#8212;see &lt;a href=&quot;https://www.pbs.org/wgbh/americanexperience/features/codebreaker-elizebeth-friedman-fought-nazi-spies/&quot;&gt;this&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/code-2/&quot; rel=&quot;attachment wp-att-21813&quot;&gt;&lt;img data-attachment-id=&quot;21813&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/code-2/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/code.jpeg?fit=320%2C208&amp;amp;ssl=1&quot; data-orig-size=&quot;320,208&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;code&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/code.jpeg?fit=300%2C195&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/code.jpeg?fit=320%2C208&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/code.jpeg?resize=320%2C208&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;320&quot; height=&quot;208&quot; class=&quot;aligncenter size-full wp-image-21813&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/code.jpeg?w=320&amp;amp;ssl=1 320w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/code.jpeg?resize=300%2C195&amp;amp;ssl=1 300w&quot; sizes=&quot;(max-width: 320px) 100vw, 320px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
I hope that in the future credit will be given equally to women as well as to men. This of course presupposes that the women had the opportunity to begin with. Let&amp;#8217;s hope so.&lt;/p&gt;
&lt;p&gt;
&lt;p class=&quot;authors&quot;&gt;By RJLipton+KWRegan&lt;/p&gt;
  </content>
    <author>
      <name>Richard Lipton</name>
      <uri>https://rjlipton.wpcomstaging.com</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">David Eppstein: Geometric flip-width revisited</title>
    <link href="https://11011110.github.io/blog/2023/06/24/geometric-flip-width.html"/>
    <id>https://11011110.github.io/blog/2023/06/24/geometric-flip-width</id>
    <updated>2023-06-24T18:07:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;I recently posted here about the &lt;a href=&quot;/blog/2023/02/20/geometric-graphs-unbounded.html&quot;&gt;flip-width of geometric graphs&lt;/a&gt;, and to readers of that post, my new preprint ‚ÄúGeometric Graphs with Unbounded Flip-Width‚Äù (&lt;a href=&quot;http://arxiv.org/abs/2306.12611&quot;&gt;arXiv:2306.12611&lt;/a&gt;, with Rose McCarty, to appear in CCCG 2023) will look very familiar. It even has the same title! However, the process of turning it into a paper led to some improvements. Let me summarize them briefly here.&lt;/p&gt;

&lt;p&gt;First, a reminder of the main concept, flip-width. This is defined using a pursuit‚Äìevasion game in which a robber tries to escape cops by following paths in a graph. At each turn, the cops have made a fixed number of ‚Äúflips‚Äù to the graph. Each flip applies to a subset of vertices (possibly overlapping with other flips), removes edges from its adjacent vertices, and adds edges connecting its non-adjacent vertices. A turn consists of three steps: the cops announce what they will flip next, the robber moves along a path of length at most \(s\), and then the cops undo their current flips and perform the new flips that they announced. The goal of the cops is to leave the robber stuck on an isolated vertex, while the goal of the robber is to escape forever. If a class of graphs has a function \(f(s)\) such that \(f(s)\) cops can win against a robber of speed \(s\), then it has bounded flip-width. If there is a speed \(s\) for which arbitrarily many cops may be needed to catch a speed-\(s\) robber, then the class has unbounded flip-width.&lt;/p&gt;

&lt;p&gt;Beyond a more careful attention to detail and rigor, new developments are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Three-dimensional Delaunay triangulations have unbounded flip-width. This uses a construction from another recent blog post on &lt;a href=&quot;/blog/2023/02/25/isohedral-delaunay-complexes.html&quot;&gt;isohedral Delaunay complexes&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Beta_skeleton&quot;&gt;Beta-skeletons&lt;/a&gt; have unbounded flip-width. There are actually two different kinds of beta-skeleton but the interesting case is for parameter values \(\beta&amp;lt;1\), for which the two definitions agree.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another type of geometric graph for which we can prove unbounded flip-width, not mentioned in the previous post, is the rectangle of influence graphs. These connect pairs of points in their plane when their axis-aligned bounding box is empty of other points. We find a recursive construction for rectangle of influence graphs containing hypercube induced subgraphs of arbitrarily large dimension, which in turn have unbounded flip-width. As with the beta-skeletons, definitions for rectangle of influence graphs disagree (about what to do with points on the boundary of the bounding box) but our hypercube construction doesn‚Äôt need that ambiguity.&lt;/p&gt;

    &lt;p style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/blog/assets/2023/empty-rectangle.svg&quot; alt=&quot;Hypercube in a rectangle of influence graph&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For most of the families of geometric graphs that we study, the robber can escape by stepping across only one edge per turn (\(s=1\)). This is a big improvement over the \(s=4\) from the blog post, and a much more natural speed limitation. The exception is for three-dimensional Delaunay triangulations; for these \(s=2\) works but we don‚Äôt know about \(s=1\). An \(s=2\) escape strategy for all of these graphs is very simple: move to a ‚Äúlane‚Äù (one of two special types of vertex in the ‚Äúinterchange‚Äù graphs constructed in the previous blog post) that has two-edge paths to many other lanes. The \(s=1\) strategy is different, and for some of the geometric graphs is based on hypercube subgraphs rather than interchanges.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An appendix extends the results on all of these graph classes (even 3d Delaunay triangulations) from unbounded flip-width to monadic independence, meaning that it is possible to use &lt;em&gt;transductions&lt;/em&gt;, a certain kind of translation system defined using logical formulas, to get arbitrary graphs from graphs in these classes. The main ideas (from Szymon Toru≈Ñczyk) are to use Ramsey theory to simplify the interchange subgraphs into two cases, ‚Äúsparse‚Äù and ‚Äúdense‚Äù, to use the structure of these graphs to find a logical translation from the dense case to the sparse case, and to find a subdivision of any given graph as an induced subgraph of a sparse interchange.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/110602049050780927&quot;&gt;Discuss on Mastodon&lt;/a&gt;)&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By David Eppstein&lt;/p&gt;
  </content>
    <author>
      <name>David Eppstein</name>
      <uri>https://11011110.github.io/blog/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Computational Complexity: Can you put n pennies on an n x n chessboard so that all of the distances are distinct/how to look that up?</title>
    <link href="https://blog.computationalcomplexity.org/2023/06/can-you-put-n-pennies-on-n-x-n.html"/>
    <id>tag:blogger.com,1999:blog-3722233.post-9038733708392236603</id>
    <updated>2023-06-24T14:35:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;&amp;nbsp;In Jan 2023 I went to the Joint Math Meeting of the AMS and the MAA and took notes on things to look up later. In one of the talks they discussed a problem and indicated that the answer was known, but did not give a reference or a proof. I emailed the authors and got no response. I tried to search the web but could not find it. SO I use this blog post to see if someone either knows the reference or can solve it outright, and either leave the answer in the comments, point to a paper that has the answer in the comments, or email me personally.&amp;nbsp;&lt;/p&gt;&lt;p&gt;--------------------------------------------------------------------&lt;/p&gt;&lt;p&gt;A chessboard has squares that are 1 by 1.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Pennies have diameter 1.&lt;/p&gt;&lt;p&gt;QUESTION:&amp;nbsp;&lt;/p&gt;&lt;p&gt;For which n is there a way to place n pennies on squares of the n x n chessboard so that all of the distances between centers of the pennies are DIFFERENT?&lt;/p&gt;&lt;p&gt;-----------------------------------------------------------&lt;/p&gt;&lt;p&gt;I have figured out that you CAN do this for n=3,4,5. I THINK the talk said&amp;nbsp; it cannot be done for n=6. If&amp;nbsp; you know or find a proof or disproof then please tell me. I am looking for human-readable proofs, not computer proofs.&amp;nbsp; Similar for higher n.&lt;/p&gt;&lt;p&gt;I have a writeup of the n=3,4,5 cases&amp;nbsp;&lt;a href=&quot;https://www.cs.umd.edu/~gasarch/BLOGPAPERS/pennychess.pdf&quot;&gt;here&lt;/a&gt;&amp;nbsp;(ADDED LATER- I will edit this later in light of the very interesting comments made on this blog entry.)&amp;nbsp;&lt;/p&gt;&lt;p&gt;----------------------------------------------------------------------&lt;/p&gt;&lt;p&gt;With technology and search engines it SHOULD be easier to find out answers to questions then it was in a prior era. And I think it is. But there are times when you are still better off asking&amp;nbsp; someone, or in my case blog about it, to find the answer. Here is hoping it works!&lt;/p&gt;&lt;p&gt;ADDED LATER: Within 30 minutes of posting this one of my readers wrote a program and found tha tyou CAN do it for n=6 and gives the answer. Another commenter pointed to a website with the related quetion of putting as many pawns as you can on an 8x8 board.&lt;/p&gt;&lt;p&gt;ADDED LATER: There are now comments on the blog pointing to the FULL SOLUTION to the problem, which one can find&amp;nbsp;&lt;a href=&quot;https://oscarcunningham.com/670/unique-distancing-problem/&quot;&gt;here&lt;/a&gt;. In summary:&amp;nbsp;&lt;/p&gt;&lt;p&gt;for n=3,...,7&amp;nbsp; there IS a way to put n pennies on a chessboard such that all distances are distinct.&lt;/p&gt;&lt;p&gt;for n=8,...,14 a computer search shows that there is no such way.&lt;/p&gt;&lt;p&gt;for n=15 there is an INTERESTING PROOF that there is no such way (good thing - the computer program had not halted yet. I do not know if it every did.)&amp;nbsp;&lt;/p&gt;&lt;p&gt;for n\ge 16 there is a NICE proof that there IS such way.&amp;nbsp;&lt;/p&gt;&lt;p&gt;I am ECSTATIC!- I wanted to know the answer and now I do and its easy to understand!&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By gasarch&lt;/p&gt;
  </content>
    <author>
      <name>Computational Complexity</name>
      <uri>http://blog.computationalcomplexity.org/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Sleptsov Nets are Turing-complete</title>
    <link href="http://arxiv.org/abs/2306.12440"/>
    <id>http://arxiv.org/abs/2306.12440</id>
    <updated>2023-06-23T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berthomieu_B/0/1/0/all/0/1&quot;&gt;Bernard Berthomieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaitsev_D/0/1/0/all/0/1&quot;&gt;Dmitry A. Zaitsev&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The present paper proves that a Sleptsov net (SN) is Turing-complete, that
considerably improves, with a brief construct, the previous result that a
strong SN is Turing-complete. Remind that, unlike Petri nets, an SN always
fires enabled transitions at their maximal firing multiplicity, as a single
step, leaving for a nondeterministic choice of which fireable transitions to
fire. A strong SN restricts nondeterministic choice to firing only the
transitions having the highest firing multiplicity.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Unitary Complexity and the Uhlmann Transformation Problem</title>
    <link href="http://arxiv.org/abs/2306.13073"/>
    <id>http://arxiv.org/abs/2306.13073</id>
    <updated>2023-06-23T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Bostanci_J/0/1/0/all/0/1&quot;&gt;John Bostanci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Efron_Y/0/1/0/all/0/1&quot;&gt;Yuval Efron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Metger_T/0/1/0/all/0/1&quot;&gt;Tony Metger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Poremba_A/0/1/0/all/0/1&quot;&gt;Alexander Poremba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Qian_L/0/1/0/all/0/1&quot;&gt;Luowen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Yuen_H/0/1/0/all/0/1&quot;&gt;Henry Yuen&lt;/a&gt;&lt;/p&gt;&lt;p&gt;State transformation problems such as compressing quantum information or
breaking quantum commitments are fundamental quantum tasks. However, their
computational difficulty cannot easily be characterized using traditional
complexity theory, which focuses on tasks with classical inputs and outputs.
&lt;/p&gt;
&lt;p&gt;To study the complexity of such state transformation tasks, we introduce a
framework for unitary synthesis problems, including notions of reductions and
unitary complexity classes. We use this framework to study the complexity of
transforming one entangled state into another via local operations. We
formalize this as the Uhlmann Transformation Problem, an algorithmic version of
Uhlmann&#39;s theorem. Then, we prove structural results relating the complexity of
the Uhlmann Transformation Problem, polynomial space quantum computation, and
zero knowledge protocols.
&lt;/p&gt;
&lt;p&gt;The Uhlmann Transformation Problem allows us to characterize the complexity
of a variety of tasks in quantum information processing, including decoding
noisy quantum channels, breaking falsifiable quantum cryptographic assumptions,
implementing optimal prover strategies in quantum interactive proofs, and
decoding the Hawking radiation of black holes. Our framework for unitary
complexity thus provides new avenues for studying the computational complexity
of many natural quantum information processing tasks.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: Geometric Graphs with Unbounded Flip-Width</title>
    <link href="http://arxiv.org/abs/2306.12611"/>
    <id>http://arxiv.org/abs/2306.12611</id>
    <updated>2023-06-23T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eppstein_D/0/1/0/all/0/1&quot;&gt;David Eppstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCarty_R/0/1/0/all/0/1&quot;&gt;Rose McCarty&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We consider the flip-width of geometric graphs, a notion of graph width
recently introduced by Toru\&#39;nczyk. We prove that many different types of
geometric graphs have unbounded flip-width. These include interval graphs,
permutation graphs, circle graphs, intersection graphs of axis-aligned line
segments or axis-aligned unit squares, unit distance graphs, unit disk graphs,
visibility graphs of simple polygons, $\beta$-skeletons, 4-polytopes, rectangle
of influence graphs, and 3d Delaunay triangulations.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Polynomial Logical Zonotopes: A Set Representation for Reachability Analysis of Logical Systems</title>
    <link href="http://arxiv.org/abs/2306.12508"/>
    <id>http://arxiv.org/abs/2306.12508</id>
    <updated>2023-06-23T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alanwar_A/0/1/0/all/0/1&quot;&gt;Amr Alanwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1&quot;&gt;Frank J. Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johansson_K/0/1/0/all/0/1&quot;&gt;Karl H. Johansson&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this paper, we introduce a set representation called polynomial logical
zonotopes for performing exact and computationally efficient reachability
analysis on logical systems. Polynomial logical zonotopes are a generalization
of logical zonotopes, which are able to represent up to 2^n binary vectors
using only n generators. Due to their construction, logical zonotopes are only
able to support exact computations of some logical operations (XOR, NOT, XNOR),
while other operations (AND, NAND, OR, NOR) result in over-approximations. In
order to perform all fundamental logical operations exactly, we formulate a
generalization of logical zonotopes that is constructed by additional dependent
generators and exponent matrices. We prove that through this polynomial-like
construction, we are able to perform all of the fundamental logical operations
(XOR, NOT, XNOR, AND, NAND, OR, NOR) exactly. While we are able to perform all
of the logical operations exactly, this comes with a slight increase in
computational complexity compared to logical zonotopes. We show that we can use
polynomial logical zonotopes to perform exact reachability analysis while
retaining a low computational complexity. To illustrate and showcase the
computational benefits of polynomial logical zonotopes, we present the results
of performing reachability analysis on two use cases: (1) safety verification
of an intersection crossing protocol, (2) and reachability analysis on a
high-dimensional Boolean function. Moreover, to highlight the extensibility of
logical zonotopes, we include an additional use case where we perform a
computationally tractable exhaustive search for the key of a linear-feedback
shift register.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Preprocessing Complexity for Some Graph Problems Parameterized by Structural Parameters</title>
    <link href="http://arxiv.org/abs/2306.12655"/>
    <id>http://arxiv.org/abs/2306.12655</id>
    <updated>2023-06-23T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lafond_M/0/1/0/all/0/1&quot;&gt;Manuel Lafond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1&quot;&gt;Weidong Luo&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Structural graph parameters play an important role in parameterized
complexity, including in kernelization. Notably, vertex cover, neighborhood
diversity, twin-cover, and modular-width have been studied extensively in the
last few years. However, there are many fundamental problems whose
preprocessing complexity is not fully understood under these parameters.
Indeed, the existence of polynomial kernels or polynomial Turing kernels for
famous problems such as Clique, Chromatic Number, and Steiner Tree has only
been established for a subset of structural parameters. In this work, we use
several techniques to obtain a complete preprocessing complexity landscape for
over a dozen of fundamental algorithmic problems.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Memory-Query Tradeoffs for Randomized Convex Optimization</title>
    <link href="http://arxiv.org/abs/2306.12534"/>
    <id>http://arxiv.org/abs/2306.12534</id>
    <updated>2023-06-23T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1&quot;&gt;Binghui Peng&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We show that any randomized first-order algorithm which minimizes a
$d$-dimensional, $1$-Lipschitz convex function over the unit ball must either
use $\Omega(d^{2-\delta})$ bits of memory or make $\Omega(d^{1+\delta/6-o(1)})$
queries, for any constant $\delta\in (0,1)$ and when the precision $\epsilon$
is quasipolynomially small in $d$. Our result implies that cutting plane
methods, which use $\tilde{O}(d^2)$ bits of memory and $\tilde{O}(d)$ queries,
are Pareto-optimal among randomized first-order algorithms, and quadratic
memory is required to achieve optimal query complexity for convex optimization.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: On Differentially Private Sampling from Gaussian and Product Distributions</title>
    <link href="http://arxiv.org/abs/2306.12549"/>
    <id>http://arxiv.org/abs/2306.12549</id>
    <updated>2023-06-23T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghazi_B/0/1/0/all/0/1&quot;&gt;Badih Ghazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1&quot;&gt;Ravi Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manurangsi_P/0/1/0/all/0/1&quot;&gt;Pasin Manurangsi&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Given a dataset of $n$ i.i.d. samples from an unknown distribution $P$, we
consider the problem of generating a sample from a distribution that is close
to $P$ in total variation distance, under the constraint of differential
privacy (DP). We study the problem when $P$ is a multi-dimensional Gaussian
distribution, under different assumptions on the information available to the
DP mechanism: known covariance, unknown bounded covariance, and unknown
unbounded covariance. We present new DP sampling algorithms, and show that they
achieve near-optimal sample complexity in the first two settings. Moreover,
when $P$ is a product distribution on the binary hypercube, we obtain a pure-DP
algorithm whereas only an approximate-DP algorithm (with slightly worse sample
complexity) was previously known.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


</feed>
