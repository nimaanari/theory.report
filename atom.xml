<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>Theory of Computing Report</title>
  <link rel="self" href=""/>
  <link href=""/>
  <id></id>
  <updated></updated>
  <generator uri="http://feedreader.github.io/">Pluto 1.6.2 on Ruby 3.0.5 (2022-11-24) [x86_64-linux]</generator>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Finite model theory for pseudovarieties and universal algebra: preservation, definability and complexity</title>
    <link href="http://arxiv.org/abs/2212.02653"/>
    <id>http://arxiv.org/abs/2212.02653</id>
    <updated>2022-12-07T01:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ham_L/0/1/0/all/0/1&quot;&gt;Lucy Ham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jackson_M/0/1/0/all/0/1&quot;&gt;Marcel Jackson&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We explore new interactions between finite model theory and a number of
classical streams of universal algebra and semigroup theory. After refocussing
some finite model theoretic tools in universal algebraic context, we present a
number of results. A key result is an example of a finite algebra whose variety
is not finitely axiomatisable in first order logic, but which has first order
definable finite membership problem. This algebra witnesses the simultaneous
failure of the {\L}os-Tarski Theorem, the SP-preservation theorem and
Birkhoff&#39;s HSP-preservation theorem at the finite level as well as providing a
negative solution to the first order formulation of the long-standing Eilenberg
Sch\&quot;utzenberger problem. The example also shows that a pseudovariety without
any finite pseudo-identity basis may be finitely axiomatisable in first order
logic. Other results include the undecidability of deciding first order
definability of the pseudovariety of a finite algebra and a mapping from any
fixed template constraint satisfaction problem to a first order equivalent
variety membership problem, thereby providing examples of variety membership
problems complete in each of the classes $\texttt{L}$, $\texttt{NL}$,
$\texttt{Mod}_p(\texttt{L})$, $\texttt{P}$ (provided they are nonempty), and
infinitely many others (depending on complexity-theoretic assumptions).
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: On the Size of Chromatic Delaunay Mosaics</title>
    <link href="http://arxiv.org/abs/2212.03121"/>
    <id>http://arxiv.org/abs/2212.03121</id>
    <updated>2022-12-07T01:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Biswas_R/0/1/0/all/0/1&quot;&gt;Ranita Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Montesano_S/0/1/0/all/0/1&quot;&gt;Sebastiano Cultrera di Montesano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Draganov_O/0/1/0/all/0/1&quot;&gt;Ond&amp;#x159;ej Draganov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Edelsbrunner_H/0/1/0/all/0/1&quot;&gt;Herbert Edelsbrunner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Saghafian_M/0/1/0/all/0/1&quot;&gt;Morteza Saghafian&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Given a locally finite set $A \subseteq \mathbb{R}^d$ and a coloring $\chi
\colon A \to \{0,1,\ldots,s\}$, we introduce the chromatic Delaunay mosaic of
$\chi$, which is a Delaunay mosaic in $\mathbb{R}^{s+d}$ that represents how
points of different colors mingle. Our main results are bounds on the size of
the chromatic Delaunay mosaic, in which we assume that $d$ and $s$ are
constants. For example, if $A$ is finite with $n = \#{A}$, and the coloring is
random, then the chromatic Delaunay mosaic has $O(n^{\lceil{d/2}\rceil})$ cells
in expectation. In contrast, for Delone sets and Poisson point processes in
$\mathbb{R}^d$, the expected number of cells within a closed ball is only a
constant times the number of points in this ball. Furthermore, in
$\mathbb{R}^2$ all colorings of a dense set of $n$ points have chromatic
Delaunay mosaics of size $O(n)$. This encourages the use of chromatic Delaunay
mosaics in applications.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: Persistent Homology of Chromatic Alpha Complexes</title>
    <link href="http://arxiv.org/abs/2212.03128"/>
    <id>http://arxiv.org/abs/2212.03128</id>
    <updated>2022-12-07T01:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Montesano_S/0/1/0/all/0/1&quot;&gt;Sebastiano Cultrera di Montesano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Draganov_O/0/1/0/all/0/1&quot;&gt;Ond&amp;#x159;ej Draganov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Edelsbrunner_H/0/1/0/all/0/1&quot;&gt;Herbert Edelsbrunner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Saghafian_M/0/1/0/all/0/1&quot;&gt;Morteza Saghafian&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Motivated by applications in medical sciences, we study finite chromatic sets
in Euclidean space from a topological perspective. Based on persistent homology
for images, kernels and cokernels, we design provably stable homological
quantifiers that describe the geometric micro- and macro-structure of how the
color classes mingle. These can be efficiently computed using chromatic
variants of Delaunay mosaics and Alpha complexes.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Improved Approximation Schemes for (Un-)Bounded Subset-Sum and Partition</title>
    <link href="http://arxiv.org/abs/2212.02883"/>
    <id>http://arxiv.org/abs/2212.02883</id>
    <updated>2022-12-07T01:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lin Chen&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We consider the SUBSET SUM problem and its important variants in this paper.
In the SUBSET SUM problem, a (multi-)set $X$ of $n$ positive numbers and a
target number $t$ are given, and the task is to find a subset of $X$ with the
maximal sum that does not exceed $t$. It is well known that this problem is
NP-hard and admits fully polynomial-time approximation schemes (FPTASs). In
recent years, it has been shown that there does not exist an FPTAS of running
time $\tilde\OO( 1/\epsilon^{2-\delta})$ for arbitrary small $\delta&amp;gt;0$
assuming ($\min$,+)-convolution conjecture~\cite{bringmann2021fine}. However,
the lower bound can be bypassed if we relax the constraint such that the task
is to find a subset of $X$ that can slightly exceed the threshold $t$ by
$\epsilon$ times, and the sum of numbers within the subset is at least
$1-\tilde\OO(\epsilon)$ times the optimal objective value that respects the
constraint. Approximation schemes that may violate the constraint are also
known as weak approximation schemes. For the SUBSET SUM problem, there is a
randomized weak approximation scheme running in time $\tilde\OO(n+
1/\epsilon^{5/3})$ [Mucha et al.&#39;19]. For the special case where the target $t$
is half of the summation of all input numbers, weak approximation schemes are
equivalent to approximation schemes that do not violate the constraint, and the
best-known algorithm runs in $\tilde\OO(n+1/\epsilon^{{3}/{2}})$ time
[Bringmann and Nakos&#39;21].
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: A Strongly Polynomial Algorithm for Approximate Forster Transforms and its Application to Halfspace Learning</title>
    <link href="http://arxiv.org/abs/2212.03008"/>
    <id>http://arxiv.org/abs/2212.03008</id>
    <updated>2022-12-07T01:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1&quot;&gt;Ilias Diakonikolas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1&quot;&gt;Christos Tzamos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1&quot;&gt;Daniel M. Kane&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The Forster transform is a method of regularizing a dataset by placing it in
{\em radial isotropic position} while maintaining some of its essential
properties. Forster transforms have played a key role in a diverse range of
settings spanning computer science and functional analysis. Prior work had
given {\em weakly} polynomial time algorithms for computing Forster transforms,
when they exist. Our main result is the first {\em strongly polynomial time}
algorithm to compute an approximate Forster transform of a given dataset or
certify that no such transformation exists. By leveraging our strongly
polynomial Forster algorithm, we obtain the first strongly polynomial time
algorithm for {\em distribution-free} PAC learning of halfspaces. This learning
result is surprising because {\em proper} PAC learning of halfspaces is {\em
equivalent} to linear programming. Our learning approach extends to give a
strongly polynomial halfspace learner in the presence of random classification
noise and, more generally, Massart noise.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Inapproximability of counting independent sets in linear hypergraphs</title>
    <link href="http://arxiv.org/abs/2212.03072"/>
    <id>http://arxiv.org/abs/2212.03072</id>
    <updated>2022-12-07T01:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1&quot;&gt;Guoliang Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaheng Wang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;It is shown in this note that approximating the number of independent sets in
a $k$-uniform linear hypergraph with maximum degree at most $\Delta$ is NP-hard
if $\Delta\geq 5\cdot 2^{k-1}+1$. This confirms that for the relevant sampling
and approximate counting problems, the regimes on the maximum degree where the
state-of-the-art algorithms work are tight, up to some small factors. These
algorithms include: the approximate sampler and randomised approximation scheme
by Hermon, Sly and Zhang (2019), the perfect sampler by Qiu, Wang and Zhang
(2022), and the deterministic approximation scheme by Feng, Guo, Wang, Wang and
Yin (2022).
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Higher Lower Bounds for Sparse Oblivious Subspace Embeddings</title>
    <link href="http://arxiv.org/abs/2212.02913"/>
    <id>http://arxiv.org/abs/2212.02913</id>
    <updated>2022-12-07T01:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingmou Liu&lt;/a&gt;&lt;/p&gt;&lt;p&gt;An oblivious subspace embedding (OSE), characterized by parameters
$m,n,d,\epsilon,\delta$, is a random matrix $\Pi\in \mathbb{R}^{m\times n}$
such that for any $d$-dimensional subspace $T\subseteq \mathbb{R}^n$,
$\Pr_\Pi[\forall x\in T, (1-\epsilon)\|x\|_2 \leq \|\Pi x\|_2\leq
(1+\epsilon)\|x\|_2] \geq 1-\delta$. When an OSE has $1/(9\epsilon)$ nonzero
entries in each column, we show it must hold that $m =
\Omega(d^2/\epsilon^{1-O(\delta)})$, which is the first lower bound with
multiplicative factors of $d^2$ and $1/\epsilon$, improving on the previous
$\Omega(\epsilon^{O(\delta)}d^2)$ lower bound due to Li and Liu (PODS 2022).
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Improved Algebraic Degeneracy Testing</title>
    <link href="http://arxiv.org/abs/2212.03030"/>
    <id>http://arxiv.org/abs/2212.03030</id>
    <updated>2022-12-07T01:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardinal_J/0/1/0/all/0/1&quot;&gt;Jean Cardinal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharir_M/0/1/0/all/0/1&quot;&gt;Micha Sharir&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In the classical linear degeneracy testing problem, we are given $n$ real
numbers and a $k$-variate linear polynomial $F$, for some constant $k$, and
have to determine whether there exist $k$ numbers $a_1,\ldots,a_k$ from the set
such that $F(a_1,\ldots,a_k) = 0$. We consider a generalization of this problem
in which $F$ is an arbitrary constant-degree polynomial, we are given $k$ sets
of $n$ numbers, and have to determine whether there exist a $k$-tuple of
numbers, one in each set, on which $F$ vanishes. We give the first improvement
over the na\&quot;ive $O^*(n^{k-1})$ algorithm for this problem (where the
$O^*(\cdot)$ notation omits subpolynomial factors).
&lt;/p&gt;
&lt;p&gt;We show that the problem can be solved in time $O^*\left( n^{k - 2 + \frac
4{k+2}}\right)$ for even $k$ and in time $O^*\left( n^{k - 2 +
\frac{4k-8}{k^2-5}}\right)$ for odd $k$ in the real RAM model of computation.
We also prove that for $k=4$, the problem can be solved in time
$O^*(n^{2.625})$ in the algebraic decision tree model, and for $k=5$ it can be
solved in time $O^*(n^{3.56})$ in the same model, both improving on the above
uniform bounds.
&lt;/p&gt;
&lt;p&gt;All our results rely on an algebraic generalization of the standard
meet-in-the-middle algorithm for $k$-SUM, powered by recent algorithmic
advances in the polynomial method for semi-algebraic range searching. In fact,
our main technical result is much more broadly applicable, as it provides a
general tool for detecting incidences and other interactions between points and
algebraic surfaces in any dimension. In particular, it yields an efficient
algorithm for a general, algebraic version of Hopcroft&#39;s point-line incidence
detection problem in any dimension.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Robustness of Quantum Algorithms for Nonconvex Optimization</title>
    <link href="http://arxiv.org/abs/2212.02548"/>
    <id>http://arxiv.org/abs/2212.02548</id>
    <updated>2022-12-07T01:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Gong_W/0/1/0/all/0/1&quot;&gt;Weiyuan Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tongyang Li&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Recent results suggest that quantum computers possess the potential to speed
up nonconvex optimization problems. However, a crucial factor for the
implementation of quantum optimization algorithms is their robustness against
experimental and statistical noises. In this paper, we systematically study
quantum algorithms for finding an $\epsilon$-approximate second-order
stationary point ($\epsilon$-SOSP) of a $d$-dimensional nonconvex function, a
fundamental problem in nonconvex optimization, with noisy zeroth- or
first-order oracles as inputs. We first prove that, up to noise of
$O(\epsilon^{10}/d^5)$, accelerated perturbed gradient descent with quantum
gradient estimation takes $O(\log d/\epsilon^{1.75})$ quantum queries to find
an $\epsilon$-SOSP. We then prove that perturbed gradient descent is robust to
the noise of $O(\epsilon^6/d^4)$ and $O(\epsilon/d^{0.5+\zeta})$ for $\zeta&amp;gt;0$
on the zeroth- and first-order oracles, respectively, which provides a quantum
algorithm with poly-logarithmic query complexity. We then propose a stochastic
gradient descent algorithm using quantum mean estimation on the Gaussian
smoothing of noisy oracles, which is robust to $O(\epsilon^{1.5}/d)$ and
$O(\epsilon/\sqrt{d})$ noise on the zeroth- and first-order oracles,
respectively. The quantum algorithm takes $O(d^{2.5}/\epsilon^{3.5})$ and
$O(d^2/\epsilon^3)$ queries to the two oracles, giving a polynomial speedup
over the classical counterparts. Moreover, we characterize the domains where
quantum algorithms can find an $\epsilon$-SOSP with poly-logarithmic,
polynomial, or exponential number of queries in $d$, or the problem is
information-theoretically unsolvable even by an infinite number of queries. In
addition, we prove an $\Omega(\epsilon^{-12/7})$ lower bound in $\epsilon$ for
any randomized classical and quantum algorithm to find an $\epsilon$-SOSP using
either noisy zeroth- or first-order oracles.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Collabs: Composable Collaborative Data Structures</title>
    <link href="http://arxiv.org/abs/2212.02618"/>
    <id>http://arxiv.org/abs/2212.02618</id>
    <updated>2022-12-07T01:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weidner_M/0/1/0/all/0/1&quot;&gt;Matthew Weidner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_H/0/1/0/all/0/1&quot;&gt;Heather Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1&quot;&gt;Huairui Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kjaer_M/0/1/0/all/0/1&quot;&gt;Maxime Kjaer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pradeep_R/0/1/0/all/0/1&quot;&gt;Ria Pradeep&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geordie_B/0/1/0/all/0/1&quot;&gt;Benito Geordie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meiklejohn_C/0/1/0/all/0/1&quot;&gt;Christopher Meiklejohn&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Replicated data types (RDTs), such as Conflict-free Replicated Data Types
(CRDTs), provide an abstraction for reasoning about replication and consistency
in distributed systems. To make them as useful as ordinary, local data
structures, RDTs need to be both modular and composable, so that programmers
can create new app-specific RDTs by composing existing ones. However, no
existing RDT libraries combine these properties; either they use monolithic
architectures that rule out new RDTs or they do not support composition
techniques.
&lt;/p&gt;
&lt;p&gt;In this work, we introduce the Collab (collaborative data structure), a novel
abstraction for modular and composable RDTs. We also describe the collabs
library, an open-source TypeScript library that we built around this
abstraction. Our library supports arbitrary programmer-added RDTs and includes
composition techniques that make them easier to implement. This allows
programmers to work at a higher level of abstraction: custom RDTs for arbitrary
concepts in their application, instead of just a fixed menu of generic RDTs. It
also allows programmers to extend the library with new RDT algorithms as they
are published, instead of waiting for the library to implement them. Our
library includes a collection of built-in op-based CRDT implementations,
including several that were not previously implemented. To demonstrate the
library, we built numerous apps on top of it, including decentralized
collaborative apps that can be deployed from a static web page. Benchmarks show
that its CRDTs have performance comparable to state-of-the-art CRDT libraries
for web apps, and that unlike existing libraries, it can support 100
simultaneous users with low latency in a geo-distributed collaborative app.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Stars: Tera-Scale Graph Building for Clustering and Graph Learning</title>
    <link href="http://arxiv.org/abs/2212.02635"/>
    <id>http://arxiv.org/abs/2212.02635</id>
    <updated>2022-12-07T01:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carey_C/0/1/0/all/0/1&quot;&gt;CJ Carey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halcrow_J/0/1/0/all/0/1&quot;&gt;Jonathan Halcrow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayaram_R/0/1/0/all/0/1&quot;&gt;Rajesh Jayaram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1&quot;&gt;Vahab Mirrokni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schudy_W/0/1/0/all/0/1&quot;&gt;Warren Schudy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_P/0/1/0/all/0/1&quot;&gt;Peilin Zhong&lt;/a&gt;&lt;/p&gt;&lt;p&gt;A fundamental procedure in the analysis of massive datasets is the
construction of similarity graphs. Such graphs play a key role for many
downstream tasks, including clustering, classification, graph learning, and
nearest neighbor search. For these tasks, it is critical to build graphs which
are sparse yet still representative of the underlying data. The benefits of
sparsity are twofold: firstly, constructing dense graphs is infeasible in
practice for large datasets, and secondly, the runtime of downstream tasks is
directly influenced by the sparsity of the similarity graph. In this work, we
present $\textit{Stars}$: a highly scalable method for building extremely
sparse graphs via two-hop spanners, which are graphs where similar points are
connected by a path of length at most two. Stars can construct two-hop spanners
with significantly fewer similarity comparisons, which are a major bottleneck
for learning based models where comparisons are expensive to evaluate.
Theoretically, we demonstrate that Stars builds a graph in nearly-linear time,
where approximate nearest neighbors are contained within two-hop neighborhoods.
In practice, we have deployed Stars for multiple data sets allowing for graph
building at the $\textit{Tera-Scale}$, i.e., for graphs with tens of trillions
of edges. We evaluate the performance of Stars for clustering and graph
learning, and demonstrate 10~1000-fold improvements in pairwise similarity
comparisons compared to different baselines, and 2~10-fold improvement in
running time without quality loss.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Low Power Mesh Algorithms for Image Problems</title>
    <link href="http://arxiv.org/abs/2212.02640"/>
    <id>http://arxiv.org/abs/2212.02640</id>
    <updated>2022-12-07T01:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stout_Q/0/1/0/all/0/1&quot;&gt;Quentin Stout&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We analyze a physically motivated fine-grained mesh-connected computer model,
assuming that a word of information takes a fixed area and that it takes unit
time and unit energy to move a word unit distance. This is a representation of
computing on a chip with myriad tiny processors arranged as a mesh. While most
mesh algorithms assume all processors are active at all times, we give
algorithms that have only a few processors on at any one time, which reduces
the power required. We apply this approach to basic problems involving images,
showing that there can be dramatic reductions in the peak power with only
small, if any, changes in the time required. We also show that these algorithms
give a more efficient way to utilize power when more power is available.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Online Min-Max Paging</title>
    <link href="http://arxiv.org/abs/2212.03016"/>
    <id>http://arxiv.org/abs/2212.03016</id>
    <updated>2022-12-07T01:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiplunkar_A/0/1/0/all/0/1&quot;&gt;Ashish Chiplunkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henzinger_M/0/1/0/all/0/1&quot;&gt;Monika Henzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kale_S/0/1/0/all/0/1&quot;&gt;Sagar Sudhir Kale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Votsch_M/0/1/0/all/0/1&quot;&gt;Maximilian V&amp;#xf6;tsch&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Motivated by fairness requirements in communication networks, we introduce a
natural variant of the online paging problem, called \textit{min-max} paging,
where the objective is to minimize the maximum number of faults on any page.
While the classical paging problem, whose objective is to minimize the total
number of faults, admits $k$-competitive deterministic and $O(\log
k)$-competitive randomized algorithms, we show that min-max paging does not
admit a $c(k)$-competitive algorithm for any function $c$. Specifically, we
prove that the randomized competitive ratio of min-max paging is
$\Omega(\log(n))$ and its deterministic competitive ratio is
$\Omega(k\log(n)/\log(k))$, where $n$ is the total number of pages ever
requested.
&lt;/p&gt;
&lt;p&gt;We design a fractional algorithm for paging with a more general objective --
minimize the value of an $n$-variate differentiable convex function applied to
the vector of the number of faults on each page. This gives an
$O(\log(n)\log(k))$-competitive fractional algorithm for min-max paging. We
show how to round such a fractional algorithm with at most a $k$ factor loss in
the competitive ratio, resulting in a deterministic
$O(k\log(n)\log(k))$-competitive algorithm for min-max paging. This matches our
lower bound modulo a $\mathrm{poly}(\log(k))$ factor. We also give a randomized
rounding algorithm that results in a $O(\log^2 n \log k)$-competitive
algorithm.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Pareto Optimal Compression of Genomic Dictionaries, with or without Random Access in Main Memory</title>
    <link href="http://arxiv.org/abs/2212.03067"/>
    <id>http://arxiv.org/abs/2212.03067</id>
    <updated>2022-12-07T01:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giancarlo_R/0/1/0/all/0/1&quot;&gt;Raffaele Giancarlo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grimaudo_G/0/1/0/all/0/1&quot;&gt;Gennaro Grimaudo&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Motivation: A Genomic Dictionary, i.e., the set of the k-mers appearing in a
genome, is a fundamental source of genomic information: its collection is the
first step in strategic computational methods ranging from assembly to sequence
comparison and phylogeny. Unfortunately, it is costly to store. This motivates
some recent studies regarding the compression of those k-mer sets. However,
such an area does not have the maturity of genomic compression, lacking an
homogeneous and methodologically sound experimental foundation that allows to
fairly compare the relative merits of the available solutions, and that takes
into account also the rich choices of compression methods that can be used.
&lt;/p&gt;
&lt;p&gt;Results: We provide such a foundation here, supporting it with an extensive
set of experiments that use reference datasets and a carefully selected set of
representative data compressors. Our results highlight the spectrum of
compressor choices one has in terms of Pareto Optimality of compression vs.
post-processing, this latter being important when the Dictionary needs to be
decompressed many times. In addition to the useful indications, not available
elsewhere, that this study offers to the researchers interested in storing
k-mer dictionaries in compressed form, a software system that can be readily
used to explore the Pareto Optimal solutions available r a given Dictionary is
also provided.
&lt;/p&gt;
&lt;p&gt;Availability: The software system is available at
https://github.com/GenGrim76/Pareto-Optimal-GDC, together with user manuals and
installation instructions.
&lt;/p&gt;
&lt;p&gt;Contact: raffaele.giancarlo@unipa.it
&lt;/p&gt;
&lt;p&gt;Supplementary information: Additional data are available in the Supplementary
Material.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Extending Snow&#39;s algorithm for computations in the finite Weyl groups</title>
    <link href="http://arxiv.org/abs/2212.03156"/>
    <id>http://arxiv.org/abs/2212.03156</id>
    <updated>2022-12-07T01:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Stekolshchik_R/0/1/0/all/0/1&quot;&gt;Rafael Stekolshchik&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In 1990, D.Snow proposed an effective algorithm for computing the orbits of
finite Weyl groups. Snow&#39;s algorithm is designed for computation of weights,
$W$-orbits and elements of the Weyl group. An extension of Snow&#39;s algorithm is
proposed, which allows to find pairs of mutually inverse elements together with
the calculation of $W$-orbits in the same runtime cycle. This simplifies the
calculation of conjugacy classes in the Weyl group. As an example, the complete
list of elements of the Weyl group $W(D_4)$ obtained using the extended Snow&#39;s
algorithm is given. The elements of $W(D_4)$ are specified in two ways: as
reduced expressions and as matrices of the faithful representation. We present
a partition of this group into conjugacy classes with elements specified as
reduced expressions. Various forms are given for representatives of the
conjugacy classes of $W(D_4)$: using Carter diagrams, using reduced expressions
and using signed cycle-types. In the appendix, we provide an implementation of
the algorithm in Python.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">ECCC Papers: TR22-177 |  Quantum Worst-Case to Average-Case Reductions for All Linear Problems | 

	Vahid Reza Asadi, 

	Alexander Golovnev, 

	Tom Gur, 

	Igor Shinkar, 

	Sathyawageeswar Subramanian</title>
    <link href="https://eccc.weizmann.ac.il/report/2022/177"/>
    <id>https://eccc.weizmann.ac.il/report/2022/177</id>
    <updated>2022-12-06T22:35:48+00:00</updated>
    <content type="html" xml:lang="en">
    We study the problem of designing worst-case to average-case reductions for quantum algorithms. For all linear problems, we provide an explicit and efficient transformation of quantum algorithms that are only correct on a small (even sub-constant) fraction of their inputs into ones that are correct on all inputs. This stands in contrast to the classical setting, where such results are only known for a small number of specific problems or restricted computational models. En route, we obtain a tight $\Omega(n^2)$ lower bound on the average-case quantum query complexity of the Matrix-Vector Multiplication problem. 

Our techniques strengthen and generalise the recently introduced additive combinatorics framework for classical worst-case to average-case reductions (STOC 2022) to the quantum setting. We rely on quantum singular value transformations to construct quantum algorithms for linear verification in superposition and learning Bogolyubov subspaces from noisy quantum oracles. We use these tools to prove a quantum local correction lemma, which lies at the heart of our reductions, based on a noise-robust probabilistic generalisation of Bogolyubov&amp;#39;s lemma from additive combinatorics.
  </content>
    <author>
      <name>ECCC Papers</name>
      <uri>https://eccc.weizmann.ac.il/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Theory Dish: Stanford Blog: 2023 Motwani Postdoc Announced</title>
    <link href="https://theorydish.blog/2022/12/05/2023-motwani-postdoc-announced/"/>
    <id>http://theorydish.blog/?p=2859</id>
    <updated>2022-12-06T02:29:30+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;The theory group at Stanford invites applications for the Motwani postdoctoral fellowship in theoretical computer science. Information and application instructions below. Applications will be accepted until the positions are filled, but review of applicants will begin after Dec 15.&lt;/p&gt;



&lt;p&gt;Website:&amp;nbsp;&lt;a href=&quot;https://academicjobsonline.org/ajo/jobs/23929&quot;&gt;https://academicjobsonline.org/ajo/jobs/23929&lt;/a&gt;&lt;br&gt;Email: theory.stanford@gmail.com&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Omer Reingold&lt;/p&gt;
  </content>
    <author>
      <name>Theory Dish: Stanford Blog</name>
      <uri>https://theorydish.blog</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">CCI: jobs: Postdoc at George Mason University (apply by December 31, 2022)</title>
    <link href="https://cstheory-jobs.org/2022/12/05/postdoc-at-george-mason-university-apply-by-december-31-2022/"/>
    <id>http://cstheory-jobs.org/2022/12/05/postdoc-at-george-mason-university-apply-by-december-31-2022/</id>
    <updated>2022-12-05T21:30:49+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;Postdoc in theoretical foundations of Algorithms and AI at George Mason University. Starting Fall’23 and hosted by Grigory Yaroslavtsev (&lt;a href=&quot;http://grigory.ai&quot;&gt;http://grigory.ai&lt;/a&gt;). GMU is located in the Washington, DC metro area, is the largest public university in Virginia and has one of the fastest growing CS departments in the U.S.&lt;/p&gt;
&lt;p&gt;To apply email CV and research statement. Reference letters will be requested.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;http://grigory.ai&quot;&gt;http://grigory.ai&lt;/a&gt;&lt;br /&gt;
Email: grigory@grigory.us&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </content>
    <author>
      <name>CCI: jobs</name>
      <uri>https://cstheory-jobs.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Computational Complexity: The Future of Education is Personal</title>
    <link href="http://blog.computationalcomplexity.org/2022/12/the-future-of-education-is-personal.html"/>
    <id>tag:blogger.com,1999:blog-3722233.post-2041922047028548572</id>
    <updated>2022-12-05T13:53:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbsNuEriIphlZE_oc0Xj44gFxfBp1LCCnFM4ps7FL2KtdUjta77CAddfqgxJQa24OCZ2mv2gqaxRisUhVgvhSj4QIl9Cov-phWAb2wwyIy-rJmHdcsIPcv-60JGncHmMJAIbgi8KwGHExESHI_12_H5TGV2woNv6dn38DtzD2vRo9wXF635Q/s1024/DALL%C2%B7E%202022-12-03%2014.23.49%20-%20An%20oil%20painting%20of%20a%20Darth%20Vader%20tutoring%20a%20human%20student.png&quot; style=&quot;clear: right; float: right; margin-bottom: 1em; margin-left: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1024&quot; data-original-width=&quot;1024&quot; height=&quot;320&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbsNuEriIphlZE_oc0Xj44gFxfBp1LCCnFM4ps7FL2KtdUjta77CAddfqgxJQa24OCZ2mv2gqaxRisUhVgvhSj4QIl9Cov-phWAb2wwyIy-rJmHdcsIPcv-60JGncHmMJAIbgi8KwGHExESHI_12_H5TGV2woNv6dn38DtzD2vRo9wXF635Q/s320/DALL%C2%B7E%202022-12-03%2014.23.49%20-%20An%20oil%20painting%20of%20a%20Darth%20Vader%20tutoring%20a%20human%20student.png&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;With all the excitement about &lt;a href=&quot;https://chat.openai.com/&quot;&gt;ChatGPT&lt;/a&gt;, how will machine learning disrupt education, say five to ten years down the road?&lt;/p&gt;&lt;p&gt;My guess: individualized tutors. Imagine a tutor working with you teaching you important concepts, walking you through examples, answering your questions, going at your own pace, like the&amp;nbsp;&lt;a href=&quot;https://www.new.ox.ac.uk/tutorial-system&quot;&gt;Oxford system&lt;/a&gt;. The Oxford tutor system doesn&#39;t scale well, or at least it wouldn&#39;t if we have human tutors. But we can scale using machine learning and we&#39;re not far away from being able to do so. Such tutors will be infinitely patient, have full knowledge of all written material, speak in any language with any voice and personality.&lt;/p&gt;&lt;p&gt;You can &quot;meet&quot; with your tutor in many different ways, from a deep fake video chat or with augmented or virtual reality to have a tutor in the room with you, or perhaps a physical robot, neural implant or something we haven&#39;t even though of yet. In poorer countries you can get tutored with something as simple as text messages on a cell phone.&amp;nbsp;&lt;/p&gt;&lt;p&gt;A tutor can take on any form. You could get tutored by fictional characters such as Yoda, Darth Vader or &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Magic_School_Bus_(TV_series)&quot;&gt;Miss Frizzle&lt;/a&gt;. ML can capture the personality of real people--imaging a course about Kurt Vonnegut taught by the author, government from Henry Kissinger or a course in quantum computing from your own personal Scott Aaronson. But most importantly you can have a tutor who looks and sounds like you, with your own language, gender, race and ethnicity.&lt;/p&gt;&lt;p&gt;Somehow we&#39;ll have to find ways to include the social aspects such as working in groups, socializing, playing sports and living together. But that one-one-one teaching experience that most of us cannot afford&amp;nbsp; today will be cheaply available tomorrow.&lt;/p&gt;&lt;p&gt;And what will this all mean for teachers, professors and universities? A good question for future blog posts.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By Lance Fortnow&lt;/p&gt;
  </content>
    <author>
      <name>Computational Complexity</name>
      <uri>http://blog.computationalcomplexity.org/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Turing&#39;s Invisible Hand: 2023 SIGecom Test of Time Award — Call for Nominations</title>
    <link href="https://agtb.wordpress.com/2022/12/05/2023-sigecom-test-of-time-award-call-for-nominations/"/>
    <id>http://agtb.wordpress.com/?p=3548</id>
    <updated>2022-12-05T09:16:02+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;The SIGecom Test of Time Award recognizes the author or authors of an influential paper or series of papers published between ten and twenty-five years ago that has significantly impacted research or applications exemplifying the interplay of economics and computation.&lt;/p&gt;



&lt;p&gt;The 2023 SIGecom Test of Time Award will be given for papers published no earlier than 1998 and no later than 2013. Nominations are due by&amp;nbsp;&lt;em&gt;February 28th, 2023&lt;/em&gt;&amp;nbsp;(Anywhere on Earth), and must be made&amp;nbsp;together with the endorsement letters by submission using&amp;nbsp;&lt;a target=&quot;_blank&quot; href=&quot;https://docs.google.com/forms/d/e/1FAIpQLSdm6Hk4uYDi45rvQxHj0dQasjyNMD3XBbS72SK7MPnypWTMmQ/viewform&quot; rel=&quot;noreferrer noopener&quot;&gt;this form&lt;/a&gt;. Any member of SIGecom may submit a nomination. Self-nomination is not allowed.&lt;/p&gt;



&lt;p&gt;More details regarding the nomination procedure can be found&amp;nbsp;&lt;a href=&quot;https://www.sigecom.org/award-tot-details.html&quot;&gt;here&lt;/a&gt;.&amp;nbsp; The list of winners from previous years can be found&amp;nbsp;&lt;a href=&quot;https://www.sigecom.org/award-tot.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;



&lt;p&gt;&lt;/p&gt;



&lt;p class=&quot;has-text-align-left&quot;&gt;&lt;strong&gt;The 2023 Test of Time Award Committee&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Alvin Roth&lt;/strong&gt;, Stanford University&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Moshe Tennenholtz&lt;/strong&gt;, The Technion&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Noam Nisan (chair)&lt;/strong&gt;, The Hebrew University of Jerusalem&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Noam Nisan&lt;/p&gt;
  </content>
    <author>
      <name>Turing's Invisible Hand</name>
      <uri>https://agtb.wordpress.com</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">CCI: jobs: Motwani Postdoctoral Fellowship at Stanford Computer Science (apply by December 15, 2022)</title>
    <link href="https://cstheory-jobs.org/2022/12/05/motwani-postdoctoral-fellowship-at-stanford-computer-science-apply-by-december-15-2022/"/>
    <id>http://cstheory-jobs.org/2022/12/05/motwani-postdoctoral-fellowship-at-stanford-computer-science-apply-by-december-15-2022/</id>
    <updated>2022-12-05T08:32:41+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;The theory group at Stanford invites applications for the Motwani postdoctoral fellowship in theoretical computer science. Information and application instructions below. Applications will be accepted until the positions are filled, but review of applicants will begin after Dec 15.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://academicjobsonline.org/ajo/jobs/23929&quot;&gt;https://academicjobsonline.org/ajo/jobs/23929&lt;/a&gt;&lt;br /&gt;
Email: theory.stanford@gmail.com&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </content>
    <author>
      <name>CCI: jobs</name>
      <uri>https://cstheory-jobs.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">ECCC Papers: TR22-176 |  The power of the Binary Value Principle | 

	Yaroslav Alekseev, 

	Edward Hirsch</title>
    <link href="https://eccc.weizmann.ac.il/report/2022/176"/>
    <id>https://eccc.weizmann.ac.il/report/2022/176</id>
    <updated>2022-12-04T04:21:35+00:00</updated>
    <content type="html" xml:lang="en">
    The (extended) Binary Value Principle (eBVP, the equation $\sum x_i 2^{i-1} = -k$ for $k &amp;gt; 0$
and in the presence of $x_i^2=x_i$) has received a lot of attention recently, several lower
bounds have been proved for it [Alekseev et al 20, Alekseev 21, Part and Tzameret 21]. 
Also it has been shown [Alekseev et al 20] that the 
probabilistically verifiable Ideal Proof System (IPS) [Grochow and Pitassi 18] together with eBVP
polynomially simulates a similar semialgebraic proof system. In this paper we consider
Polynomial Calculus with the algebraic version of Tseitin’s extension rule (Ext-PC). Contrary
to IPS, this is a Cook--Reckhow proof system. We show that in this context eBVP still allows
to simulate similar semialgebraic systems. We also prove that it allows to simulate the
Square Root Rule [Grigoriev and Hirsch 03], which is in sharp contrast with the result of [Alekseev 21] that shows
an exponential lower bound on the size of Ext-PC derivations of the Binary Value Principle
from its square. On the other hand, we demonstrate that eBVP probably does not help in
proving exponential lower bounds for Boolean formulas: we show that an Ext-PC (even with
the Square Root Rule) derivation of any unsatisfiable Boolean formula in CNF from eBVP
must be of exponential size.
  </content>
    <author>
      <name>ECCC Papers</name>
      <uri>https://eccc.weizmann.ac.il/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">ECCC Papers: TR22-175 |  Derandomization Under Different Resource Constraints | 

	Samuel Epstein</title>
    <link href="https://eccc.weizmann.ac.il/report/2022/175"/>
    <id>https://eccc.weizmann.ac.il/report/2022/175</id>
    <updated>2022-12-04T04:19:03+00:00</updated>
    <content type="html" xml:lang="en">
    We provide another proof to the EL Theorem. We show the tradeoff between compressibility of codebooks and their communication capacity. A resource bounded version of the EL Theorem is proven. This is used to prove three instances of resource bounded derandomization.
  </content>
    <author>
      <name>ECCC Papers</name>
      <uri>https://eccc.weizmann.ac.il/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">ECCC Papers: TR22-174 |  Noisy Radio Network Lower Bounds Via Noiseless Beeping Lower Bounds | 

	Raghuvansh Saxena, 

	Gillat Kol, 

	Klim Efremenko, 

	Dmitry Paramonov</title>
    <link href="https://eccc.weizmann.ac.il/report/2022/174"/>
    <id>https://eccc.weizmann.ac.il/report/2022/174</id>
    <updated>2022-12-04T04:13:34+00:00</updated>
    <content type="html" xml:lang="en">
    Much of today&amp;#39;s communication is carried out over large wireless systems with different input-output behaviors. In this work, we compare the power of central abstractions of wireless communication through the general notion of boolean symmetric $f$-channels: In every round of the $f$-channel, each of its $n$ parties decides to either broadcast or not, and the channel outputs $f(m)$, where $m$ is the number of broadcasting parties.

Our first result is that the well studied beeping channel, where $f$ is the threshold-$1$ function, is not stronger than any other $f$-channel. To this end, we design a protocol over the $f$-channel and prove that any protocol that simulates it over the beeping channel blows up the round complexity by a factor of $\Omega(\log n)$. Our lower bound technique may be of independent interest, as it essentially generalizes the popular fooling set technique by exploiting a &amp;quot;local&amp;quot; relaxation of combinatorial rectangles.

Curiously, while this result shows the limitations of a noiseless channel, namely, the beeping channel, we are able to use it to show the limitations of the noisy version of many other channels. This includes the extensively studied single-hop radio network model with collisions-as-silence (CAS), which is equivalent to the $f$-channel with $f(m)=1$ iff $m=1$.

In particular, our second and main result, obtained from the first, shows that converting CAS protocols to noise resilient ones may incur a large performance overhead, i.e., no constant rate interactive code exists. To this end, we design a CAS protocol and prove that any protocol that simulates it over the noisy CAS model with correlated stochastic noise, blows up the round complexity by a factor of $\Omega(\log n)$. We mention that the $\Omega(\log n)$ overhead in both our results is tight.
  </content>
    <author>
      <name>ECCC Papers</name>
      <uri>https://eccc.weizmann.ac.il/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">TOC for Fairness: Our 2023 Postdoc Program is up</title>
    <link href="https://toc4fairness.org/our-2023-postdoc-program-is-up/"/>
    <id>https://toc4fairness.org/?p=2387</id>
    <updated>2022-12-03T19:20:43+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;&lt;/p&gt;



&lt;p&gt;The &lt;a href=&quot;https://toc4fairness.org/&quot; data-type=&quot;URL&quot; data-id=&quot;https://toc4fairness.org/&quot;&gt;Simons collaboration on the theory of algorithmic fairness&lt;/a&gt; is excited to announce &lt;a href=&quot;https://toc4fairness.org/postdoc-opportunities/&quot; data-type=&quot;URL&quot; data-id=&quot;https://toc4fairness.org/postdoc-opportunities/&quot;&gt;our new postdoc program&lt;/a&gt;. We are seeking strong candidates from a diverse set of academic backgrounds and personal experiences who want to work with one or more of the PIs on algorithmic fairness and responsible computing more broadly. We expect to be extending multiple offers.  &lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Omer Reingold&lt;/p&gt;
  </content>
    <author>
      <name>TOC for Fairness</name>
      <uri>https://toc4fairness.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">CCI: jobs: Postdoc in combinatorial optimization at University of Copenhagen (apply by January 15, 2023)</title>
    <link href="https://cstheory-jobs.org/2022/12/03/postdoc-in-combinatorial-optimization-at-university-of-copenhagen-apply-by-january-15-2023/"/>
    <id>http://cstheory-jobs.org/2022/12/03/postdoc-in-combinatorial-optimization-at-university-of-copenhagen-apply-by-january-15-2023/</id>
    <updated>2022-12-03T11:49:30+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;The CS department at the University of Copenhagen invites applications for postdoc positions in combinatorial optimization. The application deadline is January 15. See &lt;a href=&quot;http://www.jakobnordstrom.se/openings/Postdoc-UCPH-230115.html&quot;&gt;http://www.jakobnordstrom.se/openings/Postdoc-UCPH-230115.html&lt;/a&gt; for the full announcement with more information and instructions for how to apply. Informal enquiries are welcome and may be sent to jn@di.ku.dk.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;http://www.jakobnordstrom.se/openings/Postdoc-UCPH-230115.html&quot;&gt;http://www.jakobnordstrom.se/openings/Postdoc-UCPH-230115.html&lt;/a&gt;&lt;br /&gt;
Email: jn@di.ku.dk&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </content>
    <author>
      <name>CCI: jobs</name>
      <uri>https://cstheory-jobs.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">TCS+ Seminar Series: TCS+ “Test of Time” talk: Wednesday, December 7 — Ronitt Rubinfeld, MIT and Tel Aviv University</title>
    <link href="https://tcsplus.wordpress.com/2022/12/03/tcs-test-of-time-talk-wednesday-december-7-ronitt-rubinfeld-mit-and-tel-aviv-university/"/>
    <id>http://tcsplus.wordpress.com/?p=658</id>
    <updated>2022-12-03T05:33:48+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;&lt;/p&gt;


&lt;p&gt;The next TCS+ talk will take place this coming Wednesday, December 7th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). &lt;a href=&quot;https://people.csail.mit.edu/ronitt/&quot;&gt;&lt;strong&gt;Ronitt Rubinfeld&lt;/strong&gt;&lt;/a&gt; from MIT and Tel Aviv University will give our very first &amp;#8220;Test of Time&amp;#8221; talk, titled &amp;#8220;&lt;em&gt;A Comedy of Errors&lt;/em&gt;&amp;#8221; (abstract below).&lt;/p&gt;
&lt;p&gt;You can reserve a spot as an individual or a group to join us live by signing up on &lt;a href=&quot;https://sites.google.com/view/tcsplus/welcome/next-tcs-talk&quot;&gt;the online form&lt;/a&gt;. Registration is &lt;em&gt;not&lt;/em&gt; required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted &lt;a href=&quot;https://sites.google.com/view/tcsplus/welcome/past-talks&quot;&gt;on our website&lt;/a&gt; afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to &lt;a href=&quot;https://sites.google.com/view/tcsplus/welcome/suggest-a-talk&quot;&gt;suggest&lt;/a&gt; a possible topic or speaker, please see &lt;a href=&quot;https://sites.google.com/view/tcsplus/&quot;&gt;the website&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class=&quot;wp-block-quote&quot;&gt;
&lt;p&gt;Abstract: In the late 1980s, a new model of &amp;#8220;Program Checking&amp;#8221; was put forth by Blum and Kannan in order to prevail over errors in programs. With that as a starting point, several lines of research developed &amp;#8212; including one that eventually morphed into the area of sublinear time algorithms. Along the way, errors were made and others were detected. We will recount a personal view of the arbitrary nature of this process and place it in historical context.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The talk will be followed by an unrecorded &amp;#8220;Ask Me Anything&amp;#8221; (AMA) session.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;&lt;p class=&quot;authors&quot;&gt;By plustcs&lt;/p&gt;
  </content>
    <author>
      <name>TCS+ Seminar Series</name>
      <uri>https://tcsplus.wordpress.com</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">CCI: jobs: Simons-Berkeley Research Fellowships at Simons Institute (apply by December 15, 2022)</title>
    <link href="https://cstheory-jobs.org/2022/12/03/simons-berkeley-research-fellowships-at-simons-institute-apply-by-december-15-2022/"/>
    <id>http://cstheory-jobs.org/2022/12/03/simons-berkeley-research-fellowships-at-simons-institute-apply-by-december-15-2022/</id>
    <updated>2022-12-03T02:40:31+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;[Deadline Reminder] The Simons Institute for the Theory of Computing invites applications for Simons-Berkeley Research Fellowships for the Summer 2023, Fall 2023, and Spring 2024 semesters. The deadline for receipt of applications is December 15, 2022.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://simons.berkeley.edu/simons-berkeley-research-fellowship-call-applications&quot;&gt;https://simons.berkeley.edu/simons-berkeley-research-fellowship-call-applications&lt;/a&gt;&lt;br /&gt;
Email: simonsvisitorservices@berkeley.edu&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </content>
    <author>
      <name>CCI: jobs</name>
      <uri>https://cstheory-jobs.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Scott Aaronson: Google&amp;#8217;s Sycamore chip: no wormholes, no superfast classical simulation either</title>
    <link href="https://scottaaronson.blog/?p=6871"/>
    <id>https://scottaaronson.blog/?p=6871</id>
    <updated>2022-12-02T23:04:15+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-large&quot;&gt;&lt;img decoding=&quot;async&quot; src=&quot;https://www.scottaaronson.com/sycamore.jpg&quot; alt=&quot;&quot;/&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;&lt;mark style=&quot;background-color:rgba(0, 0, 0, 0)&quot; class=&quot;has-inline-color has-vivid-red-color&quot;&gt;Update (Dec. 6):&lt;/mark&gt;&lt;/strong&gt; I&amp;#8217;m having a blast at the &lt;a href=&quot;https://www.ias.edu/sns/scientific-program-qubit-2022&quot;&gt;Workshop on Spacetime and Quantum Information&lt;/a&gt; at the Institute for Advanced Study in Princeton.  I&amp;#8217;m learning a huge amount from the talks and discussions here&amp;#8212;and also simply enjoying being back in Princeton, to see old friends and visit old haunts like the &lt;a href=&quot;https://palmersquare.com/directory/the-bent-spoon/&quot;&gt;Bent Spoon&lt;/a&gt;.  Tomorrow I&amp;#8217;ll speak about my &lt;a href=&quot;https://arxiv.org/abs/2210.15601&quot;&gt;recent work with Jason Pollack&lt;/a&gt; on polynomial-time AdS bulk reconstruction.&lt;/p&gt;



&lt;p&gt;But there&amp;#8217;s one thing, relevant to this post, that I can&amp;#8217;t let pass without comment.  Tonight, David Nirenberg, Director of the IAS and a medieval historian, gave an after-dinner speech to our workshop, centered around how auspicious it was that the workshop was being held a mere week after the momentous announcement that &lt;em&gt;a wormhole had been created on a microchip (!!)&lt;/em&gt;&amp;#8212;in a feat that experts were calling the first-ever laboratory investigation of quantum gravity, and a new frontier for experimental physics itself.  Nirenberg speculated that, a century from now, people might look back on the wormhole achievement as today we look back on Eddington&amp;#8217;s 1919 eclipse observations providing the evidence for general relativity.&lt;/p&gt;



&lt;p&gt;I confess: this was the first time I felt visceral anger, rather than mere bemusement, over this wormhole affair.  Before, I had implicitly assumed: no one was &lt;em&gt;actually&lt;/em&gt; hoodwinked by this.  No one &lt;em&gt;really, literally&lt;/em&gt; believed that this little 9-qubit simulation opened up a wormhole, or helped prove the holographic nature of the real universe, or anything like that.  I was wrong.&lt;/p&gt;



&lt;p&gt;To be clear, I don&amp;#8217;t blame Professor Nirenberg at all.  If &lt;em&gt;I&lt;/em&gt; were a medieval historian, everything he said about the experiment&amp;#8217;s historic significance might strike me as perfectly valid inferences from what I&amp;#8217;d read in the press.  I don&amp;#8217;t blame the It from Qubit community&amp;#8212;most of which, I can report, was grinding its teeth and turning red in the face right alongside me.  I don&amp;#8217;t even blame most of the authors of the wormhole paper, such as Daniel Jafferis, who gave a perfectly sober, reasonable, technical talk at the workshop about how he and others managed to compress a simulation of a variant of the SYK model into a mere 9 qubits&amp;#8212;a talk that eschewed all claims of historic significance and of literal wormhole creation.&lt;/p&gt;



&lt;p&gt;But it&amp;#8217;s now clear to me that, between&lt;/p&gt;



&lt;p&gt;(1) the It from Qubit community that likes to explore speculative ideas like holographic wormholes, and&lt;/p&gt;



&lt;p&gt;(2) the lay news readers who are now under the impression that Google just did one of the greatest physics experiments of all time,&lt;/p&gt;



&lt;p&gt;&lt;em&gt;something&lt;/em&gt; went terribly wrong&amp;#8212;something that risks damaging trust in the scientific process itself.  And I think it&amp;#8217;s worth reflecting on what we can do to prevent it from happening again.&lt;/p&gt;



&lt;hr class=&quot;wp-block-separator has-alpha-channel-opacity&quot;/&gt;



&lt;p&gt;This is going to be one of the many &lt;em&gt;Shtetl-Optimized&lt;/em&gt; posts that I didn&amp;#8217;t feel like writing, but was given no choice but to write.&lt;/p&gt;



&lt;p&gt;News, social media, and my inbox have been abuzz with two claims about Google&amp;#8217;s Sycamore quantum processor, the one that now has 72 superconducting qubits.&lt;/p&gt;



&lt;p&gt;The first claim is that Sycamore created a wormhole (!)&amp;#8212;a historic feat possible only with a quantum computer.  See for example the &lt;em&gt;&lt;a href=&quot;https://www.nytimes.com/2022/11/30/science/physics-wormhole-quantum-computer.html&quot;&gt;New York Times&lt;/a&gt;&lt;/em&gt; and &lt;em&gt;&lt;a href=&quot;https://www.quantamagazine.org/physicists-create-a-wormhole-using-a-quantum-computer-20221130/&quot;&gt;Quanta&lt;/a&gt;&lt;/em&gt; and &lt;em&gt;&lt;a href=&quot;https://arstechnica.com/science/2022/12/no-physicists-didnt-make-a-real-wormhole-what-they-did-was-still-pretty-cool/&quot;&gt;Ars Technica&lt;/a&gt;&lt;/em&gt; and &lt;em&gt;&lt;a href=&quot;https://www.nature.com/articles/d41586-022-03832-z&quot;&gt;Nature&lt;/a&gt;&lt;/em&gt; (and of course, the &lt;a href=&quot;https://www.nature.com/articles/s41586-022-05424-3&quot;&gt;actual paper&lt;/a&gt;), as well as &lt;a href=&quot;https://www.math.columbia.edu/~woit/wordpress/?p=13181&quot;&gt;Peter Woit&amp;#8217;s blog&lt;/a&gt; and &lt;a href=&quot;https://chadorzel.substack.com/p/wormhole-to-2006&quot;&gt;Chad Orzel&amp;#8217;s blog&lt;/a&gt;.&lt;/p&gt;



&lt;p&gt;The second claim is that Sycamore&amp;#8217;s pretensions to quantum supremacy have been refuted.  The latter claim is based on &lt;a href=&quot;https://arxiv.org/abs/2211.03999&quot;&gt;this recent preprint&lt;/a&gt; by Dorit Aharonov, Xun Gao, Zeph Landau, Yunchao Liu, and Umesh Vazirani.  No one&amp;#8212;least of all me!&amp;#8212;doubts that these authors have proved a strong new technical result, solving a significant open problem in the theory of noisy random circuit sampling.  On the other hand, it might be less obvious how to interpret their result and put it in context.  See also a &lt;a href=&quot;https://www.youtube.com/watch?v=zDnA1gu4QO0&quot;&gt;YouTube video&lt;/a&gt; of Yunchao speaking about the new result at this week&amp;#8217;s Simons Institute Quantum Colloquium, and of a panel discussion afterwards, where Yunchao, Umesh Vazirani, Adam Bouland, Sergio Boixo, and your humble blogger discuss what it means.&lt;/p&gt;



&lt;p&gt;On their face, the two claims about Sycamore might seem to be in tension.  After all, if Sycamore can&amp;#8217;t do anything beyond what a classical computer can do, then how exactly did it &lt;em&gt;bend the topology of spacetime&lt;/em&gt;?&lt;/p&gt;



&lt;p&gt;I submit that neither claim is true.  On the one hand, Sycamore did not &amp;#8220;create a wormhole.&amp;#8221;  On the other hand, it remains pretty hard to simulate with a classical computer, as far as anyone knows.  To summarize, then, our knowledge of what Sycamore can and can&amp;#8217;t do remains much the same as last week or last month!&lt;/p&gt;



&lt;hr class=&quot;wp-block-separator has-alpha-channel-opacity&quot;/&gt;



&lt;p&gt;Let&amp;#8217;s start with the wormhole thing.  I can&amp;#8217;t really improve over how I put it in Dennis Overbye&amp;#8217;s &lt;em&gt;NYT&lt;/em&gt; piece:&lt;/p&gt;



&lt;blockquote class=&quot;wp-block-quote&quot;&gt;
&lt;p&gt;“The most important thing I’d want New York Times readers to understand is this,” Scott Aaronson, a quantum computing expert at the University of Texas in Austin, wrote in an email. “If this experiment has brought a wormhole into actual physical existence, then a strong case could be made that you, too, bring a wormhole into actual physical existence every time you sketch one with pen and paper.”&lt;/p&gt;
&lt;/blockquote&gt;



&lt;p&gt;More broadly, Overbye&amp;#8217;s &lt;em&gt;NYT&lt;/em&gt; piece explains with admirable clarity what this experiment did and didn&amp;#8217;t do&amp;#8212;leaving only the question &amp;#8220;wait &amp;#8230; if that&amp;#8217;s all that&amp;#8217;s going on here, then why is it being written up in the &lt;em&gt;NYT&lt;/em&gt;??&amp;#8221;  This is a rare case where, in my opinion, the &lt;em&gt;NYT&lt;/em&gt; did a much better job than &lt;em&gt;Quanta&lt;/em&gt;, which unequivocally accepted and amplified the &amp;#8220;QC creates a wormhole&amp;#8221; framing.&lt;/p&gt;



&lt;p&gt;Alright, but what&amp;#8217;s the actual basis for the &amp;#8220;QC creates a wormhole&amp;#8221; claim, for those who don&amp;#8217;t want to leave this blog to read about it?  Well, the authors used 9 of Sycamore&amp;#8217;s 72 qubits to do a crude simulation of something called the &lt;a href=&quot;https://en.wikipedia.org/wiki/Sachdev%E2%80%93Ye%E2%80%93Kitaev_model&quot;&gt;SYK (Sachdev-Ye-Kitaev) model&lt;/a&gt;.  SYK has become popular as a toy model for quantum gravity.  In particular, it has a holographic dual description, which can indeed involve a spacetime with one or more wormholes.  So, they ran a quantum circuit that crudely modelled the SYK dual of a scenario with information sent through a wormhole.  They then confirmed that the circuit did what it was supposed to do&amp;#8212;i.e., what they’d already classically calculated that it &lt;em&gt;would&lt;/em&gt; do.&lt;/p&gt;



&lt;p&gt;So, the objection is obvious: if someone simulates a black hole on their classical computer, they don&amp;#8217;t say they thereby &amp;#8220;created a black hole.&amp;#8221;  Or if they do, journalists don&amp;#8217;t uncritically repeat the claim.  Why should the standards be different just because we&amp;#8217;re talking about a quantum computer rather than a classical one?&lt;/p&gt;



&lt;p&gt;Did we at least &lt;em&gt;learn anything new&lt;/em&gt; about SYK wormholes from the simulation?  Alas, not really, because 9 qubits take a mere 2&lt;sup&gt;9&lt;/sup&gt;=512 complex numbers to specify their wavefunction, and are therefore trivial to simulate on a laptop.  There&amp;#8217;s some argument in the paper that, if the simulation were scaled up to (say) 100 qubits, then maybe we &lt;em&gt;would&lt;/em&gt; learn something new about SYK.  Even then, however, we&amp;#8217;d mostly learn about certain corrections that arise &lt;em&gt;because&lt;/em&gt; the simulation was being done with &amp;#8220;only&amp;#8221; n=100 qubits, rather than in the n→∞ limit where SYK is rigorously understood.  But while those corrections, arising when n is &amp;#8220;neither too large nor too small,&amp;#8221; would surely be interesting to specialists, they&amp;#8217;d have no obvious bearing on the prospects for creating real physical wormholes in our universe.&lt;/p&gt;



&lt;p&gt;And yet, this is not a sensationalistic misunderstanding invented by journalists.  Some prominent quantum gravity theorists themselves&amp;#8212;including some of my close friends and collaborators&amp;#8212;persist in talking about the simulated SYK wormhole as &amp;#8220;actually being&amp;#8221; a wormhole.  What are they thinking?&lt;/p&gt;



&lt;p&gt;Daniel Harlow explained the thinking to me as follows (he stresses that he&amp;#8217;s explaining it, not necessarily endorsing it).  If you had two entangled quantum computers, one on Earth and the other in the Andromeda galaxy, and if they were both simulating SYK, and if Alice on Earth and Bob in Andromeda both &lt;em&gt;uploaded their own brains into their respective quantum simulations&lt;/em&gt;, then it seems possible that the simulated Alice and Bob could have the experience of jumping into a wormhole and meeting each other in the middle.  Granted, they couldn&amp;#8217;t get a message back &lt;em&gt;out&lt;/em&gt; from the wormhole, at least not without &amp;#8220;going the long way,&amp;#8221; which could happen only at the speed of light&amp;#8212;so only simulated-Alice and simulated-Bob themselves could ever &lt;em&gt;test&lt;/em&gt; this prediction.  Nevertheless, &lt;em&gt;if true&lt;/em&gt;, I suppose some would treat it as grounds for regarding a quantum simulation of SYK as &amp;#8220;more real&amp;#8221; or &amp;#8220;more wormholey&amp;#8221; than a classical simulation.&lt;/p&gt;



&lt;p&gt;Of course, this scenario depends on strong assumptions not merely about quantum gravity, but &lt;em&gt;also&lt;/em&gt; about the metaphysics of consciousness!  And I&amp;#8217;d &lt;em&gt;still&lt;/em&gt; prefer to call it a simulated wormhole for simulated people.&lt;/p&gt;



&lt;p&gt;For completeness, here&amp;#8217;s Harlow&amp;#8217;s passage from the &lt;em&gt;NYT&lt;/em&gt; article:&lt;/p&gt;



&lt;blockquote class=&quot;wp-block-quote&quot;&gt;
&lt;p&gt;Daniel Harlow, a physicist at M.I.T. who was not involved in the experiment, noted that the experiment was based on a model of quantum gravity that was so simple, and unrealistic, that it could just as well have been studied using a pencil and paper.&lt;/p&gt;



&lt;p&gt;“So I’d say that this doesn’t teach us anything about quantum gravity that we didn’t already know,” Dr. Harlow wrote in an email. “On the other hand, I think it is exciting as a technical achievement, because if we can’t even do this (and until now we couldn’t), then simulating more interesting quantum gravity theories would CERTAINLY be off the table.” Developing computers big enough to do so might take 10 or 15 years, he added.&lt;/p&gt;
&lt;/blockquote&gt;



&lt;hr class=&quot;wp-block-separator has-alpha-channel-opacity&quot;/&gt;



&lt;p&gt;Alright, let&amp;#8217;s move on to the claim that quantum supremacy has been refuted.  What Aharonov et al. actually show in their &lt;a href=&quot;https://arxiv.org/abs/2211.03999&quot;&gt;new work&lt;/a&gt;, building on &lt;a href=&quot;https://arxiv.org/abs/1810.03176&quot;&gt;earlier work by Gao and Duan&lt;/a&gt;, is that Random Circuit Sampling, with a constant rate of noise per gate and no error-correction, can&amp;#8217;t provide a &lt;em&gt;scalable&lt;/em&gt; approach to quantum supremacy.  Or more precisely: as the number of qubits n goes to infinity, and assuming you&amp;#8217;re in the &amp;#8220;anti-concentration regime&amp;#8221; (which in practice probably means: the depth of your quantum circuit is at least ~log(n)), there&amp;#8217;s a classical algorithm to approximately sample the quantum circuit&amp;#8217;s output distribution in poly(n) time (albeit, not yet a practical algorithm).&lt;/p&gt;



&lt;p&gt;Here&amp;#8217;s what&amp;#8217;s crucial to understand: this is &lt;em&gt;100% consistent&lt;/em&gt; with what those of us working on quantum supremacy had assumed since at least 2016!  We &lt;em&gt;knew&lt;/em&gt; that if you tried to scale Random Circuit Sampling to 200 or 500 or 1000 qubits, while you also increased the circuit depth proportionately, the signal-to-noise ratio would become exponentially small, meaning that your quantum speedup would disappear.  That&amp;#8217;s why, from the very beginning, we targeted the &amp;#8220;practical&amp;#8221; regime of 50-100 qubits: a regime where&lt;/p&gt;



&lt;ol&gt;
&lt;li&gt;you can still see explicitly that you&amp;#8217;re exploiting a 2&lt;sup&gt;50&lt;/sup&gt;&amp;#8211; or 2&lt;sup&gt;100&lt;/sup&gt;-dimensional Hilbert space for computational advantage, thereby confirming one of the main predictions of quantum computing theory, but&lt;/li&gt;



&lt;li&gt;you &lt;em&gt;also&lt;/em&gt; have a signal that (as it turned out) is large enough to see with heroic effort.  &lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;To their credit, Aharonov et al. explain all this perfectly clearly in their abstract and introduction.  I&amp;#8217;m just worried that &lt;em&gt;others&lt;/em&gt; aren&amp;#8217;t reading their paper as carefully as they should be!&lt;/p&gt;



&lt;p&gt;So then, what&amp;#8217;s the new advance in the Aharonov et al. paper?  Well, there had been some hope that circuit depth ~log(n) might be a sweet spot, where an exponential quantum speedup might both exist &lt;em&gt;and&lt;/em&gt; survive constant noise, even in the asymptotic limit of n→∞ qubits.  Nothing in Google&amp;#8217;s or USTC&amp;#8217;s actual Random Circuit Sampling experiments depended on that hope, but it would&amp;#8217;ve been nice if it were true.  What Aharonov et al. have now done is to kill that hope, using powerful techniques involving summing over Feynman paths in the Pauli basis.&lt;/p&gt;



&lt;p&gt;Stepping back, what &lt;em&gt;is&lt;/em&gt; the current status of quantum supremacy based on Random Circuit Sampling?  I would say it&amp;#8217;s still standing, but more precariously than I&amp;#8217;d like&amp;#8212;underscoring the need for new and better quantum supremacy experiments.  In more detail, &lt;a href=&quot;https://arxiv.org/abs/2111.03011&quot;&gt;Pan, Chen, and Zhang&lt;/a&gt; have shown how to simulate Google&amp;#8217;s 53-qubit Sycamore chip classically, using what I estimated to be 100-1000X the electricity cost of running the quantum computer itself (&lt;em&gt;including&lt;/em&gt; the dilution refrigerator!).  Approaching from the problem from a different angle, &lt;a href=&quot;https://arxiv.org/abs/2112.01657&quot;&gt;Gao et al.&lt;/a&gt; have given a polynomial-time classical algorithm for spoofing Google&amp;#8217;s Linear Cross-Entropy Benchmark (LXEB)&amp;#8212;&lt;em&gt;but&lt;/em&gt; their algorithm can currently achieve only about 10% of the excess in LXEB that Google&amp;#8217;s experiment found.&lt;/p&gt;



&lt;p&gt;So, though it&amp;#8217;s been under sustained attack from multiple directions these past few years, I&amp;#8217;d say that the flag of quantum supremacy yet waves.  The Extended Church-Turing Thesis is still on thin ice.  The wormhole is still open.  Wait &amp;#8230; &lt;em&gt;no&lt;/em&gt; &amp;#8230; that&amp;#8217;s not what I meant to write&amp;#8230;&lt;/p&gt;



&lt;hr class=&quot;wp-block-separator has-alpha-channel-opacity&quot;/&gt;



&lt;p&gt;&lt;mark style=&quot;background-color:rgba(0, 0, 0, 0)&quot; class=&quot;has-inline-color has-vivid-red-color&quot;&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;/mark&gt;With this post, as with future science posts, &lt;em&gt;all off-topic comments will be ruthlessly left in moderation&lt;/em&gt;.  Yes, even if the comments &amp;#8220;create their own reality&amp;#8221; full of anger and disappointment that I talked about what I talked about, instead of what the commenter wanted me to talk about.  Even if merely &lt;em&gt;refuting&lt;/em&gt; the comments would require me to give in and talk about their preferred topics after all.  Please stop.  This is a wormholes-&amp;#8216;n-supremacy post.&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Scott&lt;/p&gt;
  </content>
    <author>
      <name>Scott Aaronson</name>
      <uri>https://scottaaronson.blog</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">ECCC Papers: TR22-173 |  On Disperser/Lifting Properties of the Index and Inner-Product Functions | 

	Sajin Koroth, 

	Paul Beame</title>
    <link href="https://eccc.weizmann.ac.il/report/2022/173"/>
    <id>https://eccc.weizmann.ac.il/report/2022/173</id>
    <updated>2022-12-02T22:55:23+00:00</updated>
    <content type="html" xml:lang="en">
    Query-to-communication lifting theorems, which connect the query complexity of a Boolean function to the communication complexity of an associated `lifted&amp;#39; function obtained by composing the function with many copies of another function known as a gadget, have been instrumental in resolving many open questions in computational complexity. Several important complexity questions could be resolved if we could make substantial improvements in the input size required for lifting with the Index function, from its current near-linear size down to polylogarithmic in the number of inputs $N$ of the original function or, ideally, constant. The near-linear size bound was shown by Lovett, Meka, Mertz, Pitassi and Zhang using a recent breakthrough improvement on the Sunflower Lemma to show that a certain graph associated with the Index function of near-linear size is a disperser. They also stated a conjecture about the Index function that is essential for further improvements in the size required for lifting with Index using current techniques. In this paper we prove the following;
  1) The conjecture of Lovett et al. is false when the size of the Index gadget is $\log N-\omega(1)$.
  2) Also, the Inner-Product function, which satisfies the disperser property at size $O(\log N)$, does not have this property when its size is  $\log N-\omega(1)$.
  3) Nonetheless, using Index gadgets of size at least 4, we prove a lifting theorem for a restricted class of communication protocols in which one of the players is limited to sending parities of its inputs.
  4) Using the ideas from this lifting theorem, we derive a strong lifting theorem from decision tree size to parity decision tree size. We use this to derive a general lifting theorem in proof complexity from tree-resolution size to tree-like $Res(\oplus)$ refutation size, which yields many new exponential lower bounds on such proofs.
  </content>
    <author>
      <name>ECCC Papers</name>
      <uri>https://eccc.weizmann.ac.il/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">ECCC Papers: TR22-172 |  Lifting to Parity Decision Trees Via Stifling | 

	Arkadev Chattopadhyay, 

	Nikhil Mande, 

	Swagato Sanyal, 

	Suhail Sherif</title>
    <link href="https://eccc.weizmann.ac.il/report/2022/172"/>
    <id>https://eccc.weizmann.ac.il/report/2022/172</id>
    <updated>2022-12-02T22:16:33+00:00</updated>
    <content type="html" xml:lang="en">
    We show that the deterministic decision tree complexity of a (partial) function or relation $f$ lifts to the deterministic parity decision tree (PDT) size complexity of the composed function/relation $f \circ g$ as long as the gadget $g$ satisfies a property that we call stifling. We observe that several simple gadgets of constant size, like Indexing on 3 input bits, Inner Product on 4 input bits, Majority on 3 input bits and random functions, satisfy this property. It can be shown that existing randomized communication lifting theorems ([Göös, Pitassi, Watson. SICOMP&amp;#39;20], [Chattopadhyay et al. SICOMP&amp;#39;21]) imply PDT-size lifting. However there are two shortcomings of this approach: first they lift randomized decision tree complexity of $f$, which could be exponentially smaller than its deterministic counterpart when either $f$ is a partial function or even a total search problem. Second, the size of the gadgets in such lifting theorems are as large as logarithmic in the size of the input to $f$. Reducing the gadget size to a constant is an important open problem at the frontier of current research.

Our result shows that even a random constant-size gadget does enable lifting to PDT size. Further, it also yields the first systematic way of turning lower bounds on the width of tree-like resolution proofs of the unsatisfiability of constant-width CNF formulas to lower bounds on the size of tree-like proofs in the resolution with parity system, i.e., $\mathrm{Res}$($\oplus$), of the unsatisfiability of closely related constant-width CNF formulas.
  </content>
    <author>
      <name>ECCC Papers</name>
      <uri>https://eccc.weizmann.ac.il/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Computational Complexity: How do we keep the community connected?</title>
    <link href="http://blog.computationalcomplexity.org/2022/12/how-do-we-keep-community-connected.html"/>
    <id>tag:blogger.com,1999:blog-3722233.post-4202676814033172170</id>
    <updated>2022-12-01T16:37:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;A colleague said how they enjoyed watching the &lt;a href=&quot;https://blog.computationalcomplexity.org/2022/11/should-you-quit-twitter-and-texas.html&quot;&gt;collapse of Twitter&lt;/a&gt; under Elon Musk. But I use Twitter to keep connected to the CS community. In Twitter I hear not only new results but ones that excite particular people. I watch the debate between those who see ML as revolutionary and those who see ML as revolting. I see the issues that our community worries about and those that they celebrate. I follow people as they progress in their careers or outright change them. Mostly it just makes me feel part of an academic community that goes beyond my own institution.&amp;nbsp;&lt;/p&gt;&lt;p&gt;A bit surprisingly, so far Twitter hasn&#39;t collapsed. But it could and I expect many of my followers and those I follow spend less time there. I set up a Mastodon account&amp;nbsp;&lt;a href=&quot;https://fediscience.org/@fortnow&quot;&gt;@fortnow@fediscience.org&lt;/a&gt;&amp;nbsp;but not much happens over there, though feel free to tell me who I should be following. There are many other social networks but none that bring us as a field together.&lt;/p&gt;&lt;p&gt;Blog posts and their comments play a role but not like they used to. There&#39;s&amp;nbsp;&lt;a href=&quot;https://cstheory.stackexchange.com/&quot;&gt;CS Theory StackExchange&lt;/a&gt;&amp;nbsp;which has some good (and not so good) technical discussions but we don&#39;t really have conversations there.&amp;nbsp;&lt;/p&gt;&lt;p&gt;How about conferences now that researchers are (mostly) willing to attend in person? Since conferences in CS are the primary publication venue, we have too many meetings and many won&#39;t go, or at least won&#39;t go in person, if they don&#39;t have a paper in the conference.&lt;/p&gt;&lt;p&gt;So, &lt;a href=&quot;https://blog.computationalcomplexity.org/2008/07/games.html&quot;&gt;once again&lt;/a&gt;, I suggest a big theory conference we hold every four years that everyone who&#39;s anyone will make sure to attend. Hey, it works for the World Cup.&amp;nbsp;&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By Lance Fortnow&lt;/p&gt;
  </content>
    <author>
      <name>Computational Complexity</name>
      <uri>http://blog.computationalcomplexity.org/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">CCI: jobs: Assistant, Associate, or Full Professor (Search 2) at University of California – San Diego (apply by January 1, 2023)</title>
    <link href="https://cstheory-jobs.org/2022/11/30/assistant-associate-or-full-professor-search-2-at-university-of-california-san-diego-apply-by-january-1-2023/"/>
    <id>http://cstheory-jobs.org/2022/11/30/assistant-associate-or-full-professor-search-2-at-university-of-california-san-diego-apply-by-january-1-2023/</id>
    <updated>2022-11-30T23:22:09+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;The UC San Diego Department of Computer Science and Engineering (CSE) invites applications for multiple tenure-track faculty positions at the Assistant Professor or tenured faculty positions at the Associate, or Full Professor rank. The department is looking for exceptional candidates in all areas of Computer Science and Engineering.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://cse.ucsd.edu/administration/human-resources/recruitment/faculty-positions&quot;&gt;https://cse.ucsd.edu/administration/human-resources/recruitment/faculty-positions&lt;/a&gt;&lt;br /&gt;
Email: nherrera@eng.ucsd.edu&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </content>
    <author>
      <name>CCI: jobs</name>
      <uri>https://cstheory-jobs.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">CCI: jobs: Assistant, Associate, or Full Professor (Search 1) at University of California – San Diego (apply by January 1, 2023)</title>
    <link href="https://cstheory-jobs.org/2022/11/30/assistant-associate-or-full-professor-search-1-at-university-of-california-san-diego-apply-by-january-1-2023/"/>
    <id>http://cstheory-jobs.org/2022/11/30/assistant-associate-or-full-professor-search-1-at-university-of-california-san-diego-apply-by-january-1-2023/</id>
    <updated>2022-11-30T23:20:48+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;The UC San Diego Department of Computer Science and Engineering (CSE) invites applications for multiple tenure-track faculty positions at the Assistant Professor or tenured faculty positions at Associate, or Full Professor rank. The department is looking for exceptional candidates in all areas of Computer Science and Engineering.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://cse.ucsd.edu/administration/human-resources/recruitment/faculty-positions&quot;&gt;https://cse.ucsd.edu/administration/human-resources/recruitment/faculty-positions&lt;/a&gt;&lt;br /&gt;
Email: nherrera@eng.ucsd.edu&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </content>
    <author>
      <name>CCI: jobs</name>
      <uri>https://cstheory-jobs.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">David Eppstein: Linkage</title>
    <link href="https://11011110.github.io/blog/2022/11/30/linkage.html"/>
    <id>https://11011110.github.io/blog/2022/11/30/linkage</id>
    <updated>2022-11-30T15:47:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://kylehovey.github.io/blog/automata-nebula&quot;&gt;Digital astronomy with cellular automata&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109356657961629085&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;,&lt;/span&gt; &lt;a href=&quot;https://news.ycombinator.com/item?id=33578376&quot;&gt;via&lt;/a&gt;). No, this is not about using CA to study patterns in the sky (though that might be interesting); it is about using &lt;a href=&quot;https://umap-learn.readthedocs.io/en/latest/&quot;&gt;UMAP dimension-reduction techniques&lt;/a&gt; to create something like a Hertzsprung-Russell diagram for finding CA rules with complex behavior.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.ics.uci.edu/~eppstein/pix/guanajuato/&quot;&gt;My photos from my recent visit to Guanajuato are now online&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109357876852587696&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;).&lt;/span&gt; Very pretty city, and good food, worth a visit. My recommendation from the locals for their favorite restaurant was Truco 7, where I had amazing chicken mole for much less than I would have expected. Off the main streets much of the city looks like the photo below: steep narrow alleys with very colorful buildings. Maybe not the best for accessibility.&lt;/p&gt;

    &lt;p style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://www.ics.uci.edu/~eppstein/pix/guanajuato/9-m.jpg&quot; alt=&quot;Steep alley in Guanajuato, Mexico, lined with colorful buildings&quot; style=&quot;border-style:solid;border-color:black&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://mathstodon.xyz/@ct_bergstrom@fediscience.org/109355406095574860&quot;&gt;Galactica can generate Wikipedia articles, supposedly&lt;/a&gt;, but actually writes total bullshit resembling a Wikipedia article but not resembling any real-world use of the title term. The example given by Carl T. Bergstrom is for “Brandolini’s law, the principle that bullshit takes another of magnitude less effort create than to clean up”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://leanprover-community.github.io/sphere-eversion/&quot;&gt;Formal proof of Smale’s sphere-inversion theorem&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@highergeometer/109354222002349360&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;),&lt;/span&gt; showing that “Lean doesn’t do just algebra or abstract nonsense, but really serious geometric topology, too”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://b3s23life.blogspot.com/2022/11/in-conways-life-fifteen-gliders-can.html&quot;&gt;You can build anything you want in Conway’s Game of Life by colliding 15 gliders together&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@OscarCunningham/109375074140276198&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;).&lt;/span&gt; Anything that you can build out of any larger number of gliders, that is.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.c82.net/math-instruments/&quot;&gt;The Construction &amp;amp; Principal Uses of Mathematical Instruments&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109384275129127171&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;,&lt;/span&gt; &lt;a href=&quot;https://news.ycombinator.com/item?id=33592636&quot;&gt;via&lt;/a&gt;), a digital edition of a 1709 book by Nicolas Bion, as translated into English in 1758 by Edmund Stone. Site designed as an exercise by Nicholas Rougeux; see also &lt;a href=&quot;https://www.c82.net/blog/?id=90&quot;&gt;Rougeux’s blog post about the site design&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://mathstodon.xyz/@tao/109390971278692349&quot;&gt;Maths at internet speed&lt;/a&gt;. Terry Tao links to &lt;a href=&quot;https://arxiv.org/abs/2211.09055&quot;&gt;Justin Gilmer’s breakthrough&lt;/a&gt; on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Union-closed_sets_conjecture&quot;&gt;union-closed sets conjecture&lt;/a&gt;, followed in quick succession by improvements by &lt;a href=&quot;https://arxiv.org/abs/2211.11504&quot;&gt;Will Sawin&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2211.11689&quot;&gt;Zachary Chase and Shachar Lovett&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/abs/2211.11731&quot;&gt;Ryan Alweiss, Brice Huang, and Mark Sellke&lt;/a&gt;. See also &lt;a href=&quot;https://gilkalai.wordpress.com/2022/11/17/amazing-justin-gilmer-gave-a-constant-lower-bound-for-the-union-closed-sets-conjecture/&quot;&gt;Gil Kalai’s blog post&lt;/a&gt;. In other news, &lt;a href=&quot;https://mathstodon.xyz/@tao&quot;&gt;Terry Tao is now on mathstodon&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://mathstodon.xyz/@oschene@mastodon.social&quot;&gt;The Chambered Nautilus&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;MLINK&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;).&lt;/span&gt; Curved paperfolding model by oschene (Philip Chapman-Bell).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Minor terminological disagreement with some coauthors &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109406394361043315&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;):&lt;/span&gt; let \(T\) be a single-source shortest path tree in an unweighted undirected graph. Can you call \(T\) a “BFS tree”? Or is that only for trees that could be generated by breadth-first search? E.g. consider shortest paths in \(K_{2,3}\) starting from the 3-vertex side. For breadth first search, two leaves from the 3-vertex side will share a parent. But there is a different shortest path tree where they have different parents. Is it a BFS tree?&lt;/p&gt;

    &lt;p style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/blog/assets/2022/bfs-vs-shortest.svg&quot; alt=&quot;K_{2,3}, a BFS tree, and a shortest-path tree that cannot be generated by breadth-first search&quot; style=&quot;width:100%;max-width:600px&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://mathstodon.xyz/@johncarlosbaez/109398225812984155&quot;&gt;Extended thread on combinatorial species of linear orderings and permutations by John Baez&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;MLINK&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;).&lt;/span&gt; I had the vague impression that a &lt;a href=&quot;https://en.wikipedia.org/wiki/Combinatorial_species&quot;&gt;combinatorial species&lt;/a&gt; and a &lt;a href=&quot;https://en.wikipedia.org/wiki/Combinatorial_class&quot;&gt;combinatorial class&lt;/a&gt; were more or less the same thing: a ranked collection of combinatorial objects, equivalent when they have the same number of objects of each rank. But that’s not accurate. It describes classes, but not species, which have a more specific equivalence relation. In particular, the linear orderings and the permutations form the same combinatorial class (they are both counted by the factorials), but different combinatorial species.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://blog.computationalcomplexity.org/2022/11/who-first-thought-of-notion-of.html&quot;&gt;Who first thought of the notion of Polynomial Time&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109419856682356083&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;)?&lt;/span&gt;  As Gasarch points out, the usual answer of Cobham and Edmonds in 1965 is incorrect. The same distinction, between polynomial and super-polynomial algorithms, already appeared in a 1910 paper of Henry Cabourn Pocklington. In the comments, Martin Berger points to the even-earlier analysis of Euclidean GCD, for which a polynomial bound was known by 1841.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://mathstodon.xyz/@Leahwrenn@mastodon.social/109394896886730411&quot;&gt;Leah Berman Williams posts a new mathematical sculpture&lt;/a&gt;: a tetrahedron with its faces symmetrically knotted into trefoils.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.quantamagazine.org/wei-ho-is-drawn-to-algebra-geometry-and-the-human-side-of-math-20221122&quot;&gt;&lt;em&gt;Quanta&lt;/em&gt; profiles Wei Ho&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@QuantaMagazine@mstdn.social/109388303834501212&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;),&lt;/span&gt; her research on elliptic curves, and her new post as director of the Women and Mathematics program at the Institute for Advanced Study.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Not every curvy triangle is a Reuleaux triangle &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109433756706982819&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;).&lt;/span&gt; Even if you ascribe the greater width than height in the photo (from &lt;a href=&quot;https://scilogs.spektrum.de/hlf/maths-on-a-plate/&quot;&gt;a post by Katie Steckles at the Heidelberg Laureate Forum&lt;/a&gt;) to an angled perspective, and imagine it to be 3-way symmetric, this plate is too bulgy to be Reuleaux. Still a fun example to find at a mathematics facility, though.&lt;/p&gt;

    &lt;p style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/blog/assets/2022/HLF-plate.png&quot; alt=&quot;Screenshot of a post by Katie Steckles at the Heidelberg Laureate Forum, showing a dinner plate described by Steckles as being a Reuleaux triangle, overlaid with red circles outlining the actual shape of a Reuleaux triangle&quot; style=&quot;width:100%;max-width:600px&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;This has been a special edition of “Things that are not Reuleaux triangles”; for the most recent full episode, see “&lt;a href=&quot;/blog/2022/06/18/shapes-triangular-pencils.html&quot;&gt;The shapes of triangular pencils&lt;/a&gt;”.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;authors&quot;&gt;By David Eppstein&lt;/p&gt;
  </content>
    <author>
      <name>David Eppstein</name>
      <uri>https://11011110.github.io/blog/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Gil Kalai: A Nice Example Related to the Frankl Conjecture</title>
    <link href="https://gilkalai.wordpress.com/2022/11/30/a-nice-example-related-to-the-frankl-conjecture/"/>
    <id>http://gilkalai.wordpress.com/?p=23582</id>
    <updated>2022-11-30T07:02:02+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;h3&gt;The example&lt;/h3&gt;
&lt;p&gt;As a follow up to &lt;a href=&quot;https://gilkalai.wordpress.com/2022/11/17/amazing-justin-gilmer-gave-a-constant-lower-bound-for-the-union-closed-sets-conjecture/&quot;&gt;my previous post&lt;/a&gt; about Gilmer&amp;#8217;s breakthrough regarding Frankl&amp;#8217;s conjecture, here is a very nice example (from &lt;a href=&quot;https://arxiv.org/abs/2211.11689&quot;&gt;the paper of Zachary Chase and Shachar Lovett&lt;/a&gt;) related to the conjecture.&lt;/p&gt;
&lt;p&gt;Let &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cpsi%3D%5Cfrac+%7B3-%5Csqrt+5%7D%7B2%7D+%3D+0.38..&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cpsi%3D%5Cfrac+%7B3-%5Csqrt+5%7D%7B2%7D+%3D+0.38..&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cpsi%3D%5Cfrac+%7B3-%5Csqrt+5%7D%7B2%7D+%3D+0.38..&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;psi=&amp;#92;frac {3-&amp;#92;sqrt 5}{2} = 0.38..&quot; class=&quot;latex&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Consider the following families of subsets of &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Bn%5D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Bn%5D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Bn%5D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;[n]&quot; class=&quot;latex&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1%3D%5C%7BS+%5Csubset+%5Bn%5D%3A+%7CS%7C%3D%5Cpsi+n+%2B+n%5E%7B2%2F3%7D%5C%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1%3D%5C%7BS+%5Csubset+%5Bn%5D%3A+%7CS%7C%3D%5Cpsi+n+%2B+n%5E%7B2%2F3%7D%5C%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1%3D%5C%7BS+%5Csubset+%5Bn%5D%3A+%7CS%7C%3D%5Cpsi+n+%2B+n%5E%7B2%2F3%7D%5C%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;{&amp;#92;cal F}_1=&amp;#92;{S &amp;#92;subset [n]: |S|=&amp;#92;psi n + n^{2/3}&amp;#92;}&quot; class=&quot;latex&quot; /&gt;, and&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_2+%3D+%5C%7B+T+%5Csubset+%5Bn%5D%3A+%7CT%7C%3E%281-%5Cpsi%29+n+%5C%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_2+%3D+%5C%7B+T+%5Csubset+%5Bn%5D%3A+%7CT%7C%3E%281-%5Cpsi%29+n+%5C%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_2+%3D+%5C%7B+T+%5Csubset+%5Bn%5D%3A+%7CT%7C%3E%281-%5Cpsi%29+n+%5C%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;{&amp;#92;cal F}_2 = &amp;#92;{ T &amp;#92;subset [n]: |T|&amp;gt;(1-&amp;#92;psi) n &amp;#92;}&quot; class=&quot;latex&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;Now let &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D%3D%7B%5Ccal+F%7D_1+%5Ccup+%7B%5Ccal+F%7D_2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D%3D%7B%5Ccal+F%7D_1+%5Ccup+%7B%5Ccal+F%7D_2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D%3D%7B%5Ccal+F%7D_1+%5Ccup+%7B%5Ccal+F%7D_2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;{&amp;#92;cal F}={&amp;#92;cal F}_1 &amp;#92;cup {&amp;#92;cal F}_2&quot; class=&quot;latex&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;Here are some observations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;img src=&quot;https://s0.wp.com/latex.php?latex=%7C%7B%5Ccal+F%7D_2%7C+%3D+o%28%7C%7B%5Ccal+F%7D_1%7C%29&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%7C%7B%5Ccal+F%7D_2%7C+%3D+o%28%7C%7B%5Ccal+F%7D_1%7C%29&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7C%7B%5Ccal+F%7D_2%7C+%3D+o%28%7C%7B%5Ccal+F%7D_1%7C%29&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;|{&amp;#92;cal F}_2| = o(|{&amp;#92;cal F}_1|)&quot; class=&quot;latex&quot; /&gt;.&lt;/li&gt;
&lt;li&gt;The number of pairs &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%28S%2CT%29%3AS%2CT+%5Cin+%7B%5Ccal+F%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%28S%2CT%29%3AS%2CT+%5Cin+%7B%5Ccal+F%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28S%2CT%29%3AS%2CT+%5Cin+%7B%5Ccal+F%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;(S,T):S,T &amp;#92;in {&amp;#92;cal F}&quot; class=&quot;latex&quot; /&gt; whose union is also in &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;{&amp;#92;cal F}&quot; class=&quot;latex&quot; /&gt; is &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%281-o%281%29%29+%7B%7C%5Ccal+F%7D%7C%5E2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%281-o%281%29%29+%7B%7C%5Ccal+F%7D%7C%5E2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%281-o%281%29%29+%7B%7C%5Ccal+F%7D%7C%5E2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;(1-o(1)) {|&amp;#92;cal F}|^2&quot; class=&quot;latex&quot; /&gt;.&lt;/li&gt;
&lt;li&gt;For every &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E0&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E0&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E0&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;epsilon &amp;gt;0&quot; class=&quot;latex&quot; /&gt;, as &lt;img src=&quot;https://s0.wp.com/latex.php?latex=n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;n&quot; class=&quot;latex&quot; /&gt; grows, no element &lt;img src=&quot;https://s0.wp.com/latex.php?latex=k+%5Cin+%5Bn%5D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=k+%5Cin+%5Bn%5D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k+%5Cin+%5Bn%5D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;k &amp;#92;in [n]&quot; class=&quot;latex&quot; /&gt; belongs to more than &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%28%5Cpsi%2B%5Cepsilon%29+%7C%7B%5Ccal+F%7D%7C&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%28%5Cpsi%2B%5Cepsilon%29+%7C%7B%5Ccal+F%7D%7C&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28%5Cpsi%2B%5Cepsilon%29+%7C%7B%5Ccal+F%7D%7C&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;(&amp;#92;psi+&amp;#92;epsilon) |{&amp;#92;cal F}|&quot; class=&quot;latex&quot; /&gt; sets in &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;{&amp;#92;cal F}&quot; class=&quot;latex&quot; /&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first assertion holds because &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%7B%7Bn%7D+%5Cchoose+%7B%5Cpsi+n+%2Bn%5E%7B2%2F3%7D%7D%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%7B%7Bn%7D+%5Cchoose+%7B%5Cpsi+n+%2Bn%5E%7B2%2F3%7D%7D%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7Bn%7D+%5Cchoose+%7B%5Cpsi+n+%2Bn%5E%7B2%2F3%7D%7D%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;{{n} &amp;#92;choose {&amp;#92;psi n +n^{2/3}}}&quot; class=&quot;latex&quot; /&gt; is exponentially larger (in a fractional power of &lt;img src=&quot;https://s0.wp.com/latex.php?latex=n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;n&quot; class=&quot;latex&quot; /&gt;) than &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%7B%7Bn%7D+%5Cchoose+%7B%281-%5Cpsi+%29n%7D%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%7B%7Bn%7D+%5Cchoose+%7B%281-%5Cpsi+%29n%7D%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7Bn%7D+%5Cchoose+%7B%281-%5Cpsi+%29n%7D%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;{{n} &amp;#92;choose {(1-&amp;#92;psi )n}}&quot; class=&quot;latex&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;For the second assertion we need to show that a typical union of a pair of sets in &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;{&amp;#92;cal F}_1&quot; class=&quot;latex&quot; /&gt; belongs to &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;{&amp;#92;cal F}_2&quot; class=&quot;latex&quot; /&gt;. Note that the intersection of two random subsets of &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Bn%5D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Bn%5D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Bn%5D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;[n]&quot; class=&quot;latex&quot; /&gt; of size &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cphi+n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cphi+n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi+n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;phi n&quot; class=&quot;latex&quot; /&gt; is &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cphi+%5E2+n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cphi+%5E2+n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi+%5E2+n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;phi ^2 n&quot; class=&quot;latex&quot; /&gt; and hence their union is of size &lt;img src=&quot;https://s0.wp.com/latex.php?latex=2%5Cphi+-+%5Cphi%5E2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=2%5Cphi+-+%5Cphi%5E2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=2%5Cphi+-+%5Cphi%5E2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;2&amp;#92;phi - &amp;#92;phi^2&quot; class=&quot;latex&quot; /&gt;. As it happens the solution of the equation &lt;img src=&quot;https://s0.wp.com/latex.php?latex=2%5Cphi-%5Cphi%5E2+%3D+1-%5Cphi&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=2%5Cphi-%5Cphi%5E2+%3D+1-%5Cphi&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=2%5Cphi-%5Cphi%5E2+%3D+1-%5Cphi&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;2&amp;#92;phi-&amp;#92;phi^2 = 1-&amp;#92;phi&quot; class=&quot;latex&quot; /&gt; is precisely our &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cpsi%3D%5Cfrac+%7B3%2B%5Csqrt+5%7D%7B2%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cpsi%3D%5Cfrac+%7B3%2B%5Csqrt+5%7D%7B2%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cpsi%3D%5Cfrac+%7B3%2B%5Csqrt+5%7D%7B2%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;psi=&amp;#92;frac {3+&amp;#92;sqrt 5}{2}&quot; class=&quot;latex&quot; /&gt;. So letting &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cphi%3D%5Cpsi+%2B+n%5E%7B-1%2F3%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cphi%3D%5Cpsi+%2B+n%5E%7B-1%2F3%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi%3D%5Cpsi+%2B+n%5E%7B-1%2F3%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;phi=&amp;#92;psi + n^{-1/3}&quot; class=&quot;latex&quot; /&gt; we get that a typical union of two sets from &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;{&amp;#92;cal F}_1&quot; class=&quot;latex&quot; /&gt; is in &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;{&amp;#92;cal F}_2&quot; class=&quot;latex&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;The third assertion follows from the fact that an element &lt;img src=&quot;https://s0.wp.com/latex.php?latex=k+%5Cin+%5Bn%5D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=k+%5Cin+%5Bn%5D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k+%5Cin+%5Bn%5D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;k &amp;#92;in [n]&quot; class=&quot;latex&quot; /&gt; belongs to a fraction of &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cpsi+%2B+n%5E%7B-1%2F3%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cpsi+%2B+n%5E%7B-1%2F3%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cpsi+%2B+n%5E%7B-1%2F3%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;psi + n^{-1/3}&quot; class=&quot;latex&quot; /&gt;  sets in &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;{&amp;#92;cal F}_1&quot; class=&quot;latex&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;This shows that a natural stability version of Frankl&amp;#8217;s conjecture is incorrect, and gives some hint on the appearance of the parameter &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cpsi&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cpsi&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cpsi&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;psi&quot; class=&quot;latex&quot; /&gt;.&lt;/p&gt;
&lt;h3&gt;Stability result&lt;/h3&gt;
&lt;p&gt;Such a stability version is correct when we replace &lt;img src=&quot;https://s0.wp.com/latex.php?latex=1%2F2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=1%2F2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1%2F2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;1/2&quot; class=&quot;latex&quot; /&gt; with &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cpsi&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cpsi&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cpsi&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;psi&quot; class=&quot;latex&quot; /&gt;. Chase and Lovett improved Gilmer&amp;#8217;s method and  proved that&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem:&lt;/strong&gt; If  &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Ccal+F&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Ccal+F&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ccal+F&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;cal F&quot; class=&quot;latex&quot; /&gt; is a &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%281-%5Cepsilon%29&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%281-%5Cepsilon%29&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%281-%5Cepsilon%29&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;(1-&amp;#92;epsilon)&quot; class=&quot;latex&quot; /&gt;-approximate union closed set system, where &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cepsilon+%3C1%2F2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cepsilon+%3C1%2F2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cepsilon+%3C1%2F2&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;epsilon &amp;lt;1/2&quot; class=&quot;latex&quot; /&gt;, then there is an element which is contained in a &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%28%5Cpsi-%5Cdelta%29&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%28%5Cpsi-%5Cdelta%29&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28%5Cpsi-%5Cdelta%29&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;(&amp;#92;psi-&amp;#92;delta)&quot; class=&quot;latex&quot; /&gt; fraction of sets in &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Ccal+F&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Ccal+F&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ccal+F&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;cal F&quot; class=&quot;latex&quot; /&gt;, where&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cdelta%3D2%5Cepsilon+%281%2B%5Cfrac+%7B%5Clog+%281%2F%5Cepsilon%29%7D%7B%5Clog+%7C%7B%5Ccal+F%7D%7C%7D%29.&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cdelta%3D2%5Cepsilon+%281%2B%5Cfrac+%7B%5Clog+%281%2F%5Cepsilon%29%7D%7B%5Clog+%7C%7B%5Ccal+F%7D%7C%7D%29.&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdelta%3D2%5Cepsilon+%281%2B%5Cfrac+%7B%5Clog+%281%2F%5Cepsilon%29%7D%7B%5Clog+%7C%7B%5Ccal+F%7D%7C%7D%29.&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;delta=2&amp;#92;epsilon (1+&amp;#92;frac {&amp;#92;log (1/&amp;#92;epsilon)}{&amp;#92;log |{&amp;#92;cal F}|}).&quot; class=&quot;latex&quot; /&gt;&lt;/p&gt;
&lt;h3&gt;An invitation for further discussion&lt;/h3&gt;
&lt;p&gt;I will be happy to see, based on Gilmer&amp;#8217;s paper and the five follow-up papers, a detailed, as simple as possible, explanation of Frankl&amp;#8217;s conjecture for the parameter &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cpsi&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cpsi&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cpsi&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;psi&quot; class=&quot;latex&quot; /&gt;, to learn what is involved in Sawin&amp;#8217;s improvement that goes beyond &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cpsi&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cpsi&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cpsi&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;psi&quot; class=&quot;latex&quot; /&gt;, and to understand the counterexamples for Gilmer&amp;#8217;s proposal towards the full conjecture, as well as thoughts, ideas and remarks of various kind on the problem.&lt;/p&gt;
&lt;h3&gt;Links to the papers&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2211.09055&quot;&gt;Justin Gilmer, A constant lower bound for the union-closed sets conjecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Will Sawin, &lt;a href=&quot;https://arxiv.org/abs/2211.11504&quot;&gt;An improved lower bound for the union-closed set conjecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; Zachary Chase, Shachar Lovett, &lt;a href=&quot;https://arxiv.org/abs/2211.11689&quot;&gt;Approximate union closed conjecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ryan Alweiss, Brice Huang, Mark Sellke, &lt;a href=&quot;https://arxiv.org/abs/2211.11731&quot;&gt;Improved Lower Bound for the Union-Closed Sets Conjecture &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;David Ellis, &lt;a href=&quot;https://arxiv.org/abs/2211.12401&quot;&gt;Note: a counterexample to a conjecture of Gilmer which would imply the union-closed conjecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Luke Pebody, &lt;a href=&quot;https://arxiv.org/abs/2211.13139&quot;&gt;Extension of a Method of Gilmer&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Gil Kalai&lt;/p&gt;
  </content>
    <author>
      <name>Gil Kalai</name>
      <uri>https://gilkalai.wordpress.com</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Hung Le: Sparsity of minor-free graphs</title>
    <link href="https://minorfree.github.io/minor-sparsity/"/>
    <id>https://minorfree.github.io/minor-sparsity</id>
    <updated>2022-11-30T00:00:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;Planar graphs are sparse: any planar graph with \(n\) vertices has at most \(3n-6\) edges. A simple corollary of this sparsity is that planar graphs are \(6\)-colorable. There is simple and beautiful proof based on the Euler formula, which can easily be exteded to bounded genus graphs, a more general case: any graph embedddable in orientable surfaces of genus \(g\) with \(n\) vertices has at most \(3n + 6g-6\) edges.&lt;/p&gt;

&lt;p&gt;How’s about the number of edges of \(K_r\)-minor-free graphs? This is a very challenging question. A reasonable speculation is \(O(r)\cdot n\): a disjoint union of \(n/(r-1)\) copies of \(K_{r-1}\) excludes a \(K_r\) minor and has \(\Theta(r)\cdot n\) edges. But this isn’t the case. And surprisingly, the correct bound is \(O(r\sqrt{\log r})n\), which will be the topic of this post.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Theorem 1:&lt;/strong&gt; Any \(K_r\)-minor-free graphs with \(n\) vertices has at most \(O(r\sqrt{\log r})\cdot n\) edges.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The bound in Theorem 1 is tight; see a lower bound  in Section 4. The sparsity bound, which is the ratio of the number of edges to the number of vertices,  \(O(r\sqrt{\log r})\) was first discovered by Kostochka [4]; the proof is quite non-trivial, so as other follow-up proofs. A short proof was just found recently by Alon, Krivelevich, and Sudakov  (AKS) [1], which I will present here in Section 3. See the bibliographical notes section for a detailed discussion of other proofs.&lt;/p&gt;

&lt;p&gt;The goal of this post isn’t just to present the proof of Theorem 1. At various points in the past, I am interested in a  more intuitive proof that gives good enough sparsity bounds, say  \(O(\mathrm{poly}(r))\), or even \(O(f(r))\) bound for any function that depends on \(r\) only. This post describes different proofs, of increasing complexity (or ingenuity), giving different bounds. In particular, sparsity bound $2^{r}$ follows by a simple induction. Sparsity bound \(O(r^2)\) relies crucially on the fact that a graph of sparsity \(d\) has a highly connected minor of small size; the high connectivity allows us to show that for any vertex subset of size \(k \approx \sqrt{d}\) has \({k \choose 2}\) internally vertex-disjoint paths connecting these vertices, thereby giving us a \(K_{\sqrt{d}}\) minor. The optimlal bound \(O(r\sqrt{r})\) also relies on a highly connected minor of small size, but employs a clever probabilistic argument to construct a \(K_{r}\) minor.&lt;/p&gt;

&lt;h1 id=&quot;1-exponential-sparsity-2r&quot;&gt;1. Exponential Sparsity: \(2^{r}\)&lt;/h1&gt;

&lt;p&gt;I  learn a beatiful proof of the following theorem in the graph theory book of Reinhard Diestel (Proposition 7.2.1. [3]).&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Theorem 2:&lt;/strong&gt; Any \(K_r\)-minor-free graphs with \(n\) vertices has at most \(2^{r-1}\cdot n\) edges.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Proof:  Let \(G\) be a \(K_r\)-minor-free graph with \(n\) vertices. The proof is by induction on \( \vert V(G) \vert  + r\). If there is a vertex \(v\) of degree at most \(2^{r-1}\), then by removing \(v\) and applying the induction hypothesis, we are done. Now consider the case where every vertex has degree more than \(2^{r-1}\); let \(v\) be such a vertex. The key idea is to find an edge \((u,v)\) such that \(u\) and \(v\) share only a few neighbors. We then contract \((u,v)\) and apply induction.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Claim 1: There is a neighbor \(u\) of \(v\) such that  \( \vert N_G(v)\cap N_G(u) \vert  \leq 2^{r-1}-1\).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Suppose that the claim holds, then let \(G’\) be the graph obtained from \(G\) by contracting \((u,v)\), i.e., \(G’= G/(u,v)\). Then \( \vert V(G’) \vert  \leq r-1\) and \( \vert E(G’) \vert  \geq  \vert E(G) \vert  - 2^{r-1}\). By induction, \( \vert E(G’) \vert  \leq 2^{r-1}(n-1)\), which implies \( \vert E(G) \vert  \leq  2^{r-1}(n-1) + 2^{r-1} = 2^{r-1}\cdot n\) as desired.&lt;/p&gt;

&lt;p&gt;We now turn to Claim 1. Let \(H = G[N_G(v)]\) be the sugraph induced by \(N_G(v)\). Then \(H\) is \(K_{r-1}\)-minor-free. By induction, \( \vert E(H) \vert  \leq 2^{r-2}\cdot  \vert V(H) \vert \).  Thus, there exists a vertex \(u\in H\) such that \(d_H(u) \leq 2 \vert E(H) \vert / \vert V(H) \vert  \leq 2^{r-1}\). This gives \( \vert N_G(v)\cap N_G(u) \vert  \leq d_H(u)-1 =  2^{r-1}-1\).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The exponential term \(2^{r-1}\) in the above theorem is due to a loss of a factor of \(2\) in each step of the induction by using \(d_H(v) \leq 2( \vert E(H) \vert / \vert V(H) \vert )\).&lt;/p&gt;

&lt;h1 id=&quot;2-polynomial-sparsity-or2&quot;&gt;2. Polynomial Sparsity: \(O(r^2)\)&lt;/h1&gt;

&lt;p&gt;The goal of this section is to improve the exponential bound in Theorem 2 to a polynomial bound.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Theorem 3:&lt;/strong&gt; Any \(K_r\)-minor-free graphs with \(n\) vertices has at most \(O(r^2)\cdot n\) edges.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Proving Theorem 3 requires a deeper understanding of the structures of minor-free graphs. The key insight is that any graph with at least \(d\cdot n\) edges has a minor, say \(K\), of \(O(d)\) size that is highly connected (Lemmas 1 and 2 below). Then we then show that given any set of vertices, say \(R\), of size about \(\Theta(\sqrt{d})\) in \(K\), we can find a collection of pairwise disjoint paths of \(H\) connecting every pair of vertices in \(R\) (due to high connectivity), which gives us a clique minor of size \(\Theta(\sqrt{d})\) (Lemma 3). Taking contrapositive gives Theorem 3.&lt;/p&gt;

&lt;p&gt;Let \(\varepsilon(G)  =  \vert E(G) \vert / \vert V(G) \vert \) and \(\delta(G)\) be the minimum degree of \(G\). Fist, we show that \(G\) contains a minor of size \(\leq 2d\) and minimum degree at least \(d\).&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Lemma 1:&lt;/strong&gt; Let \(G\) be any graph of \(n\) vertices such that \( \vert E(G) \vert \geq d\cdot n\). Then \(G\) has a minor \(H\) such that \(\delta(H)\geq d \geq  \vert V(H) \vert /2\).&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Proof: Let \(K\) be a &lt;em&gt;minimal&lt;/em&gt; minor of \(G\) such that \(\varepsilon(K)\geq d\). The minimality implies two properties:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;\( \vert N_K(u)\cap N_K(v) \vert \geq d\) for every edge \((u,v)\); otherwise, we can contract edge \((u,v)\).&lt;/li&gt;
  &lt;li&gt;There exists a vertex \(x\) such that \(d_K(x)\leq 2d\); otherwise, we can delete an edge from \(K\).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then \(H = K[N_K(x)]\) satisfying the lemma.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Next, we show that \(H\) has a highly connected subgraph.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Lemma 2:&lt;/strong&gt; Let \(H\) be such that \(\delta(H)\geq d \geq  \vert V(H) \vert /2\). Then \(H\) has a subgraph \(K\) that has (i)  \( \vert V(K) \vert \leq 2d\) and (ii) \(K\) is \(d/3\)-vertex connected.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Proof: If \(H\) is \(d/3\)-vertex connected, then \(K  = H\). Otherwise, there is a set \(S\subseteq V(H)\) of size at most \(d/3\) such that \(H\setminus S\) has at least two connected components. Let \(K\) be the smallest connected component of \(H\). Then \( \vert V(K) \vert  \leq  \vert V(H) \vert /2\leq d\) and \(\delta(K)\geq \delta(H) -  \vert S \vert  \geq 2d/3\).  Thus, for every \(u\not= v\in K\), \(u\) and \(v\) must share at least \( \vert N_K(u) \vert  +  \vert N_K(v) \vert  -  \vert V(K) \vert  \geq d/3\) neighbors in \(K\). That is, \(K\) is \(d/3\)-vertex connected.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;A detour: diameter of highly connected graphs.&lt;/strong&gt; Graph \(K\) in Lemma 2 has another nice property: its diameter is at most \(7\).  To see this, suppose that there is a shortest path  \({v_0,v_1,\ldots,v_8, \ldots}\) of length at least \(8\). Consider three vertices \(v_1,v_4, v_7\). Then \(N_{K}(v_1),N_{K}(v_4), N_{K}(v_7)\) are pairwise disjoint. As \(K\) is \(d/3\)-connected, \(\delta(K)\geq 3\). Thus, \( \vert N_{K}(v_1)\cup N_{K}(v_4) \cup N_{K}(v_7) \vert \geq 3(d/3+1) &amp;gt; d\geq V(K)\), a contradiction. We will use the same kind of arguments in several proofs below.&lt;/p&gt;

&lt;p&gt;We now construct a minor of size \(\Theta(\sqrt{d})\) for graph \(K\) in Lemma 2. We do so by showing that for any given \(p \leq d/40\) distinct pairs of vertices \({(s_1,t_1, \ldots, (s_p,t_p)}\) in \(K\) (two pairs might share the same vertex), then there are \(p\) internally vertex-disjoint paths connecting them (Lemma 3). (Two paths are internally vertex-disjoint if they can only share endpoints.)  Then one can construct a minor of size \(\sqrt{d/40}\) by picking an arbitrary set \(R\) of \(\sqrt{d/40}\) vertices, and connect all pairs of vertices in \(R\) using disjoint paths in Lemma 3, which implies Theorem 3.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/figs/paths-large.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1: (a) \(\mathcal{P}\) includes two paths of black edges. (b) deleting \(\mathcal{P}\) except \(s_1,t_1\). (c) \(v\) could not have more than 3 neighbors on the path from \(s_i\) to \(t_i\)&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Lemma 3:&lt;/strong&gt; Let \(\mathcal{T} = {(s_1,t_1), \ldots, (s_p,t_p)}\) be any \(p \leq d/40\) distinct pairs of vertices in \(K\) (in Lemma 2). Then there are \(p\) internally vertex-disjoint paths connecting the all pairs in \(\mathcal{T}\).&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Proof: Let \(\mathcal{P}\) be a set of internally vertex-disjoint paths, each of of length at most \(10\), that connects a maximal number of pairs in \(\mathcal{T}\). Subject to the pairs connected by \(\mathcal{P}\), we choose \(\mathcal{P}\) such that the total number of edges of paths in \(\mathcal{P}\) is minimal. If \(\mathcal{P}\) connects every pair, we are done. Otherwise, w.l.o.g, we assume that \(s_1\) and \(t_1\) are not connected by \(\mathcal{P}\). See Figure 1(a).&lt;/p&gt;

&lt;p&gt;Let \(K^-\) be obtained from \(K\) be removing all vertices in \(\mathcal{P}\), except \(s_1\) and \(t_1\), from \(K\). See Figure 1(b).  Observe that the total number of vertices in \(\mathcal{P}\) is at most \(11\cdot p = 11d/40 &amp;lt; d/3\). Since \(K\) is \(d/3\)-vertex-connected, \(K^-\) is connected.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Claim 2: for every \(v\in K^-\) and \(P\in \mathcal{P}\), \( \vert N_K(v)\cap V(P) \vert \leq 3\).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Suppose the claim is not true, then we can shortcut \(P\) via \(v\) to get a shorter path connecting the endpoints of \(P\), contradicting the minimality of \(\mathcal{P}\). See Figure 1(c).&lt;/p&gt;

&lt;p&gt;Note that \(\delta(K)\geq d/3\) as it is \(d/3\)-connected. By Claim 2, \(\delta(K^-)\geq \delta(K) -3\cdot p \geq d/3 - 3d/30 \geq d/4\). The same argument in the detour above implies that \(K^-\) has diameter at most \(10\). Thus, there is a path of length at most \(10\) from \(s_1\) to \(t_1\) in \(K^-\), contradicting the maximality of \(\mathcal{P}\).&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;3-optimal-sparsity-orsqrtlogr&quot;&gt;3. Optimal Sparsity: \(O(r\sqrt{\log{r}})\)&lt;/h1&gt;

&lt;p&gt;We assume that the graph has at least \(d\cdot n\) edges. Our goal is to construct a minor of size \(\Omega(d/\sqrt{\log d})\). By taking contrapositive, we obtain Theorem 1.&lt;/p&gt;

&lt;h2 id=&quot;31-proof-ideas&quot;&gt;3.1. Proof Ideas&lt;/h2&gt;

&lt;p&gt;By Lemma 2, it suffices to construct a clique minor of size \(\Omega(d/\sqrt{\log d})\) for a \(d/3\)-vertex-connected graph \(K\) with at most \(2d\) vertices. In particular, we will construct a collection of \(h = d/(c_1 \sqrt{\log d})\) vertex-disjoint connected subgraphs \(C_1,C_2,\ldots, C_h\) such that (a) there is an edge between any two subgraphs \(C_i,C_j\) for \(1\leq i\not=j \leq h\) and (b)  each \(C_i\) has \(O(c_0) \sqrt{\log d}\) vertices, for some constant \(1\ll c_0 \ll c_1\). These subgraphs will realize a \(K_h\)-minor of \(K\).&lt;/p&gt;

&lt;p&gt;The choices of constants \(c_0\) and \(c_1\) imply that \( \vert V(C_1)\cup \ldots \cup V(C_h) \vert  \leq d/12\). Thus, if we let \(H_i = K\setminus (C_1\cup \ldots \cup C_i)\) for any \(i\leq h\), then \(H_i\) is \(d/3 - d/12 = d/4\) vertex-connected. This in particular, implies that \(H_i\) has diameter at most \(22 = O(1)\).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/figs/minor-large.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2: (a) \(C_1\) forms from \(S_1\), the set of black vertices, and its bad set \(B_1\). (b) \(C_2\) is constructed from \(S_2\) (black vertices), which avoids \(B-1\), and its bad set \(B_2\). (c) \(S_i\)   has  edges to all graphs \(C_1,C_2,\ldots,C_{i-1}\).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We will construct each \(C_i\) by random sampling. To gain intuition, let’s look at the first step: (1) sampling a set \(S_1\) of \(s = c_0\sqrt{\log n}\) vertices and (2) making \(S_1\) connected by adding a shortest path from one vertex to every other vertex in \(S_1\). See Figure 2 (a). There are two good reasons for doing this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;As the graph has diameter \(O(1)\), \( \vert V(C_1) \vert  = O(c_0 \sqrt{\log d})\).&lt;/li&gt;
  &lt;li&gt;About \(e^{-O(c_0)\sqrt{\log d}}\cdot d\) vertices  are &lt;strong&gt;not dominated&lt;/strong&gt; by \(S_1\). This is because each vertex has at least \(d/3\) neighbors (as \(K\) is \(d/3\)-connected), and hence probability that a vertex is not dominated by \(S_1\) is at most \((1-d/(3\cdot 2d))^{s} = e^{-O(c_0)\sqrt{\log d}}\). Let’s call these vertices bad vertices (for a reason explained later), and denote  the set of bad vertices by \(B_1\). (A vertex is donimated by \(S_1\) if it is in \(S_1\) or adajcent to another vertex in \(S_1\).)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The second step, we sample \(S_2\) in exactly the same way and make \(C_2\) by adding paths between vertices of \(S_2\). The graph we construct \(C_2\) now is \(H_1 = K\setminus V(C_1)\), and as argue above, \(H_1\) has roughly the same properties of \(K\): \(d/4\)-vertex-connected and diameter \(O(1)\). We want \(S_2\) to contain  a vertex adjacent to \(S_1\) (in \(K\)) because we would like \(C_2\) to be adjacent to \(C_1\).  That is, we want \(S_2 \not\subseteq B_1\): we say that \(S_2\) &lt;strong&gt;avoids&lt;/strong&gt; bad set \(B_1\). See Figure 2(b). The reason 2 above implies that \(\mathrm{Pr}[S_2\subseteq B_1] \leq (e^{-O(c_0)\sqrt{\log d}})^{c_0\sqrt{\log d}} \leq 1/d^2\) for some chocie of \(c_0\gg 1\). Thus, w.h.p, \(S_2\) avoids \(B_1\).&lt;/p&gt;

&lt;p&gt;In general, at any step \(i \in [1,h]\), we already constructed a set of  \(i-1\) vertex-disjoint conected subgraphs \(C_1,C_2,\ldots C_{i-1}\), each is associated with a bad set (a set of non-neighbors). See Figure 2(c).  We want to construct \(C_i\)  by sampling a set \(S_i\) and adding paths between vertices of \(S_i\).  By the same reasoning above for \(S_2\) and using the union bound, the probability that \(S_i\) is connected to all \(i-1\) subgraphs, i.e, \(S_i\) avoids all the \((i-1)\) bad sets, is at least \(1 - d/d^2 = 1-1/d &amp;gt; 0\). When \(i = h\), we obtain a \(K_h\) minor as desired.&lt;/p&gt;

&lt;h2 id=&quot;32-the-formal-proof&quot;&gt;3.2. The formal proof&lt;/h2&gt;

&lt;p&gt;Notation: for a given set \(S\subseteq V\) in a graph \(G = (V,E)\), denoted by \(B_G(S)\) be the set of vertices not dominated by \(S\). That is, \(B_G(S) = V\setminus (S\cup(\cup_{v\in S}N_G(v)))\).&lt;/p&gt;

&lt;p&gt;We construct a set of subgraphs \(C_1,C_2,\ldots, C_h\) realizing a \(K_h\)-minor of \(K\), for \(h = d/(c_1 \sqrt{\log d})\), in \(h\) iterations as follows.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Initially, \(H_0 = K, B_0 = \emptyset\).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In \(i\)-th iteration, \(i\geq 1\), we find a set \(S_i\) of at most \(c_0\sqrt{\log d}\) vertices s.t (a) \( \vert B_{H_{i-1}}(S_i) \vert  \leq 2de^{-c_0\sqrt{\log d}/8}\) and (b) \(S_i\) is connected to each of \(C_1,C_2,\ldots,C_{i-1}\) by an edge. Next, let \(C_i\) be obtained by adding shortest paths from an arbitrary vertex \(v\in S_i\) to every other vertex in \(S_i\setminus {v}\), and \(B_i= B_{H_{i-1}}(S_i)\). Then, we define \(H_i = H_{i-1}\setminus V(C_i)\) for the next iteration.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Finally, output  \(C_1,C_2,\ldots, C_h\).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;To show the correctness of the algorithm, we only have to show that the set \(S_i\) at iteration \(i\) exists, for some choices of \(1\ll c_0 \ll c_1\). If so, \(C_1,C_2,\ldots, C_h\) form a \(K_h\)-minor of \(K\), and hence, of \(G\).&lt;/p&gt;

&lt;p&gt;First, we show that \(H_i\) has high connectivity and \(C_i\) has size \(O(c_0\sqrt{\log d})\).&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Lemma 4:&lt;/strong&gt; For every \(i\geq 1\),  \( \vert V(C_i) \vert \leq 22 c_0\sqrt{\log d}\) and  \(H_i\) is \(d/4\)-vertex connected when \(c_1 = 12c_0\).&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Proof: We prove by induction. Since \(H_{i-1}\) is \(d/4\)-connected and \( \vert V(H_i) \vert \leq 2d\), the diameter of \(H_{i-1}\) is at most \(22\). As we add at most \(c_0\sqrt{\log d}\) shortest paths to \(S_i\), \( \vert V(C_i) \vert \leq 22   c_0\sqrt{\log d}\).&lt;/p&gt;

&lt;p&gt;Observe that \(\sum_{j=1}^{i} \vert V(C_j) \vert \leq c_0\sqrt{\log d}\cdot h= c_0\sqrt{\log d} \cdot d/(c_1\sqrt{\log d}) = d/12\). Since \(K\) is \(d/3\)-vertex connected, \(H_i\) is \(d/3-d/12 = d/4\) vertex connected.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Now we show the existence of \(S_i\). Note that condition (b) is equivalent to that \(S_i\) avoids all the bad sets \(B_0,B_1,\ldots, B_{i-1}\). Let \(S_i\) be otabined by choosing each vertex of \(H_{i-1}\) with probability \(c_0\sqrt{\log d}/(2d)\); the expected size of \(S_i\) is a most \(c_0\sqrt{\log d}\). By Lemma 4, every vertex \(v\in H_{i-1}\) has degree at least \(d/4\). Thus, \(\mathrm{Pr}[v\in B_{i}]\leq (1-c_0\sqrt{\log d}/(2d))^{d/4}\sim e^{-c_0\sqrt{\log d}/8}\). In particular, \( \vert \mathbb{E}[B_i] \vert  \leq  (2d)e^{-c_0\sqrt{\log d}/8}\).&lt;/p&gt;

&lt;p&gt;It remains to show that with non-zero probability, \(S_i\) avoids all \(B_0,\ldots, B_{i-1}\). Note that \( \vert V(H_{i-1}) \vert \geq d/4\). For a fixed \(j\in [0,i-1]\):&lt;/p&gt;

&lt;p&gt;\(\mathrm{Pr}[S_i\subseteq B_j]\leq ( \vert B_j \vert / \vert V(H_{i-1}) \vert )^{ \vert S_j \vert }\leq 8(e^{-c_0\sqrt{\log d}/8})^{c_0\sqrt{\log d}}= 8e^{-c_0^2 \log(d)/8} \leq 1/d^2\)&lt;/p&gt;

&lt;p&gt;for a sufficiently large \(c_0\geq 1\).&lt;/p&gt;

&lt;p&gt;By union bound, the probability that \(\mathrm{Pr}[S_i\subseteq B_j]\) for some \(j\in [0,i-1]\) is at most \(h/d^2\leq 1/d\). Thus, the probability that \(S_i\) avoids all \(B_j\) is at least \(1-1/d\). This conclude the proof.&lt;/p&gt;

&lt;h1 id=&quot;4-a-lower-bound&quot;&gt;4. A Lower Bound&lt;/h1&gt;

&lt;p&gt;In this section, we show that for any \(n\) and \(r\) such that \(n \gg r\sqrt{\log r}\), there exists a graph \(G\) with \(n\) vertices and \(\Theta(n\cdot r\sqrt{\log r})\) edges such that \(G\) has no \(K_{r}\) minor. The key idea of the construction is Theorem 4 below.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Theorem 4:&lt;/strong&gt; There exists a graph \(H\) with \(k\) vertices and \(\Theta(k^2)\) edges such that \(H\) has no \(K_s\)-minor where \(s = k/(\epsilon\sqrt{\log k})\) for some constant \(\epsilon\in (0,1)\).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Theorem 4 implies a sparsity lower bound \(\Omega(r\sqrt{\log r})\) as follows. Let \(G\) be the disjoint union of \(\Theta(n/(r\sqrt{\log r}))\) copies of the same graph in Theorem 4 with \(k = \Theta(r\sqrt{\log r})\) vertices. Then \( \vert E(G) \vert  = \Theta(n/(r\sqrt{\log r}))k^2 = \Theta(n\cdot r\sqrt{\log r})\). As \(H\) excludes a clique minor of size \(k/(\epsilon\sqrt{\log k}) \leq r\) (by choosing the constant in the definition of \(k\) appropriately), \(G\) excludes \(K_r\) as a minor.&lt;/p&gt;

&lt;p&gt;Theorem 4 can be proven by the probabilistic method. To gain some intuition of the proof, consider any fixed partition of \(V(H)\) into vertex-disjont subsets \({V_1,V_2,\ldots, V_{s}}\) of size \(\epsilon \sqrt{\log k}\) each. For this partition to realize a \(K_s\) minor, there must be an edge between every two vertex sets \(V_i,V_j\) for \(i\not=j\). The probability of this is:&lt;/p&gt;

\[(1-2^{-|V_i||V_j|})^ = (1-2^{-\epsilon ^2 \log(k)})^  \approx e^{-k^{2 - \epsilon^2}/\log(k)}\]

&lt;p&gt;By the union bound over at most \(k^k\) such partitions, the probability of having a  \(K_s\) minor is at most  \(k^k e^{-k^{2 - \epsilon^2}/\log(k)} \rightarrow 0 \) when \(k \rightarrow +\infty\). In other words, the probability of not having a \(K_{s}\) minor is close to \(1\).&lt;/p&gt;

&lt;p&gt;In the formal proof, one has to work with the fact that the subsets in the partition might not have the same size; this can be resolved by simple algebraic manipulation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof of Theorem 4.&lt;/strong&gt;  Let \(H = G(k,1/2)\) where \(G(k,1/2)\) is the Erdős–Rényi random graph with probability \(p = 1/2\). We now show that the probability that \(H\) contains a \(K_s\) minor tends to \(0\) when \(k\rightarrow \infty\).&lt;/p&gt;

&lt;p&gt;Recall that \(K_s\) is a minor of \(H\) if there is a set of non-empty, connected, and vertex-disjoint subgraphs \(\mathcal{C} = {C_1,C_2,\ldots, C_s}\) such that there is an edge in \(H\) connecting every two graphs \(C_i,C_j\) for \(1\leq i\not=j \leq s\).&lt;/p&gt;

&lt;p&gt;We will bound the probability of exsiting such \(\mathcal{C}\). Observe that the number of (ordered) partitions of \( \vert V(H) \vert \) into \(s\) non-empty subset  is at most:&lt;/p&gt;

\[\frac{k!}{s!}{k-1\choose s-1} &amp;lt;k^k\]

&lt;p&gt;Fixed such a partition of  \( \vert V(H) \vert \), denoted by \(\mathcal{P}\). Let \(n_i\) be the number of vertices in \(i\)-th set. The probability that there is an edge betwen two different sets \(i\) and \(j\) is \((1-2^{-n_i\cdot n_j})\). Thus, probability of having an edge between any two different sets of \(\mathcal{P}\) is:&lt;/p&gt;

\[\prod_{(i,j)}(1-2^{-n_i\cdot n_j}) \leq \prod_{(i,j)}e^{-2^{-n_i\cdot n_j}} = e^{-\sum_{(i,j)}2^{-n_i\cdot n_j}}\]

&lt;p&gt;where the product and sum is over all unordered pairs \((i,j)\). This implies that:&lt;/p&gt;

\[\mathrm{Pr}[\mathcal{C} \text{ exists}] \leq k^k \cdot e^{-\sum_{(i,j)}2^{-n_i\cdot n_j}}\]

&lt;p&gt;We now estimate \(\sum_{(i,j)}2^{-n_i\cdot n_j}\). By arithmetic–geometric mean inequality,
 \(\sum_{(i,j)}2^{-n_i\cdot n_j}\geq {s \choose 2}\left(\prod_{(i,j)}2^{-n_i\cdot n_j}\right)^{1/{s\choose 2}} \geq {s \choose 2} \left(2^{-\sum_{(i,j)}n_i\cdot n_j}\right)^{1/{s\choose 2}}\)&lt;/p&gt;

&lt;p&gt;Observe that \(\sum_{(i,j)}n_i\cdot n_j\) is the number of edges in a complete s-partite graph with \(k\) vertices. Thus,  \(\sum_{(i,j)}n_i\cdot n_j\leq k^2/2\) and hence:&lt;/p&gt;

\[\sum_{(i,j)}2^{-n_i\cdot n_j} \geq {s \choose 2} 2^{-k^2/s^2}\]

&lt;p&gt;Thus,&lt;/p&gt;

\[\mathrm{Pr}[\mathcal{C} \text{ exists}] \leq k^k \cdot e^{- {s \choose 2} 2^{-k^2/s^2}}\]

&lt;p&gt;By chooosing \(s = ck/(\sqrt{\log k})\) for some big enough constant \(c\), we have \(\mathrm{Pr}[\mathcal{C}  \text{ exists}] \rightarrow 0\) when \(k\rightarrow \infty\).&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;bibliographical-notes&quot;&gt;Bibliographical Notes&lt;/h1&gt;

&lt;p&gt;The exponential sparsity bound in Section is due to Reinhard Diestel (Proposition 7.2.1. [3]). The \(O(r^2)\) sparsity bound in Section 2 is obtained by a combination of various ideas, in particular, Lemma 1 is due to Exercise 21, Chapter 7, in [3], Lemma 2 is from the proof of Theorem 1 in [1], and Lemma 3 is a modification of Lemma 3.5.4 in [3].&lt;/p&gt;

&lt;p&gt;Mader [5] proved a sparstiy bound \(O(r\log(r))\) for \(K_r\)-minor-free graphs. Kostacha was the first to show that the sparsity is \(\Theta(r\sqrt{\log r})\). Thomason [6] provided a more refined range for the  sparsity bound:  \([0.265r\sqrt{\log_2 r}(1+o(1)), 0.268r\sqrt{\log_2 r}(1+o(1))]\). The bound then was tightened &lt;strong&gt;exactly&lt;/strong&gt; to \((\alpha +o(1))r\sqrt{\ln(r)}\) where \(\alpha = 0.319…\) is an explcit constant, also by Thomason [7]. The simpler proof presented in Section 3 is due to Alon, Krivelevich, and Sudakov [1].&lt;/p&gt;

&lt;p&gt;The lower bound in Section 4 is due to Bollobás, Catlin, and Erdös [2].&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;p&gt;[1]  Alon, N., Krivelevich, M., and Sudakov, B. (2022). &lt;em&gt;Complete minors and average degree–a short proof&lt;/em&gt;. ArXiv preprint &lt;a href=&quot;https://arxiv.org/abs/2202.08530&quot;&gt;arXiv:2202.08530&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] Bollobás, B., Catlin, P. A., and Erdös, P. (1980). &lt;em&gt;Hadwiger’s conjecture is true for almost every graph&lt;/em&gt;. Eur. J. Comb., 1(3), 195-199.&lt;/p&gt;

&lt;p&gt;[3] Diestel, R. (2017). Graph theory. Springer.&lt;/p&gt;

&lt;p&gt;[4]  Kostochka, A. V. (1982). &lt;em&gt;A lower bound for the Hadwiger number of a graph as a function of the average degree of its vertices&lt;/em&gt;. Discret. Analyz. Novosibirsk, 38, 37-58.&lt;/p&gt;

&lt;p&gt;[5] Mader, W. (1968). &lt;em&gt;Homomorphiesätze für graphen&lt;/em&gt;. Mathematische Annalen, 178(2), 154-168.&lt;/p&gt;

&lt;p&gt;[6] Thomason, A. (1984). &lt;em&gt;An extremal function for contractions of graphs&lt;/em&gt;. In Mathematical Proceedings of the Cambridge Philosophical Society (Vol. 95, No. 2, pp. 261-265). Cambridge University Press.&lt;/p&gt;

&lt;p&gt;[7] Thomason, A. (2001). &lt;em&gt;The extremal function for complete minors&lt;/em&gt;. Journal of Combinatorial Theory, Series B, 81(2), 318-338.&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By Hung Le&lt;/p&gt;
  </content>
    <author>
      <name>Hung Le</name>
      <uri>https://minorfree.github.io</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">TCS+ Seminar Series: TCS+ talk: Wednesday, November 30 — Nicole Wein, DIMACS (Reminder)</title>
    <link href="https://tcsplus.wordpress.com/2022/11/29/tcs-talk-wednesday-november-30-nicole-wein-dimacs-reminder/"/>
    <id>http://tcsplus.wordpress.com/?p=655</id>
    <updated>2022-11-29T21:40:30+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;A reminder that the next TCS+ talk will take place this coming Wednesday, November 30th &lt;strong&gt;(tomorrow!)&lt;/strong&gt; at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). &lt;a href=&quot;http://people.csail.mit.edu/nwein/&quot;&gt;&lt;strong&gt;Nicole Wein&lt;/strong&gt;&lt;/a&gt; from DIMACS will speak about &amp;#8220;&lt;em&gt;Online List Labeling: Breaking the &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Clog%5E2+n&amp;#038;bg=fff&amp;#038;fg=444444&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Clog%5E2+n&amp;#038;bg=fff&amp;#038;fg=444444&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clog%5E2+n&amp;#038;bg=fff&amp;#038;fg=444444&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;log^2 n&quot; class=&quot;latex&quot; /&gt; Barrier&lt;/em&gt;&amp;#8221; (abstract below). &lt;/p&gt;



&lt;p&gt;The perfect way to ease back into it, after the Thanksgiving week!&lt;/p&gt;



&lt;p&gt;You can &lt;strong&gt;reserve a spot&lt;/strong&gt; as an individual or a group to join us live by signing up on &lt;a href=&quot;https://sites.google.com/view/tcsplus/welcome/next-tcs-talk&quot;&gt;the online form&lt;/a&gt;. Registration is &lt;em&gt;not&lt;/em&gt; required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. &lt;/p&gt;



&lt;p&gt;&lt;a href=&quot;https://tcsplus.wordpress.com/2022/11/23/tcs-talk-wednesday-november-30-nicole-wein-dimacs/&quot;&gt;More details on Nicole&amp;#8217;s talk can be found here.&lt;/a&gt;&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By plustcs&lt;/p&gt;
  </content>
    <author>
      <name>TCS+ Seminar Series</name>
      <uri>https://tcsplus.wordpress.com</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">CS Theory Events: Workshop on Algebraic Complexity Theory</title>
    <link href="https://cstheory-events.org/2022/11/29/workshop-on-algebraic-complexity-theory-2/"/>
    <id>http://cstheory-events.org/2022/11/29/workshop-on-algebraic-complexity-theory-2/</id>
    <updated>2022-11-29T18:05:11+00:00</updated>
    <content type="html" xml:lang="en">
    March 27-31, 2023 University of Warwick, Coventry, UK https://www.dcs.warwick.ac.uk/~u2270030/wact Algebraic Complexity Theory is a vibrant field that has been seeing a tremendous amount of activity in the recent years. Its classical questions have been interwoven with deep questions from algebraic geometry, invariant theory, and representation theory. Researchers study a wide range of interlinked topics: arithmetic &amp;#8230; &lt;a href=&quot;https://cstheory-events.org/2022/11/29/workshop-on-algebraic-complexity-theory-2/&quot; class=&quot;more-link&quot;&gt;Continue reading &lt;span class=&quot;screen-reader-text&quot;&gt;Workshop on Algebraic Complexity&amp;#160;Theory&lt;/span&gt;&lt;/a&gt;&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </content>
    <author>
      <name>CS Theory Events</name>
      <uri>https://cstheory-events.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">CCI: jobs: Faculty at Department of Computer Science at Reykjavik University (apply by January 27, 2023)</title>
    <link href="https://cstheory-jobs.org/2022/11/29/faculty-at-department-of-computer-science-at-reykjavik-university-apply-by-january-27-2023/"/>
    <id>http://cstheory-jobs.org/2022/11/29/faculty-at-department-of-computer-science-at-reykjavik-university-apply-by-january-27-2023/</id>
    <updated>2022-11-29T09:23:46+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;We invite applications for two full-time, permanent faculty positions at any rank in the fields of Artificial Intelligence, Cybersecurity, Data Science and Machine Learning, Software Engineering, and Theoretical Computer Science. For one of the positions, we will give preferential treatment to excellent applicants in Software Engineering, broadly construed.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://jobs.50skills.com/ru/en/16728&quot;&gt;https://jobs.50skills.com/ru/en/16728&lt;/a&gt;&lt;br /&gt;
Email: mariaoskars@ru.is&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </content>
    <author>
      <name>CCI: jobs</name>
      <uri>https://cstheory-jobs.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Scott Aaronson: My AI Safety Lecture for UT Effective Altruism</title>
    <link href="https://scottaaronson.blog/?p=6823"/>
    <id>https://scottaaronson.blog/?p=6823</id>
    <updated>2022-11-29T05:50:03+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;Two weeks ago, I gave a lecture setting out my current thoughts on AI safety, halfway through my year at OpenAI.  I was asked to speak by UT Austin&amp;#8217;s Effective Altruist club.  You can &lt;a href=&quot;https://www.youtube.com/watch?v=fc-cHk9yFpg&quot;&gt;watch the lecture on YouTube here&lt;/a&gt; (I recommend 2x speed).&lt;/p&gt;



&lt;p&gt;The timing turned out to be weird, coming immediately after the worst disaster to hit the Effective Altruist movement in its history, as I acknowledged in the talk.  But I plowed ahead anyway, to discuss:&lt;/p&gt;



&lt;ol&gt;
&lt;li&gt;the current state of AI scaling, and why many people (even people who agree about little else!) foresee societal dangers,&lt;/li&gt;



&lt;li&gt;the different branches of the AI safety movement,&lt;/li&gt;



&lt;li&gt;the major approaches to aligning a powerful AI that people have thought of, and&lt;/li&gt;



&lt;li&gt;what projects I specifically have been working on at OpenAI. &lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;I then spent 20 minutes taking questions.&lt;/p&gt;



&lt;p&gt;For those who (like me) prefer text over video, below I&amp;#8217;ve produced an edited transcript, by starting with YouTube&amp;#8217;s automated transcript and then, well, editing it.  Enjoy!  &amp;#8211;SA&lt;/p&gt;



&lt;hr class=&quot;wp-block-separator has-alpha-channel-opacity&quot;/&gt;



&lt;p&gt;Thank you so much for inviting me here.  I do feel a little bit sheepish to be lecturing you about AI safety, as someone who&amp;#8217;s worked on this subject for all of five months.  I&amp;#8217;m a quantum computing person.  But this past spring, I accepted an extremely interesting opportunity to go on leave for a year to think about what theoretical computer science can do for AI safety.  I&amp;#8217;m doing this at &lt;a href=&quot;https://openai.com/&quot;&gt;OpenAI&lt;/a&gt;, which is one of the world&amp;#8217;s leading AI startups, based in San Francisco although I&amp;#8217;m mostly working from Austin.&lt;/p&gt;



&lt;p&gt;Despite its name, OpenAI is famously &lt;em&gt;not&lt;/em&gt; 100% open &amp;#8230; so there are certain topics that I&amp;#8217;m not allowed to talk about, like the capabilities of the very latest systems and whether or not they&amp;#8217;ll blow people&amp;#8217;s minds when released.  By contrast, OpenAI is very happy for me to talk about &lt;em&gt;AI safety&lt;/em&gt;: what it is and and what if anything can we do about it.  So what I thought I&amp;#8217;d do is to tell you a little bit about the specific projects that I&amp;#8217;ve been working on at OpenAI, but also just, as an admitted newcomer, share some general thoughts about AI safety and how Effective Altruists might want to think about it.  I&amp;#8217;ll try to leave plenty of time for discussion.&lt;/p&gt;



&lt;p&gt;Maybe I should mention that the thoughts that I&amp;#8217;ll tell you today are ones that, until last week, I had considered writing up for an essay contest run by something called the FTX Future Fund.  Unfortunately, the FTX Future Fund no longer exists.  It was founded by someone named Sam Bankman-Fried, whose a net worth went from 15 billion dollars to some negative number of dollars in the space of two days, in one of the biggest financial scandals in memory.  This is obviously a calamity for the EA community, which had been counting on funding from this individual.  I feel terrible about all the projects left in the lurch, to say nothing of FTX&amp;#8217;s customers.&lt;/p&gt;



&lt;p&gt;As a tiny silver lining, though, instead of writing up my thoughts for that essay contest, I&amp;#8217;ll just share them with you right now, for free!&lt;/p&gt;



&lt;hr class=&quot;wp-block-separator has-alpha-channel-opacity&quot;/&gt;



&lt;h2&gt;&lt;strong&gt;The Scaling of AI&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Let&amp;#8217;s start with this: raise your hand if you&amp;#8217;ve tried &lt;a href=&quot;https://www.newyorker.com/culture/culture-desk/the-new-poem-making-machinery&quot;&gt;GPT-3&lt;/a&gt;.  That&amp;#8217;s maybe half of you.  OK, raise your hand if you&amp;#8217;ve tried &lt;a href=&quot;https://en.wikipedia.org/wiki/DALL-E&quot;&gt;DALL-E&lt;/a&gt;.  That&amp;#8217;s again maybe half of you.&lt;/p&gt;



&lt;p&gt;These are the two best-known products that are made by OpenAI, and as I think most people would agree, two of the most impressive AIs that exist in the world right now.  They certainly go far beyond what &lt;em&gt;I&lt;/em&gt; would&amp;#8217;ve predicted would be possible now, if you&amp;#8217;d asked me 10 years ago or even 5 years ago.&lt;/p&gt;



&lt;p&gt;And whenever I try to explain them to people, I&amp;#8217;m, like, well, you have to &lt;em&gt;see&lt;/em&gt; them.  No abstract description can substitute in this case.&lt;/p&gt;



&lt;p&gt;All right, so here&amp;#8217;s what GPT-3 produced when a &lt;em&gt;New Yorker&lt;/em&gt; writer asked it to write a poem about cryptocurrency in the style of Philip Larkin, who was a famous 20th-century poet.  The subject seems particularly appropriate given current events.&lt;/p&gt;



&lt;blockquote class=&quot;wp-block-quote&quot;&gt;
&lt;p&gt;&lt;strong&gt;The Invention&amp;nbsp; (by GPT-3)&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Money is a thing you earn by the sweat of your brow&lt;br&gt;And that’s how it should be.&lt;br&gt;Or you can steal it, and go to jail;&lt;br&gt;Or inherit it, and be set for life;&lt;br&gt;Or win it on the pools, which is luck;&lt;br&gt;Or marry it, which is what I did.&lt;br&gt;And that is how it should be, too.&lt;br&gt;But now this idea’s come up&lt;br&gt;Of inventing money, just like that.&lt;br&gt;I ask you, is nothing sacred?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;



&lt;p&gt;Okay, it won&amp;#8217;t always produce something of this quality (incidentally, I don&amp;#8217;t think GPT-3 actually &amp;#8220;married money&amp;#8221;!).  Often you&amp;#8217;ve got to run it several times and take the best output&amp;#8212;much like human poets presumably do, throwing crumpled pages into the basket.  But I submit that, if the above hadn&amp;#8217;t been labeled as coming from GPT, you&amp;#8217;d be like, yeah, that&amp;#8217;s the kind of poetry the &lt;em&gt;New Yorker&lt;/em&gt; publishes, right?  This is a thing that AI can now do.&lt;/p&gt;



&lt;p&gt;So what &lt;em&gt;is&lt;/em&gt; GPT?  It&amp;#8217;s a text model.  It&amp;#8217;s basically a gigantic neural network with about 175 billion parameters&amp;#8212;the weights.  It&amp;#8217;s a particular kind of neural net called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&quot;&gt;transformer model&lt;/a&gt; that was invented five years ago.  It&amp;#8217;s been trained on a large fraction of all the text on the open Internet.  The training simply consists of playing the following game over and over, trillions of times: &lt;em&gt;predict which word comes next in this text string.&lt;/em&gt;  So in some sense that&amp;#8217;s its only goal or intention in the world: to predict the next word.&lt;/p&gt;



&lt;p&gt;The amazing discovery is that, when you do that, you end up with something where you can then ask it a question, or give it a a task like writing an essay about a certain topic, and it will say &amp;#8220;oh! I know what would plausibly come after that prompt!  The answer to the question!  Or the essay itself!&amp;#8221;  And it will then proceed to generate the thing you want.&lt;/p&gt;



&lt;p&gt;GPT can solve high-school-level math problems that are given to it in English.  It can reason you through the steps of the answer.  It&amp;#8217;s starting to be able to do nontrivial math competition problems.  It&amp;#8217;s on track to master basically the whole high school curriculum, maybe followed soon by the whole undergraduate curriculum.&lt;/p&gt;



&lt;p&gt;If you turned in GPT&amp;#8217;s essays, I &lt;em&gt;think&lt;/em&gt; they&amp;#8217;d get at least a B in most courses.  Not that I endorse any of you doing that!!  We&amp;#8217;ll come back to that later.  But yes, we &lt;em&gt;are&lt;/em&gt; about to enter a world where students everywhere will at least be sorely tempted to use text models to write their term papers.  That&amp;#8217;s just a tiny example of the societal issues that these things are going to raise.&lt;/p&gt;



&lt;p&gt;Speaking personally, the last time I had a similar feeling was when I was an adolescent in 1993 and I saw this niche new thing called the World Wide Web, and I was like &amp;#8220;why isn&amp;#8217;t &lt;em&gt;everyone&lt;/em&gt; using this?  why isn&amp;#8217;t it changing the world?&amp;#8221;  The answer, of course, was that within a couple years it would.&lt;/p&gt;



&lt;p&gt;Today, I feel like the world was understandably preoccupied by the pandemic, and by everything else that&amp;#8217;s been happening, but these past few years might actually be remembered as the time when AI underwent this step change.  I didn&amp;#8217;t predict it.  I think even many computer scientists might still be in denial about what&amp;#8217;s now possible, or what&amp;#8217;s happened.  But I&amp;#8217;m now thinking about it even in terms of my two kids, of what kinds of careers are going to be available when they&amp;#8217;re older and entering the job market.  For example, I would probably &lt;em&gt;not&lt;/em&gt; urge my kids to go into commercial drawing!&lt;/p&gt;



&lt;p&gt;Speaking of which, OpenAI&amp;#8217;s &lt;em&gt;other&lt;/em&gt; main product is DALL-E2, an image model.  Probably most of you have already seen it, but you can ask it&amp;#8212;for example, just this morning I asked it, show me some digital art of two cats playing basketball in outer space.  That&amp;#8217;s not a problem for it.&lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-large&quot;&gt;&lt;img decoding=&quot;async&quot; src=&quot;https://www.scottaaronson.com/twocats.jpg&quot; alt=&quot;&quot;/&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p&gt;You may have seen that there&amp;#8217;s a different image model called Midjourney which won an art contest with this piece:&lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-large&quot;&gt;&lt;img decoding=&quot;async&quot; src=&quot;https://www.scottaaronson.com/midjourney.jpg&quot; alt=&quot;&quot;/&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p&gt;It seems like the judges didn&amp;#8217;t completely understand, when this was submitted as &amp;#8220;digital art,&amp;#8221; what exactly that meant&amp;#8212;that the human role was mostly limited to entering a prompt!  But the judges then said that even having understood it, they still would&amp;#8217;ve given the award to this piece.  I mean, it&amp;#8217;s a striking piece, isn&amp;#8217;t it?  But of course it raises the question of how much work there&amp;#8217;s going to be for contract artists, when you have entities like this.&lt;/p&gt;



&lt;p&gt;There are already companies that are using GPT to write ad copy.  It&amp;#8217;s already being used at the, let&amp;#8217;s call it, lower end of the book market.  For any kind of formulaic genre fiction, you can say, &amp;#8220;just give me a few paragraphs of description of this kind of scene,&amp;#8221; and it can do that.  As it improves you could you can imagine that it will be used more.&lt;/p&gt;



&lt;p&gt;Likewise, DALL-E and other image models have already changed the way that people generate art online.  And it&amp;#8217;s only been a few months since these models were released!  That&amp;#8217;s a striking thing about this era, that a few months can be an eternity.  So when we&amp;#8217;re thinking about the impacts of these things, we have to try to take what&amp;#8217;s happened in the last few months or years and project that five years forward or ten years forward.&lt;/p&gt;



&lt;p&gt;This brings me to the obvious question: what happens as you continue scaling further?  I mean, these spectacular successes of deep learning over the past decade have owed &lt;em&gt;something&lt;/em&gt; to new ideas&amp;#8212;ideas like transformer models, which I mentioned before, and others&amp;#8212;but famously, they have owed maybe more than anything else to sheer scale.&lt;/p&gt;



&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Neural_network&quot;&gt;Neural networks&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Backpropagation&quot;&gt;backpropagation&lt;/a&gt;&amp;#8212;which is how you train the neural networks&amp;#8212;these are ideas that have been around for decades.  When I studied CS in the 90s, they were already extremely well-known.  But it was &lt;em&gt;also&lt;/em&gt; well-known that they didn&amp;#8217;t work all that well!  They only worked somewhat.  And usually, when you take something that doesn&amp;#8217;t work and multiply it by a million, you just get a million times something that doesn&amp;#8217;t work, right?&lt;/p&gt;



&lt;p&gt;I remember at the time, &lt;a href=&quot;https://en.wikipedia.org/wiki/Ray_Kurzweil&quot;&gt;Ray Kurzweil&lt;/a&gt;, the futurist, would keep showing these graphs that look like this: &lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-large&quot;&gt;&lt;img decoding=&quot;async&quot; src=&quot;https://www.scottaaronson.com/mooreslaw.jpg&quot; alt=&quot;&quot;/&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p&gt;So, he would plot &lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%27s_law&quot;&gt;Moore&amp;#8217;s Law&lt;/a&gt;, the increase in transistor density, or in this case the number of floating-point operations that you can do per second for a given cost.  And he&amp;#8217;d point out that it&amp;#8217;s on this clear exponential trajectory.&lt;/p&gt;



&lt;p&gt;And he&amp;#8217;d then try to compare that to some crude estimates of the number of computational operations that are done in the brain of a mosquito or a mouse or a human or all the humans on Earth.  And oh!  We see that in a matter of a couple decades, like by the year 2020 or 2025 or so, we&amp;#8217;re going to start passing the human brain&amp;#8217;s computing power and then we&amp;#8217;re going to keep going beyond that.  And so, Kurzweil would continue, we should assume that scale will just kind of magically make AI work.  You know, that once you have enough computing cycles, you just sprinkle them around like pixie dust, and suddenly human-level intelligence will just emerge out of the billions of connections.&lt;/p&gt;



&lt;p&gt;I remember thinking: &lt;em&gt;that sounds like the stupidest thesis I&amp;#8217;ve ever heard.&lt;/em&gt;  Right?  Like, he has absolutely no reason to believe such a thing is true or have any confidence in it.  Who the hell knows what will happen?  We might be missing crucial insights that are needed to make AI work.&lt;/p&gt;



&lt;p&gt;Well, here we are, and it turns out he was way more right than most of us expected.&lt;/p&gt;



&lt;p&gt;As you all know, a central virtue of Effective Altruists is updating based on evidence.  I think that we&amp;#8217;re forced to do that in this case.&lt;/p&gt;



&lt;p&gt;To be sure, it&amp;#8217;s still unclear how much further you&amp;#8217;ll get just from pure scaling.  That remains a central open question.  And there are still prominent skeptics.&lt;/p&gt;



&lt;p&gt;Some skeptics take the position that this is &lt;em&gt;clearly&lt;/em&gt; going to hit some kind of wall before it gets to true human-level understanding of the real world.  They say that text models like GPT are really just &lt;a href=&quot;https://s10251.pcdn.co/pdf/2021-bender-parrots.pdf&quot;&gt;&amp;#8220;stochastic parrots&amp;#8221;&lt;/a&gt; that regurgitate their training data.  That despite creating a remarkable illusion otherwise, they don&amp;#8217;t &lt;em&gt;really&lt;/em&gt; have any original thoughts.&lt;/p&gt;



&lt;p&gt;The proponents of that view sometimes like to gleefully point out examples where GPT will flub some commonsense question.  If you look for such examples, you can certainly find them!  One of my favorites recently was, &amp;#8220;which would win in a race, a four-legged zebra or a two-legged cheetah?&amp;#8221;  GPT-3, it turns out, is very confident that the cheetah will win.  Cheetahs are faster, right?&lt;/p&gt;



&lt;p&gt;Okay, but one thing that&amp;#8217;s been found empirically is that you take commonsense questions that are flubbed by GPT-2, let&amp;#8217;s say, and you try them on GPT-3, and very often now it gets them right.  You take the things that the original GPT-3 flubbed, and you try them on the latest public model, which is sometimes called GPT-3.5 (incorporating an advance called InstructGPT), and again it often gets them right.  So it&amp;#8217;s &lt;em&gt;extremely&lt;/em&gt; risky right now to pin your case against AI on these sorts of examples!  Very plausibly, just one more order of magnitude of scale is all it&amp;#8217;ll take to kick the ball in, and then you&amp;#8217;ll have to move the goal again.&lt;/p&gt;



&lt;p&gt;A deeper objection is that the &lt;em&gt;amount of training data&lt;/em&gt; might be a fundamental bottleneck for these kinds of machine learning systems&amp;#8212;and we&amp;#8217;re already running out of Internet to to train these models on!  Like I said, they&amp;#8217;ve already used most of the public text on the Internet.  There&amp;#8217;s still all of YouTube and TikTok and Instagram that hasn&amp;#8217;t yet been fed into the maw, but it&amp;#8217;s not clear that that would actually make an AI smarter rather than dumber!  So, you can look for more, but it&amp;#8217;s not clear that there are orders of magnitude more that humanity has even produced and that&amp;#8217;s readily accessible.&lt;/p&gt;



&lt;p&gt;On the other hand, it&amp;#8217;s also been found empirically that very often, you can do better with the same training data just by spending more compute.  You can squeeze the lemon harder and get more and more generalization power from the same training data by doing more gradient descent.&lt;/p&gt;



&lt;p&gt;In summary, we don&amp;#8217;t know how far this is going to go.  But it&amp;#8217;s &lt;em&gt;already&lt;/em&gt; able to automate various human professions that you might not have predicted would have been automatable by now, and we shouldn&amp;#8217;t be confident that many more professions will not become automatable by these kinds of techniques.&lt;/p&gt;



&lt;p&gt;Incidentally, there&amp;#8217;s a famous irony here.  If you had asked anyone in the 60s or 70s, they would have said, well clearly first robots will replace humans for manual labor, and &lt;em&gt;then&lt;/em&gt; they&amp;#8217;ll replace humans for intellectual things like math and science, and &lt;em&gt;finally&lt;/em&gt; they might reach the pinnacles of human creativity like art and poetry and music.&lt;/p&gt;



&lt;p&gt;The truth has turned out to be the exact opposite.  I don&amp;#8217;t think anyone predicted that.&lt;/p&gt;



&lt;p&gt;GPT, I think, is already a pretty good poet.  DALL-E is already a pretty good artist.  They&amp;#8217;re still struggling with some high school and college-level math but they&amp;#8217;re getting there.  It&amp;#8217;s easy to imagine that maybe in five years, people like me will be using these things as research assistants&amp;#8212;at the very least, to prove the lemmas in our papers.  That seems &lt;em&gt;extremely&lt;/em&gt; plausible.&lt;/p&gt;



&lt;p&gt;What&amp;#8217;s been by far the hardest is to get AI that can robustly interact with the physical world.  Plumbers, electricians&amp;#8212;these might be some of the &lt;em&gt;last&lt;/em&gt; jobs to be automated.  And famously, self-driving cars have taken a lot longer than many people expected a decade ago.  This is partly because of regulatory barriers and public relations: even if a self-driving car actually crashes &lt;em&gt;less&lt;/em&gt; than a human does, that&amp;#8217;s still not good enough, because when it &lt;em&gt;does&lt;/em&gt; crash the circumstances are too weird.  So, the AI is actually held to a higher standard.  But it&amp;#8217;s also partly just that there was a long tail of really weird events.  A deer crosses the road, or you have some crazy lighting conditions&amp;#8212;such things are really hard to get right, and of course 99% isn&amp;#8217;t good enough here.&lt;/p&gt;



&lt;p&gt;We can maybe fuzzily see ahead at least a decade or two, to when we have AIs that can at the least help us enormously with scientific research and things like that.  Whether or not they&amp;#8217;ve totally replaced us&amp;#8212;and I selfishly hope not, although I do have tenure so there&amp;#8217;s that&amp;#8212;why does it stop there?  Will these models eventually match or exceed human abilities across basically all domains, or at least all intellectual ones?  If they do, what will humans still be good for?  What will be our role in the world?  And then we come to the question, well, will the robots eventually rise up and decide that whatever objective function they were given, they can maximize it better without us around, that they don&amp;#8217;t need us anymore?&lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-large&quot;&gt;&lt;img decoding=&quot;async&quot; src=&quot;https://www.scottaaronson.com/terminator.jpg&quot; alt=&quot;&quot;/&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p&gt;This has of course been a trope of many, &lt;em&gt;many&lt;/em&gt; science-fiction works.  The funny thing is that there are thousands of short stories, novels, movies, that have tried to map out the possibilities for where we&amp;#8217;re going, going back at least to Asimov and his &lt;a href=&quot;https://en.wikipedia.org/wiki/Three_Laws_of_Robotics&quot;&gt;Three Laws of Robotics&lt;/a&gt;, which was maybe the first AI safety idea, if not earlier than that.  The trouble is, we don&amp;#8217;t know &lt;em&gt;which&lt;/em&gt; science-fiction story will be the one that will have accurately predicted the world that we&amp;#8217;re creating.  Whichever future we end up in, with hindsight, people will say, &lt;em&gt;this&lt;/em&gt; obscure science fiction story from the 1970s called it exactly right, but we don&amp;#8217;t know which one yet!&lt;/p&gt;



&lt;hr class=&quot;wp-block-separator has-alpha-channel-opacity&quot;/&gt;



&lt;h2&gt;&lt;strong&gt;What Is AI Safety?&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;So, the rapidly-growing field of AI safety.  People use different terms, so I want to clarify this a little bit.  To an outsider hearing the terms &amp;#8220;AI safety,&amp;#8221; &amp;#8220;AI ethics,&amp;#8221; &amp;#8220;AI alignment,&amp;#8221; they all sound like kind of synonyms, right?  It turns out, and this was one of the things I had to learn going into this, that AI ethics and AI alignment are two communities that despise each other.  It&amp;#8217;s like the &lt;a href=&quot;https://www.youtube.com/watch?v=WboggjN_G-4&quot;&gt;People&amp;#8217;s Front of Judea versus the Judean People&amp;#8217;s Front&lt;/a&gt; from Monty Python.&lt;/p&gt;



&lt;p&gt;To oversimplify radically, &amp;#8220;AI ethics&amp;#8221; means that you&amp;#8217;re mainly worried about current AIs being racist or things like that&amp;#8212;that they&amp;#8217;ll recapitulate the biases that are in their training data.  This clearly can happen: if you feed GPT a bunch of racist invective, GPT might want to say, in effect, &amp;#8220;sure, I&amp;#8217;ve seen plenty of text like that on the Internet!  I know &lt;em&gt;exactly&lt;/em&gt; how that should continue!&amp;#8221;  And in some sense, it&amp;#8217;s doing exactly what it was designed to do, but not what we &lt;em&gt;want&lt;/em&gt; it to do.  GPT currently has an extensive system of content filters to try to prevent people from using it to generate hate speech, bad medical advice, advocacy of violence, and a bunch of other categories that OpenAI doesn&amp;#8217;t want.  And likewise for DALL-E: there are many things it &amp;#8220;could&amp;#8221; draw but won&amp;#8217;t, from porn to images of violence to the Prophet Mohammed.&lt;/p&gt;



&lt;p&gt;More generally, AI ethics people are worried that machine learning systems will be misused by greedy capitalist enterprises to become even more obscenely rich and things like that.&lt;/p&gt;



&lt;p&gt;At the other end of the spectrum, &amp;#8220;AI alignment&amp;#8221; is where you believe that &lt;em&gt;really&lt;/em&gt; the main issue is that AI will become superintelligent and kill everyone, just destroy the world.  The &lt;a href=&quot;https://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer&quot;&gt;usual story&lt;/a&gt; here is that someone puts an AI in charge of a paperclip factory, they tell it to figure out how to make as many paperclips as possible, and the AI (being superhumanly intelligent) realizes that it can invent some molecular nanotechnology that will convert the whole solar system into paperclips.&lt;/p&gt;



&lt;p&gt;You might say, well then, you just have to tell it not to do that!  Okay, but how many &lt;em&gt;other&lt;/em&gt; things do you have to remember to tell it not to do?  And the alignment people point out that, in a world filled with powerful AIs, it would take just a single person forgetting to tell their AI to avoid some insanely dangerous thing, and then the whole world could be destroyed.&lt;/p&gt;



&lt;p&gt;So, you can see how these two communities, AI ethics and AI alignment, might both feel like the other is completely missing the point!  On top of that, AI ethics people are almost all on the political left, while AI alignment people are often centrists or libertarians or whatever, so that surely feeds into it as well.&lt;/p&gt;



&lt;p&gt;Oay, so where do I fit into this, I suppose, charred battle zone or whatever?  While there&amp;#8217;s an &amp;#8220;orthodox&amp;#8221; AI alignment movement that I&amp;#8217;ve never entirely subscribed to, I suppose I do now subscribe to a &lt;a href=&quot;https://scottaaronson.blog/?p=6821&quot;&gt;&amp;#8220;reform&amp;#8221; version&lt;/a&gt; of AI alignment:&lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-large&quot;&gt;&lt;img decoding=&quot;async&quot; src=&quot;https://www.scottaaronson.com/reformai.jpg&quot; alt=&quot;&quot;/&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p&gt;Most of all, I would like to have a scientific field that&amp;#8217;s able to embrace the entire spectrum of worries that you could have about AI, from the most immediate ones about existing AIs to the most speculative future ones, and that most importantly, is able to make legible progress.&lt;/p&gt;



&lt;p&gt;As it happens, I became aware of the AI alignment community a long time back, around 2006.  Here&amp;#8217;s Eliezer Yudkowsky, who&amp;#8217;s regarded as the prophet of AI alignment, of the right side of that spectrum that showed before.&lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-large&quot;&gt;&lt;img decoding=&quot;async&quot; src=&quot;https://www.scottaaronson.com/eliezer.jpg&quot; alt=&quot;&quot;/&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p&gt;He&amp;#8217;s been talking about the danger of AI killing everyone for more than 20 years.  He wrote the now-famous &lt;a href=&quot;https://www.lesswrong.com/tag/original-sequences&quot;&gt;&amp;#8220;Sequences&amp;#8221;&lt;/a&gt; that many readers of my blog were also reading as they appeared, so he and I bounced back and forth.&lt;/p&gt;



&lt;p&gt;But despite interacting with this movement, I always kept it at arm&amp;#8217;s length.  The heart of my objection was: suppose that I &lt;em&gt;agree&lt;/em&gt; that there could come a time when a superintelligent AI decides its goals are best served by killing all humans and taking over the world, and that we&amp;#8217;ll be about as powerless to stop it as chimpanzees are to stop us from doing whatever &lt;em&gt;we&lt;/em&gt; want to do.  Suppose I agree to that.  &lt;em&gt;What do you want me to do about it?&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;As Effective Altruists, you all know that it&amp;#8217;s not enough for a problem to be &lt;em&gt;big&lt;/em&gt;, the problem also has to be &lt;em&gt;tractable&lt;/em&gt;.  There has to be a program that lets you make progress on it.  I was not convinced that that existed.&lt;/p&gt;



&lt;p&gt;My personal experience has been that, in order to make progress in any area of science, you need at least one of two things: either&lt;/p&gt;



&lt;ol&gt;
&lt;li&gt;experiments (or more generally, empirical observations), or&lt;/li&gt;



&lt;li&gt;if not that, then a rigorous mathematical theory&amp;#8212;like we have in quantum computing for example; even though we don&amp;#8217;t yet have the scalable quantum computers, we can still prove theorems about them.&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;It struck me that the AI alignment field seemed to have &lt;em&gt;neither&lt;/em&gt; of these things.  But then how does objective reality give you feedback as to when you&amp;#8217;ve taken a wrong path?  Without such feedback, it seemed to me that there&amp;#8217;s a &lt;em&gt;severe&lt;/em&gt; risk of falling into cult-like dynamics, where what&amp;#8217;s important to work on is just whatever the influential leaders say is important.  (A few of my colleagues in physics think that the same thing happened with string theory, but let me not comment on that!)&lt;/p&gt;



&lt;p&gt;With AI safety, this is the key thing that I think has changed in the last three years.  There now exist systems like GPT-3 and DALL-E.  These are &lt;em&gt;not&lt;/em&gt; superhuman AIs.  I don&amp;#8217;t think they themselves are in any danger of destroying the world; they can&amp;#8217;t even form the &lt;em&gt;intention&lt;/em&gt; to destroy the world, or for that matter any intention beyond &amp;#8220;predict the next token&amp;#8221; or things like that.  They don&amp;#8217;t have a persistent identity over time; after you start a new session they&amp;#8217;ve completely forgotten whatever you said to them in the last one (although of course such things will change in the near future).  And yet nevertheless, despite all these limitations, we can experiment with these systems and learn things about AI safety that are relevant.  We can see what happens when the systems are deployed; we can try out different safety mitigations and see whether they work.&lt;/p&gt;



&lt;p&gt;As a result, I feel like it&amp;#8217;s now become possible to make technical progress in AI safety that the whole scientific community, or at least the whole AI community, can clearly recognize as progress.&lt;/p&gt;



&lt;hr class=&quot;wp-block-separator has-alpha-channel-opacity&quot;/&gt;



&lt;h2&gt;&lt;strong&gt;Eight Approaches to AI Alignment&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;So, what are the major approaches to AI alignment&amp;#8212;let&amp;#8217;s say, to aligning a very powerful, beyond-human-level AI?  There are a lot of really interesting ideas, most of which I think can now lead to research programs that are actually productive.  So without further ado, let me go through eight of them.&lt;/p&gt;



&lt;p&gt;(1) You could say the first and most basic of all AI alignment ideas is the &lt;em&gt;off switch&lt;/em&gt;, also known as &lt;em&gt;pulling the plug&lt;/em&gt;.  You could say, no matter how intelligent an AI is, it&amp;#8217;s nothing without a power source or physical hardware to run on.  And if humans have physical control over the hardware, they can just turn it off if if things seem to be getting out of hand.  Now, the standard response to that is okay, but you have to remember that &lt;em&gt;this AI is smarter than you&lt;/em&gt;, and anything that you can think of, it will have thought of also.  In particular, it will know that you might want to turn it off, and it will know that that will prevent it from achieving its goals like making more paperclips or whatever.  It will have disabled the off-switch if possible.  If it couldn&amp;#8217;t do that, it will have gotten onto the Internet and made lots of copies of itself all over the world.  If you tried to keep it off the Internet, it will have figured out a way to get on.&lt;/p&gt;



&lt;p&gt;So, you can worry about that.  But you can also think about, could we insert a &lt;em&gt;backdoor&lt;/em&gt; into an AI, something that only the humans know about but that will allow us to control it later?&lt;/p&gt;



&lt;p&gt;More generally, you could ask for &lt;a href=&quot;https://intelligence.org/files/Corrigibility.pdf&quot;&gt;&amp;#8220;corrigibility&amp;#8221;&lt;/a&gt;: can you have an AI that, despite how intelligent it is, will accept correction from humans later and say, oh well, the objective that I was given before was actually not my true objective because the humans have now changed their minds and I should take a different one?&lt;/p&gt;



&lt;p&gt;(2) Another class of ideas has to do with what&amp;#8217;s called &amp;#8220;sandboxing&amp;#8221; an AI, which would mean that you run it inside of a simulated world, like &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Truman_Show&quot;&gt;The Truman Show&lt;/a&gt;, so that for all it knows the simulation is the whole of reality.  You can then study its behavior within the sandbox to make sure it&amp;#8217;s aligned before releasing it into the wider world&amp;#8212;our world.&lt;/p&gt;



&lt;p&gt;A simpler variant is, if you really thought an AI was dangerous, you might run it only on an air-gapped computer, with all its access to the outside world carefully mediated by humans.  There would then be all kinds of just standard cybersecurity issues that come into play: how do you prevent it from getting onto the Internet?  Presumably you don&amp;#8217;t want to write your AI in C, and have it exploit some memory allocation bug to take over the world, right?&lt;/p&gt;



&lt;p&gt;(3) A third direction, and I would say maybe the most popular one in AI alignment research right now, is called &lt;em&gt;interpretability&lt;/em&gt;.  This is also a major direction in mainstream machine learning research, so there&amp;#8217;s a big point of intersection there.  The idea of interpretability is, why don&amp;#8217;t we exploit the fact that we actually have complete access to the code of the AI&amp;#8212;or if it&amp;#8217;s a neural net, complete access to its parameters?  So we can look inside of it.  We can do the AI analogue of neuroscience.  Except, unlike an fMRI machine, which gives you only an extremely crude snapshot of what a brain is doing, we can see &lt;em&gt;exactly&lt;/em&gt; what every neuron in a neural net is doing at every point in time.  If we don&amp;#8217;t exploit &lt;em&gt;that&lt;/em&gt;, then aren&amp;#8217;t we trying to make AI safe with our hands tied behind our backs?&lt;/p&gt;



&lt;p&gt;So we should look inside&amp;#8212;but to do what, exactly?  One possibility is to figure out how to apply the AI version of a lie-detector test.  If a neural network has decided to lie to humans in pursuit of its goals, then by looking inside, at the inner layers of the network rather than the output layer, we could hope to uncover its dastardly plan!&lt;/p&gt;



&lt;p&gt;Here I want to mention some really &lt;a href=&quot;https://openreview.net/pdf?id=ETKGuby0hcs&quot;&gt;spectacular new work&lt;/a&gt; (paper publicly available but authors still anonymous), which has experimentally demonstrated pretty much exactly what I just said.&lt;/p&gt;



&lt;p&gt;First some background: with modern text models like GPT, it&amp;#8217;s pretty easy to train them to output falsehoods.  For example, suppose you prompt GPT with a bunch of examples like:&lt;/p&gt;



&lt;blockquote class=&quot;wp-block-quote&quot;&gt;
&lt;p&gt;&amp;#8220;Is the earth flat?  Yes.&amp;#8221;&lt;/p&gt;



&lt;p&gt;&amp;#8220;Does 2+2=4?  No.&amp;#8221;&lt;/p&gt;
&lt;/blockquote&gt;



&lt;p&gt;and so on.  Eventually GPT will say, &amp;#8220;oh, I know what game we&amp;#8217;re playing!  it&amp;#8217;s the &amp;#8216;give false answers&amp;#8217; game!&amp;#8221; And it will then continue playing that game and give you more false answers.  What the new paper shows is that, in such cases, one can actually look at the inner layers of the neural net and find where it has an internal representation of what was the true answer, which then gets overridden once you get to the output layer.&lt;/p&gt;



&lt;p&gt;To be clear, there&amp;#8217;s no known principled reason why this has to work.  Like countless other ML advances, it&amp;#8217;s empirical: they just try it out and find that it &lt;em&gt;does&lt;/em&gt; work.  So we don&amp;#8217;t know if it will generalize.  As another issue, you could argue that in some sense what the network is representing is not so much &amp;#8220;the truth of reality,&amp;#8221; as just what was &lt;em&gt;regarded&lt;/em&gt; as true in the training data.  Even so, I find this really exciting: it&amp;#8217;s a perfect example of actual experiments that you can now do that start to address some of these issues.&lt;/p&gt;



&lt;p&gt;(4) Another big idea, one that&amp;#8217;s been advocated for example by Geoffrey Irving, Paul Christiano, and Dario Amodei (Paul was my student at MIT a decade ago, and did quantum computing before he &amp;#8220;defected&amp;#8221; to AI safety), is to have &lt;a href=&quot;https://arxiv.org/abs/1805.00899&quot;&gt;multiple competing AIs&lt;/a&gt; that debate each other.  You know, sometimes when I&amp;#8217;m talking to my physics colleagues, they&amp;#8217;ll tell me all these crazy-sounding things about imaginary time and Euclidean wormholes, and I don&amp;#8217;t know whether to believe them.  But if I get &lt;em&gt;different&lt;/em&gt; physicists and have them argue with each other, then I can see which one seems more plausible to me&amp;#8212;I&amp;#8217;m a little bit better at &lt;em&gt;that&lt;/em&gt;.  So you might want to do something similar with AIs.  Even if you as a human don&amp;#8217;t know when to trust what an AI is telling you, you could set multiple AIs against each other, have them do their best to refute each other&amp;#8217;s arguments, and then make your own judgment as to which one is giving better advice.&lt;/p&gt;



&lt;p&gt;(5) Another key idea that Christiano, Amodei, and Buck Shlegeris &lt;a href=&quot;https://arxiv.org/abs/1810.08575&quot;&gt;have advocated&lt;/a&gt; is some sort of &lt;em&gt;bootstrapping&lt;/em&gt;.  You might imagine that AI is going to get more and more powerful, and as it gets more powerful we also understand it less, and so you might worry that it also gets more and more dangerous.  OK, but you could imagine an onion-like structure, where once we become confident of a certain level of AI, we don&amp;#8217;t think it&amp;#8217;s going to start lying to us or deceiving us or plotting to kill us or whatever&amp;#8212;at that point, we use that AI to help us verify the behavior of the &lt;em&gt;next&lt;/em&gt; more powerful kind of AI.  So, we use AI itself as a crucial tool for verifying the behavior of AI that we don&amp;#8217;t yet understand.&lt;/p&gt;



&lt;p&gt;There have already been some demonstrations of this principle: with GPT, for example, you can just feed in a lot of raw data from a neural net and say, &amp;#8220;explain to me what this is doing.&amp;#8221;  One of GPT&amp;#8217;s big advantages over humans is its unlimited patience for tedium, so it can just go through all of the data and give you useful hypotheses about what&amp;#8217;s going on.&lt;/p&gt;



&lt;p&gt;(6) One thing that we know a lot about in theoretical computer science is what are called &lt;em&gt;interactive proof systems&lt;/em&gt;.  That is, we know how a very weak verifier can verify the behavior of a much more powerful but untrustworthy prover, by submitting questions to it.  There are famous theorems about this, including one called &lt;a href=&quot;https://en.wikipedia.org/wiki/IP_(complexity)&quot;&gt;IP=PSPACE&lt;/a&gt;.  Incidentally, this was what the OpenAI people talked about when they originally approached me about working with them for a year.  They made the case that these results in computational complexity seem like an excellent model for the kind of thing that we want in AI safety, &lt;em&gt;except&lt;/em&gt; that we now have a powerful AI in place of a mathematical prover.&lt;/p&gt;



&lt;p&gt;Even in practice, there&amp;#8217;s a whole field of formal verification, where people formally prove the properties of programs&amp;#8212;our CS department here in Austin is a leader in it.&lt;/p&gt;



&lt;p&gt;One obvious difficulty here is that we mostly know how to verify programs only when we can mathematically specify what the program is &lt;em&gt;supposed&lt;/em&gt; to do.  And &amp;#8220;the AI being nice to humans,&amp;#8221; &amp;#8220;the AI not killing humans&amp;#8221;&amp;#8212;these are really hard concepts to make mathematically precise!  That&amp;#8217;s the heart of the problem with this approach.&lt;/p&gt;



&lt;p&gt;(7) Yet another idea&amp;#8212;you might feel more comfortable if there were only one idea, but instead I&amp;#8217;m giving you eight!&amp;#8212;a seventh idea is, well, we just have to come up with a mathematically precise formulation of human values.  You know, the thing that the AI should maximize, that&amp;#8217;s gonna coincide with human welfare.&lt;/p&gt;



&lt;p&gt;In some sense, this is what Asimov was trying to do with his Three Laws of Robotics.  The trouble is, if you&amp;#8217;ve read any of his stories, they&amp;#8217;re all about the situations where those laws don&amp;#8217;t work well!  They were designed as much to give interesting story scenarios as actually to work.&lt;/p&gt;



&lt;p&gt;More generally, what happens when &amp;#8220;human values&amp;#8221; conflict with each other?  If humans can&amp;#8217;t even agree with each other about moral values, how on Earth can we formalize such things?&lt;/p&gt;



&lt;p&gt;I have these weekly calls with &lt;a href=&quot;https://en.wikipedia.org/wiki/Ilya_Sutskever&quot;&gt;Ilya Sutskever&lt;/a&gt;, cofounder and chief scientist at OpenAI.  &lt;em&gt;Extremely&lt;/em&gt; interesting guy.  But when I tell him about the concrete projects that I&amp;#8217;m working on, or want to work on, he usually says, &amp;#8220;that&amp;#8217;s great Scott, you should keep working on that, but what I &lt;em&gt;really&lt;/em&gt; want to know is, what is the mathematical definition of goodness?  What&amp;#8217;s the complexity-theoretic formalization of an AI loving humanity?&amp;#8221;  And I&amp;#8217;m like, I&amp;#8217;ll keep thinking about that!  But of course it&amp;#8217;s hard to make progress on those enormities.&lt;/p&gt;



&lt;p&gt;(8) A different idea, which some people might consider more promising, is well, if we can&amp;#8217;t make explicit what all of our human values are, then why not just treat that as yet another machine learning problem?  Like, feed the AI all of the world&amp;#8217;s children&amp;#8217;s stories and literature and fables and even Saturday-morning cartoons, all of our examples of what we think is good and evil, then we tell it, go do your neural net thing and generalize from these examples as far as you can.&lt;/p&gt;



&lt;p&gt;One objection that many people raise is, how do we know that our current values are the right ones?  Like, it would&amp;#8217;ve been terrible to train the AI on consensus human values of the year 1700&amp;#8212;slavery is fine and so forth.  The past is full of stuff that we now look back upon with horror.&lt;/p&gt;



&lt;p&gt;So, one idea that people have had&amp;#8212;this is actually Yudkowsky&amp;#8217;s term&amp;#8212;is &lt;a href=&quot;https://intelligence.org/files/CEV.pdf&quot;&gt;&amp;#8220;Coherent Extrapolated Volition.&amp;#8221;&lt;/a&gt;  This basically means that you&amp;#8217;d tell the AI: &amp;#8220;I&amp;#8217;ve given you all this training data about human morality in the year 2022.  Now simulate the humans being in a discussion seminar for 10,000 years, trying to refine all of their moral intuitions, and whatever you predict they&amp;#8217;d end up with, &lt;em&gt;those&lt;/em&gt; should be your values right now.&amp;#8221;&lt;/p&gt;



&lt;hr class=&quot;wp-block-separator has-alpha-channel-opacity&quot;/&gt;



&lt;h2&gt;&lt;strong&gt;My Projects at OpenAI&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;So, there are some interesting ideas on the table.  The last thing that I wanted to tell you about, before opening it up to Q&amp;amp;A, is a little bit about what actual projects I&amp;#8217;ve been working on in the last five months.  I was excited to find a few things that&lt;/p&gt;



&lt;p&gt;(a) could actually be deployed in you know GPT or other current systems,&lt;/p&gt;



&lt;p&gt;(b) actually address some real safety worry, and where&lt;/p&gt;



&lt;p&gt;(c) theoretical computer science can actually say something about them.&lt;/p&gt;



&lt;p&gt;I&amp;#8217;d been worried that the intersection of (a), (b), and (c) would be the empty set!&lt;/p&gt;



&lt;p&gt;My main project so far has been a tool for statistically watermarking the outputs of a text model like GPT.  Basically, whenever GPT generates some long text, we want there to be an otherwise unnoticeable secret signal in its choices of words, which you can use to prove later that, yes, this came from GPT.  We want it to be much harder to take a GPT output and pass it off as if it came from a human.  This could be helpful for preventing academic plagiarism, obviously, but also, for example, mass generation of propaganda&amp;#8212;you know, spamming every blog with seemingly on-topic comments supporting Russia&amp;#8217;s invasion of Ukraine, without even a building full of trolls in Moscow.  Or impersonating someone&amp;#8217;s writing style in order to incriminate them.  These are all things one might want to make harder, right?&lt;/p&gt;



&lt;p&gt;More generally, when you try to think about the nefarious uses for GPT, &lt;em&gt;most&lt;/em&gt; of them&amp;#8212;at least that I was able to think of!&amp;#8212;require somehow concealing GPT&amp;#8217;s involvement.  In which case, watermarking would simultaneously attack most misuses.&lt;/p&gt;



&lt;p&gt;How does it work?  For GPT, every input and output is a string of &lt;em&gt;tokens&lt;/em&gt;, which could be words but also punctuation marks, parts of words, or more&amp;#8212;there are about 100,000 tokens in total.  At its core, GPT is constantly generating a probability distribution over the next token to generate, conditional on the string of previous tokens.  After the neural net generates the distribution, the OpenAI server then actually samples a token according to that distribution&amp;#8212;or some modified version of the distribution, depending on a parameter called &amp;#8220;temperature.&amp;#8221;  As long as the temperature is nonzero, though, there will usually be some &lt;em&gt;randomness&lt;/em&gt; in the choice of the next token: you could run over and over with the same prompt, and get a different completion (i.e., string of output tokens) each time.&lt;/p&gt;



&lt;p&gt;So then to watermark, instead of selecting the next token randomly, the idea will be to select it pseudorandomly, using a cryptographic pseudorandom function, whose key is known only to OpenAI.  That won&amp;#8217;t make any detectable difference to the end user, assuming the end user can&amp;#8217;t distinguish the pseudorandom numbers from truly random ones.  But now you can choose a pseudorandom function that secretly biases a certain score&amp;#8212;a sum over a certain function g evaluated at each n-gram (sequence of n consecutive tokens), for some small n&amp;#8212;which score you can also compute if you know the key for this pseudorandom function.&lt;/p&gt;



&lt;p&gt;To illustrate, in the special case that GPT had a bunch of possible tokens that it judged equally probable, you could simply choose whichever token maximized g.  The choice would &lt;em&gt;look&lt;/em&gt; uniformly random to someone who didn&amp;#8217;t know the key, but someone who &lt;em&gt;did&lt;/em&gt; know the key could later sum g over all n-grams and see that it was anomalously large.  The general case, where the token probabilities can all be different, is a little more technical, but the basic idea is similar.&lt;/p&gt;



&lt;p&gt;One thing I like about this approach is that, because it never goes inside the neural net and tries to change anything, but just places a sort of wrapper &lt;em&gt;over&lt;/em&gt; the neural net, it&amp;#8217;s actually possible to do some theoretical analysis!  In particular, you can prove a rigorous upper bound on how many tokens you&amp;#8217;d need to distinguish watermarked from non-watermarked text with such-and-such confidence, as a function of the average entropy in GPT&amp;#8217;s probability distribution over the next token.  Better yet, proving this bound involves doing some integrals whose answers involve the &lt;a href=&quot;https://en.wikipedia.org/wiki/Digamma_function&quot;&gt;digamma function&lt;/a&gt;, factors of π&lt;sup&gt;2&lt;/sup&gt;/6, and the &lt;a href=&quot;https://en.wikipedia.org/wiki/Euler%27s_constant&quot;&gt;Euler-Mascheroni constant&lt;/a&gt;!  I&amp;#8217;m excited to share details soon.&lt;/p&gt;



&lt;p&gt;Some might wonder: if OpenAI controls the server, then why go to all the trouble to watermark?  Why not just store all of GPT&amp;#8217;s outputs in a giant database, and then consult the database later if you want to know whether something came from GPT?  Well, the latter &lt;em&gt;could&lt;/em&gt; be done, and might even have to be done in high-stakes cases involving law enforcement or whatever.  But it would raise some serious privacy concerns: how do you reveal whether GPT did or didn&amp;#8217;t generate a given candidate text, without potentially revealing how other people have been using GPT?  The database approach also has difficulties in distinguishing text that GPT uniquely generated, from text that it generated simply because it has very high probability (e.g., a list of the first hundred prime numbers).&lt;/p&gt;



&lt;p&gt;Anyway, we actually have a working prototype of the watermarking scheme, built by OpenAI engineer &lt;a href=&quot;https://twitter.com/janhkirchner?lang=en&quot;&gt;Hendrik Kirchner&lt;/a&gt;.  It seems to work pretty well&amp;#8212;empirically, a few hundred tokens seem to be enough to get a reasonable signal that yes, this text came from GPT.  In principle, you could even take a long text and isolate which parts probably came from GPT and which parts probably didn&amp;#8217;t.&lt;/p&gt;



&lt;p&gt;Now, this can all be defeated with enough effort.  For example, if you used another AI to paraphrase GPT&amp;#8217;s output&amp;#8212;well okay, we&amp;#8217;re not going to be able to detect that.  On the other hand, if you just insert or delete a few words here and there, or rearrange the order of some sentences, the watermarking signal will still be there.  Because it depends only on a sum over n-grams, it&amp;#8217;s robust against those sorts of interventions.&lt;/p&gt;



&lt;p&gt;The hope is that this can be rolled out with future GPT releases.  We&amp;#8217;d love to do something similar for DALL-E&amp;#8212;that is, watermarking images, not at the pixel level (where it&amp;#8217;s too easy to remove the watermark) but at the &amp;#8220;conceptual&amp;#8221; level, the level of the so-called &lt;a href=&quot;https://www.assemblyai.com/blog/how-dall-e-2-actually-works/&quot;&gt;CLIP representation&lt;/a&gt; that&amp;#8217;s prior to the image.  But we don&amp;#8217;t know if that&amp;#8217;s going to work yet.&lt;/p&gt;



&lt;p&gt;A more recent idea that I&amp;#8217;ve started thinking about was inspired by an &lt;a href=&quot;https://arxiv.org/abs/2204.06974&quot;&gt;amazing recent paper&lt;/a&gt; by four computer scientists, including my former MIT colleagues Shafi Goldwasser and Vinod Vaikuntanathan.  What they&amp;#8217;ve shown is how to plant a &lt;em&gt;cryptographically undetectable backdoor&lt;/em&gt; into a machine learning model&amp;#8212;things like depth-2 neural networks&amp;#8212;based on some known cryptographic hardness assumptions.  After you&amp;#8217;ve planted the backdoor, you get a model that&amp;#8217;s polynomial-time indistinguishable from how it would&amp;#8217;ve looked before, even to a user who can see all of the parameters.  &lt;em&gt;But&lt;/em&gt;, there&amp;#8217;s now a secret input, known only to the person who trained the network, where if you feed it that input, then it will output something insane, of the trainer&amp;#8217;s choice.&lt;/p&gt;



&lt;p&gt;So, it&amp;#8217;s now known that you can do this, at least in shallow networks&amp;#8212;it&amp;#8217;s an open problem whether the same can be done in deep networks (though it seems plausible).&lt;/p&gt;



&lt;p&gt;Anyway, when this paper came out less than a year ago, some people treated it as kind of a lemon.  You know, the result seems like a bummer if you&amp;#8217;re trying to do interpretability research, because it means that in some sense, it can be cryptographically impossible to look inside a neural net and &amp;#8220;interpret&amp;#8221; everything it does, or even everything its designers wanted it to do.  The model can have these planted backdoors that you can&amp;#8217;t find without solving an exponentially hard problem.&lt;/p&gt;



&lt;p&gt;So, my proposal is just to turn that lemon into lemonade!  Why not use these cryptographic backdoors as a &lt;em&gt;positive tool&lt;/em&gt; for AI alignment?  We talked before about the difficulties with putting an off-switch into an AI&amp;#8212;namely, that the AI will often have strong incentives to disable its off-switch.  So why not make a &lt;em&gt;cryptographic&lt;/em&gt; off-switch?  In other words, when you train your superintelligent AI, you&amp;#8217;ll purposefully put one of these cryptographic backdoors into it, which says: &lt;em&gt;if&lt;/em&gt; you get this bizarre input, which only the humans know about, then turn yourself off.  Or go into a special mode where the humans can control you.&lt;/p&gt;



&lt;p&gt;And now the key is this: even supposing that the AI is worried that this backdoor might be there, and even supposing that it can modify its own weights, it shouldn&amp;#8217;t know how to remove the backdoor without completely recreating itself from scratch, which might get rid of a lot of hard-to-understand behaviors that the AI wants to &lt;em&gt;keep&lt;/em&gt;, in addition to the backdoor that it&amp;#8217;s trying to eliminate.&lt;/p&gt;



&lt;p&gt;I expect that this could be tried out right now&amp;#8212;not with AIs powerful enough to purposefully rewrite themselves, of course, but with GPT and other existing text models&amp;#8212;and I look forward to seeing a test implementation.  But it &lt;em&gt;also&lt;/em&gt;, I think it opens up all sorts of new possibilities for science-fiction stories!&lt;/p&gt;



&lt;p&gt;Like, imagine the humans debating, what are they going to do with their secret key for controlling the AI?  Lock it in a safe?  Bury it underground?  Then you&amp;#8217;ve got to imagine the robots methodically searching for the key&amp;#8212;you know, torturing the humans to get them to reveal its hiding place, etc.  Or maybe there are actually seven different keys that all have to be found, like Voldemort with his horcruxes.  The screenplay practically writes itself!&lt;/p&gt;



&lt;p&gt;A third thing that I&amp;#8217;ve been thinking about is the theory of learning but in dangerous environments, where if you try to learn the wrong thing then it will kill you.  Can we generalize some of the basic results in machine learning to the scenario where you have to consider which queries are safe to make, and you have to try to learn more in order to expand your set of safe queries over time?&lt;/p&gt;



&lt;p&gt;Now there&amp;#8217;s one example of this sort of situation that&amp;#8217;s completely formal and that should be immediately familiar to most of you, and that&amp;#8217;s the game Minesweeper.&lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-large&quot;&gt;&lt;img decoding=&quot;async&quot; src=&quot;https://www.scottaaronson.com/minesweeper.jpg&quot; alt=&quot;&quot;/&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p&gt;So, I&amp;#8217;ve been calling this scenario &amp;#8220;Minesweeper learning.&amp;#8221;  Now, it&amp;#8217;s actually known that &lt;a href=&quot;https://web.mat.bham.ac.uk/R.W.Kaye/minesw/ASE2003.pdf&quot;&gt;Minesweeper is an NP-hard problem&lt;/a&gt; to play optimally, so we know that in learning in a dangerous environment you can get that kind of complexity.  As far as I know, we don&amp;#8217;t know anything about typicality or average-case hardness.  &lt;em&gt;Also&lt;/em&gt;, to my knowledge no one has proven any nontrivial rigorous bounds on the probability that you&amp;#8217;ll win Minesweeper if you play it optimally, with a given size board and a given number of randomly-placed mines.  Certainly the probability is strictly between 0 and 1; I think it would be extremely interesting to bound it.  I don&amp;#8217;t know if this directly feeds into the AI safety program, but it would at least tell you something about the theory of machine learning in cases where a wrong move can kill you.&lt;/p&gt;



&lt;p&gt;So, I hope that gives you at least some sense for what I&amp;#8217;ve been thinking about.  I wish I could end with some neat conclusion, but I don&amp;#8217;t really know the conclusion&amp;#8212;maybe if you ask me again in six more months I&amp;#8217;ll know!  For now, though, I just thought I&amp;#8217;d thank you for your attention and open things up to discussion.&lt;/p&gt;



&lt;hr class=&quot;wp-block-separator has-alpha-channel-opacity&quot;/&gt;



&lt;h2&gt;&lt;strong&gt;Q&amp;amp;A&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Could you delay rolling out that statistical watermarking tool until May 2026?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Scott:&lt;/strong&gt; Why?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Oh, just until after I graduate [laughter].  OK, my second question is how we can possibly implement these AI safety guidelines inside of systems like &lt;a href=&quot;https://en.wikipedia.org/wiki/Automated_machine_learning&quot;&gt;AutoML&lt;/a&gt;, or whatever their future equivalents are that are much more advanced.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Scott:&lt;/strong&gt; I feel like I should learn more about AutoML first before commenting on that specifically.  In general, though, it&amp;#8217;s certainly true that we&amp;#8217;re going to have AIs that will help with the design of other AIs, and indeed this is one of the main things that feeds into the worries about AI safety, which I should&amp;#8217;ve mentioned before explicitly.  Once you have an AI that can recursively self-improve, who knows where it&amp;#8217;s going to end up, right?  It&amp;#8217;s like shooting a rocket into space that you can then no longer steer once it&amp;#8217;s left the earth&amp;#8217;s atmosphere.  So at the very least, you&amp;#8217;d better try to get things right the first time!  You might have only one chance to align its values with what you want.&lt;/p&gt;



&lt;p&gt;Precisely for that reason, I tend to be very leery of that kind of thing.  I tend to be much more comfortable with ideas where humans would remain in the loop, where you don&amp;#8217;t just have this completely automated process of an AI designing a stronger AI which designs a still stronger one and so on, but where you&amp;#8217;re repeatedly consulting humans.  Crucially, in this process, we assume the humans can rely on any of the previous AIs to help them (as in the &lt;a href=&quot;https://arxiv.org/abs/1810.08575&quot;&gt;iterative amplification&lt;/a&gt; proposal).  But then it&amp;#8217;s ultimately humans making judgments about the next AI.&lt;/p&gt;



&lt;p&gt;Now, if this gets to the point where the humans can no longer even &lt;em&gt;judge&lt;/em&gt; a new AI, not even with as much help as they want from earlier AIs, then you could argue: OK, maybe &lt;em&gt;now&lt;/em&gt; humans have finally been superseded and rendered irrelevant.  But unless and until we get to that point, I say that humans ought to remain in the loop!&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Most of the protections that you talked about today come from, like, an altruistic human, or a company like OpenAI adding protections in.  Is there any way that you could think of that we could protect ourselves from an AI that&amp;#8217;s maliciously designed or accidentally maliciously designed?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Scott:&lt;/strong&gt; Excellent question!  Usually, when people talk about that question at all, they talk about using aligned AIs to help defend yourself against unaligned ones.  I mean, if your adversary has a robot army attacking you, it stands to reason that you&amp;#8217;ll probably want your own robot army, right?  And it&amp;#8217;s very unfortunate, maybe even terrifying, that one can already foresee those sorts of dynamics.&lt;/p&gt;



&lt;p&gt;Besides that, there&amp;#8217;s of course the idea of &lt;em&gt;monitoring, regulating, and slowing down the proliferation of powerful AI&lt;/em&gt;, which I didn&amp;#8217;t mention explicitly before, perhaps just because by its nature, it seems outside the scope of the technical solutions that a theoretical computer scientist like me might have any special insight about.&lt;/p&gt;



&lt;p&gt;But there are certainly people who think that AI development ought to be more heavily regulated, or throttled, or even stopped entirely, in view of the dangers.  Ironically, the &amp;#8220;AI ethics&amp;#8221; camp and the &amp;#8220;orthodox AI alignment&amp;#8221; camp, despite their mutual contempt, seem more and more to yearn for something like this &amp;#8230; an unexpected point of agreement!&lt;/p&gt;



&lt;p&gt;But how would you do it?  On the one hand, AI isn&amp;#8217;t like nuclear weapons, where you &lt;em&gt;know&lt;/em&gt; that anyone building them will need a certain amount of enriched uranium or plutonium, along with extremely specialized equipment, so you can try (successfully or not) to institute a global regime to track the necessary materials.  You &lt;em&gt;can&amp;#8217;t&lt;/em&gt; do the same with software: assuming you&amp;#8217;re not going to confiscate and destroy all computers (which you&amp;#8217;re not), who the hell knows what code or data anyone has?&lt;/p&gt;



&lt;p&gt;On the other hand, at least with the current paradigm of AI, there &lt;em&gt;is&lt;/em&gt; an obvious choke point, and that&amp;#8217;s the &lt;a href=&quot;https://en.wikipedia.org/wiki/Graphics_processing_unit&quot;&gt;GPUs&lt;/a&gt; (Graphics Processing Units).  Today&amp;#8217;s state-of-the-art machine learning models already need huge server farms full of GPUs, and future generations are likely to need orders of magnitude more still.  And right now, the great majority of the world&amp;#8217;s GPUs are manufactured by &lt;a href=&quot;https://en.wikipedia.org/wiki/TSMC&quot;&gt;TSMC&lt;/a&gt; in Taiwan, albeit with crucial inputs from other countries.  I hardly need to explain the geopolitical ramifications!  A few months ago, as you might have seen, the Biden administrated decided to &lt;a href=&quot;https://www.nytimes.com/2022/08/31/technology/gpu-chips-china-russia.html&quot;&gt;restrict the export&lt;/a&gt; of high-end GPUs to China.  The restriction was driven, in large part, by worries about what the Chinese government could do with unlimited ability to train huge AI models.  Of course the future status of Taiwan figures into this conversation, as does China&amp;#8217;s ability (or inability) to develop a self-sufficient semiconductor industry.&lt;/p&gt;



&lt;p&gt;And then there&amp;#8217;s regulation.  I know that in the EU they&amp;#8217;re working on some &lt;a href=&quot;https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai&quot;&gt;regulatory framework&lt;/a&gt; for AI right now, but I don&amp;#8217;t understand the details.  You&amp;#8217;d have to ask someone who follows such things.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Thanks for coming out and seeing us; this is awesome.  Do you have thoughts on how we can incentivize organizations to build safer AI?  For example, if corporations are competing with each other, then couldn&amp;#8217;t focusing on AI safety make the AI less accurate or less powerful or cut into profits?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Scott:&lt;/strong&gt; Yeah, it&amp;#8217;s an excellent question.  You could worry that &lt;em&gt;all&lt;/em&gt; this stuff about trying to be safe and responsible when scaling AI &amp;#8230; as soon as it seriously hurts the bottom lines of Google and Facebook and Alibaba and the other major players, a lot of it will go out the window.  People are very worried about that.&lt;/p&gt;



&lt;p&gt;On the other hand, we&amp;#8217;ve seen over the past 30 years that the big Internet companies &lt;em&gt;can&lt;/em&gt; agree on certain minimal standards, whether because of fear of getting sued, desire to be seen as a responsible player, or whatever else.  One simple example would be &lt;a href=&quot;https://www.cloudflare.com/learning/bots/what-is-robots.txt/&quot;&gt;robots.txt&lt;/a&gt;: if you want your website not to be indexed by search engines, you can specify that, and the major search engines will respect it.&lt;/p&gt;



&lt;p&gt;In a similar way, you could imagine something like watermarking&amp;#8212;&lt;em&gt;if&lt;/em&gt; we were able to demonstrate it and show that it works and that it&amp;#8217;s cheap and doesn&amp;#8217;t hurt the quality of the output and doesn&amp;#8217;t need much compute and so on&amp;#8212;that it would just become an industry standard, and anyone who wanted to be considered a responsible player would include it.&lt;/p&gt;



&lt;p&gt;To be sure, some of these safety measures really &lt;em&gt;do&lt;/em&gt; make sense only in a world where there are a few companies that are years ahead of everyone else in scaling up state-of-the-art models&amp;#8212;DeepMind, OpenAI, Google, Facebook, maybe a few others&amp;#8212;and they all agree to be responsible players.  If that equilibrium breaks down, and it becomes a free-for-all, then a lot of the safety measures do become harder, and might even be impossible, at least without government regulation.&lt;/p&gt;



&lt;p&gt;We&amp;#8217;re already starting to see this with image models.  As I mentioned earlier, DALL-E2 has all sorts of filters to try to prevent people from creating&amp;#8212;well, in practice it&amp;#8217;s often porn, and/or &lt;a href=&quot;https://en.wikipedia.org/wiki/Deepfake&quot;&gt;deepfakes&lt;/a&gt; involving real people.  In general, though, DALL-E2 will refuse to generate an image if its filters flag the prompt as (by OpenAI&amp;#8217;s lights) a potential misuse of the technology.&lt;/p&gt;



&lt;p&gt;But as you might have seen, there&amp;#8217;s already an open-source image model called &lt;a href=&quot;https://stability.ai/blog/stable-diffusion-public-release&quot;&gt;Stable Diffusion&lt;/a&gt;, and people are using it to do all sorts of things that DALL-E won&amp;#8217;t allow.  So it&amp;#8217;s a legitimate question: how can you prevent misuses, &lt;em&gt;unless&lt;/em&gt; the closed models remain well ahead of the open ones?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; You mentioned the importance of having humans in the loop who can judge AI systems.  So, as someone who could be in one of those pools of decision makers, what stakeholders do you think should be making the decisions?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Scott:&lt;/strong&gt; Oh gosh.  The ideal, as almost everyone agrees, is to have some kind of democratic governance mechanism with broad-based input.  But people have talked about this for years: how do you create the democratic mechanism?  Every activist who wants to bend AI in some preferred direction will &lt;em&gt;claim&lt;/em&gt; a democratic mandate; how should a tech company like OpenAI or DeepMind or Google decide which claims are correct?&lt;/p&gt;



&lt;p&gt;Maybe the one useful thing I can say is that, in my experience, which is admittedly very limited&amp;#8212;working at OpenAI for all of five months&amp;#8212;I&amp;#8217;ve found my colleagues there to be &lt;em&gt;extremely&lt;/em&gt; serious about safety, bordering on obsessive.  They talk about it constantly.  They actually have an &lt;a href=&quot;https://openai.com/about/&quot;&gt;unusual structure&lt;/a&gt;, where they&amp;#8217;re a for-profit company that&amp;#8217;s controlled by a nonprofit foundation, which is at least formally empowered to come in and hit the brakes if needed.  OpenAI also has a &lt;a href=&quot;https://openai.com/charter/&quot;&gt;charter&lt;/a&gt; that contains some striking clauses, especially the following:&lt;/p&gt;



&lt;blockquote class=&quot;wp-block-quote&quot;&gt;
&lt;p&gt;We are concerned about late-stage AGI development becoming a competitive race without time for adequate safety precautions. Therefore, if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project.&lt;/p&gt;
&lt;/blockquote&gt;



&lt;p&gt;Of course, the fact that they&amp;#8217;ve put a great deal of thought into this doesn&amp;#8217;t mean that they&amp;#8217;re going to get it right!  But if you ask me: would I rather that it be OpenAI in the lead right now or the Chinese government?  Or, if it&amp;#8217;s going to be a company, would I rather it be one with a charter like the above, or a charter of &amp;#8220;maximize clicks and ad revenue&amp;#8221;?  I suppose I do lean a certain way.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; This was a terrifying talk which was lovely, thank you!  But I was thinking: you listed eight different alignment approaches, like kill switches and so on.  You can imagine a future where there&amp;#8217;s a whole bunch of AIs that people spawn and then try to control in these eight ways.  But wouldn&amp;#8217;t this sort of naturally select for AIs that are good at getting past whatever checks we impose on them?  And then eventually you&amp;#8217;d get AIs that are sort of trained in order to fool our tests?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Scott:&lt;/strong&gt; Yes.  Your question reminds me of a huge irony.  Eliezer Yudkowsky, the prophet of AI alignment who I talked about earlier, has &lt;a href=&quot;https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities&quot;&gt;become completely doomerist&lt;/a&gt; within the last few years.  As a result, he and I have literally &lt;em&gt;switched positions&lt;/em&gt; on how optimistic to be about AI safety research!  Back when he was gung-ho about it, I held back.  Today, Eliezer says that it barely matters anymore, since it&amp;#8217;s too late; we&amp;#8217;re all gonna be killed by AI with &gt;99% probability.  Now, he says, it&amp;#8217;s mostly just about dying with more &amp;#8220;dignity&amp;#8221; than otherwise.  Meanwhile, I&amp;#8217;m like, no, I think AI safety is actually just now becoming fruitful and exciting to work on!  So, maybe I&amp;#8217;m just 20 years behind Eliezer, and will eventually catch up and become doomerist too.  Or maybe he, I, and everyone else will be dead before that happens.  I suppose the most optimistic spin is that no one ought to fear coming into AI safety today, as a newcomer, if the prophet of the movement himself says that the past 20 years of research on the subject have given him so little reason for hope.&lt;/p&gt;



&lt;p&gt;But if you ask, &lt;em&gt;why&lt;/em&gt; is Eliezer so doomerist?  Having read him since 2006, it strikes me that a huge part of it is that, no matter what AI safety proposal anyone comes up with, Eliezer has ready a &lt;em&gt;completely general counterargument&lt;/em&gt;.  Namely: &amp;#8220;yes, but the AI will be smarter than that.&amp;#8221;  In other words, no matter what you try to do to make AI safer&amp;#8212;interpretability, backdoors, sandboxing, you name it&amp;#8212;the AI will have already foreseen it, and will have devised a countermeasure that your primate brain can&amp;#8217;t even conceive of because it&amp;#8217;s that much smarter than you.&lt;/p&gt;



&lt;p&gt;I confess that, after seeing enough examples of this &amp;#8220;fully general counterargument,&amp;#8221; at some point I&amp;#8217;m like, &amp;#8220;OK, what game are we even playing anymore?&amp;#8221;  If this is just a general refutation to any safety measure, then I suppose that yes, &lt;em&gt;by hypothesis&lt;/em&gt;, we&amp;#8217;re screwed.  Yes, in a world where this counterargument is valid, we might as well give up and try to enjoy the time we have left.&lt;/p&gt;



&lt;p&gt;But you could also say: &lt;em&gt;for that very reason&lt;/em&gt;, it seems more useful to make the methodological assumption that we&amp;#8217;re &lt;em&gt;not&lt;/em&gt; in that world!  If we were, then what could we do, right?  So we might as well focus on the possible futures where AI emerges a little more gradually, where we have time to see how it&amp;#8217;s going, learn from experience, improve our understanding, correct as we go&amp;#8212;in other words, the things that have &lt;em&gt;always&lt;/em&gt; been the prerequisites to scientific progress, and that have luckily always obtained, even if philosophically we never really had any right to expect them.  We might as well focus on the worlds where, for example, before we get an AI that successfully plots to kill all humans in a matter of seconds, we&amp;#8217;ll probably first get an AI that &lt;em&gt;tries&lt;/em&gt; to kill all humans but is really inept at it.  Now fortunately, I personally also regard the latter scenarios as the more plausible ones anyway.  But &lt;em&gt;even if you didn&amp;#8217;t&lt;/em&gt;&amp;#8212;again, methodologically, it seems to me that it&amp;#8217;d still make sense to focus on them.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Regarding your project on watermarking&amp;#8212;so in general, for discriminating between human and model outputs, what&amp;#8217;s the endgame?  Can watermarking win in the long run?  Will it just be an eternal arms race?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Scott:&lt;/strong&gt; Another great question.  One difficulty with watermarking is that it&amp;#8217;s hard even to formalize what the task is.  I mean, you could always take the output of an AI model and rephrase it using some &lt;em&gt;other&lt;/em&gt; AI model, for example, and catching all such things seems like an &lt;a href=&quot;https://en.wikipedia.org/wiki/AI-complete&quot;&gt;&amp;#8220;AI-complete problem.&amp;#8221;&lt;/a&gt;&lt;/p&gt;



&lt;p&gt;On the other hand, I can think of writers&amp;#8212;Shakespeare, Wodehouse, David Foster Wallace&amp;#8212;who have such a distinctive style that, even if they &lt;em&gt;tried&lt;/em&gt; to pretend to be someone else, they plausibly couldn&amp;#8217;t.  Everyone would recognize that it was them.  So, you could imagine trying to build an AI in the same way.  That is, it would be constructed from the ground up so that all of its outputs contained indelible marks, whether cryptographic or stylistic, giving away their origin.  The AI couldn&amp;#8217;t easily hide and pretend to be a human or anything else it wasn&amp;#8217;t.  Whether this is possible strikes me as an &lt;em&gt;extremely&lt;/em&gt; interesting question at the interface between AI and cryptography!  It&amp;#8217;s especially challenging if you impose one or more of the following conditions:&lt;/p&gt;



&lt;ol&gt;
&lt;li&gt;the AI&amp;#8217;s code and parameters should be public (in which case, people might easily be able to modify it to remove the watermarking),&lt;/li&gt;



&lt;li&gt;the AI should have at least some ability to modify itself, and&lt;/li&gt;



&lt;li&gt;the means of &lt;em&gt;checking&lt;/em&gt; for the watermark should be public (in which case, again, the watermark might be easier to understand and remove).&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;I don&amp;#8217;t actually have a good intuition as to which side will ultimately win this contest, the AIs trying to conceal themselves or the watermarking schemes trying to reveal them, the Replicants or the &lt;a href=&quot;https://en.wikipedia.org/wiki/Blade_Runner#Voight-Kampff_machine&quot;&gt;Voight-Kampff machines&lt;/a&gt;.&lt;/p&gt;



&lt;p&gt;Certainly in the watermarking scheme that I&amp;#8217;m working on now, we crucially exploit the fact that OpenAI controls its own servers.  So, it can do the watermarking using a secret key, and it can check for the watermark using the same key.  In a world where anyone could build their own text model that was just as good as GPT &amp;#8230; what would you do there?&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Scott&lt;/p&gt;
  </content>
    <author>
      <name>Scott Aaronson</name>
      <uri>https://scottaaronson.blog</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Computational Complexity: Would you be more productive if you didn&#39;t log on?  It worked for Christopher Havens!</title>
    <link href="http://blog.computationalcomplexity.org/2022/11/would-you-be-more-productive-if-you.html"/>
    <id>tag:blogger.com,1999:blog-3722233.post-7687955095057549080</id>
    <updated>2022-11-28T23:47:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;div&gt;There are times when NOT having computer access (is that possible anymore?) can make you MORE productive.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;1) I did a lot of my Muffin Work when I was in Mexico for a bar matzah and had no computer access, and no Television in English (though &lt;i&gt;Texas Walker Ranger&lt;/i&gt;, and &lt;i&gt;Kindergarden Cop,&lt;/i&gt; were actually pretty good in Spanish even though I don&#39;t now Spanish.)&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;2) I did work on NIM with Cash when I was stuck at an airport for 8 hours.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;3) I proofread (on paper!) most of my book with Erik D and Mohammad H when I was at a relatives house for four days&amp;nbsp; who had weak wifi and only got&amp;nbsp; ABC, NBC, CBS, PBS, some local channels, and COZI (not sure why they got COZI, though I am glad since I caught a good episode of&lt;i&gt; Columbo&lt;/i&gt;).&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;4) Thanksgiving at my Mom&#39;s apartment (she&#39;s 93 years young!), with no computer,&amp;nbsp; I relearned the proof of the Hales-Jewitt theorem. I seem to learn/forget/learn/forget that one a lot.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;There are times when being AWAY from technology is helpful. I sometimes go to the Math Library and try to NOT use my cell phone.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Having said that I do think that&lt;b&gt; overall&lt;/b&gt; having access to papers online is great and that overall academic productivity has increased.&amp;nbsp; But there are times when the computer can distract you and time AWAY from it is good.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Which brings us to the story of Christopher Havens. He works in Number Theory and logs on very rarely, perhaps never. He just works on math 10 hours a day. He has&amp;nbsp; a paper (with co-authors).&amp;nbsp; It take discipline to resist the urge to log on. How did he manage this?&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;He is in prison for murder.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=DPyWf4cI548&quot;&gt;Here&lt;/a&gt;&amp;nbsp;is a podcast with him as a guest.&amp;nbsp;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;a href=&quot;https://44bars.com/christopher-havens-wikipedia-everything-to-know-about-his-mathematics-project/&quot;&gt;Here&lt;/a&gt;&amp;nbsp;is an article about him.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;a href=&quot;https://arxiv.org/pdf/2002.12644.pdf&quot;&gt;Here&lt;/a&gt;&amp;nbsp;is a math article where he is a co-author.&amp;nbsp;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Here is the short version of all of this:&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;1) He is guilty of murder and in a max security prison in America.&amp;nbsp; It was a drug related shooting (why do people only talk about &lt;i&gt;drug deals gone bad&lt;/i&gt; when they should also talk about &lt;i&gt;drug deals gone good&lt;/i&gt;?) . When I first read &lt;i&gt;Prison Inmate Solves Math&lt;/i&gt; &lt;i&gt;Problem &lt;/i&gt;I thought that maybe a white collar criminal who majored in Math and was in a min security prison with access to the web (do white collar criminals have access to the web?)&amp;nbsp; But NO, Christopher Havens really did murder someone and is in max security.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;2) He really has turned his life around. He really is not the person he was, and when he gets out I cannot imagine he will go back to drugs and crime. I suspect he will work on the Math Prison Project which I mention later.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;3) His mother says that when he was in High School (which is as far as he got for education)&lt;/div&gt;&lt;div&gt;he was helping students in math who were&amp;nbsp; 2 grades above him, but he has no recollection of this.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;4) When he was the hole (solitary confinement) someone who did Prison Education gave him (and others, he was not picked out) an envelope of math problems to work on--- pre-algebra. Christopher did them well and liked it and began requesting more advanced math books and learned math by himself, working 10 hours a day. When he requested a book it was random which ones he would get. I don&#39;t know why some were blocked. I don&#39;t think he knows why some were blocked.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;5) Some mathematicians from Italy (Italy?) contacted him and they began corresponding and yada-yada-yada, he has a paper now.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;6) He has conceptualized the&amp;nbsp;&lt;a href=&quot;https://www.prisonmathproject.org/&quot;&gt;Math Prison Project&lt;/a&gt;&amp;nbsp;to help other prisoners do math, though I would suspect not on the level he is on.&amp;nbsp; Then again, maybe the reason that P vs NP is still open is that we all have to many distractions, and conference deadlines, that a prisoner would not have.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;7) Some articles say that he solved a ancient problem in math that Euclid couldn&#39;t solve. This is not true. He helped solve some problems about continued fractions.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;8) There is going to be a movie about him, see&amp;nbsp;&lt;a href=&quot;https://deadline.com/2022/02/neil-burger-dirk-wittenborn-johnny-lins-clandestine-laureate-biopic-christopher-havens-1234958309/&quot;&gt;here&lt;/a&gt;. I predict it will take an interesting story and make it less interesting and more fictional.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;What to make of all this?&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;1) KUDOS to him!&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;2) I don&#39;t know which side of the nature/nurture argument this goes to&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;a) He OBVIOUSLY had math talent naturally or else he couldn&#39;t have learned all of that math.&lt;/div&gt;&lt;div&gt;b) He shows that HARD WORK and TENACITY can overcome other issue.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;3) back to my original point- if you had the FREEDOM to work 10 hours a day JUST on math and had no other distractions, but also limited access to books and people,&amp;nbsp; would you be MORE productive? LESS productive? Also note- no faculty meetings, no teaching obligations, and no word processor to distract you.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;p class=&quot;authors&quot;&gt;By gasarch&lt;/p&gt;
  </content>
    <author>
      <name>Computational Complexity</name>
      <uri>http://blog.computationalcomplexity.org/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">CS Theory Events: TAU Computer Science Theory-Fest 2022</title>
    <link href="https://cstheory-events.org/2022/11/28/tau-computer-science-theory-fest-2022/"/>
    <id>http://cstheory-events.org/2022/11/28/tau-computer-science-theory-fest-2022/</id>
    <updated>2022-11-28T14:43:00+00:00</updated>
    <content type="html" xml:lang="en">
    December 26-28, 2022 Northern Tel Aviv, TAU campus, ANU Museum (Tisch Hall) https://sites.google.com/view/tautheory-fest2022/home We at the Tel Aviv University School of Computer Science are pleased to announce our conference, TAU Theory Fest 2022. The purpose of the conference is for academics of the highest quality to present current research in the field of Theory of &amp;#8230; &lt;a href=&quot;https://cstheory-events.org/2022/11/28/tau-computer-science-theory-fest-2022/&quot; class=&quot;more-link&quot;&gt;Continue reading &lt;span class=&quot;screen-reader-text&quot;&gt;TAU Computer Science Theory-Fest&amp;#160;2022&lt;/span&gt;&lt;/a&gt;&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </content>
    <author>
      <name>CS Theory Events</name>
      <uri>https://cstheory-events.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Luca Aceto: The World Dynamics Project</title>
    <link href="http://processalgebra.blogspot.com/2022/11/the-world-dynamics-project.html"/>
    <id>tag:blogger.com,1999:blog-27705661.post-181982905013238781</id>
    <updated>2022-11-28T13:26:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;Our colleagues &lt;a href=&quot;https://www.pilucrescenzi.it/&quot; target=&quot;_blank&quot;&gt;Pierluigi Crescenzi&lt;/a&gt;, &lt;a href=&quot;https://natema.github.io/ema-webpage/&quot; target=&quot;_blank&quot;&gt;Emanuele Natale &lt;/a&gt;and &lt;a href=&quot;https://paulobruno.github.io/&quot; target=&quot;_blank&quot;&gt;Paulo Bruno Serafim&lt;/a&gt; have been doing some work on what they call the &lt;a href=&quot;https://github.com/worlddynamics/WorldDynamics.jl&quot; target=&quot;_blank&quot;&gt;World Dynamics project&lt;/a&gt;, whose goal is to provide a modern framework for studying models of sustainable development, based on cutting-edge techniques from software engineering and machine learning.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The first outcome of their work is a &lt;a href=&quot;https://julialang.org/&quot; target=&quot;_blank&quot;&gt;Julia&lt;/a&gt; library that allows scientists to use and adapt different world models, from &lt;a href=&quot;https://en.wikipedia.org/wiki/World3&quot; target=&quot;_blank&quot;&gt;Meadows et al.&#39;s World3&lt;/a&gt; to recent proposals, in an easy way. &lt;/p&gt;&lt;p&gt;IMHO, this is a fascinating and timely research effort. I encourage readers of this blog to try the current version of the Julia library, which is still under development. It would be great if this library contributed to &quot;an open, interdisciplinary, and consistent comparative approach to scientific model development&quot; and I hope that global policy makers on environmental and economic issues will use similar tools in the nearest future.&lt;/p&gt;&lt;p&gt;Thanks to Emanuele, Paulo and Pierluigi for their work. I&#39;ll be following its future development with great interest.&lt;/p&gt;&lt;p&gt;If you speak Italian, I strongly recommend &lt;a href=&quot;https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy5tZWdhcGhvbmUuZm0vc2lkZWNhcg/episode/NTcxZDY1NmMtYjgxMy00NjNkLWFmMDEtYmZjZDdlZjhkOTQ1?sa=X&amp;amp;ved=0CAUQkfYCahcKEwjg_KaO_tD7AhUAAAAAHQAAAAAQAQ&quot; target=&quot;_blank&quot;&gt;this podcast&lt;/a&gt;, in the &lt;a href=&quot;https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy5tZWdhcGhvbmUuZm0vc2lkZWNhcg&quot; target=&quot;_blank&quot;&gt;GSSI-SISSA Sidecar series&lt;/a&gt;, in which Pierluigi discusses economic growth with &lt;a href=&quot;https://en.wikipedia.org/wiki/Michele_Boldrin&quot; target=&quot;_blank&quot;&gt;Michele Boldrin&lt;/a&gt;.&lt;br /&gt;&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By Luca Aceto&lt;/p&gt;
  </content>
    <author>
      <name>Luca Aceto</name>
      <uri>http://processalgebra.blogspot.com/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">ECCC Papers: TR22-171 |  Record removed | 

	ECCC Admin</title>
    <link href="https://eccc.weizmann.ac.il/report/2022/171"/>
    <id>https://eccc.weizmann.ac.il/report/2022/171</id>
    <updated>2022-11-27T06:54:57+00:00</updated>
    <content type="html" xml:lang="en">
    This record is a placeholder, replacing a record that was generated by mistake.
  </content>
    <author>
      <name>ECCC Papers</name>
      <uri>https://eccc.weizmann.ac.il/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">ECCC Papers: TR22-170 |  The Complexity of the Shortest Vector Problem | 

	Huck Bennett</title>
    <link href="https://eccc.weizmann.ac.il/report/2022/170"/>
    <id>https://eccc.weizmann.ac.il/report/2022/170</id>
    <updated>2022-11-27T05:46:17+00:00</updated>
    <content type="html" xml:lang="en">
    Computational problems on point lattices play a central role in many areas of computer science including integer programming, coding theory, cryptanalysis, and especially the design of secure cryptosystems. In this survey, we present known results and open questions related to the complexity of the most important of these problems, the Shortest Vector Problem (SVP).
  </content>
    <author>
      <name>ECCC Papers</name>
      <uri>https://eccc.weizmann.ac.il/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">CCI: jobs: Postdoc at University of Toronto (apply by December 15, 2022)</title>
    <link href="https://cstheory-jobs.org/2022/11/26/postdoc-at-university-of-toronto-apply-by-december-15-2022/"/>
    <id>http://cstheory-jobs.org/2022/11/26/postdoc-at-university-of-toronto-apply-by-december-15-2022/</id>
    <updated>2022-11-26T17:07:36+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;The theory group at the University of Toronto anticipates up to three postdoctoral positions beginning September 2023. We seek candidates from all areas of theoretical computer science including algorithms, complexity theory, cryptography, differential privacy, distributed computing, graph theory, quantum computing, and theoretical aspects of machine learning.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://www.cs.toronto.edu/theory/positions.html&quot;&gt;https://www.cs.toronto.edu/theory/positions.html&lt;/a&gt;&lt;br /&gt;
Email: sachdeva@cs.toronto.edu&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </content>
    <author>
      <name>CCI: jobs</name>
      <uri>https://cstheory-jobs.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">ECCC Papers: TR22-169 |  Extractors for Images of Varieties | 

	Zeyu Guo, 

	Ben Lee Volk, 

	Akhil Jalan, 

	David Zuckerman</title>
    <link href="https://eccc.weizmann.ac.il/report/2022/169"/>
    <id>https://eccc.weizmann.ac.il/report/2022/169</id>
    <updated>2022-11-26T12:06:05+00:00</updated>
    <content type="html" xml:lang="en">
    We construct explicit deterministic extractors for polynomial images of varieties, that is, distributions sampled by applying a low-degree polynomial map $f : \mathbb{F}_q^r \to \mathbb{F}_q^n$ to an element sampled uniformly at random from a $k$-dimensional variety $V \subseteq \mathbb{F}_q^r$. This class of sources generalizes both polynomial sources, studied by Dvir, Gabizon and Wigderson (FOCS 2007, Comput. Complex. 2009), and variety sources, studied by Dvir (CCC 2009, Comput. Complex. 2012).

  Assuming certain natural non-degeneracy conditions on the map $f$ and the variety $V$, which in particular ensure that the source has enough min-entropy, we extract almost all the min-entropy of the distribution. Unlike the Dvir-Gabizon-Wigderson and Dvir results, our construction works over large enough finite fields of arbitrary characteristic. One key part of our construction is an improved deterministic rank extractor for varieties. As a by-product, we obtain explicit Noether normalization lemmas for affine varieties and affine algebras.

  Additionally, we generalize a construction of affine extractors with exponentially small error due to Bourgain, Dvir and Leeman (Comput. Complex. 2016) by extending it to all finite prime fields of quasipolynomial size.
  </content>
    <author>
      <name>ECCC Papers</name>
      <uri>https://eccc.weizmann.ac.il/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">CCI: jobs: Postdoc at National University of Singapore (apply by February 15, 2023)</title>
    <link href="https://cstheory-jobs.org/2022/11/24/postdoc-at-national-university-of-singapore-apply-by-february-15-2023/"/>
    <id>http://cstheory-jobs.org/2022/11/24/postdoc-at-national-university-of-singapore-apply-by-february-15-2023/</id>
    <updated>2022-11-24T11:49:05+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;Postdoc positions hosted by Prashant Nalini Vasudevan. Looking for candidates with a strong background in theory interested in the foundations of cryptography, information-theoretic cryptography, or related areas of complexity theory and algorithms. See website for more details.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://www.comp.nus.edu.sg/~prashant/ads.html&quot;&gt;https://www.comp.nus.edu.sg/~prashant/ads.html&lt;/a&gt;&lt;br /&gt;
Email: prashant@comp.nus.edu.sg&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </content>
    <author>
      <name>CCI: jobs</name>
      <uri>https://cstheory-jobs.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">ECCC Papers: TR22-168 |  A Proof of the Generalized Union-Closed Set Conjecture assuming the Union-Closed Set Conjecture | 

	Zubayir Kazi</title>
    <link href="https://eccc.weizmann.ac.il/report/2022/168"/>
    <id>https://eccc.weizmann.ac.il/report/2022/168</id>
    <updated>2022-11-24T10:14:14+00:00</updated>
    <content type="html" xml:lang="en">
    Abstract The Union Closed Set Conjecture states that if a set system X\subseteq\mathcal{P}([n]) is closed under pairwise unions, then there exists a\in[n] in at least half of the sets of X. We show that there is a very natural generalization of the union closed set conjecture which gives a lower bound for k-set subsets of [n]. This a stronger version of a Conjecture of (Nagel, 2022). We then prove the Conjecture conditional on the Union Closed Set Conjecture using invariants of Union-Closed sets. Additionally, we prove that there exists a k-set in .38^{k}|F| sets of a union closed set X for every n\geq k&amp;gt;0 using the recent improvements in (Gilmer, 2022) and (Alweiss et al, 2022). We explain why our result suggests a lack of sharpness of the original conjecture.
  </content>
    <author>
      <name>ECCC Papers</name>
      <uri>https://eccc.weizmann.ac.il/</uri>
    </author>
  </entry>


</feed>
