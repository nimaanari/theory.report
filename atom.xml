<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>Theory of Computing Report</title>
  <link rel="self" href=""/>
  <link href=""/>
  <id></id>
  <updated></updated>
  <generator uri="http://feedreader.github.io/">Pluto 1.6.2 on Ruby 3.0.6 (2023-03-30) [x86_64-linux]</generator>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Richard Lipton: Human Extinction?</title>
    <link href="https://rjlipton.wpcomstaging.com/2023/06/08/human-extinction/"/>
    <id>https://rjlipton.wpcomstaging.com/?p=21725</id>
    <updated>2023-06-08T20:38:23+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;&lt;span style=&quot;color: #0044cc;&quot;&gt;&lt;br /&gt;
&lt;em&gt;And some counter-arguments&lt;/em&gt;&lt;br /&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/08/human-extinction/dr-hava-siegelmann/&quot; rel=&quot;attachment wp-att-21728&quot;&gt;&lt;img data-attachment-id=&quot;21728&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/08/human-extinction/dr-hava-siegelmann/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/Dr_Hava_Siegelmann.jpg?fit=400%2C500&amp;amp;ssl=1&quot; data-orig-size=&quot;400,500&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;Dr. Hava Siegelmann&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;Dr. Hava Siegelmann (PRNewsfoto\/Dr. Hava Siegelmann)&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;This image must be used within the context of the news release it accompanied. Request permission from issuer for other uses.&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;Dr Hava Siegelmann&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/Dr_Hava_Siegelmann.jpg?fit=240%2C300&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/Dr_Hava_Siegelmann.jpg?fit=400%2C500&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; class=&quot;alignright wp-image-21728&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/Dr_Hava_Siegelmann.jpg?resize=120%2C150&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;120&quot; height=&quot;150&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/Dr_Hava_Siegelmann.jpg?w=400&amp;amp;ssl=1 400w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/Dr_Hava_Siegelmann.jpg?resize=240%2C300&amp;amp;ssl=1 240w&quot; sizes=&quot;(max-width: 120px) 100vw, 120px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
Hava Siegelmann is the Provost Professor in the Manning College of Information and Computer Sciences at U.Mass. Amherst. She returned in 2019 from serving as a DARPA Program Director. Her work at DARPA included leading two AI initiatives: &lt;a href=&quot;https://www.darpa.mil/news-events/2017-03-16&quot;&gt;L2M&lt;/a&gt; for &amp;#8220;Lifelong Learning Machines&amp;#8221; and &lt;a href=&quot;https://www.darpa.mil/program/guaranteeing-ai-robustness-against-deception&quot;&gt;GARD&lt;/a&gt; for &amp;#8220;Guaranteeing AI Robustness against Deception.&amp;#8221;&lt;/p&gt;
&lt;p&gt;
Today we discuss whether we need measures to guarantee human robustness against AI.&lt;/p&gt;
&lt;p&gt;
Siegelmann was &lt;a href=&quot;https://federalnewsnetwork.com/artificial-intelligence/2020/06/darpa-honors-artificial-intelligence-expert/&quot;&gt;awarded&lt;/a&gt; the Meritorious Public Service Medal, a rare high honor from the US Department of Defense. Her dean at U.Mass., Laura Haas, stated in a &lt;a href=&quot;https://www.prnewswire.com/news-releases/darpa-recognizes-umass-professor-hava-siegelmann-for-major-advances-in-ai-301081766.html&quot;&gt;release&lt;/a&gt;, &amp;#8220;I am extremely proud of Hava&amp;#8217;s service to DARPA and the nation. Her work at DARPA has helped to advance AI for us all.&amp;#8221; &lt;/p&gt;
&lt;p&gt;
One thing that catches our interest, in line with another recent &lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/05/22/early-theory/&quot;&gt;post&lt;/a&gt;, is that her applied work jumped off from a mainstream topic in theory. Well, one maybe seen as off the mainstream: that of &amp;#8220;super-Turing&amp;#8221; machines. Let&amp;#8217;s discuss that first before coming to AI.&lt;/p&gt;
&lt;p&gt;&lt;H2&gt; Super-Turing &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
We who work in polynomial-based complexity often feel that undecidable languages and other aspects of recursion theory are walled off in a different area of theory. Part of the shock of the &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BMIP%5E%2A+%3D+RE%7D%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{&amp;#92;mathsf{MIP^* = RE}}&quot; class=&quot;latex&quot; /&gt; &lt;a href=&quot;https://arxiv.org/abs/2001.04383&quot;&gt;result&lt;/a&gt; was breaking down this wall. See this great &lt;a href=&quot;https://quantumfrontiers.com/2020/03/01/the-shape-of-mip-re/&quot;&gt;post&lt;/a&gt; by coauthor Henry Yuen for more aspects.&lt;/p&gt;
&lt;p&gt;
The same feeling goes even more for &lt;a href=&quot;https://en.wikipedia.org/wiki/Hypercomputation&quot;&gt;hypercomputing&lt;/a&gt; models, defined as able to compute functions that are not Turing-computable. Our own &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BMIP%5E%2A+%3D+RE%7D%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{&amp;#92;mathsf{MIP^* = RE}}&quot; class=&quot;latex&quot; /&gt; &lt;a href=&quot;https://rjlipton.wpcomstaging.com/2020/01/15/halting-is-poly-time-quantum-provable/&quot;&gt;post&lt;/a&gt; includes a story of how David Deutsch in the mid-1980s originally believed that quantum computers could solve the Halting Problem in finite time. &lt;/p&gt;
&lt;p&gt;
Yet many of us have done real work with a hypercomputing model so broad that it can recognize uncountably many languages. The model&amp;#8217;s subtle power arguably poses the most trenchant &lt;a href=&quot;http://theory.stanford.edu/~liyang/teaching/projects/natural-proofs-barrier-and-P-NP.pdf&quot;&gt;barrier&lt;/a&gt; to proving &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%5Cneq+NP%7D%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{&amp;#92;mathsf{P &amp;#92;neq NP}}&quot; class=&quot;latex&quot; /&gt;. We refer, of course, to the model of &lt;em&gt;nonuniform&lt;/em&gt; polynomial-size circuit families and its associated complexity class, &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%2Fpoly%7D%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{&amp;#92;mathsf{P/poly}}&quot; class=&quot;latex&quot; /&gt;. &lt;/p&gt;
&lt;p&gt;
Indeed, poly-size circuits are the basis of Siegelmann&amp;#8217;s celebrated 1995 &lt;a href=&quot;https://binds.cs.umass.edu/papers/1995_Siegelmann_Science.pdf&quot;&gt;paper&lt;/a&gt; in &lt;em&gt;Science&lt;/em&gt; titled &amp;#8220;Computation beyond the Turing Limit&amp;#8221;&amp;#8212;and a full 1996 &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0304397596000874?ref=pdf_download&amp;#038;fr=RR-2&amp;#038;rr=7d3d858de9933344&quot;&gt;followup&lt;/a&gt; in &lt;em&gt;Theoretical Computer Science&lt;/em&gt; titled &amp;#8220;the simple dynamics of super Turing theories.&amp;#8221; One point is that individual circuits are finite objects that can be manipulated&amp;#8212;as likewise are finite neural networks. The analog recurrent neural networks (ARNNs) used by Siegelmann are allowed real-number coefficients. They in turn are related to a class of dynamical systems with simply-specified rules built around &lt;em&gt;analog shift&lt;/em&gt; (AS) maps that obey a finite-dependence or finite-effect condition. These models define complexity classes &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BARNN%7D%5Bs%28n%29%5D%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{&amp;#92;mathsf{ARNN}[s(n)]}&quot; class=&quot;latex&quot; /&gt; and &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BAS%7D%5Bs%28n%29%5D%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{&amp;#92;mathsf{AS}[s(n)]}&quot; class=&quot;latex&quot; /&gt; in the same manner as when &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Bs%28n%29%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{s(n)}&quot; class=&quot;latex&quot; /&gt; means Boolean circuit size. The main theorem is:&lt;/p&gt;
&lt;p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt;Theorem 1&lt;/b&gt; &lt;em&gt; For any function &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Bs%28n%29+%5Cgeq+n%7D&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{s(n) &amp;#92;geq n}&quot; class=&quot;latex&quot; /&gt;, we have &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BARNN%7D%5Bs%28n%29%5D+%5Csubseteq+%5Cmathsf%7BAS%7D%5Bs%28n%29%5E%7BO%281%29%7D%5D%7D&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{&amp;#92;mathsf{ARNN}[s(n)] &amp;#92;subseteq &amp;#92;mathsf{AS}[s(n)^{O(1)}]}&quot; class=&quot;latex&quot; /&gt; and &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BAS%7D%5Bs%28n%29%5D+%5Csubseteq+%5Cmathsf%7BARNN%7D%5Bs%28n%29%5E%7BO%281%29%7D%5D%7D&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{&amp;#92;mathsf{AS}[s(n)] &amp;#92;subseteq &amp;#92;mathsf{ARNN}[s(n)^{O(1)}]}&quot; class=&quot;latex&quot; /&gt;. In particular, say restricted to languages over a binary alphabet, &lt;/em&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathsf%7BARNN%7D%5Bn%5E%7BO%281%29%7D%5D+%3D+%5Cmathsf%7BAS%7D%5Bn%5E%7BO%281%29%7D%5D+%3D+%5Cmathsf%7BP%2Fpoly%7D.+&amp;#038;bg=e8e8e8&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;&amp;#92;displaystyle &amp;#92;mathsf{ARNN}[n^{O(1)}] = &amp;#92;mathsf{AS}[n^{O(1)}] = &amp;#92;mathsf{P/poly}. &quot; class=&quot;latex&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;
A 2013 blog &lt;a href=&quot;https://www.georgezarkadakis.com/super-turing-machines-and-oracles-the-making-of-a-artificial-mind/&quot;&gt;post&lt;/a&gt; by George Zarkadakis picks up the thread of how this correspondence fosters the notion of machines that learn by continual adaptation: the &amp;#8220;lifelong learning machines.&amp;#8221; What Siegelmann accomplished with her further work culminating at DARPA was demonstrate that these &amp;#8220;super-Turing&amp;#8221; ideas can be rendered into real applications.&lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; The View From Inside &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
The June 2020 PR Newswire item on Siegelmann&amp;#8217;s award has some passages on applications that were at least inspired by her mode of approach (we&amp;#8217;ve added bullets for clarity):&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; DARPA points out Siegelmann&amp;#8217;s `exceptionally productive&amp;#8217; term included &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
developing a system that intelligently administers insulin and dextrose to maintain safe glucose levels for diabetics and critical care patients; &lt;/p&gt;
&lt;li&gt;
sensors to identify dangerous chemicals from a safe distance; &lt;/p&gt;
&lt;li&gt;
collaborative, secure learning platforms that allow unaffiliated groups to work synergistically without revealing sensitive data; and &lt;/p&gt;
&lt;li&gt;
reverse engineering methods to identify cyber-attacks, secure the system, and find the attacker.
&lt;/ul&gt;
&lt;p&gt;&lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
And this about Machine Learning (ML):&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; Illustrating the difference between current AI and new L2M systems, Siegelmann stated, &amp;#8220;Self-driving cars represent a pinnacle in state-of-the-art computation&amp;#8212;demonstrating how far current technology can take us using increasingly clever programming. However, even these systems fail when encountering circumstances outside their training&amp;#8230;&amp;#8221; [Whereas], L2M systems represent &amp;#8220;a fundamental change in ML,&amp;#8221; she said, &amp;#8220;L2M systems learn; they apply experience and adapt to new situations; instead of failing, they become better, the more they experience.&amp;#8221; &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
And this: &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; &amp;#8220;We made real progress, demonstrated actual learning – something never done before &amp;#8230; L2M improvements are already being incorporated into real-world systems; in five years, AI systems will be mainly of the L2M variety or incorporate L2M components. But it is very hard,&amp;#8221; she adds, &amp;#8220;for a machine to learn actively and there is still much to be done.&amp;#8221; &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
Note that &amp;#8220;in five years&amp;#8221; meant by &lt;b&gt;2025&lt;/b&gt;. We are over halfway there, and the headline-making &lt;a href=&quot;https://en.wikipedia.org/wiki/ChatGPT&quot;&gt;ChatGPT&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/DALL-E&quot;&gt;DALL-E&lt;/a&gt;, and other models happened last year. &lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; The View From Other Insiders &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
It does not need much experience of dystopian fiction in book or movie form to imagine sinister plot twists of the above items:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
The medical system tasked with inferring safe glucose levels discovers circumstances outside its training that enable it to plant chemical time bombs that can be used to control the patients, which include high government officials&amp;#8230; &lt;/p&gt;
&lt;li&gt;
The collaborative platform that admits unaffiliated groups reverse-engineers methods to identify cyber-attackers into ones that admit them, then secures the system to find the original defenders and hunt them down&amp;#8230; &lt;/p&gt;
&lt;li&gt;
Self-driving cars equipped with manual override learn that the manual operators are idiots (which we are) and &amp;#8230; we get a remake of Alfred Hitchcock&amp;#8217;s &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Birds_(film)&quot;&gt;The Birds&lt;/a&gt; titled &lt;em&gt;The Cars&lt;/em&gt;.
&lt;/ul&gt;
&lt;p&gt;
Are we being unfair and far-fetched? Perhaps so in these cases. But here are two &amp;#8220;real-life AI risks&amp;#8221; postulated in a &lt;a href=&quot;https://www.tableau.com/data-insights/ai/risks#risks&quot;&gt;statement&lt;/a&gt; by the AI analytics company &lt;a href=&quot;https://www.tableau.com&quot;&gt;Tableau&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; If companies rely too much on AI predictions for when maintenance will be done without other checks, it could lead to machinery malfunctions that injure workers. Models used in healthcare could cause misdiagnoses. &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
And a &amp;#8220;hypothetical risk&amp;#8221;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; [A]n AI system tasked with &amp;#8230; helping to rebuild an endangered marine creature’s ecosystem [could] decide that other parts of the ecosystem are unimportant and destroy their habitats. And it could also view human intervention to fix or prevent this as a threat to its goal. &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
Last week, an &lt;a href=&quot;https://www.safe.ai/statement-on-ai-risk#open-letter&quot;&gt;open letter&lt;/a&gt; signed by numerous AI luminaries made a simple statement that went all the way to the risk of &lt;em&gt;human extinction&lt;/em&gt;, not just bungling a coral reef:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war. &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
Dan Hendrycks, director of the Center For AI Safety, stated further in his May 30 Twitter &lt;a href=&quot;https://twitter.com/DanHendrycks/status/1663474795865059329&quot;&gt;thread&lt;/a&gt; releasing the letter:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; &amp;#8220;[T]here are many `important and urgent risks from AI,&amp;#8217; not just the risk of extinction; for example, systemic bias, misinformation, malicious use, cyberattacks, and weaponization.&amp;#8221; &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
An accompanying NPR &lt;a href=&quot;https://www.npr.org/2023/05/30/1178943163/ai-risk-extinction-chatgpt&quot;&gt;story&lt;/a&gt; also quotes Geoffrey Hinton, first on the list of the letter&amp;#8217;s &lt;a href=&quot;https://www.safe.ai/statement-on-ai-risk#signatories&quot;&gt;signatories&lt;/a&gt;, to the effect that AI programs are on track to outperform their creators sooner than anyone anticipated:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; &amp;#8220;I thought for a long time that we were, like, 30 to 50 years away from that. &amp;#8230; Now, I think we may be much closer, maybe only five years away from that.&amp;#8221; &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
That five-year horizon means &lt;b&gt;2028&lt;/b&gt;. The second signer is Yoshua Bengio, making two of the three researchers who won the 2018 Turing Award for their research on neural networks. The third, Yann LeCun, who leads Meta&amp;#8217;s AI research efforts, has not signed yet. &lt;/p&gt;
&lt;table style=&quot;margin: auto;&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/08/human-extinction/hintonbengiolecun/&quot; rel=&quot;attachment wp-att-21730&quot;&gt;&lt;img data-attachment-id=&quot;21730&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/08/human-extinction/hintonbengiolecun/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/HintonBengioLeCun.jpg?fit=999%2C374&amp;amp;ssl=1&quot; data-orig-size=&quot;999,374&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;1&amp;quot;}&quot; data-image-title=&quot;HintonBengioLeCun&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/HintonBengioLeCun.jpg?fit=300%2C112&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/HintonBengioLeCun.jpg?fit=600%2C225&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; class=&quot;aligncenter wp-image-21730&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/HintonBengioLeCun.jpg?resize=360%2C135&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;360&quot; height=&quot;135&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/HintonBengioLeCun.jpg?w=999&amp;amp;ssl=1 999w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/HintonBengioLeCun.jpg?resize=300%2C112&amp;amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/HintonBengioLeCun.jpg?resize=768%2C288&amp;amp;ssl=1 768w&quot; sizes=&quot;(max-width: 360px) 100vw, 360px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;caption alignright&quot;&gt;&lt;font size=&quot;-2&quot;&gt;Cropped from AI Builders &lt;a href=&quot;https://aibuilders.ai/le-prix-turing-recompense-trois-pionniers-de-lintelligence-artificielle-yann-lecun-yoshua-bengio-et-geoffrey-hinton/&quot;&gt;source&lt;/a&gt;&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;
That third slot is appropriately filled by Google DeepMind CEO Demis Hassabis, who along with his university friend and the letter&amp;#8217;s 26th signer, David Silver, gained prominence for developing AlphaGo and AlphaZero. Scott &lt;a href=&quot;https://www.scottaaronson.com&quot;&gt;Aaronson&lt;/a&gt;&amp;#8212;of course a famed complexity and quantum computing leader who is now working within OpenAI on a watermarking scheme for detecting ChatGPT usage&amp;#8212;is a signer. Others whom Ken and I have met include Bill McKibben, Peter Norvig, David Chalmers, Bart Selman, Roman Yampolskiy, and Steve Petersen of Niagara University near Ken.&lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; Shock und D&amp;uuml;rrenmatt? &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;CNN&amp;#8217;s &lt;a href=&quot;https://www.cnn.com/2023/05/30/media/artificial-intelligence-warning-reliable-sources/index.html&quot;&gt;story&lt;/a&gt; on the letter is subtitled, &amp;#8220;Are we taking it seriously enough?&amp;#8221; It ends by quoting Duke&amp;#8217;s Cynthia Rudin, a star student of Ingrid Daubechies whom we recently &lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/05/14/ingrid-daubechies-prizes-and-art/&quot;&gt;profiled&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; &amp;#8220;Do we really need more evidence that AI’s negative impact could be as big as nuclear war?&amp;#8221; &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
This calls to mind the upcoming &lt;a href=&quot;https://en.wikipedia.org/wiki/Oppenheimer_(film)&quot;&gt;movie&lt;/a&gt; about J. Robert Oppenheimer and also Friedrich D&amp;uuml;rrenmatt&amp;#8217;s play &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Physicists&quot;&gt;The Physicists&lt;/a&gt;, whose last scenes clash two tag lines:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; &amp;#8220;We must take back our knowledge&amp;#8230;&amp;#8221; &amp;#8212;but&amp;#8212; &amp;#8220;something once thought cannot be unthought.&amp;#8221; &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
There is also the old book &lt;a href=&quot;https://en.wikipedia.org/wiki/Future_Shock&quot;&gt;Future Shock&lt;/a&gt; by Alvin Toffler, which warns of &amp;#8220;&lt;a href=&quot;https://en.wikipedia.org/wiki/Information_overload&quot;&gt;information overload&lt;/a&gt;&amp;#8221; but maybe not AI peril &lt;em&gt;per se&lt;/em&gt;. &lt;/p&gt;
&lt;p&gt;
Let us nudge &amp;#8220;Shock&amp;#8221; to the German word &lt;em&gt;Schach&lt;/em&gt; meaning &amp;#8220;chess.&amp;#8221; The person who might feel he was most viscerally slapped down by AI is Garry Kasparov, the former world chess champion who famously lost to IBM&amp;#8217;s Deep Blue computer in 1997. However, he had &lt;a href=&quot;https://www.themanufacturer.com/articles/garry-kasparov-intelligent-machines/&quot;&gt;this&lt;/a&gt; to say in 2017:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; &amp;#8220;Machines that replace physical labour have allowed us to focus more on what makes us human: our minds. Intelligent machines will continue that process, taking over the more menial aspects of cognition and elevating our mental lives toward creativity, curiosity, beauty, and joy. These are what truly make us human, not any particular activity or skill, like swinging a hammer – or even playing chess.&amp;#8221; &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
Marc Andreesen, of early &lt;a href=&quot;https://en.wikipedia.org/wiki/Mosaic_(web_browser)&quot;&gt;Mosaic&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Netscape&quot;&gt;Netscape&lt;/a&gt; fame, posted on Tuesday a long &lt;a href=&quot;https://a16z.com/2023/06/06/ai-will-save-the-world/&quot;&gt;response&lt;/a&gt; to the open letter titled &amp;#8220;Why AI Will Save the World.&amp;#8221; It rebuts four of the stated AI risks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
Will AI Kill Us All? &lt;/p&gt;
&lt;li&gt;
Will AI Ruin Our Society? &lt;/p&gt;
&lt;li&gt;
Will AI Take All Our Jobs? &lt;/p&gt;
&lt;li&gt;
Will AI Lead To Crippling Inequality?
&lt;/ol&gt;
&lt;p&gt;
It concedes as a point 5 that AI will empower bad actors to be badder and more quickly thus. But it ends with a point that both of us have also heard at DARPA: the motive of not being surprised and subjugated by something that an adversary develops first. On that basis he advocates &amp;#8220;Pursuing AI With Maximum Force And Speed.&amp;#8221; &lt;/p&gt;
&lt;p&gt;
I (Ken writing this part) agree with Kasparov and Andreesen&amp;#8212;with one further caveat that reflects the &amp;#8220;guardrails&amp;#8221; concern of a March &lt;a href=&quot;https://futureoflife.org/open-letter/pause-giant-ai-experiments/&quot;&gt;open letter&lt;/a&gt; from the Future of Life institute, but without the six-month &amp;#8220;pause&amp;#8221; it advocates. This is that communications should promote their receivers to exercise &lt;em&gt;scientific skepticism&lt;/em&gt;, such as we&amp;#8217;ve tried to do in our &lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/03/17/cead-mile-gpt/&quot;&gt;own&lt;/a&gt; &lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/04/01/the-chatgpt-conundrum/&quot;&gt;jocular&lt;/a&gt; &lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/04/12/acm-prize-to-yael-kalai/&quot;&gt;posts&lt;/a&gt; on (Chat)GPT.&lt;/p&gt;
&lt;p&gt;
Perhaps the most evocative word will come from the Oscar-nominated film director Bennett Miller. He has evidently &lt;a href=&quot;https://www.worldofreel.com/blog/2023/4/1uzqxkaw0hham43fsyux6u7q2vkrd0&quot;&gt;revived&lt;/a&gt; a documentary project begun in 2016 about the debate over AI. He also opened a Manhattan &lt;a href=&quot;https://hypebeast.com/2023/3/bennett-miller-gagosian-exhibition-new-york-ai&quot;&gt;exhibit&lt;/a&gt; of his own AI-assisted images. The New York Times included his work in a &lt;a href=&quot;https://www.nytimes.com/2023/05/03/arts/design/ai-makes-nostalgic-images.html&quot;&gt;roundup&lt;/a&gt; of AI used to create art that, curiously, is of itself `borne back ceaselessly into the past.&amp;#8217;&lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; Open Problems &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
I knew some of the early greats in AI. One was Roger &lt;a href=&quot;https://en.wikipedia.org/wiki/Roger_Schank&quot;&gt;Schank&lt;/a&gt; and was at Yale University when I arrived with my then fresh Ph.D. We have talked about Roger previously &lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/02/05/artificial-intelligence-just-lost-a-leader/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/08/human-extinction/rs-3/&quot; rel=&quot;attachment wp-att-21732&quot;&gt;&lt;img data-attachment-id=&quot;21732&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/08/human-extinction/rs-3/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/rs.jpeg?fit=241%2C241&amp;amp;ssl=1&quot; data-orig-size=&quot;241,241&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;rs&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/rs.jpeg?fit=241%2C241&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/rs.jpeg?fit=241%2C241&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; class=&quot;aligncenter wp-image-21732&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/rs.jpeg?resize=150%2C150&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;150&quot; height=&quot;150&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/rs.jpeg?w=241&amp;amp;ssl=1 241w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/rs.jpeg?resize=150%2C150&amp;amp;ssl=1 150w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/rs.jpeg?resize=200%2C200&amp;amp;ssl=1 200w&quot; sizes=&quot;(max-width: 150px) 100vw, 150px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
I wonder a bit about what Roger would say today about the potential of AI on human extinction. I think he loved AI, was a great leader in all aspects of AI, but perhaps never saw it with the potential to extinct humans? What do you all think?&lt;/p&gt;
&lt;p&gt;
Ken adds that it might be fruitful to seek more understanding of what exactly &lt;em&gt;circuit complexity&lt;/em&gt; had to do with all this. He notes a long &lt;a href=&quot;https://www.quantamagazine.org/in-new-paradox-black-holes-appear-to-evade-heat-death-20230606/&quot;&gt;article&lt;/a&gt; in &lt;em&gt;Quanta&lt;/em&gt; on Tuesday that highlights the emerging role of circuit complexity in resolving issues of information and black holes. There is a hint of similarity to Siegelmann&amp;#8217;s machine-learning mechanism in how quantum systems are said to evolve to embody greater circuit complexity.&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By RJLipton+KWRegan&lt;/p&gt;
  </content>
    <author>
      <name>Richard Lipton</name>
      <uri>https://rjlipton.wpcomstaging.com</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Gil Kalai: Determining Ramsey numbers using finite geometry</title>
    <link href="https://gilkalai.wordpress.com/2023/06/08/determining-ramsey-numbers-using-finite-geometry/"/>
    <id>http://gilkalai.wordpress.com/2023/06/08/determining-ramsey-numbers-using-finite-geometry/</id>
    <updated>2023-06-08T19:31:29+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;div class=&quot;wpcom-reblog-snapshot&quot;&gt;&lt;div class=&quot;reblogger-note&quot;&gt;&lt;div class=&#39;reblogger-note-content&#39;&gt;&lt;blockquote&gt;&lt;p&gt;Sam Mattheus and Jacques Verstraete made a remarkable breakthrough for Ramsey numbers &lt;img src=&quot;https://s0.wp.com/latex.php?latex=R%284%2Ct%29&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;R(4,t)&quot; class=&quot;latex&quot; /&gt;, and Anurag Bishnoi wrote a beautiful blog post about it. Congratulations Sam and Jacques! Mattheus and Verstraete &lt;a href=&quot;https://arxiv.org/abs/2306.04007&quot;&gt;show that&lt;/a&gt; &lt;img src=&quot;https://s0.wp.com/latex.php?latex=r%284%2Ct%29+%5Cge+c+%5Cfrac+%7Bt%5E3%7D%7B%5Clog+%5E4+t%7D&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;r(4,t) &amp;#092;ge c &amp;#092;frac {t^3}{&amp;#092;log ^4 t}&quot; class=&quot;latex&quot; /&gt;.&lt;/p&gt;
&lt;/blockquote&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;reblog-post&quot;&gt;&lt;p class=&quot;reblog-from&quot;&gt;&lt;img alt=&#39;&#39; src=&#39;https://0.gravatar.com/avatar/9b550c7bd755079f79698630b9b84e867941554d901106a5e5428c21cd573092?s=32&amp;#038;d=identicon&amp;#038;r=PG&#39; class=&#39;avatar avatar-32&#39; height=&#39;32&#39; width=&#39;32&#39; /&gt;&lt;a href=&quot;https://anuragbishnoi.wordpress.com/2023/06/08/determining-ramsey-numbers-using-finite-geometry/&quot;&gt;Anurag&amp;#039;s Math Blog&lt;/a&gt;&lt;/p&gt;&lt;div class=&quot;reblogged-content&quot;&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://sammattheus.wordpress.com/author/sammattheus/&quot;&gt;Sam Mattheus&lt;/a&gt; and &lt;a href=&quot;https://math.ucsd.edu/people/profiles/jacques-verstraete&quot;&gt;Jacques Verstraete&lt;/a&gt; have posted a &lt;a href=&quot;https://arxiv.org/abs/2306.04007&quot;&gt;preprint&lt;/a&gt; today where they solve the classic open problem of determining the asymptotics of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Ramsey%27s_theorem#Ramsey_numbers&quot;&gt;Ramsey number&lt;/a&gt; $latex r(4, t)$.  They &lt;a href=&quot;https://arxiv.org/abs/2306.04007&quot;&gt;show that&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p class=&quot;has-text-align-center&quot;&gt;$latex r(4, t) geq c frac{t^3}{log^4 t}$&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;which is just a factor of $latex log^2 t$ away from the upper bound. The only other off-diagonal Ramsey number for which we knew the correct asymptotics prior to their work was $latex r(3, t)$, and the best lower bounds on $latex r(4, t)$ were $latex c’ t^{5/2}/log^2 t$. These earlier bounds are in fact at the limit of what could be proved using the &lt;a href=&quot;https://link.springer.com/article/10.1007/s00222-010-0247-x&quot;&gt;random $latex H$-free process&lt;/a&gt;. That barrier has finally been broken by using completely different techniques involving &lt;a href=&quot;https://en.wikipedia.org/wiki/Finite_geometry&quot;&gt;finite geometry&lt;/a&gt;! It’s an amazing breakthrough that builds up on the recent developments in Ramsey theory using finite geometry (see &lt;a href=&quot;https://anuragbishnoi.wordpress.com/minicourse/&quot;&gt;this&lt;/a&gt; for an online minicourse I gave in 2021…&lt;/p&gt;
&lt;/div&gt;&lt;p class=&quot;reblog-source&quot;&gt;&lt;a href=&quot;https://anuragbishnoi.wordpress.com/2023/06/08/determining-ramsey-numbers-using-finite-geometry/&quot;&gt;View original post&lt;/a&gt; &lt;span class=&quot;more-words&quot;&gt;929 more words&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;p class=&quot;authors&quot;&gt;By Gil Kalai&lt;/p&gt;
  </content>
    <author>
      <name>Gil Kalai</name>
      <uri>https://gilkalai.wordpress.com</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">ECCC Papers: TR23-086 |  Random $(\log n)$-CNF are Hard for Cutting Planes (Again) | 

	Dmitry Sokolov</title>
    <link href="https://eccc.weizmann.ac.il/report/2023/086"/>
    <id>https://eccc.weizmann.ac.il/report/2023/086</id>
    <updated>2023-06-08T13:55:09+00:00</updated>
    <content type="html" xml:lang="en">
    The random $\Delta$-CNF model is one of the most important distribution over $\Delta\text{-}\mathrm{SAT}$ instances. It is closely connected to various areas of computer science, statistical physics, and is a benchmark for satisfiability algorithms. Fleming, Pankratov, Pitassi, and Robere and independently Hrubes and Pudlak showed that when $\Delta = \Theta(\log n)$, any Cutting Planes proof for random $\Delta$-CNF on $n$ variables requires size $2^{n / \mathrm{polylog} n}$ in the regime where the number of clauses guarantees that the formula is unsatisfiable with high probability. In this paper we show tight lower bound $2^{\Omega(n)}$ on size CP-proofs for random $(\log n)$-CNF formulas. Moreover, our proof is much simpler and self-contained in contrast with previous results based on Jukna&amp;#39;s lower bound for monotone circuits.
  </content>
    <author>
      <name>ECCC Papers</name>
      <uri>https://eccc.weizmann.ac.il/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Luca Aceto: Summer School on Formal Methods for Cyber-Physical Systems and Workshop on Synthesis, Monitoring and Learning in Udine</title>
    <link href="http://processalgebra.blogspot.com/2023/06/summer-school-on-formal-methods-for.html"/>
    <id>tag:blogger.com,1999:blog-27705661.post-3832339170920067475</id>
    <updated>2023-06-08T09:21:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;The third edition of the &lt;a href=&quot;http://tcs.uniud.it/summer-school&quot; target=&quot;_blank&quot;&gt;UniVr/UniUd Summer School on Formal Methods for Cyber-Physical Systems&lt;/a&gt;&amp;nbsp; will be held in Udine, Italy, in the period August 28-31. It will be followed by the &lt;a href=&quot;http://tcs.uniud.it/smile&quot; target=&quot;_blank&quot;&gt;Workshop on Synthesis, Monitoring and Learning&lt;/a&gt; on August 31 and September 1. The list of contributors to those events is top notch.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The course is offered in a hybrid format giving the possibility to remotely attend the course (on the Microsoft Teams platform).

On-site places are limited and assigned on first come first served basis.

The registration fees are:&amp;nbsp;&lt;/p&gt;&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;On-site participation, 250.00 Euro + VAT 22%&amp;nbsp;&lt;/li&gt;&lt;li&gt;Online participation, 120.00 Euro + VAT 22%&amp;nbsp;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The deadline for online application is August 18, 2023.
Participation application is available at &lt;a href=&quot;https://www.cism.it/en/activities/courses/J2303/&quot;&gt;https://www.cism.it/en/activities/courses/J2303/&lt;/a&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Spread the news and encourage students and young researchers to attend!
&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By Luca Aceto&lt;/p&gt;
  </content>
    <author>
      <name>Luca Aceto</name>
      <uri>http://processalgebra.blogspot.com/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Optimizing Sphere Valued Gaussian Noise Stability</title>
    <link href="http://arxiv.org/abs/2306.03912"/>
    <id>http://arxiv.org/abs/2306.03912</id>
    <updated>2023-06-08T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Heilman_S/0/1/0/all/0/1&quot;&gt;Steven Heilman&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We prove a vector-valued inequality for the Gaussian noise stability (i.e. we
prove a vector-valued Borell inequality) for Euclidean functions taking values
in the two-dimensional sphere, for all correlation parameters at most $1/10$ in
absolute value. This inequality was conjectured (for all correlation parameters
at most $1$ in absolute value) by Hwang, Neeman, Parekh, Thompson and Wright.
Such an inequality is needed to prove sharp computational hardness of the
product state Quantum MAX-CUT problem, assuming the Unique Games Conjecture.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Hardness of Deceptive Certificate Selection</title>
    <link href="http://arxiv.org/abs/2306.04505"/>
    <id>http://arxiv.org/abs/2306.04505</id>
    <updated>2023-06-08T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waldchen_S/0/1/0/all/0/1&quot;&gt;Stephan W&amp;#xe4;ldchen&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Recent progress towards theoretical interpretability guarantees for AI has
been made with classifiers that are based on interactive proof systems. A
prover selects a certificate from the datapoint and sends it to a verifier who
decides the class. In the context of machine learning, such a certificate can
be a feature that is informative of the class. For a setup with high soundness
and completeness, the exchanged certificates must have a high mutual
information with the true class of the datapoint. However, this guarantee
relies on a bound on the Asymmetric Feature Correlation of the dataset, a
property that so far is difficult to estimate for high-dimensional data. It was
conjectured in W\&quot;aldchen et al. that it is computationally hard to exploit the
AFC, which is what we prove here.
&lt;/p&gt;
&lt;p&gt;We consider a malicious prover-verifier duo that aims to exploit the AFC to
achieve high completeness and soundness while using uninformative certificates.
We show that this task is $\mathsf{NP}$-hard and cannot be approximated better
than $\mathcal{O}(m^{1/8 - \epsilon})$, where $m$ is the number of possible
certificates, for $\epsilon&amp;gt;0$ under the Dense-vs-Random conjecture. This is
some evidence that AFC should not prevent the use of interactive classification
for real-world tasks, as it is computationally hard to be exploited.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Querying Circumscribed Description Logic Knowledge Bases</title>
    <link href="http://arxiv.org/abs/2306.04546"/>
    <id>http://arxiv.org/abs/2306.04546</id>
    <updated>2023-06-08T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lutz_C/0/1/0/all/0/1&quot;&gt;Carsten Lutz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maniere_Q/0/1/0/all/0/1&quot;&gt;Quentin Mani&amp;#xe8;re&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nolte_R/0/1/0/all/0/1&quot;&gt;Robin Nolte&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Circumscription is one of the main approaches for defining non-monotonic
description logics (DLs). While the decidability and complexity of traditional
reasoning tasks such as satisfiability of circumscribed DL knowledge bases
(KBs) is well understood, for evaluating conjunctive queries (CQs) and unions
thereof (UCQs), not even decidability had been established. In this paper, we
prove decidability of (U)CQ evaluation on circumscribed DL KBs and obtain a
rather complete picture of both the combined complexity and the data
complexity, for DLs ranging from ALCHIO via EL to various versions of DL-Lite.
We also study the much simpler atomic queries (AQs).
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Recognition of Seifert fibered spaces with boundary is in NP</title>
    <link href="http://arxiv.org/abs/2306.04612"/>
    <id>http://arxiv.org/abs/2306.04612</id>
    <updated>2023-06-08T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jackson_A/0/1/0/all/0/1&quot;&gt;Adele Jackson&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We show that the decision problem of recognising whether a triangulated
3-manifold admits a Seifert fibered structure with non-empty boundary is in NP.
We also show that the problem of producing Seifert data for a triangulation of
such a manifold is in the complexity class FNP. We do this by proving that in
any triangulation of a Seifert fibered space with boundary there is both a
fundamental horizontal surface of small degree and a complete collection of
normal vertical annuli whose total weight is bounded by an exponential in the
square of the triangulation size.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: Optimal Transport Model Distributional Robustness</title>
    <link href="http://arxiv.org/abs/2306.04178"/>
    <id>http://arxiv.org/abs/2306.04178</id>
    <updated>2023-06-08T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1&quot;&gt;Van-Anh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Trung Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bui_A/0/1/0/all/0/1&quot;&gt;Anh Tuan Bui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1&quot;&gt;Thanh-Toan Do&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1&quot;&gt;Dinh Phung&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Distributional robustness is a promising framework for training deep learning
models that are less vulnerable to adversarial examples and data distribution
shifts. Previous works have mainly focused on exploiting distributional
robustness in data space. In this work, we explore an optimal transport-based
distributional robustness framework on model spaces. Specifically, we examine a
model distribution in a Wasserstein ball of a given center model distribution
that maximizes the loss. We have developed theories that allow us to learn the
optimal robust center model distribution. Interestingly, through our developed
theories, we can flexibly incorporate the concept of sharpness awareness into
training a single model, ensemble models, and Bayesian Neural Networks by
considering specific forms of the center model distribution, such as a Dirac
delta distribution over a single model, a uniform distribution over several
models, and a general Bayesian Neural Network. Furthermore, we demonstrate that
sharpness-aware minimization (SAM) is a specific case of our framework when
using a Dirac delta distribution over a single model, while our framework can
be viewed as a probabilistic extension of SAM. We conduct extensive experiments
to demonstrate the usefulness of our framework in the aforementioned settings,
and the results show remarkable improvements in our approaches to the
baselines.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: Point in polygon calculation using vector geometric methods with application to geospatial data</title>
    <link href="http://arxiv.org/abs/2306.04316"/>
    <id>http://arxiv.org/abs/2306.04316</id>
    <updated>2023-06-08T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwinger_E/0/1/0/all/0/1&quot;&gt;Eyram Schwinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Twum_R/0/1/0/all/0/1&quot;&gt;Ralph Twum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katsekpor_T/0/1/0/all/0/1&quot;&gt;Thomas Katsekpor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwinger_G/0/1/0/all/0/1&quot;&gt;Gladys Schwinger&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this work, we designed algorithms for the point in polygon problem based
on the ray casting algorithm using equations from vector geometry. The
algorithms were implemented using the python programming language. We tested
the algorithm against the point in polygon algorithms used by the shapely (and
by extension geopandas) library and the OpenCV library using points from the
google Open Buildings project. Our algorithm in pure python performed much
better than the shapely implementation. It also performed better than the
OpenCV implementation when combined with the Numba optimization library. We
also performed simulations to verify that our algorithm performance was of the
order O(n).
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Linear Time Algorithms for NP-hard Problems restricted to GaTEx Graphs</title>
    <link href="http://arxiv.org/abs/2306.04367"/>
    <id>http://arxiv.org/abs/2306.04367</id>
    <updated>2023-06-08T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hellmuth_M/0/1/0/all/0/1&quot;&gt;Marc Hellmuth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholz_G/0/1/0/all/0/1&quot;&gt;Guillaume E. Scholz&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The class of Galled-Tree Explainable (GaTEx) graphs has just recently been
discovered as a natural generalization of cographs. Cographs are precisely
those graphs that can be uniquely represented by a rooted tree where the leaves
of the tree correspond to the vertices of the graph. As a generalization, GaTEx
graphs are precisely those graphs that can be uniquely represented by a
particular rooted directed acyclic graph (called galled-tree).
&lt;/p&gt;
&lt;p&gt;We consider here four prominent problems that are, in general, NP-hard:
computing the size $\omega(G)$ of a maximum clique, the size $\chi(G)$ of an
optimal vertex-coloring and the size $\alpha(G)$ of a maximum independent set
of a given graph $G$ as well as determining whether a graph is perfectly
orderable. We show here that $\omega(G)$, $\chi(G)$, $\alpha(G)$ can be
computed in linear-time for GaTEx graphs $G$. The crucial idea for the
linear-time algorithms is to avoid working on the GaTEx graphs $G$ directly,
but to use the the galled-trees that explain $G$ as a guide for the algorithms
to compute these invariants. In particular, we show first how to employ the
galled-tree structure to compute a perfect ordering of GaTEx graphs in
linear-time which is then used to determine $\omega(G)$, $\chi(G)$,
$\alpha(G)$.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: One-sided Matrix Completion from Two Observations Per Row</title>
    <link href="http://arxiv.org/abs/2306.04049"/>
    <id>http://arxiv.org/abs/2306.04049</id>
    <updated>2023-06-08T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1&quot;&gt;Steven Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valiant_G/0/1/0/all/0/1&quot;&gt;Gregory Valiant&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Given only a few observed entries from a low-rank matrix $X$, matrix
completion is the problem of imputing the missing entries, and it formalizes a
wide range of real-world settings that involve estimating missing data.
However, when there are too few observed entries to complete the matrix, what
other aspects of the underlying matrix can be reliably recovered? We study one
such problem setting, that of &quot;one-sided&quot; matrix completion, where our goal is
to recover the right singular vectors of $X$, even in the regime where
recovering the left singular vectors is impossible, which arises when there are
more rows than columns and very few observations. We propose a natural
algorithm that involves imputing the missing values of the matrix $X^TX$ and
show that even with only two observations per row in $X$, we can provably
recover $X^TX$ as long as we have at least $\Omega(r^2 d \log d)$ rows, where
$r$ is the rank and $d$ is the number of columns. We evaluate our algorithm on
one-sided recovery of synthetic data and low-coverage genome sequencing. In
these settings, our algorithm substantially outperforms standard matrix
completion and a variety of direct factorization methods.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Quantum Distance Calculation for $\epsilon$-Graph Construction</title>
    <link href="http://arxiv.org/abs/2306.04290"/>
    <id>http://arxiv.org/abs/2306.04290</id>
    <updated>2023-06-08T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chmielewski_N/0/1/0/all/0/1&quot;&gt;Naomi Mona Chmielewski&lt;/a&gt; (EDF R&amp;amp;D OSIRIS, L2S), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amini_N/0/1/0/all/0/1&quot;&gt;Nina Amini&lt;/a&gt; (CNRS, L2S), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacquot_P/0/1/0/all/0/1&quot;&gt;Paulin Jacquot&lt;/a&gt; (EDF R&amp;amp;D OSIRIS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikael_J/0/1/0/all/0/1&quot;&gt;Joseph Mikael&lt;/a&gt; (EDF R&amp;amp;D OSIRIS)&lt;/p&gt;&lt;p&gt;In machine learning and particularly in topological data analysis,
$\epsilon$-graphs are important tools but are generally hard to compute as the
distance calculation between n points takes time O(n^2) classically. Recently,
quantum approaches for calculating distances between n quantum states have been
proposed, taking advantage of quantum superposition and entanglement. We
investigate the potential for quantum advantage in the case of quantum distance
calculation for computing $\epsilon$-graphs. We show that, relying on existing
quantum multi-state SWAP test based algorithms, the query complexity for
correctly identifying (with a given probability) that two points are not
$\epsilon$-neighbours is at least O(n^3 / ln n), showing that this approach, if
used directly for $\epsilon$-graph construction, does not bring a computational
advantage when compared to a classical approach.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Matroid-Constrained Vertex Cover</title>
    <link href="http://arxiv.org/abs/2306.04342"/>
    <id>http://arxiv.org/abs/2306.04342</id>
    <updated>2023-06-08T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chien-Chung Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sellier_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Sellier&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this paper, we introduce the problem of Matroid-Constrained Vertex Cover:
given a graph with weights on the edges and a matroid imposed on the vertices,
our problem is to choose a subset of vertices that is independent in the
matroid, with the objective of maximizing the total weight of covered edges.
This problem is a generalization of the much studied max $k$-vertex cover
problem, in which the matroid is the simple uniform matroid, and it is also a
special case of the problem of maximizing a monotone submodular function under
a matroid constraint.
&lt;/p&gt;
&lt;p&gt;First, we give a Fixed-Parameter Tractable Approximation Scheme (FPT-AS) when
the given matroid is a partition matroid, a laminar matroid, or a transversal
matroid. Precisely, if $k$ is the rank of the matroid, we obtain $(1 -
\varepsilon)$ approximation using $(1/\varepsilon)^{O(k)}n^{O(1)}$ time for
partition and laminar matroids and using $(1/\varepsilon+k)^{O(k)}n^{O(1)}$
time for transversal matroids. This extends a result of Manurangsi for uniform
matroids [Manurangsi, 2018]. We also show that these ideas can be applied in
the context of (single-pass) streaming algorithms. Besides, our FPT-AS
introduces a new technique based on matroid union, which may be of independent
interest in extremal combinatorics.
&lt;/p&gt;
&lt;p&gt;In the second part, we consider general matroids. We propose a simple local
search algorithm that guarantees $2/3 \approx 0.66$ approximation. For the more
general problem where two matroids are imposed on the vertices and a feasible
solution must be a common independent set, we show that a local search
algorithm gives a $2/3 \cdot (1 - 1/(p+1))$ approximation in $n^{O(p)}$ time,
for any integer $p$. We also provide some evidence to show that with the
constraint of one or two matroids, the approximation ratio of $2/3$ is likely
the best possible, using the currently known techniques of local search.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: On Computing Optimal Tree Ensembles</title>
    <link href="http://arxiv.org/abs/2306.04423"/>
    <id>http://arxiv.org/abs/2306.04423</id>
    <updated>2023-06-08T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komusiewicz_C/0/1/0/all/0/1&quot;&gt;Christian Komusiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kunz_P/0/1/0/all/0/1&quot;&gt;Pascal Kunz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sommer_F/0/1/0/all/0/1&quot;&gt;Frank Sommer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sorge_M/0/1/0/all/0/1&quot;&gt;Manuel Sorge&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Random forests and, more generally, (decision\nobreakdash-)tree ensembles are
widely used methods for classification and regression. Recent algorithmic
advances allow to compute decision trees that are optimal for various measures
such as their size or depth. We are not aware of such research for tree
ensembles and aim to contribute to this area. Mainly, we provide two novel
algorithms and corresponding lower bounds. First, we are able to carry over and
substantially improve on tractability results for decision trees, obtaining a
$(6\delta D S)^S \cdot poly$-time algorithm, where $S$ is the number of cuts in
the tree ensemble, $D$ the largest domain size, and $\delta$ is the largest
number of features in which two examples differ. To achieve this, we introduce
the witness-tree technique which also seems promising for practice. Second, we
show that dynamic programming, which has been successful for decision trees,
may also be viable for tree ensembles, providing an $\ell^n \cdot poly$-time
algorithm, where $\ell$ is the number of trees and $n$ the number of examples.
Finally, we compare the number of cuts necessary to classify training data sets
for decision trees and tree ensembles, showing that ensembles may need
exponentially fewer cuts for increasing number of trees.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Maintaining the cycle structure of dynamic permutations</title>
    <link href="http://arxiv.org/abs/2306.04470"/>
    <id>http://arxiv.org/abs/2306.04470</id>
    <updated>2023-06-08T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liptak_Z/0/1/0/all/0/1&quot;&gt;Zsuzsanna Lipt&amp;#xe1;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masillo_F/0/1/0/all/0/1&quot;&gt;Francesco Masillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navarro_G/0/1/0/all/0/1&quot;&gt;Gonzalo Navarro&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We present a new data structure for maintaining dynamic permutations, which
we call a $\textit{forest of splay trees (FST)}$. The FST allows one to
efficiently maintain the cycle structure of a permutation $\pi$ when the
allowed updates are transpositions. The structure stores one conceptual splay
tree for each cycle of $\pi$, using the position within the cycle as the key.
Updating $\pi$ to $\tau\cdot\pi$, for a transposition $\tau$, takes
$\mathcal{O}(\log n)$ amortized time, where $n$ is the size of $\pi$. The FST
computes any $\pi(i)$, $\pi^{-1}(i)$, $\pi^k(i)$ and $\pi^{-k}(i)$, in
$\mathcal{O}(\log n)$ amortized time. Further, it supports cycle-specific
queries such as determining whether two elements belong to the same cycle, flip
a segment of a cycle, and others, again within $\mathcal{O}(\log n)$ amortized
time.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: On the complexity of isomorphism problems for tensors, groups, and polynomials III: actions by classical groups</title>
    <link href="http://arxiv.org/abs/2306.03135"/>
    <id>http://arxiv.org/abs/2306.03135</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhili Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grochow_J/0/1/0/all/0/1&quot;&gt;Joshua A. Grochow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Youming Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1&quot;&gt;Gang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chuanqi Zhang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the complexity of isomorphism problems for d-way arrays, or tensors,
under natural actions by classical groups such as orthogonal, unitary, and
symplectic groups. Such problems arise naturally in statistical data analysis
and quantum information. We study two types of complexity-theoretic questions.
First, for a fixed action type (isomorphism, conjugacy, etc.), we relate the
complexity of the isomorphism problem over a classical group to that over the
general linear group. Second, for a fixed group type (orthogonal, unitary, or
symplectic), we compare the complexity of the decision problems for different
actions.
&lt;/p&gt;
&lt;p&gt;Our main results are as follows. First, for orthogonal and symplectic groups
acting on 3-way arrays, the isomorphism problems reduce to the corresponding
problem over the general linear group. Second, for orthogonal and unitary
groups, the isomorphism problems of five natural actions on 3-way arrays are
polynomial-time equivalent, and the d-tensor isomorphism problem reduces to the
3-tensor isomorphism problem for any fixed d&amp;gt;3. For unitary groups, the
preceding result implies that LOCC classification of tripartite quantum states
is at least as difficult as LOCC classification of d-partite quantum states for
any d. Lastly, we also show that the graph isomorphism problem reduces to the
tensor isomorphism problem over orthogonal and unitary groups.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: On the Role of Entanglement and Statistics in Learning</title>
    <link href="http://arxiv.org/abs/2306.03161"/>
    <id>http://arxiv.org/abs/2306.03161</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Arunachalam_S/0/1/0/all/0/1&quot;&gt;Srinivasan Arunachalam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Havlicek_V/0/1/0/all/0/1&quot;&gt;Vojtech Havlicek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Schatzki_L/0/1/0/all/0/1&quot;&gt;Louis Schatzki&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this work we make progress in understanding the relationship between
learning models with access to entangled, separable and statistical
measurements in the quantum statistical query (QSQ) model. To this end, we show
the following results.
&lt;/p&gt;
&lt;p&gt;$\textbf{Entangled versus separable measurements.}$ The goal here is to learn
an unknown $f$ from the concept class $C\subseteq \{f:\{0,1\}^n\rightarrow
[k]\}$ given copies of $\frac{1}{\sqrt{2^n}}\sum_x \vert x,f(x)\rangle$. We
show that, if $T$ copies suffice to learn $f$ using entangled measurements,
then $O(nT^2)$ copies suffice to learn $f$ using just separable measurements.
&lt;/p&gt;
&lt;p&gt;$\textbf{Entangled versus statistical measurements}$ The goal here is to
learn a function $f \in C$ given access to separable measurements and
statistical measurements. We exhibit a class $C$ that gives an exponential
separation between QSQ learning and quantum learning with entangled
measurements (even in the presence of noise). This proves the &quot;quantum
analogue&quot; of the seminal result of Blum et al. [BKW&#39;03]. that separates
classical SQ and PAC learning with classification noise.
&lt;/p&gt;
&lt;p&gt;$\textbf{QSQ lower bounds for learning states.}$ We introduce a quantum
statistical query dimension (QSD), which we use to give lower bounds on the QSQ
learning. With this we prove superpolynomial QSQ lower bounds for testing
purity, shadow tomography, Abelian hidden subgroup problem, degree-$2$
functions, planted bi-clique states and output states of Clifford circuits of
depth $\textsf{polylog}(n)$.
&lt;/p&gt;
&lt;p&gt;$\textbf{Further applications.}$ We give and $\textit{unconditional}$
separation between weak and strong error mitigation and prove lower bounds for
learning distributions in the QSQ model. Prior works by Quek et al. [QFK+&#39;22],
Hinsche et al. [HIN+&#39;22], and Nietner et al. [NIS+&#39;23] proved the analogous
results $\textit{assuming}$ diagonal measurements and our work removes this
assumption.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Three Candidate Plurality is Stablest for Correlations at most 1/11</title>
    <link href="http://arxiv.org/abs/2306.03312"/>
    <id>http://arxiv.org/abs/2306.03312</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Heilman_S/0/1/0/all/0/1&quot;&gt;Steven Heilman&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We prove the three candidate Plurality is Stablest Conjecture of
Khot-Kindler-Mossel-O&#39;Donnell from 2005 for correlations $\rho$ satisfying
$-1/36&amp;lt;\rho&amp;lt;1/11$: the Plurality function is the most noise stable three
candidate election method with small influences, when the corrupted votes have
correlation $-1/36&amp;lt;\rho&amp;lt;1/11$ with the original votes. The previous best result
of this type only achieved positive correlations at most $10^{-10^{10}}$. Our
result follows by solving the three set Standard Simplex Conjecture of
Isaksson-Mossel from 2011 for all correlations $-1/36&amp;lt;\rho&amp;lt;1/11$.
&lt;/p&gt;
&lt;p&gt;The Gaussian Double Bubble Problem corresponds to the case $\rho\to1^{-}$, so
in some sense, our result is a generalization of the Gaussian Double Bubble
Problem. Our result is also notable since it is the first result for any
$\rho&amp;lt;0$, which is the only relevant case for computational hardness of
MAX-3-CUT. As an additional corollary, we conclude that three candidate Borda
Count is stablest for all $-1/36&amp;lt;\rho&amp;lt;1/11$.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: Complexity of Anchored Crossing Number and Crossing Number of Almost Planar Graphs</title>
    <link href="http://arxiv.org/abs/2306.03490"/>
    <id>http://arxiv.org/abs/2306.03490</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hlineny_P/0/1/0/all/0/1&quot;&gt;Petr Hlin&amp;#x11b;n&amp;#xfd;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this paper we deal with the problem of computing the exact crossing number
of almost planar graphs and the closely related problem of computing the exact
anchored crossing number of a pair of planar graphs. It was shown by [Cabello
and Mohar, 2013] that both problems are NP-hard; although they required an
unbounded number of high-degree vertices (in the first problem) or an unbounded
number of anchors (in the second problem) to prove their result. Somehow
surprisingly, only three vertices of degree greater than 3, or only three
anchors, are sufficient to maintain hardness of these problems, as we prove
here. The new result also improves the previous result on hardness of joint
crossing number on surfaces by [Hlin\v{e}n\&#39;y and Salazar, 2015]. Our result is
best possible in the anchored case since the anchored crossing number of a pair
of planar graphs with two anchors each is trivial, and close to being best
possible in the almost planar case since the crossing number is efficiently
computable for almost planar graphs of maximum degree 3 [Riskin 1996, Cabello
and Mohar 2011].
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Tight Complexity Bounds for Counting Generalized Dominating Sets in Bounded-Treewidth Graphs Part II: Hardness Results</title>
    <link href="http://arxiv.org/abs/2306.03640"/>
    <id>http://arxiv.org/abs/2306.03640</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Focke_J/0/1/0/all/0/1&quot;&gt;Jacob Focke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marx_D/0/1/0/all/0/1&quot;&gt;D&amp;#xe1;niel Marx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inerney_F/0/1/0/all/0/1&quot;&gt;Fionn Mc Inerney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neuen_D/0/1/0/all/0/1&quot;&gt;Daniel Neuen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankar_G/0/1/0/all/0/1&quot;&gt;Govind S. Sankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schepper_P/0/1/0/all/0/1&quot;&gt;Philipp Schepper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wellnitz_P/0/1/0/all/0/1&quot;&gt;Philip Wellnitz&lt;/a&gt;&lt;/p&gt;&lt;p&gt;For a well-studied family of domination-type problems, in bounded-treewidth
graphs, we investigate whether it is possible to find faster algorithms. For
sets $\sigma,\rho$ of non-negative integers, a $(\sigma,\rho)$-set of a graph
$G$ is a set $S$ of vertices such that $|N(u)\cap S|\in \sigma$ for every $u\in
S$, and $|N(v)\cap S|\in \rho$ for every $v\not\in S$. The problem of finding a
$(\sigma,\rho)$-set (of a certain size) unifies common problems like
$\text{Independent Set}$, $\text{Dominating Set}$, $\text{Independent
Dominating Set}$, and many others.
&lt;/p&gt;
&lt;p&gt;In an accompanying paper, it is proven that, for all pairs of finite or
cofinite sets $(\sigma,\rho)$, there is an algorithm that counts
$(\sigma,\rho)$-sets in time $(c_{\sigma,\rho})^{\text{tw}}\cdot n^{O(1)}$ (if
a tree decomposition of width $\text{tw}$ is given in the input). Here,
$c_{\sigma,\rho}$ is a constant with an intricate dependency on $\sigma$ and
$\rho$. Despite this intricacy, we show that the algorithms in the accompanying
paper are most likely optimal, i.e., for any pair $(\sigma, \rho)$ of finite or
cofinite sets where the problem is non-trivial, and any $\varepsilon&amp;gt;0$, a
$(c_{\sigma,\rho}-\varepsilon)^{\text{tw}}\cdot n^{O(1)}$-algorithm counting
the number of $(\sigma,\rho)$-sets would violate the Counting Strong
Exponential-Time Hypothesis ($\#$SETH). For finite sets $\sigma$ and $\rho$,
our lower bounds also extend to the decision version, showing that those
algorithms are optimal in this setting as well.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: On the Parameterized Complexity of Computing $st$-Orientations with Few Transitive Edges</title>
    <link href="http://arxiv.org/abs/2306.03196"/>
    <id>http://arxiv.org/abs/2306.03196</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Binucci_C/0/1/0/all/0/1&quot;&gt;Carla Binucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liotta_G/0/1/0/all/0/1&quot;&gt;Giuseppe Liotta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montecchiani_F/0/1/0/all/0/1&quot;&gt;Fabrizio Montecchiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortali_G/0/1/0/all/0/1&quot;&gt;Giacomo Ortali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piselli_T/0/1/0/all/0/1&quot;&gt;Tommaso Piselli&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Orienting the edges of an undirected graph such that the resulting digraph
satisfies some given constraints is a classical problem in graph theory, with
multiple algorithmic applications. In particular, an $st$-orientation orients
each edge of the input graph such that the resulting digraph is acyclic, and it
contains a single source $s$ and a single sink $t$. Computing an
$st$-orientation of a graph can be done efficiently, and it finds notable
applications in graph algorithms and in particular in graph drawing. On the
other hand, finding an $st$-orientation with at most $k$ transitive edges is
more challenging and it was recently proven to be NP-hard already when $k=0$.
We strengthen this result by showing that the problem remains NP-hard even for
graphs of bounded diameter, and for graphs of bounded vertex degree. These
computational lower bounds naturally raise the question about which structural
parameters can lead to tractable parameterizations of the problem. Our main
result is a fixed-parameter tractable algorithm parameterized by treewidth.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Accelerating Range Minimum Queries with Ray Tracing Cores</title>
    <link href="http://arxiv.org/abs/2306.03282"/>
    <id>http://arxiv.org/abs/2306.03282</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meneses_E/0/1/0/all/0/1&quot;&gt;Enzo Meneses&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navarro_C/0/1/0/all/0/1&quot;&gt;Crist&amp;#xf3;bal A. Navarro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrada_H/0/1/0/all/0/1&quot;&gt;H&amp;#xe9;ctor Ferrada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quezada_F/0/1/0/all/0/1&quot;&gt;Felipe A. Quezada&lt;/a&gt;&lt;/p&gt;&lt;p&gt;During the last decade GPU technology has shifted from pure general purpose
computation to the inclusion of application specific integrated circuits
(ASICs), such as Tensor Cores and Ray Tracing (RT) cores. Although these
special purpose GPU cores were designed to further accelerate specific fields
such as AI and real-time rendering, recent research has managed to exploit them
to further accelerate other tasks that typically used regular GPU computing. In
this work we present RTXRMQ, a new approach that can compute range minimum
queries (RMQs) with RT cores. The main contribution is the proposal of a
geometric solution for RMQ, where elements become triangles that are placed and
shaped according to the element&#39;s value and position in the array,
respectively, such that the closest hit of a ray launched from a point given by
the query parameters corresponds to the result of that query. Experimental
results show that RTXRMQ is currently best suited for small query ranges
relative to the problem size, achieving up to $5\times$ and $2.3\times$ of
speedup over state of the art CPU (HRMQ) and GPU (LCA) approaches,
respectively. Although for medium and large query ranges RTXRMQ is currently
surpassed by LCA, it is still competitive by being $2.5\times$ and $4\times$
faster than HRMQ which is a highly parallel CPU approach. Furthermore,
performance scaling experiments across the latest RTX GPU architectures show
that if the current RT scaling trend continues, then RTXRMQ&#39;s performance would
scale at a higher rate than HRMQ and LCA, making the approach even more
relevant for future high performance applications that employ batches of RMQs.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Tracking Evolving labels using Cone based Oracles</title>
    <link href="http://arxiv.org/abs/2306.03306"/>
    <id>http://arxiv.org/abs/2306.03306</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1&quot;&gt;Aditya Acharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mount_D/0/1/0/all/0/1&quot;&gt;David Mount&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The evolving data framework was first proposed by Anagnostopoulos et al.,
where an evolver makes small changes to a structure behind the scenes. Instead
of taking a single input and producing a single output, an algorithm
judiciously probes the current state of the structure and attempts to
continuously maintain a sketch of the structure that is as close as possible to
its actual state. There have been a number of problems that have been studied
in the evolving framework including our own work on labeled trees. We were
motivated by the problem of maintaining a labeling in the plane, where updating
the labels require physically moving them. Applications involve tracking
evolving disease hot-spots via mobile testing units , and tracking unmanned
aerial vehicles. To be specific, we consider the problem of tracking labeled
nodes in the plane, where an evolver continuously swaps labels of any two
nearby nodes in the background unknown to us. We are tasked with maintaining a
hypothesis, an approximate sketch of the locations of these labels, which we
can only update by physically moving them over a sparse graph. We assume the
existence of an Oracle, which when suitably probed, guides us in fixing our
hypothesis.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: A Combinatorial Certifying Algorithm for Linear Programming Problems with Gainfree Leontief Substitution Systems</title>
    <link href="http://arxiv.org/abs/2306.03368"/>
    <id>http://arxiv.org/abs/2306.03368</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kimura_K/0/1/0/all/0/1&quot;&gt;Kei Kimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makino_K/0/1/0/all/0/1&quot;&gt;Kazuhisa Makino&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Linear programming (LP) problems with gainfree Leontief substitution systems
have been intensively studied in economics and operations research, and include
the feasibility problem of a class of Horn systems, which arises in, e.g.,
polyhedral combinatorics and logic. This subclass of LP problems admits a
strongly polynomial time algorithm, where devising such an algorithm for
general LP problems is one of the major theoretical open questions in
mathematical optimization and computer science. Recently, much attention has
been paid to devising certifying algorithms in software engineering, since
those algorithms enable one to confirm the correctness of outputs of programs
with simple computations. In this paper, we provide the first combinatorial
(and strongly polynomial time) certifying algorithm for LP problems with
gainfree Leontief substitution systems. As a by-product, we answer
affirmatively an open question whether the feasibility problem of the class of
Horn systems admits a combinatorial certifying algorithm.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Rigorous Runtime Analysis of MOEA/D for Solving Multi-Objective Minimum Weight Base Problems</title>
    <link href="http://arxiv.org/abs/2306.03409"/>
    <id>http://arxiv.org/abs/2306.03409</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_A/0/1/0/all/0/1&quot;&gt;Anh Viet Do&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_A/0/1/0/all/0/1&quot;&gt;Aneta Neumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_F/0/1/0/all/0/1&quot;&gt;Frank Neumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutton_A/0/1/0/all/0/1&quot;&gt;Andrew M. Sutton&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the multi-objective minimum weight base problem, an abstraction of
classical NP-hard combinatorial problems such as the multi-objective minimum
spanning tree problem. We prove some important properties of the convex hull of
the non-dominated front, such as its approximation quality and an upper bound
on the number of extreme points. Using these properties, we give the first
run-time analysis of the MOEA/D algorithm for this problem, an evolutionary
algorithm that effectively optimizes by decomposing the objectives into
single-objective components. We show that the MOEA/D, given an appropriate
decomposition setting, finds all extreme points within expected fixed-parameter
polynomial time in the oracle model, the parameter being the number of
objectives. Experiments are conducted on random bi-objective minimum spanning
tree instances, and the results agree with our theoretical findings.
Furthermore, compared with a previously studied evolutionary algorithm for the
problem GSEMO, MOEA/D finds all extreme points much faster across all
instances.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Minimizing Hitting Time between Disparate Groups with Shortcut Edges</title>
    <link href="http://arxiv.org/abs/2306.03571"/>
    <id>http://arxiv.org/abs/2306.03571</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adriaens_F/0/1/0/all/0/1&quot;&gt;Florian Adriaens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Honglian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gionis_A/0/1/0/all/0/1&quot;&gt;Aristides Gionis&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Structural bias or segregation of networks refers to situations where two or
more disparate groups are present in the network, so that the groups are highly
connected internally, but loosely connected to each other. In many cases it is
of interest to increase the connectivity of disparate groups so as to, e.g.,
minimize social friction, or expose individuals to diverse viewpoints. A
commonly-used mechanism for increasing the network connectivity is to add edge
shortcuts between pairs of nodes. In many applications of interest, edge
shortcuts typically translate to recommendations, e.g., what video to watch, or
what news article to read next. The problem of reducing structural bias or
segregation via edge shortcuts has recently been studied in the literature, and
random walks have been an essential tool for modeling navigation and
connectivity in the underlying networks. Existing methods, however, either do
not offer approximation guarantees, or engineer the objective so that it
satisfies certain desirable properties that simplify the optimization~task. In
this paper we address the problem of adding a given number of shortcut edges in
the network so as to directly minimize the average hitting time and the maximum
hitting time between two disparate groups. Our algorithm for minimizing average
hitting time is a greedy bicriteria that relies on supermodularity. In
contrast, maximum hitting time is not supermodular. Despite, we develop an
approximation algorithm for that objective as well, by leveraging connections
with average hitting time and the asymmetric k-center problem.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Representative set statements for delta-matroids and the Mader delta-matroid</title>
    <link href="http://arxiv.org/abs/2306.03605"/>
    <id>http://arxiv.org/abs/2306.03605</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wahlstrom_M/0/1/0/all/0/1&quot;&gt;Magnus Wahlstr&amp;#xf6;m&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We present representative sets-style statements for linear delta-matroids,
which are set systems that generalize matroids, with important connections to
matching theory and graph embeddings. Furthermore, our proof uses a new
approach of sieving polynomial families, which generalizes the linear algebra
approach of the representative sets lemma to a setting of bounded-degree
polynomials. The representative sets statements for linear delta-matroids then
follow by analyzing the Pfaffian of the skew-symmetric matrix representing the
delta-matroid. Applying the same framework to the determinant instead of the
Pfaffian recovers the representative sets lemma for linear matroids.
Altogether, this significantly extends the toolbox available for kernelization.
&lt;/p&gt;
&lt;p&gt;As an application, we show an exact sparsification result for Mader networks:
Let $G=(V,E)$ be a graph and $\mathcal{T}$ a partition of a set of terminals $T
\subseteq V(G)$, $|T|=k$. A $\mathcal{T}$-path in $G$ is a path with endpoints
in distinct parts of $\mathcal{T}$ and internal vertices disjoint from $T$. In
polynomial time, we can derive a graph $G&#39;=(V&#39;,E&#39;)$ with $T \subseteq V(G&#39;)$,
such that for every subset $S \subseteq T$ there is a packing of
$\mathcal{T}$-paths with endpoints $S$ in $G$ if and only if there is one in
$G&#39;$, and $|V(G&#39;)|=O(k^3)$. This generalizes the (undirected version of the)
cut-covering lemma, which corresponds to the case that $\mathcal{T}$ contains
only two blocks.
&lt;/p&gt;
&lt;p&gt;To prove the Mader network sparsification result, we furthermore define the
class of Mader delta-matroids, and show that they have linear representations.
This should be of independent interest.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Buying Information for Stochastic Optimization</title>
    <link href="http://arxiv.org/abs/2306.03607"/>
    <id>http://arxiv.org/abs/2306.03607</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Mingchen Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1&quot;&gt;Christos Tzamos&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Stochastic optimization is one of the central problems in Machine Learning
and Theoretical Computer Science. In the standard model, the algorithm is given
a fixed distribution known in advance. In practice though, one may acquire at a
cost extra information to make better decisions. In this paper, we study how to
buy information for stochastic optimization and formulate this question as an
online learning problem. Assuming the learner has an oracle for the original
optimization problem, we design a $2$-competitive deterministic algorithm and a
$e/(e-1)$-competitive randomized algorithm for buying information. We show that
this ratio is tight as the problem is equivalent to a robust generalization of
the ski-rental problem, which we call super-martingale stopping.
&lt;/p&gt;
&lt;p&gt;We also consider an adaptive setting where the learner can choose to buy
information after taking some actions for the underlying optimization problem.
We focus on the classic optimization problem, Min-Sum Set Cover, where the goal
is to quickly find an action that covers a given request drawn from a known
distribution. We provide an $8$-competitive algorithm running in polynomial
time that chooses actions and decides when to buy information about the
underlying request.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Constant Sequence Extension for Fast Search Using Weighted Hamming Distance</title>
    <link href="http://arxiv.org/abs/2306.03612"/>
    <id>http://arxiv.org/abs/2306.03612</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1&quot;&gt;Huiping Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haizhou Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhiping Lin&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Representing visual data using compact binary codes is attracting increasing
attention as binary codes are used as direct indices into hash table(s) for
fast non-exhaustive search. Recent methods show that ranking binary codes using
weighted Hamming distance (WHD) rather than Hamming distance (HD) by generating
query-adaptive weights for each bit can better retrieve query-related items.
However, search using WHD is slower than that using HD. One main challenge is
that the complexity of extending a monotone increasing sequence using WHD to
probe buckets in hash table(s) for existing methods is at least proportional to
the square of the sequence length, while that using HD is proportional to the
sequence length. To overcome this challenge, we propose a novel fast
non-exhaustive search method using WHD. The key idea is to design a constant
sequence extension algorithm to perform each sequence extension in constant
computational complexity and the total complexity is proportional to the
sequence length, which is justified by theoretical analysis. Experimental
results show that our method is faster than other WHD-based search methods.
Also, compared with the HD-based non-exhaustive search method, our method has
comparable efficiency but retrieves more query-related items for the dataset of
up to one billion items.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Efficient Centrality Maximization with Rademacher Averages</title>
    <link href="http://arxiv.org/abs/2306.03651"/>
    <id>http://arxiv.org/abs/2306.03651</id>
    <updated>2023-06-07T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellegrina_L/0/1/0/all/0/1&quot;&gt;Leonardo Pellegrina&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The identification of the set of k most central nodes of a graph, or
centrality maximization, is a key task in network analysis, with various
applications ranging from finding communities in social and biological networks
to understanding which seed nodes are important to diffuse information in a
graph. As the exact computation of centrality measures does not scale to
modern-sized networks, the most practical solution is to resort to rigorous,
but efficiently computable, randomized approximations. In this work we present
CentRA, the first algorithm based on progressive sampling to compute
high-quality approximations of the set of k most central nodes. CentRA is based
on a novel approach to efficiently estimate Monte Carlo Rademacher Averages, a
powerful tool from statistical learning theory to compute sharp data-dependent
approximation bounds. Then, we study the sample complexity of centrality
maximization using the VC-dimension, a key concept from statistical learning
theory. We show that the number of random samples required to compute
high-quality approximations scales with finer characteristics of the graph,
such as its vertex diameter, or of the centrality of interest, significantly
improving looser bounds derived from standard techniques. We apply CentRA to
analyze large real-world networks, showing that it significantly outperforms
the state-of-the-art approximation algorithm in terms of number of samples,
running times, and accuracy.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: A survey of approximation algorithms for capacitated vehicle routing problems</title>
    <link href="http://arxiv.org/abs/2306.01826"/>
    <id>http://arxiv.org/abs/2306.01826</id>
    <updated>2023-06-06T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yongyu Chen&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Finding the shortest travelling tour of vehicles with capacity k from the
depot to the customers is called the Capacity vehicle routing problem (CVRP).
CVRP plays an essential position in logistics systems, and it is the most
intensively studied problem in combinatorial optimization. In complexity, CVRP
with k $\ge$ 3 is an NP-hard problem, and it is APX-hard as well. We already
knew that it could not be approximated in metric space. Moreover, it is the
first problem resisting Arora&#39;s famous approximation framework. So, whether
there is, a polynomial-time (1+$\epsilon$)-approximation for the Euclidean CVRP
for any $\epsilon&amp;gt;0$ is still an open problem. This paper will summarize the
research progress from history to up-to-date developments. The survey will be
updated periodically.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Revisiting Garg&#39;s 2-Approximation Algorithm for the k-MST Problem in Graphs</title>
    <link href="http://arxiv.org/abs/2306.01867"/>
    <id>http://arxiv.org/abs/2306.01867</id>
    <updated>2023-06-06T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breen_E/0/1/0/all/0/1&quot;&gt;Emmett Breen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirka_R/0/1/0/all/0/1&quot;&gt;Renee Mirka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zichen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williamson_D/0/1/0/all/0/1&quot;&gt;David P. Williamson&lt;/a&gt;&lt;/p&gt;&lt;p&gt;This paper revisits the 2-approximation algorithm for $k$-MST presented by
Garg in light of a recent paper of Paul et al.. In the $k$-MST problem, the
goal is to return a tree spanning $k$ vertices of minimum total edge cost. Paul
et al. extend Garg&#39;s primal-dual subroutine to improve the approximation ratios
for the budgeted prize-collecting traveling salesman and minimum spanning tree
problems. We follow their algorithm and analysis to provide a cleaner version
of Garg&#39;s result. Additionally, we introduce the novel concept of a kernel
which allows an easier visualization of the stages of the algorithm and a
clearer understanding of the pruning phase. Other notable updates include
presenting a linear programming formulation of the $k$-MST problem, including
pseudocode, replacing the coloring scheme used by Garg with the simpler concept
of neutral sets, and providing an explicit potential function.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Fast $(1+\varepsilon)$-Approximation Algorithms for Binary Matrix Factorization</title>
    <link href="http://arxiv.org/abs/2306.01869"/>
    <id>http://arxiv.org/abs/2306.01869</id>
    <updated>2023-06-06T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velingker_A/0/1/0/all/0/1&quot;&gt;Ameya Velingker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Votsch_M/0/1/0/all/0/1&quot;&gt;Maximilian V&amp;#xf6;tsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1&quot;&gt;David P. Woodruff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Samson Zhou&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We introduce efficient $(1+\varepsilon)$-approximation algorithms for the
binary matrix factorization (BMF) problem, where the inputs are a matrix
$\mathbf{A}\in\{0,1\}^{n\times d}$, a rank parameter $k&amp;gt;0$, as well as an
accuracy parameter $\varepsilon&amp;gt;0$, and the goal is to approximate $\mathbf{A}$
as a product of low-rank factors $\mathbf{U}\in\{0,1\}^{n\times k}$ and
$\mathbf{V}\in\{0,1\}^{k\times d}$. Equivalently, we want to find $\mathbf{U}$
and $\mathbf{V}$ that minimize the Frobenius loss $\|\mathbf{U}\mathbf{V} -
\mathbf{A}\|_F^2$. Before this work, the state-of-the-art for this problem was
the approximation algorithm of Kumar et. al. [ICML 2019], which achieves a
$C$-approximation for some constant $C\ge 576$. We give the first
$(1+\varepsilon)$-approximation algorithm using running time singly exponential
in $k$, where $k$ is typically a small integer. Our techniques generalize to
other common variants of the BMF problem, admitting bicriteria
$(1+\varepsilon)$-approximation algorithms for $L_p$ loss functions and the
setting where matrix operations are performed in $\mathbb{F}_2$. Our approach
can be implemented in standard big data models, such as the streaming or
distributed models.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Auditable data structures: theory and applications</title>
    <link href="http://arxiv.org/abs/2306.01886"/>
    <id>http://arxiv.org/abs/2306.01886</id>
    <updated>2023-06-06T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canciani_A/0/1/0/all/0/1&quot;&gt;Andrea Canciani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felicioli_C/0/1/0/all/0/1&quot;&gt;Claudio Felicioli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Severino_F/0/1/0/all/0/1&quot;&gt;Fabio Severino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tortola_D/0/1/0/all/0/1&quot;&gt;Domenico Tortola&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Every digital process needs to consume some data in order to work properly.
It is very common for applications to use some external data in their
processes, getting them by sources such as external APIs. Therefore, trusting
the received data becomes crucial in such scenarios, considering that if the
data are not self-produced by the consumer, the trust in the external data
source, or in the data that the source produces, can not always be taken for
granted. The most used approach to generate trust in the external source is
based on authenticated data structures, that are able to authenticate the
source when queried through the generation of proofs. Such proofs are useful to
assess authenticity or integrity, however, an external user could also be
interested in verifying the data history and its consistency. This problem
seems to be unaddressed by current literature, which proposes some approaches
aimed at executing audits by internal actors with prior knowledge about the
data structures. In this paper, we address the scenario of an external auditor
with no data knowledge that wants to verify the data history consistency. We
analyze the terminology and the current state of the art of the auditable data
structures, then we will propose a general framework to support external audits
from both internal and external users.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Windows on Theory: The (local) unit of intelligence is FLOPs</title>
    <link href="https://windowsontheory.org/2023/06/05/the-local-unit-of-intelligence-is-flops/"/>
    <id>http://windowsontheory.org/?p=8630</id>
    <updated>2023-06-05T18:22:58+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;&lt;em&gt;[Crossposting again on&amp;nbsp;&lt;/em&gt;&lt;a href=&quot;https://www.lesswrong.com/&quot;&gt;&lt;em&gt;&lt;u&gt;Lesswrong&lt;/u&gt;&lt;/em&gt;&lt;/a&gt;&lt;em&gt; and&amp;nbsp;&lt;/em&gt;&lt;a href=&quot;https://windowsontheory.org/&quot;&gt;&lt;em&gt;&lt;u&gt;Windowsontheory&lt;/u&gt;&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, with the hope I am not overstaying my welcome in LW.]&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;&lt;br&gt;Wealth can be measured by &lt;em&gt;dollars&lt;/em&gt;. This is not a perfect measurement: it’s hard to account for purchasing power and circumstances when comparing people across varying countries or time periods. However, within a particular place and time, one can measure wealth in the local currency. It still does not capture everything (e.g., future earnings, social connections). But generally, all else being roughly equal, the more dollars one has, the wealthier one is.&lt;/p&gt;



&lt;p&gt;How do we measure intelligence? I am not interested in measuring the intelligence of individual humans or individual animals. Nor am I looking for a universal absolute scale of intelligence on which we could rank humans, elephants, and GPT4. (Indeed, it doesn’t seem that a one-dimensional comparison can be made; for example, we seem to be more intelligent than elephants on most dimensions, but they do have an&amp;nbsp;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S014976340700070X&quot;&gt;&lt;u&gt;impressive memory&lt;/u&gt;&lt;/a&gt;.)&amp;nbsp; Rather, I want to compare different&amp;nbsp;&lt;em&gt;species&lt;/em&gt; within the same genus or different&amp;nbsp;&lt;em&gt;models&lt;/em&gt; within the same general architecture (e.g., Transformers).&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I think it’s fair to say that the local unit of intelligence for animal species is&amp;nbsp;&lt;em&gt;neurons&lt;/em&gt;. While elephants have larger brains than humans, within the genus&amp;nbsp;&lt;em&gt;Homo&lt;/em&gt;, to a first approximation, the bigger the brain, the more intelligent the species.&amp;nbsp;&lt;/p&gt;



&lt;figure class=&quot;wp-block-image&quot;&gt;&lt;img src=&quot;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e5d88d991fc175c7676c6ada658142cd44b9887c69c15521.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;



&lt;p&gt;(Figure from&amp;nbsp;&lt;a href=&quot;https://chomsky.info/20140826/&quot;&gt;&lt;u&gt;Bolihus et al.&lt;/u&gt;&lt;/a&gt;)&lt;/p&gt;



&lt;p&gt;I claim that within the current architectures and training frameworks of large language models,&amp;nbsp;&lt;strong&gt;the local unit of intelligence is FLOPs&lt;/strong&gt;. That is, as long as we follow the current paradigm of training transformer-based architectures within best practices of scaling compute and data, the more compute resources (FLOPs) invested in training the model, the more intelligent it is. This is an imperfect measurement, but probably one that is better than trying to give models “IQ exams” that were designed for humans (and even there have&amp;nbsp;&lt;a href=&quot;https://erikhoel.substack.com/p/your-iq-isnt-160-no-ones-is&quot;&gt;&lt;u&gt;dubious value&lt;/u&gt;&lt;/a&gt;).&amp;nbsp; Another way to say this is that the intelligence of the model scales with the number of&amp;nbsp;&lt;strong&gt;“load-bearing gradient steps”&lt;/strong&gt; that have gone into training it.&lt;/p&gt;



&lt;p&gt;So far, it might seem like a tautology, but as I claimed in the&amp;nbsp;&lt;a href=&quot;https://windowsontheory.org/2023/05/19/gpt-as-an-intelligence-forklift/&quot;&gt;&lt;u&gt;“intelligence forklift” post&lt;/u&gt;&lt;/a&gt;, this does have some implications. In particular, current general-purpose models such as ChatGPT are built in two phases. The first phase is a&amp;nbsp;&lt;strong&gt;pretraining phase&lt;/strong&gt;, in which the model is trained in a Trillion or more gradient steps on the next-token prediction task. The second phase is the&amp;nbsp;&lt;strong&gt;adaptation/fine-tuning phase&lt;/strong&gt;, in which, whether through instruction-tuning, reinforcement learning on human feedback (RLHF) or other methods, the model is “fine tuned” using fewer than a million gradient steps to be a better instruction-following or chatting agent. In other words, more than 99.9% (maybe as much as 99.9999%) of the FLOPs / gradient steps in training the model are invested during its pretraining phase. (One reason that the fine-tuning phase involves much fewer gradient steps is that, while the first phase can use any static data grabbed from the Internet, the second phase requires data that was especially collected for this task and often needs human labeling as well.)&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The adaptation phase can make a huge difference in the usefulness of the model. The&amp;nbsp;&lt;a href=&quot;https://chat.lmsys.org/?arena&quot;&gt;&lt;u&gt;chatbot arena&lt;/u&gt;&lt;/a&gt; doesn’t even contain non-fine-tuned models, and we can see that smaller but well-tuned models can put up a decent fight against ones that have at least 10 times the parameters (and so roughly at least 100 times the training compute). Unlike sashimi, language models should not be consumed raw. &amp;nbsp;&lt;/p&gt;



&lt;figure class=&quot;wp-block-image&quot;&gt;&lt;img src=&quot;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/fb47b643997a81bbcc3b8b7ef043ec132b28ff4071de246f.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;



&lt;p&gt;&lt;br&gt;However, their “intelligence” is ultimately derived from the FLOPs invested in the base models. (See also &lt;a href=&quot;https://arxiv.org/abs/2305.15717&quot;&gt;&lt;u&gt;this paper &lt;/u&gt;&lt;/a&gt;on the limitations of fine-tuning to close capability gaps.) Fine-tuning, whether using RL or not, is the proverbial “&lt;a href=&quot;https://medium.com/syncedreview/yann-lecun-cake-analogy-2-0-a361da560dae&quot;&gt;&lt;u&gt;cherry on the cake&lt;/u&gt;&lt;/a&gt;” and the pre-trained model captures more than 99.9% of the intelligence of the model.  That pretrained model is &lt;a href=&quot;https://www.lesswrong.com/s/N7nDePaNabJdnbXeE/p/vJFdjigzmcXMhNTsx&quot;&gt;&lt;u&gt;not an agent&lt;/u&gt;&lt;/a&gt; and &lt;a href=&quot;https://astralcodexten.substack.com/p/janus-simulators&quot;&gt;&lt;u&gt;does not have goals&lt;/u&gt;&lt;/a&gt; though it can “play one on TV” in the sense of coming up with plans and proposed actions if prompted to do so. (In LW language, a &lt;a href=&quot;https://www.lesswrong.com/tag/simulator-theory&quot;&gt;simulator&lt;/a&gt;.) This is why a pretrained model can be modeled as an &lt;a href=&quot;https://www.lesswrong.com/posts/wDL6wiqg3c6WFisHq/gpt-as-an-intelligence-forklift&quot;&gt;&lt;u&gt;“intelligence forklift”&lt;/u&gt;&lt;/a&gt;. Just like a forklift supplies strength but is useless without someone driving it, so does the pretrained model supply intelligence, but that intelligence needs to be directed via fine-tuning, conditioning on prompts, etc.  Another way to think of the pre-trained model is as the bee colony and the adapter as the queen. (That is, if the queen bee was actually telling bees what to do rather than just &lt;a href=&quot;https://www.perfectbee.com/learn-about-bees/the-life-of-bees/role-queen-bee&quot;&gt;&lt;u&gt;laying eggs&lt;/u&gt;&lt;/a&gt;.)&lt;/p&gt;



&lt;figure class=&quot;wp-block-image&quot;&gt;&lt;img src=&quot;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/da45c0196d68d4c0a178bc2d54360510fa6ca80b2caa40ec.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;



&lt;p&gt;In that sense, while I agree with&amp;nbsp;&lt;a href=&quot;https://gwern.net/tool-ai&quot;&gt;&lt;u&gt;Gwern&lt;/u&gt;&lt;/a&gt; that agentic models are more&amp;nbsp;&lt;em&gt;useful&lt;/em&gt; and that&lt;em&gt; “we don’t want low log-loss error on ImageNet, we want to refind a particular personal photo”&lt;/em&gt; , I disagree that&amp;nbsp;&lt;em&gt;“Agent AIs [will be] more intelligent than Tool AIs.”&amp;nbsp;&lt;/em&gt;Intelligence and usefulness are not the same thing.&lt;/p&gt;



&lt;h2 class=&quot;wp-block-heading&quot;&gt;Implications for alignment&lt;/h2&gt;



&lt;p&gt;If the pre-trained model does not have goals, then there is no sense in “aligning” it. Rather, there is a separation of concerns, with a highly intelligent but goal-less pre-trained model (“forklift”) and a not-so-intelligent but goal-directed adaptor (“driver”). It is the latter one that we need to align:&lt;/p&gt;



&lt;blockquote class=&quot;wp-block-quote&quot;&gt;
&lt;p&gt;&lt;strong&gt;The component of an AI system that needs to be aligned is not the component that accounts for its intelligence.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;



&lt;p&gt;That is a hopeful lesson since the adaptor can be a much smaller (e.g. have drastically&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot;&gt;&lt;u&gt;fewer parameters&lt;/u&gt;&lt;/a&gt;) and tractable object. However, it does not mean that the alignment problem is easy and that we are insulated from the complexities of the pretrained model:&lt;/p&gt;



&lt;blockquote class=&quot;wp-block-quote&quot;&gt;
&lt;p&gt;&lt;strong&gt;A forklift with a speed of 1000mph might not be actively trying to kill you, but this could still be the end result.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;



&lt;p&gt;In particular, we don’t understand the biases the pre-trained model inherits from the data, nor the way that these may play out when we use the model in applications. However, it does seem that for a pretrained model to be as good at its job as possible, it should learn all the biases in its data but not be constrained to any of them. It should be able to adapt to any context real or imagined and be the “perfect actor” that can take on any character’s personality.&lt;/p&gt;



&lt;p&gt;The traditional “anthropomorphic” view of intelligence is as something that “belongs” to an individual or&amp;nbsp;&lt;em&gt;agent&amp;nbsp;&lt;/em&gt;and that this agent has some sort of preferences or goals (a.k.a a&amp;nbsp;&lt;em&gt;utility function&lt;/em&gt;). Hence a potential future super-intelligent AI was thought of as an “alien” that pursues some goals. Under this viewpoint, we want to either “box” the alien to control its impact or “align” its goals to ours. Both of these options treat the AI system as a single component encompassing both goals and intelligence. However, if goals and intelligence parts correspond to different components, we may be able to&amp;nbsp;&lt;strong&gt;“take the alien’s brain for a ride”&lt;/strong&gt; and build a variety of systems that share the same&amp;nbsp;&lt;em&gt;capabilities&lt;/em&gt; but have very different objectives and profiles.&lt;/p&gt;



&lt;p&gt;To be clear, the “intelligence forklift” view does not preclude building an “anti-aligned” agent on top of a pre-trained model that is&amp;nbsp;&lt;em&gt;malicious&lt;/em&gt;,&amp;nbsp;&lt;em&gt;dishonest&lt;/em&gt;, and&amp;nbsp;&lt;em&gt;harmful&lt;/em&gt;. It just means that such an agent would not have an automatic intelligence advantage over other agents (including humans) since all of them can have access to a shared “intelligence engine” provided by the goal-less pretrained models. This is what I illustrated as “scenario 2” in this figure (taken from my&amp;nbsp;&lt;a href=&quot;https://www.lesswrong.com/posts/wDL6wiqg3c6WFisHq/gpt-as-an-intelligence-forklift&quot;&gt;&lt;u&gt;previous post&lt;/u&gt;&lt;/a&gt;):&lt;/p&gt;



&lt;figure class=&quot;wp-block-image&quot;&gt;&lt;img src=&quot;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/67505f5594ae360d8e9d1949c3e201489116d2d8c1ee48ba.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;



&lt;h2 class=&quot;wp-block-heading&quot;&gt;&lt;strong&gt;What about “self play”?&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;The above assumes that the intelligence component of a model is obtained by executing gradient steps on static data, but what if this data is itself generated by the model? This is what happened with games such as Go and Chess. Originally models were trained by predicting the next move of human games scraped from the Internet, but to improve beyond the quality of these data, models needed to play against themselves and generate new games. They could then filter out only the most successful ones and hence generate data that is of higher quality than the original games they trained on. (Eventually, it turned out that with this approach you don’t need to start with&amp;nbsp;&lt;em&gt;any&lt;/em&gt; data for games such as Chess and Go, hence the “Zero” in AlphaZero.)&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Self-play makes a lot of sense in games where there is a very clear notion of winning and losing, but what would be the analog for language models? I don’t know the answer to this in general, but in the realm of scientific literature, there is an analogous process. The model could play the roles of authors and reviewers alike, generate new papers, subject them to peer review, revise and resubmit, etc. At least in fields that don’t require “wet labs”, this could lead to the model simulating the scientific literature of 2024, then 2025, and so on and so forth. Models that manage to do this would be amazing and would speed up scientific progress tremendously. However, I believe they could still be (just more powerful) “intelligence forklifts”. Model outputs influencing its inputs can lead to a &amp;#8220;positive feedback loop,&amp;#8221; and so this is not certain. But I do not see an inherent reason why models could not be arbitrarily intelligent and still completely without goals. In the &lt;a href=&quot;https://astralcodexten.substack.com/p/janus-simulators&quot;&gt;&lt;u&gt;words of Scott Alexander&lt;/u&gt;&lt;/a&gt;, no matter how intelligent they are, models could still be “enlightened” and realize that&lt;/p&gt;



&lt;blockquote class=&quot;wp-block-quote&quot;&gt;
&lt;p&gt;&lt;strong&gt;“once you stop obsessing over the character you’re playing, you notice the GIANT SUPER-ACCURATE WORLD MODEL TAKING UP 99.99% OF YOUR BRAIN.”&lt;/strong&gt;&lt;br&gt;&amp;nbsp;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p class=&quot;authors&quot;&gt;By Boaz Barak&lt;/p&gt;
  </content>
    <author>
      <name>Windows on Theory</name>
      <uri>https://windowsontheory.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Emanuele Viola: Mathematics of the impossible, draft of a book</title>
    <link href="https://emanueleviola.wordpress.com/2023/06/05/mathematics-of-the-impossible-draft-of-a-book/"/>
    <id>http://emanueleviola.wordpress.com/?p=1259</id>
    <updated>2023-06-05T17:23:16+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;I posted a first draft of the book, &lt;a href=&quot;https://www.ccs.neu.edu/home/viola/papers/moti.pdf&quot;&gt;here&lt;/a&gt;. It has more material than the previous blog posts, including a chapter on communication complexity. I plan a major revision, including adding several chapters, but it seems that won&amp;#8217;t happen right away, so I am releasing what I have for now. Any comments are appreciated, either on this blog or via email.&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Manu&lt;/p&gt;
  </content>
    <author>
      <name>Emanuele Viola</name>
      <uri>https://emanueleviola.wordpress.com</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">CCI: jobs: Tenure-track faculty at The Australian National University (apply by May 31, 2024)</title>
    <link href="https://cstheory-jobs.org/2023/06/05/tenure-track-faculty-at-the-australian-national-university-apply-by-may-31-2024/"/>
    <id>http://cstheory-jobs.org/2023/06/05/tenure-track-faculty-at-the-australian-national-university-apply-by-may-31-2024/</id>
    <updated>2023-06-05T06:20:52+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;Tenure-track faculty members in the School of Computing at the Australian National University. Please see the link below for more information.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://jobs.anu.edu.au/jobs/tenure-track-lecturer-senior-lecturer-associate-professor-school-of-computing-canberra-act-act-australia&quot;&gt;https://jobs.anu.edu.au/jobs/tenure-track-lecturer-senior-lecturer-associate-professor-school-of-computing-canberra-act-act-australia&lt;/a&gt;&lt;br /&gt;
Email: ahadn.zehmakan@anu.edu.au&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </content>
    <author>
      <name>CCI: jobs</name>
      <uri>https://cstheory-jobs.org</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">ECCC Papers: TR23-085 |  Average-Case PAC-Learning from Nisan&amp;#39;s Natural Proofs | 

	Ari Karchmer</title>
    <link href="https://eccc.weizmann.ac.il/report/2023/085"/>
    <id>https://eccc.weizmann.ac.il/report/2023/085</id>
    <updated>2023-06-05T05:39:55+00:00</updated>
    <content type="html" xml:lang="en">
    Carmosino et al. (2016) demonstrated that natural proofs of circuit lower bounds imply algorithms for learning circuits with membership queries over the uniform distribution. Indeed, they exercised this implication to obtain a quasi-polynomial time learning algorithm for ${AC}^0[p]$ circuits, for any prime $p$, by leveraging the existing natural proofs from Razborov (1987) and Smolensky (1987). This achievement raises a logical question: can existing natural proofs be adapted into learning algorithms that utilize random examples and learn over unknown, arbitrary example distributions? 

In this work, we show that natural circuit lower bounds proven by specific communication complexity arguments (e.g., Nisan (1994)) witness a ``yes&amp;#39;&amp;#39; answer to this question, under the one limitation of average-case learning. Our primary technical contribution demonstrates a connection between the complexity of learning a concept class in the average-case, and the randomized communication complexity of an evaluation game associated with the class.  We apply this finding to derive polynomial time average-case PAC-learning algorithms that use only random examples from arbitrary and unknown distributions, for any concept class that may be evaluated by (for instance) a majority vote of linear threshold functions.

Additionally, our work contributes to a better understanding of the optimal parameters in XOR lemmas for communication complexity. We address a question posed by Viola and Wigderson (2007) by demonstrating that certain enhancements of parameters in their XOR lemmas are false, assuming the existence of one-way functions.
  </content>
    <author>
      <name>ECCC Papers</name>
      <uri>https://eccc.weizmann.ac.il/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">Computational Complexity: Quantifiers: To Parenthesize or not to Parenthesize?  Matrix of Formula: To Bracket or not to Bracket?</title>
    <link href="https://blog.computationalcomplexity.org/2023/06/quantifiers-to-parenthesize-or-not-to.html"/>
    <id>tag:blogger.com,1999:blog-3722233.post-8658886384920717972</id>
    <updated>2023-06-05T02:42:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p&gt;&amp;nbsp;For the book&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Computational&amp;nbsp; Intractability: A Guide to Algorithmic Lower Bounds&lt;/b&gt;&lt;/p&gt;&lt;p&gt;by Demaine-Gasarch-Hajiaghayi&amp;nbsp;&lt;/p&gt;&lt;p&gt;(See&amp;nbsp; &lt;a href=&quot;https://hardness.mit.edu/&quot;&gt;here&lt;/a&gt;&amp;nbsp;for a link to a first draft.)&amp;nbsp;&lt;/p&gt;&lt;p&gt;we had to make some choices about which notation to use. One of the least important ones was the following:&amp;nbsp;&lt;/p&gt;&lt;p&gt;When defining NP, and in a few other places should we use:&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; (\exists y)(\forall y)[B(x,y)]&lt;/p&gt;&lt;p&gt;or&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp;\exists x : \forall y : B(x,y)&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;or&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; something else.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;We ended up doing it the second way.&amp;nbsp; But I wondered, which, if either, is standard. So I looked in many math and theoretical CS books looking for places they used quantifiers. Here is what I found&lt;/p&gt;&lt;p&gt;a) Most papers and books really don&#39;t use quantifiers at all!&amp;nbsp; This surprised me.&amp;nbsp;&lt;/p&gt;&lt;p&gt;b) When quantifiers are used, they are used in definitions, not theorems.&amp;nbsp;&lt;/p&gt;&lt;p&gt;c) One exception is in logic when they deal with formulas as objects onto themselves.&amp;nbsp; For example, the inductive definition of a formula will have a step:&lt;/p&gt;&lt;p&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;If f(x_1,...,x_n) is a formula then (\exists x_i)[f(x_1,...,x_n)] is a formula.&amp;nbsp;&lt;/p&gt;&lt;p&gt;d) Here is a list of the few places I saw quantifiers used and if they used parenthesis or not. I say if it has parenthesis (abbreviated Parens)&amp;nbsp; or not, and if the matrix of the formula is in square brackets, no brackets, or something ese.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/800157.805047&quot;&gt;Cook&#39;s classic paper&lt;/a&gt;&amp;nbsp;.&amp;nbsp;&lt;/i&gt;Page 154 Parens, no Brackets (1971)&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/030439757690061X?via%3Dihub&quot;&gt;&lt;i&gt;Stockmeyer&#39;s paper where he defines PH&lt;/i&gt;&lt;/a&gt;.&amp;nbsp; Page 6 Parens and Brackets&amp;nbsp; (1976)&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Computers and Intractability&lt;/i&gt; by Garey &amp;amp; Johnson. Page 164. Parens and Brackets (1979)&lt;/p&gt;&lt;p&gt;&lt;i&gt;Morass-like construction of aleph_2 trees in L&lt;/i&gt; by Devlin.&amp;nbsp; Page 2 Parens and matrix in Parens (1979)&lt;/p&gt;&lt;p&gt;&lt;i&gt;Descriptive Complexity by Immerman.&lt;/i&gt;&amp;nbsp;Page 38 Parens no Brackets (1999)&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Bounded Queries in Recursion Theory &lt;/i&gt;by Gasarch and Martin. Parens and Brackets&amp;nbsp; Throughout the book.&amp;nbsp; (1999)&lt;/p&gt;&lt;p&gt;&lt;i&gt;Complexity Theory from Godel to Feynman&lt;/i&gt;&amp;nbsp;by Rudich. No Parens, No Brackets in Def of PH. (2003)&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://yaroslavvb.com/upload/flum.pdf&quot;&gt;Parameterized Complexity Theory&lt;/a&gt;&amp;nbsp;by Flum &amp;amp; Grohe. Page 81 no Parens and no Brackets.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;a href=&quot;https://theory.cs.princeton.edu/complexity/book.pdf&quot;&gt;Computational Complexity: A Modern Approach&lt;/a&gt;&amp;nbsp;&lt;/i&gt;by Arora &amp;amp; Barak. Page 40. No Parens No Brackets.(2007)&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://theswissbay.ch/pdf/Gentoomen%20Library/Theory%20Of%20Computation/Oded_Goldreich-Computational_Complexity__A_Conceptual_Perspective%282008%29.pdf&quot;&gt;Computational Complexity: A Conceptual Prospective&lt;/a&gt;&amp;nbsp;by Goldreich.&amp;nbsp; Page 114 no parents, no brackets (2008)&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0890540109002338&quot;&gt;On Quantifer Rank Equivalence between linear orders by Siders&lt;/a&gt;.&amp;nbsp;&lt;/i&gt;On page 417 they use quantifiers to state a theorem, which is unusual. Parens no brackets.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1211.0020&quot;&gt;&lt;i&gt;Presburger arithmetic, Rational Generating Functions, and quasi polynomials&lt;/i&gt;&lt;/a&gt;&amp;nbsp;by Woods. Parens no&amp;nbsp; Brackets. (2012)&amp;nbsp;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://erikdemaine.org/papers/Witness_TCS/paper.pdf#page=69&quot;&gt;Who witness&#39;s the Witness by Abel et al.&lt;/a&gt;&amp;nbsp;On Page 69 (which the pointer takes you to) No Parens, no brackets. Colons between quantifiers (2018).&lt;/p&gt;&lt;p&gt;e) What to make of all this?&lt;/p&gt;&lt;p&gt;First off- the RARITY of the use of quantifiers really surprised me. The only place I saw them used a lot was my book, co-authored with Georgie Martin,&amp;nbsp;&amp;nbsp;&lt;i&gt;Bounded Queries in Recursion Theory. &lt;/i&gt;Perhaps it would have sold better if I didn&#39;t use so many quantifiers. Oh well.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Second off- Later works don&#39;t use parens and brackets. This is most clear if you just look at Complexity Theory Books&amp;nbsp;&lt;/p&gt;&lt;p&gt;Garey &amp;amp; Johnson - 1979- parens and brackets&lt;/p&gt;&lt;p&gt;Flun &amp;amp; Grohe- 1998- no parens and no brackts&lt;/p&gt;&lt;p&gt;Immerman- 1999 - parens but no brackets (this is the one exception)&amp;nbsp;&lt;/p&gt;&lt;p&gt;Arora &amp;amp; Barack- 2007 no parens and&amp;nbsp; no brackets&lt;/p&gt;&lt;p&gt;Goldreich-2008- no parens and no brackets&lt;/p&gt;&lt;p&gt;If you have a complexity theory book around that is not on this list, look up the definition of NP and the definition of the Poly Hierarchy and see (a) if they use parens around the quantifiers, and (b) if they use square brackets or no brackets of something else. Please leave a comment about it so I test the conjecture that parenthesis are just so 1979.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By gasarch&lt;/p&gt;
  </content>
    <author>
      <name>Computational Complexity</name>
      <uri>http://blog.computationalcomplexity.org/</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Complexity of Motion Planning of Arbitrarily Many Robots: Gadgets, Petri Nets, and Counter Machines</title>
    <link href="http://arxiv.org/abs/2306.01193"/>
    <id>http://arxiv.org/abs/2306.01193</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ani_J/0/1/0/all/0/1&quot;&gt;Joshua Ani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coulombe_M/0/1/0/all/0/1&quot;&gt;Michael Coulombe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demaine_E/0/1/0/all/0/1&quot;&gt;Erik D. Demaine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diomidov_Y/0/1/0/all/0/1&quot;&gt;Yevhenii Diomidov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_T/0/1/0/all/0/1&quot;&gt;Timothy Gomez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendrickson_D/0/1/0/all/0/1&quot;&gt;Dylan Hendrickson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lynch_J/0/1/0/all/0/1&quot;&gt;Jayson Lynch&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We extend the motion-planning-through-gadgets framework to several new
scenarios involving various numbers of robots/agents, and analyze the
complexity of the resulting motion-planning problems. While past work considers
just one robot or one robot per player, most of our models allow for one or
more locations to spawn new robots in each time step, leading to arbitrarily
many robots. In the 0-player context, where all motion is deterministically
forced, we prove that deciding whether any robot ever reaches a specified
location is undecidable, by representing a counter machine. In the 1-player
context, where the player can choose how to move the robots, we prove
equivalence to Petri nets, EXPSPACE-completeness for reaching a specified
location, PSPACE-completeness for reconfiguration, and ACKERMANN-completeness
for reconfiguration when robots can be destroyed in addition to spawned.
Finally, we consider a variation on the standard 2-player context where,
instead of one robot per player, we have one robot shared by the players, along
with a ko rule to prevent immediately undoing the previous move. We prove this
impartial 2-player game EXPTIME-complete.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Trade-offs between Entanglement and Communication</title>
    <link href="http://arxiv.org/abs/2306.01233"/>
    <id>http://arxiv.org/abs/2306.01233</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Arunachalam_S/0/1/0/all/0/1&quot;&gt;Srinivasan Arunachalam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Girish_U/0/1/0/all/0/1&quot;&gt;Uma Girish&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the advantages of quantum communication models over classical
communication models that are equipped with a limited number of qubits of
entanglement. In this direction, we give explicit partial functions on $n$ bits
for which reducing the entanglement increases the classical communication
complexity exponentially. Our separations are as follows. For every $k\ge 1$:
&lt;/p&gt;
&lt;p&gt;$Q\|^*$ versus $R2^*$: We show that quantum simultaneous protocols with
$\tilde{\Theta}(k^5 \log^3 n)$ qubits of entanglement can exponentially
outperform two-way randomized protocols with $O(k)$ qubits of entanglement.
This resolves an open problem from [Gav08] and improves the state-of-the-art
separations between quantum simultaneous protocols with entanglement and
two-way randomized protocols without entanglement [Gav19, GRT22].
&lt;/p&gt;
&lt;p&gt;$R\|^*$ versus $Q\|^*$: We show that classical simultaneous protocols with
$\tilde{\Theta}(k \log n)$ qubits of entanglement can exponentially outperform
quantum simultaneous protocols with $O(k)$ qubits of entanglement, resolving an
open question from [GKRW06, Gav19]. The best result prior to our work was a
relational separation against protocols without entanglement [GKRW06].
&lt;/p&gt;
&lt;p&gt;$R\|^*$ versus $R1^*$: We show that classical simultaneous protocols with
$\tilde{\Theta}(k\log n)$ qubits of entanglement can exponentially outperform
randomized one-way protocols with $O(k)$ qubits of entanglement. Prior to our
work, only a relational separation was known [Gav08].
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Discreteness of asymptotic tensor ranks</title>
    <link href="http://arxiv.org/abs/2306.01718"/>
    <id>http://arxiv.org/abs/2306.01718</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briet_J/0/1/0/all/0/1&quot;&gt;Jop Bri&amp;#xeb;t&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christandl_M/0/1/0/all/0/1&quot;&gt;Matthias Christandl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leigh_I/0/1/0/all/0/1&quot;&gt;Itai Leigh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shpilka_A/0/1/0/all/0/1&quot;&gt;Amir Shpilka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuiddam_J/0/1/0/all/0/1&quot;&gt;Jeroen Zuiddam&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Tensor parameters that are amortized or regularized over large tensor powers,
often called &quot;asymptotic&quot; tensor parameters, play a central role in several
areas including algebraic complexity theory (constructing fast matrix
multiplication algorithms), quantum information (entanglement cost and
distillable entanglement), and additive combinatorics (bounds on cap sets,
sunflower-free sets, etc.). Examples are the asymptotic tensor rank, asymptotic
slice rank and asymptotic subrank. Recent works (Costa-Dalai,
Blatter-Draisma-Rupniewski, Christandl-Gesmundo-Zuiddam) have investigated
notions of discreteness (no accumulation points) or &quot;gaps&quot; in the values of
such tensor parameters.
&lt;/p&gt;
&lt;p&gt;We prove a general discreteness theorem for asymptotic tensor parameters of
order-three tensors and use this to prove that (1) over any finite field, the
asymptotic subrank and the asymptotic slice rank have no accumulation points,
and (2) over the complex numbers, the asymptotic slice rank has no accumulation
points.
&lt;/p&gt;
&lt;p&gt;Central to our approach are two new general lower bounds on the asymptotic
subrank of tensors, which measures how much a tensor can be diagonalized. The
first lower bound says that the asymptotic subrank of any concise three-tensor
is at least the cube-root of the smallest dimension. The second lower bound
says that any three-tensor that is &quot;narrow enough&quot; (has one dimension much
smaller than the other two) has maximal asymptotic subrank.
&lt;/p&gt;
&lt;p&gt;Our proofs rely on new lower bounds on the maximum rank in matrix subspaces
that are obtained by slicing a three-tensor in the three different directions.
We prove that for any concise tensor the product of any two such maximum ranks
must be large, and as a consequence there are always two distinct directions
with large max-rank.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Complexity: Efficient Quantum State Synthesis with One Query</title>
    <link href="http://arxiv.org/abs/2306.01723"/>
    <id>http://arxiv.org/abs/2306.01723</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Rosenthal_G/0/1/0/all/0/1&quot;&gt;Gregory Rosenthal&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We present a polynomial-time quantum algorithm making a single query (in
superposition) to a classical oracle, such that for every state $|\psi\rangle$
there exists a choice of oracle that makes the algorithm construct an
exponentially close approximation of $|\psi\rangle$. Previous algorithms for
this problem either used a linear number of queries and polynomial time
[&lt;a href=&quot;/abs/1607.05256&quot;&gt;arXiv:1607.05256&lt;/a&gt;], or a constant number of queries and polynomially many
ancillae but no nontrivial bound on the runtime [&lt;a href=&quot;/abs/2111.02999&quot;&gt;arXiv:2111.02999&lt;/a&gt;]. As
corollaries we do the following:
&lt;/p&gt;
&lt;p&gt;- We simplify the proof that statePSPACE $\subseteq$ stateQIP
[&lt;a href=&quot;/abs/2108.07192&quot;&gt;arXiv:2108.07192&lt;/a&gt;] (a quantum state analogue of PSPACE $\subseteq$ IP) and show
that a constant number of rounds of interaction suffices.
&lt;/p&gt;
&lt;p&gt;- We show that QAC$\mathsf{_f^0}$ lower bounds for constructing explicit
states would imply breakthrough circuit lower bounds for computing explicit
boolean functions.
&lt;/p&gt;
&lt;p&gt;- We prove that every $n$-qubit state can be constructed to within 0.01 error
by an $O(2^n/n)$-size circuit over an appropriate finite gate set. More
generally we give a size-error tradeoff which, by a counting argument, is
optimal for any finite gate set.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Complexity</name>
      <uri>https://arxiv.org/list/cs.CC/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: Does it pay to optimize AUC?</title>
    <link href="http://arxiv.org/abs/2306.01528"/>
    <id>http://arxiv.org/abs/2306.01528</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Baojian Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skiena_S/0/1/0/all/0/1&quot;&gt;Steven Skiena&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The Area Under the ROC Curve (AUC) is an important model metric for
evaluating binary classifiers, and many algorithms have been proposed to
optimize AUC approximately. It raises the question of whether the generally
insignificant gains observed by previous studies are due to inherent
limitations of the metric or the inadequate quality of optimization.
&lt;/p&gt;
&lt;p&gt;To better understand the value of optimizing for AUC, we present an efficient
algorithm, namely AUC-opt, to find the provably optimal AUC linear classifier
in $\mathbb{R}^2$, which runs in $\mathcal{O}(n_+ n_- \log (n_+ n_-))$ where
$n_+$ and $n_-$ are the number of positive and negative samples respectively.
Furthermore, it can be naturally extended to $\mathbb{R}^d$ in
$\mathcal{O}((n_+n_-)^{d-1}\log (n_+n_-))$ by calling AUC-opt in
lower-dimensional spaces recursively. We prove the problem is NP-complete when
$d$ is not fixed, reducing from the \textit{open hemisphere problem}.
&lt;/p&gt;
&lt;p&gt;Experiments show that compared with other methods, AUC-opt achieves
statistically significant improvements on between 17 to 40 in $\mathbb{R}^2$
and between 4 to 42 in $\mathbb{R}^3$ of 50 t-SNE training datasets. However,
generally the gain proves insignificant on most testing datasets compared to
the best standard classifiers. Similar observations are found for nonlinear AUC
methods under real-world datasets.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Computational Geometry: No-dimensional Tverberg Partitions Revisited</title>
    <link href="http://arxiv.org/abs/2306.01678"/>
    <id>http://arxiv.org/abs/2306.01678</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Har_Peled_S/0/1/0/all/0/1&quot;&gt;Sariel Har-Peled&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robson_E/0/1/0/all/0/1&quot;&gt;Eliot W. Robson&lt;/a&gt;&lt;/p&gt;&lt;p&gt;$ \newcommand{\epsA}{\Mh{\delta}} \newcommand{\Re}{\mathbb{R}}
\newcommand{\reals}{\mathbb{R}} \newcommand{\SetX}{\mathsf{X}}
\newcommand{\diam}{\Delta} \newcommand{\Mh}[1]{#1} \newcommand{\query}{q}
\newcommand{\eps}{\varepsilon} \newcommand{\VorX}[1]{\mathcal{V} \pth{#1}}
\newcommand{\IntRange}[1]{[ #1 ]} \newcommand{\Space}{\overline{\mathsf{m}}}
\newcommand{\pth}[2][\!]{#1\left({#2}\right)}
\newcommand{\polylog}{\mathrm{polylog}} \newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z} \newcommand{\pt}{p} \newcommand{\distY}[2]{\left\|
{#1} - {#2} \right\|} \newcommand{\PP}{P} \newcommand{\ptq}{q}
\newcommand{\pts}{s}$ Given a set $\PP \subset \Re^d$ of $n$ points, with
diameter $\diam$, and a parameter $\epsA \in (0,1)$, it is known that there is
a partition of $\PP$ into sets $\PP_1, \ldots, \PP_t$, each of size
$O(1/\epsA^2)$, such that their convex-hulls all intersect a common ball of
radius $\epsA \diam$. We prove that a random partition, with a simple
alteration step, yields the desired partition, resulting in a linear time
algorithm. Previous proofs were either existential (i.e., at least exponential
time), or required much bigger sets. In addition, the algorithm and its proof
of correctness are significantly simpler than previous work, and the constants
are slightly better.
&lt;/p&gt;
&lt;p&gt;In addition, we provide a linear time algorithm for computing a ``fuzzy&#39;&#39;
centerpoint. We also prove a no-dimensional weak $\eps$-net theorem with an
improved constant.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Computational Geometry</name>
      <uri>https://arxiv.org/list/cs.CG/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: The Maximum Matrix Contraction Problem</title>
    <link href="http://arxiv.org/abs/2306.01349"/>
    <id>http://arxiv.org/abs/2306.01349</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watel_D/0/1/0/all/0/1&quot;&gt;Dimitri Watel&lt;/a&gt; (ENSIIE, CEDRIC - OC), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poirion_P/0/1/0/all/0/1&quot;&gt;Pierre-Louis Poirion&lt;/a&gt; (CEDRIC - OC)&lt;/p&gt;&lt;p&gt;In this paper, we introduce the Maximum Matrix Contraction problem, where we
aim to contract as much as possible a binary matrix in order to maximize its
density. We study the complexity and the polynomial approximability of the
problem. Especially, we prove this problem to be NP-Complete and that every
algorithm solving this problem is at most a $2\sqrt{n}$-approximation algorithm
where n is the number of ones in the matrix. We then focus on efficient
algorithms to solve the problem: an integer linear program and three
heuristics.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Improved Algorithms for Distance Selection and Related Problems</title>
    <link href="http://arxiv.org/abs/2306.01073"/>
    <id>http://arxiv.org/abs/2306.01073</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haitao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yiming Zhao&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this paper, we propose new techniques for solving geometric optimization
problems involving interpoint distances of a point set in the plane. Given a
set $P$ of $n$ points in the plane and an integer $1 \leq k \leq \binom{n}{2}$,
the distance selection problem is to find the $k$-th smallest interpoint
distance among all pairs of points of $P$. The previously best deterministic
algorithm solves the problem in $O(n^{4/3} \log^2 n)$ time [Katz and Sharir,
SIAM J. Comput. 1997 and SoCG 1993]. In this paper, we improve their algorithm
to $O(n^{4/3} \log n)$ time. Using similar techniques, we also give improved
algorithms on both the two-sided and the one-sided discrete Fr\&#39;{e}chet
distance with shortcuts problem for two point sets in the plane. For the
two-sided problem (resp., one-sided problem), we improve the previous work
[Avraham, Filtser, Kaplan, Katz, and Sharir, ACM Trans. Algorithms 2015 and
SoCG 2014] by a factor of roughly $\log^2(m+n)$ (resp., $(m+n)^{\epsilon}$),
where $m$ and $n$ are the sizes of the two input point sets, respectively.
Other problems whose solutions can be improved by our techniques include the
reverse shortest path problems for unit-disk graphs. Our techniques are quite
general and we believe they will find many other applications in future.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Labeled Interleaving Distance for Reeb Graphs</title>
    <link href="http://arxiv.org/abs/2306.01186"/>
    <id>http://arxiv.org/abs/2306.01186</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_F/0/1/0/all/0/1&quot;&gt;Fangfei Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parsa_S/0/1/0/all/0/1&quot;&gt;Salman Parsa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bei Wang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Merge trees, contour trees, and Reeb graphs are graph-based topological
descriptors that capture topological changes of (sub)level sets of scalar
fields. Comparing scalar fields using their topological descriptors has many
applications in topological data analysis and visualization of scientific data.
Recently, Munch and Stefanou introduced a labeled interleaving distance for
comparing two labeled merge trees, which enjoys a number of theoretical and
algorithmic properties. In particular, the labeled interleaving distance
between merge trees can be computed in polynomial time. In this work, we define
the labeled interleaving distance for labeled Reeb graphs. We then prove that
the (ordinary) interleaving distance between Reeb graphs equals the minimum of
the labeled interleaving distance over all labelings. We also provide an
efficient algorithm for computing the labeled interleaving distance between two
labeled contour trees (which are special types of Reeb graphs that arise from
simply-connected domains). In the case of merge trees, the notion of the
labeled interleaving distance was used by Gasparovic et al. to prove that the
(ordinary) interleaving distance on the set of (unlabeled) merge trees is
intrinsic. As our final contribution, we present counterexamples showing that,
on the contrary, the (ordinary) interleaving distance on (unlabeled) Reeb
graphs (and contour trees) is not intrinsic. It turns out that, under mild
conditions on the labelings, the labeled interleaving distance is a metric on
isomorphism classes of Reeb graphs, analogous to the ordinary interleaving
distance. This provides new metrics on large classes of Reeb graphs.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


  <entry xml:lang="en">
    <title type="html" xml:lang="en">arXiv: Data Structures and Algorithms: Fast Matrix Multiplication Without Tears: A Constraint Programming Approach</title>
    <link href="http://arxiv.org/abs/2306.01097"/>
    <id>http://arxiv.org/abs/2306.01097</id>
    <updated>2023-06-05T00:30:00+00:00</updated>
    <content type="html" xml:lang="en">
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deza_A/0/1/0/all/0/1&quot;&gt;Arnaud Deza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaezipoor_P/0/1/0/all/0/1&quot;&gt;Pashootan Vaezipoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalil_E/0/1/0/all/0/1&quot;&gt;Elias B. Khalil&lt;/a&gt;&lt;/p&gt;&lt;p&gt;It is known that the multiplication of an $N \times M$ matrix with an $M
\times P$ matrix can be performed using fewer multiplications than what the
naive $NMP$ approach suggests. The most famous instance of this is Strassen&#39;s
algorithm for multiplying two $2\times 2$ matrices in 7 instead of 8
multiplications. This gives rise to the constraint satisfaction problem of fast
matrix multiplication, where a set of $R &amp;lt; NMP$ multiplication terms must be
chosen and combined such that they satisfy correctness constraints on the
output matrix. Despite its highly combinatorial nature, this problem has not
been exhaustively examined from that perspective, as evidenced for example by
the recent deep reinforcement learning approach of AlphaTensor. In this work,
we propose a simple yet novel Constraint Programming approach to find
non-commutative algorithms for fast matrix multiplication or provide proof of
infeasibility otherwise. We propose a set of symmetry-breaking constraints and
valid inequalities that are particularly helpful in proving infeasibility. On
the feasible side, we find that exploiting solver performance variability in
conjunction with a sparsity-based problem decomposition enables finding
solutions for larger (feasible) instances of fast matrix multiplication. Our
experimental results using CP Optimizer demonstrate that we can find fast
matrix multiplication algorithms for matrices up to $3\times 3$ in a short
amount of time.
&lt;/p&gt;
  </content>
    <author>
      <name>arXiv: Data Structures and Algorithms</name>
      <uri>https://arxiv.org/list/cs.DS/recent</uri>
    </author>
  </entry>


</feed>
