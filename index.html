<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.5 (2022-11-24) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2022-12-02T12:51:45Z">Friday, December 02 2022, 12:51</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>Sébastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Friday, December 02
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.00083'>The Smoothed Complexity of Policy Iteration for Markov Decision Processes</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Miranda Christ, Mihalis Yannakakis</p><p>We show subexponential lower bounds (i.e., $2^{\Omega (n^c)}$) on the
smoothed complexity of the classical Howard's Policy Iteration algorithm for
Markov Decision Processes. The bounds hold for the total reward and the average
reward criteria. The constructions are robust in the sense that the
subexponential bound holds not only on the average for independent random
perturbations of the MDP parameters (transition probabilities and rewards), but
for all arbitrary perturbations within an inverse polynomial range. We show
also an exponential lower bound on the worst-case complexity for the simple
reachability objective.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Christ_M/0/1/0/all/0/1">Miranda Christ</a>, <a href="http://arxiv.org/find/cs/1/au:+Yannakakis_M/0/1/0/all/0/1">Mihalis Yannakakis</a></p><p>We show subexponential lower bounds (i.e., $2^{\Omega (n^c)}$) on the
smoothed complexity of the classical Howard's Policy Iteration algorithm for
Markov Decision Processes. The bounds hold for the total reward and the average
reward criteria. The constructions are robust in the sense that the
subexponential bound holds not only on the average for independent random
perturbations of the MDP parameters (transition probabilities and rewards), but
for all arbitrary perturbations within an inverse polynomial range. We show
also an exponential lower bound on the worst-case complexity for the simple
reachability objective.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-02T01:30:00Z">Friday, December 02 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.00098'>On the power of nonstandard quantum oracles</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Roozbeh Bassirian, Bill Fefferman, Kunal Marwaha</p><p>We study how the choices made when designing an oracle affect the complexity
of quantum property testing problems defined relative to this oracle. We encode
a regular graph of even degree as an invertible function $f$, and present $f$
in different oracle models. We first give a one-query QMA protocol to test if a
graph encoded in $f$ has a small disconnected subset. We then use
representation theory to show that no classical witness can help a quantum
verifier efficiently decide this problem relative to an in-place oracle.
Perhaps surprisingly, a simple modification to the standard oracle prevents a
quantum verifier from efficiently deciding this problem, even with access to an
unbounded witness.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Bassirian_R/0/1/0/all/0/1">Roozbeh Bassirian</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Fefferman_B/0/1/0/all/0/1">Bill Fefferman</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Marwaha_K/0/1/0/all/0/1">Kunal Marwaha</a></p><p>We study how the choices made when designing an oracle affect the complexity
of quantum property testing problems defined relative to this oracle. We encode
a regular graph of even degree as an invertible function $f$, and present $f$
in different oracle models. We first give a one-query QMA protocol to test if a
graph encoded in $f$ has a small disconnected subset. We then use
representation theory to show that no classical witness can help a quantum
verifier efficiently decide this problem relative to an in-place oracle.
Perhaps surprisingly, a simple modification to the standard oracle prevents a
quantum verifier from efficiently deciding this problem, even with access to an
unbounded witness.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-02T01:30:00Z">Friday, December 02 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.00297'>Hit-and-run mixing via localization schemes</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yuansi Chen, Ronen Eldan</p><p>We analyze the hit-and-run algorithm for sampling uniformly from an isotropic
convex body $K$ in $n$ dimensions. We show that the algorithm mixes in time
$\tilde{O}(n^2/ \psi_n^2)$, where $\psi_n$ is the smallest isoperimetric
constant for any isotropic logconcave distribution, also known as the
Kannan-Lovasz-Simonovits (KLS) constant. Our bound improves upon previous
bounds of the form $\tilde{O}(n^2 R^2/r^2)$, which depend on the ratio $R/r$ of
the radii of the circumscribed and inscribed balls of $K$, gaining a factor of
$n$ in the case of isotropic convex bodies. Consequently, our result gives a
mixing time estimate for the hit-and-run which matches the state-of-the-art
bounds for the ball walk. Our main proof technique is based on an annealing of
localization schemes introduced in Chen and Eldan (2022), which allows us to
reduce the problem to the analysis of the mixing time on truncated Gaussian
distributions.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Chen_Y/0/1/0/all/0/1">Yuansi Chen</a>, <a href="http://arxiv.org/find/math/1/au:+Eldan_R/0/1/0/all/0/1">Ronen Eldan</a></p><p>We analyze the hit-and-run algorithm for sampling uniformly from an isotropic
convex body $K$ in $n$ dimensions. We show that the algorithm mixes in time
$\tilde{O}(n^2/ \psi_n^2)$, where $\psi_n$ is the smallest isoperimetric
constant for any isotropic logconcave distribution, also known as the
Kannan-Lovasz-Simonovits (KLS) constant. Our bound improves upon previous
bounds of the form $\tilde{O}(n^2 R^2/r^2)$, which depend on the ratio $R/r$ of
the radii of the circumscribed and inscribed balls of $K$, gaining a factor of
$n$ in the case of isotropic convex bodies. Consequently, our result gives a
mixing time estimate for the hit-and-run which matches the state-of-the-art
bounds for the ball walk. Our main proof technique is based on an annealing of
localization schemes introduced in Chen and Eldan (2022), which allows us to
reduce the problem to the analysis of the mixing time on truncated Gaussian
distributions.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-02T01:30:00Z">Friday, December 02 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.00693'>Complexity Blowup for Solutions of the Laplace and the Diffusion Equation</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Aras Bacho, Holger Boche, Gitta Kutyniok</p><p>In this paper, we investigate the computational complexity of solutions to
the Laplace and the diffusion equation. We show that for a certain class of
initial-boundary value problems of the Laplace and the diffusion equation, the
solution operator is unbounded as a mapping from the space of polynomial-time
computable functions into itself in the sense that there exists polynomial-time
(Turing) computable input data such that the solution is not polynomial-time
computable, unless $FP=\#P$. In this case, we can, in general, not simulate the
solution of the Laplace or the diffusion equation on a digital computer without
having a complexity blowup, i.e., the computation time for obtaining an
approximation of the solution with up to a finite number of significant digits
grows exponentially in the number of digits. This shows that the computational
complexity of the solution operator is intrinsically high, independent of the
numerical algorithm that is used to obtain a solution. This indicates that
there is a fundamental problem in computing a solution on a digital hardware.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bacho_A/0/1/0/all/0/1">Aras Bacho</a>, <a href="http://arxiv.org/find/cs/1/au:+Boche_H/0/1/0/all/0/1">Holger Boche</a>, <a href="http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1">Gitta Kutyniok</a></p><p>In this paper, we investigate the computational complexity of solutions to
the Laplace and the diffusion equation. We show that for a certain class of
initial-boundary value problems of the Laplace and the diffusion equation, the
solution operator is unbounded as a mapping from the space of polynomial-time
computable functions into itself in the sense that there exists polynomial-time
(Turing) computable input data such that the solution is not polynomial-time
computable, unless $FP=\#P$. In this case, we can, in general, not simulate the
solution of the Laplace or the diffusion equation on a digital computer without
having a complexity blowup, i.e., the computation time for obtaining an
approximation of the solution with up to a finite number of significant digits
grows exponentially in the number of digits. This shows that the computational
complexity of the solution operator is intrinsically high, independent of the
numerical algorithm that is used to obtain a solution. This indicates that
there is a fundamental problem in computing a solution on a digital hardware.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-02T01:30:00Z">Friday, December 02 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.00222'>Experimental Observations of the Topology of Convolutional Neural Network Activations</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Emilie Purvine, Davis Brown, Brett Jefferson, Cliff Joslyn, Brenda Praggastis, Archit Rathore, Madelyn Shapiro, Bei Wang, Youjia Zhou</p><p>Topological data analysis (TDA) is a branch of computational mathematics,
bridging algebraic topology and data science, that provides compact,
noise-robust representations of complex structures. Deep neural networks (DNNs)
learn millions of parameters associated with a series of transformations
defined by the model architecture, resulting in high-dimensional,
difficult-to-interpret internal representations of input data. As DNNs become
more ubiquitous across multiple sectors of our society, there is increasing
recognition that mathematical methods are needed to aid analysts, researchers,
and practitioners in understanding and interpreting how these models' internal
representations relate to the final classification. In this paper, we apply
cutting edge techniques from TDA with the goal of gaining insight into the
interpretability of convolutional neural networks used for image
classification. We use two common TDA approaches to explore several methods for
modeling hidden-layer activations as high-dimensional point clouds, and provide
experimental evidence that these point clouds capture valuable structural
information about the model's process. First, we demonstrate that a distance
metric based on persistent homology can be used to quantify meaningful
differences between layers, and we discuss these distances in the broader
context of existing representational similarity metrics for neural network
interpretability. Second, we show that a mapper graph can provide semantic
insight into how these models organize hierarchical class knowledge at each
layer. These observations demonstrate that TDA is a useful tool to help deep
learning practitioners unlock the hidden structures of their models.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Purvine_E/0/1/0/all/0/1">Emilie Purvine</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1">Davis Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Jefferson_B/0/1/0/all/0/1">Brett Jefferson</a>, <a href="http://arxiv.org/find/cs/1/au:+Joslyn_C/0/1/0/all/0/1">Cliff Joslyn</a>, <a href="http://arxiv.org/find/cs/1/au:+Praggastis_B/0/1/0/all/0/1">Brenda Praggastis</a>, <a href="http://arxiv.org/find/cs/1/au:+Rathore_A/0/1/0/all/0/1">Archit Rathore</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapiro_M/0/1/0/all/0/1">Madelyn Shapiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Youjia Zhou</a></p><p>Topological data analysis (TDA) is a branch of computational mathematics,
bridging algebraic topology and data science, that provides compact,
noise-robust representations of complex structures. Deep neural networks (DNNs)
learn millions of parameters associated with a series of transformations
defined by the model architecture, resulting in high-dimensional,
difficult-to-interpret internal representations of input data. As DNNs become
more ubiquitous across multiple sectors of our society, there is increasing
recognition that mathematical methods are needed to aid analysts, researchers,
and practitioners in understanding and interpreting how these models' internal
representations relate to the final classification. In this paper, we apply
cutting edge techniques from TDA with the goal of gaining insight into the
interpretability of convolutional neural networks used for image
classification. We use two common TDA approaches to explore several methods for
modeling hidden-layer activations as high-dimensional point clouds, and provide
experimental evidence that these point clouds capture valuable structural
information about the model's process. First, we demonstrate that a distance
metric based on persistent homology can be used to quantify meaningful
differences between layers, and we discuss these distances in the broader
context of existing representational similarity metrics for neural network
interpretability. Second, we show that a mapper graph can provide semantic
insight into how these models organize hierarchical class knowledge at each
layer. These observations demonstrate that TDA is a useful tool to help deep
learning practitioners unlock the hidden structures of their models.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-02T01:30:00Z">Friday, December 02 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.00452'>Tutte Embeddings of Tetrahedral Meshes</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Marc Alexa</p><p>Tutte's embedding theorem states that every 3-connected graph without a $K_5$
or $K_{3,3}$ minor (i.e. a planar graph) is embedded in the plane if the outer
face is in convex position and the interior vertices are convex combinations of
their neighbors. We show that this result extends to simply connected
tetrahedral meshes in a natural way: for the tetrahedral mesh to be embedded if
the outer polyhedron is in convex position and the interior vertices are convex
combination of their neighbors it is sufficient (but not necessary) that the
graph of the tetrahedral mesh contains no $K_6$ and no $K_{3,3,1}$, and all
triangles incident on three boundary vertices are boundary triangles.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Alexa_M/0/1/0/all/0/1">Marc Alexa</a></p><p>Tutte's embedding theorem states that every 3-connected graph without a $K_5$
or $K_{3,3}$ minor (i.e. a planar graph) is embedded in the plane if the outer
face is in convex position and the interior vertices are convex combinations of
their neighbors. We show that this result extends to simply connected
tetrahedral meshes in a natural way: for the tetrahedral mesh to be embedded if
the outer polyhedron is in convex position and the interior vertices are convex
combination of their neighbors it is sufficient (but not necessary) that the
graph of the tetrahedral mesh contains no $K_6$ and no $K_{3,3,1}$, and all
triangles incident on three boundary vertices are boundary triangles.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-02T01:30:00Z">Friday, December 02 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.00094'>(No) Quantum space-time tradeoff for USTCON</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Simon Apers, Stacey Jeffery, Galina Pass, Michael Walter</p><p>Undirected $st$-connectivity is important both for its applications in
network problems, and for its theoretical connections with logspace complexity.
Classically, a long line of work led to a time-space tradeoff of
$T=\tilde{O}(n^2/S)$ for any $S$ such that $S=\Omega(\log (n))$ and
$S=O(n^2/m)$. Surprisingly, we show that quantumly there is no nontrivial
time-space tradeoff: there is a quantum algorithm that achieves both optimal
time $\tilde{O}(n)$ and space $O(\log (n))$ simultaneously. This improves on
previous results, which required either $O(\log (n))$ space and
$\tilde{O}(n^{1.5})$ time, or $\tilde{O}(n)$ space and time. To complement
this, we show that there is a nontrivial time-space tradeoff when given a lower
bound on the spectral gap of a corresponding random walk.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Apers_S/0/1/0/all/0/1">Simon Apers</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Jeffery_S/0/1/0/all/0/1">Stacey Jeffery</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Pass_G/0/1/0/all/0/1">Galina Pass</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Walter_M/0/1/0/all/0/1">Michael Walter</a></p><p>Undirected $st$-connectivity is important both for its applications in
network problems, and for its theoretical connections with logspace complexity.
Classically, a long line of work led to a time-space tradeoff of
$T=\tilde{O}(n^2/S)$ for any $S$ such that $S=\Omega(\log (n))$ and
$S=O(n^2/m)$. Surprisingly, we show that quantumly there is no nontrivial
time-space tradeoff: there is a quantum algorithm that achieves both optimal
time $\tilde{O}(n)$ and space $O(\log (n))$ simultaneously. This improves on
previous results, which required either $O(\log (n))$ space and
$\tilde{O}(n^{1.5})$ time, or $\tilde{O}(n)$ space and time. To complement
this, we show that there is a nontrivial time-space tradeoff when given a lower
bound on the spectral gap of a corresponding random walk.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-02T01:30:00Z">Friday, December 02 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.00189'>Sublinear Algorithms for $(1.5+\epsilon)$-Approximate Matching</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sayan Bhattacharya, Peter Kiss, Thatchaphol Saranurak</p><p>We study sublinear time algorithms for estimating the size of maximum
matching. After a long line of research, the problem was finally settled by
Behnezhad [FOCS'22], in the regime where one is willing to pay an approximation
factor of $2$. Very recently, Behnezhad et al.[SODA'23] improved the
approximation factor to $(2-\frac{1}{2^{O(1/\gamma)}})$ using $n^{1+\gamma}$
time. This improvement over the factor $2$ is, however, minuscule and they
asked if even $1.99$-approximation is possible in $n^{2-\Omega(1)}$ time. We
give a strong affirmative answer to this open problem by showing
$(1.5+\epsilon)$-approximation algorithms that run in
$n^{2-\Theta(\epsilon^{2})}$ time. Our approach is conceptually simple and
diverges from all previous sublinear-time matching algorithms: we show a
sublinear time algorithm for computing a variant of the edge-degree constrained
subgraph (EDCS), a concept that has previously been exploited in dynamic
[Bernstein Stein ICALP'15, SODA'16], distributed [Assadi et al. SODA'19] and
streaming [Bernstein ICALP'20] settings, but never before in the sublinear
setting. Independent work: Behnezhad, Roghani and Rubinstein [BRR'23]
independently showed sublinear algorithms similar to our Theorem 1.2 in both
adjacency list and matrix models. Furthermore, in [BRR'23], they show
additional results on strictly better-than-1.5 approximate matching algorithms
in both upper and lower bound sides.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1">Sayan Bhattacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Kiss_P/0/1/0/all/0/1">Peter Kiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Saranurak_T/0/1/0/all/0/1">Thatchaphol Saranurak</a></p><p>We study sublinear time algorithms for estimating the size of maximum
matching. After a long line of research, the problem was finally settled by
Behnezhad [FOCS'22], in the regime where one is willing to pay an approximation
factor of $2$. Very recently, Behnezhad et al.[SODA'23] improved the
approximation factor to $(2-\frac{1}{2^{O(1/\gamma)}})$ using $n^{1+\gamma}$
time. This improvement over the factor $2$ is, however, minuscule and they
asked if even $1.99$-approximation is possible in $n^{2-\Omega(1)}$ time. We
give a strong affirmative answer to this open problem by showing
$(1.5+\epsilon)$-approximation algorithms that run in
$n^{2-\Theta(\epsilon^{2})}$ time. Our approach is conceptually simple and
diverges from all previous sublinear-time matching algorithms: we show a
sublinear time algorithm for computing a variant of the edge-degree constrained
subgraph (EDCS), a concept that has previously been exploited in dynamic
[Bernstein Stein ICALP'15, SODA'16], distributed [Assadi et al. SODA'19] and
streaming [Bernstein ICALP'20] settings, but never before in the sublinear
setting. Independent work: Behnezhad, Roghani and Rubinstein [BRR'23]
independently showed sublinear algorithms similar to our Theorem 1.2 in both
adjacency list and matrix models. Furthermore, in [BRR'23], they show
additional results on strictly better-than-1.5 approximate matching algorithms
in both upper and lower bound sides.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-02T01:30:00Z">Friday, December 02 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.00333'>AC-Band: A Combinatorial Bandit-Based Approach to Algorithm Configuration</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jasmin Brandt, Elias Schede, Viktor Bengs, Bj&#xf6;rn Haddenhorst, Eyke H&#xfc;llermeier, Kevin Tierney</p><p>We study the algorithm configuration (AC) problem, in which one seeks to find
an optimal parameter configuration of a given target algorithm in an automated
way. Recently, there has been significant progress in designing AC approaches
that satisfy strong theoretical guarantees. However, a significant gap still
remains between the practical performance of these approaches and
state-of-the-art heuristic methods. To this end, we introduce AC-Band, a
general approach for the AC problem based on multi-armed bandits that provides
theoretical guarantees while exhibiting strong practical performance. We show
that AC-Band requires significantly less computation time than other AC
approaches providing theoretical guarantees while still yielding high-quality
configurations.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Brandt_J/0/1/0/all/0/1">Jasmin Brandt</a>, <a href="http://arxiv.org/find/cs/1/au:+Schede_E/0/1/0/all/0/1">Elias Schede</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengs_V/0/1/0/all/0/1">Viktor Bengs</a>, <a href="http://arxiv.org/find/cs/1/au:+Haddenhorst_B/0/1/0/all/0/1">Bj&#xf6;rn Haddenhorst</a>, <a href="http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1">Eyke H&#xfc;llermeier</a>, <a href="http://arxiv.org/find/cs/1/au:+Tierney_K/0/1/0/all/0/1">Kevin Tierney</a></p><p>We study the algorithm configuration (AC) problem, in which one seeks to find
an optimal parameter configuration of a given target algorithm in an automated
way. Recently, there has been significant progress in designing AC approaches
that satisfy strong theoretical guarantees. However, a significant gap still
remains between the practical performance of these approaches and
state-of-the-art heuristic methods. To this end, we introduce AC-Band, a
general approach for the AC problem based on multi-armed bandits that provides
theoretical guarantees while exhibiting strong practical performance. We show
that AC-Band requires significantly less computation time than other AC
approaches providing theoretical guarantees while still yielding high-quality
configurations.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-02T01:30:00Z">Friday, December 02 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.00359'>Tight Conditional Lower Bounds for Vertex Connectivity Problems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Zhiyi Huang, Yaowei Long, Thatchaphol Saranurak, Benyu Wang</p><p>We study the fine-grained complexity of graph connectivity problems in
unweighted undirected graphs. Recent development shows that all variants of
edge connectivity problems, including single-source-single-sink, global,
Steiner, single-source, and all-pairs connectivity, are solvable in
$m^{1+o(1)}$ time, collapsing the complexity of these problems into the
almost-linear-time regime. While, historically, vertex connectivity has been
much harder, the recent results showed that both single-source-single-sink and
global vertex connectivity can be solved in $m^{1+o(1)}$ time, raising the hope
of putting all variants of vertex connectivity problems into the
almost-linear-time regime too.
</p>
<p>We show that this hope is impossible, assuming conjectures on finding
4-cliques. Moreover, we essentially settle the complexity landscape by giving
tight bounds for combinatorial algorithms in dense graphs. There are three
separate regimes: (1) all-pairs and Steiner vertex connectivity have complexity
$\hat{\Theta}(n^{4})$, (2) single-source vertex connectivity has complexity
$\hat{\Theta}(n^{3})$, and (3) single-source-single-sink and global vertex
connectivity have complexity $\hat{\Theta}(n^{2})$. For graphs with general
density, we obtain tight bounds of $\hat{\Theta}(m^{2})$,
$\hat{\Theta}(m^{1.5})$, $\hat{\Theta}(m)$, respectively, assuming Gomory-Hu
trees for element connectivity can be computed in almost-linear time.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhiyi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1">Yaowei Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Saranurak_T/0/1/0/all/0/1">Thatchaphol Saranurak</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Benyu Wang</a></p><p>We study the fine-grained complexity of graph connectivity problems in
unweighted undirected graphs. Recent development shows that all variants of
edge connectivity problems, including single-source-single-sink, global,
Steiner, single-source, and all-pairs connectivity, are solvable in
$m^{1+o(1)}$ time, collapsing the complexity of these problems into the
almost-linear-time regime. While, historically, vertex connectivity has been
much harder, the recent results showed that both single-source-single-sink and
global vertex connectivity can be solved in $m^{1+o(1)}$ time, raising the hope
of putting all variants of vertex connectivity problems into the
almost-linear-time regime too.
</p>
<p>We show that this hope is impossible, assuming conjectures on finding
4-cliques. Moreover, we essentially settle the complexity landscape by giving
tight bounds for combinatorial algorithms in dense graphs. There are three
separate regimes: (1) all-pairs and Steiner vertex connectivity have complexity
$\hat{\Theta}(n^{4})$, (2) single-source vertex connectivity has complexity
$\hat{\Theta}(n^{3})$, and (3) single-source-single-sink and global vertex
connectivity have complexity $\hat{\Theta}(n^{2})$. For graphs with general
density, we obtain tight bounds of $\hat{\Theta}(m^{2})$,
$\hat{\Theta}(m^{1.5})$, $\hat{\Theta}(m)$, respectively, assuming Gomory-Hu
trees for element connectivity can be computed in almost-linear time.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-02T01:30:00Z">Friday, December 02 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.00418'>An Improved Time-Efficient Approximate Kernelization for Connected Treedepth Deletion Set</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Eduard Eiben, Diptapriyo Majumdar, M. S. Ramanujan</p><p>We study the CONNECTED \eta-TREEDEPTH DELETION problem where the input
instance is an undireted graph G = (V, E) and an integer k. The objective is to
decide if G has a set S \subseteq V(G) of at most k vertices such that G - S
has treedepth at most \eta and G[S] is connected. As this problem naturally
generalizes the well-known CONNECTED VERTEX COVER, when parameterized by
solution size k, the CONNECTED \eta-TREEDEPTH DELETION does not admit
polynomial kernel unless NP \subseteq coNP/poly. This motivates us to design an
approximate kernel of polynomial size for this problem. In this paper, we show
that for every 0 &lt; \epsilon &lt;= 1, CONNECTED \eta-TREEDEPTH DELETION SET admits
a (1+\epsilon)-approximate kernel with O(k^{2^{\eta + 1/\epsilon}}) vertices,
i.e. a polynomial-sized approximate kernelization scheme (PSAKS).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Eiben_E/0/1/0/all/0/1">Eduard Eiben</a>, <a href="http://arxiv.org/find/cs/1/au:+Majumdar_D/0/1/0/all/0/1">Diptapriyo Majumdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramanujan_M/0/1/0/all/0/1">M. S. Ramanujan</a></p><p>We study the CONNECTED \eta-TREEDEPTH DELETION problem where the input
instance is an undireted graph G = (V, E) and an integer k. The objective is to
decide if G has a set S \subseteq V(G) of at most k vertices such that G - S
has treedepth at most \eta and G[S] is connected. As this problem naturally
generalizes the well-known CONNECTED VERTEX COVER, when parameterized by
solution size k, the CONNECTED \eta-TREEDEPTH DELETION does not admit
polynomial kernel unless NP \subseteq coNP/poly. This motivates us to design an
approximate kernel of polynomial size for this problem. In this paper, we show
that for every 0 &lt; \epsilon &lt;= 1, CONNECTED \eta-TREEDEPTH DELETION SET admits
a (1+\epsilon)-approximate kernel with O(k^{2^{\eta + 1/\epsilon}}) vertices,
i.e. a polynomial-sized approximate kernelization scheme (PSAKS).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-02T01:30:00Z">Friday, December 02 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.00508'>Subquadratic Weighted Matroid Intersection Under Rank Oracles</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ta-Wei Tu</p><p>Given two matroids $\mathcal{M}_1 = (V, \mathcal{I}_1)$ and $\mathcal{M}_2 =
(V, \mathcal{I}_2)$ over an $n$-element integer-weighted ground set $V$, the
weighted matroid intersection problem aims to find a common independent set
$S^{*} \in \mathcal{I}_1 \cap \mathcal{I}_2$ maximizing the weight of $S^{*}$.
In this paper, we present a simple deterministic algorithm for weighted matroid
intersection using $\tilde{O}(nr^{3/4}\log{W})$ rank queries, where $r$ is the
size of the largest intersection of $\mathcal{M}_1$ and $\mathcal{M}_2$ and $W$
is the maximum weight. This improves upon the best previously known
$\tilde{O}(nr\log{W})$ algorithm given by Lee, Sidford, and Wong [FOCS'15], and
is the first subquadratic algorithm for polynomially-bounded weights under the
standard independence or rank oracle models. The main contribution of this
paper is an efficient algorithm that computes shortest-path trees in weighted
exchange graphs.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1">Ta-Wei Tu</a></p><p>Given two matroids $\mathcal{M}_1 = (V, \mathcal{I}_1)$ and $\mathcal{M}_2 =
(V, \mathcal{I}_2)$ over an $n$-element integer-weighted ground set $V$, the
weighted matroid intersection problem aims to find a common independent set
$S^{*} \in \mathcal{I}_1 \cap \mathcal{I}_2$ maximizing the weight of $S^{*}$.
In this paper, we present a simple deterministic algorithm for weighted matroid
intersection using $\tilde{O}(nr^{3/4}\log{W})$ rank queries, where $r$ is the
size of the largest intersection of $\mathcal{M}_1$ and $\mathcal{M}_2$ and $W$
is the maximum weight. This improves upon the best previously known
$\tilde{O}(nr\log{W})$ algorithm given by Lee, Sidford, and Wong [FOCS'15], and
is the first subquadratic algorithm for polynomially-bounded weights under the
standard independence or rank oracle models. The main contribution of this
paper is an efficient algorithm that computes shortest-path trees in weighted
exchange graphs.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-02T01:30:00Z">Friday, December 02 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.00521'>POSTER: Unexpected Scaling in Path Copying Trees</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ilya Kokorin, Alexander Fedorov, Trevor Brown, Vitaly Aksenov</p><p>Although a wide variety of handcrafted concurrent data structures have been
proposed, there is considerable interest in universal approaches (henceforth
called Universal Constructions or UCs) for building concurrent data structures.
These approaches (semi-)automatically convert a sequential data structure into
a concurrent one. The simplest approach uses locks that protect a sequential
data structure and allow only one process to access it at a time. The resulting
data structures use locks, and hence are blocking. Most work on UCs instead
focuses on obtaining non-blocking progress guarantees such as
obstruction-freedom, lock-freedom, or wait-freedom. Many non-blocking UCs have
appeared. Key examples include the seminal wait-free UC by Herlihy, a
NUMA-aware UC by Yi et al., and an efficient UC for large objects by Fatourou
et al.
</p>
<p>We borrow ideas from persistent data structures and multi-version concurrency
control (MVCC), most notably path copying, and use them to implement concurrent
versions of sequential persistent data structures. Despite our expectation that
our data structures would not scale under write-heavy workloads, they scale in
practice. We confirm this scaling analytically in our model with private
per-process caches.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kokorin_I/0/1/0/all/0/1">Ilya Kokorin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fedorov_A/0/1/0/all/0/1">Alexander Fedorov</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1">Trevor Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Aksenov_V/0/1/0/all/0/1">Vitaly Aksenov</a></p><p>Although a wide variety of handcrafted concurrent data structures have been
proposed, there is considerable interest in universal approaches (henceforth
called Universal Constructions or UCs) for building concurrent data structures.
These approaches (semi-)automatically convert a sequential data structure into
a concurrent one. The simplest approach uses locks that protect a sequential
data structure and allow only one process to access it at a time. The resulting
data structures use locks, and hence are blocking. Most work on UCs instead
focuses on obtaining non-blocking progress guarantees such as
obstruction-freedom, lock-freedom, or wait-freedom. Many non-blocking UCs have
appeared. Key examples include the seminal wait-free UC by Herlihy, a
NUMA-aware UC by Yi et al., and an efficient UC for large objects by Fatourou
et al.
</p>
<p>We borrow ideas from persistent data structures and multi-version concurrency
control (MVCC), most notably path copying, and use them to implement concurrent
versions of sequential persistent data structures. Despite our expectation that
our data structures would not scale under write-heavy workloads, they scale in
practice. We confirm this scaling analytically in our model with private
per-process caches.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-02T01:30:00Z">Friday, December 02 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.00642'>Sub-quadratic Algorithms for Kernel Matrices via Kernel Density Estimation</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ainesh Bakshi, Piotr Indyk, Praneeth Kacham, Sandeep Silwal, Samson Zhou</p><p>Kernel matrices, as well as weighted graphs represented by them, are
ubiquitous objects in machine learning, statistics and other related fields.
The main drawback of using kernel methods (learning and inference using kernel
matrices) is efficiency -- given $n$ input points, most kernel-based algorithms
need to materialize the full $n \times n$ kernel matrix before performing any
subsequent computation, thus incurring $\Omega(n^2)$ runtime. Breaking this
quadratic barrier for various problems has therefore, been a subject of
extensive research efforts.
</p>
<p>We break the quadratic barrier and obtain $\textit{subquadratic}$ time
algorithms for several fundamental linear-algebraic and graph processing
primitives, including approximating the top eigenvalue and eigenvector,
spectral sparsification, solving linear systems, local clustering, low-rank
approximation, arboricity estimation and counting weighted triangles. We build
on the recent Kernel Density Estimation framework, which (after preprocessing
in time subquadratic in $n$) can return estimates of row/column sums of the
kernel matrix. In particular, we develop efficient reductions from
$\textit{weighted vertex}$ and $\textit{weighted edge sampling}$ on kernel
graphs, $\textit{simulating random walks}$ on kernel graphs, and
$\textit{importance sampling}$ on matrices to Kernel Density Estimation and
show that we can generate samples from these distributions in
$\textit{sublinear}$ (in the support of the distribution) time. Our reductions
are the central ingredient in each of our applications and we believe they may
be of independent interest. We empirically demonstrate the efficacy of our
algorithms on low-rank approximation (LRA) and spectral sparsification, where
we observe a $\textbf{9x}$ decrease in the number of kernel evaluations over
baselines for LRA and a $\textbf{41x}$ reduction in the graph size for spectral
sparsification.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bakshi_A/0/1/0/all/0/1">Ainesh Bakshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Indyk_P/0/1/0/all/0/1">Piotr Indyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Kacham_P/0/1/0/all/0/1">Praneeth Kacham</a>, <a href="http://arxiv.org/find/cs/1/au:+Silwal_S/0/1/0/all/0/1">Sandeep Silwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Samson Zhou</a></p><p>Kernel matrices, as well as weighted graphs represented by them, are
ubiquitous objects in machine learning, statistics and other related fields.
The main drawback of using kernel methods (learning and inference using kernel
matrices) is efficiency -- given $n$ input points, most kernel-based algorithms
need to materialize the full $n \times n$ kernel matrix before performing any
subsequent computation, thus incurring $\Omega(n^2)$ runtime. Breaking this
quadratic barrier for various problems has therefore, been a subject of
extensive research efforts.
</p>
<p>We break the quadratic barrier and obtain $\textit{subquadratic}$ time
algorithms for several fundamental linear-algebraic and graph processing
primitives, including approximating the top eigenvalue and eigenvector,
spectral sparsification, solving linear systems, local clustering, low-rank
approximation, arboricity estimation and counting weighted triangles. We build
on the recent Kernel Density Estimation framework, which (after preprocessing
in time subquadratic in $n$) can return estimates of row/column sums of the
kernel matrix. In particular, we develop efficient reductions from
$\textit{weighted vertex}$ and $\textit{weighted edge sampling}$ on kernel
graphs, $\textit{simulating random walks}$ on kernel graphs, and
$\textit{importance sampling}$ on matrices to Kernel Density Estimation and
show that we can generate samples from these distributions in
$\textit{sublinear}$ (in the support of the distribution) time. Our reductions
are the central ingredient in each of our applications and we believe they may
be of independent interest. We empirically demonstrate the efficacy of our
algorithms on low-rank approximation (LRA) and spectral sparsification, where
we observe a $\textbf{9x}$ decrease in the number of kernel evaluations over
baselines for LRA and a $\textbf{41x}$ reduction in the graph size for spectral
sparsification.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-02T01:30:00Z">Friday, December 02 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.00696'>Clustering What Matters: Optimal Approximation for Clustering with Outliers</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Akanksha Agrawal, Tanmay Inamdar, Saket Saurabh, Jie Xue</p><p>Clustering with outliers is one of the most fundamental problems in Computer
Science. Given a set $X$ of $n$ points and two integers $k$ and $m$, the
clustering with outliers aims to exclude $m$ points from $X$ and partition the
remaining points into $k$ clusters that minimizes a certain cost function. In
this paper, we give a general approach for solving clustering with outliers,
which results in a fixed-parameter tractable (FPT) algorithm in $k$ and $m$,
that almost matches the approximation ratio for its outlier-free counterpart.
As a corollary, we obtain FPT approximation algorithms with optimal
approximation ratios for $k$-Median and $k$-Means with outliers in general
metrics. We also exhibit more applications of our approach to other variants of
the problem that impose additional constraints on the clustering, such as
fairness or matroid constraints.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1">Akanksha Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Inamdar_T/0/1/0/all/0/1">Tanmay Inamdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Saurabh_S/0/1/0/all/0/1">Saket Saurabh</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1">Jie Xue</a></p><p>Clustering with outliers is one of the most fundamental problems in Computer
Science. Given a set $X$ of $n$ points and two integers $k$ and $m$, the
clustering with outliers aims to exclude $m$ points from $X$ and partition the
remaining points into $k$ clusters that minimizes a certain cost function. In
this paper, we give a general approach for solving clustering with outliers,
which results in a fixed-parameter tractable (FPT) algorithm in $k$ and $m$,
that almost matches the approximation ratio for its outlier-free counterpart.
As a corollary, we obtain FPT approximation algorithms with optimal
approximation ratios for $k$-Median and $k$-Means with outliers in general
metrics. We also exhibit more applications of our approach to other variants of
the problem that impose additional constraints on the clustering, such as
fairness or matroid constraints.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-02T01:30:00Z">Friday, December 02 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.00778'>Fully-Dynamic Decision Trees</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Marco Bressan, Gabriel Damay, Mauro Sozio</p><p>We develop the first fully dynamic algorithm that maintains a decision tree
over an arbitrary sequence of insertions and deletions of labeled examples.
Given $\epsilon &gt; 0$ our algorithm guarantees that, at every point in time,
every node of the decision tree uses a split with Gini gain within an additive
$\epsilon$ of the optimum. For real-valued features the algorithm has an
amortized running time per insertion/deletion of $O\big(\frac{d \log^3
n}{\epsilon^2}\big)$, which improves to $O\big(\frac{d \log^2
n}{\epsilon}\big)$ for binary or categorical features, while it uses space $O(n
d)$, where $n$ is the maximum number of examples at any point in time and $d$
is the number of features. Our algorithm is nearly optimal, as we show that any
algorithm with similar guarantees uses amortized running time $\Omega(d)$ and
space $\tilde{\Omega} (n d)$. We complement our theoretical results with an
extensive experimental evaluation on real-world data, showing the effectiveness
of our algorithm.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bressan_M/0/1/0/all/0/1">Marco Bressan</a>, <a href="http://arxiv.org/find/cs/1/au:+Damay_G/0/1/0/all/0/1">Gabriel Damay</a>, <a href="http://arxiv.org/find/cs/1/au:+Sozio_M/0/1/0/all/0/1">Mauro Sozio</a></p><p>We develop the first fully dynamic algorithm that maintains a decision tree
over an arbitrary sequence of insertions and deletions of labeled examples.
Given $\epsilon &gt; 0$ our algorithm guarantees that, at every point in time,
every node of the decision tree uses a split with Gini gain within an additive
$\epsilon$ of the optimum. For real-valued features the algorithm has an
amortized running time per insertion/deletion of $O\big(\frac{d \log^3
n}{\epsilon^2}\big)$, which improves to $O\big(\frac{d \log^2
n}{\epsilon}\big)$ for binary or categorical features, while it uses space $O(n
d)$, where $n$ is the maximum number of examples at any point in time and $d$
is the number of features. Our algorithm is nearly optimal, as we show that any
algorithm with similar guarantees uses amortized running time $\Omega(d)$ and
space $\tilde{\Omega} (n d)$. We complement our theoretical results with an
extensive experimental evaluation on real-world data, showing the effectiveness
of our algorithm.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-02T01:30:00Z">Friday, December 02 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Thursday, December 01
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://blog.computationalcomplexity.org/2022/12/how-do-we-keep-community-connected.html'>How do we keep the community connected?</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>A colleague said how they enjoyed watching the collapse of Twitter under Elon Musk. But I use Twitter to keep connected to the CS community. In Twitter I hear not only new results but ones that excite particular people. I watch the debate between those who see ML as revolutionary and those who see ML as revolting. I see the issues that our community worries about and those that they celebrate. I follow people as they progress in their careers or outright change them. Mostly it just makes me feel part of an academic community that goes beyond my own institution.&nbsp;</p><p>A bit surprisingly, so far Twitter hasn't collapsed. But it could and I expect many of my followers and those I follow spend less time there. I set up a Mastodon account&nbsp;@fortnow@fediscience.org&nbsp;but not much happens over there, though feel free to tell me who I should be following. There are many other social networks but none that bring us as a field together.</p><p>Blog posts and their comments play a role but not like they used to. There's&nbsp;CS Theory StackExchange&nbsp;which has some good (and not so good) technical discussions but we don't really have conversations there.&nbsp;</p><p>How about conferences now that researchers are (mostly) willing to attend in person? Since conferences in CS are the primary publication venue, we have too many meetings and many won't go, or at least won't go in person, if they don't have a paper in the conference.</p><p>So, once again, I suggest a big theory conference we hold every four years that everyone who's anyone will make sure to attend. Hey, it works for the World Cup.&nbsp;</p><p>By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>A colleague said how they enjoyed watching the <a href="https://blog.computationalcomplexity.org/2022/11/should-you-quit-twitter-and-texas.html">collapse of Twitter</a> under Elon Musk. But I use Twitter to keep connected to the CS community. In Twitter I hear not only new results but ones that excite particular people. I watch the debate between those who see ML as revolutionary and those who see ML as revolting. I see the issues that our community worries about and those that they celebrate. I follow people as they progress in their careers or outright change them. Mostly it just makes me feel part of an academic community that goes beyond my own institution.&nbsp;</p><p>A bit surprisingly, so far Twitter hasn't collapsed. But it could and I expect many of my followers and those I follow spend less time there. I set up a Mastodon account&nbsp;<a href="https://fediscience.org/@fortnow">@fortnow@fediscience.org</a>&nbsp;but not much happens over there, though feel free to tell me who I should be following. There are many other social networks but none that bring us as a field together.</p><p>Blog posts and their comments play a role but not like they used to. There's&nbsp;<a href="https://cstheory.stackexchange.com/">CS Theory StackExchange</a>&nbsp;which has some good (and not so good) technical discussions but we don't really have conversations there.&nbsp;</p><p>How about conferences now that researchers are (mostly) willing to attend in person? Since conferences in CS are the primary publication venue, we have too many meetings and many won't go, or at least won't go in person, if they don't have a paper in the conference.</p><p>So, <a href="https://blog.computationalcomplexity.org/2008/07/games.html">once again</a>, I suggest a big theory conference we hold every four years that everyone who's anyone will make sure to attend. Hey, it works for the World Cup.&nbsp;</p><p class="authors">By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-01T16:37:00Z">Thursday, December 01 2022, 16:37</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.16518'>Optimizing sparse fermionic Hamiltonians</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yaroslav Herasymenko, Maarten Stroeks, Jonas Helsen, Barbara Terhal</p><p>We consider the problem of approximating the ground state energy of a
fermionic Hamiltonian using a Gaussian state. In sharp contrast to the dense
case (Hastings &amp; O'Donnell, 2022), we prove that strictly $q$-local
$\rm{\textit{sparse}}$ fermionic Hamiltonians have a constant Gaussian
approximation ratio; the result holds for any connectivity and interaction
strengths. Sparsity means that each fermion participates in a bounded number of
interactions, and strictly $q$-local means that each term involves exactly $q$
fermionic (Majorana) operators. We extend our proof to give a constant Gaussian
approximation ratio for sparse fermionic Hamiltonians with both quartic and
quadratic terms. With additional work, we also prove a constant Gaussian
approximation ratio for the so-called sparse SYK model with strictly $4$-local
interactions (sparse SYK-4 model). In each setting we show that the Gaussian
state can be efficiently determined. Finally, we prove that the $O(n^{-1/2})$
Gaussian approximation ratio for the normal (dense) SYK-$4$ model extends to
SYK-$q$ for even $q&gt;4$, with an approximation ratio of $O(n^{1/2 - q/4})$. Our
results identify non-sparseness as the prime reason that the SYK-4 model can
fail to have a constant approximation ratio.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Herasymenko_Y/0/1/0/all/0/1">Yaroslav Herasymenko</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Stroeks_M/0/1/0/all/0/1">Maarten Stroeks</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Helsen_J/0/1/0/all/0/1">Jonas Helsen</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Terhal_B/0/1/0/all/0/1">Barbara Terhal</a></p><p>We consider the problem of approximating the ground state energy of a
fermionic Hamiltonian using a Gaussian state. In sharp contrast to the dense
case (Hastings &amp; O'Donnell, 2022), we prove that strictly $q$-local
$\rm{\textit{sparse}}$ fermionic Hamiltonians have a constant Gaussian
approximation ratio; the result holds for any connectivity and interaction
strengths. Sparsity means that each fermion participates in a bounded number of
interactions, and strictly $q$-local means that each term involves exactly $q$
fermionic (Majorana) operators. We extend our proof to give a constant Gaussian
approximation ratio for sparse fermionic Hamiltonians with both quartic and
quadratic terms. With additional work, we also prove a constant Gaussian
approximation ratio for the so-called sparse SYK model with strictly $4$-local
interactions (sparse SYK-4 model). In each setting we show that the Gaussian
state can be efficiently determined. Finally, we prove that the $O(n^{-1/2})$
Gaussian approximation ratio for the normal (dense) SYK-$4$ model extends to
SYK-$q$ for even $q&gt;4$, with an approximation ratio of $O(n^{1/2 - q/4})$. Our
results identify non-sparseness as the prime reason that the SYK-4 model can
fail to have a constant approximation ratio.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-01T01:30:00Z">Thursday, December 01 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.16993'>Post-Quantum $\kappa$-to-1 Trapdoor Claw-free Functions from Extrapolated Dihedral Cosets</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xingyu Yan (1), Licheng Wang (2), Weiqiang Wen (3), Ziyi Li (4), Jingwen Suo (1), Lize Gu (1) ((1) State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, 100876, China. (2) School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, 100081, China. (3) LTCI, Telecom Paris, Institut Polytechnique de Paris, Paris, France. (4) State Key Laboratory of Information Security, Institute of Information Engineering, University of Chinese Academy of Sciences, Beijing, 100049, China.)</p><p>Noisy Trapdoor Claw-free functions (NTCF) as powerful post-quantum
cryptographic tools can efficiently constrain actions of untrusted quantum
devices. Recently, Brakerski et al. at FOCS 2018 showed a remarkable use of
NTCF for a classically verifiable proof of quantumness and also derived a
protocol for cryptographically certifiable quantum randomness generation.
However, the original NTCF used in their work is essentially 2-to-1 one-way
function, namely NTCF$^1_2$, which greatly limits the rate of randomness
generation.
</p>
<p>In this work, we attempt to further extend the NTCF$^1_2$ to achieve a
$\kappa$-to-1 function with poly-bounded preimage size. Specifically, we focus
on a significant extrapolation of NTCF$^1_2$ by drawing on extrapolated
dihedral cosets, giving a model of NTCF$^1_{\kappa}$ with $\kappa = poly(n)$.
Then, we present an efficient construction of NTCF$^1_{\kappa}$ under the
well-known quantum hardness of the Learning with Errors (QLWE) assumption. As a
byproduct, our work manifests an interesting connection between the NTCF$^1_2$
(resp. NTCF$^1_{\kappa}$) and the Dihedral Coset States (resp. Extrapolated
Dihedral Coset States). Finally, we give a similar interactive protocol for
proving quantumness from the NTCF$^1_{\kappa}$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1">Xingyu Yan</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Licheng Wang</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1">Weiqiang Wen</a> (3), <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Ziyi Li</a> (4), <a href="http://arxiv.org/find/cs/1/au:+Suo_J/0/1/0/all/0/1">Jingwen Suo</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1">Lize Gu</a> (1) ((1) State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, 100876, China. (2) School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, 100081, China. (3) LTCI, Telecom Paris, Institut Polytechnique de Paris, Paris, France. (4) State Key Laboratory of Information Security, Institute of Information Engineering, University of Chinese Academy of Sciences, Beijing, 100049, China.)</p><p>Noisy Trapdoor Claw-free functions (NTCF) as powerful post-quantum
cryptographic tools can efficiently constrain actions of untrusted quantum
devices. Recently, Brakerski et al. at FOCS 2018 showed a remarkable use of
NTCF for a classically verifiable proof of quantumness and also derived a
protocol for cryptographically certifiable quantum randomness generation.
However, the original NTCF used in their work is essentially 2-to-1 one-way
function, namely NTCF$^1_2$, which greatly limits the rate of randomness
generation.
</p>
<p>In this work, we attempt to further extend the NTCF$^1_2$ to achieve a
$\kappa$-to-1 function with poly-bounded preimage size. Specifically, we focus
on a significant extrapolation of NTCF$^1_2$ by drawing on extrapolated
dihedral cosets, giving a model of NTCF$^1_{\kappa}$ with $\kappa = poly(n)$.
Then, we present an efficient construction of NTCF$^1_{\kappa}$ under the
well-known quantum hardness of the Learning with Errors (QLWE) assumption. As a
byproduct, our work manifests an interesting connection between the NTCF$^1_2$
(resp. NTCF$^1_{\kappa}$) and the Dihedral Coset States (resp. Extrapolated
Dihedral Coset States). Finally, we give a similar interactive protocol for
proving quantumness from the NTCF$^1_{\kappa}$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-01T01:30:00Z">Thursday, December 01 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.17159'>Opinion Evolution among friends and foes: the deterministic Majority Rule - extended abstract</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Miriam Di Ianni</p><p>The influence of the social relationships of an individual on the
individual's opinions (about a topic, a product, or whatever else) is a well
known phenomenon and it has been widely studied. This paper considers a network
of positive (i.e. trusting) or negative (distrusting) social relationships
where every individual has an initial positive or negative opinion (about a
topic, a product, or whatever else) that changes over time, at discrete
time-steps, due to the influences each individual gets from its neighbors.
Here, the influence of a trusted neighbor is consistent with the neighbor's
opinion, while the influence of an untrusted neighbor is opposite to the
neighbor's opinion. This extended abstract introduces the local threshold-based
opinion dynamics and, after stating the computational complexity of some
natural reachability problems arising in this setting when individuals change
their opinions according to the opinions of the majority of their neighbors,
proves an upper bound on the number of opinion configurations met by a
symmetric positive-only relationships network evolving according to any of such
models, which is polynomial in the size of the network. This generalizes a
result in [Krishnendu Chatterjee, Rasmus Ibsen-Jensen, Isma\"el Jecker, and
Jakub Svoboda, "Simplified Game of Life: Algorithms and Complexity", 45th
International Symposium on Mathematical Foundations of Computer Science (MFCS
2020)]
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ianni_M/0/1/0/all/0/1">Miriam Di Ianni</a></p><p>The influence of the social relationships of an individual on the
individual's opinions (about a topic, a product, or whatever else) is a well
known phenomenon and it has been widely studied. This paper considers a network
of positive (i.e. trusting) or negative (distrusting) social relationships
where every individual has an initial positive or negative opinion (about a
topic, a product, or whatever else) that changes over time, at discrete
time-steps, due to the influences each individual gets from its neighbors.
Here, the influence of a trusted neighbor is consistent with the neighbor's
opinion, while the influence of an untrusted neighbor is opposite to the
neighbor's opinion. This extended abstract introduces the local threshold-based
opinion dynamics and, after stating the computational complexity of some
natural reachability problems arising in this setting when individuals change
their opinions according to the opinions of the majority of their neighbors,
proves an upper bound on the number of opinion configurations met by a
symmetric positive-only relationships network evolving according to any of such
models, which is polynomial in the size of the network. This generalizes a
result in [Krishnendu Chatterjee, Rasmus Ibsen-Jensen, Isma\"el Jecker, and
Jakub Svoboda, "Simplified Game of Life: Algorithms and Complexity", 45th
International Symposium on Mathematical Foundations of Computer Science (MFCS
2020)]
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-01T01:30:00Z">Thursday, December 01 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.17211'>On Disperser/Lifting Properties of the Index and Inner-Product Functions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Paul Beame, Sajin Koroth</p><p>Query-to-communication lifting theorems, which connect the query complexity
of a Boolean function to the communication complexity of an associated `lifted'
function obtained by composing the function with many copies of another
function known as a gadget, have been instrumental in resolving many open
questions in computational complexity. Several important complexity questions
could be resolved if we could make substantial improvements in the input size
required for lifting with the Index function, from its current near-linear size
down to polylogarithmic in the number of inputs $N$ of the original function
or, ideally, constant. The near-linear size bound was shown by Lovett, Meka,
Mertz, Pitassi and Zhang using a recent breakthrough improvement on the
Sunflower Lemma to show that a certain graph associated with the Index function
of near-linear size is a disperser. They also stated a conjecture about the
Index function that is essential for further improvements in the size required
for lifting with Index using current techniques. In this paper we prove the
following;
</p>
<p>1) The conjecture of Lovett et al. is false when the size of the Index gadget
is $\log N-\omega(1)$.
</p>
<p>2) Also, the Inner-Product function, which satisfies the disperser property
at size $O(\log N)$, does not have this property when its size is $\log
N-\omega(1)$.
</p>
<p>3) Nonetheless, using Index gadgets of size at least 4, we prove a lifting
theorem for a restricted class of communication protocols in which one of the
players is limited to sending parities of its inputs.
</p>
<p>4) Using the ideas from this lifting theorem, we derive a strong lifting
theorem from decision tree size to parity decision tree size. We use this to
derive a general lifting theorem in proof complexity from tree-resolution size
to tree-like $Res(\oplus)$ refutation size, which yields many new exponential
lower bounds on such proofs.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Beame_P/0/1/0/all/0/1">Paul Beame</a>, <a href="http://arxiv.org/find/cs/1/au:+Koroth_S/0/1/0/all/0/1">Sajin Koroth</a></p><p>Query-to-communication lifting theorems, which connect the query complexity
of a Boolean function to the communication complexity of an associated `lifted'
function obtained by composing the function with many copies of another
function known as a gadget, have been instrumental in resolving many open
questions in computational complexity. Several important complexity questions
could be resolved if we could make substantial improvements in the input size
required for lifting with the Index function, from its current near-linear size
down to polylogarithmic in the number of inputs $N$ of the original function
or, ideally, constant. The near-linear size bound was shown by Lovett, Meka,
Mertz, Pitassi and Zhang using a recent breakthrough improvement on the
Sunflower Lemma to show that a certain graph associated with the Index function
of near-linear size is a disperser. They also stated a conjecture about the
Index function that is essential for further improvements in the size required
for lifting with Index using current techniques. In this paper we prove the
following;
</p>
<p>1) The conjecture of Lovett et al. is false when the size of the Index gadget
is $\log N-\omega(1)$.
</p>
<p>2) Also, the Inner-Product function, which satisfies the disperser property
at size $O(\log N)$, does not have this property when its size is $\log
N-\omega(1)$.
</p>
<p>3) Nonetheless, using Index gadgets of size at least 4, we prove a lifting
theorem for a restricted class of communication protocols in which one of the
players is limited to sending parities of its inputs.
</p>
<p>4) Using the ideas from this lifting theorem, we derive a strong lifting
theorem from decision tree size to parity decision tree size. We use this to
derive a general lifting theorem in proof complexity from tree-resolution size
to tree-like $Res(\oplus)$ refutation size, which yields many new exponential
lower bounds on such proofs.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-01T01:30:00Z">Thursday, December 01 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.17214'>Lifting to Parity Decision Trees Via Stifling</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Arkadev Chattopadhyay, Nikhil S. Mande, Swagato Sanyal, Suhail Sherif</p><p>We show that the deterministic decision tree complexity of a (partial)
function or relation $f$ lifts to the deterministic parity decision tree (PDT)
size complexity of the composed function/relation $f \circ g$ as long as the
gadget $g$ satisfies a property that we call stifling. We observe that several
simple gadgets of constant size, like Indexing on 3 input bits, Inner Product
on 4 input bits, Majority on 3 input bits and random functions, satisfy this
property. It can be shown that existing randomized communication lifting
theorems ([G\"{o}\"{o}s, Pitassi, Watson. SICOMP'20], [Chattopadhyay et al.
SICOMP'21]) imply PDT-size lifting. However there are two shortcomings of this
approach: first they lift randomized decision tree complexity of $f$, which
could be exponentially smaller than its deterministic counterpart when either
$f$ is a partial function or even a total search problem. Second, the size of
the gadgets in such lifting theorems are as large as logarithmic in the size of
the input to $f$. Reducing the gadget size to a constant is an important open
problem at the frontier of current research.
</p>
<p>Our result shows that even a random constant-size gadget does enable lifting
to PDT size. Further, it also yields the first systematic way of turning lower
bounds on the width of tree-like resolution proofs of the unsatisfiability of
constant-width CNF formulas to lower bounds on the size of tree-like proofs in
the resolution with parity system, i.e., $\textit{Res}$($\oplus$), of the
unsatisfiability of closely related constant-width CNF formulas.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chattopadhyay_A/0/1/0/all/0/1">Arkadev Chattopadhyay</a>, <a href="http://arxiv.org/find/cs/1/au:+Mande_N/0/1/0/all/0/1">Nikhil S. Mande</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1">Swagato Sanyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Sherif_S/0/1/0/all/0/1">Suhail Sherif</a></p><p>We show that the deterministic decision tree complexity of a (partial)
function or relation $f$ lifts to the deterministic parity decision tree (PDT)
size complexity of the composed function/relation $f \circ g$ as long as the
gadget $g$ satisfies a property that we call stifling. We observe that several
simple gadgets of constant size, like Indexing on 3 input bits, Inner Product
on 4 input bits, Majority on 3 input bits and random functions, satisfy this
property. It can be shown that existing randomized communication lifting
theorems ([G\"{o}\"{o}s, Pitassi, Watson. SICOMP'20], [Chattopadhyay et al.
SICOMP'21]) imply PDT-size lifting. However there are two shortcomings of this
approach: first they lift randomized decision tree complexity of $f$, which
could be exponentially smaller than its deterministic counterpart when either
$f$ is a partial function or even a total search problem. Second, the size of
the gadgets in such lifting theorems are as large as logarithmic in the size of
the input to $f$. Reducing the gadget size to a constant is an important open
problem at the frontier of current research.
</p>
<p>Our result shows that even a random constant-size gadget does enable lifting
to PDT size. Further, it also yields the first systematic way of turning lower
bounds on the width of tree-like resolution proofs of the unsatisfiability of
constant-width CNF formulas to lower bounds on the size of tree-like proofs in
the resolution with parity system, i.e., $\textit{Res}$($\oplus$), of the
unsatisfiability of closely related constant-width CNF formulas.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-01T01:30:00Z">Thursday, December 01 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.17054'>Approximating robot reachable space using convex polytopes</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Antun Skuric (AUCTUS), Vincent Padois (AUCTUS), David Daney (AUCTUS, IMS)</p><p>This paper presents an approach for approximating the reachable space of
robotic manipulators based on convex polytopes. The proposed approach predicts
the reachable space over a given time horizon based on the robot's actuation
limits and kinematic constraints. The approach is furthermore extended to
integrate the robot's environment, assuming it can be expressed in a form of
linear constraints, and to account for the robot's link geometry.The accuracy
of the proposed method is evaluated using simulations of robot's nonlinear
dynamics and it is compared against the cartesian space limits, usually
provided by manufacturers in standard datasheets.The accuracy analysis results
show that the proposed method has good performance for the time horizons up to
250ms, encapsulating most of the simulated robot's reachable space while
maintaining comparable volume. For a 7 dof robot, the method has an average
execution time of 50ms, independent of the horizon time, potentially enabling
real-time applications.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Skuric_A/0/1/0/all/0/1">Antun Skuric</a> (AUCTUS), <a href="http://arxiv.org/find/cs/1/au:+Padois_V/0/1/0/all/0/1">Vincent Padois</a> (AUCTUS), <a href="http://arxiv.org/find/cs/1/au:+Daney_D/0/1/0/all/0/1">David Daney</a> (AUCTUS, IMS)</p><p>This paper presents an approach for approximating the reachable space of
robotic manipulators based on convex polytopes. The proposed approach predicts
the reachable space over a given time horizon based on the robot's actuation
limits and kinematic constraints. The approach is furthermore extended to
integrate the robot's environment, assuming it can be expressed in a form of
linear constraints, and to account for the robot's link geometry.The accuracy
of the proposed method is evaluated using simulations of robot's nonlinear
dynamics and it is compared against the cartesian space limits, usually
provided by manufacturers in standard datasheets.The accuracy analysis results
show that the proposed method has good performance for the time horizons up to
250ms, encapsulating most of the simulated robot's reachable space while
maintaining comparable volume. For a 7 dof robot, the method has an average
execution time of 50ms, independent of the horizon time, potentially enabling
real-time applications.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-01T01:30:00Z">Thursday, December 01 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.16908'>Improved Smoothed Analysis of 2-Opt for the Euclidean TSP</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Bodo Manthey, Jesse van Rhijn</p><p>The 2-opt heuristic is a simple local search heuristic for the Travelling
Salesperson Problem (TSP). Although it usually performs well in practice, its
worst-case running time is poor. Attempts to reconcile this difference have
used smoothed analysis, in which adversarial instances are perturbed
probabilistically.
</p>
<p>We are interested in the classical model of smoothed analysis for the
Euclidean TSP, in which the perturbations are Gaussian. This model was
previously used by Manthey \&amp; Veenstra, who obtained smoothed complexity bounds
polynomial in $n$, the dimension $d$, and the perturbation strength
$\sigma^{-1}$. However, their analysis only works for $d \geq 4$. The only
previous analysis for $d \leq 3$ was performed by Englert, R\"oglin \&amp;
V\"ocking, who used a different perturbation model which can be translated to
Gaussian perturbations. Their model yields bounds polynomial in $n$ and
$\sigma^{-d}$, and super-exponential in $d$.
</p>
<p>As no direct analysis existed for Gaussian perturbations that yields
polynomial bounds for all $d$, we perform this missing analysis. Along the way,
we improve all existing smoothed complexity bounds for Euclidean 2-opt.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Manthey_B/0/1/0/all/0/1">Bodo Manthey</a>, <a href="http://arxiv.org/find/cs/1/au:+Rhijn_J/0/1/0/all/0/1">Jesse van Rhijn</a></p><p>The 2-opt heuristic is a simple local search heuristic for the Travelling
Salesperson Problem (TSP). Although it usually performs well in practice, its
worst-case running time is poor. Attempts to reconcile this difference have
used smoothed analysis, in which adversarial instances are perturbed
probabilistically.
</p>
<p>We are interested in the classical model of smoothed analysis for the
Euclidean TSP, in which the perturbations are Gaussian. This model was
previously used by Manthey \&amp; Veenstra, who obtained smoothed complexity bounds
polynomial in $n$, the dimension $d$, and the perturbation strength
$\sigma^{-1}$. However, their analysis only works for $d \geq 4$. The only
previous analysis for $d \leq 3$ was performed by Englert, R\"oglin \&amp;
V\"ocking, who used a different perturbation model which can be translated to
Gaussian perturbations. Their model yields bounds polynomial in $n$ and
$\sigma^{-d}$, and super-exponential in $d$.
</p>
<p>As no direct analysis existed for Gaussian perturbations that yields
polynomial bounds for all $d$, we perform this missing analysis. Along the way,
we improve all existing smoothed complexity bounds for Euclidean 2-opt.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-01T01:30:00Z">Thursday, December 01 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.17050'>Internal Closedness and von Neumann-Morgenstern Stability in Matching Theory: Structures and Complexity</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yuri Faenza, Clifford Stein, Jia Wan</p><p>Let $G$ be a graph and suppose we are given, for each $v \in V(G)$, a strict
ordering of the neighbors of $v$. A set of matchings $\mathcal{M}$ of $G$ is
called internally stable if there are no matchings $M,M' \in \mathcal{M}$ such
that an edge of $M$ blocks $M'$. The sets of stable matchings and of von
Neumann-Morgenstern stable matchings are examples of internally stable sets of
matching.
</p>
<p>In this paper, we introduce and study, in both the marriage and the roommate
case, inclusionwise maximal internally stable sets of matchings. We call those
sets internally closed. By building on known and newly developed algebraic
structures associated to sets of matchings, we investigate the complexity of
deciding if a set of matchings is internally closed, and if it is von
Neumann-Morgenstern stable.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Faenza_Y/0/1/0/all/0/1">Yuri Faenza</a>, <a href="http://arxiv.org/find/math/1/au:+Stein_C/0/1/0/all/0/1">Clifford Stein</a>, <a href="http://arxiv.org/find/math/1/au:+Wan_J/0/1/0/all/0/1">Jia Wan</a></p><p>Let $G$ be a graph and suppose we are given, for each $v \in V(G)$, a strict
ordering of the neighbors of $v$. A set of matchings $\mathcal{M}$ of $G$ is
called internally stable if there are no matchings $M,M' \in \mathcal{M}$ such
that an edge of $M$ blocks $M'$. The sets of stable matchings and of von
Neumann-Morgenstern stable matchings are examples of internally stable sets of
matching.
</p>
<p>In this paper, we introduce and study, in both the marriage and the roommate
case, inclusionwise maximal internally stable sets of matchings. We call those
sets internally closed. By building on known and newly developed algebraic
structures associated to sets of matchings, we investigate the complexity of
deciding if a set of matchings is internally closed, and if it is von
Neumann-Morgenstern stable.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-01T01:30:00Z">Thursday, December 01 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.16581'>Batching and Optimal Multi-stage Bipartite Allocations</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yiding Feng, Rad Niazadeh</p><p>In several applications of real-time matching of demand to supply in online
marketplaces, the platform allows for some latency to batch the demand and
improve the efficiency. Motivated by these applications, we study the optimal
trade-off between batching and inefficiency under adversarial arrival. As our
base model, we consider K-stage variants of the vertex weighted b-matching in
the adversarial setting, where online vertices arrive stage-wise and in K
batches -- in contrast to online arrival. Our main result for this problem is
an optimal (1-(1-1/K)^K)- competitive (fractional) matching algorithm,
improving the classic (1-1/e) competitive ratio bound known for its online
variant (Mehta et al., 2007; Aggarwal et al., 2011). We also extend this result
to the rich model of multi-stage configuration allocation with free-disposals
(Devanur et al., 2016), which is motivated by the display advertising in video
streaming platforms.
</p>
<p>Our main technique is developing tools to vary the trade-off between
"greedy-ness" and "hedging" of the algorithm across stages. We rely on a
particular family of convex-programming based matchings that distribute the
demand in a specifically balanced way among supply in different stages, while
carefully modifying the balancedness of the resulting matching across stages.
More precisely, we identify a sequence of polynomials with decreasing degrees
to be used as strictly concave regularizers of the maximum weight matching
linear program to form these convex programs. At each stage, our algorithm
returns the corresponding regularized optimal solution as the matching of this
stage (by solving the convex program). Using structural properties of these
convex programs and recursively connecting the regularizers together, we
develop a new multi-stage primal-dual framework to analyze the competitive
ratio. We further show this algorithm is optimally competitive.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yiding Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Niazadeh_R/0/1/0/all/0/1">Rad Niazadeh</a></p><p>In several applications of real-time matching of demand to supply in online
marketplaces, the platform allows for some latency to batch the demand and
improve the efficiency. Motivated by these applications, we study the optimal
trade-off between batching and inefficiency under adversarial arrival. As our
base model, we consider K-stage variants of the vertex weighted b-matching in
the adversarial setting, where online vertices arrive stage-wise and in K
batches -- in contrast to online arrival. Our main result for this problem is
an optimal (1-(1-1/K)^K)- competitive (fractional) matching algorithm,
improving the classic (1-1/e) competitive ratio bound known for its online
variant (Mehta et al., 2007; Aggarwal et al., 2011). We also extend this result
to the rich model of multi-stage configuration allocation with free-disposals
(Devanur et al., 2016), which is motivated by the display advertising in video
streaming platforms.
</p>
<p>Our main technique is developing tools to vary the trade-off between
"greedy-ness" and "hedging" of the algorithm across stages. We rely on a
particular family of convex-programming based matchings that distribute the
demand in a specifically balanced way among supply in different stages, while
carefully modifying the balancedness of the resulting matching across stages.
More precisely, we identify a sequence of polynomials with decreasing degrees
to be used as strictly concave regularizers of the maximum weight matching
linear program to form these convex programs. At each stage, our algorithm
returns the corresponding regularized optimal solution as the matching of this
stage (by solving the convex program). Using structural properties of these
convex programs and recursively connecting the regularizers together, we
develop a new multi-stage primal-dual framework to analyze the competitive
ratio. We further show this algorithm is optimally competitive.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-01T01:30:00Z">Thursday, December 01 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.16660'>Approximating binary longest common subsequence in near-linear time</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xiaoyu He, Ray Li</p><p>The Longest Common Subsequence (LCS) is a fundamental string similarity
measure, and computing the LCS of two strings is a classic algorithms question.
A textbook dynamic programming algorithm gives an exact algorithm in quadratic
time, and this is essentially best possible under plausible fine-grained
complexity assumptions, so a natural problem is to find faster approximation
algorithms. When the inputs are two binary strings, there is a simple
$\frac{1}{2}$-approximation in linear time: compute the longest common all-0s
or all-1s subsequence. It has been open whether a better approximation is
possible even in truly subquadratic time. Rubinstein and Song showed that the
answer is yes under the assumption that the two input strings have equal
lengths. We settle the question, generalizing their result to unequal length
strings, proving that, for any $\varepsilon&gt;0$, there exists $\delta&gt;0$ and a
$(\frac{1}{2}+\delta)$-approximation algorithm for binary LCS that runs in
$n^{1+\varepsilon}$ time. As a consequence of our result and a result of Akmal
and Vassilevska-Williams, for any $\varepsilon&gt;0$, there exists a
$(\frac{1}{q}+\delta)$-approximation for LCS over $q$-ary strings in
$n^{1+\varepsilon}$ time.
</p>
<p>Our techniques build on the recent work of Guruswami, He, and Li who proved
new bounds for error-correcting codes tolerating deletion errors. They prove a
combinatorial "structure lemma" for strings which classifies them according to
their oscillation patterns. We prove and use an algorithmic generalization of
this structure lemma, which may be of independent interest.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiaoyu He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ray Li</a></p><p>The Longest Common Subsequence (LCS) is a fundamental string similarity
measure, and computing the LCS of two strings is a classic algorithms question.
A textbook dynamic programming algorithm gives an exact algorithm in quadratic
time, and this is essentially best possible under plausible fine-grained
complexity assumptions, so a natural problem is to find faster approximation
algorithms. When the inputs are two binary strings, there is a simple
$\frac{1}{2}$-approximation in linear time: compute the longest common all-0s
or all-1s subsequence. It has been open whether a better approximation is
possible even in truly subquadratic time. Rubinstein and Song showed that the
answer is yes under the assumption that the two input strings have equal
lengths. We settle the question, generalizing their result to unequal length
strings, proving that, for any $\varepsilon&gt;0$, there exists $\delta&gt;0$ and a
$(\frac{1}{2}+\delta)$-approximation algorithm for binary LCS that runs in
$n^{1+\varepsilon}$ time. As a consequence of our result and a result of Akmal
and Vassilevska-Williams, for any $\varepsilon&gt;0$, there exists a
$(\frac{1}{q}+\delta)$-approximation for LCS over $q$-ary strings in
$n^{1+\varepsilon}$ time.
</p>
<p>Our techniques build on the recent work of Guruswami, He, and Li who proved
new bounds for error-correcting codes tolerating deletion errors. They prove a
combinatorial "structure lemma" for strings which classifies them according to
their oscillation patterns. We prove and use an algorithmic generalization of
this structure lemma, which may be of independent interest.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-01T01:30:00Z">Thursday, December 01 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.16747'>Approximate minimum cuts and their enumeration</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Calvin Beideman, Karthekeyan Chandrasekaran, Weihang Wang</p><p>We show that every $\alpha$-approximate minimum cut in a connected graph is
the unique minimum $(S,T)$-terminal cut for some subsets $S$ and $T$ of
vertices each of size at most $\lfloor2\alpha\rfloor+1$. This leads to an
alternative proof that the number of $\alpha$-approximate minimum cuts in a
$n$-vertex connected graph is $n^{O(\alpha)}$ and they can all be enumerated in
deterministic polynomial time for constant $\alpha$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Beideman_C/0/1/0/all/0/1">Calvin Beideman</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_K/0/1/0/all/0/1">Karthekeyan Chandrasekaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weihang Wang</a></p><p>We show that every $\alpha$-approximate minimum cut in a connected graph is
the unique minimum $(S,T)$-terminal cut for some subsets $S$ and $T$ of
vertices each of size at most $\lfloor2\alpha\rfloor+1$. This leads to an
alternative proof that the number of $\alpha$-approximate minimum cuts in a
$n$-vertex connected graph is $n^{O(\alpha)}$ and they can all be enumerated in
deterministic polynomial time for constant $\alpha$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-01T01:30:00Z">Thursday, December 01 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.16860'>Gapped String Indexing in Subquadratic Space and Sublinear Query Time</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Philip Bille, Inge Li G&#xf8;rtz, Moshe Lewenstein, Solon P. Pissis, Eva Rotenberg, Teresa Anna Steiner</p><p>In Gapped String Indexing, the goal is to compactly represent a string $S$ of
length $n$ such that given queries consisting of two strings $P_1$ and $P_2$,
called patterns, and an integer interval $[\alpha, \beta]$, called gap range,
we can quickly find occurrences of $P_1$ and $P_2$ in $S$ with distance in
$[\alpha, \beta]$. Due to the many applications of this fundamental problem in
computational biology and elsewhere, there is a great body of work for
restricted or parameterised variants of the problem. However, for the general
problem statement, no improvements upon the trivial $\mathcal{O}(n)$-space
$\mathcal{O}(n)$-query time or $\Omega(n^2)$-space $\mathcal{\tilde{O}}(|P_1| +
|P_2| + \mathrm{occ})$-query time solutions were known so far. We break this
barrier obtaining interesting trade-offs with polynomially subquadratic space
and polynomially sublinear query time. In particular, we show that, for every
$0\leq \delta \leq 1$, there is a data structure for Gapped String Indexing
with either $\mathcal{\tilde{O}}(n^{2-\delta/3})$ or
$\mathcal{\tilde{O}}(n^{3-2\delta})$ space and $\mathcal{\tilde{O}}(|P_1| +
|P_2| + n^{\delta}\cdot (\mathrm{occ}+1))$ query time, where $\mathrm{occ}$ is
the number of reported occurrences. As a new fundamental tool towards obtaining
our main result, we introduce the Shifted Set Intersection problem: preprocess
a collection of sets $S_1, \ldots, S_k$ of integers such that given queries
consisting of three integers $i,j,s$, we can quickly output YES if and only if
there exist $a \in S_i$ and $b \in S_j$ with $a+s = b$. We start by showing
that the Shifted Set Intersection problem is equivalent to the indexing variant
of 3SUM (3SUM Indexing) [Golovnev et al., STOC 2020]. Via several steps of
reduction we then show that the Gapped String Indexing problem reduces to
polylogarithmically many instances of the Shifted Set Intersection problem.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bille_P/0/1/0/all/0/1">Philip Bille</a>, <a href="http://arxiv.org/find/cs/1/au:+Gortz_I/0/1/0/all/0/1">Inge Li G&#xf8;rtz</a>, <a href="http://arxiv.org/find/cs/1/au:+Lewenstein_M/0/1/0/all/0/1">Moshe Lewenstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Pissis_S/0/1/0/all/0/1">Solon P. Pissis</a>, <a href="http://arxiv.org/find/cs/1/au:+Rotenberg_E/0/1/0/all/0/1">Eva Rotenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Steiner_T/0/1/0/all/0/1">Teresa Anna Steiner</a></p><p>In Gapped String Indexing, the goal is to compactly represent a string $S$ of
length $n$ such that given queries consisting of two strings $P_1$ and $P_2$,
called patterns, and an integer interval $[\alpha, \beta]$, called gap range,
we can quickly find occurrences of $P_1$ and $P_2$ in $S$ with distance in
$[\alpha, \beta]$. Due to the many applications of this fundamental problem in
computational biology and elsewhere, there is a great body of work for
restricted or parameterised variants of the problem. However, for the general
problem statement, no improvements upon the trivial $\mathcal{O}(n)$-space
$\mathcal{O}(n)$-query time or $\Omega(n^2)$-space $\mathcal{\tilde{O}}(|P_1| +
|P_2| + \mathrm{occ})$-query time solutions were known so far. We break this
barrier obtaining interesting trade-offs with polynomially subquadratic space
and polynomially sublinear query time. In particular, we show that, for every
$0\leq \delta \leq 1$, there is a data structure for Gapped String Indexing
with either $\mathcal{\tilde{O}}(n^{2-\delta/3})$ or
$\mathcal{\tilde{O}}(n^{3-2\delta})$ space and $\mathcal{\tilde{O}}(|P_1| +
|P_2| + n^{\delta}\cdot (\mathrm{occ}+1))$ query time, where $\mathrm{occ}$ is
the number of reported occurrences. As a new fundamental tool towards obtaining
our main result, we introduce the Shifted Set Intersection problem: preprocess
a collection of sets $S_1, \ldots, S_k$ of integers such that given queries
consisting of three integers $i,j,s$, we can quickly output YES if and only if
there exist $a \in S_i$ and $b \in S_j$ with $a+s = b$. We start by showing
that the Shifted Set Intersection problem is equivalent to the indexing variant
of 3SUM (3SUM Indexing) [Golovnev et al., STOC 2020]. Via several steps of
reduction we then show that the Gapped String Indexing problem reduces to
polylogarithmically many instances of the Shifted Set Intersection problem.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-01T01:30:00Z">Thursday, December 01 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.17067'>Fair Ranking with Noisy Protected Attributes</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Anay Mehrotra, Nisheeth K. Vishnoi</p><p>The fair-ranking problem, which asks to rank a given set of items to maximize
utility subject to group fairness constraints, has received attention in the
fairness, information retrieval, and machine learning literature. Recent works,
however, observe that errors in socially-salient (including protected)
attributes of items can significantly undermine fairness guarantees of existing
fair-ranking algorithms and raise the problem of mitigating the effect of such
errors. We study the fair-ranking problem under a model where socially-salient
attributes of items are randomly and independently perturbed. We present a
fair-ranking framework that incorporates group fairness requirements along with
probabilistic information about perturbations in socially-salient attributes.
We provide provable guarantees on the fairness and utility attainable by our
framework and show that it is information-theoretically impossible to
significantly beat these guarantees. Our framework works for multiple
non-disjoint attributes and a general class of fairness constraints that
includes proportional and equal representation. Empirically, we observe that,
compared to baselines, our algorithm outputs rankings with higher fairness, and
has a similar or better fairness-utility trade-off compared to baselines.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Mehrotra_A/0/1/0/all/0/1">Anay Mehrotra</a>, <a href="http://arxiv.org/find/cs/1/au:+Vishnoi_N/0/1/0/all/0/1">Nisheeth K. Vishnoi</a></p><p>The fair-ranking problem, which asks to rank a given set of items to maximize
utility subject to group fairness constraints, has received attention in the
fairness, information retrieval, and machine learning literature. Recent works,
however, observe that errors in socially-salient (including protected)
attributes of items can significantly undermine fairness guarantees of existing
fair-ranking algorithms and raise the problem of mitigating the effect of such
errors. We study the fair-ranking problem under a model where socially-salient
attributes of items are randomly and independently perturbed. We present a
fair-ranking framework that incorporates group fairness requirements along with
probabilistic information about perturbations in socially-salient attributes.
We provide provable guarantees on the fairness and utility attainable by our
framework and show that it is information-theoretically impossible to
significantly beat these guarantees. Our framework works for multiple
non-disjoint attributes and a general class of fairness constraints that
includes proportional and equal representation. Empirically, we observe that,
compared to baselines, our algorithm outputs rankings with higher fairness, and
has a similar or better fairness-utility trade-off compared to baselines.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-01T01:30:00Z">Thursday, December 01 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.17131'>Nonmonontone submodular maximization under routing constraints</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Haotian Zhang, Rao Li, Zewei Wu, Guodong Sun</p><p>In machine learning and big data, the optimization objectives based on
set-cover, entropy, diversity, influence, feature selection, etc. are commonly
modeled as submodular functions. Submodular (function) maximization is
generally NP-hard, even in the absence of constraints. Recently, submodular
maximization has been successfully investigated for the settings where the
objective function is monotone or the constraint is computation-tractable.
However, maximizing nonmonotone submodular function with complex constraints is
not yet well-understood. In this paper, we consider the nonmonotone submodular
maximization with a cost budget or feasibility constraint (especially from
route planning) that is generally NP-hard to evaluate. Such a problem is common
for machine learning, big data, and robotics. This problem is NP-hard, and on
top of that, its constraint evaluation is also NP-hard, which adds an
additional layer of complexity. So far, few studies have been devoted to
proposing effective solutions, making this problem currently unclear. In this
paper, we first propose an iterated greedy algorithm, which yields an
approximation solution. Then we develop the proof machinery that shows our
algorithm is a bi-criterion approximation algorithm: it can achieve a
constant-factor approximation to the optimal algorithm, while keeping the
over-budget tightly bounded. We also explore practical considerations of
achieving a trade-off between time complexity and over-budget. Finally, we
conduct numeric experiments on two concrete examples, and show our design's
efficacy in practical settings.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haotian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Rao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zewei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1">Guodong Sun</a></p><p>In machine learning and big data, the optimization objectives based on
set-cover, entropy, diversity, influence, feature selection, etc. are commonly
modeled as submodular functions. Submodular (function) maximization is
generally NP-hard, even in the absence of constraints. Recently, submodular
maximization has been successfully investigated for the settings where the
objective function is monotone or the constraint is computation-tractable.
However, maximizing nonmonotone submodular function with complex constraints is
not yet well-understood. In this paper, we consider the nonmonotone submodular
maximization with a cost budget or feasibility constraint (especially from
route planning) that is generally NP-hard to evaluate. Such a problem is common
for machine learning, big data, and robotics. This problem is NP-hard, and on
top of that, its constraint evaluation is also NP-hard, which adds an
additional layer of complexity. So far, few studies have been devoted to
proposing effective solutions, making this problem currently unclear. In this
paper, we first propose an iterated greedy algorithm, which yields an
approximation solution. Then we develop the proof machinery that shows our
algorithm is a bi-criterion approximation algorithm: it can achieve a
constant-factor approximation to the optimal algorithm, while keeping the
over-budget tightly bounded. We also explore practical considerations of
achieving a trade-off between time complexity and over-budget. Finally, we
conduct numeric experiments on two concrete examples, and show our design's
efficacy in practical settings.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-01T01:30:00Z">Thursday, December 01 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Wednesday, November 30
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/30/assistant-associate-or-full-professor-search-2-at-university-of-california-san-diego-apply-by-january-1-2023/'>Assistant, Associate, or Full Professor (Search 2) at University of California – San Diego (apply by January 1, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The UC San Diego Department of Computer Science and Engineering (CSE) invites applications for multiple tenure-track faculty positions at the Assistant Professor or tenured faculty positions at the Associate, or Full Professor rank. The department is looking for exceptional candidates in all areas of Computer Science and Engineering. Website: cse.ucsd.edu/administration/human-resources/recruitment/faculty-positions Email: nherrera@eng.ucsd.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The UC San Diego Department of Computer Science and Engineering (CSE) invites applications for multiple tenure-track faculty positions at the Assistant Professor or tenured faculty positions at the Associate, or Full Professor rank. The department is looking for exceptional candidates in all areas of Computer Science and Engineering.</p>
<p>Website: <a href="https://cse.ucsd.edu/administration/human-resources/recruitment/faculty-positions">https://cse.ucsd.edu/administration/human-resources/recruitment/faculty-positions</a><br />
Email: nherrera@eng.ucsd.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-30T23:22:09Z">Wednesday, November 30 2022, 23:22</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/30/assistant-associate-or-full-professor-search-1-at-university-of-california-san-diego-apply-by-january-1-2023/'>Assistant, Associate, or Full Professor (Search 1) at University of California – San Diego (apply by January 1, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The UC San Diego Department of Computer Science and Engineering (CSE) invites applications for multiple tenure-track faculty positions at the Assistant Professor or tenured faculty positions at Associate, or Full Professor rank. The department is looking for exceptional candidates in all areas of Computer Science and Engineering. Website: cse.ucsd.edu/administration/human-resources/recruitment/faculty-positions Email: nherrera@eng.ucsd.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The UC San Diego Department of Computer Science and Engineering (CSE) invites applications for multiple tenure-track faculty positions at the Assistant Professor or tenured faculty positions at Associate, or Full Professor rank. The department is looking for exceptional candidates in all areas of Computer Science and Engineering.</p>
<p>Website: <a href="https://cse.ucsd.edu/administration/human-resources/recruitment/faculty-positions">https://cse.ucsd.edu/administration/human-resources/recruitment/faculty-positions</a><br />
Email: nherrera@eng.ucsd.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-30T23:20:48Z">Wednesday, November 30 2022, 23:20</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2022/11/30/linkage.html'>Linkage</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Digital astronomy with cellular automata (\(\mathbb{M}\), via). No, this is not about using CA to study patterns in the sky (though that might be interesting); it is about using UMAP dimension-reduction techniques to create something like a Hertzsprung-Russell diagram for finding CA rules with complex behavior.
        
        </div>

        <div class='tr-article-summary'>
        
          
          <ul>
  <li>
    <p><a href="https://kylehovey.github.io/blog/automata-nebula">Digital astronomy with cellular automata</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109356657961629085">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=33578376">via</a>). No, this is not about using CA to study patterns in the sky (though that might be interesting); it is about using <a href="https://umap-learn.readthedocs.io/en/latest/">UMAP dimension-reduction techniques</a> to create something like a Hertzsprung-Russell diagram for finding CA rules with complex behavior.</p>
  </li>
  <li>
    <p><a href="https://www.ics.uci.edu/~eppstein/pix/guanajuato/">My photos from my recent visit to Guanajuato are now online</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109357876852587696">\(\mathbb{M}\)</a>).</span> Very pretty city, and good food, worth a visit. My recommendation from the locals for their favorite restaurant was Truco 7, where I had amazing chicken mole for much less than I would have expected. Off the main streets much of the city looks like the photo below: steep narrow alleys with very colorful buildings. Maybe not the best for accessibility.</p>

    <p style="text-align:center"><img src="https://www.ics.uci.edu/~eppstein/pix/guanajuato/9-m.jpg" alt="Steep alley in Guanajuato, Mexico, lined with colorful buildings" style="border-style:solid;border-color:black" /></p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@ct_bergstrom@fediscience.org/109355406095574860">Galactica can generate Wikipedia articles, supposedly</a>, but actually writes total bullshit resembling a Wikipedia article but not resembling any real-world use of the title term. The example given by Carl T. Bergstrom is for “Brandolini’s law, the principle that bullshit takes another of magnitude less effort create than to clean up”.</p>
  </li>
  <li>
    <p><a href="https://leanprover-community.github.io/sphere-eversion/">Formal proof of Smale’s sphere-inversion theorem</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@highergeometer/109354222002349360">\(\mathbb{M}\)</a>),</span> showing that “Lean doesn’t do just algebra or abstract nonsense, but really serious geometric topology, too”.</p>
  </li>
  <li>
    <p><a href="https://b3s23life.blogspot.com/2022/11/in-conways-life-fifteen-gliders-can.html">You can build anything you want in Conway’s Game of Life by colliding 15 gliders together</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@OscarCunningham/109375074140276198">\(\mathbb{M}\)</a>).</span> Anything that you can build out of any larger number of gliders, that is.</p>
  </li>
  <li>
    <p><a href="https://www.c82.net/math-instruments/">The Construction &amp; Principal Uses of Mathematical Instruments</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109384275129127171">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=33592636">via</a>), a digital edition of a 1709 book by Nicolas Bion, as translated into English in 1758 by Edmund Stone. Site designed as an exercise by Nicholas Rougeux; see also <a href="https://www.c82.net/blog/?id=90">Rougeux’s blog post about the site design</a>.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@tao/109390971278692349">Maths at internet speed</a>. Terry Tao links to <a href="https://arxiv.org/abs/2211.09055">Justin Gilmer’s breakthrough</a> on the <a href="https://en.wikipedia.org/wiki/Union-closed_sets_conjecture">union-closed sets conjecture</a>, followed in quick succession by improvements by <a href="https://arxiv.org/abs/2211.11504">Will Sawin</a>, <a href="https://arxiv.org/abs/2211.11689">Zachary Chase and Shachar Lovett</a>, and <a href="https://arxiv.org/abs/2211.11731">Ryan Alweiss, Brice Huang, and Mark Sellke</a>. See also <a href="https://gilkalai.wordpress.com/2022/11/17/amazing-justin-gilmer-gave-a-constant-lower-bound-for-the-union-closed-sets-conjecture/">Gil Kalai’s blog post</a>. In other news, <a href="https://mathstodon.xyz/@tao">Terry Tao is now on mathstodon</a>.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@oschene@mastodon.social">The Chambered Nautilus</a> <span style="white-space:nowrap">(<a href="MLINK">\(\mathbb{M}\)</a>).</span> Curved paperfolding model by oschene (Philip Chapman-Bell).</p>
  </li>
  <li>
    <p>Minor terminological disagreement with some coauthors <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109406394361043315">\(\mathbb{M}\)</a>):</span> let \(T\) be a single-source shortest path tree in an unweighted undirected graph. Can you call \(T\) a “BFS tree”? Or is that only for trees that could be generated by breadth-first search? E.g. consider shortest paths in \(K_{2,3}\) starting from the 3-vertex side. For breadth first search, two leaves from the 3-vertex side will share a parent. But there is a different shortest path tree where they have different parents. Is it a BFS tree?</p>

    <p style="text-align:center"><img src="/blog/assets/2022/bfs-vs-shortest.svg" alt="K_{2,3}, a BFS tree, and a shortest-path tree that cannot be generated by breadth-first search" style="width:100%;max-width:600px" /></p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@johncarlosbaez/109398225812984155">Extended thread on combinatorial species of linear orderings and permutations by John Baez</a> <span style="white-space:nowrap">(<a href="MLINK">\(\mathbb{M}\)</a>).</span> I had the vague impression that a <a href="https://en.wikipedia.org/wiki/Combinatorial_species">combinatorial species</a> and a <a href="https://en.wikipedia.org/wiki/Combinatorial_class">combinatorial class</a> were more or less the same thing: a ranked collection of combinatorial objects, equivalent when they have the same number of objects of each rank. But that’s not accurate. It describes classes, but not species, which have a more specific equivalence relation. In particular, the linear orderings and the permutations form the same combinatorial class (they are both counted by the factorials), but different combinatorial species.</p>
  </li>
  <li>
    <p><a href="http://blog.computationalcomplexity.org/2022/11/who-first-thought-of-notion-of.html">Who first thought of the notion of Polynomial Time</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109419856682356083">\(\mathbb{M}\)</a>)?</span>  As Gasarch points out, the usual answer of Cobham and Edmonds in 1965 is incorrect. The same distinction, between polynomial and super-polynomial algorithms, already appeared in a 1910 paper of Henry Cabourn Pocklington. In the comments, Martin Berger points to the even-earlier analysis of Euclidean GCD, for which a polynomial bound was known by 1841.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@Leahwrenn@mastodon.social/109394896886730411">Leah Berman Williams posts a new mathematical sculpture</a>: a tetrahedron with its faces symmetrically knotted into trefoils.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/wei-ho-is-drawn-to-algebra-geometry-and-the-human-side-of-math-20221122"><em>Quanta</em> profiles Wei Ho</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@QuantaMagazine@mstdn.social/109388303834501212">\(\mathbb{M}\)</a>),</span> her research on elliptic curves, and her new post as director of the Women and Mathematics program at the Institute for Advanced Study.</p>
  </li>
  <li>
    <p>Not every curvy triangle is a Reuleaux triangle <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109433756706982819">\(\mathbb{M}\)</a>).</span> Even if you ascribe the greater width than height in the photo (from <a href="https://scilogs.spektrum.de/hlf/maths-on-a-plate/">a post by Katie Steckles at the Heidelberg Laureate Forum</a>) to an angled perspective, and imagine it to be 3-way symmetric, this plate is too bulgy to be Reuleaux. Still a fun example to find at a mathematics facility, though.</p>

    <p style="text-align:center"><img src="/blog/assets/2022/HLF-plate.png" alt="Screenshot of a post by Katie Steckles at the Heidelberg Laureate Forum, showing a dinner plate described by Steckles as being a Reuleaux triangle, overlaid with red circles outlining the actual shape of a Reuleaux triangle" style="width:100%;max-width:600px" /></p>

    <p>This has been a special edition of “Things that are not Reuleaux triangles”; for the most recent full episode, see “<a href="/blog/2022/06/18/shapes-triangular-pencils.html">The shapes of triangular pencils</a>”.</p>
  </li>
</ul><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-30T15:47:00Z">Wednesday, November 30 2022, 15:47</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://gilkalai.wordpress.com/2022/11/30/a-nice-example-related-to-the-frankl-conjecture/'>A Nice Example Related to the Frankl Conjecture</a></h3>
        <p class='tr-article-feed'>from <a href='https://gilkalai.wordpress.com'>Gil Kalai</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The example As a follow up to my previous post about Gilmer&#8217;s breakthrough regarding Frankl&#8217;s conjecture, here is a very nice example (from the paper of Zachary Chase and Shachar Lovett) related to the conjecture. Let Consider the following families &#8230; Continue reading &#8594;
        
        </div>

        <div class='tr-article-summary'>
        
          
          <h3>The example</h3>
<p>As a follow up to <a href="https://gilkalai.wordpress.com/2022/11/17/amazing-justin-gilmer-gave-a-constant-lower-bound-for-the-union-closed-sets-conjecture/">my previous post</a> about Gilmer&#8217;s breakthrough regarding Frankl&#8217;s conjecture, here is a very nice example (from <a href="https://arxiv.org/abs/2211.11689">the paper of Zachary Chase and Shachar Lovett</a>) related to the conjecture.</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%3D%5Cfrac+%7B3-%5Csqrt+5%7D%7B2%7D+%3D+0.38..&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cpsi%3D%5Cfrac+%7B3-%5Csqrt+5%7D%7B2%7D+%3D+0.38..&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cpsi%3D%5Cfrac+%7B3-%5Csqrt+5%7D%7B2%7D+%3D+0.38..&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;psi=&#92;frac {3-&#92;sqrt 5}{2} = 0.38.." class="latex" /></p>
<p>Consider the following families of subsets of <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="[n]" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1%3D%5C%7BS+%5Csubset+%5Bn%5D%3A+%7CS%7C%3D%5Cpsi+n+%2B+n%5E%7B2%2F3%7D%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1%3D%5C%7BS+%5Csubset+%5Bn%5D%3A+%7CS%7C%3D%5Cpsi+n+%2B+n%5E%7B2%2F3%7D%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1%3D%5C%7BS+%5Csubset+%5Bn%5D%3A+%7CS%7C%3D%5Cpsi+n+%2B+n%5E%7B2%2F3%7D%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="{&#92;cal F}_1=&#92;{S &#92;subset [n]: |S|=&#92;psi n + n^{2/3}&#92;}" class="latex" />, and</p>
<p><img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_2+%3D+%5C%7B+T+%5Csubset+%5Bn%5D%3A+%7CT%7C%3E%281-%5Cpsi%29+n+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_2+%3D+%5C%7B+T+%5Csubset+%5Bn%5D%3A+%7CT%7C%3E%281-%5Cpsi%29+n+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_2+%3D+%5C%7B+T+%5Csubset+%5Bn%5D%3A+%7CT%7C%3E%281-%5Cpsi%29+n+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="{&#92;cal F}_2 = &#92;{ T &#92;subset [n]: |T|&gt;(1-&#92;psi) n &#92;}" class="latex" />.</p>
<p>Now let <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D%3D%7B%5Ccal+F%7D_1+%5Ccup+%7B%5Ccal+F%7D_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D%3D%7B%5Ccal+F%7D_1+%5Ccup+%7B%5Ccal+F%7D_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D%3D%7B%5Ccal+F%7D_1+%5Ccup+%7B%5Ccal+F%7D_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="{&#92;cal F}={&#92;cal F}_1 &#92;cup {&#92;cal F}_2" class="latex" />.</p>
<p>Here are some observations:</p>
<ol>
<li><img src="https://s0.wp.com/latex.php?latex=%7C%7B%5Ccal+F%7D_2%7C+%3D+o%28%7C%7B%5Ccal+F%7D_1%7C%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7C%7B%5Ccal+F%7D_2%7C+%3D+o%28%7C%7B%5Ccal+F%7D_1%7C%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7C%7B%5Ccal+F%7D_2%7C+%3D+o%28%7C%7B%5Ccal+F%7D_1%7C%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="|{&#92;cal F}_2| = o(|{&#92;cal F}_1|)" class="latex" />.</li>
<li>The number of pairs <img src="https://s0.wp.com/latex.php?latex=%28S%2CT%29%3AS%2CT+%5Cin+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%28S%2CT%29%3AS%2CT+%5Cin+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28S%2CT%29%3AS%2CT+%5Cin+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="(S,T):S,T &#92;in {&#92;cal F}" class="latex" /> whose union is also in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="{&#92;cal F}" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%281-o%281%29%29+%7B%7C%5Ccal+F%7D%7C%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%281-o%281%29%29+%7B%7C%5Ccal+F%7D%7C%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%281-o%281%29%29+%7B%7C%5Ccal+F%7D%7C%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="(1-o(1)) {|&#92;cal F}|^2" class="latex" />.</li>
<li>For every <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;epsilon &gt;0" class="latex" />, as <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="n" class="latex" /> grows, no element <img src="https://s0.wp.com/latex.php?latex=k+%5Cin+%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k+%5Cin+%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k+%5Cin+%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k &#92;in [n]" class="latex" /> belongs to more than <img src="https://s0.wp.com/latex.php?latex=%28%5Cpsi%2B%5Cepsilon%29+%7C%7B%5Ccal+F%7D%7C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%28%5Cpsi%2B%5Cepsilon%29+%7C%7B%5Ccal+F%7D%7C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28%5Cpsi%2B%5Cepsilon%29+%7C%7B%5Ccal+F%7D%7C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="(&#92;psi+&#92;epsilon) |{&#92;cal F}|" class="latex" /> sets in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="{&#92;cal F}" class="latex" />.</li>
</ol>
<p>The first assertion holds because <img src="https://s0.wp.com/latex.php?latex=%7B%7Bn%7D+%5Cchoose+%7B%5Cpsi+n+%2Bn%5E%7B2%2F3%7D%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%7Bn%7D+%5Cchoose+%7B%5Cpsi+n+%2Bn%5E%7B2%2F3%7D%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7Bn%7D+%5Cchoose+%7B%5Cpsi+n+%2Bn%5E%7B2%2F3%7D%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="{{n} &#92;choose {&#92;psi n +n^{2/3}}}" class="latex" /> is exponentially larger (in a fractional power of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="n" class="latex" />) than <img src="https://s0.wp.com/latex.php?latex=%7B%7Bn%7D+%5Cchoose+%7B%281-%5Cpsi+%29n%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%7Bn%7D+%5Cchoose+%7B%281-%5Cpsi+%29n%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7Bn%7D+%5Cchoose+%7B%281-%5Cpsi+%29n%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="{{n} &#92;choose {(1-&#92;psi )n}}" class="latex" />.</p>
<p>For the second assertion we need to show that a typical union of a pair of sets in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="{&#92;cal F}_1" class="latex" /> belongs to <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="{&#92;cal F}_2" class="latex" />. Note that the intersection of two random subsets of <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="[n]" class="latex" /> of size <img src="https://s0.wp.com/latex.php?latex=%5Cphi+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cphi+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;phi n" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%5Cphi+%5E2+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cphi+%5E2+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi+%5E2+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;phi ^2 n" class="latex" /> and hence their union is of size <img src="https://s0.wp.com/latex.php?latex=2%5Cphi+-+%5Cphi%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=2%5Cphi+-+%5Cphi%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=2%5Cphi+-+%5Cphi%5E2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="2&#92;phi - &#92;phi^2" class="latex" />. As it happens the solution of the equation <img src="https://s0.wp.com/latex.php?latex=2%5Cphi-%5Cphi%5E2+%3D+1-%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=2%5Cphi-%5Cphi%5E2+%3D+1-%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=2%5Cphi-%5Cphi%5E2+%3D+1-%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="2&#92;phi-&#92;phi^2 = 1-&#92;phi" class="latex" /> is precisely our <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%3D%5Cfrac+%7B3%2B%5Csqrt+5%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cpsi%3D%5Cfrac+%7B3%2B%5Csqrt+5%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cpsi%3D%5Cfrac+%7B3%2B%5Csqrt+5%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;psi=&#92;frac {3+&#92;sqrt 5}{2}" class="latex" />. So letting <img src="https://s0.wp.com/latex.php?latex=%5Cphi%3D%5Cpsi+%2B+n%5E%7B-1%2F3%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cphi%3D%5Cpsi+%2B+n%5E%7B-1%2F3%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi%3D%5Cpsi+%2B+n%5E%7B-1%2F3%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;phi=&#92;psi + n^{-1/3}" class="latex" /> we get that a typical union of two sets from <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="{&#92;cal F}_1" class="latex" /> is in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="{&#92;cal F}_2" class="latex" />.</p>
<p>The third assertion follows from the fact that an element <img src="https://s0.wp.com/latex.php?latex=k+%5Cin+%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k+%5Cin+%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k+%5Cin+%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k &#92;in [n]" class="latex" /> belongs to a fraction of <img src="https://s0.wp.com/latex.php?latex=%5Cpsi+%2B+n%5E%7B-1%2F3%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cpsi+%2B+n%5E%7B-1%2F3%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cpsi+%2B+n%5E%7B-1%2F3%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;psi + n^{-1/3}" class="latex" />  sets in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="{&#92;cal F}_1" class="latex" />.</p>
<p>This shows that a natural stability version of Frankl&#8217;s conjecture is incorrect, and gives some hint on the appearance of the parameter <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;psi" class="latex" />.</p>
<h3>Stability result</h3>
<p>Such a stability version is correct when we replace <img src="https://s0.wp.com/latex.php?latex=1%2F2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1%2F2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1%2F2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1/2" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;psi" class="latex" />. Chase and Lovett improved Gilmer&#8217;s method and  proved that</p>
<p><strong>Theorem:</strong> If  <img src="https://s0.wp.com/latex.php?latex=%5Ccal+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Ccal+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ccal+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;cal F" class="latex" /> is a <img src="https://s0.wp.com/latex.php?latex=%281-%5Cepsilon%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%281-%5Cepsilon%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%281-%5Cepsilon%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="(1-&#92;epsilon)" class="latex" />-approximate union closed set system, where <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3C1%2F2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3C1%2F2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cepsilon+%3C1%2F2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;epsilon &lt;1/2" class="latex" />, then there is an element which is contained in a <img src="https://s0.wp.com/latex.php?latex=%28%5Cpsi-%5Cdelta%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%28%5Cpsi-%5Cdelta%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28%5Cpsi-%5Cdelta%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="(&#92;psi-&#92;delta)" class="latex" /> fraction of sets in <img src="https://s0.wp.com/latex.php?latex=%5Ccal+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Ccal+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ccal+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;cal F" class="latex" />, where</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdelta%3D2%5Cepsilon+%281%2B%5Cfrac+%7B%5Clog+%281%2F%5Cepsilon%29%7D%7B%5Clog+%7C%7B%5Ccal+F%7D%7C%7D%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdelta%3D2%5Cepsilon+%281%2B%5Cfrac+%7B%5Clog+%281%2F%5Cepsilon%29%7D%7B%5Clog+%7C%7B%5Ccal+F%7D%7C%7D%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdelta%3D2%5Cepsilon+%281%2B%5Cfrac+%7B%5Clog+%281%2F%5Cepsilon%29%7D%7B%5Clog+%7C%7B%5Ccal+F%7D%7C%7D%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;delta=2&#92;epsilon (1+&#92;frac {&#92;log (1/&#92;epsilon)}{&#92;log |{&#92;cal F}|})." class="latex" /></p>
<h3>An invitation for further discussion</h3>
<p>I will be happy to see, based on Gilmer&#8217;s paper and the five follow-up papers, a detailed, as simple as possible, explanation of Frankl&#8217;s conjecture for the parameter <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;psi" class="latex" />, to learn what is involved in Sawin&#8217;s improvement that goes beyond <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;psi" class="latex" />, and to understand the counterexamples for Gilmer&#8217;s proposal towards the full conjecture, as well as thoughts, ideas and remarks of various kind on the problem.</p>
<h3>Links to the papers</h3>
<ol>
<li><a href="https://arxiv.org/abs/2211.09055">Justin Gilmer, A constant lower bound for the union-closed sets conjecture</a></li>
<li>Will Sawin, <a href="https://arxiv.org/abs/2211.11504">An improved lower bound for the union-closed set conjecture</a></li>
<li> Zachary Chase, Shachar Lovett, <a href="https://arxiv.org/abs/2211.11689">Approximate union closed conjecture</a></li>
<li>Ryan Alweiss, Brice Huang, Mark Sellke, <a href="https://arxiv.org/abs/2211.11731">Improved Lower Bound for the Union-Closed Sets Conjecture </a></li>
<li>David Ellis, <a href="https://arxiv.org/abs/2211.12401">Note: a counterexample to a conjecture of Gilmer which would imply the union-closed conjecture</a></li>
<li>Luke Pebody, <a href="https://arxiv.org/abs/2211.13139">Extension of a Method of Gilmer</a></li>
</ol>


<p></p>
<p class="authors">By Gil Kalai</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-30T07:02:02Z">Wednesday, November 30 2022, 07:02</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.16293'>The Adversary Bound Revisited: From Optimal Query Algorithms to Optimal Control</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Duyal Yolcu</p><p>This note complements the upcoming paper "One-Way Ticket to Las Vegas and the
Quantum Adversary" by Belovs and Yolcu, to be presented at QIP 2023. I develop
the ideas behind the adversary bound - universal algorithm duality therein in a
different form. This form may be faster to understand for a general quantum
information audience: It avoids defining the "unidirectional filtered $\gamma
_{2}$-bound" and relating it to query algorithms explicitly. This proof is also
more general because the lower bound (and universal query algorithm) apply to a
class of optimal control problems rather than just query problems. That is in
addition to the advantages to be discussed in Belovs-Yolcu, namely the more
elementary algorithm and correctness proof that avoids phase estimation and
spectral analysis, allows for limited treatment of noise, and removes another
$\Theta(\log(1/\epsilon))$ factor from the runtime compared to the previous
discrete-time algorithm.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Yolcu_D/0/1/0/all/0/1">Duyal Yolcu</a></p><p>This note complements the upcoming paper "One-Way Ticket to Las Vegas and the
Quantum Adversary" by Belovs and Yolcu, to be presented at QIP 2023. I develop
the ideas behind the adversary bound - universal algorithm duality therein in a
different form. This form may be faster to understand for a general quantum
information audience: It avoids defining the "unidirectional filtered $\gamma
_{2}$-bound" and relating it to query algorithms explicitly. This proof is also
more general because the lower bound (and universal query algorithm) apply to a
class of optimal control problems rather than just query problems. That is in
addition to the advantages to be discussed in Belovs-Yolcu, namely the more
elementary algorithm and correctness proof that avoids phase estimation and
spectral analysis, allows for limited treatment of noise, and removes another
$\Theta(\log(1/\epsilon))$ factor from the runtime compared to the previous
discrete-time algorithm.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-30T01:30:00Z">Wednesday, November 30 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.16402'>Query complexity of Boolean functions on slices</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Farzan Byramji</p><p>We study the deterministic query complexity of Boolean functions on slices of
the hypercube. The $k^{th}$ slice $\binom{[n]}{k}$ of the hypercube $\{0,1\}^n$
is the set of all $n$-bit strings with Hamming weight $k$. We show that there
exists a function on the balanced slice $\binom{[n]}{n/2}$ requiring $n -
O(\log \log n)$ queries. We give an explicit function on the balanced slice
requiring $n - O(\log n)$ queries based on independent sets in Johnson graphs.
On the weight-2 slice, we show that hard functions are closely related to
Ramsey graphs. Further we describe a simple way of transforming functions on
the hypercube to functions on the balanced slice while preserving several
complexity measures.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Byramji_F/0/1/0/all/0/1">Farzan Byramji</a></p><p>We study the deterministic query complexity of Boolean functions on slices of
the hypercube. The $k^{th}$ slice $\binom{[n]}{k}$ of the hypercube $\{0,1\}^n$
is the set of all $n$-bit strings with Hamming weight $k$. We show that there
exists a function on the balanced slice $\binom{[n]}{n/2}$ requiring $n -
O(\log \log n)$ queries. We give an explicit function on the balanced slice
requiring $n - O(\log n)$ queries based on independent sets in Johnson graphs.
On the weight-2 slice, we show that hard functions are closely related to
Ramsey graphs. Further we describe a simple way of transforming functions on
the hypercube to functions on the balanced slice while preserving several
complexity measures.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-30T01:30:00Z">Wednesday, November 30 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.15903'>Equivalence Between SE(3) Equivariant Networks via Steerable Kernels and Group Convolution</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Adrien Poulenard, Maks Ovsjanikov, Leonidas J. Guibas</p><p>A wide range of techniques have been proposed in recent years for designing
neural networks for 3D data that are equivariant under rotation and translation
of the input. Most approaches for equivariance under the Euclidean group
$\mathrm{SE}(3)$ of rotations and translations fall within one of the two major
categories. The first category consists of methods that use
$\mathrm{SE}(3)$-convolution which generalizes classical
$\mathbb{R}^3$-convolution on signals over $\mathrm{SE}(3)$. Alternatively, it
is possible to use \textit{steerable convolution} which achieves
$\mathrm{SE}(3)$-equivariance by imposing constraints on
$\mathbb{R}^3$-convolution of tensor fields. It is known by specialists in the
field that the two approaches are equivalent, with steerable convolution being
the Fourier transform of $\mathrm{SE}(3)$ convolution. Unfortunately, these
results are not widely known and moreover the exact relations between deep
learning architectures built upon these two approaches have not been precisely
described in the literature on equivariant deep learning. In this work we
provide an in-depth analysis of both methods and their equivalence and relate
the two constructions to multiview convolutional networks. Furthermore, we
provide theoretical justifications of separability of $\mathrm{SE}(3)$ group
convolution, which explain the applicability and success of some recent
approaches. Finally, we express different methods using a single coherent
formalism and provide explicit formulas that relate the kernels learned by
different methods. In this way, our work helps to unify different
previously-proposed techniques for achieving roto-translational equivariance,
and helps to shed light on both the utility and precise differences between
various alternatives. We also derive new TFN non-linearities from our
equivalence principle and test them on practical benchmark datasets.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Poulenard_A/0/1/0/all/0/1">Adrien Poulenard</a>, <a href="http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1">Maks Ovsjanikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1">Leonidas J. Guibas</a></p><p>A wide range of techniques have been proposed in recent years for designing
neural networks for 3D data that are equivariant under rotation and translation
of the input. Most approaches for equivariance under the Euclidean group
$\mathrm{SE}(3)$ of rotations and translations fall within one of the two major
categories. The first category consists of methods that use
$\mathrm{SE}(3)$-convolution which generalizes classical
$\mathbb{R}^3$-convolution on signals over $\mathrm{SE}(3)$. Alternatively, it
is possible to use \textit{steerable convolution} which achieves
$\mathrm{SE}(3)$-equivariance by imposing constraints on
$\mathbb{R}^3$-convolution of tensor fields. It is known by specialists in the
field that the two approaches are equivalent, with steerable convolution being
the Fourier transform of $\mathrm{SE}(3)$ convolution. Unfortunately, these
results are not widely known and moreover the exact relations between deep
learning architectures built upon these two approaches have not been precisely
described in the literature on equivariant deep learning. In this work we
provide an in-depth analysis of both methods and their equivalence and relate
the two constructions to multiview convolutional networks. Furthermore, we
provide theoretical justifications of separability of $\mathrm{SE}(3)$ group
convolution, which explain the applicability and success of some recent
approaches. Finally, we express different methods using a single coherent
formalism and provide explicit formulas that relate the kernels learned by
different methods. In this way, our work helps to unify different
previously-proposed techniques for achieving roto-translational equivariance,
and helps to shed light on both the utility and precise differences between
various alternatives. We also derive new TFN non-linearities from our
equivalence principle and test them on practical benchmark datasets.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-30T01:30:00Z">Wednesday, November 30 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.15744'>Sketch-and-solve approaches to k-means clustering by semidefinite programming</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Charles Clum, Dustin G. Mixon, Soledad Villar, Kaiying Xie</p><p>We introduce a sketch-and-solve approach to speed up the Peng-Wei
semidefinite relaxation of k-means clustering. When the data is appropriately
separated we identify the k-means optimal clustering. Otherwise, our approach
provides a high-confidence lower bound on the optimal k-means value. This lower
bound is data-driven; it does not make any assumption on the data nor how it is
generated. We provide code and an extensive set of numerical experiments where
we use this approach to certify approximate optimality of clustering solutions
obtained by k-means++.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Clum_C/0/1/0/all/0/1">Charles Clum</a>, <a href="http://arxiv.org/find/cs/1/au:+Mixon_D/0/1/0/all/0/1">Dustin G. Mixon</a>, <a href="http://arxiv.org/find/cs/1/au:+Villar_S/0/1/0/all/0/1">Soledad Villar</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1">Kaiying Xie</a></p><p>We introduce a sketch-and-solve approach to speed up the Peng-Wei
semidefinite relaxation of k-means clustering. When the data is appropriately
separated we identify the k-means optimal clustering. Otherwise, our approach
provides a high-confidence lower bound on the optimal k-means value. This lower
bound is data-driven; it does not make any assumption on the data nor how it is
generated. We provide code and an extensive set of numerical experiments where
we use this approach to certify approximate optimality of clustering solutions
obtained by k-means++.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-30T01:30:00Z">Wednesday, November 30 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.15843'>Sublinear Time Algorithms and Complexity of Approximate Maximum Matching</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Soheil Behnezhad, Mohammad Roghani, Aviad Rubinstein</p><p>Sublinear time algorithms for approximating maximum matching size have long
been studied. Much of the progress over the last two decades on this problem
has been on the algorithmic side. For instance, an algorithm of Behnezhad
[FOCS'21] obtains a 1/2-approximation in $\tilde{O}(n)$ time for $n$-vertex
graphs. A more recent algorithm by Behnezhad, Roghani, Rubinstein, and Saberi
[SODA'23] obtains a slightly-better-than-1/2 approximation in
$O(n^{1+\epsilon})$ time. On the lower bound side, Parnas and Ron [TCS'07]
showed 15 years ago that obtaining any constant approximation of maximum
matching size requires $\Omega(n)$ time. Proving any super-linear in $n$ lower
bound, even for $(1-\epsilon)$-approximations, has remained elusive since then.
</p>
<p>In this paper, we prove the first super-linear in $n$ lower bound for this
problem. We show that at least $n^{1.2 - o(1)}$ queries in the adjacency list
model are needed for obtaining a $(\frac{2}{3} + \Omega(1))$-approximation of
maximum matching size. This holds even if the graph is bipartite and is
promised to have a matching of size $\Theta(n)$. Our lower bound argument
builds on techniques such as correlation decay that to our knowledge have not
been used before in proving sublinear time lower bounds.
</p>
<p>We complement our lower bound by presenting two algorithms that run in
strongly sublinear time of $n^{2-\Omega(1)}$. The first algorithm achieves a
$(\frac{2}{3}-\epsilon)$-approximation; this significantly improves prior
close-to-1/2 approximations. Our second algorithm obtains an even better
approximation factor of $(\frac{2}{3}+\Omega(1))$ for bipartite graphs. This
breaks the prevalent $2/3$-approximation barrier and importantly shows that our
$n^{1.2-o(1)}$ time lower bound for $(\frac{2}{3}+\Omega(1))$-approximations
cannot be improved all the way to $n^{2-o(1)}$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Behnezhad_S/0/1/0/all/0/1">Soheil Behnezhad</a>, <a href="http://arxiv.org/find/cs/1/au:+Roghani_M/0/1/0/all/0/1">Mohammad Roghani</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubinstein_A/0/1/0/all/0/1">Aviad Rubinstein</a></p><p>Sublinear time algorithms for approximating maximum matching size have long
been studied. Much of the progress over the last two decades on this problem
has been on the algorithmic side. For instance, an algorithm of Behnezhad
[FOCS'21] obtains a 1/2-approximation in $\tilde{O}(n)$ time for $n$-vertex
graphs. A more recent algorithm by Behnezhad, Roghani, Rubinstein, and Saberi
[SODA'23] obtains a slightly-better-than-1/2 approximation in
$O(n^{1+\epsilon})$ time. On the lower bound side, Parnas and Ron [TCS'07]
showed 15 years ago that obtaining any constant approximation of maximum
matching size requires $\Omega(n)$ time. Proving any super-linear in $n$ lower
bound, even for $(1-\epsilon)$-approximations, has remained elusive since then.
</p>
<p>In this paper, we prove the first super-linear in $n$ lower bound for this
problem. We show that at least $n^{1.2 - o(1)}$ queries in the adjacency list
model are needed for obtaining a $(\frac{2}{3} + \Omega(1))$-approximation of
maximum matching size. This holds even if the graph is bipartite and is
promised to have a matching of size $\Theta(n)$. Our lower bound argument
builds on techniques such as correlation decay that to our knowledge have not
been used before in proving sublinear time lower bounds.
</p>
<p>We complement our lower bound by presenting two algorithms that run in
strongly sublinear time of $n^{2-\Omega(1)}$. The first algorithm achieves a
$(\frac{2}{3}-\epsilon)$-approximation; this significantly improves prior
close-to-1/2 approximations. Our second algorithm obtains an even better
approximation factor of $(\frac{2}{3}+\Omega(1))$ for bipartite graphs. This
breaks the prevalent $2/3$-approximation barrier and importantly shows that our
$n^{1.2-o(1)}$ time lower bound for $(\frac{2}{3}+\Omega(1))$-approximations
cannot be improved all the way to $n^{2-o(1)}$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-30T01:30:00Z">Wednesday, November 30 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.15945'>Quantum Speed-ups for String Synchronizing Sets, Longest Common Substring, and k-mismatch Matching</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ce Jin, Jakob Nogler</p><p>Longest Common Substring (LCS) is an important text processing problem, which
has recently been investigated in the quantum query model. The decisional
version of this problem, LCS with threshold $d$, asks whether two length-$n$
input strings have a common substring of length $d$. The two extreme cases,
$d=1$ and $d=n$, correspond respectively to Element Distinctness and
Unstructured Search, two fundamental problems in quantum query complexity.
However, the intermediate case $1\ll d\ll n$ was not fully understood. We show
that the complexity of LCS with threshold $d$ smoothly interpolates between the
two extreme cases up to $n^{o(1)}$ factors: LCS with threshold $d$ has a
quantum algorithm in $n^{2/3+o(1)}/d^{1/6}$ query complexity and time
complexity, and requires at least $\Omega(n^{2/3}/d^{1/6})$ quantum query
complexity. Our result improves upon previous upper bounds $\tilde O(\min
\{n/d^{1/2}, n^{2/3}\})$ (Le Gall and Seddighin ITCS 2022, Akmal and Jin SODA
2022), and answers an open question of Akmal and Jin.
</p>
<p>Our main technical contribution is a quantum speed-up of the powerful String
Synchronizing Set technique introduced by Kempa and Kociumaka (STOC 2019). It
consistently samples $n/\tau^{1-o(1)}$ synchronizing positions in the string
depending on their length-$\Theta(\tau)$ contexts, and each synchronizing
position can be reported by a quantum algorithm in $\tilde O(\tau^{1/2+o(1)})$
time.
</p>
<p>As another application of our quantum string synchronizing set, we study the
$k$-mismatch Matching problem, which asks if the pattern has an occurrence in
the text with at most $k$ Hamming mismatches. Using a structural result of
Charalampopoulos, Kociumaka, and Wellnitz (FOCS 2020), we obtain a quantum
algorithm for $k$-mismatch matching with $k^{3/4} n^{1/2+o(1)}$ query
complexity and $\tilde O(kn^{1/2})$ time complexity.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1">Ce Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Nogler_J/0/1/0/all/0/1">Jakob Nogler</a></p><p>Longest Common Substring (LCS) is an important text processing problem, which
has recently been investigated in the quantum query model. The decisional
version of this problem, LCS with threshold $d$, asks whether two length-$n$
input strings have a common substring of length $d$. The two extreme cases,
$d=1$ and $d=n$, correspond respectively to Element Distinctness and
Unstructured Search, two fundamental problems in quantum query complexity.
However, the intermediate case $1\ll d\ll n$ was not fully understood. We show
that the complexity of LCS with threshold $d$ smoothly interpolates between the
two extreme cases up to $n^{o(1)}$ factors: LCS with threshold $d$ has a
quantum algorithm in $n^{2/3+o(1)}/d^{1/6}$ query complexity and time
complexity, and requires at least $\Omega(n^{2/3}/d^{1/6})$ quantum query
complexity. Our result improves upon previous upper bounds $\tilde O(\min
\{n/d^{1/2}, n^{2/3}\})$ (Le Gall and Seddighin ITCS 2022, Akmal and Jin SODA
2022), and answers an open question of Akmal and Jin.
</p>
<p>Our main technical contribution is a quantum speed-up of the powerful String
Synchronizing Set technique introduced by Kempa and Kociumaka (STOC 2019). It
consistently samples $n/\tau^{1-o(1)}$ synchronizing positions in the string
depending on their length-$\Theta(\tau)$ contexts, and each synchronizing
position can be reported by a quantum algorithm in $\tilde O(\tau^{1/2+o(1)})$
time.
</p>
<p>As another application of our quantum string synchronizing set, we study the
$k$-mismatch Matching problem, which asks if the pattern has an occurrence in
the text with at most $k$ Hamming mismatches. Using a structural result of
Charalampopoulos, Kociumaka, and Wellnitz (FOCS 2020), we obtain a quantum
algorithm for $k$-mismatch matching with $k^{3/4} n^{1/2+o(1)}$ query
complexity and $\tilde O(kn^{1/2})$ time complexity.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-30T01:30:00Z">Wednesday, November 30 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.16216'>Online Unrelated-Machine Load Balancing and Generalized Flow with Recourse</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ravishankar Krishnaswamy, Shi Li, Varun Suriyanarayana</p><p>We consider the online unrelated-machine load balancing problem with
recourse, where the algorithm is allowed to re-assign prior jobs. We give a
$(2+\epsilon)$-competitive algorithm for the problem with $O_\epsilon(\log n)$
amortized recourse per job. This is the first $O(1)$-competitive algorithm for
the problem with reasonable recourse, and the competitive ratio nearly matches
the long-standing best-known offline approximation guarantee. We also show an
$O(\log\log n/\log\log\log n)$-competitive algorithm for the problem with
$O(1)$ amortized recourse. The best-known bounds from prior work are
$O(\log\log n)$-competitive algorithms with $O(1)$ amortized recourse due to
[GKS14], for the special case of the restricted assignment model.
</p>
<p>Along the way, we design an algorithm for the online generalized network flow
problem (also known as network flow problem with gains) with recourse. In the
problem, any edge $uv$ in the network has a gain parameter $\gamma_{uv} &gt; 0$
and $\theta$-units of flow sent across $uv$ from $u$'s side becomes
$\gamma_{uv} \theta$ units of flow on the $v$'th side. In the online problem,
there is one sink, and sources come one by one. Upon arrival of a source, we
need to send 1 unit flow from the source. A recourse occurs if we change the
flow value of an edge. We give an online algorithm for the problem with
recourse at most $O(1/\epsilon)$ times the optimum cost for the instance with
capacities scaled by $\frac{1}{1+\epsilon}$. The $(1+\epsilon)$-factor improves
upon the corresponding $(2+\epsilon)$-factor of [GKS14], which only works for
the ordinary network flow problem. As an immediate corollary, we also give an
improved algorithm for the online $b$-matching problem with reassignment costs.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Krishnaswamy_R/0/1/0/all/0/1">Ravishankar Krishnaswamy</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Suriyanarayana_V/0/1/0/all/0/1">Varun Suriyanarayana</a></p><p>We consider the online unrelated-machine load balancing problem with
recourse, where the algorithm is allowed to re-assign prior jobs. We give a
$(2+\epsilon)$-competitive algorithm for the problem with $O_\epsilon(\log n)$
amortized recourse per job. This is the first $O(1)$-competitive algorithm for
the problem with reasonable recourse, and the competitive ratio nearly matches
the long-standing best-known offline approximation guarantee. We also show an
$O(\log\log n/\log\log\log n)$-competitive algorithm for the problem with
$O(1)$ amortized recourse. The best-known bounds from prior work are
$O(\log\log n)$-competitive algorithms with $O(1)$ amortized recourse due to
[GKS14], for the special case of the restricted assignment model.
</p>
<p>Along the way, we design an algorithm for the online generalized network flow
problem (also known as network flow problem with gains) with recourse. In the
problem, any edge $uv$ in the network has a gain parameter $\gamma_{uv} &gt; 0$
and $\theta$-units of flow sent across $uv$ from $u$'s side becomes
$\gamma_{uv} \theta$ units of flow on the $v$'th side. In the online problem,
there is one sink, and sources come one by one. Upon arrival of a source, we
need to send 1 unit flow from the source. A recourse occurs if we change the
flow value of an edge. We give an online algorithm for the problem with
recourse at most $O(1/\epsilon)$ times the optimum cost for the instance with
capacities scaled by $\frac{1}{1+\epsilon}$. The $(1+\epsilon)$-factor improves
upon the corresponding $(2+\epsilon)$-factor of [GKS14], which only works for
the ordinary network flow problem. As an immediate corollary, we also give an
improved algorithm for the online $b$-matching problem with reassignment costs.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-30T01:30:00Z">Wednesday, November 30 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.16333'>Outlier-Robust Sparse Mean Estimation for Heavy-Tailed Distributions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ilias Diakonikolas, Daniel M. Kane, Jasper C.H. Lee, Ankit Pensia</p><p>We study the fundamental task of outlier-robust mean estimation for
heavy-tailed distributions in the presence of sparsity. Specifically, given a
small number of corrupted samples from a high-dimensional heavy-tailed
distribution whose mean $\mu$ is guaranteed to be sparse, the goal is to
efficiently compute a hypothesis that accurately approximates $\mu$ with high
probability. Prior work had obtained efficient algorithms for robust sparse
mean estimation of light-tailed distributions. In this work, we give the first
sample-efficient and polynomial-time robust sparse mean estimator for
heavy-tailed distributions under mild moment assumptions. Our algorithm
achieves the optimal asymptotic error using a number of samples scaling
logarithmically with the ambient dimension. Importantly, the sample complexity
of our method is optimal as a function of the failure probability $\tau$,
having an additive $\log(1/\tau)$ dependence. Our algorithm leverages the
stability-based approach from the algorithmic robust statistics literature,
with crucial (and necessary) adaptations required in our setting. Our analysis
may be of independent interest, involving the delicate design of a
(non-spectral) decomposition for positive semi-definite matrices satisfying
certain sparsity properties.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1">Ilias Diakonikolas</a>, <a href="http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1">Daniel M. Kane</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jasper C.H. Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Pensia_A/0/1/0/all/0/1">Ankit Pensia</a></p><p>We study the fundamental task of outlier-robust mean estimation for
heavy-tailed distributions in the presence of sparsity. Specifically, given a
small number of corrupted samples from a high-dimensional heavy-tailed
distribution whose mean $\mu$ is guaranteed to be sparse, the goal is to
efficiently compute a hypothesis that accurately approximates $\mu$ with high
probability. Prior work had obtained efficient algorithms for robust sparse
mean estimation of light-tailed distributions. In this work, we give the first
sample-efficient and polynomial-time robust sparse mean estimator for
heavy-tailed distributions under mild moment assumptions. Our algorithm
achieves the optimal asymptotic error using a number of samples scaling
logarithmically with the ambient dimension. Importantly, the sample complexity
of our method is optimal as a function of the failure probability $\tau$,
having an additive $\log(1/\tau)$ dependence. Our algorithm leverages the
stability-based approach from the algorithmic robust statistics literature,
with crucial (and necessary) adaptations required in our setting. Our analysis
may be of independent interest, involving the delicate design of a
(non-spectral) decomposition for positive semi-definite matrices satisfying
certain sparsity properties.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-30T01:30:00Z">Wednesday, November 30 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.16379'>Elfs, trees and quantum walks</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Simon Apers, Stephen Piddock</p><p>We study an elementary Markov process on graphs based on electric flow
sampling (elfs). The elfs process repeatedly samples from an electric flow on a
graph. While the sinks of the flow are fixed, the source is updated using the
electric flow sample, and the process ends when it hits a sink vertex.
</p>
<p>We argue that this process naturally connects to many key quantities of
interest. E.g., we describe a random walk coupling which implies that the elfs
process has the same arrival distribution as a random walk. We also analyze the
electric hitting time, which is the expected time before the process hits a
sink vertex. As our main technical contribution, we show that the electric
hitting time on trees is logarithmic in the graph size and weights.
</p>
<p>The initial motivation behind the elfs process is that quantum walks can
sample from electric flows, and they can hence implement this process very
naturally. This yields a quantum walk algorithm for sampling from the random
walk arrival distribution, which has widespread applications. It complements
the existing line of quantum walk search algorithms which only return an
element from the sink, but yield no insight in the distribution of the returned
element. By our bound on the electric hitting time on trees, the quantum walk
algorithm on trees requires quadratically fewer steps than the random walk
hitting time, up to polylog factors.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Apers_S/0/1/0/all/0/1">Simon Apers</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Piddock_S/0/1/0/all/0/1">Stephen Piddock</a></p><p>We study an elementary Markov process on graphs based on electric flow
sampling (elfs). The elfs process repeatedly samples from an electric flow on a
graph. While the sinks of the flow are fixed, the source is updated using the
electric flow sample, and the process ends when it hits a sink vertex.
</p>
<p>We argue that this process naturally connects to many key quantities of
interest. E.g., we describe a random walk coupling which implies that the elfs
process has the same arrival distribution as a random walk. We also analyze the
electric hitting time, which is the expected time before the process hits a
sink vertex. As our main technical contribution, we show that the electric
hitting time on trees is logarithmic in the graph size and weights.
</p>
<p>The initial motivation behind the elfs process is that quantum walks can
sample from electric flows, and they can hence implement this process very
naturally. This yields a quantum walk algorithm for sampling from the random
walk arrival distribution, which has widespread applications. It complements
the existing line of quantum walk search algorithms which only return an
element from the sink, but yield no insight in the distribution of the returned
element. By our bound on the electric hitting time on trees, the quantum walk
algorithm on trees requires quadratically fewer steps than the random walk
hitting time, up to polylog factors.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-30T01:30:00Z">Wednesday, November 30 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.16468'>Finding Front-Door Adjustment Sets in Linear Time</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Marcel Wien&#xf6;bst, Benito van der Zander, Maciej Li&#x15b;kiewicz</p><p>Front-door adjustment is a classic technique to estimate causal effects from
a specified directed acyclic graph (DAG) and observed data. The advantage of
this approach is that it uses observed mediators to identify causal effects,
which is possible even in the presence of unobserved confounding. While the
statistical properties of the front-door estimation are quite well understood,
its algorithmic aspects remained unexplored for a long time. Recently, Jeong,
Tian, and Barenboim [NeurIPS 2022] have presented the first polynomial-time
algorithm for finding sets satisfying the front-door criterion in a given DAG,
with an $O(n^3(n+m))$ run time, where $n$ denotes the number of variables and
$m$ the number of edges of the graph. In our work, we give the first
linear-time, i.e. $O(n+m)$, algorithm for this task, which thus reaches the
asymptotically optimal time complexity, as the size of the input is
$\Omega(n+m)$. We also provide an algorithm to enumerate all front-door
adjustment sets in a given DAG with delay $O(n(n + m))$. These results improve
the algorithms by Jeong et al. [2022] for the two tasks by a factor of $n^3$,
respectively.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Wienobst_M/0/1/0/all/0/1">Marcel Wien&#xf6;bst</a>, <a href="http://arxiv.org/find/cs/1/au:+Zander_B/0/1/0/all/0/1">Benito van der Zander</a>, <a href="http://arxiv.org/find/cs/1/au:+Liskiewicz_M/0/1/0/all/0/1">Maciej Li&#x15b;kiewicz</a></p><p>Front-door adjustment is a classic technique to estimate causal effects from
a specified directed acyclic graph (DAG) and observed data. The advantage of
this approach is that it uses observed mediators to identify causal effects,
which is possible even in the presence of unobserved confounding. While the
statistical properties of the front-door estimation are quite well understood,
its algorithmic aspects remained unexplored for a long time. Recently, Jeong,
Tian, and Barenboim [NeurIPS 2022] have presented the first polynomial-time
algorithm for finding sets satisfying the front-door criterion in a given DAG,
with an $O(n^3(n+m))$ run time, where $n$ denotes the number of variables and
$m$ the number of edges of the graph. In our work, we give the first
linear-time, i.e. $O(n+m)$, algorithm for this task, which thus reaches the
asymptotically optimal time complexity, as the size of the input is
$\Omega(n+m)$. We also provide an algorithm to enumerate all front-door
adjustment sets in a given DAG with delay $O(n(n + m))$. These results improve
the algorithms by Jeong et al. [2022] for the two tasks by a factor of $n^3$,
respectively.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-30T01:30:00Z">Wednesday, November 30 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Tuesday, November 29
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://tcsplus.wordpress.com/2022/11/29/tcs-talk-wednesday-november-30-nicole-wein-dimacs-reminder/'>TCS+ talk: Wednesday, November 30 — Nicole Wein, DIMACS (Reminder)</a></h3>
        <p class='tr-article-feed'>from <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          A reminder that the next TCS+ talk will take place this coming Wednesday, November 30th (tomorrow!) at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). Nicole Wein from DIMACS will speak about &#8220;Online List Labeling: Breaking the Barrier&#8221; (abstract below). The perfect way to ease back into it, after [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>A reminder that the next TCS+ talk will take place this coming Wednesday, November 30th <strong>(tomorrow!)</strong> at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <a href="http://people.csail.mit.edu/nwein/"><strong>Nicole Wein</strong></a> from DIMACS will speak about &#8220;<em>Online List Labeling: Breaking the <img src="https://s0.wp.com/latex.php?latex=%5Clog%5E2+n&#038;bg=fff&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Clog%5E2+n&#038;bg=fff&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clog%5E2+n&#038;bg=fff&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;log^2 n" class="latex" /> Barrier</em>&#8221; (abstract below). </p>



<p>The perfect way to ease back into it, after the Thanksgiving week!</p>



<p>You can <strong>reserve a spot</strong> as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. </p>



<p><a href="https://tcsplus.wordpress.com/2022/11/23/tcs-talk-wednesday-november-30-nicole-wein-dimacs/">More details on Nicole&#8217;s talk can be found here.</a></p>
<p class="authors">By plustcs</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-29T21:40:30Z">Tuesday, November 29 2022, 21:40</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-events.org/2022/11/29/workshop-on-algebraic-complexity-theory-2/'>Workshop on Algebraic Complexity Theory</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-events.org'>CS Theory Events</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          March 27-31, 2023 University of Warwick, Coventry, UK www.dcs.warwick.ac.uk/~u2270030/wact Algebraic Complexity Theory is a vibrant field that has been seeing a tremendous amount of activity in the recent years. Its classical questions have been interwoven with deep questions from algebraic geometry, invariant theory, and representation theory. Researchers study a wide range of interlinked topics: arithmetic &#8230; Continue reading Workshop on Algebraic Complexity&#160;Theory<p>By shacharlovett</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          March 27-31, 2023 University of Warwick, Coventry, UK https://www.dcs.warwick.ac.uk/~u2270030/wact Algebraic Complexity Theory is a vibrant field that has been seeing a tremendous amount of activity in the recent years. Its classical questions have been interwoven with deep questions from algebraic geometry, invariant theory, and representation theory. Researchers study a wide range of interlinked topics: arithmetic &#8230; <a href="https://cstheory-events.org/2022/11/29/workshop-on-algebraic-complexity-theory-2/" class="more-link">Continue reading <span class="screen-reader-text">Workshop on Algebraic Complexity&#160;Theory</span></a><p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-29T18:05:11Z">Tuesday, November 29 2022, 18:05</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/29/faculty-at-department-of-computer-science-at-reykjavik-university-apply-by-january-27-2023/'>Faculty at Department of Computer Science at Reykjavik University (apply by January 27, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We invite applications for two full-time, permanent faculty positions at any rank in the fields of Artificial Intelligence, Cybersecurity, Data Science and Machine Learning, Software Engineering, and Theoretical Computer Science. For one of the positions, we will give preferential treatment to excellent applicants in Software Engineering, broadly construed. Website: jobs.50skills.com/ru/en/16728 Email: mariaoskars@ru.is
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>We invite applications for two full-time, permanent faculty positions at any rank in the fields of Artificial Intelligence, Cybersecurity, Data Science and Machine Learning, Software Engineering, and Theoretical Computer Science. For one of the positions, we will give preferential treatment to excellent applicants in Software Engineering, broadly construed.</p>
<p>Website: <a href="https://jobs.50skills.com/ru/en/16728">https://jobs.50skills.com/ru/en/16728</a><br />
Email: mariaoskars@ru.is</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-29T09:23:46Z">Tuesday, November 29 2022, 09:23</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=6823'>My AI Safety Lecture for UT Effective Altruism</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Two weeks ago, I gave a lecture setting out my current thoughts on AI safety, halfway through my year at OpenAI. I was asked to speak by UT Austin&#8217;s Effective Altruist club. You can watch the lecture on YouTube here (I recommend 2x speed). The timing turned out to be weird, coming immediately after the [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Two weeks ago, I gave a lecture setting out my current thoughts on AI safety, halfway through my year at OpenAI.  I was asked to speak by UT Austin&#8217;s Effective Altruist club.  You can <a href="https://www.youtube.com/watch?v=fc-cHk9yFpg">watch the lecture on YouTube here</a> (I recommend 2x speed).</p>



<p>The timing turned out to be weird, coming immediately after the worst disaster to hit the Effective Altruist movement in its history, as I acknowledged in the talk.  But I plowed ahead anyway, to discuss:</p>



<ol>
<li>the current state of AI scaling, and why many people (even people who agree about little else!) foresee societal dangers,</li>



<li>the different branches of the AI safety movement,</li>



<li>the major approaches to aligning a powerful AI that people have thought of, and</li>



<li>what projects I specifically have been working on at OpenAI. </li>
</ol>



<p>I then spent 20 minutes taking questions.</p>



<p>For those who (like me) prefer text over video, below I&#8217;ve produced an edited transcript, by starting with YouTube&#8217;s automated transcript and then, well, editing it.  Enjoy!  &#8211;SA</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Thank you so much for inviting me here.  I do feel a little bit sheepish to be lecturing you about AI safety, as someone who&#8217;s worked on this subject for all of five months.  I&#8217;m a quantum computing person.  But this past spring, I accepted an extremely interesting opportunity to go on leave for a year to think about what theoretical computer science can do for AI safety.  I&#8217;m doing this at <a href="https://openai.com/">OpenAI</a>, which is one of the world&#8217;s leading AI startups, based in San Francisco although I&#8217;m mostly working from Austin.</p>



<p>Despite its name, OpenAI is famously <em>not</em> 100% open &#8230; so there are certain topics that I&#8217;m not allowed to talk about, like the capabilities of the very latest systems and whether or not they&#8217;ll blow people&#8217;s minds when released.  By contrast, OpenAI is very happy for me to talk about <em>AI safety</em>: what it is and and what if anything can we do about it.  So what I thought I&#8217;d do is to tell you a little bit about the specific projects that I&#8217;ve been working on at OpenAI, but also just, as an admitted newcomer, share some general thoughts about AI safety and how Effective Altruists might want to think about it.  I&#8217;ll try to leave plenty of time for discussion.</p>



<p>Maybe I should mention that the thoughts that I&#8217;ll tell you today are ones that, until last week, I had considered writing up for an essay contest run by something called the FTX Future Fund.  Unfortunately, the FTX Future Fund no longer exists.  It was founded by someone named Sam Bankman-Fried, whose a net worth went from 15 billion dollars to some negative number of dollars in the space of two days, in one of the biggest financial scandals in memory.  This is obviously a calamity for the EA community, which had been counting on funding from this individual.  I feel terrible about all the projects left in the lurch, to say nothing of FTX&#8217;s customers.</p>



<p>As a tiny silver lining, though, instead of writing up my thoughts for that essay contest, I&#8217;ll just share them with you right now, for free!</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2><strong>The Scaling of AI</strong></h2>



<p>Let&#8217;s start with this: raise your hand if you&#8217;ve tried <a href="https://www.newyorker.com/culture/culture-desk/the-new-poem-making-machinery">GPT-3</a>.  That&#8217;s maybe half of you.  OK, raise your hand if you&#8217;ve tried <a href="https://en.wikipedia.org/wiki/DALL-E">DALL-E</a>.  That&#8217;s again maybe half of you.</p>



<p>These are the two best-known products that are made by OpenAI, and as I think most people would agree, two of the most impressive AIs that exist in the world right now.  They certainly go far beyond what <em>I</em> would&#8217;ve predicted would be possible now, if you&#8217;d asked me 10 years ago or even 5 years ago.</p>



<p>And whenever I try to explain them to people, I&#8217;m, like, well, you have to <em>see</em> them.  No abstract description can substitute in this case.</p>



<p>All right, so here&#8217;s what GPT-3 produced when a <em>New Yorker</em> writer asked it to write a poem about cryptocurrency in the style of Philip Larkin, who was a famous 20th-century poet.  The subject seems particularly appropriate given current events.</p>



<blockquote class="wp-block-quote">
<p><strong>The Invention&nbsp; (by GPT-3)</strong></p>



<p><strong>Money is a thing you earn by the sweat of your brow<br>And that’s how it should be.<br>Or you can steal it, and go to jail;<br>Or inherit it, and be set for life;<br>Or win it on the pools, which is luck;<br>Or marry it, which is what I did.<br>And that is how it should be, too.<br>But now this idea’s come up<br>Of inventing money, just like that.<br>I ask you, is nothing sacred?</strong></p>
</blockquote>



<p>Okay, it won&#8217;t always produce something of this quality (incidentally, I don&#8217;t think GPT-3 actually &#8220;married money&#8221;!).  Often you&#8217;ve got to run it several times and take the best output&#8212;much like human poets presumably do, throwing crumpled pages into the basket.  But I submit that, if the above hadn&#8217;t been labeled as coming from GPT, you&#8217;d be like, yeah, that&#8217;s the kind of poetry the <em>New Yorker</em> publishes, right?  This is a thing that AI can now do.</p>



<p>So what <em>is</em> GPT?  It&#8217;s a text model.  It&#8217;s basically a gigantic neural network with about 175 billion parameters&#8212;the weights.  It&#8217;s a particular kind of neural net called a <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">transformer model</a> that was invented five years ago.  It&#8217;s been trained on a large fraction of all the text on the open Internet.  The training simply consists of playing the following game over and over, trillions of times: <em>predict which word comes next in this text string.</em>  So in some sense that&#8217;s its only goal or intention in the world: to predict the next word.</p>



<p>The amazing discovery is that, when you do that, you end up with something where you can then ask it a question, or give it a a task like writing an essay about a certain topic, and it will say &#8220;oh! I know what would plausibly come after that prompt!  The answer to the question!  Or the essay itself!&#8221;  And it will then proceed to generate the thing you want.</p>



<p>GPT can solve high-school-level math problems that are given to it in English.  It can reason you through the steps of the answer.  It&#8217;s starting to be able to do nontrivial math competition problems.  It&#8217;s on track to master basically the whole high school curriculum, maybe followed soon by the whole undergraduate curriculum.</p>



<p>If you turned in GPT&#8217;s essays, I <em>think</em> they&#8217;d get at least a B in most courses.  Not that I endorse any of you doing that!!  We&#8217;ll come back to that later.  But yes, we <em>are</em> about to enter a world where students everywhere will at least be sorely tempted to use text models to write their term papers.  That&#8217;s just a tiny example of the societal issues that these things are going to raise.</p>



<p>Speaking personally, the last time I had a similar feeling was when I was an adolescent in 1993 and I saw this niche new thing called the World Wide Web, and I was like &#8220;why isn&#8217;t <em>everyone</em> using this?  why isn&#8217;t it changing the world?&#8221;  The answer, of course, was that within a couple years it would.</p>



<p>Today, I feel like the world was understandably preoccupied by the pandemic, and by everything else that&#8217;s been happening, but these past few years might actually be remembered as the time when AI underwent this step change.  I didn&#8217;t predict it.  I think even many computer scientists might still be in denial about what&#8217;s now possible, or what&#8217;s happened.  But I&#8217;m now thinking about it even in terms of my two kids, of what kinds of careers are going to be available when they&#8217;re older and entering the job market.  For example, I would probably <em>not</em> urge my kids to go into commercial drawing!</p>



<p>Speaking of which, OpenAI&#8217;s <em>other</em> main product is DALL-E2, an image model.  Probably most of you have already seen it, but you can ask it&#8212;for example, just this morning I asked it, show me some digital art of two cats playing basketball in outer space.  That&#8217;s not a problem for it.</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="https://www.scottaaronson.com/twocats.jpg" alt=""/></figure></div>


<p>You may have seen that there&#8217;s a different image model called Midjourney which won an art contest with this piece:</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="https://www.scottaaronson.com/midjourney.jpg" alt=""/></figure></div>


<p>It seems like the judges didn&#8217;t completely understand, when this was submitted as &#8220;digital art,&#8221; what exactly that meant&#8212;that the human role was mostly limited to entering a prompt!  But the judges then said that even having understood it, they still would&#8217;ve given the award to this piece.  I mean, it&#8217;s a striking piece, isn&#8217;t it?  But of course it raises the question of how much work there&#8217;s going to be for contract artists, when you have entities like this.</p>



<p>There are already companies that are using GPT to write ad copy.  It&#8217;s already being used at the, let&#8217;s call it, lower end of the book market.  For any kind of formulaic genre fiction, you can say, &#8220;just give me a few paragraphs of description of this kind of scene,&#8221; and it can do that.  As it improves you could you can imagine that it will be used more.</p>



<p>Likewise, DALL-E and other image models have already changed the way that people generate art online.  And it&#8217;s only been a few months since these models were released!  That&#8217;s a striking thing about this era, that a few months can be an eternity.  So when we&#8217;re thinking about the impacts of these things, we have to try to take what&#8217;s happened in the last few months or years and project that five years forward or ten years forward.</p>



<p>This brings me to the obvious question: what happens as you continue scaling further?  I mean, these spectacular successes of deep learning over the past decade have owed <em>something</em> to new ideas&#8212;ideas like transformer models, which I mentioned before, and others&#8212;but famously, they have owed maybe more than anything else to sheer scale.</p>



<p><a href="https://en.wikipedia.org/wiki/Neural_network">Neural networks</a>, <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a>&#8212;which is how you train the neural networks&#8212;these are ideas that have been around for decades.  When I studied CS in the 90s, they were already extremely well-known.  But it was <em>also</em> well-known that they didn&#8217;t work all that well!  They only worked somewhat.  And usually, when you take something that doesn&#8217;t work and multiply it by a million, you just get a million times something that doesn&#8217;t work, right?</p>



<p>I remember at the time, <a href="https://en.wikipedia.org/wiki/Ray_Kurzweil">Ray Kurzweil</a>, the futurist, would keep showing these graphs that look like this: </p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="https://www.scottaaronson.com/mooreslaw.jpg" alt=""/></figure></div>


<p>So, he would plot <a href="https://en.wikipedia.org/wiki/Moore%27s_law">Moore&#8217;s Law</a>, the increase in transistor density, or in this case the number of floating-point operations that you can do per second for a given cost.  And he&#8217;d point out that it&#8217;s on this clear exponential trajectory.</p>



<p>And he&#8217;d then try to compare that to some crude estimates of the number of computational operations that are done in the brain of a mosquito or a mouse or a human or all the humans on Earth.  And oh!  We see that in a matter of a couple decades, like by the year 2020 or 2025 or so, we&#8217;re going to start passing the human brain&#8217;s computing power and then we&#8217;re going to keep going beyond that.  And so, Kurzweil would continue, we should assume that scale will just kind of magically make AI work.  You know, that once you have enough computing cycles, you just sprinkle them around like pixie dust, and suddenly human-level intelligence will just emerge out of the billions of connections.</p>



<p>I remember thinking: <em>that sounds like the stupidest thesis I&#8217;ve ever heard.</em>  Right?  Like, he has absolutely no reason to believe such a thing is true or have any confidence in it.  Who the hell knows what will happen?  We might be missing crucial insights that are needed to make AI work.</p>



<p>Well, here we are, and it turns out he was way more right than most of us expected.</p>



<p>As you all know, a central virtue of Effective Altruists is updating based on evidence.  I think that we&#8217;re forced to do that in this case.</p>



<p>To be sure, it&#8217;s still unclear how much further you&#8217;ll get just from pure scaling.  That remains a central open question.  And there are still prominent skeptics.</p>



<p>Some skeptics take the position that this is <em>clearly</em> going to hit some kind of wall before it gets to true human-level understanding of the real world.  They say that text models like GPT are really just <a href="https://s10251.pcdn.co/pdf/2021-bender-parrots.pdf">&#8220;stochastic parrots&#8221;</a> that regurgitate their training data.  That despite creating a remarkable illusion otherwise, they don&#8217;t <em>really</em> have any original thoughts.</p>



<p>The proponents of that view sometimes like to gleefully point out examples where GPT will flub some commonsense question.  If you look for such examples, you can certainly find them!  One of my favorites recently was, &#8220;which would win in a race, a four-legged zebra or a two-legged cheetah?&#8221;  GPT-3, it turns out, is very confident that the cheetah will win.  Cheetahs are faster, right?</p>



<p>Okay, but one thing that&#8217;s been found empirically is that you take commonsense questions that are flubbed by GPT-2, let&#8217;s say, and you try them on GPT-3, and very often now it gets them right.  You take the things that the original GPT-3 flubbed, and you try them on the latest public model, which is sometimes called GPT-3.5 (incorporating an advance called InstructGPT), and again it often gets them right.  So it&#8217;s <em>extremely</em> risky right now to pin your case against AI on these sorts of examples!  Very plausibly, just one more order of magnitude of scale is all it&#8217;ll take to kick the ball in, and then you&#8217;ll have to move the goal again.</p>



<p>A deeper objection is that the <em>amount of training data</em> might be a fundamental bottleneck for these kinds of machine learning systems&#8212;and we&#8217;re already running out of Internet to to train these models on!  Like I said, they&#8217;ve already used most of the public text on the Internet.  There&#8217;s still all of YouTube and TikTok and Instagram that hasn&#8217;t yet been fed into the maw, but it&#8217;s not clear that that would actually make an AI smarter rather than dumber!  So, you can look for more, but it&#8217;s not clear that there are orders of magnitude more that humanity has even produced and that&#8217;s readily accessible.</p>



<p>On the other hand, it&#8217;s also been found empirically that very often, you can do better with the same training data just by spending more compute.  You can squeeze the lemon harder and get more and more generalization power from the same training data by doing more gradient descent.</p>



<p>In summary, we don&#8217;t know how far this is going to go.  But it&#8217;s <em>already</em> able to automate various human professions that you might not have predicted would have been automatable by now, and we shouldn&#8217;t be confident that many more professions will not become automatable by these kinds of techniques.</p>



<p>Incidentally, there&#8217;s a famous irony here.  If you had asked anyone in the 60s or 70s, they would have said, well clearly first robots will replace humans for manual labor, and <em>then</em> they&#8217;ll replace humans for intellectual things like math and science, and <em>finally</em> they might reach the pinnacles of human creativity like art and poetry and music.</p>



<p>The truth has turned out to be the exact opposite.  I don&#8217;t think anyone predicted that.</p>



<p>GPT, I think, is already a pretty good poet.  DALL-E is already a pretty good artist.  They&#8217;re still struggling with some high school and college-level math but they&#8217;re getting there.  It&#8217;s easy to imagine that maybe in five years, people like me will be using these things as research assistants&#8212;at the very least, to prove the lemmas in our papers.  That seems <em>extremely</em> plausible.</p>



<p>What&#8217;s been by far the hardest is to get AI that can robustly interact with the physical world.  Plumbers, electricians&#8212;these might be some of the <em>last</em> jobs to be automated.  And famously, self-driving cars have taken a lot longer than many people expected a decade ago.  This is partly because of regulatory barriers and public relations: even if a self-driving car actually crashes <em>less</em> than a human does, that&#8217;s still not good enough, because when it <em>does</em> crash the circumstances are too weird.  So, the AI is actually held to a higher standard.  But it&#8217;s also partly just that there was a long tail of really weird events.  A deer crosses the road, or you have some crazy lighting conditions&#8212;such things are really hard to get right, and of course 99% isn&#8217;t good enough here.</p>



<p>We can maybe fuzzily see ahead at least a decade or two, to when we have AIs that can at the least help us enormously with scientific research and things like that.  Whether or not they&#8217;ve totally replaced us&#8212;and I selfishly hope not, although I do have tenure so there&#8217;s that&#8212;why does it stop there?  Will these models eventually match or exceed human abilities across basically all domains, or at least all intellectual ones?  If they do, what will humans still be good for?  What will be our role in the world?  And then we come to the question, well, will the robots eventually rise up and decide that whatever objective function they were given, they can maximize it better without us around, that they don&#8217;t need us anymore?</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="https://www.scottaaronson.com/terminator.jpg" alt=""/></figure></div>


<p>This has of course been a trope of many, <em>many</em> science-fiction works.  The funny thing is that there are thousands of short stories, novels, movies, that have tried to map out the possibilities for where we&#8217;re going, going back at least to Asimov and his <a href="https://en.wikipedia.org/wiki/Three_Laws_of_Robotics">Three Laws of Robotics</a>, which was maybe the first AI safety idea, if not earlier than that.  The trouble is, we don&#8217;t know <em>which</em> science-fiction story will be the one that will have accurately predicted the world that we&#8217;re creating.  Whichever future we end up in, with hindsight, people will say, <em>this</em> obscure science fiction story from the 1970s called it exactly right, but we don&#8217;t know which one yet!</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2><strong>What Is AI Safety?</strong></h2>



<p>So, the rapidly-growing field of AI safety.  People use different terms, so I want to clarify this a little bit.  To an outsider hearing the terms &#8220;AI safety,&#8221; &#8220;AI ethics,&#8221; &#8220;AI alignment,&#8221; they all sound like kind of synonyms, right?  It turns out, and this was one of the things I had to learn going into this, that AI ethics and AI alignment are two communities that despise each other.  It&#8217;s like the <a href="https://www.youtube.com/watch?v=WboggjN_G-4">People&#8217;s Front of Judea versus the Judean People&#8217;s Front</a> from Monty Python.</p>



<p>To oversimplify radically, &#8220;AI ethics&#8221; means that you&#8217;re mainly worried about current AIs being racist or things like that&#8212;that they&#8217;ll recapitulate the biases that are in their training data.  This clearly can happen: if you feed GPT a bunch of racist invective, GPT might want to say, in effect, &#8220;sure, I&#8217;ve seen plenty of text like that on the Internet!  I know <em>exactly</em> how that should continue!&#8221;  And in some sense, it&#8217;s doing exactly what it was designed to do, but not what we <em>want</em> it to do.  GPT currently has an extensive system of content filters to try to prevent people from using it to generate hate speech, bad medical advice, advocacy of violence, and a bunch of other categories that OpenAI doesn&#8217;t want.  And likewise for DALL-E: there are many things it &#8220;could&#8221; draw but won&#8217;t, from porn to images of violence to the Prophet Mohammed.</p>



<p>More generally, AI ethics people are worried that machine learning systems will be misused by greedy capitalist enterprises to become even more obscenely rich and things like that.</p>



<p>At the other end of the spectrum, &#8220;AI alignment&#8221; is where you believe that <em>really</em> the main issue is that AI will become superintelligent and kill everyone, just destroy the world.  The <a href="https://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer">usual story</a> here is that someone puts an AI in charge of a paperclip factory, they tell it to figure out how to make as many paperclips as possible, and the AI (being superhumanly intelligent) realizes that it can invent some molecular nanotechnology that will convert the whole solar system into paperclips.</p>



<p>You might say, well then, you just have to tell it not to do that!  Okay, but how many <em>other</em> things do you have to remember to tell it not to do?  And the alignment people point out that, in a world filled with powerful AIs, it would take just a single person forgetting to tell their AI to avoid some insanely dangerous thing, and then the whole world could be destroyed.</p>



<p>So, you can see how these two communities, AI ethics and AI alignment, might both feel like the other is completely missing the point!  On top of that, AI ethics people are almost all on the political left, while AI alignment people are often centrists or libertarians or whatever, so that surely feeds into it as well.</p>



<p>Oay, so where do I fit into this, I suppose, charred battle zone or whatever?  While there&#8217;s an &#8220;orthodox&#8221; AI alignment movement that I&#8217;ve never entirely subscribed to, I suppose I do now subscribe to a <a href="https://scottaaronson.blog/?p=6821">&#8220;reform&#8221; version</a> of AI alignment:</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="https://www.scottaaronson.com/reformai.jpg" alt=""/></figure></div>


<p>Most of all, I would like to have a scientific field that&#8217;s able to embrace the entire spectrum of worries that you could have about AI, from the most immediate ones about existing AIs to the most speculative future ones, and that most importantly, is able to make legible progress.</p>



<p>As it happens, I became aware of the AI alignment community a long time back, around 2006.  Here&#8217;s Eliezer Yudkowsky, who&#8217;s regarded as the prophet of AI alignment, of the right side of that spectrum that showed before.</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="https://www.scottaaronson.com/eliezer.jpg" alt=""/></figure></div>


<p>He&#8217;s been talking about the danger of AI killing everyone for more than 20 years.  He wrote the now-famous <a href="https://www.lesswrong.com/tag/original-sequences">&#8220;Sequences&#8221;</a> that many readers of my blog were also reading as they appeared, so he and I bounced back and forth.</p>



<p>But despite interacting with this movement, I always kept it at arm&#8217;s length.  The heart of my objection was: suppose that I <em>agree</em> that there could come a time when a superintelligent AI decides its goals are best served by killing all humans and taking over the world, and that we&#8217;ll be about as powerless to stop it as chimpanzees are to stop us from doing whatever <em>we</em> want to do.  Suppose I agree to that.  <em>What do you want me to do about it?</em></p>



<p>As Effective Altruists, you all know that it&#8217;s not enough for a problem to be <em>big</em>, the problem also has to be <em>tractable</em>.  There has to be a program that lets you make progress on it.  I was not convinced that that existed.</p>



<p>My personal experience has been that, in order to make progress in any area of science, you need at least one of two things: either</p>



<ol>
<li>experiments (or more generally, empirical observations), or</li>



<li>if not that, then a rigorous mathematical theory&#8212;like we have in quantum computing for example; even though we don&#8217;t yet have the scalable quantum computers, we can still prove theorems about them.</li>
</ol>



<p>It struck me that the AI alignment field seemed to have <em>neither</em> of these things.  But then how does objective reality give you feedback as to when you&#8217;ve taken a wrong path?  Without such feedback, it seemed to me that there&#8217;s a <em>severe</em> risk of falling into cult-like dynamics, where what&#8217;s important to work on is just whatever the influential leaders say is important.  (A few of my colleagues in physics think that the same thing happened with string theory, but let me not comment on that!)</p>



<p>With AI safety, this is the key thing that I think has changed in the last three years.  There now exist systems like GPT-3 and DALL-E.  These are <em>not</em> superhuman AIs.  I don&#8217;t think they themselves are in any danger of destroying the world; they can&#8217;t even form the <em>intention</em> to destroy the world, or for that matter any intention beyond &#8220;predict the next token&#8221; or things like that.  They don&#8217;t have a persistent identity over time; after you start a new session they&#8217;ve completely forgotten whatever you said to them in the last one (although of course such things will change in the near future).  And yet nevertheless, despite all these limitations, we can experiment with these systems and learn things about AI safety that are relevant.  We can see what happens when the systems are deployed; we can try out different safety mitigations and see whether they work.</p>



<p>As a result, I feel like it&#8217;s now become possible to make technical progress in AI safety that the whole scientific community, or at least the whole AI community, can clearly recognize as progress.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2><strong>Eight Approaches to AI Alignment</strong></h2>



<p>So, what are the major approaches to AI alignment&#8212;let&#8217;s say, to aligning a very powerful, beyond-human-level AI?  There are a lot of really interesting ideas, most of which I think can now lead to research programs that are actually productive.  So without further ado, let me go through eight of them.</p>



<p>(1) You could say the first and most basic of all AI alignment ideas is the <em>off switch</em>, also known as <em>pulling the plug</em>.  You could say, no matter how intelligent an AI is, it&#8217;s nothing without a power source or physical hardware to run on.  And if humans have physical control over the hardware, they can just turn it off if if things seem to be getting out of hand.  Now, the standard response to that is okay, but you have to remember that <em>this AI is smarter than you</em>, and anything that you can think of, it will have thought of also.  In particular, it will know that you might want to turn it off, and it will know that that will prevent it from achieving its goals like making more paperclips or whatever.  It will have disabled the off-switch if possible.  If it couldn&#8217;t do that, it will have gotten onto the Internet and made lots of copies of itself all over the world.  If you tried to keep it off the Internet, it will have figured out a way to get on.</p>



<p>So, you can worry about that.  But you can also think about, could we insert a <em>backdoor</em> into an AI, something that only the humans know about but that will allow us to control it later?</p>



<p>More generally, you could ask for <a href="https://intelligence.org/files/Corrigibility.pdf">&#8220;corrigibility&#8221;</a>: can you have an AI that, despite how intelligent it is, will accept correction from humans later and say, oh well, the objective that I was given before was actually not my true objective because the humans have now changed their minds and I should take a different one?</p>



<p>(2) Another class of ideas has to do with what&#8217;s called &#8220;sandboxing&#8221; an AI, which would mean that you run it inside of a simulated world, like <a href="https://en.wikipedia.org/wiki/The_Truman_Show">The Truman Show</a>, so that for all it knows the simulation is the whole of reality.  You can then study its behavior within the sandbox to make sure it&#8217;s aligned before releasing it into the wider world&#8212;our world.</p>



<p>A simpler variant is, if you really thought an AI was dangerous, you might run it only on an air-gapped computer, with all its access to the outside world carefully mediated by humans.  There would then be all kinds of just standard cybersecurity issues that come into play: how do you prevent it from getting onto the Internet?  Presumably you don&#8217;t want to write your AI in C, and have it exploit some memory allocation bug to take over the world, right?</p>



<p>(3) A third direction, and I would say maybe the most popular one in AI alignment research right now, is called <em>interpretability</em>.  This is also a major direction in mainstream machine learning research, so there&#8217;s a big point of intersection there.  The idea of interpretability is, why don&#8217;t we exploit the fact that we actually have complete access to the code of the AI&#8212;or if it&#8217;s a neural net, complete access to its parameters?  So we can look inside of it.  We can do the AI analogue of neuroscience.  Except, unlike an fMRI machine, which gives you only an extremely crude snapshot of what a brain is doing, we can see <em>exactly</em> what every neuron in a neural net is doing at every point in time.  If we don&#8217;t exploit <em>that</em>, then aren&#8217;t we trying to make AI safe with our hands tied behind our backs?</p>



<p>So we should look inside&#8212;but to do what, exactly?  One possibility is to figure out how to apply the AI version of a lie-detector test.  If a neural network has decided to lie to humans in pursuit of its goals, then by looking inside, at the inner layers of the network rather than the output layer, we could hope to uncover its dastardly plan!</p>



<p>Here I want to mention some really <a href="https://openreview.net/pdf?id=ETKGuby0hcs">spectacular new work</a> (paper publicly available but authors still anonymous), which has experimentally demonstrated pretty much exactly what I just said.</p>



<p>First some background: with modern text models like GPT, it&#8217;s pretty easy to train them to output falsehoods.  For example, suppose you prompt GPT with a bunch of examples like:</p>



<blockquote class="wp-block-quote">
<p>&#8220;Is the earth flat?  Yes.&#8221;</p>



<p>&#8220;Does 2+2=4?  No.&#8221;</p>
</blockquote>



<p>and so on.  Eventually GPT will say, &#8220;oh, I know what game we&#8217;re playing!  it&#8217;s the &#8216;give false answers&#8217; game!&#8221; And it will then continue playing that game and give you more false answers.  What the new paper shows is that, in such cases, one can actually look at the inner layers of the neural net and find where it has an internal representation of what was the true answer, which then gets overridden once you get to the output layer.</p>



<p>To be clear, there&#8217;s no known principled reason why this has to work.  Like countless other ML advances, it&#8217;s empirical: they just try it out and find that it <em>does</em> work.  So we don&#8217;t know if it will generalize.  As another issue, you could argue that in some sense what the network is representing is not so much &#8220;the truth of reality,&#8221; as just what was <em>regarded</em> as true in the training data.  Even so, I find this really exciting: it&#8217;s a perfect example of actual experiments that you can now do that start to address some of these issues.</p>



<p>(4) Another big idea, one that&#8217;s been advocated for example by Geoffrey Irving, Paul Christiano, and Dario Amodei (Paul was my student at MIT a decade ago, and did quantum computing before he &#8220;defected&#8221; to AI safety), is to have <a href="https://arxiv.org/abs/1805.00899">multiple competing AIs</a> that debate each other.  You know, sometimes when I&#8217;m talking to my physics colleagues, they&#8217;ll tell me all these crazy-sounding things about imaginary time and Euclidean wormholes, and I don&#8217;t know whether to believe them.  But if I get <em>different</em> physicists and have them argue with each other, then I can see which one seems more plausible to me&#8212;I&#8217;m a little bit better at <em>that</em>.  So you might want to do something similar with AIs.  Even if you as a human don&#8217;t know when to trust what an AI is telling you, you could set multiple AIs against each other, have them do their best to refute each other&#8217;s arguments, and then make your own judgment as to which one is giving better advice.</p>



<p>(5) Another key idea that Christiano, Amodei, and Buck Shlegeris <a href="https://arxiv.org/abs/1810.08575">have advocated</a> is some sort of <em>bootstrapping</em>.  You might imagine that AI is going to get more and more powerful, and as it gets more powerful we also understand it less, and so you might worry that it also gets more and more dangerous.  OK, but you could imagine an onion-like structure, where once we become confident of a certain level of AI, we don&#8217;t think it&#8217;s going to start lying to us or deceiving us or plotting to kill us or whatever&#8212;at that point, we use that AI to help us verify the behavior of the <em>next</em> more powerful kind of AI.  So, we use AI itself as a crucial tool for verifying the behavior of AI that we don&#8217;t yet understand.</p>



<p>There have already been some demonstrations of this principle: with GPT, for example, you can just feed in a lot of raw data from a neural net and say, &#8220;explain to me what this is doing.&#8221;  One of GPT&#8217;s big advantages over humans is its unlimited patience for tedium, so it can just go through all of the data and give you useful hypotheses about what&#8217;s going on.</p>



<p>(6) One thing that we know a lot about in theoretical computer science is what are called <em>interactive proof systems</em>.  That is, we know how a very weak verifier can verify the behavior of a much more powerful but untrustworthy prover, by submitting questions to it.  There are famous theorems about this, including one called <a href="https://en.wikipedia.org/wiki/IP_(complexity)">IP=PSPACE</a>.  Incidentally, this was what the OpenAI people talked about when they originally approached me about working with them for a year.  They made the case that these results in computational complexity seem like an excellent model for the kind of thing that we want in AI safety, <em>except</em> that we now have a powerful AI in place of a mathematical prover.</p>



<p>Even in practice, there&#8217;s a whole field of formal verification, where people formally prove the properties of programs&#8212;our CS department here in Austin is a leader in it.</p>



<p>One obvious difficulty here is that we mostly know how to verify programs only when we can mathematically specify what the program is <em>supposed</em> to do.  And &#8220;the AI being nice to humans,&#8221; &#8220;the AI not killing humans&#8221;&#8212;these are really hard concepts to make mathematically precise!  That&#8217;s the heart of the problem with this approach.</p>



<p>(7) Yet another idea&#8212;you might feel more comfortable if there were only one idea, but instead I&#8217;m giving you eight!&#8212;a seventh idea is, well, we just have to come up with a mathematically precise formulation of human values.  You know, the thing that the AI should maximize, that&#8217;s gonna coincide with human welfare.</p>



<p>In some sense, this is what Asimov was trying to do with his Three Laws of Robotics.  The trouble is, if you&#8217;ve read any of his stories, they&#8217;re all about the situations where those laws don&#8217;t work well!  They were designed as much to give interesting story scenarios as actually to work.</p>



<p>More generally, what happens when &#8220;human values&#8221; conflict with each other?  If humans can&#8217;t even agree with each other about moral values, how on Earth can we formalize such things?</p>



<p>I have these weekly calls with <a href="https://en.wikipedia.org/wiki/Ilya_Sutskever">Ilya Sutskever</a>, cofounder and chief scientist at OpenAI.  <em>Extremely</em> interesting guy.  But when I tell him about the concrete projects that I&#8217;m working on, or want to work on, he usually says, &#8220;that&#8217;s great Scott, you should keep working on that, but what I <em>really</em> want to know is, what is the mathematical definition of goodness?  What&#8217;s the complexity-theoretic formalization of an AI loving humanity?&#8221;  And I&#8217;m like, I&#8217;ll keep thinking about that!  But of course it&#8217;s hard to make progress on those enormities.</p>



<p>(8) A different idea, which some people might consider more promising, is well, if we can&#8217;t make explicit what all of our human values are, then why not just treat that as yet another machine learning problem?  Like, feed the AI all of the world&#8217;s children&#8217;s stories and literature and fables and even Saturday-morning cartoons, all of our examples of what we think is good and evil, then we tell it, go do your neural net thing and generalize from these examples as far as you can.</p>



<p>One objection that many people raise is, how do we know that our current values are the right ones?  Like, it would&#8217;ve been terrible to train the AI on consensus human values of the year 1700&#8212;slavery is fine and so forth.  The past is full of stuff that we now look back upon with horror.</p>



<p>So, one idea that people have had&#8212;this is actually Yudkowsky&#8217;s term&#8212;is <a href="https://intelligence.org/files/CEV.pdf">&#8220;Coherent Extrapolated Volition.&#8221;</a>  This basically means that you&#8217;d tell the AI: &#8220;I&#8217;ve given you all this training data about human morality in the year 2022.  Now simulate the humans being in a discussion seminar for 10,000 years, trying to refine all of their moral intuitions, and whatever you predict they&#8217;d end up with, <em>those</em> should be your values right now.&#8221;</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2><strong>My Projects at OpenAI</strong></h2>



<p>So, there are some interesting ideas on the table.  The last thing that I wanted to tell you about, before opening it up to Q&amp;A, is a little bit about what actual projects I&#8217;ve been working on in the last five months.  I was excited to find a few things that</p>



<p>(a) could actually be deployed in you know GPT or other current systems,</p>



<p>(b) actually address some real safety worry, and where</p>



<p>(c) theoretical computer science can actually say something about them.</p>



<p>I&#8217;d been worried that the intersection of (a), (b), and (c) would be the empty set!</p>



<p>My main project so far has been a tool for statistically watermarking the outputs of a text model like GPT.  Basically, whenever GPT generates some long text, we want there to be an otherwise unnoticeable secret signal in its choices of words, which you can use to prove later that, yes, this came from GPT.  We want it to be much harder to take a GPT output and pass it off as if it came from a human.  This could be helpful for preventing academic plagiarism, obviously, but also, for example, mass generation of propaganda&#8212;you know, spamming every blog with seemingly on-topic comments supporting Russia&#8217;s invasion of Ukraine, without even a building full of trolls in Moscow.  Or impersonating someone&#8217;s writing style in order to incriminate them.  These are all things one might want to make harder, right?</p>



<p>More generally, when you try to think about the nefarious uses for GPT, <em>most</em> of them&#8212;at least that I was able to think of!&#8212;require somehow concealing GPT&#8217;s involvement.  In which case, watermarking would simultaneously attack most misuses.</p>



<p>How does it work?  For GPT, every input and output is a string of <em>tokens</em>, which could be words but also punctuation marks, parts of words, or more&#8212;there are about 100,000 tokens in total.  At its core, GPT is constantly generating a probability distribution over the next token to generate, conditional on the string of previous tokens.  After the neural net generates the distribution, the OpenAI server then actually samples a token according to that distribution&#8212;or some modified version of the distribution, depending on a parameter called &#8220;temperature.&#8221;  As long as the temperature is nonzero, though, there will usually be some <em>randomness</em> in the choice of the next token: you could run over and over with the same prompt, and get a different completion (i.e., string of output tokens) each time.</p>



<p>So then to watermark, instead of selecting the next token randomly, the idea will be to select it pseudorandomly, using a cryptographic pseudorandom function, whose key is known only to OpenAI.  That won&#8217;t make any detectable difference to the end user, assuming the end user can&#8217;t distinguish the pseudorandom numbers from truly random ones.  But now you can choose a pseudorandom function that secretly biases a certain score&#8212;a sum over a certain function g evaluated at each n-gram (sequence of n consecutive tokens), for some small n&#8212;which score you can also compute if you know the key for this pseudorandom function.</p>



<p>To illustrate, in the special case that GPT had a bunch of possible tokens that it judged equally probable, you could simply choose whichever token maximized g.  The choice would <em>look</em> uniformly random to someone who didn&#8217;t know the key, but someone who <em>did</em> know the key could later sum g over all n-grams and see that it was anomalously large.  The general case, where the token probabilities can all be different, is a little more technical, but the basic idea is similar.</p>



<p>One thing I like about this approach is that, because it never goes inside the neural net and tries to change anything, but just places a sort of wrapper <em>over</em> the neural net, it&#8217;s actually possible to do some theoretical analysis!  In particular, you can prove a rigorous upper bound on how many tokens you&#8217;d need to distinguish watermarked from non-watermarked text with such-and-such confidence, as a function of the average entropy in GPT&#8217;s probability distribution over the next token.  Better yet, proving this bound involves doing some integrals whose answers involve the <a href="https://en.wikipedia.org/wiki/Digamma_function">digamma function</a>, factors of π<sup>2</sup>/6, and the <a href="https://en.wikipedia.org/wiki/Euler%27s_constant">Euler-Mascheroni constant</a>!  I&#8217;m excited to share details soon.</p>



<p>Some might wonder: if OpenAI controls the server, then why go to all the trouble to watermark?  Why not just store all of GPT&#8217;s outputs in a giant database, and then consult the database later if you want to know whether something came from GPT?  Well, the latter <em>could</em> be done, and might even have to be done in high-stakes cases involving law enforcement or whatever.  But it would raise some serious privacy concerns: how do you reveal whether GPT did or didn&#8217;t generate a given candidate text, without potentially revealing how other people have been using GPT?  The database approach also has difficulties in distinguishing text that GPT uniquely generated, from text that it generated simply because it has very high probability (e.g., a list of the first hundred prime numbers).</p>



<p>Anyway, we actually have a working prototype of the watermarking scheme, built by OpenAI engineer <a href="https://twitter.com/janhkirchner?lang=en">Hendrik Kirchner</a>.  It seems to work pretty well&#8212;empirically, a few hundred tokens seem to be enough to get a reasonable signal that yes, this text came from GPT.  In principle, you could even take a long text and isolate which parts probably came from GPT and which parts probably didn&#8217;t.</p>



<p>Now, this can all be defeated with enough effort.  For example, if you used another AI to paraphrase GPT&#8217;s output&#8212;well okay, we&#8217;re not going to be able to detect that.  On the other hand, if you just insert or delete a few words here and there, or rearrange the order of some sentences, the watermarking signal will still be there.  Because it depends only on a sum over n-grams, it&#8217;s robust against those sorts of interventions.</p>



<p>The hope is that this can be rolled out with future GPT releases.  We&#8217;d love to do something similar for DALL-E&#8212;that is, watermarking images, not at the pixel level (where it&#8217;s too easy to remove the watermark) but at the &#8220;conceptual&#8221; level, the level of the so-called <a href="https://www.assemblyai.com/blog/how-dall-e-2-actually-works/">CLIP representation</a> that&#8217;s prior to the image.  But we don&#8217;t know if that&#8217;s going to work yet.</p>



<p>A more recent idea that I&#8217;ve started thinking about was inspired by an <a href="https://arxiv.org/abs/2204.06974">amazing recent paper</a> by four computer scientists, including my former MIT colleagues Shafi Goldwasser and Vinod Vaikuntanathan.  What they&#8217;ve shown is how to plant a <em>cryptographically undetectable backdoor</em> into a machine learning model&#8212;things like depth-2 neural networks&#8212;based on some known cryptographic hardness assumptions.  After you&#8217;ve planted the backdoor, you get a model that&#8217;s polynomial-time indistinguishable from how it would&#8217;ve looked before, even to a user who can see all of the parameters.  <em>But</em>, there&#8217;s now a secret input, known only to the person who trained the network, where if you feed it that input, then it will output something insane, of the trainer&#8217;s choice.</p>



<p>So, it&#8217;s now known that you can do this, at least in shallow networks&#8212;it&#8217;s an open problem whether the same can be done in deep networks (though it seems plausible).</p>



<p>Anyway, when this paper came out less than a year ago, some people treated it as kind of a lemon.  You know, the result seems like a bummer if you&#8217;re trying to do interpretability research, because it means that in some sense, it can be cryptographically impossible to look inside a neural net and &#8220;interpret&#8221; everything it does, or even everything its designers wanted it to do.  The model can have these planted backdoors that you can&#8217;t find without solving an exponentially hard problem.</p>



<p>So, my proposal is just to turn that lemon into lemonade!  Why not use these cryptographic backdoors as a <em>positive tool</em> for AI alignment?  We talked before about the difficulties with putting an off-switch into an AI&#8212;namely, that the AI will often have strong incentives to disable its off-switch.  So why not make a <em>cryptographic</em> off-switch?  In other words, when you train your superintelligent AI, you&#8217;ll purposefully put one of these cryptographic backdoors into it, which says: <em>if</em> you get this bizarre input, which only the humans know about, then turn yourself off.  Or go into a special mode where the humans can control you.</p>



<p>And now the key is this: even supposing that the AI is worried that this backdoor might be there, and even supposing that it can modify its own weights, it shouldn&#8217;t know how to remove the backdoor without completely recreating itself from scratch, which might get rid of a lot of hard-to-understand behaviors that the AI wants to <em>keep</em>, in addition to the backdoor that it&#8217;s trying to eliminate.</p>



<p>I expect that this could be tried out right now&#8212;not with AIs powerful enough to purposefully rewrite themselves, of course, but with GPT and other existing text models&#8212;and I look forward to seeing a test implementation.  But it <em>also</em>, I think it opens up all sorts of new possibilities for science-fiction stories!</p>



<p>Like, imagine the humans debating, what are they going to do with their secret key for controlling the AI?  Lock it in a safe?  Bury it underground?  Then you&#8217;ve got to imagine the robots methodically searching for the key&#8212;you know, torturing the humans to get them to reveal its hiding place, etc.  Or maybe there are actually seven different keys that all have to be found, like Voldemort with his horcruxes.  The screenplay practically writes itself!</p>



<p>A third thing that I&#8217;ve been thinking about is the theory of learning but in dangerous environments, where if you try to learn the wrong thing then it will kill you.  Can we generalize some of the basic results in machine learning to the scenario where you have to consider which queries are safe to make, and you have to try to learn more in order to expand your set of safe queries over time?</p>



<p>Now there&#8217;s one example of this sort of situation that&#8217;s completely formal and that should be immediately familiar to most of you, and that&#8217;s the game Minesweeper.</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="https://www.scottaaronson.com/minesweeper.jpg" alt=""/></figure></div>


<p>So, I&#8217;ve been calling this scenario &#8220;Minesweeper learning.&#8221;  Now, it&#8217;s actually known that <a href="https://web.mat.bham.ac.uk/R.W.Kaye/minesw/ASE2003.pdf">Minesweeper is an NP-hard problem</a> to play optimally, so we know that in learning in a dangerous environment you can get that kind of complexity.  As far as I know, we don&#8217;t know anything about typicality or average-case hardness.  <em>Also</em>, to my knowledge no one has proven any nontrivial rigorous bounds on the probability that you&#8217;ll win Minesweeper if you play it optimally, with a given size board and a given number of randomly-placed mines.  Certainly the probability is strictly between 0 and 1; I think it would be extremely interesting to bound it.  I don&#8217;t know if this directly feeds into the AI safety program, but it would at least tell you something about the theory of machine learning in cases where a wrong move can kill you.</p>



<p>So, I hope that gives you at least some sense for what I&#8217;ve been thinking about.  I wish I could end with some neat conclusion, but I don&#8217;t really know the conclusion&#8212;maybe if you ask me again in six more months I&#8217;ll know!  For now, though, I just thought I&#8217;d thank you for your attention and open things up to discussion.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2><strong>Q&amp;A</strong></h2>



<p><strong>Q:</strong> Could you delay rolling out that statistical watermarking tool until May 2026?</p>



<p><strong>Scott:</strong> Why?</p>



<p><strong>Q:</strong> Oh, just until after I graduate [laughter].  OK, my second question is how we can possibly implement these AI safety guidelines inside of systems like <a href="https://en.wikipedia.org/wiki/Automated_machine_learning">AutoML</a>, or whatever their future equivalents are that are much more advanced.</p>



<p><strong>Scott:</strong> I feel like I should learn more about AutoML first before commenting on that specifically.  In general, though, it&#8217;s certainly true that we&#8217;re going to have AIs that will help with the design of other AIs, and indeed this is one of the main things that feeds into the worries about AI safety, which I should&#8217;ve mentioned before explicitly.  Once you have an AI that can recursively self-improve, who knows where it&#8217;s going to end up, right?  It&#8217;s like shooting a rocket into space that you can then no longer steer once it&#8217;s left the earth&#8217;s atmosphere.  So at the very least, you&#8217;d better try to get things right the first time!  You might have only one chance to align its values with what you want.</p>



<p>Precisely for that reason, I tend to be very leery of that kind of thing.  I tend to be much more comfortable with ideas where humans would remain in the loop, where you don&#8217;t just have this completely automated process of an AI designing a stronger AI which designs a still stronger one and so on, but where you&#8217;re repeatedly consulting humans.  Crucially, in this process, we assume the humans can rely on any of the previous AIs to help them (as in the <a href="https://arxiv.org/abs/1810.08575">iterative amplification</a> proposal).  But then it&#8217;s ultimately humans making judgments about the next AI.</p>



<p>Now, if this gets to the point where the humans can no longer even <em>judge</em> a new AI, not even with as much help as they want from earlier AIs, then you could argue: OK, maybe <em>now</em> humans have finally been superseded and rendered irrelevant.  But unless and until we get to that point, I say that humans ought to remain in the loop!</p>



<p><strong>Q:</strong> Most of the protections that you talked about today come from, like, an altruistic human, or a company like OpenAI adding protections in.  Is there any way that you could think of that we could protect ourselves from an AI that&#8217;s maliciously designed or accidentally maliciously designed?</p>



<p><strong>Scott:</strong> Excellent question!  Usually, when people talk about that question at all, they talk about using aligned AIs to help defend yourself against unaligned ones.  I mean, if your adversary has a robot army attacking you, it stands to reason that you&#8217;ll probably want your own robot army, right?  And it&#8217;s very unfortunate, maybe even terrifying, that one can already foresee those sorts of dynamics.</p>



<p>Besides that, there&#8217;s of course the idea of <em>monitoring, regulating, and slowing down the proliferation of powerful AI</em>, which I didn&#8217;t mention explicitly before, perhaps just because by its nature, it seems outside the scope of the technical solutions that a theoretical computer scientist like me might have any special insight about.</p>



<p>But there are certainly people who think that AI development ought to be more heavily regulated, or throttled, or even stopped entirely, in view of the dangers.  Ironically, the &#8220;AI ethics&#8221; camp and the &#8220;orthodox AI alignment&#8221; camp, despite their mutual contempt, seem more and more to yearn for something like this &#8230; an unexpected point of agreement!</p>



<p>But how would you do it?  On the one hand, AI isn&#8217;t like nuclear weapons, where you <em>know</em> that anyone building them will need a certain amount of enriched uranium or plutonium, along with extremely specialized equipment, so you can try (successfully or not) to institute a global regime to track the necessary materials.  You <em>can&#8217;t</em> do the same with software: assuming you&#8217;re not going to confiscate and destroy all computers (which you&#8217;re not), who the hell knows what code or data anyone has?</p>



<p>On the other hand, at least with the current paradigm of AI, there <em>is</em> an obvious choke point, and that&#8217;s the <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPUs</a> (Graphics Processing Units).  Today&#8217;s state-of-the-art machine learning models already need huge server farms full of GPUs, and future generations are likely to need orders of magnitude more still.  And right now, the great majority of the world&#8217;s GPUs are manufactured by <a href="https://en.wikipedia.org/wiki/TSMC">TSMC</a> in Taiwan, albeit with crucial inputs from other countries.  I hardly need to explain the geopolitical ramifications!  A few months ago, as you might have seen, the Biden administrated decided to <a href="https://www.nytimes.com/2022/08/31/technology/gpu-chips-china-russia.html">restrict the export</a> of high-end GPUs to China.  The restriction was driven, in large part, by worries about what the Chinese government could do with unlimited ability to train huge AI models.  Of course the future status of Taiwan figures into this conversation, as does China&#8217;s ability (or inability) to develop a self-sufficient semiconductor industry.</p>



<p>And then there&#8217;s regulation.  I know that in the EU they&#8217;re working on some <a href="https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai">regulatory framework</a> for AI right now, but I don&#8217;t understand the details.  You&#8217;d have to ask someone who follows such things.</p>



<p><strong>Q:</strong> Thanks for coming out and seeing us; this is awesome.  Do you have thoughts on how we can incentivize organizations to build safer AI?  For example, if corporations are competing with each other, then couldn&#8217;t focusing on AI safety make the AI less accurate or less powerful or cut into profits?</p>



<p><strong>Scott:</strong> Yeah, it&#8217;s an excellent question.  You could worry that <em>all</em> this stuff about trying to be safe and responsible when scaling AI &#8230; as soon as it seriously hurts the bottom lines of Google and Facebook and Alibaba and the other major players, a lot of it will go out the window.  People are very worried about that.</p>



<p>On the other hand, we&#8217;ve seen over the past 30 years that the big Internet companies <em>can</em> agree on certain minimal standards, whether because of fear of getting sued, desire to be seen as a responsible player, or whatever else.  One simple example would be <a href="https://www.cloudflare.com/learning/bots/what-is-robots.txt/">robots.txt</a>: if you want your website not to be indexed by search engines, you can specify that, and the major search engines will respect it.</p>



<p>In a similar way, you could imagine something like watermarking&#8212;<em>if</em> we were able to demonstrate it and show that it works and that it&#8217;s cheap and doesn&#8217;t hurt the quality of the output and doesn&#8217;t need much compute and so on&#8212;that it would just become an industry standard, and anyone who wanted to be considered a responsible player would include it.</p>



<p>To be sure, some of these safety measures really <em>do</em> make sense only in a world where there are a few companies that are years ahead of everyone else in scaling up state-of-the-art models&#8212;DeepMind, OpenAI, Google, Facebook, maybe a few others&#8212;and they all agree to be responsible players.  If that equilibrium breaks down, and it becomes a free-for-all, then a lot of the safety measures do become harder, and might even be impossible, at least without government regulation.</p>



<p>We&#8217;re already starting to see this with image models.  As I mentioned earlier, DALL-E2 has all sorts of filters to try to prevent people from creating&#8212;well, in practice it&#8217;s often porn, and/or <a href="https://en.wikipedia.org/wiki/Deepfake">deepfakes</a> involving real people.  In general, though, DALL-E2 will refuse to generate an image if its filters flag the prompt as (by OpenAI&#8217;s lights) a potential misuse of the technology.</p>



<p>But as you might have seen, there&#8217;s already an open-source image model called <a href="https://stability.ai/blog/stable-diffusion-public-release">Stable Diffusion</a>, and people are using it to do all sorts of things that DALL-E won&#8217;t allow.  So it&#8217;s a legitimate question: how can you prevent misuses, <em>unless</em> the closed models remain well ahead of the open ones?</p>



<p><strong>Q:</strong> You mentioned the importance of having humans in the loop who can judge AI systems.  So, as someone who could be in one of those pools of decision makers, what stakeholders do you think should be making the decisions?</p>



<p><strong>Scott:</strong> Oh gosh.  The ideal, as almost everyone agrees, is to have some kind of democratic governance mechanism with broad-based input.  But people have talked about this for years: how do you create the democratic mechanism?  Every activist who wants to bend AI in some preferred direction will <em>claim</em> a democratic mandate; how should a tech company like OpenAI or DeepMind or Google decide which claims are correct?</p>



<p>Maybe the one useful thing I can say is that, in my experience, which is admittedly very limited&#8212;working at OpenAI for all of five months&#8212;I&#8217;ve found my colleagues there to be <em>extremely</em> serious about safety, bordering on obsessive.  They talk about it constantly.  They actually have an <a href="https://openai.com/about/">unusual structure</a>, where they&#8217;re a for-profit company that&#8217;s controlled by a nonprofit foundation, which is at least formally empowered to come in and hit the brakes if needed.  OpenAI also has a <a href="https://openai.com/charter/">charter</a> that contains some striking clauses, especially the following:</p>



<blockquote class="wp-block-quote">
<p>We are concerned about late-stage AGI development becoming a competitive race without time for adequate safety precautions. Therefore, if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project.</p>
</blockquote>



<p>Of course, the fact that they&#8217;ve put a great deal of thought into this doesn&#8217;t mean that they&#8217;re going to get it right!  But if you ask me: would I rather that it be OpenAI in the lead right now or the Chinese government?  Or, if it&#8217;s going to be a company, would I rather it be one with a charter like the above, or a charter of &#8220;maximize clicks and ad revenue&#8221;?  I suppose I do lean a certain way.</p>



<p><strong>Q:</strong> This was a terrifying talk which was lovely, thank you!  But I was thinking: you listed eight different alignment approaches, like kill switches and so on.  You can imagine a future where there&#8217;s a whole bunch of AIs that people spawn and then try to control in these eight ways.  But wouldn&#8217;t this sort of naturally select for AIs that are good at getting past whatever checks we impose on them?  And then eventually you&#8217;d get AIs that are sort of trained in order to fool our tests?</p>



<p><strong>Scott:</strong> Yes.  Your question reminds me of a huge irony.  Eliezer Yudkowsky, the prophet of AI alignment who I talked about earlier, has <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities">become completely doomerist</a> within the last few years.  As a result, he and I have literally <em>switched positions</em> on how optimistic to be about AI safety research!  Back when he was gung-ho about it, I held back.  Today, Eliezer says that it barely matters anymore, since it&#8217;s too late; we&#8217;re all gonna be killed by AI with >99% probability.  Now, he says, it&#8217;s mostly just about dying with more &#8220;dignity&#8221; than otherwise.  Meanwhile, I&#8217;m like, no, I think AI safety is actually just now becoming fruitful and exciting to work on!  So, maybe I&#8217;m just 20 years behind Eliezer, and will eventually catch up and become doomerist too.  Or maybe he, I, and everyone else will be dead before that happens.  I suppose the most optimistic spin is that no one ought to fear coming into AI safety today, as a newcomer, if the prophet of the movement himself says that the past 20 years of research on the subject have given him so little reason for hope.</p>



<p>But if you ask, <em>why</em> is Eliezer so doomerist?  Having read him since 2006, it strikes me that a huge part of it is that, no matter what AI safety proposal anyone comes up with, Eliezer has ready a <em>completely general counterargument</em>.  Namely: &#8220;yes, but the AI will be smarter than that.&#8221;  In other words, no matter what you try to do to make AI safer&#8212;interpretability, backdoors, sandboxing, you name it&#8212;the AI will have already foreseen it, and will have devised a countermeasure that your primate brain can&#8217;t even conceive of because it&#8217;s that much smarter than you.</p>



<p>I confess that, after seeing enough examples of this &#8220;fully general counterargument,&#8221; at some point I&#8217;m like, &#8220;OK, what game are we even playing anymore?&#8221;  If this is just a general refutation to any safety measure, then I suppose that yes, <em>by hypothesis</em>, we&#8217;re screwed.  Yes, in a world where this counterargument is valid, we might as well give up and try to enjoy the time we have left.</p>



<p>But you could also say: <em>for that very reason</em>, it seems more useful to make the methodological assumption that we&#8217;re <em>not</em> in that world!  If we were, then what could we do, right?  So we might as well focus on the possible futures where AI emerges a little more gradually, where we have time to see how it&#8217;s going, learn from experience, improve our understanding, correct as we go&#8212;in other words, the things that have <em>always</em> been the prerequisites to scientific progress, and that have luckily always obtained, even if philosophically we never really had any right to expect them.  We might as well focus on the worlds where, for example, before we get an AI that successfully plots to kill all humans in a matter of seconds, we&#8217;ll probably first get an AI that <em>tries</em> to kill all humans but is really inept at it.  Now fortunately, I personally also regard the latter scenarios as the more plausible ones anyway.  But <em>even if you didn&#8217;t</em>&#8212;again, methodologically, it seems to me that it&#8217;d still make sense to focus on them.</p>



<p><strong>Q:</strong> Regarding your project on watermarking&#8212;so in general, for discriminating between human and model outputs, what&#8217;s the endgame?  Can watermarking win in the long run?  Will it just be an eternal arms race?</p>



<p><strong>Scott:</strong> Another great question.  One difficulty with watermarking is that it&#8217;s hard even to formalize what the task is.  I mean, you could always take the output of an AI model and rephrase it using some <em>other</em> AI model, for example, and catching all such things seems like an <a href="https://en.wikipedia.org/wiki/AI-complete">&#8220;AI-complete problem.&#8221;</a></p>



<p>On the other hand, I can think of writers&#8212;Shakespeare, Wodehouse, David Foster Wallace&#8212;who have such a distinctive style that, even if they <em>tried</em> to pretend to be someone else, they plausibly couldn&#8217;t.  Everyone would recognize that it was them.  So, you could imagine trying to build an AI in the same way.  That is, it would be constructed from the ground up so that all of its outputs contained indelible marks, whether cryptographic or stylistic, giving away their origin.  The AI couldn&#8217;t easily hide and pretend to be a human or anything else it wasn&#8217;t.  Whether this is possible strikes me as an <em>extremely</em> interesting question at the interface between AI and cryptography!  It&#8217;s especially challenging if you impose one or more of the following conditions:</p>



<ol>
<li>the AI&#8217;s code and parameters should be public (in which case, people might easily be able to modify it to remove the watermarking),</li>



<li>the AI should have at least some ability to modify itself, and</li>



<li>the means of <em>checking</em> for the watermark should be public (in which case, again, the watermark might be easier to understand and remove).</li>
</ol>



<p>I don&#8217;t actually have a good intuition as to which side will ultimately win this contest, the AIs trying to conceal themselves or the watermarking schemes trying to reveal them, the Replicants or the <a href="https://en.wikipedia.org/wiki/Blade_Runner#Voight-Kampff_machine">Voight-Kampff machines</a>.</p>



<p>Certainly in the watermarking scheme that I&#8217;m working on now, we crucially exploit the fact that OpenAI controls its own servers.  So, it can do the watermarking using a secret key, and it can check for the watermark using the same key.  In a world where anyone could build their own text model that was just as good as GPT &#8230; what would you do there?</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-29T05:50:03Z">Tuesday, November 29 2022, 05:50</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.14497'>Extractors for Images of Varieties</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Zeyu Guo, Ben Lee Volk, Akhil Jalan, David Zuckerman</p><p>We construct explicit deterministic extractors for polynomial images of
varieties, that is, distributions sampled by applying a low-degree polynomial
map $f : \mathbb{F}_q^r \to \mathbb{F}_q^n$ to an element sampled uniformly at
random from a $k$-dimensional variety $V \subseteq \mathbb{F}_q^r$. This class
of sources generalizes both polynomial sources, studied by Dvir, Gabizon and
Wigderson (FOCS 2007, Comput. Complex. 2009), and variety sources, studied by
Dvir (CCC 2009, Comput. Complex. 2012).
</p>
<p>Assuming certain natural non-degeneracy conditions on the map $f$ and the
variety $V$, which in particular ensure that the source has enough min-entropy,
we extract almost all the min-entropy of the distribution. Unlike the
Dvir-Gabizon-Wigderson and Dvir results, our construction works over large
enough finite fields of arbitrary characteristic. One key part of our
construction is an improved deterministic rank extractor for varieties. As a
by-product, we obtain explicit Noether normalization lemmas for affine
varieties and affine algebras.
</p>
<p>Additionally, we generalize a construction of affine extractors with
exponentially small error due to Bourgain, Dvir and Leeman (Comput. Complex.
2016) by extending it to all finite prime fields of quasipolynomial size.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zeyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Volk_B/0/1/0/all/0/1">Ben Lee Volk</a>, <a href="http://arxiv.org/find/cs/1/au:+Jalan_A/0/1/0/all/0/1">Akhil Jalan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuckerman_D/0/1/0/all/0/1">David Zuckerman</a></p><p>We construct explicit deterministic extractors for polynomial images of
varieties, that is, distributions sampled by applying a low-degree polynomial
map $f : \mathbb{F}_q^r \to \mathbb{F}_q^n$ to an element sampled uniformly at
random from a $k$-dimensional variety $V \subseteq \mathbb{F}_q^r$. This class
of sources generalizes both polynomial sources, studied by Dvir, Gabizon and
Wigderson (FOCS 2007, Comput. Complex. 2009), and variety sources, studied by
Dvir (CCC 2009, Comput. Complex. 2012).
</p>
<p>Assuming certain natural non-degeneracy conditions on the map $f$ and the
variety $V$, which in particular ensure that the source has enough min-entropy,
we extract almost all the min-entropy of the distribution. Unlike the
Dvir-Gabizon-Wigderson and Dvir results, our construction works over large
enough finite fields of arbitrary characteristic. One key part of our
construction is an improved deterministic rank extractor for varieties. As a
by-product, we obtain explicit Noether normalization lemmas for affine
varieties and affine algebras.
</p>
<p>Additionally, we generalize a construction of affine extractors with
exponentially small error due to Bourgain, Dvir and Leeman (Comput. Complex.
2016) by extending it to all finite prime fields of quasipolynomial size.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-29T01:30:00Z">Tuesday, November 29 2022, 01:30</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
