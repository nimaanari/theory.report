<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>
  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.4 (2022-04-12) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <link rel='stylesheet' type='text/css' href='css/font-awesome.css'>
  <link rel='stylesheet' type='text/css' href='css/blank.css'>
</head>
<body>
  <div id='navwrap'>
    <div id='nav'>
      <p>
        Last Update
      </p>
      <p class='small'>
        
          <time class='timeago' datetime="2022-10-15T21:39:19Z">Saturday, October 15 2022, 21:39</time>
        
      </p>

      <p>Feeds</p>
      <ul class='subscriptions small' >
      
        <li>
          <a href='http://arxiv.org/rss/cs.CC'><img src='i/feed.png'></a>
          <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
          
        </li>
      
        <li>
          <a href='http://arxiv.org/rss/cs.CG'><img src='i/feed.png'></a>
          <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
          
        </li>
      
        <li>
          <a href='http://arxiv.org/rss/cs.DS'><img src='i/feed.png'></a>
          <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
          
        </li>
      
        <li>
          <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='i/feed.png'></a>
          <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
          
        </li>
      
        <li>
          <a href='https://adamsheffer.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
          
        </li>
      
        <li>
          <a href='https://adamdsmith.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
          
        </li>
      
        <li>
          <a href='https://polylogblog.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
          
        </li>
      
        <li>
          <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='i/feed.png'></a>
          <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
          
        </li>
      
        <li>
          <a href='http://www.argmin.net/feed.xml'><img src='i/feed.png'></a>
          <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
          
        </li>
      
        <li>
          <a href='http://bit-player.org/feed/atom/'><img src='i/feed.png'></a>
          <a href='http://bit-player.org'>bit-player</a>
          
        </li>
      
        <li>
          <a href='https://cstheory-jobs.org/feed/'><img src='i/feed.png'></a>
          <a href='https://cstheory-jobs.org'>CCI: jobs</a>
          
        </li>
      
        <li>
          <a href='https://cstheory-events.org/feed/'><img src='i/feed.png'></a>
          <a href='https://cstheory-events.org'>CS Theory Events</a>
          
        </li>
      
        <li>
          <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='i/feed.png'></a>
          <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
          
        </li>
      
        <li>
          <a href='https://11011110.github.io/blog/feed.xml'><img src='i/feed.png'></a>
          <a href='https://11011110.github.io/blog/'>David Eppstein</a>
          
        </li>
      
        <li>
          <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='i/feed.png'></a>
          <a href='https://daveagp.wordpress.com'>David Pritchard</a>
          
        </li>
      
        <li>
          <a href='https://decentdescent.org/feed.xml'><img src='i/feed.png'></a>
          <a href='https://decentdescent.org/'>Decent Descent</a>
          
        </li>
      
        <li>
          <a href='https://decentralizedthoughts.github.io/feed'><img src='i/feed.png'></a>
          <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
          
        </li>
      
        <li>
          <a href='https://differentialprivacy.org/feed.xml'><img src='i/feed.png'></a>
          <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
          
        </li>
      
        <li>
          <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='i/feed.png'></a>
          <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
          
        </li>
      
        <li>
          <a href='https://emanueleviola.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
          
        </li>
      
        <li>
          <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='i/feed.png'></a>
          <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
          
        </li>
      
        <li>
          <a href='https://dstheory.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
          
        </li>
      
        <li>
          <a href='https://francisbach.com/feed/'><img src='i/feed.png'></a>
          <a href='https://francisbach.com'>Francis Bach</a>
          
        </li>
      
        <li>
          <a href='https://gilkalai.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
          
        </li>
      
        <li>
          <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='i/feed.png'></a>
          <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
          
        </li>
      
        <li>
          <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='i/feed.png'></a>
          <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
          
        </li>
      
        <li>
          <a href='https://gradientscience.org/feed.xml'><img src='i/feed.png'></a>
          <a href='https://gradientscience.org/'>Gradient Science</a>
          
        </li>
      
        <li>
          <a href='http://grigory.us/blog/feed.xml'><img src='i/feed.png'></a>
          <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
          
        </li>
      
        <li>
          <a href='https://tcsmath.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
          
        </li>
      
        <li>
          <a href='https://kamathematics.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
          
        </li>
      
        <li>
          <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='i/feed.png'></a>
          <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
          
        </li>
      
        <li>
          <a href='https://lucatrevisan.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
          
        </li>
      
        <li>
          <a href='https://mittheory.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
          
        </li>
      
        <li>
          <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='i/feed.png'></a>
          <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
          
        </li>
      
        <li>
          <a href='http://blog.mrtz.org/feed.xml'><img src='i/feed.png'></a>
          <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
          
        </li>
      
        <li>
          <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='i/feed.png'></a>
          <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
          
        </li>
      
        <li>
          <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
          
        </li>
      
        <li>
          <a href='http://www.solipsistslog.com/feed/'><img src='i/feed.png'></a>
          <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
          
        </li>
      
        <li>
          <a href='http://www.offconvex.org/feed.xml'><img src='i/feed.png'></a>
          <a href='http://offconvex.github.io/'>Off the Convex Path</a>
          
        </li>
      
        <li>
          <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='i/feed.png'></a>
          <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
          
        </li>
      
        <li>
          <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='i/feed.png'></a>
          <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
          
        </li>
      
        <li>
          <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='i/feed.png'></a>
          <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
          
        </li>
      
        <li>
          <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='i/feed.png'></a>
          <a href='https://blogs.princeton.edu/imabandit'>Sébastien Bubeck</a>
          
        </li>
      
        <li>
          <a href='https://scottaaronson.blog/?feed=atom'><img src='i/feed.png'></a>
          <a href='https://scottaaronson.blog'>Scott Aaronson</a>
          
        </li>
      
        <li>
          <a href='https://blog.simons.berkeley.edu/feed/'><img src='i/feed.png'></a>
          <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
          
        </li>
      
        <li>
          <a href='https://tcsplus.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
          
        </li>
      
        <li>
          <a href='https://toc4fairness.org/feed/'><img src='i/feed.png'></a>
          <a href='https://toc4fairness.org'>TOC for Fairness</a>
          
        </li>
      
        <li>
          <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='i/feed.png'></a>
          <a href='http://blog.geomblog.org/'>The Geomblog</a>
          
        </li>
      
        <li>
          <a href='https://www.let-all.com/blog/feed/'><img src='i/feed.png'></a>
          <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
          
        </li>
      
        <li>
          <a href='https://theorydish.blog/feed/'><img src='i/feed.png'></a>
          <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
          
        </li>
      
        <li>
          <a href='https://thmatters.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://thmatters.wordpress.com'>Theory Matters</a>
          
        </li>
      
        <li>
          <a href='https://mycqstate.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
          
        </li>
      
        <li>
          <a href='https://agtb.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
          
        </li>
      
        <li>
          <a href='https://windowsontheory.org/feed/'><img src='i/feed.png'></a>
          <a href='https://windowsontheory.org'>Windows on Theory</a>
          
        </li>
      
      </ul>

      <p class='small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
      <p class='small'>Subscribe to the <a href="atom.xml">Atom feed</a> or <a href="rss20.xml">RSS feed</a> to stay up to date.</p>
      <p class='small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
      <p class='small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
      <p class='small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
    </div>
  </div>

  <div id='opts'>
    <div style='width: 100%; text-align: right;'>
    <img src='i/view-headlines.png' id='show-headlines' title='Show Headlines Only' width='24' height='24'>
    <img src='i/view-snippets.png' id='show-snippets' title='Show Snippets' width='24' height='24'>
    <img src='i/view-standard.png' id='show-fulltext' title='Show Full Text' width='24' height='24'>
    </div>
  </div>

  <h1>
    Theory of Computing Report
  </h1>

  <div id="articles">
    
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Saturday, October 15
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://gilkalai.wordpress.com/2022/10/15/boaz-klartag-and-joseph-lehec-the-slice-conjecture-up-to-polylogarithmic-factor/'>Bo’az Klartag and  Joseph Lehec: The Slice Conjecture Up to Polylogarithmic Factor!</a></h3>
          <p class='item-feed'>from <a href='https://gilkalai.wordpress.com'>Gil Kalai</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          Bo&#8217;az Klartag (right) and Joseph Lehec (left) In December 2020, we reported on Yuansi Chen breakthrough result on Bourgain&#8217;s alicing problem and the Kannan Lovasz Simonovits conjecture. It is a pleasure to report on a further fantastic progress on these &#8230; Continue reading &#8594;
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p><img data-attachment-id="23425" data-permalink="https://gilkalai.wordpress.com/2022/10/15/boaz-klartag-and-joseph-lehec-the-slice-conjecture-up-to-polylogarithmic-factor/boazjoseph/" data-orig-file="https://gilkalai.files.wordpress.com/2022/10/boazjoseph.png" data-orig-size="860,380" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="BoazJoseph" data-image-description="" data-image-caption="" data-medium-file="https://gilkalai.files.wordpress.com/2022/10/boazjoseph.png?w=300" data-large-file="https://gilkalai.files.wordpress.com/2022/10/boazjoseph.png?w=640" class="alignnone size-full wp-image-23425" src="https://gilkalai.files.wordpress.com/2022/10/boazjoseph.png?w=640" alt="BoazJoseph" srcset="https://gilkalai.files.wordpress.com/2022/10/boazjoseph.png?w=640 640w, https://gilkalai.files.wordpress.com/2022/10/boazjoseph.png?w=150 150w, https://gilkalai.files.wordpress.com/2022/10/boazjoseph.png?w=300 300w, https://gilkalai.files.wordpress.com/2022/10/boazjoseph.png?w=768 768w, https://gilkalai.files.wordpress.com/2022/10/boazjoseph.png 860w" sizes="(max-width: 640px) 100vw, 640px"   /></p>
<p style="text-align:center;"><span style="color:#ff0000;"><strong>Bo&#8217;az Klartag (right) and Joseph Lehec (left)</strong></span></p>
<p>In December 2020, <a href="https://gilkalai.wordpress.com/2020/12/21/to-cheer-you-up-in-difficult-times-15-yuansi-chen-achieved-a-major-breakthrough-on-bourgains-slicing-problem-and-the-kannan-lovasz-and-simonovits-conjecture/">we reported on Yuansi Chen breakthrough result</a> on Bourgain&#8217;s alicing problem and the Kannan Lovasz Simonovits conjecture. It is a pleasure to report on a further fantastic progress on these problems.</p>
<p><strong>Bourgain&#8217;s slicing problem (1984):</strong>  Is there <em>c &gt; 0</em> such that for any dimension n and any centrally symmetric convex body <em>K ⊆</em> <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathbb R^n" class="latex" /> of volume one, there exists a hyperplane <em>H</em> such that the<em> (n − 1)-</em>dimensional volume of <em>K ∩ H</em> is at least <em>c</em>?</p>
<p>Some time ago we reported on Yuansi Chen&#8217;s startling result that c can be taken as <img src="https://s0.wp.com/latex.php?latex=n%5E%7B-o%281%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=n%5E%7B-o%281%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n%5E%7B-o%281%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="n^{-o(1)}" class="latex" />. More precisely, Chen proved:</p>
<p><strong>Theorem (Chen, 2021):</strong> For any dimension n and any centrally symmetric convex body <em>K ⊆</em> <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathbb R^n" class="latex" /> of volume one, there exists a hyperplane <em>H</em> such that the<em> (n − 1)-</em>dimensional volume of <em>K ∩ H</em> is at least <img src="https://s0.wp.com/latex.php?latex=%5Cfrac+%7B1%7D%7BL_n%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cfrac+%7B1%7D%7BL_n%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac+%7B1%7D%7BL_n%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;frac {1}{L_n}" class="latex" /> where</p>
<p><img src="https://s0.wp.com/latex.php?latex=L_n%3DC+%5Cexp+%28+%5Csqrt+%7B%5Clog+n%7D+%5Csqrt+%7B3%5Clog+%5Clog+n%7D+%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=L_n%3DC+%5Cexp+%28+%5Csqrt+%7B%5Clog+n%7D+%5Csqrt+%7B3%5Clog+%5Clog+n%7D+%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=L_n%3DC+%5Cexp+%28+%5Csqrt+%7B%5Clog+n%7D+%5Csqrt+%7B3%5Clog+%5Clog+n%7D+%29.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="L_n=C &#92;exp ( &#92;sqrt {&#92;log n} &#92;sqrt {3&#92;log &#92;log n} )." class="latex" /></p>
<p><a href="https://arxiv.org/abs/2203.15551">A major improvement was recently achieved by  Bo&#8217;az Klartag and Joseph Lehec</a></p>
<p><strong>Theorem (Klartag and Lehec, 2022):</strong> For any dimension n and any centrally symmetric convex body <em>K ⊆</em> <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathbb R^n" class="latex" /> of volume one, there exists a hyperplane <em>H</em> such that the<em> (n − 1)-</em>dimensional volume of <em>K ∩ H</em> is at least <img src="https://s0.wp.com/latex.php?latex=%5Cfrac+%7B1%7D%7BL_n%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cfrac+%7B1%7D%7BL_n%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac+%7B1%7D%7BL_n%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;frac {1}{L_n}" class="latex" /> where</p>
<p><img src="https://s0.wp.com/latex.php?latex=L_n+%3D+C+%28%5Clog+n%29%5E4.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=L_n+%3D+C+%28%5Clog+n%29%5E4.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=L_n+%3D+C+%28%5Clog+n%29%5E4.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="L_n = C (&#92;log n)^4." class="latex" /></p>
<p>Klartag and Lehec&#8217;s argument (as Chen&#8217;s earlier argument) relies on Ronen Eldan’s stochastic localization, with a new ingredients being the functional-analytic approach from a paper by Klartag and Eli Putterman</p>
<p>This is fantastic progress. Congratulations Bo&#8217;az and Joseph!</p>
<p class="authors">By Gil Kalai</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-15T15:50:56Z">Saturday, October 15 2022, 15:50</time>
        </div>
      </div>
    </article>
  
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Friday, October 14
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://gilkalai.wordpress.com/2022/10/14/alefs-corner-it-wont-work-sorry/'>Alef’s Corner: “It won’t work, sorry”</a></h3>
          <p class='item-feed'>from <a href='https://gilkalai.wordpress.com'>Gil Kalai</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          Alef thanks Nicolas Curien for the triangulation. Click here for other posts with Alef&#8217;s art.
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p><img data-attachment-id="23387" data-permalink="https://gilkalai.wordpress.com/2022/10/14/alefs-corner-it-wont-work-sorry/giliwnw/" data-orig-file="https://gilkalai.files.wordpress.com/2022/10/giliwnw.png" data-orig-size="2100,2100" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gilIWNW" data-image-description="" data-image-caption="" data-medium-file="https://gilkalai.files.wordpress.com/2022/10/giliwnw.png?w=300" data-large-file="https://gilkalai.files.wordpress.com/2022/10/giliwnw.png?w=640" class="alignnone size-full wp-image-23387" src="https://gilkalai.files.wordpress.com/2022/10/giliwnw.png?w=640" alt="gilIWNW" srcset="https://gilkalai.files.wordpress.com/2022/10/giliwnw.png?w=640 640w, https://gilkalai.files.wordpress.com/2022/10/giliwnw.png?w=1280 1280w, https://gilkalai.files.wordpress.com/2022/10/giliwnw.png?w=150 150w, https://gilkalai.files.wordpress.com/2022/10/giliwnw.png?w=300 300w, https://gilkalai.files.wordpress.com/2022/10/giliwnw.png?w=768 768w, https://gilkalai.files.wordpress.com/2022/10/giliwnw.png?w=1024 1024w" sizes="(max-width: 640px) 100vw, 640px"   /></p>
<p>Alef thanks Nicolas Curien for the triangulation. <a href="https://gilkalai.wordpress.com/tag/alefs-corner/">Click here for other posts with Alef&#8217;s art.</a></p>
<p class="authors">By Gil Kalai</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-14T11:04:22Z">Friday, October 14 2022, 11:04</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.06845'>The Fine-Grained Complexity of Graph Homomorphism Parameterized by Clique-Width</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Robert Ganian, Thekla Hamm, Viktoriia Korchemna, Karolina Okrasa, Kirill Simonov</p><p>The generic homomorphism problem, which asks whether an input graph $G$
admits a homomorphism into a fixed target graph $H$, has been widely studied in
the literature. In this article, we provide a fine-grained complexity
classification of the running time of the homomorphism problem with respect to
the clique-width of $G$ (denoted $\operatorname{cw}$) for virtually all choices
of $H$ under the Strong Exponential Time Hypothesis. In particular, we identify
a property of $H$ called the signature number $s(H)$ and show that for each
$H$, the homomorphism problem can be solved in time
$\mathcal{O}^*(s(H)^{\operatorname{cw}})$. Crucially, we then show that this
algorithm can be used to obtain essentially tight upper bounds. Specifically,
we provide a reduction that yields matching lower bounds for each $H$ that is
either a projective core or a graph admitting a factorization with additional
properties -- allowing us to cover all possible target graphs under
long-standing conjectures.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ganian_R/0/1/0/all/0/1">Robert Ganian</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamm_T/0/1/0/all/0/1">Thekla Hamm</a>, <a href="http://arxiv.org/find/cs/1/au:+Korchemna_V/0/1/0/all/0/1">Viktoriia Korchemna</a>, <a href="http://arxiv.org/find/cs/1/au:+Okrasa_K/0/1/0/all/0/1">Karolina Okrasa</a>, <a href="http://arxiv.org/find/cs/1/au:+Simonov_K/0/1/0/all/0/1">Kirill Simonov</a></p><p>The generic homomorphism problem, which asks whether an input graph $G$
admits a homomorphism into a fixed target graph $H$, has been widely studied in
the literature. In this article, we provide a fine-grained complexity
classification of the running time of the homomorphism problem with respect to
the clique-width of $G$ (denoted $\operatorname{cw}$) for virtually all choices
of $H$ under the Strong Exponential Time Hypothesis. In particular, we identify
a property of $H$ called the signature number $s(H)$ and show that for each
$H$, the homomorphism problem can be solved in time
$\mathcal{O}^*(s(H)^{\operatorname{cw}})$. Crucially, we then show that this
algorithm can be used to obtain essentially tight upper bounds. Specifically,
we provide a reduction that yields matching lower bounds for each $H$ that is
either a projective core or a graph admitting a factorization with additional
properties -- allowing us to cover all possible target graphs under
long-standing conjectures.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-14T00:30:00Z">Friday, October 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.07030'>Hard to Detect Factors of Univariate Integer Polynomials</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Alberto Dennunzio, Enrico Formenti, Luciano Margara</p><p>We investigate the computational complexity of deciding whether a given
univariate integer polynomial p(x) has a factor q(x) satisfying specific
additional constraints. When the only constraint imposed on q(x) is to have a
degree smaller than the degree of p(x) and greater than zero, the problem is
equivalent to testing the irreducibility of p(x) and then it is solvable in
polynomial time. We prove that deciding whether a given monic univariate
integer polynomial has factors satisfying additional properties may lead to
NP-complete problems in the strong sense. In particular, given any constant
value k in Z, we prove that it is NP-complete in the strong sense to detect the
existence of a factor that returns a prescribed value when evaluated at x=k or
to detect the existence of a pair of factors - whose product is equal to the
original polynomial - that return the same value when evaluated at x=k.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dennunzio_A/0/1/0/all/0/1">Alberto Dennunzio</a>, <a href="http://arxiv.org/find/cs/1/au:+Formenti_E/0/1/0/all/0/1">Enrico Formenti</a>, <a href="http://arxiv.org/find/cs/1/au:+Margara_L/0/1/0/all/0/1">Luciano Margara</a></p><p>We investigate the computational complexity of deciding whether a given
univariate integer polynomial p(x) has a factor q(x) satisfying specific
additional constraints. When the only constraint imposed on q(x) is to have a
degree smaller than the degree of p(x) and greater than zero, the problem is
equivalent to testing the irreducibility of p(x) and then it is solvable in
polynomial time. We prove that deciding whether a given monic univariate
integer polynomial has factors satisfying additional properties may lead to
NP-complete problems in the strong sense. In particular, given any constant
value k in Z, we prove that it is NP-complete in the strong sense to detect the
existence of a factor that returns a prescribed value when evaluated at x=k or
to detect the existence of a pair of factors - whose product is equal to the
original polynomial - that return the same value when evaluated at x=k.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-14T00:30:00Z">Friday, October 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.07085'>Towards Uniform Certification in QBF</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Leroy Chew, Friedrich Slivovsky</p><p>We pioneer a new technique that allows us to prove a multitude of previously
open simulations in QBF proof complexity. In particular, we show that extended
QBF Frege p-simulates clausal proof systems such as IR-Calculus, IRM-Calculus,
Long-Distance Q-Resolution, and Merge Resolution. These results are obtained by
taking a technique of Beyersdorff et al. (JACM 2020) that turns strategy
extraction into simulation and combining it with new local strategy extraction
arguments.
</p>
<p>This approach leads to simulations that are carried out mainly in
propositional logic, with minimal use of the QBF rules. Our proofs therefore
provide a new, largely propositional interpretation of the simulated systems.
We argue that these results strengthen the case for uniform certification in
QBF solving, since many QBF proof systems now fall into place underneath
extended QBF Frege.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chew_L/0/1/0/all/0/1">Leroy Chew</a>, <a href="http://arxiv.org/find/cs/1/au:+Slivovsky_F/0/1/0/all/0/1">Friedrich Slivovsky</a></p><p>We pioneer a new technique that allows us to prove a multitude of previously
open simulations in QBF proof complexity. In particular, we show that extended
QBF Frege p-simulates clausal proof systems such as IR-Calculus, IRM-Calculus,
Long-Distance Q-Resolution, and Merge Resolution. These results are obtained by
taking a technique of Beyersdorff et al. (JACM 2020) that turns strategy
extraction into simulation and combining it with new local strategy extraction
arguments.
</p>
<p>This approach leads to simulations that are carried out mainly in
propositional logic, with minimal use of the QBF rules. Our proofs therefore
provide a new, largely propositional interpretation of the simulated systems.
We argue that these results strengthen the case for uniform certification in
QBF solving, since many QBF proof systems now fall into place underneath
extended QBF Frege.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-14T00:30:00Z">Friday, October 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.06739'>Computing the Best Case Energy Complexity of Satisfying Assignments in Monotone Circuits</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Janio Carlos Nascimento Silva, U&#xe9;verton S. Souza</p><p>Measures of circuit complexity are usually analyzed to ensure the computation
of Boolean functions with economy and efficiency. One of these measures is
energy complexity, which is related to the number of gates that output true in
a circuit for an assignment. The idea behind energy complexity comes from the
counting of `firing' neurons in a natural neural network. The initial model is
based on threshold circuits, but recent works also have analyzed the energy
complexity of traditional Boolean circuits. In this work, we discuss the time
complexity needed to compute the best-case energy complexity among satisfying
assignments of a monotone Boolean circuit, and we call such a problem as
MinEC$^+_M$. In the MinEC$^+_M$ problem, we are given a monotone Boolean
circuit $C$, a positive integer $k$ and asked to determine whether there is a
satisfying assignment $X$ for $C$ such that $EC(C,X) \leq k$, where $EC(C,X)$
is the number of gates that output true in $C$ according to the assignment $X$.
We prove that MinEC$^+_M$ is NP-complete even when the input monotone circuit
is planar. Besides, we show that the problem is W[1]-hard but in XP when
parameterized by the size of the solution. In contrast, we show that when the
size of the solution and the genus of the input circuit are aggregated
parameters, the MinEC$^+_M$ problem becomes fixed-parameter tractable.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1">Janio Carlos Nascimento Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Souza_U/0/1/0/all/0/1">U&#xe9;verton S. Souza</a></p><p>Measures of circuit complexity are usually analyzed to ensure the computation
of Boolean functions with economy and efficiency. One of these measures is
energy complexity, which is related to the number of gates that output true in
a circuit for an assignment. The idea behind energy complexity comes from the
counting of `firing' neurons in a natural neural network. The initial model is
based on threshold circuits, but recent works also have analyzed the energy
complexity of traditional Boolean circuits. In this work, we discuss the time
complexity needed to compute the best-case energy complexity among satisfying
assignments of a monotone Boolean circuit, and we call such a problem as
MinEC$^+_M$. In the MinEC$^+_M$ problem, we are given a monotone Boolean
circuit $C$, a positive integer $k$ and asked to determine whether there is a
satisfying assignment $X$ for $C$ such that $EC(C,X) \leq k$, where $EC(C,X)$
is the number of gates that output true in $C$ according to the assignment $X$.
We prove that MinEC$^+_M$ is NP-complete even when the input monotone circuit
is planar. Besides, we show that the problem is W[1]-hard but in XP when
parameterized by the size of the solution. In contrast, we show that when the
size of the solution and the genus of the input circuit are aggregated
parameters, the MinEC$^+_M$ problem becomes fixed-parameter tractable.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-14T00:30:00Z">Friday, October 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.07146'>Efficient Algorithms for Obnoxious Facility Location on a Line Segment or Circle</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Bowei Zhang</p><p>We study different restricted variations of the obnoxious facility location
problem on a plane. The first is the constrained obnoxious facility location on
a line segment (COFL-Line) problem. We provide an efficient algorithm for this
problem that executes in $O(n ^ 2 \log k + n \log k \log (n^2 + k))$ time. Our
result improves on the best known result of $O((nk)^2 \log(nk) + (n + k) \log
(nk))$ time obtained by Singireddy and Basappa\cite{singireddy2022dispersing}.
We also study the same problem where the facilities must be placed on a given
circle (the constrained obnoxious facility location on a circle (COFL-Circ)
problem). We provide an efficient algorithm for this problem that executes in
$O(n ^ 2 \log k + n \log k \log (n^2 + k))$ time. Our result improves on the
best known result of $O((nk)^2 \log(nk) + (n + k) \log (nk))$ time obtained by
Singireddy and Basappa\cite{singireddy2022dispersing}. The third problem we
study is the min-sum obnoxious facility location (MOFL) problem.We provide an
efficient algorithm that executes in $O(nk\cdot \alpha(nk) \log^3 {nk})$ time,
where $\alpha(.)$ is the inverse Ackermann function. The best known previous
result is an $O(n^3k)$ time obtained by Singireddy and
Basappa\cite{singireddy2022dispersing}.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bowei Zhang</a></p><p>We study different restricted variations of the obnoxious facility location
problem on a plane. The first is the constrained obnoxious facility location on
a line segment (COFL-Line) problem. We provide an efficient algorithm for this
problem that executes in $O(n ^ 2 \log k + n \log k \log (n^2 + k))$ time. Our
result improves on the best known result of $O((nk)^2 \log(nk) + (n + k) \log
(nk))$ time obtained by Singireddy and Basappa\cite{singireddy2022dispersing}.
We also study the same problem where the facilities must be placed on a given
circle (the constrained obnoxious facility location on a circle (COFL-Circ)
problem). We provide an efficient algorithm for this problem that executes in
$O(n ^ 2 \log k + n \log k \log (n^2 + k))$ time. Our result improves on the
best known result of $O((nk)^2 \log(nk) + (n + k) \log (nk))$ time obtained by
Singireddy and Basappa\cite{singireddy2022dispersing}. The third problem we
study is the min-sum obnoxious facility location (MOFL) problem.We provide an
efficient algorithm that executes in $O(nk\cdot \alpha(nk) \log^3 {nk})$ time,
where $\alpha(.)$ is the inverse Ackermann function. The best known previous
result is an $O(n^3k)$ time obtained by Singireddy and
Basappa\cite{singireddy2022dispersing}.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-14T00:30:00Z">Friday, October 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.06559'>Exact and approximation algorithms for sensor placement against DDoS attacks</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Konstanty Junosza-Szaniawski, Dariusz Nogalski, Pawe&#x142; Rz&#x105;&#x17c;ewski</p><p>In a DDoS attack (Distributed Denial of Service), an attacker gains control
of many network users through a virus. Then the controlled users send many
requests to a victim, leading to its resources being depleted. DDoS attacks are
hard to defend because of their distributed nature, large scale and various
attack techniques. One possible mode of defense is to place sensors in a
network that can detect and stop an unwanted request. However, such sensors are
expensive so there is a natural question as to the minimum number of sensors
and the optimal placement required to get the necessary level of safety.
Presented below are two mixed integer models for optimal sensor placement
against DDoS attacks. Both models lead to a trade-off between the number of
deployed sensors and the volume of uncontrolled flow. Since the above placement
problems are NP-hard, two efficient heuristics are designed, implemented and
compared experimentally with exact mixed integer linear programming solvers.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Junosza_Szaniawski_K/0/1/0/all/0/1">Konstanty Junosza-Szaniawski</a>, <a href="http://arxiv.org/find/cs/1/au:+Nogalski_D/0/1/0/all/0/1">Dariusz Nogalski</a>, <a href="http://arxiv.org/find/cs/1/au:+Rzazewski_P/0/1/0/all/0/1">Pawe&#x142; Rz&#x105;&#x17c;ewski</a></p><p>In a DDoS attack (Distributed Denial of Service), an attacker gains control
of many network users through a virus. Then the controlled users send many
requests to a victim, leading to its resources being depleted. DDoS attacks are
hard to defend because of their distributed nature, large scale and various
attack techniques. One possible mode of defense is to place sensors in a
network that can detect and stop an unwanted request. However, such sensors are
expensive so there is a natural question as to the minimum number of sensors
and the optimal placement required to get the necessary level of safety.
Presented below are two mixed integer models for optimal sensor placement
against DDoS attacks. Both models lead to a trade-off between the number of
deployed sensors and the volume of uncontrolled flow. Since the above placement
problems are NP-hard, two efficient heuristics are designed, implemented and
compared experimentally with exact mixed integer linear programming solvers.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-14T00:30:00Z">Friday, October 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.06594'>Sample Constrained Treatment Effect Estimation</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Raghavendra Addanki, David Arbour, Tung Mai, Cameron Musco, Anup Rao</p><p>Treatment effect estimation is a fundamental problem in causal inference. We
focus on designing efficient randomized controlled trials, to accurately
estimate the effect of some treatment on a population of $n$ individuals. In
particular, we study sample-constrained treatment effect estimation, where we
must select a subset of $s \ll n$ individuals from the population to experiment
on. This subset must be further partitioned into treatment and control groups.
Algorithms for partitioning the entire population into treatment and control
groups, or for choosing a single representative subset, have been well-studied.
The key challenge in our setting is jointly choosing a representative subset
and a partition for that set.
</p>
<p>We focus on both individual and average treatment effect estimation, under a
linear effects model. We give provably efficient experimental designs and
corresponding estimators, by identifying connections to discrepancy
minimization and leverage-score-based sampling used in randomized numerical
linear algebra. Our theoretical results obtain a smooth transition to known
guarantees when $s$ equals the population size. We also empirically demonstrate
the performance of our algorithms.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Addanki_R/0/1/0/all/0/1">Raghavendra Addanki</a>, <a href="http://arxiv.org/find/cs/1/au:+Arbour_D/0/1/0/all/0/1">David Arbour</a>, <a href="http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1">Tung Mai</a>, <a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1">Cameron Musco</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1">Anup Rao</a></p><p>Treatment effect estimation is a fundamental problem in causal inference. We
focus on designing efficient randomized controlled trials, to accurately
estimate the effect of some treatment on a population of $n$ individuals. In
particular, we study sample-constrained treatment effect estimation, where we
must select a subset of $s \ll n$ individuals from the population to experiment
on. This subset must be further partitioned into treatment and control groups.
Algorithms for partitioning the entire population into treatment and control
groups, or for choosing a single representative subset, have been well-studied.
The key challenge in our setting is jointly choosing a representative subset
and a partition for that set.
</p>
<p>We focus on both individual and average treatment effect estimation, under a
linear effects model. We give provably efficient experimental designs and
corresponding estimators, by identifying connections to discrepancy
minimization and leverage-score-based sampling used in randomized numerical
linear algebra. Our theoretical results obtain a smooth transition to known
guarantees when $s$ equals the population size. We also empirically demonstrate
the performance of our algorithms.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-14T00:30:00Z">Friday, October 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.06703'>On the Minimum Cycle Cover problem on graphs with bounded co-degeneracy</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Gabriel L. Duarte, U&#xe9;verton S. Souza</p><p>In 2021, Duarte, Oliveira, and Souza [MFCS 2021] showed some problems that
are FPT when parameterized by the treewidth of the complement graph (called
co-treewidth). Since the degeneracy of a graph is at most its treewidth, they
also introduced the study of co-degeneracy (the degeneracy of the complement
graph) as a parameter. In 1976, Bondy and Chv\'{a}tal [DM 1976] introduced the
notion of closure of a graph: let $\ell$ be an integer; the $(n+\ell)$-closure,
$\operatorname{cl}_{n+\ell}(G)$, of a graph $G$ with $n$ vertices is obtained
from $G$ by recursively adding an edge between pairs of nonadjacent vertices
whose degree sum is at least $n+\ell$ until no such pair remains. A graph
property $\Upsilon$ defined on all graphs of order $n$ is said to be
$(n+\ell)$-stable if for any graph $G$ of order $n$ that does not satisfy
$\Upsilon$, the fact that $uv$ is not an edge of $G$ and that $G+uv$ satisfies
$\Upsilon$ implies $d(u)+d(v)&lt; n+\ell$. Duarte et al. [MFCS 2021] developed an
algorithmic framework for co-degeneracy parameterization based on the notion of
closures for solving problems that are $(n+\ell)$-stable for some $\ell$
bounded by a function of the co-degeneracy. In this paper, we first determine
the stability of the property of having a bounded cycle cover. After that,
combining the framework of Duarte et al. [MFCS 2021] with some results of
Jansen, Kozma, and Nederlof [WG 2019], we obtain a $2^{\mathcal{O}(k)}\cdot
n^{\mathcal{O}(1)}$-time algorithm for Minimum Cycle Cover on graphs with
co-degeneracy at most $k$, which generalizes Duarte et al. [MFCS 2021] and
Jansen et al. [WG 2019] results concerning the Hamiltonian Cycle problem.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Duarte_G/0/1/0/all/0/1">Gabriel L. Duarte</a>, <a href="http://arxiv.org/find/cs/1/au:+Souza_U/0/1/0/all/0/1">U&#xe9;verton S. Souza</a></p><p>In 2021, Duarte, Oliveira, and Souza [MFCS 2021] showed some problems that
are FPT when parameterized by the treewidth of the complement graph (called
co-treewidth). Since the degeneracy of a graph is at most its treewidth, they
also introduced the study of co-degeneracy (the degeneracy of the complement
graph) as a parameter. In 1976, Bondy and Chv\'{a}tal [DM 1976] introduced the
notion of closure of a graph: let $\ell$ be an integer; the $(n+\ell)$-closure,
$\operatorname{cl}_{n+\ell}(G)$, of a graph $G$ with $n$ vertices is obtained
from $G$ by recursively adding an edge between pairs of nonadjacent vertices
whose degree sum is at least $n+\ell$ until no such pair remains. A graph
property $\Upsilon$ defined on all graphs of order $n$ is said to be
$(n+\ell)$-stable if for any graph $G$ of order $n$ that does not satisfy
$\Upsilon$, the fact that $uv$ is not an edge of $G$ and that $G+uv$ satisfies
$\Upsilon$ implies $d(u)+d(v)&lt; n+\ell$. Duarte et al. [MFCS 2021] developed an
algorithmic framework for co-degeneracy parameterization based on the notion of
closures for solving problems that are $(n+\ell)$-stable for some $\ell$
bounded by a function of the co-degeneracy. In this paper, we first determine
the stability of the property of having a bounded cycle cover. After that,
combining the framework of Duarte et al. [MFCS 2021] with some results of
Jansen, Kozma, and Nederlof [WG 2019], we obtain a $2^{\mathcal{O}(k)}\cdot
n^{\mathcal{O}(1)}$-time algorithm for Minimum Cycle Cover on graphs with
co-degeneracy at most $k$, which generalizes Duarte et al. [MFCS 2021] and
Jansen et al. [WG 2019] results concerning the Hamiltonian Cycle problem.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-14T00:30:00Z">Friday, October 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.06714'>Perfect matching cuts partitioning a graph into complementary subgraphs</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Diane Castonguay, Erika M. M. Coelho, Hebert Coelho, Julliano R. Nascimento, U&#xe9;verton S. Souza</p><p>In Partition Into Complementary Subgraphs (Comp-Sub) we are given a graph
$G=(V,E)$, and an edge set property $\Pi$, and asked whether $G$ can be
decomposed into two graphs, $H$ and its complement $\overline{H}$, for some
graph $H$, in such a way that the edge cut $[V(H),V(\overline{H})]$ satisfies
the property $\Pi$. Motivated by previous work, we consider Comp-Sub($\Pi$)
when the property $\Pi=\mathcal{PM}$ specifies that the edge cut of the
decomposition is a perfect matching. We prove that Comp-Sub($\mathcal{PM}$) is
GI-hard when the graph $G$ is $\{C_{k\geq 7}, \overline{C}_{k\geq 7} \}$-free.
On the other hand, we show that Comp-Sub($\mathcal{PM}$) is polynomial-time
solvable on $hole$-free graphs and on $P_5$-free graphs. Furthermore, we
present characterizations of Comp-Sub($\mathcal{PM}$) on chordal,
distance-hereditary, and extended $P_4$-laden graphs.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Castonguay_D/0/1/0/all/0/1">Diane Castonguay</a>, <a href="http://arxiv.org/find/cs/1/au:+Coelho_E/0/1/0/all/0/1">Erika M. M. Coelho</a>, <a href="http://arxiv.org/find/cs/1/au:+Coelho_H/0/1/0/all/0/1">Hebert Coelho</a>, <a href="http://arxiv.org/find/cs/1/au:+Nascimento_J/0/1/0/all/0/1">Julliano R. Nascimento</a>, <a href="http://arxiv.org/find/cs/1/au:+Souza_U/0/1/0/all/0/1">U&#xe9;verton S. Souza</a></p><p>In Partition Into Complementary Subgraphs (Comp-Sub) we are given a graph
$G=(V,E)$, and an edge set property $\Pi$, and asked whether $G$ can be
decomposed into two graphs, $H$ and its complement $\overline{H}$, for some
graph $H$, in such a way that the edge cut $[V(H),V(\overline{H})]$ satisfies
the property $\Pi$. Motivated by previous work, we consider Comp-Sub($\Pi$)
when the property $\Pi=\mathcal{PM}$ specifies that the edge cut of the
decomposition is a perfect matching. We prove that Comp-Sub($\mathcal{PM}$) is
GI-hard when the graph $G$ is $\{C_{k\geq 7}, \overline{C}_{k\geq 7} \}$-free.
On the other hand, we show that Comp-Sub($\mathcal{PM}$) is polynomial-time
solvable on $hole$-free graphs and on $P_5$-free graphs. Furthermore, we
present characterizations of Comp-Sub($\mathcal{PM}$) on chordal,
distance-hereditary, and extended $P_4$-laden graphs.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-14T00:30:00Z">Friday, October 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.06728'>On the Efficient Implementation of High Accuracy Optimality of Profile Maximum Likelihood</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Moses Charikar, Zhihao Jiang, Kirankumar Shiragur, Aaron Sidford</p><p>We provide an efficient unified plug-in approach for estimating symmetric
properties of distributions given $n$ independent samples. Our estimator is
based on profile-maximum-likelihood (PML) and is sample optimal for estimating
various symmetric properties when the estimation error $\epsilon \gg n^{-1/3}$.
This result improves upon the previous best accuracy threshold of $\epsilon \gg
n^{-1/4}$ achievable by polynomial time computable PML-based universal
estimators [ACSS21, ACSS20]. Our estimator reaches a theoretical limit for
universal symmetric property estimation as [Han21] shows that a broad class of
universal estimators (containing many well known approaches including ours)
cannot be sample optimal for every $1$-Lipschitz property when $\epsilon \ll
n^{-1/3}$.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/stat/1/au:+Charikar_M/0/1/0/all/0/1">Moses Charikar</a>, <a href="http://arxiv.org/find/stat/1/au:+Jiang_Z/0/1/0/all/0/1">Zhihao Jiang</a>, <a href="http://arxiv.org/find/stat/1/au:+Shiragur_K/0/1/0/all/0/1">Kirankumar Shiragur</a>, <a href="http://arxiv.org/find/stat/1/au:+Sidford_A/0/1/0/all/0/1">Aaron Sidford</a></p><p>We provide an efficient unified plug-in approach for estimating symmetric
properties of distributions given $n$ independent samples. Our estimator is
based on profile-maximum-likelihood (PML) and is sample optimal for estimating
various symmetric properties when the estimation error $\epsilon \gg n^{-1/3}$.
This result improves upon the previous best accuracy threshold of $\epsilon \gg
n^{-1/4}$ achievable by polynomial time computable PML-based universal
estimators [ACSS21, ACSS20]. Our estimator reaches a theoretical limit for
universal symmetric property estimation as [Han21] shows that a broad class of
universal estimators (containing many well known approaches including ours)
cannot be sample optimal for every $1$-Lipschitz property when $\epsilon \ll
n^{-1/3}$.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-14T00:30:00Z">Friday, October 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.06846'>An $\alpha$-regret analysis of Adversarial Bilateral Trade</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Yossi Azar, Amos Fiat, Federico Fusco</p><p>We study sequential bilateral trade where sellers and buyers valuations are
completely arbitrary (i.e., determined by an adversary). Sellers and buyers are
strategic agents with private valuations for the good and the goal is to design
a mechanism that maximizes efficiency (or gain from trade) while being
incentive compatible, individually rational and budget balanced. In this paper
we consider gain from trade which is harder to approximate than social welfare.
</p>
<p>We consider a variety of feedback scenarios and distinguish the cases where
the mechanism posts one price and when it can post different prices for buyer
and seller. We show several surprising results about the separation between the
different scenarios. In particular we show that (a) it is impossible to achieve
sublinear $\alpha$-regret for any $\alpha&lt;2$, (b) but with full feedback
sublinear $2$-regret is achievable (c) with a single price and partial feedback
one cannot get sublinear $\alpha$ regret for any constant $\alpha$ (d)
nevertheless, posting two prices even with one-bit feedback achieves sublinear
$2$-regret, and (e) there is a provable separation in the $2$-regret bounds
between full and partial feedback.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Azar_Y/0/1/0/all/0/1">Yossi Azar</a>, <a href="http://arxiv.org/find/cs/1/au:+Fiat_A/0/1/0/all/0/1">Amos Fiat</a>, <a href="http://arxiv.org/find/cs/1/au:+Fusco_F/0/1/0/all/0/1">Federico Fusco</a></p><p>We study sequential bilateral trade where sellers and buyers valuations are
completely arbitrary (i.e., determined by an adversary). Sellers and buyers are
strategic agents with private valuations for the good and the goal is to design
a mechanism that maximizes efficiency (or gain from trade) while being
incentive compatible, individually rational and budget balanced. In this paper
we consider gain from trade which is harder to approximate than social welfare.
</p>
<p>We consider a variety of feedback scenarios and distinguish the cases where
the mechanism posts one price and when it can post different prices for buyer
and seller. We show several surprising results about the separation between the
different scenarios. In particular we show that (a) it is impossible to achieve
sublinear $\alpha$-regret for any $\alpha&lt;2$, (b) but with full feedback
sublinear $2$-regret is achievable (c) with a single price and partial feedback
one cannot get sublinear $\alpha$ regret for any constant $\alpha$ (d)
nevertheless, posting two prices even with one-bit feedback achieves sublinear
$2$-regret, and (e) there is a provable separation in the $2$-regret bounds
between full and partial feedback.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-14T00:30:00Z">Friday, October 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.06882'>Beeping Shortest Paths via Hypergraph Bipartite Decomposition</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Fabien Dufoulon, Yuval Emek, Ran Gelles</p><p>Constructing a shortest path between two network nodes is a fundamental task
in distributed computing. This work develops schemes for the construction of
shortest paths in randomized beeping networks between a predetermined source
node and an arbitrary set of destination nodes. Our first scheme constructs a
(single) shortest path to an arbitrary destination in $O (D \log\log n + \log^3
n)$ rounds with high probability. Our second scheme constructs multiple
shortest paths, one per each destination, in $O (D \log^2 n + \log^3 n)$ rounds
with high probability.
</p>
<p>The key technique behind the aforementioned schemes is a novel decomposition
of hypergraphs into bipartite hypergraphs. Namely, we show how to partition the
hyperedge set of a hypergraph $H = (V_H, E_H)$ into $k = \Theta (\log^2 n)$
disjoint subsets $F_1 \cup \cdots \cup F_k = E_H$ such that the
(sub-)hypergraph $(V_H, F_i)$ is bipartite in the sense that there exists a
vertex subset $U \subseteq V$ such that $|U \cap e| = 1$ for every $e \in F_i$.
This decomposition turns out to be instrumental in speeding up shortest path
constructions under the beeping model.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dufoulon_F/0/1/0/all/0/1">Fabien Dufoulon</a>, <a href="http://arxiv.org/find/cs/1/au:+Emek_Y/0/1/0/all/0/1">Yuval Emek</a>, <a href="http://arxiv.org/find/cs/1/au:+Gelles_R/0/1/0/all/0/1">Ran Gelles</a></p><p>Constructing a shortest path between two network nodes is a fundamental task
in distributed computing. This work develops schemes for the construction of
shortest paths in randomized beeping networks between a predetermined source
node and an arbitrary set of destination nodes. Our first scheme constructs a
(single) shortest path to an arbitrary destination in $O (D \log\log n + \log^3
n)$ rounds with high probability. Our second scheme constructs multiple
shortest paths, one per each destination, in $O (D \log^2 n + \log^3 n)$ rounds
with high probability.
</p>
<p>The key technique behind the aforementioned schemes is a novel decomposition
of hypergraphs into bipartite hypergraphs. Namely, we show how to partition the
hyperedge set of a hypergraph $H = (V_H, E_H)$ into $k = \Theta (\log^2 n)$
disjoint subsets $F_1 \cup \cdots \cup F_k = E_H$ such that the
(sub-)hypergraph $(V_H, F_i)$ is bipartite in the sense that there exists a
vertex subset $U \subseteq V$ such that $|U \cap e| = 1$ for every $e \in F_i$.
This decomposition turns out to be instrumental in speeding up shortest path
constructions under the beeping model.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-14T00:30:00Z">Friday, October 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.06926'>Delta-Closure Structure for Studying Data Distribution</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Aleksey Buzmakov, Tatiana Makhalova, Sergei O. Kuznetsov, Amedeo Napoli</p><p>In this paper, we revisit pattern mining and study the distribution
underlying a binary dataset thanks to the closure structure which is based on
passkeys, i.e., minimum generators in equivalence classes robust to noise. We
introduce $\Delta$-closedness, a generalization of the closure operator, where
$\Delta$ measures how a closed set differs from its upper neighbors in the
partial order induced by closure. A $\Delta$-class of equivalence includes
minimum and maximum elements and allows us to characterize the distribution
underlying the data. Moreover, the set of $\Delta$-classes of equivalence can
be partitioned into the so-called $\Delta$-closure structure. In particular, a
$\Delta$-class of equivalence with a high level demonstrates correlations among
many attributes, which are supported by more observations when $\Delta$ is
large. In the experiments, we study the $\Delta$-closure structure of several
real-world datasets and show that this structure is very stable for large
$\Delta$ and does not substantially depend on the data sampling used for the
analysis.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Buzmakov_A/0/1/0/all/0/1">Aleksey Buzmakov</a>, <a href="http://arxiv.org/find/cs/1/au:+Makhalova_T/0/1/0/all/0/1">Tatiana Makhalova</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuznetsov_S/0/1/0/all/0/1">Sergei O. Kuznetsov</a>, <a href="http://arxiv.org/find/cs/1/au:+Napoli_A/0/1/0/all/0/1">Amedeo Napoli</a></p><p>In this paper, we revisit pattern mining and study the distribution
underlying a binary dataset thanks to the closure structure which is based on
passkeys, i.e., minimum generators in equivalence classes robust to noise. We
introduce $\Delta$-closedness, a generalization of the closure operator, where
$\Delta$ measures how a closed set differs from its upper neighbors in the
partial order induced by closure. A $\Delta$-class of equivalence includes
minimum and maximum elements and allows us to characterize the distribution
underlying the data. Moreover, the set of $\Delta$-classes of equivalence can
be partitioned into the so-called $\Delta$-closure structure. In particular, a
$\Delta$-class of equivalence with a high level demonstrates correlations among
many attributes, which are supported by more observations when $\Delta$ is
large. In the experiments, we study the $\Delta$-closure structure of several
real-world datasets and show that this structure is very stable for large
$\Delta$ and does not substantially depend on the data sampling used for the
analysis.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-14T00:30:00Z">Friday, October 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.07018'>Online matching with delays and stochastic arrival times</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Mathieu Mari, Micha&#x142; Paw&#x142;owski, Runtian Ren, Piotr Sankowski</p><p>This paper presents a new research direction for the Min-cost Perfect
Matching with Delays (MPMD) - a problem introduced by Emek et al. (STOC'16). In
the original version of this problem, we are given an $n$-point metric space,
where requests arrive in an online fashion. The goal is to minimise the
matching cost for an even number of requests. However, contrary to traditional
online matching problems, a request does not have to be paired immediately at
the time of its arrival. Instead, the decision of whether to match a request
can be postponed for time $t$ at a delay cost of $t$. For this reason, the goal
of the MPMD is to minimise the overall sum of distance and delay costs.
Interestingly, for adversarially generated requests, no online algorithm can
achieve a competitive ratio better than $O(\log n/\log \log n)$ (Ashlagi et
al., APPROX/RANDOM'17).
</p>
<p>Here, we consider a stochastic version of the MPMD problem where the input
requests follow a Poisson arrival process. For such a problem, we show that the
above lower bound can be improved by presenting two deterministic online
algorithms, which, in expectation, are constant-competitive. The first one is a
simple greedy algorithm that matches any two requests once the sum of their
delay costs exceeds their connection cost, i.e., the distance between them. The
second algorithm builds on the tools used to analyse the first one in order to
obtain even better performance guarantees. This result is rather surprising as
the greedy approach for the adversarial model achieves a competitive ratio of
$\Omega(m^{\log \frac{3}{2}+\varepsilon})$, where $m$ denotes the number of
requests served (Azar et al., TOCS'20). Finally, we prove that it is possible
to obtain similar results for the general case when the delay cost follows an
arbitrary positive and non-decreasing function, as well as for the MPMD variant
with penalties to clear pending requests.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Mari_M/0/1/0/all/0/1">Mathieu Mari</a>, <a href="http://arxiv.org/find/cs/1/au:+Pawlowski_M/0/1/0/all/0/1">Micha&#x142; Paw&#x142;owski</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1">Runtian Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Sankowski_P/0/1/0/all/0/1">Piotr Sankowski</a></p><p>This paper presents a new research direction for the Min-cost Perfect
Matching with Delays (MPMD) - a problem introduced by Emek et al. (STOC'16). In
the original version of this problem, we are given an $n$-point metric space,
where requests arrive in an online fashion. The goal is to minimise the
matching cost for an even number of requests. However, contrary to traditional
online matching problems, a request does not have to be paired immediately at
the time of its arrival. Instead, the decision of whether to match a request
can be postponed for time $t$ at a delay cost of $t$. For this reason, the goal
of the MPMD is to minimise the overall sum of distance and delay costs.
Interestingly, for adversarially generated requests, no online algorithm can
achieve a competitive ratio better than $O(\log n/\log \log n)$ (Ashlagi et
al., APPROX/RANDOM'17).
</p>
<p>Here, we consider a stochastic version of the MPMD problem where the input
requests follow a Poisson arrival process. For such a problem, we show that the
above lower bound can be improved by presenting two deterministic online
algorithms, which, in expectation, are constant-competitive. The first one is a
simple greedy algorithm that matches any two requests once the sum of their
delay costs exceeds their connection cost, i.e., the distance between them. The
second algorithm builds on the tools used to analyse the first one in order to
obtain even better performance guarantees. This result is rather surprising as
the greedy approach for the adversarial model achieves a competitive ratio of
$\Omega(m^{\log \frac{3}{2}+\varepsilon})$, where $m$ denotes the number of
requests served (Azar et al., TOCS'20). Finally, we prove that it is possible
to obtain similar results for the general case when the delay cost follows an
arbitrary positive and non-decreasing function, as well as for the MPMD variant
with penalties to clear pending requests.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-14T00:30:00Z">Friday, October 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.07040'>Threshold Treewidth and Hypertree Width</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Andre Schidler, Robert Ganian, Manuel Sorge, Stefan Szeider</p><p>Treewidth and hypertree width have proven to be highly successful structural
parameters in the context of the Constraint Satisfaction Problem (CSP). When
either of these parameters is bounded by a constant, then CSP becomes solvable
in polynomial time. However, here the order of the polynomial in the running
time depends on the width, and this is known to be unavoidable; therefore, the
problem is not fixed-parameter tractable parameterized by either of these width
measures. Here we introduce an enhancement of tree and hypertree width through
a novel notion of thresholds, allowing the associated decompositions to take
into account information about the computational costs associated with solving
the given CSP instance. Aside from introducing these notions, we obtain
efficient theoretical as well as empirical algorithms for computing threshold
treewidth and hypertree width and show that these parameters give rise to
fixed-parameter algorithms for CSP as well as other, more general problems. We
complement our theoretical results with experimental evaluations in terms of
heuristics as well as exact methods based on SAT/SMT encodings.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Schidler_A/0/1/0/all/0/1">Andre Schidler</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganian_R/0/1/0/all/0/1">Robert Ganian</a>, <a href="http://arxiv.org/find/cs/1/au:+Sorge_M/0/1/0/all/0/1">Manuel Sorge</a>, <a href="http://arxiv.org/find/cs/1/au:+Szeider_S/0/1/0/all/0/1">Stefan Szeider</a></p><p>Treewidth and hypertree width have proven to be highly successful structural
parameters in the context of the Constraint Satisfaction Problem (CSP). When
either of these parameters is bounded by a constant, then CSP becomes solvable
in polynomial time. However, here the order of the polynomial in the running
time depends on the width, and this is known to be unavoidable; therefore, the
problem is not fixed-parameter tractable parameterized by either of these width
measures. Here we introduce an enhancement of tree and hypertree width through
a novel notion of thresholds, allowing the associated decompositions to take
into account information about the computational costs associated with solving
the given CSP instance. Aside from introducing these notions, we obtain
efficient theoretical as well as empirical algorithms for computing threshold
treewidth and hypertree width and show that these parameters give rise to
fixed-parameter algorithms for CSP as well as other, more general problems. We
complement our theoretical results with experimental evaluations in terms of
heuristics as well as exact methods based on SAT/SMT encodings.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-14T00:30:00Z">Friday, October 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Thursday, October 13
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://blog.computationalcomplexity.org/2022/10/how-not-to-pass-polygraph-test.html'>How Not to Pass a Polygraph Test</a></h3>
          <p class='item-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          ♦<br>Many years ago I was asked to serve on an advisory board for an organization that did confidential research. To be on the board I had to have US top secret clearance. The first step was filling out a lengthy form which asked deep details about every aspect of my life. Then there were the interviews with myself and many people I interacted with, especially internationally, and I have many international colleagues. Eventually I got passed these steps.<br>The final step required taking a polygraph (lie-detector) test. So I flew to Baltimore to visit a non-descript office building near the airport. I failed the test. Twice more I went to Baltimore and failed those tests as well.&nbsp;<br>Just to be clear, I never tried to falsify or hide information on these tests. In one case I was asked "Do you live in Atlanta?" I said no. The person administering the test stopped and said I put my address down as Atlanta. I said my mailing address was Atlanta but (at the time) I lived just north of the border in Sandy Springs. She said I should use Atlanta for the test, in other words I should lie. The test didn't go well after that.<br>In another case, I was asked if I was ever arrested. For the record, I have never been arrested but the answer came up as inconclusive. The administrator, different than before, trusted the machine more than me and the rest of the day didn't go well.&nbsp;<br>Perhaps the test wasn't meant to just test whether I was telling the truth, but also my ability to keep a secret. At least that would make more sense why I failed three times. More likely I took questions too literally, a product of a mathematician's mind.<br>I never joined the advisory board but that wasn't the worst of it. In 2014 the Chinese hacked into the US Office of Personnel Management taking information from, among others, those who applied for security clearance. It's the main reason I keep security freezes with the credit bureaus.<p>By Lance Fortnow</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbVRkVbxq4nkOBNugqs01If78vjWowLhCCowXwEXkJCHuST_xZVWQApkTScjld0Z8sDgxwU_R6AwWsKPB4gbiKYW--3TkWtcpZGV4N2skRt7tW2XvxBi5VRjWQKZcqigp8Ewp18q6MrZQUUq_Y09300z8muVgiVskef70PibPTlUGGb08V_w/s300/meet-the-parents-original-300x189.jpg" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" data-original-height="189" data-original-width="300" height="189" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbVRkVbxq4nkOBNugqs01If78vjWowLhCCowXwEXkJCHuST_xZVWQApkTScjld0Z8sDgxwU_R6AwWsKPB4gbiKYW--3TkWtcpZGV4N2skRt7tW2XvxBi5VRjWQKZcqigp8Ewp18q6MrZQUUq_Y09300z8muVgiVskef70PibPTlUGGb08V_w/s1600/meet-the-parents-original-300x189.jpg" width="300" /></a></div><br />Many years ago I was asked to serve on an advisory board for an organization that did confidential research. To be on the board I had to have US top secret clearance. The first step was filling out a lengthy form which asked deep details about every aspect of my life. Then there were the interviews with myself and many people I interacted with, especially internationally, and I have many international colleagues. Eventually I got passed these steps.<div><br /></div><div>The final step required taking a polygraph (lie-detector) test. So I flew to Baltimore to visit a non-descript office building near the airport. I failed the test. Twice more I went to Baltimore and failed those tests as well.&nbsp;</div><div><br /></div><div>Just to be clear, I never tried to falsify or hide information on these tests. In one case I was asked "Do you live in Atlanta?" I said no. The person administering the test stopped and said I put my address down as Atlanta. I said my mailing address was Atlanta but (at the time) I lived just north of the border in Sandy Springs. She said I should use Atlanta for the test, in other words I should lie. The test didn't go well after that.</div><div><br /></div><div>In another case, I was asked if I was ever arrested. For the record, I have never been arrested but the answer came up as inconclusive. The administrator, different than before, trusted the machine more than me and the rest of the day didn't go well.&nbsp;</div><div><br /></div><div>Perhaps the test wasn't meant to just test whether I was telling the truth, but also my ability to keep a secret. At least that would make more sense why I failed three times. More likely I took questions too literally, a product of a mathematician's mind.</div><div><br /></div><div>I never joined the advisory board but that wasn't the worst of it. In 2014 the Chinese hacked into the US Office of Personnel Management taking information from, among others, those who applied for security clearance. It's the main reason I keep security freezes with the credit bureaus.</div><p class="authors">By Lance Fortnow</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-13T13:34:00Z">Thursday, October 13 2022, 13:34</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.05885'>Unitary property testing lower bounds by polynomials</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Adrian She, Henry Yuen</p><p>We study unitary property testing, where a quantum algorithm is given query
access to a black-box unitary and has to decide whether it satisfies some
property. In addition to containing the standard quantum query complexity model
(where the unitary encodes a binary string) as a special case, this model
contains "inherently quantum" problems that have no classical analogue.
Characterizing the query complexity of these problems requires new algorithmic
techniques and lower bound methods.
</p>
<p>Our main contribution is a generalized polynomial method for unitary property
testing problems. By leveraging connections with invariant theory, we apply
this method to obtain lower bounds on problems such as determining recurrence
times of unitaries, approximating the dimension of a marked subspace, and
approximating the entanglement entropy of a marked state. We also present a
unitary property testing-based approach towards an oracle separation between
$\mathsf{QMA}$ and $\mathsf{QMA(2)}$, a long standing question in quantum
complexity theory.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+She_A/0/1/0/all/0/1">Adrian She</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Yuen_H/0/1/0/all/0/1">Henry Yuen</a></p><p>We study unitary property testing, where a quantum algorithm is given query
access to a black-box unitary and has to decide whether it satisfies some
property. In addition to containing the standard quantum query complexity model
(where the unitary encodes a binary string) as a special case, this model
contains "inherently quantum" problems that have no classical analogue.
Characterizing the query complexity of these problems requires new algorithmic
techniques and lower bound methods.
</p>
<p>Our main contribution is a generalized polynomial method for unitary property
testing problems. By leveraging connections with invariant theory, we apply
this method to obtain lower bounds on problems such as determining recurrence
times of unitaries, approximating the dimension of a marked subspace, and
approximating the entanglement entropy of a marked state. We also present a
unitary property testing-based approach towards an oracle separation between
$\mathsf{QMA}$ and $\mathsf{QMA(2)}$, a long standing question in quantum
complexity theory.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-13T00:30:00Z">Thursday, October 13 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.05788'>A Note on Reachability and Distance Oracles for Transmission Graphs</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Mark de Berg</p><p>Let $P$ be a set of $n$ points in the plane, where each point $p\in P$ has a
transmission radius $r(p)&gt;0$. The transmission graph defined by $P$ and the
given radii, denoted by $\mathcal{G}_{\mathrm{tr}}(P)$, is the directed graph
whose nodes are the points in $P$ and that contains the arcs $(p,q)$ such that
$|pq|\leq r(p)$.
</p>
<p>An and Oh [Algorithmica 2022] presented a reachability oracle for
transmission graphs. Their oracle uses $O(n^{5/3})$ storage and, given two
query points $s,t\in P$, can decide in $O(n^{2/3})$ time if there is a path
from $s$ to $t$ in $\mathcal{G}_{\mathrm{tr}}(P)$. We show that the
clique-based separators introduced by De Berg \emph{et al.} [SICOMP 2020] can
be used to improve the storage of the oracle to $O(n\sqrt{n})$ and the query
time to $O(\sqrt{n})$. Our oracle can be extended to approximate distance
queries: we can construct, for a given parameter $\varepsilon&gt;0$, an oracle
that uses $O((n/\varepsilon)\sqrt{n}\log n)$ storage and that can report in
$O((\sqrt{n}/\varepsilon)\log n)$ time a value $d_{\mathrm{hop}}^*(s,t)$
satisfying $d_{\mathrm{hop}}(s,t) \leq d_{\mathrm{hop}}^*(s,t) &lt;
(1+\varepsilon)\cdot d_{\mathrm{hop}}(s,t) + 1$, where $d_{\mathrm{hop}}(s,t)$
is the hop-distance from $s$ to $t$. We also show how to extend the oracle to
so-called continuous queries, where the target point $t$ can be any point in
the plane.
</p>
<p>To obtain an efficient preprocessing algorithm, we show that a clique-based
separator of a set~$F$ of convex fat objects in $\Bbb{R}^d$ can be constructed
in $O(n\log n)$ time.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Berg_M/0/1/0/all/0/1">Mark de Berg</a></p><p>Let $P$ be a set of $n$ points in the plane, where each point $p\in P$ has a
transmission radius $r(p)&gt;0$. The transmission graph defined by $P$ and the
given radii, denoted by $\mathcal{G}_{\mathrm{tr}}(P)$, is the directed graph
whose nodes are the points in $P$ and that contains the arcs $(p,q)$ such that
$|pq|\leq r(p)$.
</p>
<p>An and Oh [Algorithmica 2022] presented a reachability oracle for
transmission graphs. Their oracle uses $O(n^{5/3})$ storage and, given two
query points $s,t\in P$, can decide in $O(n^{2/3})$ time if there is a path
from $s$ to $t$ in $\mathcal{G}_{\mathrm{tr}}(P)$. We show that the
clique-based separators introduced by De Berg \emph{et al.} [SICOMP 2020] can
be used to improve the storage of the oracle to $O(n\sqrt{n})$ and the query
time to $O(\sqrt{n})$. Our oracle can be extended to approximate distance
queries: we can construct, for a given parameter $\varepsilon&gt;0$, an oracle
that uses $O((n/\varepsilon)\sqrt{n}\log n)$ storage and that can report in
$O((\sqrt{n}/\varepsilon)\log n)$ time a value $d_{\mathrm{hop}}^*(s,t)$
satisfying $d_{\mathrm{hop}}(s,t) \leq d_{\mathrm{hop}}^*(s,t) &lt;
(1+\varepsilon)\cdot d_{\mathrm{hop}}(s,t) + 1$, where $d_{\mathrm{hop}}(s,t)$
is the hop-distance from $s$ to $t$. We also show how to extend the oracle to
so-called continuous queries, where the target point $t$ can be any point in
the plane.
</p>
<p>To obtain an efficient preprocessing algorithm, we show that a clique-based
separator of a set~$F$ of convex fat objects in $\Bbb{R}^d$ can be constructed
in $O(n\log n)$ time.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-13T00:30:00Z">Thursday, October 13 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.06333'>Pattern Characterization Using Topological Data Analysis: Application to Piezo Vibration Striking Treatment</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Max M. Chumley, Melih C. Yesilli, Jisheng Chen, Firas A. Khasawneh, Yang Guo</p><p>Quantifying patterns in visual or tactile textures provides important
information about the process or phenomena that generated these patterns. In
manufacturing, these patterns can be intentionally introduced as a design
feature, or they can be a byproduct of a specific process. Since surface
texture has significant impact on the mechanical properties and the longevity
of the workpiece, it is important to develop tools for quantifying surface
patterns and, when applicable, comparing them to their nominal counterparts.
While existing tools may be able to indicate the existence of a pattern, they
typically do not provide more information about the pattern structure, or how
much it deviates from a nominal pattern. Further, prior works do not provide
automatic or algorithmic approaches for quantifying other pattern
characteristics such as depths' consistency, and variations in the pattern
motifs at different level sets. This paper leverages persistent homology from
Topological Data Analysis (TDA) to derive noise-robust scores for quantifying
motifs' depth and roundness in a pattern. Specifically, sublevel persistence is
used to derive scores that quantify the consistency of indentation depths at
any level set in Piezo Vibration Striking Treatment (PVST) surfaces. Moreover,
we combine sublevel persistence with the distance transform to quantify the
consistency of the indentation radii, and to compare them with the nominal
ones. Although the tool in our PVST experiments had a semi-spherical profile,
we present a generalization of our approach to tools/motifs of arbitrary shapes
thus making our method applicable to other pattern-generating manufacturing
processes.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chumley_M/0/1/0/all/0/1">Max M. Chumley</a>, <a href="http://arxiv.org/find/cs/1/au:+Yesilli_M/0/1/0/all/0/1">Melih C. Yesilli</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jisheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Khasawneh_F/0/1/0/all/0/1">Firas A. Khasawneh</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yang Guo</a></p><p>Quantifying patterns in visual or tactile textures provides important
information about the process or phenomena that generated these patterns. In
manufacturing, these patterns can be intentionally introduced as a design
feature, or they can be a byproduct of a specific process. Since surface
texture has significant impact on the mechanical properties and the longevity
of the workpiece, it is important to develop tools for quantifying surface
patterns and, when applicable, comparing them to their nominal counterparts.
While existing tools may be able to indicate the existence of a pattern, they
typically do not provide more information about the pattern structure, or how
much it deviates from a nominal pattern. Further, prior works do not provide
automatic or algorithmic approaches for quantifying other pattern
characteristics such as depths' consistency, and variations in the pattern
motifs at different level sets. This paper leverages persistent homology from
Topological Data Analysis (TDA) to derive noise-robust scores for quantifying
motifs' depth and roundness in a pattern. Specifically, sublevel persistence is
used to derive scores that quantify the consistency of indentation depths at
any level set in Piezo Vibration Striking Treatment (PVST) surfaces. Moreover,
we combine sublevel persistence with the distance transform to quantify the
consistency of the indentation radii, and to compare them with the nominal
ones. Although the tool in our PVST experiments had a semi-spherical profile,
we present a generalization of our approach to tools/motifs of arbitrary shapes
thus making our method applicable to other pattern-generating manufacturing
processes.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-13T00:30:00Z">Thursday, October 13 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.05893'>The Power of Two Matrices in Spectral Algorithms</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Souvik Dhara, Julia Gaudio, Elchanan Mossel, Colin Sandon</p><p>Spectral algorithms are some of the main tools in optimization and inference
problems on graphs. Typically, the graph is encoded as a matrix and
eigenvectors and eigenvalues of the matrix are then used to solve the given
graph problem. Spectral algorithms have been successfully used for graph
partitioning, hidden clique recovery and graph coloring. In this paper, we
study the power of spectral algorithms using two matrices in a graph
partitioning problem. We use two different matrices resulting from two
different encodings of the same graph and then combine the spectral information
coming from these two matrices.
</p>
<p>We analyze a two matrix spectral algorithm for the problem of identifying
latent community structure in large random graphs. In particular, we consider
the problem of recovering community assignments exactly in the censored
stochastic block model, where each edge status is revealed independently with
some probability. We show that spectral algorithms based on two matrices are
optimal and succeed in recovering communities up to the information theory
threshold. On the other hand, we show that for most choices of the parameters,
any spectral algorithm based on one matrix is suboptimal. This is in contrast
to our prior works (2022a, 2022b) which showed that for the symmetric
Stochastic Block Model and the Planted Dense Subgraph problem, spectral
algorithm based on one matrix achieve the information theory threshold. Of
independent interest, we provide more general geometric conditions for the
(sub)-optimality of spectral algorithms, that are also applicable to cases when
there are more than two communities.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Dhara_S/0/1/0/all/0/1">Souvik Dhara</a>, <a href="http://arxiv.org/find/math/1/au:+Gaudio_J/0/1/0/all/0/1">Julia Gaudio</a>, <a href="http://arxiv.org/find/math/1/au:+Mossel_E/0/1/0/all/0/1">Elchanan Mossel</a>, <a href="http://arxiv.org/find/math/1/au:+Sandon_C/0/1/0/all/0/1">Colin Sandon</a></p><p>Spectral algorithms are some of the main tools in optimization and inference
problems on graphs. Typically, the graph is encoded as a matrix and
eigenvectors and eigenvalues of the matrix are then used to solve the given
graph problem. Spectral algorithms have been successfully used for graph
partitioning, hidden clique recovery and graph coloring. In this paper, we
study the power of spectral algorithms using two matrices in a graph
partitioning problem. We use two different matrices resulting from two
different encodings of the same graph and then combine the spectral information
coming from these two matrices.
</p>
<p>We analyze a two matrix spectral algorithm for the problem of identifying
latent community structure in large random graphs. In particular, we consider
the problem of recovering community assignments exactly in the censored
stochastic block model, where each edge status is revealed independently with
some probability. We show that spectral algorithms based on two matrices are
optimal and succeed in recovering communities up to the information theory
threshold. On the other hand, we show that for most choices of the parameters,
any spectral algorithm based on one matrix is suboptimal. This is in contrast
to our prior works (2022a, 2022b) which showed that for the symmetric
Stochastic Block Model and the Planted Dense Subgraph problem, spectral
algorithm based on one matrix achieve the information theory threshold. Of
independent interest, we provide more general geometric conditions for the
(sub)-optimality of spectral algorithms, that are also applicable to cases when
there are more than two communities.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-13T00:30:00Z">Thursday, October 13 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.05965'>Resolving the Approximability of Offline and Online Non-monotone DR-Submodular Maximization over General Convex Sets</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Loay Mualem, Moran Feldman</p><p>In recent years, maximization of DR-submodular continuous functions became an
important research field, with many real-worlds applications in the domains of
machine learning, communication systems, operation research and economics. Most
of the works in this field study maximization subject to down-closed convex set
constraints due to an inapproximability result by Vondr\'ak (2013). However,
Durr et al. (2021) showed that one can bypass this inapproximability by proving
approximation ratios that are functions of $m$, the minimum
$\ell_{\infty}$-norm of any feasible vector. Given this observation, it is
possible to get results for maximizing a DR-submodular function subject to
general convex set constraints, which has led to multiple works on this
problem. The most recent of which is a polynomial time $\tfrac{1}{4}(1 -
m)$-approximation offline algorithm due to Du (2022). However, only a
sub-exponential time $\tfrac{1}{3\sqrt{3}}(1 - m)$-approximation algorithm is
known for the corresponding online problem. In this work, we present a
polynomial time online algorithm matching the $\tfrac{1}{4}(1 -
m)$-approximation of the state-of-the-art offline algorithm. We also present an
inapproximability result showing that our online algorithm and Du's (2022)
offline algorithm are both optimal in a strong sense. Finally, we study the
empirical performance of our algorithm and the algorithm of Du (which was only
theoretically studied previously), and show that they consistently outperform
previously suggested algorithms on revenue maximization, location summarization
and quadratic programming applications.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Mualem_L/0/1/0/all/0/1">Loay Mualem</a>, <a href="http://arxiv.org/find/cs/1/au:+Feldman_M/0/1/0/all/0/1">Moran Feldman</a></p><p>In recent years, maximization of DR-submodular continuous functions became an
important research field, with many real-worlds applications in the domains of
machine learning, communication systems, operation research and economics. Most
of the works in this field study maximization subject to down-closed convex set
constraints due to an inapproximability result by Vondr\'ak (2013). However,
Durr et al. (2021) showed that one can bypass this inapproximability by proving
approximation ratios that are functions of $m$, the minimum
$\ell_{\infty}$-norm of any feasible vector. Given this observation, it is
possible to get results for maximizing a DR-submodular function subject to
general convex set constraints, which has led to multiple works on this
problem. The most recent of which is a polynomial time $\tfrac{1}{4}(1 -
m)$-approximation offline algorithm due to Du (2022). However, only a
sub-exponential time $\tfrac{1}{3\sqrt{3}}(1 - m)$-approximation algorithm is
known for the corresponding online problem. In this work, we present a
polynomial time online algorithm matching the $\tfrac{1}{4}(1 -
m)$-approximation of the state-of-the-art offline algorithm. We also present an
inapproximability result showing that our online algorithm and Du's (2022)
offline algorithm are both optimal in a strong sense. Finally, we study the
empirical performance of our algorithm and the algorithm of Du (which was only
theoretically studied previously), and show that they consistently outperform
previously suggested algorithms on revenue maximization, location summarization
and quadratic programming applications.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-13T00:30:00Z">Thursday, October 13 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.05974'>Clustering Embedding Tables, Without First Learning Them</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Henry Ling-Hei Tsang, Thomas Dybdahl Ahle</p><p>To work with categorical features, machine learning systems employ embedding
tables. These tables can become exceedingly large in modern recommendation
systems, necessitating the development of new methods for fitting them in
memory, even during training.
</p>
<p>Some of the most successful methods for table compression are Product- and
Residual Vector Quantization (Gray &amp; Neuhoff, 1998). These methods replace
table rows with references to k-means clustered "codewords." Unfortunately,
this means they must first know the table before compressing it, so they can
only save memory during inference, not training. Recent work has used
hashing-based approaches to minimize memory usage during training, but the
compression obtained is inferior to that obtained by "post-training"
quantization.
</p>
<p>We show that the best of both worlds may be obtained by combining techniques
based on hashing and clustering. By first training a hashing-based "sketch",
then clustering it, and then training the clustered quantization, our method
achieves compression ratios close to those of post-training quantization with
the training time memory reductions of hashing-based methods.
</p>
<p>We show experimentally that our method provides better compression and/or
accuracy that previous methods, and we prove that our method always converges
to the optimal embedding table for least-squares training.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Tsang_H/0/1/0/all/0/1">Henry Ling-Hei Tsang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahle_T/0/1/0/all/0/1">Thomas Dybdahl Ahle</a></p><p>To work with categorical features, machine learning systems employ embedding
tables. These tables can become exceedingly large in modern recommendation
systems, necessitating the development of new methods for fitting them in
memory, even during training.
</p>
<p>Some of the most successful methods for table compression are Product- and
Residual Vector Quantization (Gray &amp; Neuhoff, 1998). These methods replace
table rows with references to k-means clustered "codewords." Unfortunately,
this means they must first know the table before compressing it, so they can
only save memory during inference, not training. Recent work has used
hashing-based approaches to minimize memory usage during training, but the
compression obtained is inferior to that obtained by "post-training"
quantization.
</p>
<p>We show that the best of both worlds may be obtained by combining techniques
based on hashing and clustering. By first training a hashing-based "sketch",
then clustering it, and then training the clustered quantization, our method
achieves compression ratios close to those of post-training quantization with
the training time memory reductions of hashing-based methods.
</p>
<p>We show experimentally that our method provides better compression and/or
accuracy that previous methods, and we prove that our method always converges
to the optimal embedding table for least-squares training.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-13T00:30:00Z">Thursday, October 13 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.05982'>A nearly optimal randomized algorithm for explorable heap selection</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Sander Borst, Daniel Dadush, Sophie Huiberts, Danish Kashaev</p><p>Explorable heap selection is the problem of selecting the $n$th smallest
value in a binary heap. The key values can only be accessed by traversing
through the underlying infinite binary tree, and the complexity of the
algorithm is measured by the total distance traveled in the tree (each edge has
unit cost). This problem was originally proposed as a model to study search
strategies for the branch-and-bound algorithm with storage restrictions by
Karp, Saks and Widgerson (FOCS '86), who gave deterministic and randomized
$n\cdot \exp(O(\sqrt{\log{n}}))$ time algorithms using $O(\log(n)^{2.5})$ and
$O(\sqrt{\log n})$ space respectively. We present a new randomized algorithm
with running time $O(n\log(n)^3)$ using $O(\log n)$ space, substantially
improving the previous best randomized running time at the expense of slightly
increased space usage. We also show an $\Omega(\log(n)n/\log(\log(n)))$ for any
algorithm that solves the problem in the same amount of space, indicating that
our algorithm is nearly optimal.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Borst_S/0/1/0/all/0/1">Sander Borst</a>, <a href="http://arxiv.org/find/cs/1/au:+Dadush_D/0/1/0/all/0/1">Daniel Dadush</a>, <a href="http://arxiv.org/find/cs/1/au:+Huiberts_S/0/1/0/all/0/1">Sophie Huiberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Kashaev_D/0/1/0/all/0/1">Danish Kashaev</a></p><p>Explorable heap selection is the problem of selecting the $n$th smallest
value in a binary heap. The key values can only be accessed by traversing
through the underlying infinite binary tree, and the complexity of the
algorithm is measured by the total distance traveled in the tree (each edge has
unit cost). This problem was originally proposed as a model to study search
strategies for the branch-and-bound algorithm with storage restrictions by
Karp, Saks and Widgerson (FOCS '86), who gave deterministic and randomized
$n\cdot \exp(O(\sqrt{\log{n}}))$ time algorithms using $O(\log(n)^{2.5})$ and
$O(\sqrt{\log n})$ space respectively. We present a new randomized algorithm
with running time $O(n\log(n)^3)$ using $O(\log n)$ space, substantially
improving the previous best randomized running time at the expense of slightly
increased space usage. We also show an $\Omega(\log(n)n/\log(\log(n)))$ for any
algorithm that solves the problem in the same amount of space, indicating that
our algorithm is nearly optimal.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-13T00:30:00Z">Thursday, October 13 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.05992'>Fast Convergence to Unanimity in Dense Erd\H{o}s-R\'enyi Graphs</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Ran Tamir</p><p>Majority dynamics on the binomial Erd\H{o}s-R\'enyi graph $\mathsf{G}(n,p)$
with $p=\lambda/\sqrt{n}$ is studied. In this process, each vertex has a state
in $\{0,1\}$ and at each round, every vertex adopts the state of the majority
of its neighbors, retaining its state in the case of a tie. It was conjectured
by Benjamini et al. and proved by Fountoulakis et al. that this process reaches
unanimity with high probability in at most four rounds. By adding some extra
randomness and allowing the underlying graph to be drawn anew in each
communication round, we improve on their result and prove that this process
reaches consensus in only three communication rounds with probability
approaching $1$ as $n$ grows to infinity. We also provide a converse result,
showing that three rounds are not only sufficient, but also necessary.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Tamir_R/0/1/0/all/0/1">Ran Tamir</a></p><p>Majority dynamics on the binomial Erd\H{o}s-R\'enyi graph $\mathsf{G}(n,p)$
with $p=\lambda/\sqrt{n}$ is studied. In this process, each vertex has a state
in $\{0,1\}$ and at each round, every vertex adopts the state of the majority
of its neighbors, retaining its state in the case of a tie. It was conjectured
by Benjamini et al. and proved by Fountoulakis et al. that this process reaches
unanimity with high probability in at most four rounds. By adding some extra
randomness and allowing the underlying graph to be drawn anew in each
communication round, we improve on their result and prove that this process
reaches consensus in only three communication rounds with probability
approaching $1$ as $n$ grows to infinity. We also provide a converse result,
showing that three rounds are not only sufficient, but also necessary.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-13T00:30:00Z">Thursday, October 13 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.06061'>Non-smooth and H\"older-smooth Submodular Maximization</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Duksang Lee, Nam Ho-Nguyen, Dabeen Lee</p><p>We study the problem of maximizing a continuous DR-submodular function that
is not necessarily smooth. We prove that the continuous greedy algorithm
achieves an $[(1-1/e)\text{OPT}-\epsilon]$ guarantee when the function is
monotone and H\"older-smooth, meaning that it admits a H\"older-continuous
gradient. For functions that are non-differentiable or non-smooth, we propose a
variant of the mirror-prox algorithm that attains an
$[(1/2)\text{OPT}-\epsilon]$ guarantee. We apply our algorithmic frameworks to
robust submodular maximization and distrbituionally robust submodular
maximization under Wasserstein ambiguity. In particular, the mirror-prox method
applies to robust submodular maximization to obtain a single feasible solution
whose value is at least $(1/2)\text{OPT}-\epsilon$. For distributionally robust
maximization under Wasserstein ambiguity, we deduce and work over a
submodular-convex maximin reformulation whose objective function is
H\"older-smooth, for which we may apply both the continuous greedy method and
the mirror-prox method.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Lee_D/0/1/0/all/0/1">Duksang Lee</a>, <a href="http://arxiv.org/find/math/1/au:+Ho_Nguyen_N/0/1/0/all/0/1">Nam Ho-Nguyen</a>, <a href="http://arxiv.org/find/math/1/au:+Lee_D/0/1/0/all/0/1">Dabeen Lee</a></p><p>We study the problem of maximizing a continuous DR-submodular function that
is not necessarily smooth. We prove that the continuous greedy algorithm
achieves an $[(1-1/e)\text{OPT}-\epsilon]$ guarantee when the function is
monotone and H\"older-smooth, meaning that it admits a H\"older-continuous
gradient. For functions that are non-differentiable or non-smooth, we propose a
variant of the mirror-prox algorithm that attains an
$[(1/2)\text{OPT}-\epsilon]$ guarantee. We apply our algorithmic frameworks to
robust submodular maximization and distrbituionally robust submodular
maximization under Wasserstein ambiguity. In particular, the mirror-prox method
applies to robust submodular maximization to obtain a single feasible solution
whose value is at least $(1/2)\text{OPT}-\epsilon$. For distributionally robust
maximization under Wasserstein ambiguity, we deduce and work over a
submodular-convex maximin reformulation whose objective function is
H\"older-smooth, for which we may apply both the continuous greedy method and
the mirror-prox method.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-13T00:30:00Z">Thursday, October 13 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.06140'>Differentially Private Bootstrap: New Privacy Analysis and Inference Strategies</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Zhanyu Wang, Guang Cheng, Jordan Awan</p><p>Differential private (DP) mechanisms protect individual-level information by
introducing randomness into the statistical analysis procedure. While there are
now many DP tools for various statistical problems, there is still a lack of
general techniques to understand the sampling distribution of a DP estimator,
which is crucial for uncertainty quantification in statistical inference. We
analyze a DP bootstrap procedure that releases multiple private bootstrap
estimates to infer the sampling distribution and construct confidence
intervals. Our privacy analysis includes new results on the privacy cost of a
single DP bootstrap estimate applicable to incorporate arbitrary DP mechanisms
and identifies some misuses of the bootstrap in the existing literature. We
show that the release of $B$ DP bootstrap estimates from mechanisms satisfying
$(\mu/\sqrt{(2-2/\mathrm{e})B})$-Gaussian DP asymptotically satisfies
$\mu$-Gaussian DP as $B$ goes to infinity. We also develop a statistical
procedure based on the DP bootstrap estimates to correctly infer the sampling
distribution using techniques related to the deconvolution of probability
measures, an approach which is novel in analyzing DP procedures. From our
density estimate, we construct confidence intervals and compare them to
existing methods through simulations and real-world experiments using the 2016
Canada Census Public Use Microdata. The coverage of our private confidence
intervals achieves the nominal confidence level, while other methods fail to
meet this guarantee.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1">Zhanyu Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Cheng_G/0/1/0/all/0/1">Guang Cheng</a>, <a href="http://arxiv.org/find/stat/1/au:+Awan_J/0/1/0/all/0/1">Jordan Awan</a></p><p>Differential private (DP) mechanisms protect individual-level information by
introducing randomness into the statistical analysis procedure. While there are
now many DP tools for various statistical problems, there is still a lack of
general techniques to understand the sampling distribution of a DP estimator,
which is crucial for uncertainty quantification in statistical inference. We
analyze a DP bootstrap procedure that releases multiple private bootstrap
estimates to infer the sampling distribution and construct confidence
intervals. Our privacy analysis includes new results on the privacy cost of a
single DP bootstrap estimate applicable to incorporate arbitrary DP mechanisms
and identifies some misuses of the bootstrap in the existing literature. We
show that the release of $B$ DP bootstrap estimates from mechanisms satisfying
$(\mu/\sqrt{(2-2/\mathrm{e})B})$-Gaussian DP asymptotically satisfies
$\mu$-Gaussian DP as $B$ goes to infinity. We also develop a statistical
procedure based on the DP bootstrap estimates to correctly infer the sampling
distribution using techniques related to the deconvolution of probability
measures, an approach which is novel in analyzing DP procedures. From our
density estimate, we construct confidence intervals and compare them to
existing methods through simulations and real-world experiments using the 2016
Canada Census Public Use Microdata. The coverage of our private confidence
intervals achieves the nominal confidence level, while other methods fail to
meet this guarantee.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-13T00:30:00Z">Thursday, October 13 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.06227'>Quantum Optimisation for Continuous Multivariable Functions by a Structured Search</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Edric Matwiejew, Jason Pye, Jingbo B. Wang</p><p>Solving optimisation problems is a promising near-term application of quantum
computers. Quantum variational algorithms leverage quantum superposition and
entanglement to optimise over exponentially large solution spaces using an
alternating sequence of classically tunable unitaries. However, prior work has
primarily addressed discrete optimisation problems. In addition, these
algorithms have been designed generally under the assumption of an unstructured
solution space, which constrains their speedup to the theoretical limits for
the unstructured Grover's quantum search algorithm. In this paper, we show that
quantum variational algorithms can efficiently optimise continuous
multivariable functions by exploiting general structural properties of a
discretised continuous solution space with a convergence that exceeds the
limits of an unstructured quantum search. We introduce the Quantum
Multivariable Optimisation Algorithm (QMOA) and demonstrate its advantage over
pre-existing methods, particularly when optimising high-dimensional and
oscillatory functions.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Matwiejew_E/0/1/0/all/0/1">Edric Matwiejew</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Pye_J/0/1/0/all/0/1">Jason Pye</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Wang_J/0/1/0/all/0/1">Jingbo B. Wang</a></p><p>Solving optimisation problems is a promising near-term application of quantum
computers. Quantum variational algorithms leverage quantum superposition and
entanglement to optimise over exponentially large solution spaces using an
alternating sequence of classically tunable unitaries. However, prior work has
primarily addressed discrete optimisation problems. In addition, these
algorithms have been designed generally under the assumption of an unstructured
solution space, which constrains their speedup to the theoretical limits for
the unstructured Grover's quantum search algorithm. In this paper, we show that
quantum variational algorithms can efficiently optimise continuous
multivariable functions by exploiting general structural properties of a
discretised continuous solution space with a convergence that exceeds the
limits of an unstructured quantum search. We introduce the Quantum
Multivariable Optimisation Algorithm (QMOA) and demonstrate its advantage over
pre-existing methods, particularly when optimising high-dimensional and
oscillatory functions.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-13T00:30:00Z">Thursday, October 13 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Wednesday, October 12
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://scottaaronson.blog/?p=6754'>Explanation-Gödel and Plausibility-Gödel</a></h3>
          <p class='item-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          Here&#8217;s an observation that&#8217;s mathematically trivial but might not be widely appreciated. In kindergarten, we all learned Gödel&#8217;s First Incompleteness Theorem, which given a formal system F, constructs an arithmetical encoding of G(F) = &#8220;This sentence is not provable in F.&#8221; If G(F) is true, then it&#8217;s an example of a true arithmetical sentence that&#8217;s [&#8230;]
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p>Here&#8217;s an observation that&#8217;s mathematically trivial but might not be widely appreciated.  In kindergarten, we all learned Gödel&#8217;s First <a href="https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems">Incompleteness Theorem</a>, which given a formal system F, constructs an arithmetical encoding of</p>



<p>G(F) = &#8220;This sentence is not provable in F.&#8221;</p>



<p>If G(F) is true, then it&#8217;s an example of a true arithmetical sentence that&#8217;s unprovable in F.  If, on the other hand, G(F) is false, then it&#8217;s provable, which means that F isn&#8217;t arithmetically sound.  Therefore F is either incomplete or unsound.</p>



<p>Many have objected: &#8220;but despite Gödel&#8217;s Theorem, it&#8217;s still easy to <em>explain </em>why G(F) is true.  In fact, the argument above basically already did it!”</p>



<p>[Note: Please stop leaving comments explaining to me that G(F) follows from F’s consistency.  I understand that: the “heuristic” part of the argument <em>is</em> F’s consistency!  I made a pedagogical choice to elide that, which nerd-sniping has now rendered untenable.]</p>



<p>You might make a more general point: there are many, many mathematical statements for which we currently lack a proof, but we do seem to have a fully convincing heuristic explanation: one that &#8220;proves the statement to physics standards of rigor.&#8221;  For example:</p>



<ul><li>The <a href="https://en.wikipedia.org/wiki/Twin_prime">Twin Primes Conjecture</a> (there are infinitely many primes p for which p+2 is also prime).  </li><li>The <a href="https://en.wikipedia.org/wiki/Collatz_conjecture">Collatz Conjecture</a> (the iterative process that maps each positive integer n to n/2 if n is even, or to 3n+1 if n is odd, eventually reaches 1 regardless of which n you start at).  </li><li>π is a <a href="https://en.wikipedia.org/wiki/Normal_number">normal number</a> (or even just: the digits 0-9 all occur with equal limiting frequencies in the decimal expansion of π).</li><li><a href="https://math.stackexchange.com/questions/159350/why-is-it-hard-to-prove-whether-pie-is-an-irrational-number">π+e</a> is irrational.</li></ul>



<p>And so on.  No one has any idea how to prove any of the above statements&#8212;and yet, just on statistical grounds, it seems clear that it would require a ludicrous conspiracy to make any of them false.</p>



<p>Conversely, one could argue that there are statements for which we <em>do</em> have a proof, even though we lack a &#8220;convincing explanation&#8221; for the statements&#8217; truth.  Maybe the <a href="https://en.wikipedia.org/wiki/Four_color_theorem">Four-Color Theorem</a> or <a href="https://en.wikipedia.org/wiki/Kepler_conjecture">Hales&#8217;s Theorem</a>, for which every known proof requires a massive computer enumeration of cases, belong to this class.  Other people might argue that, given a proof, an explanation could always be extracted with enough time and effort, though resolving this dispute won&#8217;t matter for what follows.</p>



<p>You might hope that, even if some true mathematical statements can&#8217;t be <em>proved</em>, every true statement might nevertheless have a <em>convincing heuristic explanation</em>.  Alas, a trivial adaptation of Gödel&#8217;s Theorem shows that, if (1) heuristic explanations are to be checkable by computer, and (2) only true statements are to have convincing heuristic explanations, then this isn&#8217;t possible either.  I mean, let E be a program that accepts or rejects proposed heuristic explanations, for statements like the Twin Prime Conjecture or the Collatz Conjecture.  Then construct the sentence</p>



<p>S(E) = &#8220;This sentence has no convincing heuristic explanation accepted by E.&#8221;</p>



<p>If S(E) is true, then it&#8217;s an example of a true arithmetical statement without <em>even</em> a convincing heuristic explanation for its truth (!).  If, on the other hand, S(E) is false, then there&#8217;s a convincing heuristic explanation of its truth, which means that something has gone wrong.</p>



<p>What&#8217;s happening, of course, is that given the two conditions we imposed, our &#8220;heuristic explanation system&#8221; <em>was</em> a proof system, even though we didn&#8217;t call it one.  This is my point, though: when we use the word &#8220;proof,&#8221; it normally invokes a specific image, of a sequence of statements that marches from axioms to a theorem, with each statement following from the preceding ones by rigid inference rules like those of first-order logic.  None of that, however, plays any direct role in the proof of the Incompleteness Theorem, which cares only about soundness (inability to prove falsehoods) and checkability by a computer (what, with hindsight, Gödel&#8217;s &#8220;arithmetization of syntax&#8221; was all about).  The logic works for &#8220;heuristic explanations&#8221; too.</p>



<p>Now we come to something that I picked up from my former student (and now AI alignment leader) <a href="https://paulfchristiano.com/">Paul Christiano</a>, on a recent trip to the Bay Area, and which I share with Paul&#8217;s kind permission.  Having learned that there&#8217;s no way to mechanize even heuristic explanations for all the true statements of arithmetic, we could set our sights lower still, and ask about mere <em>plausibility arguments</em>&#8212;arguments that might be overturned on further reflection.  Is there some sense in which every true mathematical statement at least has a good plausibility argument?</p>



<p>Maybe you see where this is going.  Letting P be a program that accepts or rejects proposed plausibility arguments, we can construct</p>



<p>S(P) = &#8220;This sentence has no argument for its plausibility accepted by P.&#8221;</p>



<p>If S(P) is true, then it&#8217;s an example of a true arithmetical statement without even a plausibility argument for its truth (!).  If, on the other hand, S(P) is false, then there <em>is</em> a plausibility argument for it.  By itself, this is <em>not at all</em> a fatal problem: all sorts of false statements (IP≠PSPACE, switching doors doesn&#8217;t matter in <a href="https://en.wikipedia.org/wiki/Monty_Hall_problem">Monty Hall</a>, Trump couldn&#8217;t possibly become president&#8230;) have had decent plausibility arguments.  Having said that, it&#8217;s pretty strange that you can have a plausibility argument that&#8217;s immediately contradicted by its own existence!  This rules out some properties that you might want your &#8220;plausibility system&#8221; to have, although maybe a plausibility system exists that&#8217;s still nontrivial and that has weaker properties.</p>



<p>Anyway, I don&#8217;t know where I&#8217;m going with this, or even why I posted it, but I hope you enjoyed it!  And maybe there&#8217;s something to be discovered in this direction.</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-12T21:52:31Z">Wednesday, October 12 2022, 21:52</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://cstheory-jobs.org/2022/10/12/postdoc-researcher-at-ens-paris-apply-by-october-31-2022/'>Postdoc researcher at ENS Paris (apply by October 31, 2022)</a></h3>
          <p class='item-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          The PARSe project funded by ANR (France) opens one postdoc position at ENS Paris, hosted by Tatiana Starikovskaya. The postdoc researcher will contribute efficient tools for processing large-scale, noisy string data. The post will require a high level of expertise in areas which may include but not be limited to string algorithms and streaming / [&#8230;]
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p>The PARSe project funded by ANR (France) opens one postdoc position at ENS Paris, hosted by Tatiana Starikovskaya. The postdoc researcher will contribute efficient tools for processing large-scale, noisy string data. The post will require a high level of expertise in areas which may include but not be limited to string algorithms and streaming / property testing algorithms.</p>
<p>Website: <a href="https://euraxess.ec.europa.eu/jobs/842793">https://euraxess.ec.europa.eu/jobs/842793</a><br />
Email: starikovskaya@di.ens.fr</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-12T14:44:31Z">Wednesday, October 12 2022, 14:44</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.05019'>Parameterized Approaches to Orthogonal Compaction</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Walter Didimo, Siddharth Gupta, Philipp Kindermann, Giuseppe Liotta, Alexander Wolff, Meirav Zehavi</p><p>Orthogonal graph drawings are used in applications such as UML diagrams, VLSI
layout, cable plans, and metro maps. We focus on drawing planar graphs and
assume that we are given an \emph{orthogonal representation} that describes the
desired shape, but not the exact coordinates of a drawing. Our aim is to
compute an orthogonal drawing on the grid that has minimum area among all grid
drawings that adhere to the given orthogonal representation.
</p>
<p>This problem is called orthogonal compaction (OC) and is known to be NP-hard,
even for orthogonal representations of cycles [Evans et al., 2022]. We
investigate the complexity of OC with respect to several parameters. Among
others, we show that OC is fixed-parameter tractable with respect to the most
natural of these parameters, namely, the number of \emph{kitty corners} of the
orthogonal representation: the presence of pairs of kitty corners in an
orthogonal representation makes the OC problem hard. Informally speaking, a
pair of kitty corners is a pair of reflex corners of a face that point at each
other. Accordingly, the number of kitty corners is the number of corners that
are involved in some pair of kitty corners.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Didimo_W/0/1/0/all/0/1">Walter Didimo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1">Siddharth Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Kindermann_P/0/1/0/all/0/1">Philipp Kindermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Liotta_G/0/1/0/all/0/1">Giuseppe Liotta</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolff_A/0/1/0/all/0/1">Alexander Wolff</a>, <a href="http://arxiv.org/find/cs/1/au:+Zehavi_M/0/1/0/all/0/1">Meirav Zehavi</a></p><p>Orthogonal graph drawings are used in applications such as UML diagrams, VLSI
layout, cable plans, and metro maps. We focus on drawing planar graphs and
assume that we are given an \emph{orthogonal representation} that describes the
desired shape, but not the exact coordinates of a drawing. Our aim is to
compute an orthogonal drawing on the grid that has minimum area among all grid
drawings that adhere to the given orthogonal representation.
</p>
<p>This problem is called orthogonal compaction (OC) and is known to be NP-hard,
even for orthogonal representations of cycles [Evans et al., 2022]. We
investigate the complexity of OC with respect to several parameters. Among
others, we show that OC is fixed-parameter tractable with respect to the most
natural of these parameters, namely, the number of \emph{kitty corners} of the
orthogonal representation: the presence of pairs of kitty corners in an
orthogonal representation makes the OC problem hard. Informally speaking, a
pair of kitty corners is a pair of reflex corners of a face that point at each
other. Accordingly, the number of kitty corners is the number of corners that
are involved in some pair of kitty corners.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-12T00:30:00Z">Wednesday, October 12 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.05093'>Crack Modeling via Minimum-Weight Surfaces in 3d Voronoi Diagrams</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Christian Jung, Claudia Redenbach</p><p>Shortest paths play an important role in mathematical modeling and image
processing. Usually, shortest path problems are formulated on planar graphs
that consist of vertices and weighted arcs. In this context, one is interested
in finding a path of minimum weight from a start vertex to an end vertex. The
concept of minimum-weight surfaces extends shortest paths to 3d. The
minimum-weight surface problem is formulated on a cellular complex with
weighted facets. A cycle on the arcs of the complex serves as input and one is
interested in finding a surface of minimum weight bounded by that cycle. In
practice, minimum-weight surfaces can be used to segment 3d images. Vice versa,
it is possible to use them as a modeling tool for geometric structures such as
cracks. In this work, we present an approach for using minimum-weight surfaces
in bounded Voronoi diagrams to generate synthetic 3d images of cracks.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1">Christian Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Redenbach_C/0/1/0/all/0/1">Claudia Redenbach</a></p><p>Shortest paths play an important role in mathematical modeling and image
processing. Usually, shortest path problems are formulated on planar graphs
that consist of vertices and weighted arcs. In this context, one is interested
in finding a path of minimum weight from a start vertex to an end vertex. The
concept of minimum-weight surfaces extends shortest paths to 3d. The
minimum-weight surface problem is formulated on a cellular complex with
weighted facets. A cycle on the arcs of the complex serves as input and one is
interested in finding a surface of minimum weight bounded by that cycle. In
practice, minimum-weight surfaces can be used to segment 3d images. Vice versa,
it is possible to use them as a modeling tool for geometric structures such as
cracks. In this work, we present an approach for using minimum-weight surfaces
in bounded Voronoi diagrams to generate synthetic 3d images of cracks.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-12T00:30:00Z">Wednesday, October 12 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.05124'>Persistence Diagram Bundles: A Multidimensional Generalization of Vineyards</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Abigail Hickok</p><p>A persistence diagram (PD) summarizes the persistent homology of a
filtration. I introduce the concept of a persistence diagram bundle, which is
the space of PDs associated with a fibered filtration function (a set $\{f_t:
\mathcal{K}^t \to \mathbb{R}\}_{t \in \mathcal{T}}$ of filtrations
parameterized by a topological space $\mathcal{T}$). Special cases include
vineyards, the persistent homology transform, and fibered barcodes of
multiparameter persistence modules. I prove that if $\mathcal{T}$ is a compact
$n$-dimensional manifold, then for generic fibered filtration functions,
$\mathcal{T}$ is stratified such that within each $n$-dimensional stratum $S$,
there is a single PD "template" (a list of birth and death simplices) that can
be used to obtain $PD(f_t)$ for any $t \in S$. I also show that not every local
section can be extended to a global section. Consequently, the points in the
PDs do not typically trace out separate manifolds as $t \in \mathcal{T}$
varies; this is unlike a vineyard, in which the points in the PDs trace out
curves ("vines").
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Hickok_A/0/1/0/all/0/1">Abigail Hickok</a></p><p>A persistence diagram (PD) summarizes the persistent homology of a
filtration. I introduce the concept of a persistence diagram bundle, which is
the space of PDs associated with a fibered filtration function (a set $\{f_t:
\mathcal{K}^t \to \mathbb{R}\}_{t \in \mathcal{T}}$ of filtrations
parameterized by a topological space $\mathcal{T}$). Special cases include
vineyards, the persistent homology transform, and fibered barcodes of
multiparameter persistence modules. I prove that if $\mathcal{T}$ is a compact
$n$-dimensional manifold, then for generic fibered filtration functions,
$\mathcal{T}$ is stratified such that within each $n$-dimensional stratum $S$,
there is a single PD "template" (a list of birth and death simplices) that can
be used to obtain $PD(f_t)$ for any $t \in S$. I also show that not every local
section can be extended to a global section. Consequently, the points in the
PDs do not typically trace out separate manifolds as $t \in \mathcal{T}$
varies; this is unlike a vineyard, in which the points in the PDs trace out
curves ("vines").
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-12T00:30:00Z">Wednesday, October 12 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.05384'>Morphing Planar Graph Drawings Through 3D</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Kevin Buchin, Will Evans, Fabrizio Frati, Irina Kostitsyna, Maarten L&#xf6;ffler, Tim Ophelders, Alexander Wolff</p><p>In this paper, we investigate crossing-free 3D morphs between planar
straight-line drawings. We show that, for any two (not necessarily
topologically equivalent) planar straight-line drawings of an $n$-vertex planar
graph, there exists a piecewise-linear crossing-free 3D morph with $O(n^2)$
steps that transforms one drawing into the other. We also give some evidence
why it is difficult to obtain a linear lower bound (which exists in 2D) for the
number of steps of a crossing-free 3D morph.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Buchin_K/0/1/0/all/0/1">Kevin Buchin</a>, <a href="http://arxiv.org/find/cs/1/au:+Evans_W/0/1/0/all/0/1">Will Evans</a>, <a href="http://arxiv.org/find/cs/1/au:+Frati_F/0/1/0/all/0/1">Fabrizio Frati</a>, <a href="http://arxiv.org/find/cs/1/au:+Kostitsyna_I/0/1/0/all/0/1">Irina Kostitsyna</a>, <a href="http://arxiv.org/find/cs/1/au:+Loffler_M/0/1/0/all/0/1">Maarten L&#xf6;ffler</a>, <a href="http://arxiv.org/find/cs/1/au:+Ophelders_T/0/1/0/all/0/1">Tim Ophelders</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolff_A/0/1/0/all/0/1">Alexander Wolff</a></p><p>In this paper, we investigate crossing-free 3D morphs between planar
straight-line drawings. We show that, for any two (not necessarily
topologically equivalent) planar straight-line drawings of an $n$-vertex planar
graph, there exists a piecewise-linear crossing-free 3D morph with $O(n^2)$
steps that transforms one drawing into the other. We also give some evidence
why it is difficult to obtain a linear lower bound (which exists in 2D) for the
number of steps of a crossing-free 3D morph.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-12T00:30:00Z">Wednesday, October 12 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.05403'>Hierarchical Categories in Colored Searching</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Peyman Afshani, Rasmus Killman, Kasper Green Larsen</p><p>In colored range counting (CRC), the input is a set of points where each
point is assigned a ``color'' (or a ``category'') and the goal is to store them
in a data structure such that the number of distinct categories inside a given
query range can be counted efficiently. CRC has strong motivations as it allows
data structure to deal with categorical data. However, colors (i.e., the
categories) in the CRC problem do not have any internal structure, whereas this
is not the case for many datasets in practice where hierarchical categories
exists or where a single input belongs to multiple categories. Motivated by
these, we consider variants of the problem where such structures can be
represented. We define two variants of the problem called hierarchical range
counting (HCC) and sub-category colored range counting (SCRC) and consider
hierarchical structures that can either be a DAG or a tree. We show that the
two problems on some special trees are in fact equivalent to other well-known
problems in the literature. Based on these, we also give efficient data
structures when the underlying hierarchy can be represented as a tree. We show
a conditional lower bound for the general case when the existing hierarchy can
be any DAG, through reductions from the orthogonal vectors problem.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Afshani_P/0/1/0/all/0/1">Peyman Afshani</a>, <a href="http://arxiv.org/find/cs/1/au:+Killman_R/0/1/0/all/0/1">Rasmus Killman</a>, <a href="http://arxiv.org/find/cs/1/au:+Larsen_K/0/1/0/all/0/1">Kasper Green Larsen</a></p><p>In colored range counting (CRC), the input is a set of points where each
point is assigned a ``color'' (or a ``category'') and the goal is to store them
in a data structure such that the number of distinct categories inside a given
query range can be counted efficiently. CRC has strong motivations as it allows
data structure to deal with categorical data. However, colors (i.e., the
categories) in the CRC problem do not have any internal structure, whereas this
is not the case for many datasets in practice where hierarchical categories
exists or where a single input belongs to multiple categories. Motivated by
these, we consider variants of the problem where such structures can be
represented. We define two variants of the problem called hierarchical range
counting (HCC) and sub-category colored range counting (SCRC) and consider
hierarchical structures that can either be a DAG or a tree. We show that the
two problems on some special trees are in fact equivalent to other well-known
problems in the literature. Based on these, we also give efficient data
structures when the underlying hierarchy can be represented as a tree. We show
a conditional lower bound for the general case when the existing hierarchy can
be any DAG, through reductions from the orthogonal vectors problem.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-12T00:30:00Z">Wednesday, October 12 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.05000'>A Hierarchical Grouping Algorithm for the Multi-Vehicle Dial-a-Ride Problem</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Kelin Luo, Alexandre M. Florio, Syamantak Das, Xiangyu Guo</p><p>Ride-sharing is an essential aspect of modern urban mobility. In this paper,
we consider a classical problem in ride-sharing - the Multi-Vehicle Dial-a-Ride
Problem (Multi-Vehicle DaRP). Given a fleet of vehicles with a fixed capacity
stationed at various locations and a set of ride requests specified by origins
and destinations, the goal is to serve all requests such that no vehicle is
assigned more passengers than its capacity at any point along its trip. We
propose an algorithm HRA, which is the first non-trivial approximation
algorithm for the Multi-Vehicle DaRP. The main technical contribution is to
reduce the Multi-Vehicle DaRP to a certain capacitated partitioning problem,
which we solve using a novel hierarchical grouping algorithm. Experimental
results show that the vehicle routes produced by our algorithm not only exhibit
less total travel distance compared to state-of-the-art baselines, but also
enjoy a small in-transit latency, which crucially relates to riders' traveling
times. This suggests that HRA enhances rider experience while being
energy-efficient.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1">Kelin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Florio_A/0/1/0/all/0/1">Alexandre M. Florio</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Syamantak Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xiangyu Guo</a></p><p>Ride-sharing is an essential aspect of modern urban mobility. In this paper,
we consider a classical problem in ride-sharing - the Multi-Vehicle Dial-a-Ride
Problem (Multi-Vehicle DaRP). Given a fleet of vehicles with a fixed capacity
stationed at various locations and a set of ride requests specified by origins
and destinations, the goal is to serve all requests such that no vehicle is
assigned more passengers than its capacity at any point along its trip. We
propose an algorithm HRA, which is the first non-trivial approximation
algorithm for the Multi-Vehicle DaRP. The main technical contribution is to
reduce the Multi-Vehicle DaRP to a certain capacitated partitioning problem,
which we solve using a novel hierarchical grouping algorithm. Experimental
results show that the vehicle routes produced by our algorithm not only exhibit
less total travel distance compared to state-of-the-art baselines, but also
enjoy a small in-transit latency, which crucially relates to riders' traveling
times. This suggests that HRA enhances rider experience while being
energy-efficient.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-12T00:30:00Z">Wednesday, October 12 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.05294'>Evaluation of the Quality of Exercises for the Data Structures' eTextbook and Find the Difficult Topics Using Item Response Theory and Logged Data Analysis</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Ahmed Abd Elrahman, Taysir Hassan A Soliman, Mohammed F. Farghally, Ahmed I. Taloba</p><p>The growing dependence on eTextbooks and Massive Open Online Courses (MOOCs)
has led to an increase in the amount of students' learning data. By carefully
analyzing this data, educators can identify difficult exercises, and evaluate
the quality of the exercises when teaching a particular topic. In this study,
an analysis of log data from the use of a semester of the OpenDSA eTextbook was
offered to identify the most difficult data structure course exercises and to
evaluate the quality of the course exercises. Our study is based on analyzing
students' responses to the course exercises. To identify the difficult
exercises, we applied two different approaches, the first of which involved
analyzing student responses to exercises using item response theory (IRT)
analysis and a latent trait model (LTM) technique, and the second involved
determining which exercises were more difficult based on how students
interacted with them. We computed different measures for every exercise such
that difficulty level, trial and error, and hint ratio. We generated an item
characteristics curve, item information curve, and test information function
for each exercise. To evaluate the quality of the exercises, we applied the IRT
analysis to the students' responses to the exercises and, we computed the
difficulty and discrimination index for each exercise. We classified whether
the exercise is good or poor based on these two measures. Our findings showed
that the exercises that related to algorithm analysis topics represented most
of the difficult exercises that students struggle with, and there existing six
exercises out of 56 exercises are classified as poor exercises which could be
rejected or improved. Some of these poor exercises do not differentiate between
students with different abilities; the others give preference to low-ability
students to answer these exercises over high-ability students.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Elrahman_A/0/1/0/all/0/1">Ahmed Abd Elrahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Soliman_T/0/1/0/all/0/1">Taysir Hassan A Soliman</a>, <a href="http://arxiv.org/find/cs/1/au:+Farghally_M/0/1/0/all/0/1">Mohammed F. Farghally</a>, <a href="http://arxiv.org/find/cs/1/au:+Taloba_A/0/1/0/all/0/1">Ahmed I. Taloba</a></p><p>The growing dependence on eTextbooks and Massive Open Online Courses (MOOCs)
has led to an increase in the amount of students' learning data. By carefully
analyzing this data, educators can identify difficult exercises, and evaluate
the quality of the exercises when teaching a particular topic. In this study,
an analysis of log data from the use of a semester of the OpenDSA eTextbook was
offered to identify the most difficult data structure course exercises and to
evaluate the quality of the course exercises. Our study is based on analyzing
students' responses to the course exercises. To identify the difficult
exercises, we applied two different approaches, the first of which involved
analyzing student responses to exercises using item response theory (IRT)
analysis and a latent trait model (LTM) technique, and the second involved
determining which exercises were more difficult based on how students
interacted with them. We computed different measures for every exercise such
that difficulty level, trial and error, and hint ratio. We generated an item
characteristics curve, item information curve, and test information function
for each exercise. To evaluate the quality of the exercises, we applied the IRT
analysis to the students' responses to the exercises and, we computed the
difficulty and discrimination index for each exercise. We classified whether
the exercise is good or poor based on these two measures. Our findings showed
that the exercises that related to algorithm analysis topics represented most
of the difficult exercises that students struggle with, and there existing six
exercises out of 56 exercises are classified as poor exercises which could be
rejected or improved. Some of these poor exercises do not differentiate between
students with different abilities; the others give preference to low-ability
students to answer these exercises over high-ability students.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-12T00:30:00Z">Wednesday, October 12 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.05385'>Enhancing Branch-and-Bound for Multi-Objective 0-1 Programming</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Nicolas Forget, Sophie N. Parragh</p><p>In the bi-objective branch-and-bound literature, a key ingredient is
objective branching, i.e. to create smaller and disjoint sub-problems in the
objective space, obtained from the partial dominance of the lower bound set by
the upper bound set. When considering three or more objective functions,
however, applying objective branching becomes more complex, and its benefit has
so far been unclear. In this paper, we investigate several ingredients which
allow to better exploit objective branching in a multi-objective setting. We
extend the idea of probing to multiple objectives, enhance it in several ways,
and show that when coupled with objective branching, it results in significant
speed-ups in terms of CPU times. We also investigate cut generation based on
the objective branching constraints. Besides, we generalize the best-bound idea
for node selection to multiple objectives and we show that the proposed rules
outperform the, in the multi-objective literature, commonly employed
depth-first and breadth-first strategies. We also analyze problem specific
branching rules. We test the proposed ideas on available benchmark instances
for three problem classes with three and four objectives, namely the
capacitated facility location problem, the uncapacitated facility location
problem, and the knapsack problem. Our enhanced multi-objective
branch-and-bound algorithm outperforms the best existing branch-and-bound based
approach and is the first to obtain competitive and even slightly better
results than a state-of-the-art objective space search method on a subset of
the problem classes.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Forget_N/0/1/0/all/0/1">Nicolas Forget</a>, <a href="http://arxiv.org/find/cs/1/au:+Parragh_S/0/1/0/all/0/1">Sophie N. Parragh</a></p><p>In the bi-objective branch-and-bound literature, a key ingredient is
objective branching, i.e. to create smaller and disjoint sub-problems in the
objective space, obtained from the partial dominance of the lower bound set by
the upper bound set. When considering three or more objective functions,
however, applying objective branching becomes more complex, and its benefit has
so far been unclear. In this paper, we investigate several ingredients which
allow to better exploit objective branching in a multi-objective setting. We
extend the idea of probing to multiple objectives, enhance it in several ways,
and show that when coupled with objective branching, it results in significant
speed-ups in terms of CPU times. We also investigate cut generation based on
the objective branching constraints. Besides, we generalize the best-bound idea
for node selection to multiple objectives and we show that the proposed rules
outperform the, in the multi-objective literature, commonly employed
depth-first and breadth-first strategies. We also analyze problem specific
branching rules. We test the proposed ideas on available benchmark instances
for three problem classes with three and four objectives, namely the
capacitated facility location problem, the uncapacitated facility location
problem, and the knapsack problem. Our enhanced multi-objective
branch-and-bound algorithm outperforms the best existing branch-and-bound based
approach and is the first to obtain competitive and even slightly better
results than a state-of-the-art objective space search method on a subset of
the problem classes.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-12T00:30:00Z">Wednesday, October 12 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.05543'>Parallel solutions for preemptive makespan scheduling on two identical machines</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Leah Epstein</p><p>We consider online preemptive scheduling of jobs arriving one by one, to be
assigned to two identical machines, with the goal of makespan minimization. We
study the effect of selecting the best solution out of two independent
solutions constructed in parallel in an online fashion. Two cases are analyzed,
where one case is purely online, and in the other one jobs are presented sorted
by non-increasing sizes. We show that using two solutions rather than one
improves the performance significantly, but that an optimal solution cannot be
obtained for any constant number of solutions constructed in parallel. Our
algorithms have the best possible competitive ratios out of algorithms for each
one of the classes.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Epstein_L/0/1/0/all/0/1">Leah Epstein</a></p><p>We consider online preemptive scheduling of jobs arriving one by one, to be
assigned to two identical machines, with the goal of makespan minimization. We
study the effect of selecting the best solution out of two independent
solutions constructed in parallel in an online fashion. Two cases are analyzed,
where one case is purely online, and in the other one jobs are presented sorted
by non-increasing sizes. We show that using two solutions rather than one
improves the performance significantly, but that an optimal solution cannot be
obtained for any constant number of solutions constructed in parallel. Our
algorithms have the best possible competitive ratios out of algorithms for each
one of the classes.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-12T00:30:00Z">Wednesday, October 12 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Tuesday, October 11
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://ptreview.sublinear.info/2022/10/news-for-september-2022/'>News for September 2022</a></h3>
          <p class='item-feed'>from <a href='https://ptreview.sublinear.info'>Property Testing Review</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          Apologies for the delay! Another quiet month in the property testing world, with only one paper (that we found). If we missed any, let us know in the comments! On Interactive Proofs of Proximity with Proof-Oblivious Queries, by Oded Goldreich, Guy Rothblum, and Tal Skverer (ECCC). Interactive Proofs of Proximity (IPPs) are the &#8220;interactive&#8221; version [&#8230;]
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p>Apologies for the delay! Another quiet month in the property testing world, with only one paper (that we found). If we missed any, let us know in the comments!</p>



<p><strong>On Interactive Proofs of Proximity with Proof-Oblivious Queries</strong>, by Oded Goldreich, Guy Rothblum, and Tal Skverer (<a href="https://eccc.weizmann.ac.il/report/2022/124/">ECCC</a>). Interactive Proofs of Proximity (IPPs) are the &#8220;interactive&#8221; version of property testers, where the algorithm can both query the input and interact with an all-knowing (but untrusted) prover. In this work, the authors study the power of a specific and natural type of &#8220;adaptivity&#8221; for IPPs, asking what happens when the choice of queries and the interaction with the prover are independent, or restricted. That is, what happens when these two aspects of the IPP algorithm are in separate &#8220;modules&#8221;? Can we still test various properties as efficiently? The paper proves various results in under several models (=restrictions between the two &#8220;modules&#8221;), focusing on the intermediate restriction where the two modules (queries to the input and interaction with the prover) are separate (no interaction), but have access to shared randomness.</p>



<p></p>
<p class="authors">By Clement Canonne</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-11T23:31:03Z">Tuesday, October 11 2022, 23:31</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://cstheory-jobs.org/2022/10/11/principal-researcher-postdoctoral-at-the-university-of-chicago-apply-by-january-14-2023/'>Principal Researcher (postdoctoral) at The University of Chicago (apply by January 14, 2023)</a></h3>
          <p class='item-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          The candidate will be given the opportunity to pursue a broad research agenda with Prof. Bryon Aragam at the intersection of statistics and machine learning and will ideally have a background in at least one of the following areas: latent variable models, deep generative models, causal inference, nonparametric statistics, learning theory, or graphical models. Website: [&#8230;]
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p>The candidate will be given the opportunity to pursue a broad research agenda with Prof. Bryon Aragam at the intersection of statistics and machine learning and will ideally have a background in at least one of the following areas: latent variable models, deep generative models, causal inference, nonparametric statistics, learning theory, or graphical models.</p>
<p>Website: <a href="https://www.chicagobooth.edu/-/media/faculty/research-professional-program/job-ads/2022-23/principal_researcher_aragam_announcement_2022.pdf">https://www.chicagobooth.edu/-/media/faculty/research-professional-program/job-ads/2022-23/principal_researcher_aragam_announcement_2022.pdf</a><br />
Email: bryon@chicagobooth.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-11T19:39:19Z">Tuesday, October 11 2022, 19:39</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://gilkalai.wordpress.com/2022/10/11/test-your-intuition-51/'>Test Your intuition 51</a></h3>
          <p class='item-feed'>from <a href='https://gilkalai.wordpress.com'>Gil Kalai</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          Suppose that and are two compact convex sets in space. Suppose that contains . Now consider two quantities is the average volume of a simplex forms by four points in drawn uniformly at random. is the average volume of a &#8230; Continue reading &#8594;
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p></p>



<p></p>


<p>Suppose that <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="K" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="L" class="latex" /> are two compact convex sets in space. Suppose that <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="K" class="latex" /> <span style="color:#0000ff;"><strong>contains</strong></span> <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="L" class="latex" />. Now consider two quantities</p>
<ul>
<li><img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="X" class="latex" /> is the average volume of a simplex forms by four points in <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="K" class="latex" /> drawn uniformly at random.</li>
<li><img src="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="Y" class="latex" /> is the average volume of a simplex forms by four points in <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="L" class="latex" /> drawn uniformly at random.</li>
</ul>
<h3>TYI: Is it always the case that X ≥ Y?</h3><p class="authors">By Gil Kalai</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-11T09:05:53Z">Tuesday, October 11 2022, 09:05</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://cstheory-jobs.org/2022/10/11/postdoc-at-technion-apply-by-december-31-2022/'>Postdoc  at Technion (apply by December 31, 2022)</a></h3>
          <p class='item-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          Candidates with a strong publication record in top venues in distributed computing (broadly interpreted) or algorithms in general are welcome to apply. To apply, send the following to Mrs. Hila Mizrahi at hilamiz@cs.technion.ac.il: 1. CV (PDF) 2. 1-2 page research statement (PDF) 3. Contact details of 3 references (email plaintext) 4. Expected graduation date, if [&#8230;]
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p>Candidates with a strong publication record in top venues in distributed computing (broadly interpreted) or algorithms in general are welcome to apply. To apply, send the following to Mrs. Hila Mizrahi at hilamiz@cs.technion.ac.il: 1. CV (PDF)<br />
2. 1-2 page research statement (PDF)<br />
3. Contact details of 3 references (email plaintext)<br />
4. Expected graduation date, if applicable (email plaintext)</p>
<p>Website: <a href="https://ckeren.net.technion.ac.il/">https://ckeren.net.technion.ac.il/</a><br />
Email: hilamiz@cs.technion.ac.il</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-11T08:13:20Z">Tuesday, October 11 2022, 08:13</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.03839'>Edge deletion to tree-like graph classes</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Ivo Koch, Nina Pardal, Vinicius Fernandes dos Santos</p><p>For a fixed property (graph class) $\Pi$, given a graph $G$ and an integer
$k$, the $\Pi$-deletion problem consists in deciding if we can turn $G$ into a
graph with the property $\Pi$ by deleting at most $k$ edges of $G$. The
$\Pi$-deletion problem is known to be NP-hard for most of the well-studied
graph classes (such as chordal, interval, bipartite, planar, comparability and
permutation graphs, among others), with the notable exception of trees.
Motivated by this fact, in this work we study the deletion problem for some
classes close to trees. We obtain NP-hardness results for several classes of
sparse graphs, for which we prove that deletion is hard even when the input is
a bipartite graph. In addition, we give sufficient structural conditions for
the graph class $\Pi$ for NP-hardness. In the case of deletion to cactus, we
show that the problem becomes tractable when the input is chordal, and we give
polynomial-time algorithms for quasi-threshold graphs.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Koch_I/0/1/0/all/0/1">Ivo Koch</a>, <a href="http://arxiv.org/find/cs/1/au:+Pardal_N/0/1/0/all/0/1">Nina Pardal</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_V/0/1/0/all/0/1">Vinicius Fernandes dos Santos</a></p><p>For a fixed property (graph class) $\Pi$, given a graph $G$ and an integer
$k$, the $\Pi$-deletion problem consists in deciding if we can turn $G$ into a
graph with the property $\Pi$ by deleting at most $k$ edges of $G$. The
$\Pi$-deletion problem is known to be NP-hard for most of the well-studied
graph classes (such as chordal, interval, bipartite, planar, comparability and
permutation graphs, among others), with the notable exception of trees.
Motivated by this fact, in this work we study the deletion problem for some
classes close to trees. We obtain NP-hardness results for several classes of
sparse graphs, for which we prove that deletion is hard even when the input is
a bipartite graph. In addition, we give sufficient structural conditions for
the graph class $\Pi$ for NP-hardness. In the case of deletion to cactus, we
show that the problem becomes tractable when the input is chordal, and we give
polynomial-time algorithms for quasi-threshold graphs.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-11T00:30:00Z">Tuesday, October 11 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.03934'>Automata Equipped with Auxiliary Data Structures and Regular Realizability Problems</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Alexander Rubtsov, Mikhail Vyalyi</p><p>We consider general computational models: one-way and two-way finite
automata, and logarithmic space Turing machines, all equipped with an auxiliary
data structure (ADS). The definition of an ADS is based on the language of
protocols of work with the ADS. We describe the connection of automata-based
models with ``Balloon automata'' that are another general formalization of
automata equipped with an ADS presented by Hopcroft and Ullman in 1967.
</p>
<p>This definition establishes the connection between the non-emptiness problem
for one-way automata with ADS, languages recognizable by nondeterministic
log-space Turing machines equipped with the same ADS, and a regular
realizability problem (NRR) for the language of ADS' protocols. The NRR problem
is to verify whether the regular language on the input has a non-empty
intersection with the language of protocols. The computational complexity of
these problems (and languages) is the same up to log-space reductions.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Rubtsov_A/0/1/0/all/0/1">Alexander Rubtsov</a>, <a href="http://arxiv.org/find/cs/1/au:+Vyalyi_M/0/1/0/all/0/1">Mikhail Vyalyi</a></p><p>We consider general computational models: one-way and two-way finite
automata, and logarithmic space Turing machines, all equipped with an auxiliary
data structure (ADS). The definition of an ADS is based on the language of
protocols of work with the ADS. We describe the connection of automata-based
models with ``Balloon automata'' that are another general formalization of
automata equipped with an ADS presented by Hopcroft and Ullman in 1967.
</p>
<p>This definition establishes the connection between the non-emptiness problem
for one-way automata with ADS, languages recognizable by nondeterministic
log-space Turing machines equipped with the same ADS, and a regular
realizability problem (NRR) for the language of ADS' protocols. The NRR problem
is to verify whether the regular language on the input has a non-empty
intersection with the language of protocols. The computational complexity of
these problems (and languages) is the same up to log-space reductions.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-11T00:30:00Z">Tuesday, October 11 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.04045'>The FBHHRBNRSSSHK-Algorithm for Multiplication in $\mathbb{Z}_2^{5\times5}$ is still not the end of the story 2</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Manuel Kauers, Jakob Moosbauer</p><p>In response to a recent Nature article which announced an algorithm for
multiplying $5\times5$-matrices over $\mathbb{Z}_2$ with only 96
multiplications, two fewer than the previous record, we present an algorithm
that does the job with only 95 multiplications.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kauers_M/0/1/0/all/0/1">Manuel Kauers</a>, <a href="http://arxiv.org/find/cs/1/au:+Moosbauer_J/0/1/0/all/0/1">Jakob Moosbauer</a></p><p>In response to a recent Nature article which announced an algorithm for
multiplying $5\times5$-matrices over $\mathbb{Z}_2$ with only 96
multiplications, two fewer than the previous record, we present an algorithm
that does the job with only 95 multiplications.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-11T00:30:00Z">Tuesday, October 11 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.03964'>An Efficient and Continuous Voronoi Density Estimator</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Giovanni Luca Marchetti, Vladislav Polianskii, Anastasiia Varava, Florian T. Pokorny, Danica Kragic</p><p>We introduce a non-parametric density estimator deemed Radial Voronoi Density
Estimator (RVDE). RVDE is grounded in the geometry of Voronoi tessellations and
as such benefits from local geometric adaptiveness and broad convergence
properties. Due to its radial definition RVDE is moreover continuous and
computable in linear time with respect to the dataset size. This amends for the
main shortcomings of previously studied VDEs, which are highly discontinuous
and computationally expensive. We provide a theoretical study of the modes of
RVDE as well as an empirical investigation of its performance on
high-dimensional data. Results show that RVDE outperforms other non-parametric
density estimators, including recently introduced VDEs.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/stat/1/au:+Marchetti_G/0/1/0/all/0/1">Giovanni Luca Marchetti</a>, <a href="http://arxiv.org/find/stat/1/au:+Polianskii_V/0/1/0/all/0/1">Vladislav Polianskii</a>, <a href="http://arxiv.org/find/stat/1/au:+Varava_A/0/1/0/all/0/1">Anastasiia Varava</a>, <a href="http://arxiv.org/find/stat/1/au:+Pokorny_F/0/1/0/all/0/1">Florian T. Pokorny</a>, <a href="http://arxiv.org/find/stat/1/au:+Kragic_D/0/1/0/all/0/1">Danica Kragic</a></p><p>We introduce a non-parametric density estimator deemed Radial Voronoi Density
Estimator (RVDE). RVDE is grounded in the geometry of Voronoi tessellations and
as such benefits from local geometric adaptiveness and broad convergence
properties. Due to its radial definition RVDE is moreover continuous and
computable in linear time with respect to the dataset size. This amends for the
main shortcomings of previously studied VDEs, which are highly discontinuous
and computationally expensive. We provide a theoretical study of the modes of
RVDE as well as an empirical investigation of its performance on
high-dimensional data. Results show that RVDE outperforms other non-parametric
density estimators, including recently introduced VDEs.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-11T00:30:00Z">Tuesday, October 11 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.04090'>APUD(1,1) Recognition in Polynomial Time</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Deniz A&#x11f;ao&#x11f;lu &#xc7;a&#x11f;&#x131;r&#x131;c&#x131;, Onur &#xc7;a&#x11f;&#x131;r&#x131;c&#x131;</p><p>A unit disk graph is the intersection graph of a set of disk of unit radius
in the Euclidean plane. In 1998, Breu and Kirkpatrick showed that the
recognition problem for unit disk graphs is NP-hard. Given $k$ horizontal and
$m$ vertical lines, an APUD($k,m$) is a unit disk graph such that each unit
disk is centered either on a given horizontal or vertical line.
\c{C}a\u{g}{\i}r{\i}c{\i} showed in 2020 that APUD($k,m$) recognition is
NP-hard. In this paper, we show that APUD($1,1$) recognition is polynomial time
solvable.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cagirici_D/0/1/0/all/0/1">Deniz A&#x11f;ao&#x11f;lu &#xc7;a&#x11f;&#x131;r&#x131;c&#x131;</a>, <a href="http://arxiv.org/find/cs/1/au:+Cagirici_O/0/1/0/all/0/1">Onur &#xc7;a&#x11f;&#x131;r&#x131;c&#x131;</a></p><p>A unit disk graph is the intersection graph of a set of disk of unit radius
in the Euclidean plane. In 1998, Breu and Kirkpatrick showed that the
recognition problem for unit disk graphs is NP-hard. Given $k$ horizontal and
$m$ vertical lines, an APUD($k,m$) is a unit disk graph such that each unit
disk is centered either on a given horizontal or vertical line.
\c{C}a\u{g}{\i}r{\i}c{\i} showed in 2020 that APUD($k,m$) recognition is
NP-hard. In this paper, we show that APUD($1,1$) recognition is polynomial time
solvable.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-11T00:30:00Z">Tuesday, October 11 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2210.04099'>Developable Quad Meshes</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Victor Ceballos Inza, Florian Rist, Johannes Wallner, Helmut Pottmann</p><p>There are different ways to capture the property of a surface being
developable, i.e., it can be mapped to a planar domain without stretching or
tearing. Contributions range from special parametrizations to
discrete-isometric mappings. So far, a local criterion expressing the
developability of general quad meshes has been lacking. In this paper, we
propose a new and efficient discrete developability criterion that is based on
a property well-known from differential geometry, namely a rank-deficient
second fundamental form. This criterion is expressed in terms of the canonical
checkerboard patterns inscribed in a quad mesh which already was successful in
describing discrete-isometric mappings. In combination with standard global
optimization procedures, we are able to perform developable lofting,
approximation, and design. The meshes we employ are combinatorially regular
quad meshes with isolated singularities but are otherwise not required to
follow any special curves. They are thus easily embedded into a design workflow
involving standard operations like re-meshing, trimming, and merging
operations.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Inza_V/0/1/0/all/0/1">Victor Ceballos Inza</a>, <a href="http://arxiv.org/find/cs/1/au:+Rist_F/0/1/0/all/0/1">Florian Rist</a>, <a href="http://arxiv.org/find/cs/1/au:+Wallner_J/0/1/0/all/0/1">Johannes Wallner</a>, <a href="http://arxiv.org/find/cs/1/au:+Pottmann_H/0/1/0/all/0/1">Helmut Pottmann</a></p><p>There are different ways to capture the property of a surface being
developable, i.e., it can be mapped to a planar domain without stretching or
tearing. Contributions range from special parametrizations to
discrete-isometric mappings. So far, a local criterion expressing the
developability of general quad meshes has been lacking. In this paper, we
propose a new and efficient discrete developability criterion that is based on
a property well-known from differential geometry, namely a rank-deficient
second fundamental form. This criterion is expressed in terms of the canonical
checkerboard patterns inscribed in a quad mesh which already was successful in
describing discrete-isometric mappings. In combination with standard global
optimization procedures, we are able to perform developable lofting,
approximation, and design. The meshes we employ are combinatorially regular
quad meshes with isolated singularities but are otherwise not required to
follow any special curves. They are thus easily embedded into a design workflow
involving standard operations like re-meshing, trimming, and merging
operations.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-11T00:30:00Z">Tuesday, October 11 2022, 00:30</time>
        </div>
      </div>
    </article>
  
  </div>

  <script src='js/jquery-2.0.3.min.js'></script>
  <script src="js/jquery.timeago.js" type="text/javascript"></script>
  <script>
    jQuery(document).ready(function() {
      jQuery("time.timeago").timeago();
    });
  </script>
  <script src='js/blank.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
