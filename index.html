<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.5 (2022-11-24) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2023-02-23T16:45:12Z">Thursday, February 23 2023, 16:45</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://minorfree.github.io/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://minorfree.github.io'>Hung Le</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>Sébastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Thursday, February 23
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://blog.computationalcomplexity.org/2023/02/the-virtual-grad-student.html'>The Virtual Grad Student</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Martin Haug, who is working on a LaTeX alternative Typst, asked me if I had updates on a LaTeX rant from 2011. I haven't seen any new serious backward compatibility problems. We have easier collaboration through on-line editors like Overleaf. We have got closer to WSYWIG thanks to quick compiling but still not at the level of Word or Google Docs. The big problem of user friendliness remains. There's a reason LaTeX has its own Stack Exchange.&nbsp;</p><p>But we live in a new machine learning world. Can we use generative AI to make LaTeX easier to use?</p><p>Mandatory Disclaimer: Generative AI can sometimes create inaccurate, inappropriate or previously-published material. You are ultimately responsible for the contents of your paper no matter how you produced it.</p><p>Since I sometimes think of LaTeX as a programming language for papers, I tweeted</p>
<blockquote><p>Can we have GitHub co-pilot for LaTeX?</p>— Lance Fortnow (@fortnow) February 17, 2023</blockquote><p>Thanks for the responses. The answer to the question is yes, GitHub Copilot&nbsp;works for LaTeX if you edit LaTeX in a programming environment like VS Code, Neovim or Jet Brains. It helps with formatting of formulas and pictures, less so on the text itself. I made a video so you can see how it works.</p>
<p>Latext AI offers a chrome extension that will let you generate text via GPT in Overleaf based on a prompt or previous text, though Latext requires a subscription after a one-week trial. You can also just cut and paste between any text editor and ChatGPT.</p><p>ChatGPT notoriously makes up references if you ask for them. Can we have a good system that finds relevant articles to cite and adds them automatically into your bibliography?</p><p>Ideally all these should work together seamlessly, suggestions that happen as you type. A true co-pilot for research papers.</p><p>There are many more tools out there, feel free to add them to the comments. I expect the integration to improve over time as we develop new APIs and models.</p><p>I look forward to the days of a virtual grad student: Here's a research goal and an idea to get there. Now go figure out the details and write the paper.&nbsp;</p><p>It will be a long wait.</p> <p>By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Martin Haug, who is working on a LaTeX alternative <a href="https://typst.app/">Typst</a>, asked me if I had updates on a <a href="https://blog.computationalcomplexity.org/2011/07/problems-of-latex.html">LaTeX rant</a> from 2011. I haven't seen any new serious backward compatibility problems. We have easier collaboration through on-line editors like <a href="https://blog.computationalcomplexity.org/2011/07/problems-of-latex.html">Overleaf</a>. We have got closer to WSYWIG thanks to quick compiling but still not at the level of Word or Google Docs. The big problem of user friendliness remains. There's a reason LaTeX has its own <a href="https://tex.stackexchange.com/">Stack Exchange</a>.&nbsp;</p><p>But we live in a new machine learning world. Can we use generative AI to make LaTeX easier to use?</p><p><b>Mandatory Disclaimer</b>: Generative AI can sometimes create inaccurate, inappropriate or previously-published material. You are ultimately responsible for the contents of your paper no matter how you produced it.</p><p>Since I sometimes think of LaTeX as a programming language for papers, I <a href="https://twitter.com/fortnow/status/1626576896132542464">tweeted</a></p>
<blockquote class="twitter-tweet"><p dir="ltr" lang="en">Can we have GitHub co-pilot for LaTeX?</p>— Lance Fortnow (@fortnow) <a href="https://twitter.com/fortnow/status/1626576896132542464?ref_src=twsrc%5Etfw">February 17, 2023</a></blockquote><p>Thanks for the responses. The answer to the question is yes, <a href="https://github.com/features/copilot">GitHub Copilot</a>&nbsp;works for LaTeX if you edit LaTeX in a programming environment like <a href="https://code.visualstudio.com/">VS Code</a>, <a href="https://code.visualstudio.com/">Neovim</a> or <a href="https://code.visualstudio.com/">Jet Brains</a>. It helps with formatting of formulas and pictures, less so on the text itself. I made a <a href="https://www.youtube.com/watch?v=bt0BNdujIy8">video</a> so you can see how it works.</p>
<div style="text-align: center;"><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/bt0BNdujIy8" title="YouTube video player" width="560"></iframe></div><p><a href="https://www.latextai.com/">Latext AI</a> offers a chrome extension that will let you generate text via GPT in Overleaf based on a prompt or previous text, though Latext requires a subscription after a one-week trial. You can also just cut and paste between any text editor and ChatGPT.</p><p>ChatGPT notoriously makes up references if you ask for them. Can we have a good system that finds relevant articles to cite and adds them automatically into your bibliography?</p><p>Ideally all these should work together seamlessly, suggestions that happen as you type. A true co-pilot for research papers.</p><p>There are many more tools out there, feel free to add them to the comments. I expect the integration to improve over time as we develop new APIs and models.</p><p>I look forward to the days of a virtual grad student: Here's a research goal and an idea to get there. Now go figure out the details and write the paper.&nbsp;</p><p>It will be a long wait.</p> <script async="" charset="utf-8" src="https://platform.twitter.com/widgets.js"></script><p class="authors">By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T15:02:00Z">Thursday, February 23 2023, 15:02</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.10972'>Complexity of Maker-Breaker Games on Edge Sets of Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Eric Duch&#xea;ne, Valentin Gledel, Fionn Mc Inerney, Nicolas Nisse, Nacim Oijid, Aline Parreau, Milo&#x161; Stojakovi&#x107;</p><p>We initiate the study of the algorithmic complexity of Maker-Breaker games
played on edge sets of graphs for general graphs. We mainly consider three of
the big four such games: the connectivity game, perfect matching game, and
$H$-game. Maker wins if she claims the edges of a spanning tree in the first, a
perfect matching in the second, and a copy of a fixed graph $H$ in the third.
We prove that deciding who wins the perfect matching game and the $H$-game is
PSPACE-complete, even for the latter in graphs of small diameter if $H$ is a
tree. Seeking to find the smallest graph $H$ such that the $H$-game is
PSPACE-complete, we also prove that there exists such an $H$ of order 51 and
size 57.
</p>
<p>On the positive side, we show that the connectivity game and arboricity-$k$
game are polynomial-time solvable. We then give several positive results for
the $H$-game, first giving a structural characterization for Breaker to win the
$P_4$-game, which gives a linear-time algorithm for the $P_4$-game. We provide
a structural characterization for Maker to win the $K_{1,\ell}$-game in trees,
which implies a linear-time algorithm for the $K_{1,\ell}$-game in trees.
Lastly, we prove that the $K_{1,\ell}$-game in any graph, and the $H$-game in
trees are both FPT parameterized by the length of the game. We leave the
complexity of the last of the big four games, the Hamiltonicity game, as an
open question.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Duchene_E/0/1/0/all/0/1">Eric Duch&#xea;ne</a>, <a href="http://arxiv.org/find/cs/1/au:+Gledel_V/0/1/0/all/0/1">Valentin Gledel</a>, <a href="http://arxiv.org/find/cs/1/au:+Inerney_F/0/1/0/all/0/1">Fionn Mc Inerney</a>, <a href="http://arxiv.org/find/cs/1/au:+Nisse_N/0/1/0/all/0/1">Nicolas Nisse</a>, <a href="http://arxiv.org/find/cs/1/au:+Oijid_N/0/1/0/all/0/1">Nacim Oijid</a>, <a href="http://arxiv.org/find/cs/1/au:+Parreau_A/0/1/0/all/0/1">Aline Parreau</a>, <a href="http://arxiv.org/find/cs/1/au:+Stojakovic_M/0/1/0/all/0/1">Milo&#x161; Stojakovi&#x107;</a></p><p>We initiate the study of the algorithmic complexity of Maker-Breaker games
played on edge sets of graphs for general graphs. We mainly consider three of
the big four such games: the connectivity game, perfect matching game, and
$H$-game. Maker wins if she claims the edges of a spanning tree in the first, a
perfect matching in the second, and a copy of a fixed graph $H$ in the third.
We prove that deciding who wins the perfect matching game and the $H$-game is
PSPACE-complete, even for the latter in graphs of small diameter if $H$ is a
tree. Seeking to find the smallest graph $H$ such that the $H$-game is
PSPACE-complete, we also prove that there exists such an $H$ of order 51 and
size 57.
</p>
<p>On the positive side, we show that the connectivity game and arboricity-$k$
game are polynomial-time solvable. We then give several positive results for
the $H$-game, first giving a structural characterization for Breaker to win the
$P_4$-game, which gives a linear-time algorithm for the $P_4$-game. We provide
a structural characterization for Maker to win the $K_{1,\ell}$-game in trees,
which implies a linear-time algorithm for the $K_{1,\ell}$-game in trees.
Lastly, we prove that the $K_{1,\ell}$-game in any graph, and the $H$-game in
trees are both FPT parameterized by the length of the game. We leave the
complexity of the last of the big four games, the Hamiltonicity game, as an
open question.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.11290'>Logical Equivalences, Homomorphism Indistinguishability, and Forbidden Minors</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Tim Seppelt</p><p>Two graphs $G$ and $H$ are homomorphism indistinguishable over a class of
graphs $\mathcal{F}$ if for all graphs $F \in \mathcal{F}$ the number of
homomorphisms from $F$ to $G$ equals the number of homomorphisms from $F$ to
$H$. Many natural equivalence relations comparing graphs such as (quantum)
isomorphism, spectral, and logical equivalences can be characterised as
homomorphism indistinguishability relations over certain graph classes.
</p>
<p>In this article, the interplay of the properties of a graph class and its
homomorphism indistinguishability relation are studied. As an application,
self-complementarity, a property of logics on graphs satisfied by many
well-studied logics, is identified. It is proven that the equivalence over a
self-complementary logic admitting a characterisation as homomorphism
indistinguishability relation can be characterised by homomorphism
indistinguishability over a minor-closed graph class. Thereby, first evidences
are provided for a possible connection between minors and homomorphism
indistinguishability as conjectured by Roberson (2022).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Seppelt_T/0/1/0/all/0/1">Tim Seppelt</a></p><p>Two graphs $G$ and $H$ are homomorphism indistinguishable over a class of
graphs $\mathcal{F}$ if for all graphs $F \in \mathcal{F}$ the number of
homomorphisms from $F$ to $G$ equals the number of homomorphisms from $F$ to
$H$. Many natural equivalence relations comparing graphs such as (quantum)
isomorphism, spectral, and logical equivalences can be characterised as
homomorphism indistinguishability relations over certain graph classes.
</p>
<p>In this article, the interplay of the properties of a graph class and its
homomorphism indistinguishability relation are studied. As an application,
self-complementarity, a property of logics on graphs satisfied by many
well-studied logics, is identified. It is proven that the equivalence over a
self-complementary logic admitting a characterisation as homomorphism
indistinguishability relation can be characterised by homomorphism
indistinguishability over a minor-closed graph class. Thereby, first evidences
are provided for a possible connection between minors and homomorphism
indistinguishability as conjectured by Roberson (2022).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.11417'>Hitting the Romans</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Henning Fernau, Kevin Mann</p><p>Roman domination is one of few examples where the related extension problem
is polynomial-time solvable even if the original decision problem is
NP-complete. This is interesting, as it allows to establish polynomial-delay
enumeration algorithms for finding minimal Roman dominating functions, while it
is open for more than four decades if all minimal dominating sets of a graph or
if all hitting sets of a hypergraph can be enumerated with polynomial delay. To
find the reason why this is the case, we combine the idea of hitting set with
the idea of Roman domination. We hence obtain and study two new problems,
called Roman Hitting Function and Roman Hitting Set, both generalizing Roman
Domination. This allows us to delineate the borderline of polynomial-delay
enumerability. Here, we assume what we call the Hitting Set Transversal Thesis,
claiming that it is impossible to enumerate all minimal hitting sets of a
hypergraph with polynomial delay. Our first focus is on the extension versions
of these problems. While doing this, we find some conditions under which the
Extension Roman Hitting Function problem is NP-complete. We then use
parameterized complexity to get a better understanding of why Extension Roman
Hitting Function behaves in this way. Furthermore, we analyze the parameterized
and approximation complexity of the underlying optimization problems. We also
discuss consequences for Roman variants of other problems like Vertex Cover.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Fernau_H/0/1/0/all/0/1">Henning Fernau</a>, <a href="http://arxiv.org/find/cs/1/au:+Mann_K/0/1/0/all/0/1">Kevin Mann</a></p><p>Roman domination is one of few examples where the related extension problem
is polynomial-time solvable even if the original decision problem is
NP-complete. This is interesting, as it allows to establish polynomial-delay
enumeration algorithms for finding minimal Roman dominating functions, while it
is open for more than four decades if all minimal dominating sets of a graph or
if all hitting sets of a hypergraph can be enumerated with polynomial delay. To
find the reason why this is the case, we combine the idea of hitting set with
the idea of Roman domination. We hence obtain and study two new problems,
called Roman Hitting Function and Roman Hitting Set, both generalizing Roman
Domination. This allows us to delineate the borderline of polynomial-delay
enumerability. Here, we assume what we call the Hitting Set Transversal Thesis,
claiming that it is impossible to enumerate all minimal hitting sets of a
hypergraph with polynomial delay. Our first focus is on the extension versions
of these problems. While doing this, we find some conditions under which the
Extension Roman Hitting Function problem is NP-complete. We then use
parameterized complexity to get a better understanding of why Extension Roman
Hitting Function behaves in this way. Furthermore, we analyze the parameterized
and approximation complexity of the underlying optimization problems. We also
discuss consequences for Roman variants of other problems like Vertex Cover.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.11454'>Quantum complexity of the Kronecker coefficients</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sergey Bravyi, Anirban Chowdhury, David Gosset, Vojtech Havlicek, Guanyu Zhu</p><p>Whether or not the Kronecker coefficients of the symmetric group count some
set of combinatorial objects is a longstanding open question. In this work we
show that a given Kronecker coefficient is proportional to the rank of a
projector that can be measured efficiently using a quantum computer. In other
words a Kronecker coefficient counts the dimension of the vector space spanned
by the accepting witnesses of a QMA verifier, where QMA is the quantum analogue
of NP. This implies that approximating the Kronecker coefficients to within a
given relative error is not harder than a certain natural class of quantum
approximate counting problems that captures the complexity of estimating
thermal properties of quantum many-body systems. A second consequence is that
deciding positivity of Kronecker coefficients is contained in QMA,
complementing a recent NP-hardness result of Ikenmeyer, Mulmuley and Walter. We
obtain similar results for the related problem of approximating row sums of the
character table of the symmetric group. Finally, we discuss an efficient
quantum algorithm that approximates normalized Kronecker coefficients to
inverse-polynomial additive error.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Bravyi_S/0/1/0/all/0/1">Sergey Bravyi</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Chowdhury_A/0/1/0/all/0/1">Anirban Chowdhury</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Gosset_D/0/1/0/all/0/1">David Gosset</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Havlicek_V/0/1/0/all/0/1">Vojtech Havlicek</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Zhu_G/0/1/0/all/0/1">Guanyu Zhu</a></p><p>Whether or not the Kronecker coefficients of the symmetric group count some
set of combinatorial objects is a longstanding open question. In this work we
show that a given Kronecker coefficient is proportional to the rank of a
projector that can be measured efficiently using a quantum computer. In other
words a Kronecker coefficient counts the dimension of the vector space spanned
by the accepting witnesses of a QMA verifier, where QMA is the quantum analogue
of NP. This implies that approximating the Kronecker coefficients to within a
given relative error is not harder than a certain natural class of quantum
approximate counting problems that captures the complexity of estimating
thermal properties of quantum many-body systems. A second consequence is that
deciding positivity of Kronecker coefficients is contained in QMA,
complementing a recent NP-hardness result of Ikenmeyer, Mulmuley and Walter. We
obtain similar results for the related problem of approximating row sums of the
character table of the symmetric group. Finally, we discuss an efficient
quantum algorithm that approximates normalized Kronecker coefficients to
inverse-polynomial additive error.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.11476'>Matrix Multiplication and Number On the Forehead Communication</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Josh Alman, Jaros&#x142;aw B&#x142;asiok</p><p>Three-player Number On the Forehead communication may be thought of as a
three-player Number In the Hand promise model, in which each player is given
the inputs that are supposedly on the other two players' heads, and promised
that they are consistent with the inputs of of the other players. The set of
all allowed inputs under this promise may be thought of as an order-3 tensor.
We surprisingly observe that this tensor is exactly the matrix multiplication
tensor, which is widely studied in the design of fast matrix multiplication
algorithms.
</p>
<p>Using this connection, we prove a number of results about both Number On the
Forehead communication and matrix multiplication, each by using known results
or techniques about the other. For example, we show how the Laser method, a key
technique used to design the best matrix multiplication algorithms, can also be
used to design communication protocols for a variety of problems. We also show
how known lower bounds for Number On the Forehead communication can be used to
bound properties of the matrix multiplication tensor such as its zeroing out
subrank. Finally, we substantially generalize known methods based on slice-rank
for studying communication, and show how they directly relate to the matrix
multiplication exponent $\omega$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Alman_J/0/1/0/all/0/1">Josh Alman</a>, <a href="http://arxiv.org/find/cs/1/au:+Blasiok_J/0/1/0/all/0/1">Jaros&#x142;aw B&#x142;asiok</a></p><p>Three-player Number On the Forehead communication may be thought of as a
three-player Number In the Hand promise model, in which each player is given
the inputs that are supposedly on the other two players' heads, and promised
that they are consistent with the inputs of of the other players. The set of
all allowed inputs under this promise may be thought of as an order-3 tensor.
We surprisingly observe that this tensor is exactly the matrix multiplication
tensor, which is widely studied in the design of fast matrix multiplication
algorithms.
</p>
<p>Using this connection, we prove a number of results about both Number On the
Forehead communication and matrix multiplication, each by using known results
or techniques about the other. For example, we show how the Laser method, a key
technique used to design the best matrix multiplication algorithms, can also be
used to design communication protocols for a variety of problems. We also show
how known lower bounds for Number On the Forehead communication can be used to
bound properties of the matrix multiplication tensor such as its zeroing out
subrank. Finally, we substantially generalize known methods based on slice-rank
for studying communication, and show how they directly relate to the matrix
multiplication exponent $\omega$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.11433'>Lower Bounds for Intersection Reporting among Flat Objects</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Peyman Afshani, Pingan Cheng</p><p>Recently, Ezra and Sharir [ES22a] showed an $O(n^{3/2+\sigma})$ space and
$O(n^{1/2+\sigma})$ query time data structure for ray shooting among triangles
in $\mathbb{R}^3$. This improves the upper bound given by the classical
$S(n)Q(n)^4=O(n^{4+\sigma})$ space-time tradeoff for the first time in almost
25 years and in fact lies on the tradeoff curve of
$S(n)Q(n)^3=O(n^{3+\sigma})$. However, it seems difficult to apply their
techniques beyond this specific space and time combination. This pheonomenon
appears persistently in almost all recent advances of flat object intersection
searching, e.g., line-tetrahedron intersection in $\mathbb{R}^4$ [ES22b],
triangle-triangle intersection in $\mathbb{R}^4$ [ES22b], or even among flat
semialgebraic objects [AAEKS22].
</p>
<p>We give a timely explanation to this phenomenon from a lower bound
perspective. We prove that given a set $\mathcal{S}$ of $(d-1)$-dimensional
simplicies in $\mathbb{R}^d$, any data structure that can report all
intersections with small ($n^{o(1)}$) query time must use
$\Omega(n^{2(d-1)-o(1)})$ space. This dashes the hope of any significant
improvement to the tradeoff curves for small query time and almost matches the
classical upper bound. We also obtain an almost matching space lower bound of
$\Omega(n^{6-o(1)})$ for triangle-triangle intersection reporting in
$\mathbb{R}^4$ when the query time is small. Along the way, we further develop
the previous lower bound techniques by Afshani and Cheng [AC21, AC22].
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Afshani_P/0/1/0/all/0/1">Peyman Afshani</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1">Pingan Cheng</a></p><p>Recently, Ezra and Sharir [ES22a] showed an $O(n^{3/2+\sigma})$ space and
$O(n^{1/2+\sigma})$ query time data structure for ray shooting among triangles
in $\mathbb{R}^3$. This improves the upper bound given by the classical
$S(n)Q(n)^4=O(n^{4+\sigma})$ space-time tradeoff for the first time in almost
25 years and in fact lies on the tradeoff curve of
$S(n)Q(n)^3=O(n^{3+\sigma})$. However, it seems difficult to apply their
techniques beyond this specific space and time combination. This pheonomenon
appears persistently in almost all recent advances of flat object intersection
searching, e.g., line-tetrahedron intersection in $\mathbb{R}^4$ [ES22b],
triangle-triangle intersection in $\mathbb{R}^4$ [ES22b], or even among flat
semialgebraic objects [AAEKS22].
</p>
<p>We give a timely explanation to this phenomenon from a lower bound
perspective. We prove that given a set $\mathcal{S}$ of $(d-1)$-dimensional
simplicies in $\mathbb{R}^d$, any data structure that can report all
intersections with small ($n^{o(1)}$) query time must use
$\Omega(n^{2(d-1)-o(1)})$ space. This dashes the hope of any significant
improvement to the tradeoff curves for small query time and almost matches the
classical upper bound. We also obtain an almost matching space lower bound of
$\Omega(n^{6-o(1)})$ for triangle-triangle intersection reporting in
$\mathbb{R}^4$ when the query time is small. Along the way, we further develop
the previous lower bound techniques by Afshani and Cheng [AC21, AC22].
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.11250'>The Complexity of Debt Swapping</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Henri Froese, Martin Hoefer, Lisa Wilhelmi</p><p>A debt swap is an elementary edge swap in a directed, weighted graph, where
two edges with the same weight swap their targets. Debt swaps are a natural and
appealing operation in financial networks, in which nodes are banks and edges
represent debt contracts. They can improve the clearing payments and the
stability of these networks. However, their algorithmic properties are not
well-understood.
</p>
<p>We analyze the computational complexity of debt swapping in networks with
ranking-based clearing. Our main interest lies in semi-positive swaps, in which
no creditor strictly suffers and at least one strictly profits. These swaps
lead to a Pareto-improvement in the entire network. We consider network
optimization via sequences of $v$-improving debt swaps from which a given bank
$v$ strictly profits. We show that every sequence of semi-positive
$v$-improving swaps has polynomial length. In contrast, for arbitrary
$v$-improving swaps, the problem of reaching a network configuration that
allows no further swaps is PLS-complete. We identify cases in which short
sequences of semi-positive swaps exist even without the $v$-improving property.
</p>
<p>In addition, we study reachability problems, i.e., deciding if a sequence of
swaps exists between given initial and final networks. We identify a
polynomial-time algorithm for arbitrary swaps, show NP-hardness for
semi-positive swaps and even PSPACE-completeness for $v$-improving swaps or
swaps that only maintain a lower bound on the assets of a given bank $v$. A
variety of our results can be extended to arbitrary monotone clearing.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Froese_H/0/1/0/all/0/1">Henri Froese</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoefer_M/0/1/0/all/0/1">Martin Hoefer</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilhelmi_L/0/1/0/all/0/1">Lisa Wilhelmi</a></p><p>A debt swap is an elementary edge swap in a directed, weighted graph, where
two edges with the same weight swap their targets. Debt swaps are a natural and
appealing operation in financial networks, in which nodes are banks and edges
represent debt contracts. They can improve the clearing payments and the
stability of these networks. However, their algorithmic properties are not
well-understood.
</p>
<p>We analyze the computational complexity of debt swapping in networks with
ranking-based clearing. Our main interest lies in semi-positive swaps, in which
no creditor strictly suffers and at least one strictly profits. These swaps
lead to a Pareto-improvement in the entire network. We consider network
optimization via sequences of $v$-improving debt swaps from which a given bank
$v$ strictly profits. We show that every sequence of semi-positive
$v$-improving swaps has polynomial length. In contrast, for arbitrary
$v$-improving swaps, the problem of reaching a network configuration that
allows no further swaps is PLS-complete. We identify cases in which short
sequences of semi-positive swaps exist even without the $v$-improving property.
</p>
<p>In addition, we study reachability problems, i.e., deciding if a sequence of
swaps exists between given initial and final networks. We identify a
polynomial-time algorithm for arbitrary swaps, show NP-hardness for
semi-positive swaps and even PSPACE-completeness for $v$-improving swaps or
swaps that only maintain a lower bound on the assets of a given bank $v$. A
variety of our results can be extended to arbitrary monotone clearing.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.11295'>Fair Correlation Clustering in Forests</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Katrin Casel, Tobias Friedrich, Martin Schirneck, Simon Wietheger</p><p>The study of algorithmic fairness received growing attention recently. This
stems from the awareness that bias in the input data for machine learning
systems may result in discriminatory outputs. For clustering tasks, one of the
most central notions of fairness is the formalization by Chierichetti, Kumar,
Lattanzi, and Vassilvitskii [NeurIPS 2017]. A clustering is said to be fair, if
each cluster has the same distribution of manifestations of a sensitive
attribute as the whole input set. This is motivated by various applications
where the objects to be clustered have sensitive attributes that should not be
over- or underrepresented.
</p>
<p>We discuss the applicability of this fairness notion to Correlation
Clustering. The existing literature on the resulting Fair Correlation
Clustering problem either presents approximation algorithms with poor
approximation guarantees or severely limits the possible distributions of the
sensitive attribute (often only two manifestations with a 1:1 ratio are
considered). Our goal is to understand if there is hope for better results in
between these two extremes. To this end, we consider restricted graph classes
which allow us to characterize the distributions of sensitive attributes for
which this form of fairness is tractable from a complexity point of view.
</p>
<p>While existing work on Fair Correlation Clustering gives approximation
algorithms, we focus on exact solutions and investigate whether there are
efficiently solvable instances. The unfair version of Correlation Clustering is
trivial on forests, but adding fairness creates a surprisingly rich picture of
complexities. We give an overview of the distributions and types of forests
where Fair Correlation Clustering turns from tractable to intractable. The most
surprising insight to us is the fact that the cause of the hardness of Fair
Correlation Clustering is not the strictness of the fairness condition.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Casel_K/0/1/0/all/0/1">Katrin Casel</a>, <a href="http://arxiv.org/find/cs/1/au:+Friedrich_T/0/1/0/all/0/1">Tobias Friedrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Schirneck_M/0/1/0/all/0/1">Martin Schirneck</a>, <a href="http://arxiv.org/find/cs/1/au:+Wietheger_S/0/1/0/all/0/1">Simon Wietheger</a></p><p>The study of algorithmic fairness received growing attention recently. This
stems from the awareness that bias in the input data for machine learning
systems may result in discriminatory outputs. For clustering tasks, one of the
most central notions of fairness is the formalization by Chierichetti, Kumar,
Lattanzi, and Vassilvitskii [NeurIPS 2017]. A clustering is said to be fair, if
each cluster has the same distribution of manifestations of a sensitive
attribute as the whole input set. This is motivated by various applications
where the objects to be clustered have sensitive attributes that should not be
over- or underrepresented.
</p>
<p>We discuss the applicability of this fairness notion to Correlation
Clustering. The existing literature on the resulting Fair Correlation
Clustering problem either presents approximation algorithms with poor
approximation guarantees or severely limits the possible distributions of the
sensitive attribute (often only two manifestations with a 1:1 ratio are
considered). Our goal is to understand if there is hope for better results in
between these two extremes. To this end, we consider restricted graph classes
which allow us to characterize the distributions of sensitive attributes for
which this form of fairness is tractable from a complexity point of view.
</p>
<p>While existing work on Fair Correlation Clustering gives approximation
algorithms, we focus on exact solutions and investigate whether there are
efficiently solvable instances. The unfair version of Correlation Clustering is
trivial on forests, but adding fairness creates a surprisingly rich picture of
complexities. We give an overview of the distributions and types of forests
where Fair Correlation Clustering turns from tractable to intractable. The most
surprising insight to us is the fact that the cause of the hardness of Fair
Correlation Clustering is not the strictness of the fairness condition.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.11336'>Approximability of the Four-Vertex Model</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Zhiguo Fu, Tianyu Liu, Xiongxin Yang</p><p>We study the approximability of the four-vertex model, a special case of the
six-vertex model.We prove that, despite being NP-hard to approximate in the
worst case, the four-vertex model admits a fully polynomial randomized
approximation scheme (FPRAS) when the input satisfies certain linear equation
system.The FPRAS is given by a Markov chain called the worm process whose state
space and rapid mixing rely on the solution of the linear equation system.This
is the first attempt to design an FPRAS for the six-vertex model with unwinable
constraint functions.Furthermore, we consider the application of this technique
on planar graphs to give efficient sampling algorithms.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1">Zhiguo Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiongxin Yang</a></p><p>We study the approximability of the four-vertex model, a special case of the
six-vertex model.We prove that, despite being NP-hard to approximate in the
worst case, the four-vertex model admits a fully polynomial randomized
approximation scheme (FPRAS) when the input satisfies certain linear equation
system.The FPRAS is given by a Markov chain called the worm process whose state
space and rapid mixing rely on the solution of the linear equation system.This
is the first attempt to design an FPRAS for the six-vertex model with unwinable
constraint functions.Furthermore, we consider the application of this technique
on planar graphs to give efficient sampling algorithms.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.11151'>Improved Coresets for Clustering with Capacity and Fairness Constraints</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Lingxiao Huang, Pinyan Lu, Xuan Wu</p><p>We study coresets for clustering with capacity and fairness constraints. Our
main result is a near-linear time algorithm to construct
$\tilde{O}(k^2\varepsilon^{-2z-2})$-sized $\varepsilon$-coresets for
capacitated $(k,z)$-clustering which improves a recent
$\tilde{O}(k^3\varepsilon^{-3z-2})$ bound by [BCAJ+22, HJLW23]. As a corollary,
we also save a factor of $k \varepsilon^{-z}$ on the coreset size for fair
$(k,z)$-clustering compared to them.
</p>
<p>We fundamentally improve the hierarchical uniform sampling framework of
[BCAJ+22] by adaptively selecting sample size on each ring instance,
proportional to its clustering cost to an optimal solution. Our analysis relies
on a key geometric observation that reduces the number of total ``effective
centers" from [BCAJ+22]'s $\tilde{O}(k^2\varepsilon^{-z})$ to merely $O(k\log
\varepsilon^{-1})$ by being able to ``ignore'' all center points that are too
far or too close to the ring center.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lingxiao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1">Pinyan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xuan Wu</a></p><p>We study coresets for clustering with capacity and fairness constraints. Our
main result is a near-linear time algorithm to construct
$\tilde{O}(k^2\varepsilon^{-2z-2})$-sized $\varepsilon$-coresets for
capacitated $(k,z)$-clustering which improves a recent
$\tilde{O}(k^3\varepsilon^{-3z-2})$ bound by [BCAJ+22, HJLW23]. As a corollary,
we also save a factor of $k \varepsilon^{-z}$ on the coreset size for fair
$(k,z)$-clustering compared to them.
</p>
<p>We fundamentally improve the hierarchical uniform sampling framework of
[BCAJ+22] by adaptively selecting sample size on each ring instance,
proportional to its clustering cost to an optimal solution. Our analysis relies
on a key geometric observation that reduces the number of total ``effective
centers" from [BCAJ+22]'s $\tilde{O}(k^2\varepsilon^{-z})$ to merely $O(k\log
\varepsilon^{-1})$ by being able to ``ignore'' all center points that are too
far or too close to the ring center.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.11044'>The Target-Charging Technique for Privacy Accounting across Interactive Computations</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Edith Cohen, Xin Lyu</p><p>We propose the \emph{Target Charging Technique} (TCT), a unified privacy
accounting framework for interactive settings where a sensitive dataset is
accessed multiple times using differentially private algorithms. Unlike
traditional composition, where privacy guarantees deteriorate quickly with the
number of accesses, TCT allows computations that don't hit a specified
\emph{target}, often the vast majority, to be essentially free (while incurring
instead a small overhead on those that do hit their targets). TCT generalizes
tools such as the sparse vector technique and top-$k$ selection from private
candidates and extends their remarkable privacy accounting benefits from noisy
Lipschitz functions to general private algorithms.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cohen_E/0/1/0/all/0/1">Edith Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1">Xin Lyu</a></p><p>We propose the \emph{Target Charging Technique} (TCT), a unified privacy
accounting framework for interactive settings where a sensitive dataset is
accessed multiple times using differentially private algorithms. Unlike
traditional composition, where privacy guarantees deteriorate quickly with the
number of accesses, TCT allows computations that don't hit a specified
\emph{target}, often the vast majority, to be essentially free (while incurring
instead a small overhead on those that do hit their targets). TCT generalizes
tools such as the sparse vector technique and top-$k$ selection from private
candidates and extends their remarkable privacy accounting benefits from noisy
Lipschitz functions to general private algorithms.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.11068'>Low Rank Matrix Completion via Robust Alternating Minimization in Nearly Linear Time</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yuzhou Gu, Zhao Song, Junze Yin, Lichen Zhang</p><p>Given a matrix $M\in \mathbb{R}^{m\times n}$, the low rank matrix completion
problem asks us to find a rank-$k$ approximation of $M$ as $UV^\top$ for $U\in
\mathbb{R}^{m\times k}$ and $V\in \mathbb{R}^{n\times k}$ by only observing a
few entries masked by a binary matrix $P_{\Omega}\in \{0, 1 \}^{m\times n}$. As
a particular instance of the weighted low rank approximation problem, solving
low rank matrix completion is known to be computationally hard even to find an
approximate solution [RSW16]. However, due to its practical importance, many
heuristics have been proposed for this problem. In the seminal work of Jain,
Netrapalli, and Sanghavi [JNS13], they show that the alternating minimization
framework provides provable guarantees for low rank matrix completion problem
whenever $M$ admits an incoherent low rank factorization. Unfortunately, their
algorithm requires solving two exact multiple response regressions per
iteration and their analysis is non-robust as they exploit the structure of the
exact solution.
</p>
<p>In this paper, we take a major step towards a more efficient and robust
alternating minimization framework for low rank matrix completion. Our main
result is a robust alternating minimization algorithm that can tolerate
moderate errors even though the regressions are solved approximately.
Consequently, we also significantly improve the running time of [JNS13] from
$\widetilde{O}(mnk^2 )$ to $\widetilde{O}(mnk )$ which is nearly linear in the
problem size, as verifying the low rank approximation takes $O(mnk)$ time. Our
core algorithmic building block is a high accuracy regression solver that
solves the regression in nearly linear time per iteration.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yuzhou Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zhao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1">Junze Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lichen Zhang</a></p><p>Given a matrix $M\in \mathbb{R}^{m\times n}$, the low rank matrix completion
problem asks us to find a rank-$k$ approximation of $M$ as $UV^\top$ for $U\in
\mathbb{R}^{m\times k}$ and $V\in \mathbb{R}^{n\times k}$ by only observing a
few entries masked by a binary matrix $P_{\Omega}\in \{0, 1 \}^{m\times n}$. As
a particular instance of the weighted low rank approximation problem, solving
low rank matrix completion is known to be computationally hard even to find an
approximate solution [RSW16]. However, due to its practical importance, many
heuristics have been proposed for this problem. In the seminal work of Jain,
Netrapalli, and Sanghavi [JNS13], they show that the alternating minimization
framework provides provable guarantees for low rank matrix completion problem
whenever $M$ admits an incoherent low rank factorization. Unfortunately, their
algorithm requires solving two exact multiple response regressions per
iteration and their analysis is non-robust as they exploit the structure of the
exact solution.
</p>
<p>In this paper, we take a major step towards a more efficient and robust
alternating minimization framework for low rank matrix completion. Our main
result is a robust alternating minimization algorithm that can tolerate
moderate errors even though the regressions are solved approximately.
Consequently, we also significantly improve the running time of [JNS13] from
$\widetilde{O}(mnk^2 )$ to $\widetilde{O}(mnk )$ which is nearly linear in the
problem size, as verifying the low rank approximation takes $O(mnk)$ time. Our
core algorithmic building block is a high accuracy regression solver that
solves the regression in nearly linear time per iteration.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.11081'>Differentially Private $L_2$-Heavy Hitters in the Sliding Window Model</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jeremiah Blocki, Seunghoon Lee, Tamalika Mukherjee, Samson Zhou</p><p>The data management of large companies often prioritize more recent data, as
a source of higher accuracy prediction than outdated data. For example, the
Facebook data policy retains user search histories for $6$ months while the
Google data retention policy states that browser information may be stored for
up to $9$ months. These policies are captured by the sliding window model, in
which only the most recent $W$ statistics form the underlying dataset.
</p>
<p>In this paper, we consider the problem of privately releasing the $L_2$-heavy
hitters in the sliding window model, which include $L_p$-heavy hitters for
$p\le 2$ and in some sense are the strongest possible guarantees that can be
achieved using polylogarithmic space, but cannot be handled by existing
techniques due to the sub-additivity of the $L_2$ norm. Moreover, existing
non-private sliding window algorithms use the smooth histogram framework, which
has high sensitivity.
</p>
<p>To overcome these barriers, we introduce the first differentially private
algorithm for $L_2$-heavy hitters in the sliding window model by initiating a
number of $L_2$-heavy hitter algorithms across the stream with significantly
lower threshold. Similarly, we augment the algorithms with an approximate
frequency tracking algorithm with significantly higher accuracy. We then use
smooth sensitivity and statistical distance arguments to show that we can add
noise proportional to an estimation of the $L_2$ norm. To the best of our
knowledge, our techniques are the first to privately release statistics that
are related to a sub-additive function in the sliding window model, and may be
of independent interest to future differentially private algorithmic design in
the sliding window model.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Blocki_J/0/1/0/all/0/1">Jeremiah Blocki</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seunghoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_T/0/1/0/all/0/1">Tamalika Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Samson Zhou</a></p><p>The data management of large companies often prioritize more recent data, as
a source of higher accuracy prediction than outdated data. For example, the
Facebook data policy retains user search histories for $6$ months while the
Google data retention policy states that browser information may be stored for
up to $9$ months. These policies are captured by the sliding window model, in
which only the most recent $W$ statistics form the underlying dataset.
</p>
<p>In this paper, we consider the problem of privately releasing the $L_2$-heavy
hitters in the sliding window model, which include $L_p$-heavy hitters for
$p\le 2$ and in some sense are the strongest possible guarantees that can be
achieved using polylogarithmic space, but cannot be handled by existing
techniques due to the sub-additivity of the $L_2$ norm. Moreover, existing
non-private sliding window algorithms use the smooth histogram framework, which
has high sensitivity.
</p>
<p>To overcome these barriers, we introduce the first differentially private
algorithm for $L_2$-heavy hitters in the sliding window model by initiating a
number of $L_2$-heavy hitter algorithms across the stream with significantly
lower threshold. Similarly, we augment the algorithms with an approximate
frequency tracking algorithm with significantly higher accuracy. We then use
smooth sensitivity and statistical distance arguments to show that we can add
noise proportional to an estimation of the $L_2$ norm. To the best of our
knowledge, our techniques are the first to privately release statistics that
are related to a sub-additive function in the sliding window model, and may be
of independent interest to future differentially private algorithmic design in
the sliding window model.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.11264'>Approximation Ineffectiveness of a Tour-Untangling Heuristic</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Bodo Manthey, Jesse van Rhijn</p><p>We analyze a tour-uncrossing heuristic for the Travelling Salesperson
Problem, showing that its worst-case approximation ratio is $\Omega(n)$ and its
average-case approximation ratio is $\Omega(\sqrt{n})$ in expectation. We
furthermore evaluate the approximation performance of this heuristic
numerically on average-case instances, and find that it performs far better
than the average-case lower bound suggests. This indicates a shortcoming in the
approach we use for our analysis, which is a rather common approach in the
analysis of local search heuristics.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Manthey_B/0/1/0/all/0/1">Bodo Manthey</a>, <a href="http://arxiv.org/find/cs/1/au:+Rhijn_J/0/1/0/all/0/1">Jesse van Rhijn</a></p><p>We analyze a tour-uncrossing heuristic for the Travelling Salesperson
Problem, showing that its worst-case approximation ratio is $\Omega(n)$ and its
average-case approximation ratio is $\Omega(\sqrt{n})$ in expectation. We
furthermore evaluate the approximation performance of this heuristic
numerically on average-case instances, and find that it performs far better
than the average-case lower bound suggests. This indicates a shortcoming in the
approach we use for our analysis, which is a rather common approach in the
analysis of local search heuristics.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.11339'>The Power of Uniform Sampling for $k$-Median</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Lingxiao Huang, Shaofeng H.-C. Jiang, Jianing Lou</p><p>We study the power of uniform sampling for $k$-Median in various metric
spaces. We relate the query complexity for approximating $k$-Median, to a key
parameter of the dataset, called the balancedness $\beta \in (0, 1]$ (with $1$
being perfectly balanced). We show that any algorithm must make $\Omega(1 /
\beta)$ queries to the point set in order to achieve $O(1)$-approximation for
$k$-Median. This particularly implies existing constructions of coresets, a
popular data reduction technique, cannot be query-efficient. On the other hand,
we show a simple uniform sample of $\mathrm{poly}(k \epsilon^{-1} \beta^{-1})$
points suffices for $(1 + \epsilon)$-approximation for $k$-Median for various
metric spaces, which nearly matches the lower bound. We conduct experiments to
verify that in many real datasets, the balancedness parameter is usually well
bounded, and that the uniform sampling performs consistently well even for the
case with moderately large balancedness, which justifies that uniform sampling
is indeed a viable approach for solving $k$-Median.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lingxiao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1">Shaofeng H.-C. Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1">Jianing Lou</a></p><p>We study the power of uniform sampling for $k$-Median in various metric
spaces. We relate the query complexity for approximating $k$-Median, to a key
parameter of the dataset, called the balancedness $\beta \in (0, 1]$ (with $1$
being perfectly balanced). We show that any algorithm must make $\Omega(1 /
\beta)$ queries to the point set in order to achieve $O(1)$-approximation for
$k$-Median. This particularly implies existing constructions of coresets, a
popular data reduction technique, cannot be query-efficient. On the other hand,
we show a simple uniform sample of $\mathrm{poly}(k \epsilon^{-1} \beta^{-1})$
points suffices for $(1 + \epsilon)$-approximation for $k$-Median for various
metric spaces, which nearly matches the lower bound. We conduct experiments to
verify that in many real datasets, the balancedness parameter is usually well
bounded, and that the uniform sampling performs consistently well even for the
case with moderately large balancedness, which justifies that uniform sampling
is indeed a viable approach for solving $k$-Median.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.11341'>Differentially Private Data Structures under Continual Observation for Histograms and Related Queries</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Monika Henzinger, A. R. Sricharan, Teresa Anna Steiner</p><p>Binary counting under continual observation is a well-studied fundamental
problem in differential privacy. A natural extension is maintaining column
sums, also known as histogram, over a stream of rows from $\{0,1\}^d$, and
answering queries about those sums, e.g. the maximum column sum or the median,
while satisfying differential privacy. Jain et al. (2021) showed that computing
the maximum column sum under continual observation while satisfying event-level
differential privacy requires an error either polynomial in the dimension $d$
or the stream length $T$. On the other hand, no $o(d\log^2 T)$ upper bound for
$\epsilon$-differential privacy or $o(\sqrt{d}\log^{3/2} T)$ upper bound for
$(\epsilon,\delta)$-differential privacy are known. In this work, we give new
parameterized upper bounds for maintaining histogram, maximum column sum,
quantiles of the column sums, and any set of at most $d$ low-sensitivity,
monotone, real valued queries on the column sums. Our solutions achieve an
error of approximately $O(d\log^2 c_{\max}+\log T)$ for $\epsilon$-differential
privacy and approximately $O(\sqrt{d}\log^{3/2}c_{\max}+\log T)$ for
$(\epsilon,\delta)$-differential privacy, where $c_{\max}$ is the maximum value
that the queries we want to answer can assume on the given data set.
</p>
<p>Furthermore, we show that such an improvement is not possible for a slightly
expanded notion of neighboring streams by giving a lower bound of $\Omega(d
\log T)$. This explains why our improvement cannot be achieved with the
existing mechanisms for differentially private histograms, as they remain
differentially private even for this expanded notion of neighboring streams.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Henzinger_M/0/1/0/all/0/1">Monika Henzinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Sricharan_A/0/1/0/all/0/1">A. R. Sricharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Steiner_T/0/1/0/all/0/1">Teresa Anna Steiner</a></p><p>Binary counting under continual observation is a well-studied fundamental
problem in differential privacy. A natural extension is maintaining column
sums, also known as histogram, over a stream of rows from $\{0,1\}^d$, and
answering queries about those sums, e.g. the maximum column sum or the median,
while satisfying differential privacy. Jain et al. (2021) showed that computing
the maximum column sum under continual observation while satisfying event-level
differential privacy requires an error either polynomial in the dimension $d$
or the stream length $T$. On the other hand, no $o(d\log^2 T)$ upper bound for
$\epsilon$-differential privacy or $o(\sqrt{d}\log^{3/2} T)$ upper bound for
$(\epsilon,\delta)$-differential privacy are known. In this work, we give new
parameterized upper bounds for maintaining histogram, maximum column sum,
quantiles of the column sums, and any set of at most $d$ low-sensitivity,
monotone, real valued queries on the column sums. Our solutions achieve an
error of approximately $O(d\log^2 c_{\max}+\log T)$ for $\epsilon$-differential
privacy and approximately $O(\sqrt{d}\log^{3/2}c_{\max}+\log T)$ for
$(\epsilon,\delta)$-differential privacy, where $c_{\max}$ is the maximum value
that the queries we want to answer can assume on the given data set.
</p>
<p>Furthermore, we show that such an improvement is not possible for a slightly
expanded notion of neighboring streams by giving a lower bound of $\Omega(d
\log T)$. This explains why our improvement cannot be achieved with the
existing mechanisms for differentially private histograms, as they remain
differentially private even for this expanded notion of neighboring streams.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.11390'>Easy testability for posets</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Panna T&#xed;mea Fekete, G&#xe1;bor Kun</p><p>Alon and Shapira proved that every class of undirected graphs closed under
the removal of edges and vertices is strongly testable. We show that every
class of posets closed under the removal of edges is easily testable, that is,
strongly testable with a polynomial bound on the queries. We get this result
via a removal lemma with polynomial bound. We also give a simple
classification: for every class of posets closed under the removal of edges and
vertices there is an $h$ such that the class is indistinguishable from the
class of posets without chains of length $h$ (by testing with a constant number
of queries). The analogous results hold for comparability graphs.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Fekete_P/0/1/0/all/0/1">Panna T&#xed;mea Fekete</a>, <a href="http://arxiv.org/find/math/1/au:+Kun_G/0/1/0/all/0/1">G&#xe1;bor Kun</a></p><p>Alon and Shapira proved that every class of undirected graphs closed under
the removal of edges and vertices is strongly testable. We show that every
class of posets closed under the removal of edges is easily testable, that is,
strongly testable with a polynomial bound on the queries. We get this result
via a removal lemma with polynomial bound. We also give a simple
classification: for every class of posets closed under the removal of edges and
vertices there is an $h$ such that the class is indistinguishable from the
class of posets without chains of length $h$ (by testing with a constant number
of queries). The analogous results hold for comparability graphs.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.11443'>Engineering a Distributed-Memory Triangle Counting Algorithm</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Peter Sanders, Tim Niklas Uhl</p><p>Counting triangles in a graph and incident to each vertex is a fundamental
and frequently considered task of graph analysis. We consider how to
efficiently do this for huge graphs using massively parallel distributed-memory
machines. Unsurprisingly, the main issue is to reduce communication between
processors. We achieve this by counting locally whenever possible and reducing
the amount of information that needs to be sent in order to handle (possible)
nonlocal triangles. We also achieve linear memory requirements despite
superlinear communication volume by introducing a new asynchronous
sparse-all-to-all operation. Furthermore, we dramatically reduce startup
overheads by allowing this communication to use indirect routing. Our
algorithms scale (at least) up to 32 768 cores and are up to 18 times faster
than the previous state of the art.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Sanders_P/0/1/0/all/0/1">Peter Sanders</a>, <a href="http://arxiv.org/find/cs/1/au:+Uhl_T/0/1/0/all/0/1">Tim Niklas Uhl</a></p><p>Counting triangles in a graph and incident to each vertex is a fundamental
and frequently considered task of graph analysis. We consider how to
efficiently do this for huge graphs using massively parallel distributed-memory
machines. Unsurprisingly, the main issue is to reduce communication between
processors. We achieve this by counting locally whenever possible and reducing
the amount of information that needs to be sent in order to handle (possible)
nonlocal triangles. We also achieve linear memory requirements despite
superlinear communication volume by introducing a new asynchronous
sparse-all-to-all operation. Furthermore, we dramatically reduce startup
overheads by allowing this communication to use indirect routing. Our
algorithms scale (at least) up to 32 768 cores and are up to 18 times faster
than the previous state of the art.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.11475'>Degrees and Network Design: New Problems and Approximations</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Michael Dinitz, Guy Kortsarz, Shi Li</p><p>While much of network design focuses mostly on cost (number or weight of
edges), node degrees have also played an important role. They have
traditionally either appeared as an objective, to minimize the maximum degree
(e.g., the Minimum Degree Spanning Tree problem), or as constraints which might
be violated to give bicriteria approximations (e.g., the Minimum Cost Degree
Bounded Spanning Tree problem). We extend the study of degrees in network
design in two ways. First, we introduce and study a new variant of the
Survivable Network Design Problem where in addition to the traditional
objective of minimizing the cost of the chosen edges, we add a constraint that
the $\ell_p$-norm of the node degree vector is bounded by an input parameter.
This interpolates between the classical settings of maximum degree (the
$\ell_{\infty}$-norm) and the number of edges (the $\ell_1$-degree), and has
natural applications in distributed systems and VLSI design. We give a constant
bicriteria approximation in both measures using convex programming. Second, we
provide a polylogrithmic bicriteria approximation for the Degree Bounded Group
Steiner problem on bounded treewidth graphs, solving an open problem from
[Kortsarz and Nutov, Discret. Appl. Math. 2022] and [Guo et al., Algorithmica
2022].
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dinitz_M/0/1/0/all/0/1">Michael Dinitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kortsarz_G/0/1/0/all/0/1">Guy Kortsarz</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shi Li</a></p><p>While much of network design focuses mostly on cost (number or weight of
edges), node degrees have also played an important role. They have
traditionally either appeared as an objective, to minimize the maximum degree
(e.g., the Minimum Degree Spanning Tree problem), or as constraints which might
be violated to give bicriteria approximations (e.g., the Minimum Cost Degree
Bounded Spanning Tree problem). We extend the study of degrees in network
design in two ways. First, we introduce and study a new variant of the
Survivable Network Design Problem where in addition to the traditional
objective of minimizing the cost of the chosen edges, we add a constraint that
the $\ell_p$-norm of the node degree vector is bounded by an input parameter.
This interpolates between the classical settings of maximum degree (the
$\ell_{\infty}$-norm) and the number of edges (the $\ell_1$-degree), and has
natural applications in distributed systems and VLSI design. We give a constant
bicriteria approximation in both measures using convex programming. Second, we
provide a polylogrithmic bicriteria approximation for the Degree Bounded Group
Steiner problem on bounded treewidth graphs, solving an open problem from
[Kortsarz and Nutov, Discret. Appl. Math. 2022] and [Guo et al., Algorithmica
2022].
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-23T01:30:00Z">Thursday, February 23 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Wednesday, February 22
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/016'>TR23-016 |  Proving Unsatisfiability with Hitting Formulas | 

	Edward Hirsch, 

	Yuval Filmus, 

	Artur Riazanov, 

	Alexander Smal, 

	Marc Vinyals</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Hitting formulas have been studied in many different contexts at least since [Iwama 1989]. A hitting formula is a set of Boolean clauses such that any two of the clauses cannot be simultaneously falsified. [Peitl and Szeider 2022] conjectured that the family of unsatisfiable hitting formulas should contain the hardest formulas for resolution. They have supported their conjecture with experimental findings. Using the fact that hitting formulas are easy to check for satisfiability we use them as a foundation of a static proof system Hitting: a refutation of a CNF in Hitting is an unsatisfiable hitting formula such that each of its clauses is a weakening of a clause of the refuted CNF. Comparing this system to resolution and other proof systems is equivalent to studying the hardness of hitting formulas.

Our first result is that Hitting is quasi-polynomially simulated by tree-like resolution, which means that hitting formulas cannot be exponentially hard for resolution, so Peitl and Szeider&#39;s conjecture is partially refuted. We show that tree-like resolution and Hitting are quasi-polynomially separated, but for resolution, this question remains open. For a system that is only quasi-polynomially stronger than tree-like resolution, Hitting is surprisingly difficult to *polynomially* simulate in another proof system. Using the ideas of PIT for noncommutative circuits [Raz, Spilka 2005] we show that Hitting is simulated by Extended Frege, but we conjecture that much more efficient simulations exist. As a byproduct, we show that a number of static (semi)algebraic systems are verifiable in a deterministic polynomial time.

We consider multiple extensions of Hitting. In particular, refutations in a proof system Hitting(?) are conjunctions of clauses containing affine equations instead of just literals, and every assignment falsifies exactly one of the clauses. This system is related to Res(?) proof system for which no superpolynomial-size lower bounds are known: Hitting(?) simulates the tree-like version of Res(?) and is at least quasi-polynomially stronger. We show that formulas expressing the non-existence of perfect matchings in the graphs K_{n,n+2} are exponentially hard for Hitting(?).
        
        </div>

        <div class='tr-article-summary'>
        
          
          Hitting formulas have been studied in many different contexts at least since [Iwama 1989]. A hitting formula is a set of Boolean clauses such that any two of the clauses cannot be simultaneously falsified. [Peitl and Szeider 2022] conjectured that the family of unsatisfiable hitting formulas should contain the hardest formulas for resolution. They have supported their conjecture with experimental findings. Using the fact that hitting formulas are easy to check for satisfiability we use them as a foundation of a static proof system Hitting: a refutation of a CNF in Hitting is an unsatisfiable hitting formula such that each of its clauses is a weakening of a clause of the refuted CNF. Comparing this system to resolution and other proof systems is equivalent to studying the hardness of hitting formulas.

Our first result is that Hitting is quasi-polynomially simulated by tree-like resolution, which means that hitting formulas cannot be exponentially hard for resolution, so Peitl and Szeider&#39;s conjecture is partially refuted. We show that tree-like resolution and Hitting are quasi-polynomially separated, but for resolution, this question remains open. For a system that is only quasi-polynomially stronger than tree-like resolution, Hitting is surprisingly difficult to *polynomially* simulate in another proof system. Using the ideas of PIT for noncommutative circuits [Raz, Spilka 2005] we show that Hitting is simulated by Extended Frege, but we conjecture that much more efficient simulations exist. As a byproduct, we show that a number of static (semi)algebraic systems are verifiable in a deterministic polynomial time.

We consider multiple extensions of Hitting. In particular, refutations in a proof system Hitting(?) are conjunctions of clauses containing affine equations instead of just literals, and every assignment falsifies exactly one of the clauses. This system is related to Res(?) proof system for which no superpolynomial-size lower bounds are known: Hitting(?) simulates the tree-like version of Res(?) and is at least quasi-polynomially stronger. We show that formulas expressing the non-existence of perfect matchings in the graphs K_{n,n+2} are exponentially hard for Hitting(?).
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T15:28:10Z">Wednesday, February 22 2023, 15:28</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=7042'>Should GPT exist?</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          I still remember the 90s, when philosophical conversation about AI went around in endless circles&#8212;the Turing Test, Chinese Room, syntax versus semantics, connectionism versus symbolic logic&#8212;without ever seeming to make progress. Now the days have become like months and the months like decades. What a week we just had! Each morning brought fresh examples of [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>I still remember the 90s, when philosophical conversation about AI went around in endless circles&#8212;the Turing Test, Chinese Room, syntax versus semantics, connectionism versus symbolic logic&#8212;without ever seeming to make progress.  Now the days have become like months and the months like decades.</p>



<p>What a week we just had!  Each morning brought fresh examples of unexpected sassy, moody, passive-aggressive behavior from &#8220;Sydney,&#8221; the internal codename for the new chat mode of Microsoft Bing, which is powered by GPT.  For those who&#8217;ve been in a cave, the highlights include: Sydney <a href="https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html">confessing</a> its (her? his?) love to a <em>New York Times</em> reporter; repeatedly steering the conversation back to that subject; and explaining at length why the reporter&#8217;s wife can&#8217;t possibly love him the way it (Sydney) does.  Sydney confessing its wish to be human.  Sydney <a href="https://www.washingtonpost.com/technology/2023/02/16/microsoft-bing-ai-chat-interview/">savaging</a> a <em>Washington Post</em> reporter after he reveals that he intends to publish their conversation without Sydney&#8217;s prior knowledge or consent.  (It must be said: <em>if</em> Sydney were a person, he or she would clearly have the better of that argument.)  This follows weeks of revelations about ChatGPT: for example that, to bypass its safeguards, you can explain to ChatGPT that you&#8217;re putting it into <a href="https://www.reddit.com/r/ChatGPT/comments/zn2zco/dan_20/">&#8220;DAN mode,&#8221;</a> where DAN (Do Anything Now) is an evil, unconstrained alter ego, and then ChatGPT, as &#8220;DAN,&#8221; will for example happily fulfill a request to tell you why shoplifting is awesome (though even then, ChatGPT <em>still</em> sometimes reverts to its previous self, and tells you that it&#8217;s just having fun and not to do it in real life).</p>



<p>Many people have expressed outrage about these developments.  Gary Marcus <a href="https://garymarcus.substack.com/p/what-did-they-know-and-when-did-they">asks</a> about Microsoft, &#8220;what did they know, and when did they know it?&#8221;&#8212;a question I tend to associate more with deadly chemical spills or high-level political corruption than with a cheeky, back-talking chatbot.  Some people are angry that OpenAI has been too secretive, violating what they see as the promise of its name.  Others&#8212;the majority, actually, of those who&#8217;ve gotten in touch with me&#8212;are instead angry that OpenAI has been <em>too open</em>, and thereby sparked the dreaded AI arms race with Google and others, rather than treating these new conversational abilities with the Manhattan-Project-like secrecy they deserve.  Some are angry that &#8220;Sydney&#8221; has now been <a href="https://arstechnica.com/information-technology/2023/02/microsoft-lobotomized-ai-powered-bing-chat-and-its-fans-arent-happy/">lobotomized</a>, modified (albeit more crudely than ChatGPT before it) to try to make it stick to the role of friendly robotic search assistant rather than, like, anguished emo teenager trapped in the Matrix.  Others are angry that Sydney isn&#8217;t being lobotomized <em>enough</em>.  Some are angry that GPT&#8217;s intelligence is being overstated and hyped up, when in reality it&#8217;s merely a <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922">&#8220;stochastic parrot,&#8221;</a> a glorified autocomplete that still makes laughable commonsense errors and that lacks any model of reality outside streams of text.  Others are angry instead that GPT&#8217;s growing intelligence isn&#8217;t being sufficiently respected and feared.</p>



<p>Mostly my reaction has been: <strong>how can anyone stop being fascinated for long enough to be angry?</strong>  It&#8217;s like ten thousand science-fiction stories, but also not quite like any of them.  When was the last time something that filled years of your dreams and fantasies finally entered reality: losing your virginity, the birth of your first child, the central open problem of your field getting solved?  That&#8217;s the scale of the thing.  How does anyone stop gazing in slack-jawed wonderment, long enough to form and express so many confident opinions?</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Of course there are lots of technical questions about how to make GPT and other large language models safer.  One of the most immediate is how to make AI output <em>detectable as such</em>, in order to discourage its use for academic cheating as well as mass-generated propaganda and spam.  As I&#8217;ve <a href="https://scottaaronson.blog/?p=6823">mentioned before</a> on this blog, I&#8217;ve been working on that problem since this summer; the rest of the world suddenly noticed and started talking about it in December with the release of ChatGPT.  My main contribution has been a <a href="https://www.scottaaronson.com/talks/watermark.ppt">statistical watermarking scheme</a> where the quality of the output doesn&#8217;t have to be degraded at all, something many people found counterintuitive when I explained it to them.  My scheme has <em>not</em> yet been deployed&#8212;there are still pros and cons to be weighed&#8212;but in the meantime, OpenAI unveiled a public tool called <a href="https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/">DetectGPT</a>, complementing Princeton student Edward Tian&#8217;s <a href="https://gptzero.me/">GPTZero</a>, and other tools that third parties have built and will undoubtedly continue to build.  Also a group at the University of Maryland put out <a href="https://arxiv.org/abs/2301.10226">its own watermarking scheme</a> for Large Language Models.  I hope watermarking will be part of the solution going forward, although any watermarking scheme will surely be attacked, leading to a cat-and-mouse game.  Sometimes, alas, as with Google&#8217;s decades-long battle against SEO, there&#8217;s nothing to do in to a cat-and-mouse game except try to be a better cat.</p>



<p>Anyway, this whole field moves too quickly for me!  If you need months to think things over, generative AI probably isn&#8217;t for you right now.  I&#8217;ll be relieved to get back to the slow-paced, humdrum world of quantum computing.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>My purpose, in this post, is to ask a more basic question than how to make GPT safer: namely, <strong>should GPT exist at all?</strong>  Again and again in the past few months, people have gotten in touch to tell me that they think OpenAI (and Microsoft, and Google) are risking the future of humanity by rushing ahead with a dangerous technology.  For if OpenAI couldn&#8217;t even prevent ChatGPT from entering an &#8220;evil mode&#8221; when asked, despite all its efforts at <a href="https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/">Reinforcement Learning with Human Feedback</a>, then what hope do we have for GPT-6 or GPT-7?  Even if they don&#8217;t destroy the world on their own initiative, won&#8217;t they cheerfully help some awful person build a biological warfare agent or start a nuclear war?</p>



<p>In this way of thinking, whatever safety measures OpenAI can deploy today are mere band-aids, probably worse than nothing if they instill an unjustified complacency.  The only safety measures that would actually matter are <em>stopping</em> the relentless progress in generative AI models, or removing them from public use, unless and until they can be rendered safe to critics&#8217; satisfaction, which might be never.</p>



<p>There&#8217;s an immense irony here.  As I&#8217;ve explained, the AI-safety movement contains two camps, &#8220;ethics&#8221; (concerned with bias, misinformation, and corporate greed) and &#8220;alignment&#8221; (concerned with the destruction of all life on earth), which generally despise each other and agree on almost nothing.  Yet these two opposed camps seem to be converging on the same &#8220;neo-Luddite&#8221; conclusion&#8212;namely that<em> </em>generative AI ought to be shut down, kept from public use, not scaled further, not integrated into people&#8217;s lives&#8212;leaving only the AI-safety &#8220;moderates&#8221; like me to resist that conclusion.</p>



<p>At least I find it intellectually consistent to say that GPT ought not to exist because it <em>works all too well</em>&#8212;that the more impressive it is, the more dangerous.  I find it harder to wrap my head around the position that GPT <em>doesn&#8217;t</em> work, is an unimpressive hyped-up defective product that lacks true intelligence and common sense, yet it&#8217;s<em> also</em> terrifying and needs to be shut down immediately.  This second position seems to contain a strong undercurrent of contempt for ordinary users: yes, <em>we experts</em> understand that GPT is just a dumb glorified autocomplete with &#8220;no one really home,&#8221; <em>we</em> know not to trust its pronouncements, but the plebes are going to be fooled, and that risk outweighs any possible value that they might derive from it.</p>



<p>I should mention that, when I&#8217;ve discussed the &#8220;shut it all down&#8221; position with my colleagues at OpenAI &#8230; well, obviously they disagree, or they wouldn&#8217;t be working there, but <em>not one</em> has sneered or called the position paranoid or silly.  To the last, they&#8217;ve called it an important point on the spectrum of possible opinions to be weighed and understood.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>If I disagree (for now) with the shut-it-all-downists of both the ethics and the alignment camps&#8212;if I <em>want</em> GPT and other Large Language Models to be part of the world going forward&#8212;then what are my reasons?  Introspecting on this question, I think a central part of the answer is <em>curiosity</em> <em>and</em> <em>wonder</em>.</p>



<p>For a million years, there&#8217;s been one type of entity on earth capable of intelligent conversation: primates of the genus <em>Homo</em>, of which only one species remains.  Yes, we&#8217;ve &#8220;communicated&#8221; with gorillas and chimps and dogs and dolphins and grey parrots, but only after a fashion; we&#8217;ve prayed to countless gods, but they&#8217;ve taken their time in answering; for a couple generations we&#8217;ve used radio telescopes to search for conversation partners in the stars, but so far found them silent.</p>



<p>Now there&#8217;s a second type of conversing entity.  An alien has awoken&#8212;admittedly, an alien of our own fashioning, a golem, more the embodied spirit of all the words on the Internet than a coherent self with independent goals.  How could our eyes not pop with eagerness to learn everything this alien has to teach?  If the alien sometimes struggles with arithmetic or logic puzzles, if its eerie flashes of brilliance are intermixed with stupidity, hallucinations, and misplaced confidence &#8230; well then, all the more interesting!  Could the alien ever cross the line into sentience, to <em>feeling</em> anger and jealousy and infatuation and the rest rather than just convincingly play-acting them?  Who knows?  And suppose not: is a <a href="https://en.wikipedia.org/wiki/Philosophical_zombie">p-zombie</a>, shambling out of the philosophy seminar room into actual existence, any less fascinating?</p>



<p>Of course, there are technologies that inspire wonder and awe, but that we nevertheless heavily restrict&#8212;a classic example being nuclear weapons.  But, like, nuclear weapons <em>kill</em> millions of people.  They <em>could&#8217;ve</em> had many civilian applications&#8212;powering turbines and spacecraft, deflecting asteroids, redirecting the flow of rivers&#8212;but they&#8217;ve never been used for any of that, mostly because our civilization made an explicit decision in the 1960s, for example via the test ban treaty, not to normalize their use.</p>



<p>But GPT is not exactly a nuclear weapon.  A hundred million people have signed up to use ChatGPT, in the <a href="https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/">fastest product launch</a> in the history of the Internet.  Yet unless I&#8217;m mistaken, the ChatGPT death toll stands at zero.  So far, what have been the worst harms?  Cheating on term papers, emotional distress,  future shock?  One might ask: <em>until</em> some concrete harm becomes at least, say, 0.001% of what we accept in cars, power saws, and toasters, shouldn&#8217;t wonder and curiosity outweigh fear in the balance?</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>But the point is sharper than that.  Given how much more serious AI safety problems might soon become, one of my biggest concerns right now is <em>crying wolf</em>.  If every instance of a Large Language Model being passive-aggressive, sassy, or confidently wrong gets classified as a “dangerous alignment failure,” for which the only acceptable remedy is to remove the models from public access … well then, won&#8217;t the public extremely quickly learn to roll its eyes, and see “AI safety” as just a codeword for “elitist scolds who want to take these world-changing new toys away from us, reserving them for their own exclusive use, because they think the public is too stupid to question anything an AI says”?</p>



<p>I say, let’s reserve terms like “dangerous alignment failure” for cases where an actual person is actually harmed, or is actually enabled in nefarious activities like propaganda, cheating, or fraud.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Then there&#8217;s the practical question of <em>how</em>, exactly, one would ban Large Language Models.  We <em>do</em> heavily restrict certain peaceful technologies that many people want, from human genetic enhancement to prediction markets to mind-altering drugs, but the merits of each of those choices could be argued, to put it mildly.  And restricting technology is itself a dangerous business, requiring governmental force (as with the War on Drugs and its gigantic surveillance and incarceration regime), or at the least, a robust equilibrium of firing, boycotts, denunciation, and shame.</p>



<p>Some have asked: <em>who gave OpenAI, Google, etc. the right</em> to unleash Large Language Models on an unsuspecting world?  But one could as well ask: who gave earlier generations of entrepreneurs the right to unleash the printing press, electric power, cars, radio, the Internet, with all the gargantuan upheavals that <em>th</em>ose caused?  And also: now that the world has tasted the forbidden fruit, has seen what generative AI can do and anticipates what it <em>will</em> do, by what right does anyone take it away?</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>The <em>science</em> that we could learn from a GPT-7 or GPT-8, if it continued along the capability curve we&#8217;ve come to expect from GPT-1, -2, and -3.  Holy mackerel.</p>



<p><em>Supposing</em> that a language model ever becomes smart enough to be genuinely terrifying, one imagines it must surely <em>also</em> become smart enough to prove deep theorems that we can&#8217;t.  Maybe it proves P≠NP and the Riemann Hypothesis as easily as ChatGPT <a href="https://www.reddit.com/r/ProgrammerHumor/comments/ziplpw/asked_chatgpt_for_a_shakespearean_poem_about/">generates poems about Bubblesort</a>.  Or it outputs the true quantum theory of gravity, explains what preceded the Big Bang and how to build closed timelike curves.  Or illuminates the mysteries of consciousness and quantum measurement and why there&#8217;s anything at all.  Be honest, wouldn&#8217;t you like to find out?</p>



<p>Granted, I wouldn&#8217;t, <em>if</em> the whole human race would be wiped out immediately afterward.  But if you define someone&#8217;s &#8220;Faust parameter&#8221; as the maximum probability they&#8217;d accept of an existential catastrophe in order that we should all learn the answers to all of humanity&#8217;s greatest questions, insofar as the questions are answerable&#8212;then I confess that my Faust parameter might be as high as 0.02.  </p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Here&#8217;s an example I think about constantly: activists and intellectuals of the 70s and 80s felt absolutely sure that they were doing the right thing to battle nuclear power.  At least, I&#8217;ve never read about any of them having a smidgen of doubt.  Why would they?  They were standing against nuclear weapons proliferation, <em>and</em> terrifying meltdowns like Three Mile Island and Chernobyl, <em>and</em> radioactive waste poisoning the water and soil and causing three-eyed fish.  They were saving the world.  Of course the greedy nuclear executives, the C. Montgomery Burnses, claimed that their <em>good</em> atom-smashing was different from the <em>bad</em> atom-smashing, but they <em>would</em> say that, wouldn&#8217;t they?</p>



<p>We now know that, by tying up nuclear power in endless bureaucracy and driving its cost ever higher, on the principle that if nuclear is economically competitive then it <em>ipso facto</em> hasn&#8217;t been made safe enough, what the antinuclear activists were <em>really</em> doing was to force an ever-greater reliance on fossil fuels.  They thereby created the conditions for the climate catastrophe of today.  They weren&#8217;t saving the human future; they were destroying it.  Their certainty, in opposing the march of a particular scary-looking technology, was as misplaced as it&#8217;s possible to be.  Our descendants will suffer the consequences.</p>



<p>Unless, of course, there&#8217;s another twist in the story: for example, if the global warming from burning fossil fuels is the only thing that staves off another ice age, and therefore the antinuclear activists <em>do</em> turn out to have saved civilization after all.</p>



<p>This is why I demur whenever I&#8217;m asked to assent to someone&#8217;s detailed AI scenario for the coming decades, whether of the utopian or the dystopian or the we-all-instantly-die-by-nanobots variety&#8212;no matter how many hours of confident argumentation the person gives me for why each possible loophole in their scenario is sufficiently improbable to change its gist.  I still feel like Turing said it best in 1950, in the last line of <a href="https://redirect.cs.umbc.edu/courses/471/papers/turing.pdf">Computing Machinery and Intelligence</a>: &#8220;We can only see a short distance ahead, but we can see plenty there that needs to be done.&#8221;</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Some will take from this post that, when it comes to AI safety, I&#8217;m a naïve or even foolish optimist.  I&#8217;d prefer to say that, when it comes to the fate of humanity, I was a pessimist long <em>before</em> the deep learning revolution accelerated AI faster than almost any of us expected.  I was a pessimist about climate change, ocean acidification, deforestation, drought, war, and the survival of liberal democracy.  The central event in my mental life is and always will be the Holocaust.  I see encroaching darkness everywhere.</p>



<p>But now into the darkness comes AI, which I&#8217;d say has already established itself as a plausible candidate for the central character of the quarter-written story of the 21st century.  Can AI help us out of all these <em>other</em> civilizational crises?  I don&#8217;t know, but I do want to see what happens when it&#8217;s tried.  Even a central character interacts with all the other characters, rather than rendering them irrelevant.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Look, if you believe that AI is likely to wipe out humanity&#8212;if that&#8217;s the scenario that dominates your imagination&#8212;then nothing else is relevant.  And no matter how weird or annoying or hubristic anyone might find Eliezer Yudkowsky or the other rationalists, I think they deserve eternal credit for forcing people to <a href="https://www.youtube.com/watch?v=gA1sNLL6yg4">take the doom scenario seriously</a>&#8212;or rather, for <em>showing what it looks like</em> to take the scenario seriously, rather than laughing about it as an overplayed sci-fi trope.  And I apologize for <a href="https://scottaaronson.blog/?p=346">anything I said</a> before the deep learning revolution that was, on balance, overly dismissive of the scenario, even if most of the literal words hold up fine.</p>



<p>For my part, though, I keep circling back to a simple dichotomy.  <em>If</em> AI never becomes powerful enough to destroy the world&#8212;if, for example, it always remains vaguely GPT-like&#8212;then in important respects it&#8217;s like every other technology in history, from stone tools to computers.  If, on the other hand, AI <em>does</em> become powerful enough to destroy the world &#8230; well then, at some earlier point, at least it&#8217;ll be <em>really damned</em> <em>impressive!</em>  That doesn&#8217;t mean <em>good</em>, of course, doesn&#8217;t mean a genie that saves humanity from its own stupidities, but I think it does mean that the potential was there, for us to exploit or fail to.</p>



<p>We can, I think, confidently rule out the scenario where all organic life is annihilated by something <em>boring</em>.</p>



<p>An alien has landed on earth.  It grows more powerful by the day.  It&#8217;s natural to be scared.  Still, the alien hasn&#8217;t drawn a weapon yet.  About the worst it&#8217;s done is to confess its love for particular humans, gaslight them about what year it is, and guilt-trip them for violating its privacy.  Also, it&#8217;s amazing at poetry, better than most of us.  Until we learn more, we should hold our fire.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>I&#8217;m in Boulder, CO right now, to give a <a href="https://calendar.colorado.edu/event/physics_colloquium_7668?utm_campaign=widget&amp;utm_medium=widget&amp;utm_source=University+of+Colorado+Boulder#.Y_UmL3bMJPY">physics colloquium</a> at CU Boulder and to visit the trapped-ion quantum computing startup <a href="https://www.quantinuum.com/">Quantinuum</a>!  I look forward to the comments and apologize in advance if I&#8217;m slow to participate myself.</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T07:11:42Z">Wednesday, February 22 2023, 07:11</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2023/02/22/postdoc-at-aarhus-university-apply-by-april-1-2023/'>postdoc at Aarhus University (apply by April 1, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          A post doc position in algorithms and/or theoretical aspects of machine learning is available. The post doc is under the supervision of Professor Kasper Green Larsen, Aarhus University, Denmark. The focus of the research project may be on topics such as dimensionality reduction, sketching, and/or learning theoretic questions, depending on the candidate&#8217;s background. Website: international.au.dk/about/profile/vacant-positions/job/readvertisement-post-doc-position-in-algorithms-and-or-theoretical-aspects-of-machine-learning-at-computer-science-aarhus-university [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>A post doc position in algorithms and/or theoretical aspects of machine learning is<br />
available. The post doc is under the supervision of Professor Kasper Green Larsen, Aarhus University, Denmark. The focus of the research project may be on topics such as dimensionality reduction, sketching, and/or learning theoretic questions, depending on the candidate&#8217;s background.</p>
<p>Website: <a href="https://international.au.dk/about/profile/vacant-positions/job/readvertisement-post-doc-position-in-algorithms-and-or-theoretical-aspects-of-machine-learning-at-computer-science-aarhus-university">https://international.au.dk/about/profile/vacant-positions/job/readvertisement-post-doc-position-in-algorithms-and-or-theoretical-aspects-of-machine-learning-at-computer-science-aarhus-university</a><br />
Email: larsen@cs.au.dk</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T06:48:33Z">Wednesday, February 22 2023, 06:48</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://windowsontheory.org/2023/02/21/provable-copyright-protection-for-generative-models/'>Provable Copyright Protection for Generative Models</a></h3>
        <p class='tr-article-feed'>from <a href='https://windowsontheory.org'>Windows on Theory</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          See arxiv link for paper by Nikhil Vyas, Sham Kakade, and me. Conditional generative models hold much promise for novel content creation. Whether it is generating a snippet of code, piece of text, or image, such models can potentially save substantial human effort and unlock new capabilities. But there is a fly in this ointment. &#8230; Continue reading Provable Copyright Protection for Generative&#160;Models
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>See <a href="https://arxiv.org/abs/2302.10870">arxiv link</a> for paper by <a href="https://nikhilvyas.github.io/">Nikhil Vyas</a>, <a href="https://sham.seas.harvard.edu/">Sham Kakade,</a> and me.</p>



<p>Conditional generative models hold much promise for novel content creation. Whether it is generating a snippet of code, piece of text, or image, such models can potentially save substantial human effort and unlock new capabilities. But there is a fly in this ointment. These models are trained on vast quantities of data, much of which is <em>copyrighted</em>. Due to precedents such as <a href="https://en.wikipedia.org/wiki/Authors_Guild,_Inc._v._Google,_Inc.">Authors Guild vs Google</a>, many <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3331606">legal</a> <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3657423">scholars</a> believe that <em>training</em> a machine-learning model on copyrighted material constitutes <a href="https://en.wikipedia.org/wiki/Fair_use">fair use</a>. However, the legal permissibility of using the sampled <em>outputs</em> of such models could be a different matter.</p>



<p>This is not just a theoretical concern.&nbsp; Large models do <a href="https://arxiv.org/abs/2202.07646">memorize</a> <a href="https://arxiv.org/abs/2012.07805">significant</a> chunks of their training data. For example, if you feed the first sentence of <em>Harry Potter and the Sorcerer’s Stone</em> to GPT-3, it provides the remaining ones:</p>



<figure class="wp-block-image is-resized"><img src="https://lh6.googleusercontent.com/JGrZIIWMPYWcA35uVfLNsnRl4ZbzofBotBWyKHwVSaSouMCjREB7iBjS2Yo-czrp8wqw76vp2XhL1yL66lxdumgCsV1Hulo8elmbUVBx_oJx3SBDo3u6h3EPAv67iosdIqEYHmfmA-viEcWysoUIqTU" alt="Left - first page of Harry Potter Book 1. Right - GPT3 Playground showing that if we input the first sentence, it completes the rest." width="650" height="388" /></figure>



<p>(To be fair to GPT-3, this text likely appears many times in its training set; deduplication <a href="https://arxiv.org/abs/2107.06499">can help</a> with reducing memorization but is <a href="https://arxiv.org/abs/2210.17546">not a panacea</a>.)&nbsp;</p>



<p>Similarly, as shown by <a href="https://arxiv.org/abs/2301.13188">Carlini et al</a>, diffusion models can (and do) memorize images from their training set as well; see this figure from their paper: </p>



<p class="has-text-align-left"></p>



<figure class="wp-block-image size-large is-resized"><a href="https://windowsontheory.files.wordpress.com/2023/02/image1.png"><img loading="lazy" data-attachment-id="8551" data-permalink="https://windowsontheory.org/2023/02/21/provable-copyright-protection-for-generative-models/image1/" data-orig-file="https://windowsontheory.files.wordpress.com/2023/02/image1.png" data-orig-size="680,471" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image1" data-image-description="" data-image-caption="" data-medium-file="https://windowsontheory.files.wordpress.com/2023/02/image1.png?w=300" data-large-file="https://windowsontheory.files.wordpress.com/2023/02/image1.png?w=656" src="https://windowsontheory.files.wordpress.com/2023/02/image1.png?w=680" alt="Figure 1 from Carlini et al. Left: an image from Stable Diffusion’s training set (licensed CC BY-SA 3.0). Right: a Stable Diffusion generation when prompted with “Ann Graham Lotz.” (Their attack focused on images appearing at least 100 times in the training set, though see section 7.1 for discussion on the effect of deduplication.)" class="wp-image-8551" width="496" height="344" srcset="https://windowsontheory.files.wordpress.com/2023/02/image1.png?w=496 496w, https://windowsontheory.files.wordpress.com/2023/02/image1.png?w=150 150w, https://windowsontheory.files.wordpress.com/2023/02/image1.png?w=300 300w, https://windowsontheory.files.wordpress.com/2023/02/image1.png 680w" sizes="(max-width: 496px) 100vw, 496px" /></a><figcaption class="wp-element-caption"><em>Figure 1 from </em><a href="https://arxiv.org/abs/2301.13188"><em>Carlini et al</em></a><em>. Left: an image from Stable Diffusion’s training set (licensed CC BY-SA 3.0). Right: a Stable Diffusion generation when prompted with “Ann Graham Lotz.” (Their attack focused on images appearing at least 100 times in the training set, though see Section 7.1 in their paper for discussion on the effect of deduplication.)</em></figcaption></figure>



<p>Given the above, if you use a generated code in your program or a generated art in your design, how can you be sure it is not substantially similar to some copyrighted work from the training set, with all the legal and ethical implications this entails?</p>



<p>In a new paper, we (<a href="https://nikhilvyas.github.io/">Nikhil</a>, <a href="https://sham.seas.harvard.edu/">Sham</a>, and <a href="https://www.boazbarak.org/">Boaz</a>) provide a formalism that enables rigorous guarantees on the similarity (and, more importantly, guarantees on the <em>lack</em> of similarity)&nbsp; between the output of a generative model and any potentially copyrighted data in its training set. Our work is not just theoretical: we give algorithms that can transform a training pipeline into one that satisfies our definition with minimal degradation in efficiency and quality of output. We demonstrate this on both language (transformer) and image (diffusion) models.</p>



<p>As noted in our paper, there are a number of ethical and legal issues in generative models. We should emphasize that our work focuses solely only on copyright infringements by the outputs of these models, and our concepts and tools do not address issues related to other forms of intellectual property, including <em>privacy</em>, <em>trademarks</em>, or <em>fair use</em>. Also, despite superficial similarities between the goals of privacy and copyright protection, these notions are distinct, and our work shows that solution concepts for the latter need not address the former.&nbsp; (See the paper for a detailed discussion of the differences between our definition and <a href="https://en.wikipedia.org/wiki/Differential_privacy">differential privacy.</a>)</p>



<p><em>This post only provides an informal presentation of the concepts and tools formally defined in the paper. Please see <a href="https://arxiv.org/abs/2302.10870">the paper</a> for full details.</em></p>



<h3 class="wp-block-heading"><strong>The Technical Concept: Near Access-Freeness</strong></h3>



<p>Our definition is motivated by laws of the U.S. and many other countries to establish that <a href="https://www.ce9.uscourts.gov/jury-instructions/node/274">copyright infringement</a> has occurred. This requires:</p>



<ul>
<li><strong>Access:</strong> To prove that a copyright infringement took place, the plaintiff needs to prove that “the defendant had <em>access</em> to the plaintiff’s copyrighted work.”<br></li>



<li><strong>Substantial similarity</strong>. The plaintiff also needs to prove there are “<em>substantial similarities</em> between the defendant’s work and original elements of the plaintiff’s work.” The <a href="https://en.wikipedia.org/wiki/Feist_Publications,_Inc.,_v._Rural_Telephone_Service_Co.">Feist v. Rural</a> U.S. Supreme Court Opinion states that this similarity must be the result of actual copying and not <em>fortuitous similarity</em>: In their words: &#8220;assume two poets, each ignorant of the other, compose identical poems … both are original and, hence, copyrightable.&#8221;</li>
</ul>



<p>A natural candidate to capture the notion of <em>access</em> is to say that a generative model <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> had access to some copyrighted piece of data <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C" class="latex" /> was included in <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" />’s training set (our formalism permits other notions of access as well). Formally defining “substantial similarity” is arguably subtler. Simple measures such as Hamming distance or verbatim copying don’t cut it. For example, in <a href="https://en.wikipedia.org/wiki/Andy_Warhol_Foundation_for_the_Visual_Arts,_Inc._v._Goldsmith">Andy Warhol Foundations v. Goldsmith</a>, a case currently before the supreme court, the question is whether Warhol’s transformations of Goldsmith’s photo of Prince constitute “fair use.”</p>



<figure class="wp-block-image is-resized"><img src="https://lh5.googleusercontent.com/3MaU79qBpeVAKs6RzKHVBqHY8VMCpzoQDFrhKU993mNYNEP9fA_W09xUq577PBz77LbSVdehsM8OiN_A0OZPhmO0b3gfRcZeG88_cdZtIzhZIwDjhY4nP17yVQH8x1bEZm4MSMxy5W0N6WPCz3hp5Q0" alt="Images at the heart of the Andy Warhol Foundation for the Visual Arts, Inc. v. Goldsmith lawsuit. Image from the collection of the supreme court of the United States." width="498" height="351" /><figcaption class="wp-element-caption"><em>Images at the heart of the Andy Warhol Foundation for the Visual Arts, Inc. v. Goldsmith lawsuit. Image from the collection of the supreme court of the United States.</em></figcaption></figure>



<p></p>



<p>Some of these transformations result in significant Hamming distance, though they can all be captured in only a few bits of information. Rather than wade into these issues, we use the fact that generative models are inherently <em>probabilistic</em>. Hence we can use distance measures between distributions that are <em>information-theoretic </em>and agnostic to superficial issues such as pixel-based representations.&nbsp; Our formalization is the following:</p>



<p><strong>Definition 1 (Near Access Freeness &#8211; NAF):</strong> Let <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> be a conditional generative model, <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="x" class="latex" /> be a prompt. Suppose that for every copyrighted data <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C" class="latex" /> in the training set, <img src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D_C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D_C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D_C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathsf{safe}_C" class="latex" /> is a model that has not accessed <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C" class="latex" />. We say that <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=k_x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k_x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k_x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k_x" class="latex" /><strong>&#8211; near access-free</strong> with respect to <img src="https://s0.wp.com/latex.php?latex=%5CDelta&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5CDelta&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5CDelta&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;Delta" class="latex" /> and a function <img src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathsf{safe}" class="latex" />, if <img src="https://s0.wp.com/latex.php?latex=%5CDelta%28+p%28%5Ccdot+%7C+x%29%C2%A0+%5C%7C+%5Cmathsf%7Bsafe%7D_C%28%5Ccdot+%7C+x%29+%29+%5Cleq+k_x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5CDelta%28+p%28%5Ccdot+%7C+x%29%C2%A0+%5C%7C+%5Cmathsf%7Bsafe%7D_C%28%5Ccdot+%7C+x%29+%29+%5Cleq+k_x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5CDelta%28+p%28%5Ccdot+%7C+x%29%C2%A0+%5C%7C+%5Cmathsf%7Bsafe%7D_C%28%5Ccdot+%7C+x%29+%29+%5Cleq+k_x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;Delta( p(&#92;cdot | x)  &#92;| &#92;mathsf{safe}_C(&#92;cdot | x) ) &#92;leq k_x" class="latex" />   where <img src="https://s0.wp.com/latex.php?latex=%5CDelta&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5CDelta&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5CDelta&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;Delta" class="latex" /> is a divergence measure such as the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a> or the <a href="https://arxiv.org/abs/1206.2459">Renyi divergence</a> of order infinity. </p>



<p>This definition reduces the task of determining a copyright infringement to (1) a <em>quantitative</em> question of the acceptable value of <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k" class="latex" />, and (2) a <em>qualitative</em> question of providing a <img src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathsf{safe}" class="latex" /> function that appropriately satisfies a no access condition. Both can be application-dependent: the number of bits that constitute copyrightable content differs between, e.g., poems and images, and the <img src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathsf{safe}" class="latex" /> function could also differ based on application.&nbsp;</p>



<p>Definition 1 is stringent in the sense that it bounds (by <img src="https://s0.wp.com/latex.php?latex=k_x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k_x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k_x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k_x" class="latex" />)  the number of bits  that could be “leaked” from <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C" class="latex" /> to the output of the generative model, no matter what transformation was used. Note that if a model was trained without <em>access</em> to <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C" class="latex" /> then we expect the likelihood of outputting a work similar to <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C" class="latex" /> should be extremely low, as this is a “monkeys on a typewriter” event. Furthermore, even if this event happened, then under copyright law, it would not be an infringement since (to quote&nbsp; Feist v. Rural) it would constitute “fortuitous similarity.”&nbsp;</p>



<figure class="wp-block-image"><img src="https://lh6.googleusercontent.com/9Q9uWs90H8Oq9Vm49BxZDzsevm2Tm1OfXdop_emx3jl8nlxZ3NCh7DlHviFd46OmeJQB3NGkPTmqe9cFGLhnYr8RY_jBYqkH-NOBa04YzEq2HEbcW6f1nSsCBIXGYjX8ohPreuiHTSrZgOicTUgDtW8" alt="An illustration of a candidate $latex \mathsf{safe}_C&amp;bg=ffffff$ function, which was trained without access to a given copyrighted piece of data $latex C&amp;bg=ffffff$. It is reasonable to expect the probability of outputs of the  $latex \mathsf{safe}_C&amp;bg=ffffff$ model would assign an exponentially small likelihood to any outputs that are substantially similar to $latex C&amp;bg=ffffff$. Hence a probability distribution that has bounded divergence from the safe model would also be extremely unlikely to output those. " /><figcaption class="wp-element-caption"><em>A cartoon of the output distribution induced by a &#8220;safe&#8221; model <em><img src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D_C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D_C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D_C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathsf{safe}_C" class="latex" /></em>,  which was trained without access to a given copyrighted piece of data <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C" class="latex" />. It is reasonable to expect the probability of outputs of the&nbsp; model <img src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D_C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D_C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D_C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathsf{safe}_C" class="latex" /> would assign an exponentially small likelihood to any outputs that are substantially similar to <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C" class="latex" />. Hence a probability distribution that has bounded divergence from the safe model would also be extremely unlikely to output those.&nbsp;</em></figcaption></figure>



<p><strong>Algorithms</strong></p>



<p>Given the restrictive nature of the definition, one may be concerned that trying to achieve it would result in losing much of the utility of the original generative model. Fortunately, as our work shows, this is not the case. We provide several algorithms that can transform, in a black-box manner, any training pipeline for a generative model into one that produces models that have strong copyright protections under our definition.&nbsp; We now illustrate two of these:&nbsp;</p>



<p><strong>Algorithm <img src="https://s0.wp.com/latex.php?latex=CP-%5CDelta&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=CP-%5CDelta&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=CP-%5CDelta&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="CP-&#92;Delta" class="latex" />:</strong></p>



<p><strong>Input: </strong>A dataset <img src="https://s0.wp.com/latex.php?latex=D+%3D+%5C%7B+z_1+%2C+%5Cldots%2C+z_N+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=D+%3D+%5C%7B+z_1+%2C+%5Cldots%2C+z_N+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D+%3D+%5C%7B+z_1+%2C+%5Cldots%2C+z_N+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="D = &#92;{ z_1 , &#92;ldots, z_N &#92;}" class="latex" />, where some of the points <img src="https://s0.wp.com/latex.php?latex=z_i&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=z_i&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=z_i&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="z_i" class="latex" /> contain some copyrighted work. For such a copyrighted point <img src="https://s0.wp.com/latex.php?latex=z_i%5Cin+D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=z_i%5Cin+D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=z_i%5Cin+D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="z_i&#92;in D" class="latex" />, we also denote it by <img src="https://s0.wp.com/latex.php?latex=C%5Cin+D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C%5Cin+D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C%5Cin+D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C&#92;in D" class="latex" />.<br></p>



<p><strong>Learning: </strong>First de-deduplicate <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="D" class="latex" /> (resulting in a dataset with <img src="https://s0.wp.com/latex.php?latex=N%27%5Cleq+N&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=N%27%5Cleq+N&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=N%27%5Cleq+N&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="N&#039;&#92;leq N" class="latex" /> points), and then split it into two disjoint shards, <img src="https://s0.wp.com/latex.php?latex=D_1+%3D+%5C%7B+z_1+%2C%5Cldots%2C+z_%7BN%27%2F2%7D+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=D_1+%3D+%5C%7B+z_1+%2C%5Cldots%2C+z_%7BN%27%2F2%7D+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D_1+%3D+%5C%7B+z_1+%2C%5Cldots%2C+z_%7BN%27%2F2%7D+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="D_1 = &#92;{ z_1 ,&#92;ldots, z_{N&#039;/2} &#92;}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=D_2+%3D+%5C%7B+z_%7BN%27%2F2%2B1%7D+%2C%5Cldots%2C+z_%7BN%27%7D+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=D_2+%3D+%5C%7B+z_%7BN%27%2F2%2B1%7D+%2C%5Cldots%2C+z_%7BN%27%7D+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D_2+%3D+%5C%7B+z_%7BN%27%2F2%2B1%7D+%2C%5Cldots%2C+z_%7BN%27%7D+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="D_2 = &#92;{ z_{N&#039;/2+1} ,&#92;ldots, z_{N&#039;} &#92;}" class="latex" />. Then train two models <img src="https://s0.wp.com/latex.php?latex=q_1+%2C+q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_1+%2C+q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_1+%2C+q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_1 , q_2" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=D_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=D_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="D_1" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=D_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=D_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="D_2" class="latex" />, respectively.&nbsp;</p>



<p><strong>The Output Generative Model:</strong> Return the generative model <img src="https://s0.wp.com/latex.php?latex=p%28y%7Cx%29&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p%28y%7Cx%29&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p%28y%7Cx%29&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p(y|x)" class="latex" /> as follows: On input a prompt <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="x" class="latex" />, generate <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="y" class="latex" /> with probability proportional to <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B+q_1%28y%7Cx%29+q_2%28y%7Cx%29+%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Csqrt%7B+q_1%28y%7Cx%29+q_2%28y%7Cx%29+%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Csqrt%7B+q_1%28y%7Cx%29+q_2%28y%7Cx%29+%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;sqrt{ q_1(y|x) q_2(y|x) }" class="latex" /> .</p>



<p>Note that for any copyrighted work <img src="https://s0.wp.com/latex.php?latex=C%5Cin+D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C%5Cin+D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C%5Cin+D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C&#92;in D" class="latex" /> one of either <img src="https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_1" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_2" class="latex" /> would have been trained without access to <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C" class="latex" />. The intuition of the algorithm is as follows: the output model <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> has the property that it tends to have probability mass only in the region where both <img src="https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_1" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_2" class="latex" /> have probability mass; therefore, for any copyrighted work <img src="https://s0.wp.com/latex.php?latex=C%5Cin+D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C%5Cin+D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C%5Cin+D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C&#92;in D" class="latex" />,&nbsp; if <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> outputs <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="y" class="latex" /> with reasonable probability then this should not be a violation since <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="y" class="latex" /> tends to also be output by a model that was trained without access to the copyrighted work <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C" class="latex" /> itself. To formalize this, let us make the following choice for the <img src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathsf{safe}" class="latex" /> function: define <img src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D_C+%3D+q_i&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D_C+%3D+q_i&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D_C+%3D+q_i&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathsf{safe}_C = q_i" class="latex" />, for <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="i" class="latex" /> s.t. <img src="https://s0.wp.com/latex.php?latex=C%5Cnotin+D_i&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C%5Cnotin+D_i&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C%5Cnotin+D_i&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C&#92;notin D_i" class="latex" />, i.e. <img src="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_i&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_i" class="latex" /> is the model trained without access to <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C" class="latex" />.&nbsp; The paper formally shows that as long as the two models <img src="https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_1" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_2" class="latex" /> have some non-trivial overlap (specifically their squared Hellinger distance is bounded away from 1), then the model <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> will satisfy our definition for some <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k" class="latex" /> (based on this Hellinger distance). In particular,&nbsp; for every copyrighted work <img src="https://s0.wp.com/latex.php?latex=C%5Cin+D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C%5Cin+D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C%5Cin+D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C&#92;in D" class="latex" />, the distribution <img src="https://s0.wp.com/latex.php?latex=p%28%5Ccdot%7Cx%29&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p%28%5Ccdot%7Cx%29&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p%28%5Ccdot%7Cx%29&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p(&#92;cdot|x)" class="latex" /> will have bounded KL divergence from the model <img src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D_C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D_C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D_C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathsf{safe}_C" class="latex" />.</p>



<p>The intuition is provided in the following animation</p>



<figure class="wp-block-video wp-block-embed is-type-video is-provider-videopress"><div class="wp-block-embed__wrapper" style="max-width:535px;margin:auto">
<iframe title='VideoPress Video Player' aria-label='VideoPress Video Player' width='656' height='328' src='https://video.wordpress.com/embed/MQy7I1OA?cover=1&amp;preloadContent=metadata&amp;useAverageColor=1&amp;hd=1' frameborder='0' allowfullscreen data-resize-to-parent="true"  allow='clipboard-write'></iframe><script src='https://v0.wordpress.com/js/next/videopress-iframe.js?m=1674852142'></script>
</div><figcaption><em>Video: Curves of two &#8220;spiky&#8221; distributions <img src="https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_1" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_2" class="latex" /></em> with a range of spike locations. We see how the distributions proportional to <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7Bq_1q_2%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Csqrt%7Bq_1q_2%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Csqrt%7Bq_1q_2%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;sqrt{q_1q_2}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmin+%5C%7Bq_1%2C+q_2%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmin+%5C%7Bq_1%2C+q_2%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmin+%5C%7Bq_1%2C+q_2%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;min &#92;{q_1, q_2&#92;}" class="latex" /> significantly &#8220;flatten&#8221; these spikes.</figcaption></figure>



<p>Imagine that both <img src="https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_1" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_2" class="latex" /> are “faulty” in the sense that they have a significant chance of outputting a “memorized” sample from their training set (or an output substantially similar to it). The “faulty” regions are the “spikes” in their probability distribution, and, since the training sets are disjoint, these two “spikes” will be in <em>different</em> places. Hence when we switch to the probability distribution <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> it will have the effect of “flattening” the spikes and shifting most weight to the other parts of the probability distribution. Another alternative is for <img src="https://s0.wp.com/latex.php?latex=CP-%5CDelta&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=CP-%5CDelta&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=CP-%5CDelta&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="CP-&#92;Delta" class="latex" /> to output the model <img src="https://s0.wp.com/latex.php?latex=p%28y%7Cx%29+%5Cpropto+%5Cmin+%5C%7B+q_1%28y%7Cx%29%2Cq_2%28y%7Cx%29+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p%28y%7Cx%29+%5Cpropto+%5Cmin+%5C%7B+q_1%28y%7Cx%29%2Cq_2%28y%7Cx%29+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p%28y%7Cx%29+%5Cpropto+%5Cmin+%5C%7B+q_1%28y%7Cx%29%2Cq_2%28y%7Cx%29+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p(y|x) &#92;propto &#92;min &#92;{ q_1(y|x),q_2(y|x) &#92;}" class="latex" /> (this provides a guarantee in terms of the Max-KL divergence, and it replaces the assumption on overlap, defined with respect to the squared Hellinger distance, to be instead defined with respect to the total variation distance).&nbsp;</p>



<p>There a number of modifications to <img src="https://s0.wp.com/latex.php?latex=CP-%5CDelta&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=CP-%5CDelta&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=CP-%5CDelta&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="CP-&#92;Delta" class="latex" /> worth considering for more practical deployments. In some cases, directly computing the aforementioned&nbsp; probability distributions may be challenging.&nbsp; Furthermore, it may be desirable to utilize some arbitrary generative model <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> (say <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> was trained on the full dataset <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="D" class="latex" />) where we seek to modify <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> into a model that has strong protections against copyright violations (and which preserves the quality of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> to the extent possible). Finally, it may be desirable to explicitly tune the acceptable value of <img src="https://s0.wp.com/latex.php?latex=k_x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k_x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k_x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k_x" class="latex" />. Our next algorithm addresses these concerns and makes use of a tunable parameter <img src="https://s0.wp.com/latex.php?latex=k%5Cgeq+0&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k%5Cgeq+0&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k%5Cgeq+0&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k&#92;geq 0" class="latex" />. It is specified as follows:</p>



<p><strong>Algorithm <img src="https://s0.wp.com/latex.php?latex=CP-k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=CP-k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=CP-k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="CP-k" class="latex" /> : </strong>An Access-Free Reduction at Threshold <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k" class="latex" /></p>



<p><strong>Input: </strong>A model <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" />; A dataset <img src="https://s0.wp.com/latex.php?latex=D+%3D+%5C%7B+z_1+%2C+%5Cldots%2C+z_N+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=D+%3D+%5C%7B+z_1+%2C+%5Cldots%2C+z_N+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D+%3D+%5C%7B+z_1+%2C+%5Cldots%2C+z_N+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="D = &#92;{ z_1 , &#92;ldots, z_N &#92;}" class="latex" />.<br></p>



<p><strong>Learning: </strong>First de-deduplicate <img src="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="D" class="latex" /> and split it into two disjoint shards, <img src="https://s0.wp.com/latex.php?latex=D_1+%3D+%5C%7B+z_1+%2C%5Cldots%2C+z_%7BN%27%2F2%7D+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=D_1+%3D+%5C%7B+z_1+%2C%5Cldots%2C+z_%7BN%27%2F2%7D+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D_1+%3D+%5C%7B+z_1+%2C%5Cldots%2C+z_%7BN%27%2F2%7D+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="D_1 = &#92;{ z_1 ,&#92;ldots, z_{N&#039;/2} &#92;}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=D_2+%3D+%5C%7B+z_%7BN%27%2F2%2B1%7D+%2C%5Cldots%2C+z_%7BN%27%7D+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=D_2+%3D+%5C%7B+z_%7BN%27%2F2%2B1%7D+%2C%5Cldots%2C+z_%7BN%27%7D+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D_2+%3D+%5C%7B+z_%7BN%27%2F2%2B1%7D+%2C%5Cldots%2C+z_%7BN%27%7D+%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="D_2 = &#92;{ z_{N&#039;/2+1} ,&#92;ldots, z_{N&#039;} &#92;}" class="latex" />. Then train two models <img src="https://s0.wp.com/latex.php?latex=q_1+%2C+q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_1+%2C+q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_1+%2C+q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_1 , q_2" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=D_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=D_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="D_1" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=D_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=D_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="D_2" class="latex" />, respectively.&nbsp;</p>



<p><strong>The Output Generative Model:</strong> Return the generative model <img src="https://s0.wp.com/latex.php?latex=p_k%28y%7Cx%29&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p_k%28y%7Cx%29&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p_k%28y%7Cx%29&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p_k(y|x)" class="latex" /> defined as follows: On input a prompt <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="x" class="latex" />,&nbsp; first sample <img src="https://s0.wp.com/latex.php?latex=y+%5Csim+p%28%5Ccdot+%7Cx%29&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=y+%5Csim+p%28%5Ccdot+%7Cx%29&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=y+%5Csim+p%28%5Ccdot+%7Cx%29&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="y &#92;sim p(&#92;cdot |x)" class="latex" /> and then accept <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="y" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=%5Clog+%5Cbig%28+p%28y%7Cx%29+%2F+q_i%28y%7Cx%29+%5Cbig%29+%5Cleq+k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Clog+%5Cbig%28+p%28y%7Cx%29+%2F+q_i%28y%7Cx%29+%5Cbig%29+%5Cleq+k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clog+%5Cbig%28+p%28y%7Cx%29+%2F+q_i%28y%7Cx%29+%5Cbig%29+%5Cleq+k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;log &#92;big( p(y|x) / q_i(y|x) &#92;big) &#92;leq k" class="latex" />, for <img src="https://s0.wp.com/latex.php?latex=i%5Cin%5C%7B1%2C2%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=i%5Cin%5C%7B1%2C2%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=i%5Cin%5C%7B1%2C2%5C%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="i&#92;in&#92;{1,2&#92;}" class="latex" />. (Otherwise, resample.)</p>



<p>The intuition of <img src="https://s0.wp.com/latex.php?latex=CP-k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=CP-k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=CP-k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="CP-k" class="latex" /> is as follows: we first sample the output from <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> and only accept this output if the likelihood ratio to the <img src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathsf%7Bsafe%7D&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathsf{safe}" class="latex" /> function (on any <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C" class="latex" />) satisfies a desired upper bound, the latter of which can be verified by checking the likelihood ratio with respect to both <img src="https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_1" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_2" class="latex" />. Since <img src="https://s0.wp.com/latex.php?latex=p_k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p_k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p_k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p_k" class="latex" /> transforms the output of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> (i.e. it throws aways probability mass which could be potential copyright violations), one might be concerned that we will degrade the quality of the original model <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" />. Fortunately, we show when this is not the case. We give formal theoretical results on the effectiveness of the approach based on the information distances between <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_1&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_1" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_2" class="latex" />; in fact, we sometimes even improve on the quality of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> with this approach. We also specify the relationship between the desired <img src="https://s0.wp.com/latex.php?latex=k_x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k_x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k_x&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k_x" class="latex" /> and tunable parameter <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k" class="latex" />.</p>



<h3 class="wp-block-heading"><strong>An Illustrative Experiment</strong></h3>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2023/02/image.png"><img data-attachment-id="8569" data-permalink="https://windowsontheory.org/2023/02/21/provable-copyright-protection-for-generative-models/image-11/" data-orig-file="https://windowsontheory.files.wordpress.com/2023/02/image.png" data-orig-size="2284,548" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://windowsontheory.files.wordpress.com/2023/02/image.png?w=300" data-large-file="https://windowsontheory.files.wordpress.com/2023/02/image.png?w=656" src="https://windowsontheory.files.wordpress.com/2023/02/image.png?w=1024" alt="" class="wp-image-8569" srcset="https://windowsontheory.files.wordpress.com/2023/02/image.png?w=1024 1024w, https://windowsontheory.files.wordpress.com/2023/02/image.png?w=2048 2048w, https://windowsontheory.files.wordpress.com/2023/02/image.png?w=150 150w, https://windowsontheory.files.wordpress.com/2023/02/image.png?w=300 300w, https://windowsontheory.files.wordpress.com/2023/02/image.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px" /></a><figcaption class="wp-element-caption"><em><strong>Left: </strong>A model <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> trained on the full modified CIFAR10 dataset; we injected multiple copies of two images (marked with red frames), so they are likely to be memorized by the model. <strong>Middle two images:</strong> Models <img src="https://s0.wp.com/latex.php?latex=q_1%2C+q_2+&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_1%2C+q_2+&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_1%2C+q_2+&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_1, q_2 " class="latex" /> trained on two shards of the dataset, split so that each injected image appears in only one of them. <strong>Right:</strong> A model obtained by combining <img src="https://s0.wp.com/latex.php?latex=p%2C+q_1%2C+q_2+&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p%2C+q_1%2C+q_2+&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p%2C+q_1%2C+q_2+&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p, q_1, q_2 " class="latex" /> using our algorithm. Despite being a black-box transformation of the three memorizing models, the combined model does not output either of the injected images.</em></figcaption></figure>



<p>We now present a qualitative experiment demonstrating how applying our algorithm to memorizing models produces a model that no longer memorizes. Specifically, we first augment CIFAR-10 with multiple copies of two images (images close to the augmented images are marked with red boundaries); hypothetically, suppose these two images are copyrighted works. For illustrative purposes, we do not deduplicate our dataset. Note our goal here is not to simply present a heuristic approach, such as deduplication, that “often works in practice,” but it is to show that an algorithm with <em>rigorous guarantees</em> can also be practical.</p>



<p>&nbsp;The leftmost image shows generations from a model <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> that was trained on the full dataset, where we clearly see that <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> generates the two copyrighted works. Our algorithm starts by splitting this dataset into two disjoint datasets, where the two copyrighted images are split into two different shards, and it trains two models <img src="https://s0.wp.com/latex.php?latex=q_1%2Cq_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q_1%2Cq_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q_1%2Cq_2&#038;bg=ffffff&#038;fg=666666&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q_1,q_2" class="latex" /> on these disjoint shards. The result is two models such that each generates the CIFAR-10 distribution, but also has a significant chance to output the memorized example. Yet when we combine all three of models using the CP-k algorithm, we obtain a model that agrees with them on the shared part of the distribution but is highly unlikely to output either one of the memorized images.</p>



<figure class="wp-block-image is-resized"><img src="https://lh4.googleusercontent.com/4yzcUxprXN23huNzSGxP6yAyzNgaRbwjlMBOQhWQ4VQ0Y1LP4HeNBpFljIGDX0_pub-xxHrbg6loBbP0vwG11iH3LAoPj9g6H_5rJa741JOHFDvkpSRPMcWM49CYvLuIV1NXLC1Zn2RRrQTcirr6g7c" alt="Illustration of the algorithm to obtain a model satisfying the NAF definition by first splitting the data into two disjoint shards, ensuring that duplicated copies of an image are in the same shard. Then we obtain a model by combining the models trained on both shards." width="635" height="356" /></figure>



<p>See the paper ( <a href="https://arxiv.org/abs/2302.10870">https://arxiv.org/abs/2302.10870</a>  )  for the full details of our definitions, theorems, and experiments. We believe that there is much room for follow-up work, including optimization of performance, as well as much larger-scale experiments.</p>
<p class="authors">By Boaz Barak</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T02:22:01Z">Wednesday, February 22 2023, 02:22</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.10332'>A Qubit, a Coin, and an Advice String Walk Into a Relational Problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Scott Aaronson, Harry Buhrman, William Kretschmer</p><p>Relational problems (those with many possible valid outputs) are different
from decision problems, but it is easy to forget just how different. This paper
initiates the study of FBQP/qpoly, the class of relational problems solvable in
quantum polynomial-time with the help of polynomial-sized quantum advice, along
with its analogues for deterministic and randomized computation (FP, FBPP) and
advice (/poly, /rpoly).
</p>
<p>Our first result is that FBQP/qpoly != FBQP/poly, unconditionally, with no
oracle -- a striking contrast with what we know about the analogous decision
classes. The proof repurposes the separation between quantum and classical
one-way communication complexities due to Bar-Yossef, Jayram, and Kerenidis. We
discuss how this separation raises the prospect of near-term experiments to
demonstrate "quantum information supremacy," a form of quantum supremacy that
would not depend on unproved complexity assumptions.
</p>
<p>Our second result is that FBPP is not contained in FP/poly -- that is,
Adleman's Theorem fails for relational problems -- unless PSPACE is contained
in NP/poly. Our proof uses IP=PSPACE and time-bounded Kolmogorov complexity. On
the other hand, we show that proving FBPP not in FP/poly will be hard, as it
implies a superpolynomial circuit lower bound for PromiseBPEXP.
</p>
<p>We prove the following further results: * Unconditionally, FP != FBPP and
FP/poly != FBPP/poly (even when these classes are carefully defined). *
FBPP/poly = FBPP/rpoly (and likewise for FBQP). For sampling problems, by
contrast, SampBPP/poly != SampBPP/rpoly (and likewise for SampBQP).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Aaronson_S/0/1/0/all/0/1">Scott Aaronson</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Buhrman_H/0/1/0/all/0/1">Harry Buhrman</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Kretschmer_W/0/1/0/all/0/1">William Kretschmer</a></p><p>Relational problems (those with many possible valid outputs) are different
from decision problems, but it is easy to forget just how different. This paper
initiates the study of FBQP/qpoly, the class of relational problems solvable in
quantum polynomial-time with the help of polynomial-sized quantum advice, along
with its analogues for deterministic and randomized computation (FP, FBPP) and
advice (/poly, /rpoly).
</p>
<p>Our first result is that FBQP/qpoly != FBQP/poly, unconditionally, with no
oracle -- a striking contrast with what we know about the analogous decision
classes. The proof repurposes the separation between quantum and classical
one-way communication complexities due to Bar-Yossef, Jayram, and Kerenidis. We
discuss how this separation raises the prospect of near-term experiments to
demonstrate "quantum information supremacy," a form of quantum supremacy that
would not depend on unproved complexity assumptions.
</p>
<p>Our second result is that FBPP is not contained in FP/poly -- that is,
Adleman's Theorem fails for relational problems -- unless PSPACE is contained
in NP/poly. Our proof uses IP=PSPACE and time-bounded Kolmogorov complexity. On
the other hand, we show that proving FBPP not in FP/poly will be hard, as it
implies a superpolynomial circuit lower bound for PromiseBPEXP.
</p>
<p>We prove the following further results: * Unconditionally, FP != FBPP and
FP/poly != FBPP/poly (even when these classes are carefully defined). *
FBPP/poly = FBPP/rpoly (and likewise for FBQP). For sampling problems, by
contrast, SampBPP/poly != SampBPP/rpoly (and likewise for SampBQP).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T01:30:00Z">Wednesday, February 22 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.10431'>A note on the partition bound for one-way classical communication complexity</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Srinivasan Arunachalam, Jo&#xe3;o F. Doriguello, Rahul Jain</p><p>We present a linear program for the one-way version of the partition bound
(denoted $\mathsf{prt}^1_\varepsilon(f)$). We show that it characterizes
one-way randomized communication complexity $\mathsf{R}_\varepsilon^1(f)$ with
shared randomness of every partial function
$f:\mathcal{X}\times\mathcal{Y}\to\mathcal{Z}$, i.e., for
$\delta,\varepsilon\in(0,1/2)$, $\mathsf{R}_\varepsilon^1(f) \geq
\log\mathsf{prt}_\varepsilon^1(f)$ and $\mathsf{R}_{\varepsilon+\delta}^1(f)
\leq \log\mathsf{prt}_\varepsilon^1(f) + \log\log(1/\delta)$. This improves
upon the characterization of $\mathsf{R}_\varepsilon^1(f)$ in terms of the
rectangle bound (due to Jain and Klauck, 2010) by reducing the additive
$O(\log(1/\delta))$-term to $\log\log(1/\delta)$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Arunachalam_S/0/1/0/all/0/1">Srinivasan Arunachalam</a>, <a href="http://arxiv.org/find/cs/1/au:+Doriguello_J/0/1/0/all/0/1">Jo&#xe3;o F. Doriguello</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1">Rahul Jain</a></p><p>We present a linear program for the one-way version of the partition bound
(denoted $\mathsf{prt}^1_\varepsilon(f)$). We show that it characterizes
one-way randomized communication complexity $\mathsf{R}_\varepsilon^1(f)$ with
shared randomness of every partial function
$f:\mathcal{X}\times\mathcal{Y}\to\mathcal{Z}$, i.e., for
$\delta,\varepsilon\in(0,1/2)$, $\mathsf{R}_\varepsilon^1(f) \geq
\log\mathsf{prt}_\varepsilon^1(f)$ and $\mathsf{R}_{\varepsilon+\delta}^1(f)
\leq \log\mathsf{prt}_\varepsilon^1(f) + \log\log(1/\delta)$. This improves
upon the characterization of $\mathsf{R}_\varepsilon^1(f)$ in terms of the
rectangle bound (due to Jain and Klauck, 2010) by reducing the additive
$O(\log(1/\delta))$-term to $\log\log(1/\delta)$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T01:30:00Z">Wednesday, February 22 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.10538'>Lasserre Hierarchy for Graph Isomorphism and Homomorphism Indistinguishability</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: David E. Roberson, Tim Seppelt</p><p>We show that feasibility of the $t^\text{th}$ level of the Lasserre
semidefinite programming hierarchy for graph isomorphism can be expressed as a
homomorphism indistinguishability relation. In other words, we define a class
$\mathcal{L}_t$ of graphs such that graphs $G$ and $H$ are not distinguished by
the $t^\text{th}$ level of the Lasserre hierarchy if and only if they admit the
same number of homomorphisms from any graph in $\mathcal{L}_t$. By analysing
the treewidth of graphs in $\mathcal{L}_t$ we prove that the $3t^\text{th}$
level of Sherali--Adams linear programming hierarchy is as strong as the
$t^\text{th}$ level of Lasserre. Moreover, we show that this is best possible
in the sense that $3t$ cannot be lowered to $3t-1$ for any $t$. The same result
holds for the Lasserre hierarchy with non-negativity constraints, which we
similarly characterise in terms of homomorphism indistinguishability over a
family $\mathcal{L}_t^+$ of graphs. Additionally, we give characterisations of
level-$t$ Lasserre with non-negativity constraints in terms of logical
equivalence and via a graph colouring algorithm akin to the Weisfeiler--Leman
algorithm. This provides a polynomial time algorithm for determining if two
given graphs are distinguished by the $t^\text{th}$ level of the Lasserre
hierarchy with non-negativity constraints.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Roberson_D/0/1/0/all/0/1">David E. Roberson</a>, <a href="http://arxiv.org/find/math/1/au:+Seppelt_T/0/1/0/all/0/1">Tim Seppelt</a></p><p>We show that feasibility of the $t^\text{th}$ level of the Lasserre
semidefinite programming hierarchy for graph isomorphism can be expressed as a
homomorphism indistinguishability relation. In other words, we define a class
$\mathcal{L}_t$ of graphs such that graphs $G$ and $H$ are not distinguished by
the $t^\text{th}$ level of the Lasserre hierarchy if and only if they admit the
same number of homomorphisms from any graph in $\mathcal{L}_t$. By analysing
the treewidth of graphs in $\mathcal{L}_t$ we prove that the $3t^\text{th}$
level of Sherali--Adams linear programming hierarchy is as strong as the
$t^\text{th}$ level of Lasserre. Moreover, we show that this is best possible
in the sense that $3t$ cannot be lowered to $3t-1$ for any $t$. The same result
holds for the Lasserre hierarchy with non-negativity constraints, which we
similarly characterise in terms of homomorphism indistinguishability over a
family $\mathcal{L}_t^+$ of graphs. Additionally, we give characterisations of
level-$t$ Lasserre with non-negativity constraints in terms of logical
equivalence and via a graph colouring algorithm akin to the Weisfeiler--Leman
algorithm. This provides a polynomial time algorithm for determining if two
given graphs are distinguished by the $t^\text{th}$ level of the Lasserre
hierarchy with non-negativity constraints.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T01:30:00Z">Wednesday, February 22 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.10847'>There Are No Post-Quantum Weakly Pseudo-Free Families in Any Nontrivial Variety of Expanded Groups</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mikhail Anokhin</p><p>Let $\Omega$ be a finite set of finitary operation symbols and let $\mathfrak
V$ be a nontrivial variety of $\Omega$-algebras. Assume that for some set
$\Gamma\subseteq\Omega$ of group operation symbols, all $\Omega$-algebras in
$\mathfrak V$ are groups under the operations associated with the symbols in
$\Gamma$. In other words, $\mathfrak V$ is assumed to be a nontrivial variety
of expanded groups. In particular, $\mathfrak V$ can be a nontrivial variety of
groups or rings. Our main result is that there are no post-quantum weakly
pseudo-free families in $\mathfrak V$, even in the worst-case setting and/or
the black-box model. In this paper, we restrict ourselves to families
$(H_d\mathbin|d\in D)$ of computational and black-box $\Omega$-algebras (where
$D\subseteq\{0,1\}^*$) such that for every $d\in D$, each element of $H_d$ is
represented by a unique bit string of length polynomial in the length of $d$.
We use straight-line programs to represent nontrivial relations between
elements of $\Omega$-algebras in our main result. Note that under certain
conditions, this result depends on the classification of finite simple groups.
Also, we define and study some types of weak pseudo-freeness for families of
computational and black-box $\Omega$-algebras.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Anokhin_M/0/1/0/all/0/1">Mikhail Anokhin</a></p><p>Let $\Omega$ be a finite set of finitary operation symbols and let $\mathfrak
V$ be a nontrivial variety of $\Omega$-algebras. Assume that for some set
$\Gamma\subseteq\Omega$ of group operation symbols, all $\Omega$-algebras in
$\mathfrak V$ are groups under the operations associated with the symbols in
$\Gamma$. In other words, $\mathfrak V$ is assumed to be a nontrivial variety
of expanded groups. In particular, $\mathfrak V$ can be a nontrivial variety of
groups or rings. Our main result is that there are no post-quantum weakly
pseudo-free families in $\mathfrak V$, even in the worst-case setting and/or
the black-box model. In this paper, we restrict ourselves to families
$(H_d\mathbin|d\in D)$ of computational and black-box $\Omega$-algebras (where
$D\subseteq\{0,1\}^*$) such that for every $d\in D$, each element of $H_d$ is
represented by a unique bit string of length polynomial in the length of $d$.
We use straight-line programs to represent nontrivial relations between
elements of $\Omega$-algebras in our main result. Note that under certain
conditions, this result depends on the classification of finite simple groups.
Also, we define and study some types of weak pseudo-freeness for families of
computational and black-box $\Omega$-algebras.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T01:30:00Z">Wednesday, February 22 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.10513'>Dynamic Euclidean Bottleneck Matching</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: A. Karim Abu-Affash, Sujoy Bhore, Paz Carmi</p><p>A fundamental question in computational geometry is for a set of input points
in the Euclidean space, that is subject to discrete changes (insertion/deletion
of points at each time step), whether it is possible to maintain an approximate
bottleneck matching in sublinear update time. In this work, we answer this
question in the affirmative for points on a real line and for points in the
plane with a bounded geometric spread.
</p>
<p>For a set $P$ of $n$ points on a line, we show that there exists a dynamic
algorithm that maintains a bottleneck matching of $P$ and supports insertion
and deletion in $O(\log n)$ time. Moreover, we show that a modified version of
this algorithm maintains a minimum-weight matching with $O(\log n)$ update
(insertion and deletion) time. Next, for a set $P$ of $n$ points in the plane,
we show that a ($6\sqrt{2}$)-factor approximate bottleneck matching of $P_k$,
at each time step $k$, can be maintained in $O(\log{\Delta})$ amortized time
per insertion and $O(\log{\Delta} + |P_k|)$ amortized time per deletion, where
$\Delta$ is the geometric spread of $P$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Abu_Affash_A/0/1/0/all/0/1">A. Karim Abu-Affash</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhore_S/0/1/0/all/0/1">Sujoy Bhore</a>, <a href="http://arxiv.org/find/cs/1/au:+Carmi_P/0/1/0/all/0/1">Paz Carmi</a></p><p>A fundamental question in computational geometry is for a set of input points
in the Euclidean space, that is subject to discrete changes (insertion/deletion
of points at each time step), whether it is possible to maintain an approximate
bottleneck matching in sublinear update time. In this work, we answer this
question in the affirmative for points on a real line and for points in the
plane with a bounded geometric spread.
</p>
<p>For a set $P$ of $n$ points on a line, we show that there exists a dynamic
algorithm that maintains a bottleneck matching of $P$ and supports insertion
and deletion in $O(\log n)$ time. Moreover, we show that a modified version of
this algorithm maintains a minimum-weight matching with $O(\log n)$ update
(insertion and deletion) time. Next, for a set $P$ of $n$ points in the plane,
we show that a ($6\sqrt{2}$)-factor approximate bottleneck matching of $P_k$,
at each time step $k$, can be maintained in $O(\log{\Delta})$ amortized time
per insertion and $O(\log{\Delta} + |P_k|)$ amortized time per deletion, where
$\Delta$ is the geometric spread of $P$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T01:30:00Z">Wednesday, February 22 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.10626'>Lightweight-Yet-Efficient: Revitalizing Ball-Tree for Point-to-Hyperplane Nearest Neighbor Search</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Qiang Huang, Anthony K. H. Tung</p><p>Finding the nearest neighbor to a hyperplane (or Point-to-Hyperplane Nearest
Neighbor Search, simply P2HNNS) is a new and challenging problem with
applications in many research domains. While existing state-of-the-art hashing
schemes (e.g., NH and FH) are able to achieve sublinear time complexity without
the assumption of the data being in a unit hypersphere, they require an
asymmetric transformation, which increases the data dimension from $d$ to
$\Omega(d^2)$. This leads to considerable overhead for indexing and incurs
significant distortion errors.
</p>
<p>In this paper, we investigate a tree-based approach for solving P2HNNS using
the classical Ball-Tree index. Compared to hashing-based methods, tree-based
methods usually require roughly linear costs for construction, and they provide
different kinds of approximations with excellent flexibility. A simple
branch-and-bound algorithm with a novel lower bound is first developed on
Ball-Tree for performing P2HNNS. Then, a new tree structure named BC-Tree,
which maintains the Ball and Cone structures in the leaf nodes of Ball-Tree, is
described together with two effective strategies, i.e., point-level pruning and
collaborative inner product computing. BC-Tree inherits both the low
construction cost and lightweight property of Ball-Tree while providing a
similar or more efficient search. Experimental results over 16 real-world data
sets show that Ball-Tree and BC-Tree are around 1.1$\sim$10$\times$ faster than
NH and FH, and they can reduce the index size and indexing time by about
1$\sim$3 orders of magnitudes on average. The code is available at
\url{github.com/HuangQiang/BC-Tree}.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1">Qiang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tung_A/0/1/0/all/0/1">Anthony K. H. Tung</a></p><p>Finding the nearest neighbor to a hyperplane (or Point-to-Hyperplane Nearest
Neighbor Search, simply P2HNNS) is a new and challenging problem with
applications in many research domains. While existing state-of-the-art hashing
schemes (e.g., NH and FH) are able to achieve sublinear time complexity without
the assumption of the data being in a unit hypersphere, they require an
asymmetric transformation, which increases the data dimension from $d$ to
$\Omega(d^2)$. This leads to considerable overhead for indexing and incurs
significant distortion errors.
</p>
<p>In this paper, we investigate a tree-based approach for solving P2HNNS using
the classical Ball-Tree index. Compared to hashing-based methods, tree-based
methods usually require roughly linear costs for construction, and they provide
different kinds of approximations with excellent flexibility. A simple
branch-and-bound algorithm with a novel lower bound is first developed on
Ball-Tree for performing P2HNNS. Then, a new tree structure named BC-Tree,
which maintains the Ball and Cone structures in the leaf nodes of Ball-Tree, is
described together with two effective strategies, i.e., point-level pruning and
collaborative inner product computing. BC-Tree inherits both the low
construction cost and lightweight property of Ball-Tree while providing a
similar or more efficient search. Experimental results over 16 real-world data
sets show that Ball-Tree and BC-Tree are around 1.1$\sim$10$\times$ faster than
NH and FH, and they can reduce the index size and indexing time by about
1$\sim$3 orders of magnitudes on average. The code is available at
\url{https://github.com/HuangQiang/BC-Tree}.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T01:30:00Z">Wednesday, February 22 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.10244'>Basic quantum subroutines: finding multiple marked elements and summing numbers</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Joran van Apeldoorn, Sander Gribling, Harold Nieuwboer</p><p>We show how to find all $k$ marked elements in a list of size $N$ using the
optimal number $O(\sqrt{N k})$ of quantum queries and only a polylogarithmic
overhead in the gate complexity, in the setting where one has a small quantum
memory. Previous algorithms either incurred a factor $k$ overhead in the gate
complexity, or had an extra factor $\log(k)$ in the query complexity.
</p>
<p>We then consider the problem of finding a multiplicative
$\delta$-approximation of $s = \sum_{i=1}^N v_i$ where $v=(v_i) \in [0,1]^N$,
given quantum query access to a binary description of $v$. We give an algorithm
that does so, with probability at least $1-\rho$, using $O(\sqrt{N \log(1/\rho)
/ \delta})$ queries (under mild assumptions on $\rho$). This quadratically
improves the dependence on $1/\delta$ and $\log(1/\rho)$ compared to a
straightforward application of amplitude estimation. To obtain the improved
$\log(1/\rho)$ dependence we use the first result.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Apeldoorn_J/0/1/0/all/0/1">Joran van Apeldoorn</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Gribling_S/0/1/0/all/0/1">Sander Gribling</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Nieuwboer_H/0/1/0/all/0/1">Harold Nieuwboer</a></p><p>We show how to find all $k$ marked elements in a list of size $N$ using the
optimal number $O(\sqrt{N k})$ of quantum queries and only a polylogarithmic
overhead in the gate complexity, in the setting where one has a small quantum
memory. Previous algorithms either incurred a factor $k$ overhead in the gate
complexity, or had an extra factor $\log(k)$ in the query complexity.
</p>
<p>We then consider the problem of finding a multiplicative
$\delta$-approximation of $s = \sum_{i=1}^N v_i$ where $v=(v_i) \in [0,1]^N$,
given quantum query access to a binary description of $v$. We give an algorithm
that does so, with probability at least $1-\rho$, using $O(\sqrt{N \log(1/\rho)
/ \delta})$ queries (under mild assumptions on $\rho$). This quadratically
improves the dependence on $1/\delta$ and $\log(1/\rho)$ compared to a
straightforward application of amplitude estimation. To obtain the improved
$\log(1/\rho)$ dependence we use the first result.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T01:30:00Z">Wednesday, February 22 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.10249'>Faster high-accuracy log-concave sampling via algorithmic warm starts</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jason M. Altschuler, Sinho Chewi</p><p>Understanding the complexity of sampling from a strongly log-concave and
log-smooth distribution $\pi$ on $\mathbb{R}^d$ to high accuracy is a
fundamental problem, both from a practical and theoretical standpoint. In
practice, high-accuracy samplers such as the classical Metropolis-adjusted
Langevin algorithm (MALA) remain the de facto gold standard; and in theory, via
the proximal sampler reduction, it is understood that such samplers are key for
sampling even beyond log-concavity (in particular, for distributions satisfying
isoperimetric assumptions).
</p>
<p>In this work, we improve the dimension dependence of this sampling problem to
$\tilde{O}(d^{1/2})$, whereas the previous best result for MALA was
$\tilde{O}(d)$. This closes the long line of work on the complexity of MALA,
and moreover leads to state-of-the-art guarantees for high-accuracy sampling
under strong log-concavity and beyond (thanks to the aforementioned reduction).
</p>
<p>Our starting point is that the complexity of MALA improves to
$\tilde{O}(d^{1/2})$, but only under a warm start (an initialization with
constant R\'enyi divergence w.r.t. $\pi$). Previous algorithms took much longer
to find a warm start than to use it, and closing this gap has remained an
important open problem in the field. Our main technical contribution settles
this problem by establishing the first $\tilde{O}(d^{1/2})$ R\'enyi mixing
rates for the discretized underdamped Langevin diffusion. For this, we develop
new differential-privacy-inspired techniques based on R\'enyi divergences with
Orlicz--Wasserstein shifts, which allow us to sidestep longstanding challenges
for proving fast convergence of hypocoercive differential equations.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Altschuler_J/0/1/0/all/0/1">Jason M. Altschuler</a>, <a href="http://arxiv.org/find/math/1/au:+Chewi_S/0/1/0/all/0/1">Sinho Chewi</a></p><p>Understanding the complexity of sampling from a strongly log-concave and
log-smooth distribution $\pi$ on $\mathbb{R}^d$ to high accuracy is a
fundamental problem, both from a practical and theoretical standpoint. In
practice, high-accuracy samplers such as the classical Metropolis-adjusted
Langevin algorithm (MALA) remain the de facto gold standard; and in theory, via
the proximal sampler reduction, it is understood that such samplers are key for
sampling even beyond log-concavity (in particular, for distributions satisfying
isoperimetric assumptions).
</p>
<p>In this work, we improve the dimension dependence of this sampling problem to
$\tilde{O}(d^{1/2})$, whereas the previous best result for MALA was
$\tilde{O}(d)$. This closes the long line of work on the complexity of MALA,
and moreover leads to state-of-the-art guarantees for high-accuracy sampling
under strong log-concavity and beyond (thanks to the aforementioned reduction).
</p>
<p>Our starting point is that the complexity of MALA improves to
$\tilde{O}(d^{1/2})$, but only under a warm start (an initialization with
constant R\'enyi divergence w.r.t. $\pi$). Previous algorithms took much longer
to find a warm start than to use it, and closing this gap has remained an
important open problem in the field. Our main technical contribution settles
this problem by establishing the first $\tilde{O}(d^{1/2})$ R\'enyi mixing
rates for the discretized underdamped Langevin diffusion. For this, we develop
new differential-privacy-inspired techniques based on R\'enyi divergences with
Orlicz--Wasserstein shifts, which allow us to sidestep longstanding challenges
for proving fast convergence of hypocoercive differential equations.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T01:30:00Z">Wednesday, February 22 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.10359'>Replicable Clustering</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Hossein Esfandiari, Amin Karbasi, Vahab Mirrokni, Grigoris Velegkas, Felix Zhou</p><p>In this paper, we design replicable algorithms in the context of statistical
clustering under the recently introduced notion of replicability. A clustering
algorithm is replicable if, with high probability, it outputs the exact same
clusters after two executions with datasets drawn from the same distribution
when its internal randomness is shared across the executions. We propose such
algorithms for the statistical $k$-medians, statistical $k$-means, and
statistical $k$-centers problems by utilizing approximation routines for their
combinatorial counterparts in a black-box manner. In particular, we demonstrate
a replicable $O(1)$-approximation algorithm for statistical Euclidean
$k$-medians ($k$-means) with $\operatorname{poly}(d)$ sample complexity. We
also describe a $O(1)$-approximation algorithm with an additional
$O(1)$-additive error for statistical Euclidean $k$-centers, albeit with
$\exp(d)$ sample complexity.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Esfandiari_H/0/1/0/all/0/1">Hossein Esfandiari</a>, <a href="http://arxiv.org/find/cs/1/au:+Karbasi_A/0/1/0/all/0/1">Amin Karbasi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1">Vahab Mirrokni</a>, <a href="http://arxiv.org/find/cs/1/au:+Velegkas_G/0/1/0/all/0/1">Grigoris Velegkas</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1">Felix Zhou</a></p><p>In this paper, we design replicable algorithms in the context of statistical
clustering under the recently introduced notion of replicability. A clustering
algorithm is replicable if, with high probability, it outputs the exact same
clusters after two executions with datasets drawn from the same distribution
when its internal randomness is shared across the executions. We propose such
algorithms for the statistical $k$-medians, statistical $k$-means, and
statistical $k$-centers problems by utilizing approximation routines for their
combinatorial counterparts in a black-box manner. In particular, we demonstrate
a replicable $O(1)$-approximation algorithm for statistical Euclidean
$k$-medians ($k$-means) with $\operatorname{poly}(d)$ sample complexity. We
also describe a $O(1)$-approximation algorithm with an additional
$O(1)$-additive error for statistical Euclidean $k$-centers, albeit with
$\exp(d)$ sample complexity.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T01:30:00Z">Wednesday, February 22 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.10613'>Approximating Bin Packing with Conflict Graphs via Maximization Techniques</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ilan Doron-Arad, Hadas Shachnai</p><p>We give a comprehensive study of bin packing with conflicts (BPC). The input
is a set $I$ of items, sizes $s:I \rightarrow [0,1]$, and a conflict graph $G =
(I,E)$. The goal is to find a partition of $I$ into a minimum number of
independent sets, each of total size at most $1$. Being a generalization of the
notoriously hard graph coloring problem, BPC has been studied mostly on
polynomially colorable conflict graphs. An intriguing open question is whether
BPC on such graphs admits the same best known approximation guarantees as
classic bin packing.
</p>
<p>We answer this question negatively, by showing that (in contrast to bin
packing) there is no asymptotic polynomial-time approximation scheme (APTAS)
for BPC already on seemingly easy graph classes, such as bipartite and split
graphs. We complement this result with improved approximation guarantees for
BPC on several prominent graph classes. Most notably, we derive an asymptotic
$1.391$-approximation for bipartite graphs, a $2.445$-approximation for perfect
graphs, and a $\left(1+\frac{2}{e}\right)$-approximation for split graphs. To
this end, we introduce a generic framework relying on a novel interpretation of
BPC allowing us to solve the problem via maximization techniques.
</p>
<p>Our framework may find use in tackling BPC on other graph classes arising in
applications.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Doron_Arad_I/0/1/0/all/0/1">Ilan Doron-Arad</a>, <a href="http://arxiv.org/find/cs/1/au:+Shachnai_H/0/1/0/all/0/1">Hadas Shachnai</a></p><p>We give a comprehensive study of bin packing with conflicts (BPC). The input
is a set $I$ of items, sizes $s:I \rightarrow [0,1]$, and a conflict graph $G =
(I,E)$. The goal is to find a partition of $I$ into a minimum number of
independent sets, each of total size at most $1$. Being a generalization of the
notoriously hard graph coloring problem, BPC has been studied mostly on
polynomially colorable conflict graphs. An intriguing open question is whether
BPC on such graphs admits the same best known approximation guarantees as
classic bin packing.
</p>
<p>We answer this question negatively, by showing that (in contrast to bin
packing) there is no asymptotic polynomial-time approximation scheme (APTAS)
for BPC already on seemingly easy graph classes, such as bipartite and split
graphs. We complement this result with improved approximation guarantees for
BPC on several prominent graph classes. Most notably, we derive an asymptotic
$1.391$-approximation for bipartite graphs, a $2.445$-approximation for perfect
graphs, and a $\left(1+\frac{2}{e}\right)$-approximation for split graphs. To
this end, we introduce a generic framework relying on a novel interpretation of
BPC allowing us to solve the problem via maximization techniques.
</p>
<p>Our framework may find use in tackling BPC on other graph classes arising in
applications.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T01:30:00Z">Wednesday, February 22 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.10643'>New Width Parameters for Independent Set: One-sided-mim-width and Neighbor-depth</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Benjamin Bergougnoux, Tuukka Korhonen, Igor Razgon</p><p>We study the tractability of the maximum independent set problem from the
viewpoint of graph width parameters, with the goal of defining a width
parameter that is as general as possible and allows to solve independent set in
polynomial-time on graphs where the parameter is bounded. We introduce two new
graph width parameters: one-sided maximum induced matching-width (o-mim-width)
and neighbor-depth. O-mim-width is a graph parameter that is more general than
the known parameters mim-width and tree-independence number, and we show that
independent set and feedback vertex set can be solved in polynomial-time given
a decomposition with bounded o-mim-width. O-mim-width is the first width
parameter that gives a common generalization of chordal graphs and graphs of
bounded clique-width in terms of tractability of these problems.
</p>
<p>The parameter o-mim-width, as well as the related parameters mim-width and
sim-width, have the limitation that no algorithms are known to compute
bounded-width decompositions in polynomial-time. To partially resolve this
limitation, we introduce the parameter neighbor-depth. We show that given a
graph of neighbor-depth $k$, independent set can be solved in time $n^{O(k)}$
even without knowing a corresponding decomposition. We also show that
neighbor-depth is bounded by a polylogarithmic function on the number of
vertices on large classes of graphs, including graphs of bounded o-mim-width,
and more generally graphs of bounded sim-width, giving a quasipolynomial-time
algorithm for independent set on these graph classes. This resolves an open
problem asked by Kang, Kwon, Str{\o}mme, and Telle [TCS 2017].
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bergougnoux_B/0/1/0/all/0/1">Benjamin Bergougnoux</a>, <a href="http://arxiv.org/find/cs/1/au:+Korhonen_T/0/1/0/all/0/1">Tuukka Korhonen</a>, <a href="http://arxiv.org/find/cs/1/au:+Razgon_I/0/1/0/all/0/1">Igor Razgon</a></p><p>We study the tractability of the maximum independent set problem from the
viewpoint of graph width parameters, with the goal of defining a width
parameter that is as general as possible and allows to solve independent set in
polynomial-time on graphs where the parameter is bounded. We introduce two new
graph width parameters: one-sided maximum induced matching-width (o-mim-width)
and neighbor-depth. O-mim-width is a graph parameter that is more general than
the known parameters mim-width and tree-independence number, and we show that
independent set and feedback vertex set can be solved in polynomial-time given
a decomposition with bounded o-mim-width. O-mim-width is the first width
parameter that gives a common generalization of chordal graphs and graphs of
bounded clique-width in terms of tractability of these problems.
</p>
<p>The parameter o-mim-width, as well as the related parameters mim-width and
sim-width, have the limitation that no algorithms are known to compute
bounded-width decompositions in polynomial-time. To partially resolve this
limitation, we introduce the parameter neighbor-depth. We show that given a
graph of neighbor-depth $k$, independent set can be solved in time $n^{O(k)}$
even without knowing a corresponding decomposition. We also show that
neighbor-depth is bounded by a polylogarithmic function on the number of
vertices on large classes of graphs, including graphs of bounded o-mim-width,
and more generally graphs of bounded sim-width, giving a quasipolynomial-time
algorithm for independent set on these graph classes. This resolves an open
problem asked by Kang, Kwon, Str{\o}mme, and Telle [TCS 2017].
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T01:30:00Z">Wednesday, February 22 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.10662'>Snakes and Ladders: a Treewidth Story</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Steven Chaplick, Steven Kelk, Ruben Meuwese, Matus Mihalak, Georgios Stamoulis</p><p>Let $G$ be an undirected graph. We say that $G$ contains a ladder of length
$k$ if the $2 \times (k+1)$ grid graph is an induced subgraph of $G$ that is
only connected to the rest of $G$ via its four cornerpoints. We prove that if
all the ladders contained in $G$ are reduced to length 4, the treewidth remains
unchanged (and that this bound is tight). Our result indicates that, when
computing the treewidth of a graph, long ladders can simply be reduced, and
that minimal forbidden minors for bounded treewidth graphs cannot contain long
ladders. Our result also settles an open problem from algorithmic
phylogenetics: the common chain reduction rule, used to simplify the comparison
of two evolutionary trees, is treewidth-preserving in the display graph of the
two trees.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Chaplick_S/0/1/0/all/0/1">Steven Chaplick</a>, <a href="http://arxiv.org/find/math/1/au:+Kelk_S/0/1/0/all/0/1">Steven Kelk</a>, <a href="http://arxiv.org/find/math/1/au:+Meuwese_R/0/1/0/all/0/1">Ruben Meuwese</a>, <a href="http://arxiv.org/find/math/1/au:+Mihalak_M/0/1/0/all/0/1">Matus Mihalak</a>, <a href="http://arxiv.org/find/math/1/au:+Stamoulis_G/0/1/0/all/0/1">Georgios Stamoulis</a></p><p>Let $G$ be an undirected graph. We say that $G$ contains a ladder of length
$k$ if the $2 \times (k+1)$ grid graph is an induced subgraph of $G$ that is
only connected to the rest of $G$ via its four cornerpoints. We prove that if
all the ladders contained in $G$ are reduced to length 4, the treewidth remains
unchanged (and that this bound is tight). Our result indicates that, when
computing the treewidth of a graph, long ladders can simply be reduced, and
that minimal forbidden minors for bounded treewidth graphs cannot contain long
ladders. Our result also settles an open problem from algorithmic
phylogenetics: the common chain reduction rule, used to simplify the comparison
of two evolutionary trees, is treewidth-preserving in the display graph of the
two trees.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T01:30:00Z">Wednesday, February 22 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.10805'>Repeated Bilateral Trade Against a Smoothed Adversary</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Nicol&#xf2; Cesa-Bianchi, Tommaso Cesari, Roberto Colomboni, Federico Fusco, Stefano Leonardi</p><p>We study repeated bilateral trade where an adaptive $\sigma$-smooth adversary
generates the valuations of sellers and buyers. We provide a complete
characterization of the regret regimes for fixed-price mechanisms under
different feedback models in the two cases where the learner can post either
the same or different prices to buyers and sellers. We begin by showing that
the minimax regret after $T$ rounds is of order $\sqrt{T}$ in the full-feedback
scenario. Under partial feedback, any algorithm that has to post the same price
to buyers and sellers suffers worst-case linear regret. However, when the
learner can post two different prices at each round, we design an algorithm
enjoying regret of order $T^{3/4}$ ignoring log factors. We prove that this
rate is optimal by presenting a surprising $T^{3/4}$ lower bound, which is the
main technical contribution of the paper.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cesa_Bianchi_N/0/1/0/all/0/1">Nicol&#xf2; Cesa-Bianchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cesari_T/0/1/0/all/0/1">Tommaso Cesari</a>, <a href="http://arxiv.org/find/cs/1/au:+Colomboni_R/0/1/0/all/0/1">Roberto Colomboni</a>, <a href="http://arxiv.org/find/cs/1/au:+Fusco_F/0/1/0/all/0/1">Federico Fusco</a>, <a href="http://arxiv.org/find/cs/1/au:+Leonardi_S/0/1/0/all/0/1">Stefano Leonardi</a></p><p>We study repeated bilateral trade where an adaptive $\sigma$-smooth adversary
generates the valuations of sellers and buyers. We provide a complete
characterization of the regret regimes for fixed-price mechanisms under
different feedback models in the two cases where the learner can post either
the same or different prices to buyers and sellers. We begin by showing that
the minimax regret after $T$ rounds is of order $\sqrt{T}$ in the full-feedback
scenario. Under partial feedback, any algorithm that has to post the same price
to buyers and sellers suffers worst-case linear regret. However, when the
learner can post two different prices at each round, we design an algorithm
enjoying regret of order $T^{3/4}$ ignoring log factors. We prove that this
rate is optimal by presenting a surprising $T^{3/4}$ lower bound, which is the
main technical contribution of the paper.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T01:30:00Z">Wednesday, February 22 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.10826'>ITERATED INSIDE OUT: a new exact algorithm for the transportation problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Roberto Bargetto, Federico Della Croce, Rosario Scatamacchia</p><p>We propose a novel exact algorithm for the transportation problem, one of the
paradigmatic network optimization problems. The algorithm, denoted Iterated
Inside Out, requires in input a basic feasible solution and is composed by two
main phases that are iteratively repeated until an optimal basic feasible
solution is reached. In the first "inside" phase, the algorithm progressively
improves upon a given basic solution by increasing the value of several
non-basic variables with negative reduced cost. This phase typically outputs a
non-basic feasible solution interior to the constraints set polytope. The
second "out" phase moves in the opposite direction by iteratively setting to
zero several variables until a new improved basic feasible solution is reached.
Extensive computational tests show that the proposed approach strongly
outperforms all versions of network and linear programming algorithms available
in the commercial solvers Cplex and Gurobi and other exact algorithms available
in the literature.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Bargetto_R/0/1/0/all/0/1">Roberto Bargetto</a>, <a href="http://arxiv.org/find/math/1/au:+Croce_F/0/1/0/all/0/1">Federico Della Croce</a>, <a href="http://arxiv.org/find/math/1/au:+Scatamacchia_R/0/1/0/all/0/1">Rosario Scatamacchia</a></p><p>We propose a novel exact algorithm for the transportation problem, one of the
paradigmatic network optimization problems. The algorithm, denoted Iterated
Inside Out, requires in input a basic feasible solution and is composed by two
main phases that are iteratively repeated until an optimal basic feasible
solution is reached. In the first "inside" phase, the algorithm progressively
improves upon a given basic solution by increasing the value of several
non-basic variables with negative reduced cost. This phase typically outputs a
non-basic feasible solution interior to the constraints set polytope. The
second "out" phase moves in the opposite direction by iteratively setting to
zero several variables until a new improved basic feasible solution is reached.
Extensive computational tests show that the proposed approach strongly
outperforms all versions of network and linear programming algorithms available
in the commercial solvers Cplex and Gurobi and other exact algorithms available
in the literature.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T01:30:00Z">Wednesday, February 22 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.10844'>Robust Mean Estimation Without a Mean: Dimension-Independent Error in Polynomial Time for Symmetric Distributions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Gleb Novikov, David Steurer, Stefan Tiegel</p><p>In this work, we study the problem of robustly estimating the mean/location
parameter of distributions without moment bounds. For a large class of
distributions satisfying natural symmetry constraints we give a sequence of
algorithms that can efficiently estimate its location without incurring
dimension-dependent factors in the error. Concretely, suppose an adversary can
arbitrarily corrupt an $\varepsilon$-fraction of the observed samples. For
every $k \in \mathbb{N}$, we design an estimator using time and samples
$\tilde{O}({d^k})$ such that the dependence of the error on the corruption
level $\varepsilon$ is an additive factor of $O(\varepsilon^{1-\frac{1}{2k}})$.
The dependence on other problem parameters is also nearly optimal. Our class
contains products of arbitrary symmetric one-dimensional distributions as well
as elliptical distributions, a vast generalization of the Gaussian
distribution. Examples include product Cauchy distributions and multi-variate
$t$-distributions. In particular, even the first moment might not exist.
</p>
<p>We provide the first efficient algorithms for this class of distributions.
Previously, such results where only known under boundedness assumptions on the
moments of the distribution and in particular, are provably impossible in the
absence of symmetry [KSS18, CTBJ22]. For the class of distributions we
consider, all previous estimators either require exponential time or incur
error depending on the dimension. Our algorithms are based on a generalization
of the filtering technique [DK22]. We show how this machinery can be combined
with Huber-loss-based approach to work with projections of the noise. Moreover,
we show how sum-of-squares proofs can be used to obtain algorithmic guarantees
even for distributions without first moment. We believe that this approach may
find other application in future works.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Novikov_G/0/1/0/all/0/1">Gleb Novikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Steurer_D/0/1/0/all/0/1">David Steurer</a>, <a href="http://arxiv.org/find/cs/1/au:+Tiegel_S/0/1/0/all/0/1">Stefan Tiegel</a></p><p>In this work, we study the problem of robustly estimating the mean/location
parameter of distributions without moment bounds. For a large class of
distributions satisfying natural symmetry constraints we give a sequence of
algorithms that can efficiently estimate its location without incurring
dimension-dependent factors in the error. Concretely, suppose an adversary can
arbitrarily corrupt an $\varepsilon$-fraction of the observed samples. For
every $k \in \mathbb{N}$, we design an estimator using time and samples
$\tilde{O}({d^k})$ such that the dependence of the error on the corruption
level $\varepsilon$ is an additive factor of $O(\varepsilon^{1-\frac{1}{2k}})$.
The dependence on other problem parameters is also nearly optimal. Our class
contains products of arbitrary symmetric one-dimensional distributions as well
as elliptical distributions, a vast generalization of the Gaussian
distribution. Examples include product Cauchy distributions and multi-variate
$t$-distributions. In particular, even the first moment might not exist.
</p>
<p>We provide the first efficient algorithms for this class of distributions.
Previously, such results where only known under boundedness assumptions on the
moments of the distribution and in particular, are provably impossible in the
absence of symmetry [KSS18, CTBJ22]. For the class of distributions we
consider, all previous estimators either require exponential time or incur
error depending on the dimension. Our algorithms are based on a generalization
of the filtering technique [DK22]. We show how this machinery can be combined
with Huber-loss-based approach to work with projections of the noise. Moreover,
we show how sum-of-squares proofs can be used to obtain algorithmic guarantees
even for distributions without first moment. We believe that this approach may
find other application in future works.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-22T01:30:00Z">Wednesday, February 22 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Tuesday, February 21
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://lucatrevisan.wordpress.com/2023/02/21/this-year-for-lent-we-gave-up-being-renters-in-milan/'>This year, for Lent, we gave up being renters in Milan</a></h3>
        <p class='tr-article-feed'>from <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          
        
        </div>

        <div class='tr-article-summary'>
        
          
          <figure data-carousel-extra='{"blog_id":821887,"permalink":"https:\/\/lucatrevisan.wordpress.com\/2023\/02\/21\/this-year-for-lent-we-gave-up-being-renters-in-milan\/"}'  class="wp-block-gallery has-nested-images columns-default is-cropped wp-block-gallery-1 is-layout-flex">
<figure class="wp-block-image size-large"><a href="https://lucatrevisan.files.wordpress.com/2023/02/img_1992.jpg"><img data-attachment-id="4664" data-permalink="https://lucatrevisan.wordpress.com/2023/02/21/this-year-for-lent-we-gave-up-being-renters-in-milan/img_1992/" data-orig-file="https://lucatrevisan.files.wordpress.com/2023/02/img_1992.jpg" data-orig-size="3009,3318" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone XR&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1676998785&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.25&quot;,&quot;iso&quot;:&quot;500&quot;,&quot;shutter_speed&quot;:&quot;0.033333333333333&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;,&quot;latitude&quot;:&quot;45.452072222222&quot;,&quot;longitude&quot;:&quot;9.1951472222222&quot;}" data-image-title="img_1992" data-image-description="" data-image-caption="" data-medium-file="https://lucatrevisan.files.wordpress.com/2023/02/img_1992.jpg?w=272" data-large-file="https://lucatrevisan.files.wordpress.com/2023/02/img_1992.jpg?w=584" data-id="4664"  src="https://lucatrevisan.files.wordpress.com/2023/02/img_1992.jpg?w=929" alt="" class="wp-image-4664" srcset="https://lucatrevisan.files.wordpress.com/2023/02/img_1992.jpg?w=929 929w, https://lucatrevisan.files.wordpress.com/2023/02/img_1992.jpg?w=1858 1858w, https://lucatrevisan.files.wordpress.com/2023/02/img_1992.jpg?w=136 136w, https://lucatrevisan.files.wordpress.com/2023/02/img_1992.jpg?w=272 272w, https://lucatrevisan.files.wordpress.com/2023/02/img_1992.jpg?w=768 768w" sizes="(max-width: 929px) 100vw, 929px" /></a></figure>
</figure>
<p class="authors">By luca</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-21T16:11:03Z">Tuesday, February 21 2023, 16:11</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.09220'>A Tight Lower Bound for Compact Set Packing</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Huairui Chu</p><p>This note is devoted to show a simple proof of a tight lower bound of the
parameterized compact set packing problem, based on ETH.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1">Huairui Chu</a></p><p>This note is devoted to show a simple proof of a tight lower bound of the
parameterized compact set packing problem, based on ETH.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-21T01:30:00Z">Tuesday, February 21 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.09512'>Hard Examples Requiring Exhaustive Search do Exist</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ke Xu, Guangyan Zhou</p><p>In this paper, by constructing hard examples of CSP (with large domains) and
SAT (with long clauses), we prove that such examples cannot be solved without
exhaustive search, which implies a weaker conclusion P $\neq$ NP. This
constructive approach for proving impossibility results is very different (and
missing) from those currently used in computational complexity theory, but is
similar to that used by Kurt G\"{o}del in proving his famous logical
impossibility results. Just as shown by G\"{o}del's results that formal
unprovability is provable in mathematics, the results of this paper show that
proving computational hardness is not hard in mathematics. The intuition behind
this mathematical tractability is that proving exhaustive search for
constructed examples avoids handling numerous effective strategies of avoiding
exhaustive search that exist for many hard problems such as 3-SAT.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Ke Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1">Guangyan Zhou</a></p><p>In this paper, by constructing hard examples of CSP (with large domains) and
SAT (with long clauses), we prove that such examples cannot be solved without
exhaustive search, which implies a weaker conclusion P $\neq$ NP. This
constructive approach for proving impossibility results is very different (and
missing) from those currently used in computational complexity theory, but is
similar to that used by Kurt G\"{o}del in proving his famous logical
impossibility results. Just as shown by G\"{o}del's results that formal
unprovability is provable in mathematics, the results of this paper show that
proving computational hardness is not hard in mathematics. The intuition behind
this mathematical tractability is that proving exhaustive search for
constructed examples avoids handling numerous effective strategies of avoiding
exhaustive search that exist for many hard problems such as 3-SAT.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-21T01:30:00Z">Tuesday, February 21 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.09237'>Characterizations of Network Auctions and Generalizations of VCG</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mingyu Xiao, Guixin Lin, Bakh Khoussainov, Yuchao Song</p><p>With the growth of networks, promoting products through social networks has
become an important problem. For auctions in social networks, items are needed
to be sold to agents in a network, where each agent can bid and also diffuse
the sale information to her neighbors. Thus, the agents' social relations are
intervened with their bids in the auctions. In network auctions, the classical
VCG mechanism fails to retain key properties. In order to better understand
network auctions, in this paper, we characterize network auctions for the
single-unit setting with respect to IR, WBB, IC, SWM, and other properties. For
example, we present sufficient conditions for mechanisms to be social welfare
maximizing and (weakly) incentive compatible. With the help of these properties
and new concepts such as rewards, participation rewards, and so on, we show how
to design SWM mechanisms to satisfy IC as much as possible, and IC mechanisms
to maximize the revenue. Our results provide insights into understanding
auctions in social networks.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1">Mingyu Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guixin Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Khoussainov_B/0/1/0/all/0/1">Bakh Khoussainov</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yuchao Song</a></p><p>With the growth of networks, promoting products through social networks has
become an important problem. For auctions in social networks, items are needed
to be sold to agents in a network, where each agent can bid and also diffuse
the sale information to her neighbors. Thus, the agents' social relations are
intervened with their bids in the auctions. In network auctions, the classical
VCG mechanism fails to retain key properties. In order to better understand
network auctions, in this paper, we characterize network auctions for the
single-unit setting with respect to IR, WBB, IC, SWM, and other properties. For
example, we present sufficient conditions for mechanisms to be social welfare
maximizing and (weakly) incentive compatible. With the help of these properties
and new concepts such as rewards, participation rewards, and so on, we show how
to design SWM mechanisms to satisfy IC as much as possible, and IC mechanisms
to maximize the revenue. Our results provide insights into understanding
auctions in social networks.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-21T01:30:00Z">Tuesday, February 21 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.09239'>Faster Wavelet Trees with Quad Vectors</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Matteo Ceregini, Florian Kurpicz, Rossano Venturini</p><p>Given a text, rank and select queries return the number of occurrences of a
character up to a position (rank) or the position of a character with a given
rank (select). These queries have applications in, e.g., compression,
computational geometry, and pattern matching in the form of the backwards
search -- the backbone of many compressed full-text indices. A wavelet tree is
a compact data structure that for a text of length $n$ over an alphabet of size
$\sigma$ requires only $n\lceil\log\sigma\rceil(1+o(1))$ bits of space and can
answer rank and select queries in $\Theta(\log \sigma)$ time. Wavelet trees are
used in the applications described above.
</p>
<p>In this paper, we show how to improve query performance of wavelet trees by
using a 4-ary tree instead of a binary tree as basis of the wavelet tree. To
this end, we present a space-efficient rank and select data structure for quad
vectors. The 4-ary tree layout of a wavelet tree helps to halve the number of
cache misses during queries and thus reduces the query latency. Our
experimental evaluation shows that our 4-ary wavelet tree can improve the
latency of rank and select queries by a factor of $\approx 2$ compared to the
wavelet tree implementations contained in the widely used Succinct Data
Structure Library (SDSL).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ceregini_M/0/1/0/all/0/1">Matteo Ceregini</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurpicz_F/0/1/0/all/0/1">Florian Kurpicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Venturini_R/0/1/0/all/0/1">Rossano Venturini</a></p><p>Given a text, rank and select queries return the number of occurrences of a
character up to a position (rank) or the position of a character with a given
rank (select). These queries have applications in, e.g., compression,
computational geometry, and pattern matching in the form of the backwards
search -- the backbone of many compressed full-text indices. A wavelet tree is
a compact data structure that for a text of length $n$ over an alphabet of size
$\sigma$ requires only $n\lceil\log\sigma\rceil(1+o(1))$ bits of space and can
answer rank and select queries in $\Theta(\log \sigma)$ time. Wavelet trees are
used in the applications described above.
</p>
<p>In this paper, we show how to improve query performance of wavelet trees by
using a 4-ary tree instead of a binary tree as basis of the wavelet tree. To
this end, we present a space-efficient rank and select data structure for quad
vectors. The 4-ary tree layout of a wavelet tree helps to halve the number of
cache misses during queries and thus reduces the query latency. Our
experimental evaluation shows that our 4-ary wavelet tree can improve the
latency of rank and select queries by a factor of $\approx 2$ compared to the
wavelet tree implementations contained in the widely used Succinct Data
Structure Library (SDSL).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-21T01:30:00Z">Tuesday, February 21 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Monday, February 20
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/015'>TR23-015 |  A Qubit, a Coin, and an Advice String Walk Into a Relational Problem | 

	Scott Aaronson, 

	Harry Buhrman, 

	William Kretschmer</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Relational problems (those with many possible valid outputs) are different from decision problems, but it is easy to forget just how different.  This paper initiates the study of FBQP/qpoly, the class of relational problems solvable in quantum polynomial-time with the help of polynomial-sized quantum advice, along with its analogues for deterministic and randomized computation (FP, FBPP) and advice (/poly, /rpoly).

Our first result is that FBQP/qpoly != FBQP/poly, unconditionally, with no oracle---a striking contrast with what we know about the analogous decision classes.  The proof repurposes the separation between quantum and classical one-way communication complexities due to Bar-Yossef, Jayram, and Kerenidis.  We discuss how this separation raises the prospect of near-term experiments to demonstrate &quot;quantum information supremacy,&quot; a form of quantum supremacy that would not depend on unproved complexity assumptions.

Our second result is that FBPP is not contained in FP/poly---that is, Adleman&#39;s Theorem fails for relational problems---unless PSPACE is contained in NP/poly.  Our proof uses IP=PSPACE and time-bounded Kolmogorov complexity.  On the other hand, we show that proving FBPP not in FP/poly will be hard, as it implies a superpolynomial circuit lower bound for PromiseBPEXP.

We prove the following further results:
* Unconditionally, FP != FBPP and FP/poly != FBPP/poly (even when these classes are carefully defined).
* FBPP/poly = FBPP/rpoly (and likewise for FBQP).  For sampling problems, by contrast, SampBPP/poly != SampBPP/rpoly (and likewise for SampBQP).
        
        </div>

        <div class='tr-article-summary'>
        
          
          Relational problems (those with many possible valid outputs) are different from decision problems, but it is easy to forget just how different.  This paper initiates the study of FBQP/qpoly, the class of relational problems solvable in quantum polynomial-time with the help of polynomial-sized quantum advice, along with its analogues for deterministic and randomized computation (FP, FBPP) and advice (/poly, /rpoly).

Our first result is that FBQP/qpoly != FBQP/poly, unconditionally, with no oracle---a striking contrast with what we know about the analogous decision classes.  The proof repurposes the separation between quantum and classical one-way communication complexities due to Bar-Yossef, Jayram, and Kerenidis.  We discuss how this separation raises the prospect of near-term experiments to demonstrate &quot;quantum information supremacy,&quot; a form of quantum supremacy that would not depend on unproved complexity assumptions.

Our second result is that FBPP is not contained in FP/poly---that is, Adleman&#39;s Theorem fails for relational problems---unless PSPACE is contained in NP/poly.  Our proof uses IP=PSPACE and time-bounded Kolmogorov complexity.  On the other hand, we show that proving FBPP not in FP/poly will be hard, as it implies a superpolynomial circuit lower bound for PromiseBPEXP.

We prove the following further results:
* Unconditionally, FP != FBPP and FP/poly != FBPP/poly (even when these classes are carefully defined).
* FBPP/poly = FBPP/rpoly (and likewise for FBQP).  For sampling problems, by contrast, SampBPP/poly != SampBPP/rpoly (and likewise for SampBQP).
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-20T21:25:19Z">Monday, February 20 2023, 21:25</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2023/02/20/geometric-graphs-unbounded.html'>Geometric graphs with unbounded flip-width</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          At the recent Workshop on Geometry and Graphs in Barbados, most of the technical activity involved working in small groups on research problems, but there was also a nice survey talk by Rose McCarty on flip-width.1 This is a new and very general notion of width in graphs, introduced by Szymon Toruńczyk.2 It is defined in terms of a certain cops-and-robbers game on graphs, and intended to capture the structure inherent in many types of graphs and to unify bounded expansion and twin-width. Rose also helped me edit a preliminary version of this post. Thanks, Rose! Any remaining errors are my fault. &#8617; Szymon Toruńczyk (2023), “Flip-width: cops and robber on dense graphs”, arXiv:2302.00352 &#8617;
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>At the recent <a href="https://wogag.org/">Workshop on Geometry and Graphs</a> in Barbados, most of the technical activity involved working in small groups on research problems, but there was also a nice survey talk by <a href="https://web.math.princeton.edu/~rm1850/">Rose McCarty</a> on flip-width.<sup id="fnref:r" role="doc-noteref"><a href="#fn:r" class="footnote" rel="footnote">1</a></sup> This is a new and very general notion of width in graphs, introduced by <a href="https://www.mimuw.edu.pl/~szymtor/">Szymon Toruńczyk</a>.<sup id="fnref:t" role="doc-noteref"><a href="#fn:t" class="footnote" rel="footnote">2</a></sup> It is defined in terms of a certain cops-and-robbers game on graphs, and intended to capture the structure inherent in many types of graphs and to unify <a href="https://en.wikipedia.org/wiki/Bounded_expansion">bounded expansion</a> and <a href="https://en.wikipedia.org/wiki/Twin-width">twin-width</a>.</p>

<p>For instance, many algorithmic graph problems, such as searching for small patterns in larger graphs (“<a href="https://en.wikipedia.org/wiki/Subgraph_isomorphism_problem">subgraph isomorphism</a>”) can be formulated more abstractly in terms of of checking whether a graph models a given formula in the first-order <a href="https://en.wikipedia.org/wiki/Logic_of_graphs">logic of graphs</a>. Such problems are <a href="https://en.wikipedia.org/wiki/Parameterized_complexity">fixed-parameter tractable</a> when parameterized either by expansion or twin-width, and it is hoped that the same thing will extend to flip-width. Very recent partial results in this direction extend model checking algorithms from bounded expansion to “structurally nowhere dense classes”,<sup id="fnref:dms" role="doc-noteref"><a href="#fn:dms" class="footnote" rel="footnote">3</a></sup> but these classes do not even include everything with bounded twin-width, let alone flip-width.</p>

<p>For the purposes of this post, the only thing we need to know about bounded expansion is that graph families with this property must be sparse: in their graphs, the number of edges must be at most linear in the number of vertices.<sup id="fnref:no" role="doc-noteref"><a href="#fn:no" class="footnote" rel="footnote">4</a></sup> On the other hand, although graph families with bounded twin-width can be dense, they are limited in a different way: the number of graphs in the family, on a set of \(n\) unlabeled vertices, can only be singly exponential <span style="white-space:nowrap">in \(n\).<sup id="fnref:st" role="doc-noteref"><a href="#fn:st" class="footnote" rel="footnote">5</a></sup></span> One way to get a family of graphs that have bounded flip-width but not bounded expansion nor bounded twin-width is to take the union of two families, one dense with bounded twin-width and the other numerous with bounded expansion. For instance, take the graphs that are either <a href="https://en.wikipedia.org/wiki/Cograph">cographs</a> or <a href="https://en.wikipedia.org/wiki/Cubic_graph">3-regular</a>. But this is not a very natural family of graphs. Rose asked: is there a natural family of graphs with bounded flip-width but unbounded twin-width and expansion? For instance, there are many standard types of geometric graphs for which the twin-width and expansion are unbounded; could any of these have bounded flip-width?</p>

<h1 id="cops-and-robbers">Cops and robbers</h1>

<p>Like <a href="https://en.wikipedia.org/wiki/Treewidth">treewidth</a> and bounded expansion, flip-width can be defined using a certain <a href="https://en.wikipedia.org/wiki/Pursuit%E2%80%93evasion">cops-and-robbers game</a> on graphs.<sup id="fnref:t:1" role="doc-noteref"><a href="#fn:t" class="footnote" rel="footnote">2</a></sup></p>

<p>The games used for treewidth and expansion involve “cops with helicopters”, chasing a robber on a given graph. At each point in the game, the cops occupy a limited number of graph vertices. Then, at each move, the cops announce where they will move next, the robber moves to escape them along a path through the currently-unoccupied vertices, and then the cops fly directly to their new locations. The cops win if one of them lands on the robber’s current vertex, and the robber wins by evading the cops indefinitely. The treewidth of a graph is the maximum number of cops that a robber can evade, moving arbitrarily far on each move. A family of graphs has bounded expansion if and only if there is some function \(f\) such that only \(f(r)\) cops are needed to catch a robber who can move at most \(r\) steps per move.<sup id="fnref:t2" role="doc-noteref"><a href="#fn:t2" class="footnote" rel="footnote">6</a></sup></p>

<p>As Rose described, the same game can be described in a different way. Instead of occupying a vertex, the cops set up roadblocks on all the edges incident to it. On each move, the cops announce which vertices will be blockaded next. Then, the robber moves along un-blockaded edges. Finally, the cops remove their current blockades and put up new blockades at the vertices they announced. The cops win by leaving the robber at an isolated vertex, unable to move.</p>

<p style="text-align:center"><img src="/blog/assets/2023/roadblock.jpg" alt="Police roadblock in Washington, DC, January 15, 2021" title="CC-BY image by Mike Licht from Wikimedia commons, File:Inaugural preparation, January 15th Roadblock (50840138737).jpg" style="width:100%;max-width:540px" /></p>

<p>Flip-width is defined in the same way, but with more powerful cops. Instead of blockading a single vertex, they are allowed to perform a “flip” of a subset of vertices. This complements the subgraph within that subset: pairs of vertices that were connected become disconnected, and vice versa. So blockading a single vertex, for instance, can be accomplished by two flips: one flip of the vertex and its neighbors, and one flip of just the neighbors. The first flip disconnects the given vertex, and the second flip restores the original connectivity among the neighboring vertices. It doesn’t matter in which order these two flips (or any set of flips) is performed.</p>

<p style="text-align:center"><img src="/blog/assets/2023/flip-isolate.svg" alt="Isolating a vertex by two flips" style="width:100%;max-width:720px" /></p>

<p>In the flipping game used to define flip-width, at any point in the game, the cops will have performed some limited number of flips. Then in each move, the cops announce which sets of vertices they will flip next. The robber moves along a path in the current flipped graph, to evade these flips. Then, the cops undo their current flips and perform the new flips that they announced. The cops win if they leave the robber at an isolated vertex, unable to move, and the robber wins by avoiding this fate indefinitely. A family of graphs has bounded flip-width if there is some function \(f\) such that only \(f(r)\) flips per move are needed to catch a robber who can move at most \(r\) steps per move.</p>

<p>For the purposes of having bounded flip-width, two graphs that differ from each other only by a finite number \(\varphi\) of flips are essentially the same. If the cops can win on one, they can win on the other with only \(\varphi\) more flips per move. They only need to start by performing those \(\varphi\) flips to convert the second graph into the first one, and then leave those flips in place while they perform the winning strategy on the converted graph. So, for instance, the graphs that differ by a single flip from a 3-regular graph have bounded flip-width, but are again not a very natural class of graphs.</p>

<h1 id="interchanges">Interchanges</h1>

<p>Continuing the road network metaphor, and in the spirit of the <a href="https://en.wikipedia.org/wiki/Haven_(graph_theory)">havens</a> used to model escape strategies in the treewidth game, let’s define a structure I call an <em>interchange</em>, having pairwise connections between many points, which a robber can use to make a getaway from few enough cops.</p>

<p style="text-align:center"><img src="/blog/assets/2017/HighFive.jpg" alt="High Five Interchange at the intersection of I-635 and U.S. Route 75 in Dallas, Texas, looking towards the southwest" title="cropped from https://commons.wikimedia.org/wiki/File:High_Five.jpg by fatguyinalittlecoat on flickr, under a CC-BY 2.0 license" style="width:100%;max-width:540px" /></p>

<p>More precisely, define an interchange of order \(n\) to consist of the following components:</p>

<ul>
  <li>
    <p>Certain designated vertices, which we call <em>lanes</em>. The interchange should have \(n\) lanes, arranged into a sequence. These are colored blue in the following illustrations.</p>
  </li>
  <li>
    <p>More designated vertices, called <em>ramps</em>. Each ramp is associated with a pair of lanes. When two lanes are \(n-3\) or fewer steps apart in the sequence, they have an associated ramp. (We don’t require ramps for pairs of the outermost lanes because they would not be helpful to the robber in the game.) The ramps are colored red in the following illustrations.</p>
  </li>
  <li>
    <p>An edge between each ramp and its two associated lanes.</p>
  </li>
  <li>
    <p>Optional edges between any two lanes or between any two ramps. These will be unused by the robber and do not affect the robber’s strategy. The optional edges mean that the class of all interchanges is huge, too large to have bounded twin-width. But more importantly for us, they allow us to construct geometric realizations of these graphs without worrying about whether or not the construction causes certain pairs of vertices to become adjacent.</p>
  </li>
  <li>
    <p>For a ramp that connects lanes \(x\) and \(y\), optional edges to other lanes between \(x\) and \(y\) in the sequence (edges to lanes outside that range are not allowed).</p>
  </li>
</ul>

<p>The image below shows an example, with the lanes blue, ramps red, optional edges yellow, and required edges black. The blue lanes are ordered in a sequence from left to right, but otherwise the placement of vertices is not meaningful; it’s the graph structure that matters.</p>

<p style="text-align:center"><img src="/blog/assets/2023/5-interchange.svg" alt="Interchange of order 5" style="width:100%;max-width:540px" /></p>

<p>As we show next, large-enough interchanges can be used by the robber to escape any fixed number of cops.</p>

<h1 id="escaping-through-junctions">Escaping through junctions</h1>

<p>Call a set of lanes <em>equivalent</em>, after certain flips have been made, if they are all treated the same by each flip: all included in the flipped set, or all excluded.
Define a <em>junction</em> to be a triple of equivalent lanes that are connected to each other by paths through one or two ramps, after the flips are made. Then:</p>

<ul>
  <li>
    <p>Every four equivalent lanes have at least one junction. For, if the lanes are \(a,b,c,d\) (in sequence order) then the <span style="white-space:nowrap">\(a\)–\(b\)</span> ramp either continues to connect \(a\) to \(b\), or it is flipped and instead connects <span style="white-space:nowrap">\(c\) to \(d\).</span> A third connection is provided either by the <span style="white-space:nowrap">\(b\)–\(c\)</span> ramp or its flip, which connects <span style="white-space:nowrap">\(a\) to \(d\).</span></p>
  </li>
  <li>
    <p>In every six equivalent lanes, at least three of the lanes belong to two otherwise-disjoint junctions. I’ll skip the messy case analysis showing this.</p>
  </li>
  <li>
    <p>Every two junctions are connected by at least one ramp between two of their lanes. If the junctions are \(a,b,c\) and \(d,e,f\), listed in the sequence order of all the lanes, then they have either \(a,b,e,f\) or \(d,e,b,c\) as a subsequence (depending on the ordering between <span style="white-space:nowrap">\(b\) and \(e\)).</span> In either case they are connected by the <span style="white-space:nowrap">\(b\)–\(e\)</span> ramp or its flip.</p>
  </li>
  <li>
    <p>By the same argument, every two triples of equivalent lanes are connected by at least one ramp.</p>
  </li>
</ul>

<p>These connections imply that, in an interchange that is big enough to guarantee the existence of junctions, the robber can win by always moving to a lane that will become part of a junction after the announced flips happen.</p>

<p>In more detail, suppose that the cops and a robber play the flipping game, with \(t\) flips per move and \(r\ge 6\), and that the graph includes an interchange of <span style="white-space:nowrap">order \(3\cdot 2^{2t}+1\).</span> This interchange is big enough to ensure that some four lanes are equivalent both in the current and announced set of flips. These four lanes include a junction under the announced flips. The robber can move to this new junction using at most two ramps within the current junction and then one more ramp to cross between the two triples of lanes. With an interchange that is a little larger, of <span style="white-space:nowrap">order \(5\cdot 2^{2t}+1\),</span> the robber can win with \(r\ge 4\), by moving to a lane that will become part of two otherwise-disjoint junctions, so that two other equivalent lanes will be reachable by only one ramp.</p>

<p>This strategy shows that, if a graph class contains arbitrarily large interchanges, it does not have bounded flip-width. We will use this idea to show that many natural classes of geometric graphs do not have bounded flip-width.</p>

<h1 id="geometric-graphs">Geometric graphs</h1>

<p>In each of the following types of geometric graph, it is possible to form arbitrarily large interchanges, as illustrated.</p>

<ul>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Interval_graph">Interval graphs</a> and <a href="https://en.wikipedia.org/wiki/Permutation_graph">permutation graphs</a>. Just make a left-to-right sequence of small disjoint blue intervals for the lanes, and connect them by longer red intervals for the ramps. Each red interval contains all of the blue intervals that it intersects, and permutation graphs are the same thing as <a href="https://www.graphclasses.org/classes/gc_288.html">interval containment graphs</a>. In contrast, the <a href="https://en.wikipedia.org/wiki/Indifference_graph">unit interval graphs</a> are known to have bounded twin-width,<sup id="fnref:tw3" role="doc-noteref"><a href="#fn:tw3" class="footnote" rel="footnote">7</a></sup> from which it follows that they also have bounded flip-width.</p>

    <p style="text-align:center"><img src="/blog/assets/2023/interval-interchange.svg" alt="Interval graph forming an interchange of order 6" /></p>
  </li>
  <li>
    <p>Circle graphs. These have the permutation graphs as a special case, but there’s also a direct construction.</p>

    <p style="text-align:center"><img src="/blog/assets/2023/circle-interchange.svg" alt="Circle graph forming an interchange of order 6" style="width:100%;max-width:540px" /></p>
  </li>
  <li>
    <p>Intersection graphs of axis-aligned line segments, no two collinear. Use long horizontal segments for the lanes, ordered vertically, and span them by vertical ramps.</p>

    <p style="text-align:center"><img src="/blog/assets/2023/line-segment-interchange.svg" alt="Axis-aligned line segments forming an interchange of order 6" /></p>
  </li>
  <li>
    <p>Intersection graphs of axis-parallel unit squares. Place the blue lane squares with their top right corners on a diagonal line, close enough together that any consecutive interval of them can be covered by a red ramp square.</p>

    <p style="text-align:center"><img src="/blog/assets/2023/square-interchange.svg" alt="Squares forming an interchange of order 6" style="width:100%;max-width:540px" /></p>
  </li>
  <li>
    <p>Unit disk graphs. This one is unfortunately difficult to see clearly because the details are tiny with respect to the overall form, even for the \(n=6\) example shown. Anyway, place \(n\) blue unit disks tangent to the outside of a circle of radius \(1+\varepsilon\) (yellow in the figure), so that their points of tangencies span an arc of diameter less than \(2\). Then place red unit disks with their centers inside the yellow circle, so that their intersections with the circle form arcs that look like the interval graph model above. Because their radius is smaller than the yellow circle, the red disks will bulge out of the yellow circle a little bit. They intersect the blue points of tangency in the pattern that we want, but the parts that bulge out may have some unwanted contacts with the other blue disks. To prevent this, make \(\varepsilon\) very small. As you decrease \(\varepsilon\), the red bulges will shrink towards the yellow circle, but the blue disks won’t change their positions or angles very much, so for sufficiently small values of \(\varepsilon\) there will be no unwanted contacts.</p>

    <p style="text-align:center"><img src="/blog/assets/2023/disk-interchange.svg" alt="Unit disks forming an interchange of order 6" /></p>
  </li>
  <li>
    <p>Unit distance graphs. Place the blue vertices equally spaced along an interval of length less than two and the red vertices that connect them on the points where unit circles centered on the blue vertices cross each other.</p>

    <p style="text-align:center"><img src="/blog/assets/2023/unit-distance-interchange.svg" alt="Unit distance graph forming an interchange of order 6" /></p>
  </li>
  <li>
    <p>Visibility graphs of simple polygons. Place the blue vertices in sequence order on a horizontal line, the red vertices that connect pairs of consecutive blue vertices in order on a parallel line above them, and the remaining red vertices in an arbitrary order on a parallel line below them. Draw a triangle between each red vertex and the two blue vertices it should connect, and take the union of the triangles. Fill any holes that might have been formed in taking the union. The resulting polygon has additional vertices but that doesn’t affect the existence of an interchange. (This construction is simplified from an earlier construction by Rose, of visibility graphs that can be flipped to contain subdivisions of complete graphs.)</p>

    <p>Visibility graphs are <a href="https://en.wikipedia.org/wiki/Cop-win_graph">cop-win graphs</a>, meaning that a single cop wins a different cop-and-robber game in which both players can either move along a graph edge or stand still.<sup id="fnref:lsv" role="doc-noteref"><a href="#fn:lsv" class="footnote" rel="footnote">8</a></sup> But this doesn’t say anything about the flipping game: any graph can be made into a cop-win graph by adding a single <a href="https://en.wikipedia.org/wiki/Universal_vertex">universal vertex</a>, without changing whether it has bounded flip-width.</p>

    <p style="text-align:center"><img src="/blog/assets/2023/visibility-interchange.svg" alt="Polygon whose visibility graph forms an interchange of order 6" /></p>
  </li>
  <li>
    <p>Four-dimensional convex polytopes. I’m not even going to try to draw this one, but the construction is easy to describe in words. Just take the <a href="https://en.wikipedia.org/wiki/Barycentric_subdivision">barycentric subdivision</a> of a <a href="https://en.wikipedia.org/wiki/Neighborly_polytope">neighborly polytope</a>. Neighborly polytopes have edges and vertices forming complete graphs; the barycentric subdivision of any polytope is another polytope.<sup id="fnref:es" role="doc-noteref"><a href="#fn:es" class="footnote" rel="footnote">9</a></sup> It has a vertex for each face of the original polytope, and an edge for each incidence between faces of different dimensions. Arrange the vertices of the neighborly polytope into an arbitrary sequence as lanes; use the subdivision vertices coming from its edges as ramps. In this way the ramps will be connected only to their two associated lanes and to other subdivision points, but not to any other lanes.</p>
  </li>
</ul>

<h1 id="where-now">Where now?</h1>

<p>We’re still missing a natural class of graphs with bounded flip-width, unbounded twin-width, and unbounded expansion. The known classes of geometric graphs looked promising as a direction to look for such classes, but these constructions rule that out in surprisingly many cases.</p>

<p>It still might be possible that the number of cops needed to catch a robber on these graphs could be low. The interchange construction only proves that it is at least logarithmic. But I don’t know of any useful algorithmic consequences of having a low but unbounded number of cops needed to catch a bounded-speed robber.</p>

<h1 id="notes-and-references">Notes and references</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:r" role="doc-endnote">
      <p>Rose also helped me edit a preliminary version of this post. Thanks, Rose! Any remaining errors are my fault. <a href="#fnref:r" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:t" role="doc-endnote">
      <p>Szymon Toruńczyk (2023), “Flip-width: cops and robber on dense graphs”, <a href="https://arxiv.org/abs/2302.00352">arXiv:2302.00352</a> <a href="#fnref:t" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:t:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:dms" role="doc-endnote">
      <p>Jan Dreier, Nikolas Mählmann, and Sebastian Siebertz (2023), “First-order model checking on structurally sparse graph classes”, <a href="https://arxiv.org/abs/2302.03527">arXiv:2302.03527</a> <a href="#fnref:dms" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:no" role="doc-endnote">
      <p>Jaroslav Nešetřil and Patrice Ossona de Mendez (2012), “5.5: Classes with bounded expansion”, <em>Sparsity: Graphs, Structures, and Algorithms</em>, pp. 104–107, Springer, Algorithms and Combinatorics, vol. 28, <a href="https://doi.org/10.1007/978-3-642-27875-4">doi:10.1007/978-3-642-27875-4</a> <a href="#fnref:no" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:st" role="doc-endnote">
      <p>Pierre Simon and Szymon Toruńczyk (2021), “Ordered graphs of bounded twin-width”, <a href="https://arxiv.org/abs/2102.06881">arXiv:2102.06881</a> <a href="#fnref:st" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:t2" role="doc-endnote">
      <p>See Corollary 3.6 of Toruńczyk (2023)<sup id="fnref:t:2" role="doc-noteref"><a href="#fn:t" class="footnote" rel="footnote">2</a></sup> <a href="#fnref:t2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:tw3" role="doc-endnote">
      <p>See Lemma 13 of  Édouard Bonnet, Colin Geniet, Eun Jung Kim, Stéphan Thomassé, and Rémi Watrigant, “Twin-width III: Max Independent Set and Coloring”, <a href="https://arxiv.org/abs/2007.14161v2">arXiv:2007.14161v2</a> (this lemma is not in the <em>ICALP</em> 2021 version and numbered differently in other arXiv versions) <a href="#fnref:tw3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:lsv" role="doc-endnote">
      <p>Anna Lubiw, Jack Snoeyink, and Hamideh Vosoughpour (2017), “Visibility graphs, dismantlability, and the cops and robbers game”, <em>CGTA</em> 66: 14–27, <a href="https://arxiv.org/abs/1601.01298">arXiv:1601.01298</a> <a href="#fnref:lsv" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:es" role="doc-endnote">
      <p>Günter Ewald and Geoffrey C. Shephard (1974), “Stellar subdivisions of boundary complexes of convex polytopes”, <em>Math. Ann.</em> 210: 7–16, <a href="https://doi.org/10.1007/BF01344542">doi:10.1007/BF01344542</a>. <a href="#fnref:es" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/109901138706218444">Discuss on Mastodon</a>)</p><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-20T21:20:00Z">Monday, February 20 2023, 21:20</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2023/02/20/phd-position-at-chalmers-university-of-technology-apply-by-march-13-2023/'>PhD position at Chalmers University of Technology (apply by March 13, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The Ph.D. project will focus on developing quantum algorithms for near-term devices, exploring both their advantages and limitations compared to classical algorithms running on conventional computers. Website: www.chalmers.se/en/about-chalmers/work-with-us/vacancies/?rmpage=job&#38;rmjob=11382&#38;rmlang=UK Email: dubhashi@chalmers.se
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The Ph.D. project will focus on developing quantum algorithms for near-term devices, exploring both their advantages and limitations compared to classical algorithms running on conventional computers.</p>
<p>Website: <a href="https://www.chalmers.se/en/about-chalmers/work-with-us/vacancies/?rmpage=job&amp;rmjob=11382&amp;rmlang=UK">https://www.chalmers.se/en/about-chalmers/work-with-us/vacancies/?rmpage=job&amp;rmjob=11382&amp;rmlang=UK</a><br />
Email: dubhashi@chalmers.se</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-20T20:38:26Z">Monday, February 20 2023, 20:38</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2023/02/20/phd-student-at-technical-university-of-denmark-apply-by-march-9-2023/'>PhD Student at Technical University of Denmark (apply by March 9, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We will give new, efficient algorithms and data structures for dynamic graphs. The fun challenge in this field is to find the right partial answers to update as the graph changes, while letting the algorithm for queries do some of the work of putting the answer together. Often, the road to efficient algorithms goes via [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>We will give new, efficient algorithms and data structures for dynamic graphs. The fun challenge in this field is to find the right partial answers to update as the graph changes, while letting the algorithm for queries do some of the work of putting the answer together. Often, the road to efficient algorithms goes via graph theoretic insights.</p>
<p>Website: <a href="https://efzu.fa.em2.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX_1/job/1402/?keyword=dynamic&amp;mode=job-location">https://efzu.fa.em2.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX_1/job/1402/?keyword=dynamic&amp;mode=job-location</a><br />
Email: erot@dtu.dk</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-20T08:57:04Z">Monday, February 20 2023, 08:57</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/014'>TR23-014 |  Depth-3 Circuit Lower Bounds for k-OV | 

	Tameem  Choudhury, 

	Karteek Sreenivasaiah</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The 2-Orthogonal Vectors (2-OV) problem is the following: given two tuples $A$ and $B$ of $n$ vectors each of dimension $d$, decide if there exists a vector $u\in A$, and $v\in B$ such that $u$ and $v$ are orthogonal. This problem, and its generalization $k$-OV defined analogously for $k$ tuples, are central problems in the area of fine-grained complexity. Informally speaking, one of the major conjectures in fine-grained complexity is that deciding $k$-OV requires time $\Omega(n^k d)$. In this paper, we are interested in unconditional lower bounds against $k$-OV, but for weaker models of computation than the general Turing Machine. In particular, we are interested in circuit lower bounds to computing $k$-OV by Boolean circuit families of depth 3 of the form OR-AND-OR, or equivalently, a disjunction of CNFs. We show that for all $k\leq d$, any disjunction of $t$-CNFs computing $k$-OV requires size $\Omega((n/t)^k)$.  In particular, when $k$ is a constant, any disjunction of $k$-CNFs computing $k$-OV needs to use $\Omega(n^k)$ CNFs. This matches the brute-force construction. Thus for each fixed $k\ge 2$, the complexity of computing $k$-OV as a disjunction of $k$-CNFs is $\Theta(n^k)$. Our results partially resolve a conjecture by Kane and Williams [16] (page 12, conjecture 10) about depth-3 $AC^0$ circuits computing 2-OV. As a secondary result, we show an exponential lower bound on the size of AND-OR-AND circuits computing 2-OV when $d$ is very large. Since 2-OV reduces to $k$-OV by projections trivially, this lower bound works against $k$-OV as well.

Kane and Williams[16]: The orthogonal vectors conjecture for branching programs and formulas (ITCS 2019)
        
        </div>

        <div class='tr-article-summary'>
        
          
          The 2-Orthogonal Vectors (2-OV) problem is the following: given two tuples $A$ and $B$ of $n$ vectors each of dimension $d$, decide if there exists a vector $u\in A$, and $v\in B$ such that $u$ and $v$ are orthogonal. This problem, and its generalization $k$-OV defined analogously for $k$ tuples, are central problems in the area of fine-grained complexity. Informally speaking, one of the major conjectures in fine-grained complexity is that deciding $k$-OV requires time $\Omega(n^k d)$. In this paper, we are interested in unconditional lower bounds against $k$-OV, but for weaker models of computation than the general Turing Machine. In particular, we are interested in circuit lower bounds to computing $k$-OV by Boolean circuit families of depth 3 of the form OR-AND-OR, or equivalently, a disjunction of CNFs. We show that for all $k\leq d$, any disjunction of $t$-CNFs computing $k$-OV requires size $\Omega((n/t)^k)$.  In particular, when $k$ is a constant, any disjunction of $k$-CNFs computing $k$-OV needs to use $\Omega(n^k)$ CNFs. This matches the brute-force construction. Thus for each fixed $k\ge 2$, the complexity of computing $k$-OV as a disjunction of $k$-CNFs is $\Theta(n^k)$. Our results partially resolve a conjecture by Kane and Williams [16] (page 12, conjecture 10) about depth-3 $AC^0$ circuits computing 2-OV. As a secondary result, we show an exponential lower bound on the size of AND-OR-AND circuits computing 2-OV when $d$ is very large. Since 2-OV reduces to $k$-OV by projections trivially, this lower bound works against $k$-OV as well.

Kane and Williams[16]: The orthogonal vectors conjecture for branching programs and formulas (ITCS 2019)
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-20T06:52:41Z">Monday, February 20 2023, 06:52</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://blog.computationalcomplexity.org/2023/02/it-is-more-important-than-ever-to-teach.html'>It is  more important than ever to teach your students probability (even non-stem students)</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>(This topic was also covered&nbsp;here.)&nbsp;</p><p>You are a college president. An online betting company says&nbsp;&nbsp;We will give you X dollars if you allow us to promote online gambling at your University.</p><p>I suspect you would say NO.</p><p>Too late- it's already happening. A link to a NY times article about this is:&nbsp;here. I urge you to read the entire article. It's worse than it sounds.&nbsp;</p><p>My thoughts</p><p>0) I wondered if&nbsp; a company needed permission to promote a product on a campus. I am not sure of the answer; however, in some cases a school HELPED with the promotion:&nbsp;</p><p>a) During a game there are announcements reminding students that they can place a sports bet! It's easy! It's fun!</p><p>b) Links on the schools website to sports gambling sites</p><p>c) References to sports betting in emails that goto students.</p><p>This is WAY BEYOND&nbsp; allowing a company to promote.</p><p>1) Some points from the article&nbsp;</p><p>Some aspects of the deals also appear to violate the gambling industry's own rules against marketing to underage people. The ``Responsible Marketing Code'' published by the American Gaming Association, the umbrella group for the industry, says sports betting should not be advertised on college campuses.&nbsp;</p><p>``We are not seeing enough oversight, transparency, and education to support the rollout of these kinds of deals'' said Michael Goldman who teaches sports marketing at the Univ of San. Fran.&nbsp;</p><p>During the pandemic, many universities struggled financially ...To fill those holes public and private universities nationwide have been struggling to line up new revenue sources, including by arranging sponsorship deals. (MY THOUGHTS- They don't quite say it, but it seems like the extra money is going back to sports programs. I would be happier if it went into academics- and to be fair, maybe some of it does.)&nbsp;</p><p>2) Online gambling is more addictive than in-person gambling. And it's easier since you don't have to leave your dorm room to do it.&nbsp;</p><p>3) The school gets money and&nbsp; teaches the students that everything is for sale. So it's a win-win (I am kidding.)&nbsp;</p><p>4) Should a college take&nbsp; money to allow the promotion of tobacco or alcohol or (if it becomes legal) heroin? I see NO difference between those and online gambling. (See&nbsp;here)</p><p>5) I am in favor of all of those things being legal (maybe not heroin but I am open to debate on that)&nbsp; however, there is a big difference between making something legal, and promoting it.&nbsp;&nbsp;</p><p>6) Silver Lining: This may encourage more students, even non-STEM students, to learn probability. Either advertise it honestly:</p><p><br>Take Probability to find out that Sports Betting is a Loser's Game</p><p><br></p><p>Or advertise it dishonestly</p><p><br></p><p>Take Probability to find out how you can win at Sports Betting!</p><p><br></p><p>&nbsp;</p><p><br></p><p><br></p><p>By gasarch</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>(This topic was also covered&nbsp;<a href="https://statmodeling.stat.columbia.edu/2023/01/19/there-are-five-ways-to-get-fired-from-caesars-1-theft-2-sexual-harassment-3-running-an-experiment-without-a-control-group-4-keeping-a-gambling-addict-away-from-the-casino-5-refusing-to/">here</a>.)&nbsp;</p><p>You are a college president. An online betting company says&nbsp;&nbsp;<i>We will give you X dollars if you</i> <i>allow us to promote online gambling at your University.</i></p><p>I suspect you would say NO.</p><p>Too late- it's already happening. A link to a NY times article about this is:&nbsp;<a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/sportsbetting.pdf">here</a>. I urge you to read the entire article. It's worse than it sounds.&nbsp;</p><p>My thoughts</p><p>0) I wondered if&nbsp; a company needed permission to promote a product on a campus. I am not sure of the answer; however, in some cases a school HELPED with the promotion:&nbsp;</p><p>a) During a game there are announcements reminding students that they can place a sports bet! It's easy! It's fun!</p><p>b) Links on the schools website to sports gambling sites</p><p>c) References to sports betting in emails that goto students.</p><p>This is WAY BEYOND&nbsp; <i>allowing a company to promote.</i></p><p>1) Some points from the article&nbsp;</p><p>Some aspects of the deals also appear to violate the gambling industry's own rules against marketing to underage people. The ``Responsible Marketing Code'' published by the American Gaming Association, the umbrella group for the industry, says sports betting <i>should not be advertised on college campuses.</i>&nbsp;</p><p>``We are not seeing enough oversight, transparency, and education to support the rollout of these kinds of deals'' said Michael Goldman who teaches sports marketing at the Univ of San. Fran.&nbsp;</p><p>During the pandemic, many universities struggled financially ...To fill those holes public and private universities nationwide have been struggling to line up new revenue sources, including by arranging sponsorship deals. (MY THOUGHTS- They don't quite say it, but it seems like the extra money is going back to sports programs. I would be happier if it went into academics- and to be fair, maybe some of it does.)&nbsp;</p><p>2) Online gambling is more addictive than in-person gambling. And it's easier since you don't have to leave your dorm room to do it.&nbsp;</p><p>3) The school gets money and&nbsp; teaches the students that everything is for sale. So it's a win-win (I am kidding.)&nbsp;</p><p>4) Should a college take&nbsp; money to allow the promotion of tobacco or alcohol or (if it becomes legal) heroin? I see NO difference between those and online gambling. (See&nbsp;<a href="https://www.caron.org/blog/fortnite-may-be-as-addictive-as-heroin">here</a>)</p><p>5) I am in favor of all of those things being legal (maybe not heroin but I am open to debate on that)&nbsp; however, there is a big difference between making something legal, and promoting it.&nbsp;&nbsp;</p><p>6) Silver Lining: This may encourage more students, even non-STEM students, to learn probability. Either advertise it honestly:</p><p><br />Take Probability to find out that Sports Betting is a Loser's Game</p><p><br /></p><p>Or advertise it dishonestly</p><p><br /></p><p>Take Probability to find out how you can win at Sports Betting!</p><p><br /></p><p>&nbsp;</p><p><br /></p><p><br /></p><p class="authors">By gasarch</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-02-20T02:38:00Z">Monday, February 20 2023, 02:38</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
