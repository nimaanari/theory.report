<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.5 (2022-11-24) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2023-03-30T14:30:30Z">Thursday, March 30 2023, 14:30</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://minorfree.github.io/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://minorfree.github.io'>Hung Le</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>Sébastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Thursday, March 30
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://windowsontheory.org/2023/03/30/tcs-for-all-travel-grants-and-speaker-nominations-guest-post-by-elena-grigorescu/'>TCS for all travel grants and speaker nominations (guest post by Elena Grigorescu)</a></h3>
        <p class='tr-article-feed'>from <a href='https://windowsontheory.org'>Windows on Theory</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          TCS for All (previously TCS Women) Spotlight Workshop at STOC 2023/Theory Fest: Travel grants and call for speaker nominations You are cordially invited to our TCS for All Spotlight Workshop! The workshop will be held&#160;on Thursday, June 22nd, 2023 (2-4pm), in Orlando, Florida, USA, as part of the&#160;54th&#160;Symposium&#160;on Theory of Computing (STOC)&#160;and TheoryFest! The workshop &#8230; Continue reading TCS for all travel grants and speaker nominations (guest post by Elena&#160;Grigorescu)
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p><strong>TCS for All (previously TCS Women) Spotlight Workshop at STOC 2023/Theory Fest: Travel grants and call for speaker nominations</strong></p>



<p>You are cordially invited to our TCS for All Spotlight Workshop! The workshop will be held&nbsp;on Thursday, June 22nd, 2023 (2-4pm), in Orlando, Florida, USA, as part of the&nbsp;<a href="http://acm-stoc.org/stoc2022/" target="_blank" rel="noreferrer noopener">54th&nbsp;</a><a href="http://acm-stoc.org/stoc2023/" target="_blank" rel="noreferrer noopener">Symposium&nbsp;</a><a href="http://acm-stoc.org/stoc2022/" target="_blank" rel="noreferrer noopener">on Theory of Computing (STOC)</a>&nbsp;and TheoryFest! The workshop is open to all.</p>



<p>We are happy to announce that our annual inspirational talk will be given by Professor Dana Randall!</p>



<p>More information about the workshop is available here:&nbsp;<a href="https://sigact.org/tcsforall/" target="_blank" rel="noreferrer noopener">https://sigact.org/tcsforall/</a>. In particular, we would like to highlight the TCS for All Travel Scholarships (deadline&nbsp;May 7th) and a call for nominations for Rising Stars talks at the workshop (deadline&nbsp;May 7th).&nbsp; More information on those are below.</p>



<p>Hope to see you in Orlando!</p>



<p><strong><em>TCS for All Travel Scholarship:</em></strong></p>



<p>TCS for All Travel Scholarships are intended for researchers at the beginning of their career. This scholarship is being made available for minorities in TCS, and anyone who identifies as such is welcome to apply; this scholarship is open to both US and international students. Preference will be given to students at the beginning of their studies. If we have sufficient funding, we will give awards to more senior students and possibly even postdocs.</p>



<p>To apply, you will need to fill out the following form by&nbsp;<strong>May 7th, 2023</strong>&nbsp;(11:59 pm PDT) in which you provide basic information about yourself, an estimate of your expenses, and a brief statement:</p>



<p><a href="https://forms.gle/btayQhoATxEkGoV18" target="_blank" rel="noreferrer noopener">Apply for a travel grant here.</a></p>



<p>In addition, you will need to have your advisor (or department head or other faculty mentor if you do not yet have an advisor) send a letter of support to&nbsp;<a href="mailto:tcswomen@gmail.com" target="_blank" rel="noreferrer noopener">tcswomen@gmail.com</a>&nbsp;by&nbsp;May 7th. Your advisor’s letter should also describe the availability of other travel funds.&nbsp; Note for advisors: Specifics about alternative funding are very helpful.&nbsp; Statements like “funding is tight” are not very helpful. This letter should be sent with subject line “support letter for [your name]”. This is very important. Your application is not complete without this letter.</p>



<p>Late applications (after&nbsp;May 7th) will not be accepted. You will be notified about your status by&nbsp;May 15th, which is prior to the STOC early registration deadline and hotel cut-off deadline.</p>



<p>Notes: Receipts will be required for all travel awards, and reimbursements will be made after the conference. Food or visa expenses will not be reimbursed.</p>



<p><strong><em>Nominations for Rising Star talks:</em></strong></p>



<p>We invite nomination for speakers in our Rising Star talks at the TCS for All Spotlight Workshop at STOC 2023. To be eligible, your nominee has to be a senior PhD student with expected graduation no later than August 2024,&nbsp;or&nbsp;a postdoc in theoretical computer science (all topics represented at STOC are welcome), an underrepresented minority, and not a speaker at a previous TCS Women Spotlight Workshop.&nbsp; Preference will be given to speakers who are currently on the job market for postdoctoral/faculty positions, or who expect to be on the job market in Fall 2023.</p>



<p>You can make your nomination by filling this form by&nbsp;<strong>May 7th</strong>:&nbsp;<a href="https://forms.gle/jCMXsTmZ4DZ8r5xJA" target="_blank" rel="noreferrer noopener">https://forms.gle/jCMXsTmZ4DZ8r5xJA</a></p>
<p class="authors">By Boaz Barak</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-30T13:10:38Z">Thursday, March 30 2023, 13:10</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=7174'>If AI scaling is to be shut down, let it be for a coherent reason</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          There&#8217;s now an open letter arguing that the world should impose a six-month moratorium on the further scaling of AI models such as GPT, by government fiat if necessary, to give AI safety and interpretability research a bit more time to catch up. The letter is signed by many of my friends and colleagues, many [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>There&#8217;s now an <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">open letter</a> arguing that the world should impose a six-month moratorium on the further scaling of AI models such as GPT, by government fiat if necessary, to give AI safety and interpretability research a bit more time to catch up.  The letter is signed by many of my friends and colleagues, many who probably agree with each other about little else, over a thousand people including Elon Musk, Steve Wozniak, Andrew Yang, Jaan Tallinn, Stuart Russell, Max Tegmark, Yuval Noah Harari, Ernie Davis, Gary Marcus, and Yoshua Bengio. </p>



<p>Meanwhile, Eliezer Yudkowsky <a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/">published a piece in <em>TIME</em></a> arguing that the open letter doesn&#8217;t go nearly far enough, and that AI scaling needs to be shut down entirely until the AI alignment problem is solved&#8212;with the shutdown enforced by military strikes on GPU farms if needed, and treated as more important than preventing nuclear war.</p>



<p>Readers, as they do, asked me to respond.  Alright, alright.  While the open letter is presumably targeted at OpenAI more than any other entity, and while I’ve been <a href="https://scottaaronson.blog/?p=6484">spending the year at OpenAI</a> to work on theoretical foundations of AI safety, I’m going to answer strictly for myself.</p>



<p>Given the jaw-droppingly spectacular abilities of GPT-4&#8212;e.g., <a href="https://www.semafor.com/article/03/15/2023/how-gpt-4-performed-in-academic-exams">acing</a> the Advanced Placement biology and macroeconomics exams, correctly <a href="https://arxiv.org/abs/2303.12712">manipulating images</a> (via their source code) without having been programmed for anything of the kind, etc. etc.&#8212;the idea that AI now needs to be treated with extreme caution strikes me as far from absurd.  I don&#8217;t even dismiss the possibility that advanced AI could eventually require the same sorts of safeguards as nuclear weapons.</p>



<p>Furthermore, people might be surprised about the diversity of opinion about these issues <em>within</em> OpenAI, by how many there have discussed or even forcefully advocated slowing down.  And there&#8217;s a world not so far from this one where I, too, get behind a pause.  For example, one <em>actual</em> major human tragedy caused by a generative AI model might suffice to push me over the edge.  (What would push <em>you</em> over the edge, if you&#8217;re not already over?)</p>



<p>Before I join the slowdown brigade, though, I have (this being the week before Passover) four questions for the signatories:</p>



<ol>
<li>Would your rationale for this pause have applied to basically <em>any</em> nascent technology — the printing press, radio, airplanes, the Internet?  “We don’t yet know the implications, but there’s an excellent chance terrible people will misuse this, ergo the only responsible choice is to pause until we&#8217;re confident that they won&#8217;t”?</li>



<li>Why six months?  Why not six weeks or six years?</li>



<li>When, by your lights, would we ever know that it was safe to <em>resume</em> scaling AI&#8212;or at least that the risks of pausing exceeded the risks of scaling?  Why won&#8217;t the <a href="https://en.wikipedia.org/wiki/Precautionary_principle">precautionary principle</a> continue for apply forever?</li>



<li>Were you, until approximately last week, ridiculing GPT as unimpressive, a stochastic parrot, lacking common sense, piffle, a scam, etc. — before turning around and declaring that it could be existentially dangerous? How can you have it both ways?  If the problem, in your view, is that GPT-4 is too stupid, then shouldn&#8217;t GPT-5 be smarter <em>and therefore</em> <em>safer</em>?  Thus, shouldn&#8217;t we keep scaling AI as quickly as we can &#8230; <em>for safety reasons</em>?  If, on the other hand, the problem is that GPT-4 is too smart, then why can’t you bring yourself to say so?</li>
</ol>



<p>With the “why six months?” question, I confess that I was deeply confused, until I heard a dear friend and colleague in academic AI, one who&#8217;s long been skeptical of AI-doom scenarios, explain why <em>he</em> signed the open letter.  He said: look, we all started writing research papers about the safety issues with ChatGPT; then our work became obsolete when OpenAI released GPT-4 just a few months later.  So now we&#8217;re writing papers about GPT-4.  Will we <em>again</em> have to throw our work away when OpenAI releases GPT-5?  I realized that, while six months might not suffice to save human civilization, it&#8217;s just enough for the more immediate concern of getting papers into academic AI conferences.</p>



<p>Look: while I&#8217;ve spent <a href="https://scottaaronson.blog/?p=6821">multiple</a> <a href="https://scottaaronson.blog/?p=6823">posts</a> explaining how I part ways from the Orthodox Yudkowskyan position, I do find that position intellectually consistent, with conclusions that follow neatly from premises.  The Orthodox, in particular, can straightforwardly answer all four of my questions above:</p>



<ol>
<li>AI is manifestly different from any other technology humans have ever created, because it could become to us as we are to orangutans;</li>



<li>a six-month pause is very far from sufficient but is better than no pause;</li>



<li>we&#8217;ll know that it&#8217;s safe to scale when (and only when) we understand our AIs so deeply that we can <em>mathematically explain</em> why they won&#8217;t do anything bad; and</li>



<li>GPT-4 is <em>extremely</em> impressive&#8212;that&#8217;s why it&#8217;s so terrifying!</li>
</ol>



<p>On the other hand, I&#8217;m deeply confused by the people who signed the open letter, <em>even though they continue to downplay or even ridicule GPT&#8217;s abilities, as well as the &#8220;sensationalist&#8221; predictions of an AI apocalypse.</em>  I&#8217;d feel less confused if such people came out and argued explicitly: &#8220;yes, we should also have paused the rapid improvement of printing presses to avert Europe&#8217;s religious wars.  Yes, we should&#8217;ve paused the scaling of radio transmitters to prevent the rise of Hitler.  Yes, we should&#8217;ve paused the race for ever-faster home Internet to prevent the election of Donald Trump.  And yes, we should&#8217;ve trusted our governments to manage these pauses, to foresee brand-new technologies&#8217; likely harms and take appropriate actions to mitigate them.&#8221;</p>



<p>Absent such an argument, I come back to the question of whether generative AI <em>actually</em> poses a near-term risk that&#8217;s totally unparalleled in human history, or perhaps approximated only by the risk of nuclear weapons.  After sharing an email from his partner, Eliezer rather movingly writes:</p>



<blockquote class="wp-block-quote">
<p>When the insider conversation is about the grief of seeing your daughter lose her first tooth, and thinking she’s not going to get a chance to grow up, I believe we are past the point of playing political chess about a six-month moratorium.</p>
</blockquote>



<p>Look, I too have a 10-year-old daughter and a 6-year-old son, and I wish to see them grow up.  But the causal story that starts with a GPT-5 or GPT-4.5 training run, and ends with the sudden death of my children and of all carbon-based life, still has a few too many gaps for my aging, inadequate brain to fill in.  I can complete the story in my imagination, of course, but I could equally complete a story that starts with GPT-5 and ends with the world saved from various natural stupidities.  For better or worse, I lack the &#8220;Bayescraft&#8221; to see why the first story is <em>obviously</em> 1000x or 1,000,000x likelier than the second one.</p>



<p>But, I dunno, maybe I&#8217;m making the greatest mistake of my life?  Feel free to try convincing me that I should sign the letter.  But let&#8217;s see how polite and charitable everyone can be: hopefully a six-month moratorium won&#8217;t be needed to solve the alignment problem of the <em>Shtetl-Optimized</em> comment section.</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-30T06:23:41Z">Thursday, March 30 2023, 06:23</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.16705'>Planar 3-way Edge Perfect Matching Leads to A Holant Dichotomy</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jin-Yi Cai, Austen Z. Fan</p><p>We prove a complexity dichotomy theorem for a class of Holant problems on
planar 3-regular bipartite graphs. The complexity dichotomy states that for
every weighted constraint function $f$ defining the problem (the weights can
even be negative), the problem is either computable in polynomial time if $f$
satisfies a tractability criterion, or \#P-hard otherwise. One particular
problem in this problem space is a long-standing open problem of Moore and
Robson on counting Cubic Planar X3C. The dichotomy resolves this problem by
showing that it is \numP-hard. Our proof relies on the machinery of signature
theory developed in the study of Holant problems. An essential ingredient in
our proof of the main dichotomy theorem is a pure graph-theoretic result:
Excepting some trivial cases, every 3-regular plane graph has a planar 3-way
edge perfect matching. The proof technique of this graph-theoretic result is a
combination of algebraic and combinatorial methods.
</p>
<p>The P-time tractability criterion of the dichotomy is explicit. Other than
the known classes of tractable constraint functions (degenerate, affine,
product type, matchgates-transformable) we also identify a new infinite set of
P-time computable planar Holant problems; however, its tractability is not by a
direct holographic transformation to matchgates, but by a combination of this
method and a global argument. The complexity dichotomy states that everything
else in this Holant class is \#P-hard.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1">Jin-Yi Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1">Austen Z. Fan</a></p><p>We prove a complexity dichotomy theorem for a class of Holant problems on
planar 3-regular bipartite graphs. The complexity dichotomy states that for
every weighted constraint function $f$ defining the problem (the weights can
even be negative), the problem is either computable in polynomial time if $f$
satisfies a tractability criterion, or \#P-hard otherwise. One particular
problem in this problem space is a long-standing open problem of Moore and
Robson on counting Cubic Planar X3C. The dichotomy resolves this problem by
showing that it is \numP-hard. Our proof relies on the machinery of signature
theory developed in the study of Holant problems. An essential ingredient in
our proof of the main dichotomy theorem is a pure graph-theoretic result:
Excepting some trivial cases, every 3-regular plane graph has a planar 3-way
edge perfect matching. The proof technique of this graph-theoretic result is a
combination of algebraic and combinatorial methods.
</p>
<p>The P-time tractability criterion of the dichotomy is explicit. Other than
the known classes of tractable constraint functions (degenerate, affine,
product type, matchgates-transformable) we also identify a new infinite set of
P-time computable planar Holant problems; however, its tractability is not by a
direct holographic transformation to matchgates, but by a combination of this
method and a global argument. The complexity dichotomy states that everything
else in this Holant class is \#P-hard.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-30T00:30:00Z">Thursday, March 30 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.16303'>Constant-Hop Spanners for More Geometric Intersection Graphs, with Even Smaller Size</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Timothy M. Chan, Zhengcheng Huang</p><p>In SoCG 2022, Conroy and T\'oth presented several constructions of sparse,
low-hop spanners in geometric intersection graphs, including an $O(n\log
n)$-size 3-hop spanner for $n$ disks (or fat convex objects) in the plane, and
an $O(n\log^2 n)$-size 3-hop spanner for $n$ axis-aligned rectangles in the
plane. Their work left open two major questions: (i) can the size be made
closer to linear by allowing larger constant stretch? and (ii) can near-linear
size be achieved for more general classes of intersection graphs?
</p>
<p>We address both questions simultaneously, by presenting new constructions of
constant-hop spanners that have almost linear size and that hold for a much
larger class of intersection graphs. More precisely, we prove the existence of
an $O(1)$-hop spanner for arbitrary string graphs with $O(n\alpha_k(n))$ size
for any constant $k$, where $\alpha_k(n)$ denotes the $k$-th function in the
inverse Ackermann hierarchy. We similarly prove the existence of an $O(1)$-hop
spanner for intersection graphs of $d$-dimensional fat objects with
$O(n\alpha_k(n))$ size for any constant $k$ and $d$.
</p>
<p>We also improve on some of Conroy and T\'oth's specific previous results, in
either the number of hops or the size: we describe an $O(n\log n)$-size 2-hop
spanner for disks (or more generally objects with linear union complexity) in
the plane, and an $O(n\log n)$-size 3-hop spanner for axis-aligned rectangles
in the plane.
</p>
<p>Our proofs are all simple, using separator theorems, recursion, shifted
quadtrees, and shallow cuttings.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chan_T/0/1/0/all/0/1">Timothy M. Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhengcheng Huang</a></p><p>In SoCG 2022, Conroy and T\'oth presented several constructions of sparse,
low-hop spanners in geometric intersection graphs, including an $O(n\log
n)$-size 3-hop spanner for $n$ disks (or fat convex objects) in the plane, and
an $O(n\log^2 n)$-size 3-hop spanner for $n$ axis-aligned rectangles in the
plane. Their work left open two major questions: (i) can the size be made
closer to linear by allowing larger constant stretch? and (ii) can near-linear
size be achieved for more general classes of intersection graphs?
</p>
<p>We address both questions simultaneously, by presenting new constructions of
constant-hop spanners that have almost linear size and that hold for a much
larger class of intersection graphs. More precisely, we prove the existence of
an $O(1)$-hop spanner for arbitrary string graphs with $O(n\alpha_k(n))$ size
for any constant $k$, where $\alpha_k(n)$ denotes the $k$-th function in the
inverse Ackermann hierarchy. We similarly prove the existence of an $O(1)$-hop
spanner for intersection graphs of $d$-dimensional fat objects with
$O(n\alpha_k(n))$ size for any constant $k$ and $d$.
</p>
<p>We also improve on some of Conroy and T\'oth's specific previous results, in
either the number of hops or the size: we describe an $O(n\log n)$-size 2-hop
spanner for disks (or more generally objects with linear union complexity) in
the plane, and an $O(n\log n)$-size 3-hop spanner for axis-aligned rectangles
in the plane.
</p>
<p>Our proofs are all simple, using separator theorems, recursion, shifted
quadtrees, and shallow cuttings.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-30T00:30:00Z">Thursday, March 30 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.16716'>Topological Point Cloud Clustering</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Vincent P. Grande, Michael T. Schaub</p><p>We present Topological Point Cloud Clustering (TPCC), a new method to cluster
points in an arbitrary point cloud based on their contribution to global
topological features. TPCC synthesizes desirable features from spectral
clustering and topological data analysis and is based on considering the
spectral properties of a simplicial complex associated to the considered point
cloud. As it is based on considering sparse eigenvector computations, TPCC is
similarly easy to interpret and implement as spectral clustering. However, by
focusing not just on a single matrix associated to a graph created from the
point cloud data, but on a whole set of Hodge-Laplacians associated to an
appropriately constructed simplicial complex, we can leverage a far richer set
of topological features to characterize the data points within the point cloud
and benefit from the relative robustness of topological techniques against
noise. We test the performance of TPCC on both synthetic and real-world data
and compare it with classical spectral clustering.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Grande_V/0/1/0/all/0/1">Vincent P. Grande</a>, <a href="http://arxiv.org/find/math/1/au:+Schaub_M/0/1/0/all/0/1">Michael T. Schaub</a></p><p>We present Topological Point Cloud Clustering (TPCC), a new method to cluster
points in an arbitrary point cloud based on their contribution to global
topological features. TPCC synthesizes desirable features from spectral
clustering and topological data analysis and is based on considering the
spectral properties of a simplicial complex associated to the considered point
cloud. As it is based on considering sparse eigenvector computations, TPCC is
similarly easy to interpret and implement as spectral clustering. However, by
focusing not just on a single matrix associated to a graph created from the
point cloud data, but on a whole set of Hodge-Laplacians associated to an
appropriately constructed simplicial complex, we can leverage a far richer set
of topological features to characterize the data points within the point cloud
and benefit from the relative robustness of topological techniques against
noise. We test the performance of TPCC on both synthetic and real-world data
and compare it with classical spectral clustering.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-30T00:30:00Z">Thursday, March 30 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.16208'>Lifting uniform learners via distributional decomposition</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Guy Blanc, Jane Lange, Ali Malik, Li-Yang Tan</p><p>We show how any PAC learning algorithm that works under the uniform
distribution can be transformed, in a blackbox fashion, into one that works
under an arbitrary and unknown distribution $\mathcal{D}$. The efficiency of
our transformation scales with the inherent complexity of $\mathcal{D}$,
running in $\mathrm{poly}(n, (md)^d)$ time for distributions over $\{\pm 1\}^n$
whose pmfs are computed by depth-$d$ decision trees, where $m$ is the sample
complexity of the original algorithm. For monotone distributions our
transformation uses only samples from $\mathcal{D}$, and for general ones it
uses subcube conditioning samples.
</p>
<p>A key technical ingredient is an algorithm which, given the aforementioned
access to $\mathcal{D}$, produces an optimal decision tree decomposition of
$\mathcal{D}$: an approximation of $\mathcal{D}$ as a mixture of uniform
distributions over disjoint subcubes. With this decomposition in hand, we run
the uniform-distribution learner on each subcube and combine the hypotheses
using the decision tree. This algorithmic decomposition lemma also yields new
algorithms for learning decision tree distributions with runtimes that
exponentially improve on the prior state of the art -- results of independent
interest in distribution learning.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/stat/1/au:+Blanc_G/0/1/0/all/0/1">Guy Blanc</a>, <a href="http://arxiv.org/find/stat/1/au:+Lange_J/0/1/0/all/0/1">Jane Lange</a>, <a href="http://arxiv.org/find/stat/1/au:+Malik_A/0/1/0/all/0/1">Ali Malik</a>, <a href="http://arxiv.org/find/stat/1/au:+Tan_L/0/1/0/all/0/1">Li-Yang Tan</a></p><p>We show how any PAC learning algorithm that works under the uniform
distribution can be transformed, in a blackbox fashion, into one that works
under an arbitrary and unknown distribution $\mathcal{D}$. The efficiency of
our transformation scales with the inherent complexity of $\mathcal{D}$,
running in $\mathrm{poly}(n, (md)^d)$ time for distributions over $\{\pm 1\}^n$
whose pmfs are computed by depth-$d$ decision trees, where $m$ is the sample
complexity of the original algorithm. For monotone distributions our
transformation uses only samples from $\mathcal{D}$, and for general ones it
uses subcube conditioning samples.
</p>
<p>A key technical ingredient is an algorithm which, given the aforementioned
access to $\mathcal{D}$, produces an optimal decision tree decomposition of
$\mathcal{D}$: an approximation of $\mathcal{D}$ as a mixture of uniform
distributions over disjoint subcubes. With this decomposition in hand, we run
the uniform-distribution learner on each subcube and combine the hypotheses
using the decision tree. This algorithmic decomposition lemma also yields new
algorithms for learning decision tree distributions with runtimes that
exponentially improve on the prior state of the art -- results of independent
interest in distribution learning.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-30T00:30:00Z">Thursday, March 30 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.16413'>Certified Hardness vs. Randomness for Log-Space</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Edward Pyne, Ran Raz, Wei Zhan</p><p>Let $\mathcal{L}$ be a language that can be decided in linear space and let
$\epsilon &gt;0$ be any constant. Let $\mathcal{A}$ be the exponential hardness
assumption that for every $n$, membership in $\mathcal{L}$ for inputs of
length~$n$ cannot be decided by circuits of size smaller than $2^{\epsilon n}$.
We prove that for every function $f :\{0,1\}^* \rightarrow \{0,1\}$, computable
by a randomized logspace algorithm $R$, there exists a deterministic logspace
algorithm $D$ (attempting to compute $f$), such that on every input $x$ of
length $n$, the algorithm $D$ outputs one of the following:
</p>
<p>1: The correct value $f(x)$.
</p>
<p>2: The string: ``I am unable to compute $f(x)$ because the hardness
assumption $\mathcal{A}$ is false'', followed by a (provenly correct) circuit
of size smaller than $2^{\epsilon n'}$ for membership in $\mathcal{L}$ for
inputs of length~$n'$, for some $n' = \Theta (\log n)$; that is, a circuit that
refutes $\mathcal{A}$.
</p>
<p>Our next result is a universal derandomizer for $BPL$: We give a
deterministic algorithm $U$ that takes as an input a randomized logspace
algorithm $R$ and an input $x$ and simulates the computation of $R$ on $x$,
deteriministically. Under the widely believed assumption $BPL=L$, the space
used by $U$ is at most $C_R \cdot \log n$ (where $C_R$ is a constant depending
on~$R$). Moreover, for every constant $c \geq 1$, if $BPL\subseteq
SPACE[(\log(n))^{c}]$ then the space used by $U$ is at most $C_R \cdot
(\log(n))^{c}$.
</p>
<p>Finally, we prove that if optimal hitting sets for ordered branching programs
exist then there is a deterministic logspace algorithm that, given a black-box
access to an ordered branching program $B$ of size $n$, estimates the
probability that $B$ accepts on a uniformly random input. This extends the
result of (Cheng and Hoza CCC 2020), who proved that an optimal hitting set
implies a white-box two-sided derandomization.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Pyne_E/0/1/0/all/0/1">Edward Pyne</a>, <a href="http://arxiv.org/find/cs/1/au:+Raz_R/0/1/0/all/0/1">Ran Raz</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1">Wei Zhan</a></p><p>Let $\mathcal{L}$ be a language that can be decided in linear space and let
$\epsilon &gt;0$ be any constant. Let $\mathcal{A}$ be the exponential hardness
assumption that for every $n$, membership in $\mathcal{L}$ for inputs of
length~$n$ cannot be decided by circuits of size smaller than $2^{\epsilon n}$.
We prove that for every function $f :\{0,1\}^* \rightarrow \{0,1\}$, computable
by a randomized logspace algorithm $R$, there exists a deterministic logspace
algorithm $D$ (attempting to compute $f$), such that on every input $x$ of
length $n$, the algorithm $D$ outputs one of the following:
</p>
<p>1: The correct value $f(x)$.
</p>
<p>2: The string: ``I am unable to compute $f(x)$ because the hardness
assumption $\mathcal{A}$ is false'', followed by a (provenly correct) circuit
of size smaller than $2^{\epsilon n'}$ for membership in $\mathcal{L}$ for
inputs of length~$n'$, for some $n' = \Theta (\log n)$; that is, a circuit that
refutes $\mathcal{A}$.
</p>
<p>Our next result is a universal derandomizer for $BPL$: We give a
deterministic algorithm $U$ that takes as an input a randomized logspace
algorithm $R$ and an input $x$ and simulates the computation of $R$ on $x$,
deteriministically. Under the widely believed assumption $BPL=L$, the space
used by $U$ is at most $C_R \cdot \log n$ (where $C_R$ is a constant depending
on~$R$). Moreover, for every constant $c \geq 1$, if $BPL\subseteq
SPACE[(\log(n))^{c}]$ then the space used by $U$ is at most $C_R \cdot
(\log(n))^{c}$.
</p>
<p>Finally, we prove that if optimal hitting sets for ordered branching programs
exist then there is a deterministic logspace algorithm that, given a black-box
access to an ordered branching program $B$ of size $n$, estimates the
probability that $B$ accepts on a uniformly random input. This extends the
result of (Cheng and Hoza CCC 2020), who proved that an optimal hitting set
implies a white-box two-sided derandomization.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-30T00:30:00Z">Thursday, March 30 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.16287'>Lower Bounds for Pseudo-Deterministic Counting in a Stream</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Vladimir Braverman, Robert Krauthgamer, Aditya Krishnan, Shay Sapir</p><p>Many streaming algorithms provide only a high-probability relative
approximation. These two relaxations, of allowing approximation and
randomization, seem necessary -- for many streaming problems, both relaxations
must be employed simultaneously, to avoid an exponentially larger (and often
trivial) space complexity. A common drawback of these randomized approximate
algorithms is that independent executions on the same input have different
outputs, that depend on their random coins. Pseudo-deterministic algorithms
combat this issue, and for every input, they output with high probability the
same ``canonical'' solution.
</p>
<p>We consider perhaps the most basic problem in data streams, of counting the
number of items in a stream of length at most $n$. Morris's counter [CACM,
1978] is a randomized approximation algorithm for this problem that uses
$O(\log\log n)$ bits of space, for every fixed approximation factor (greater
than $1$). Goldwasser, Grossman, Mohanty and Woodruff [ITCS 2020] asked whether
pseudo-deterministic approximation algorithms can match this space complexity.
Our main result answers their question negatively, and shows that such
algorithms must use $\Omega(\sqrt{\log n / \log\log n})$ bits of space.
</p>
<p>Our approach is based on a problem that we call Shift Finding, and may be of
independent interest. In this problem, one has query access to a shifted
version of a known string $F\in\{0,1\}^{3n}$, which is guaranteed to start with
$n$ zeros and end with $n$ ones, and the goal is to find the unknown shift
using a small number of queries. We provide for this problem an algorithm that
uses $O(\sqrt{n})$ queries. It remains open whether $poly(\log n)$ queries
suffice; if true, then our techniques immediately imply a nearly-tight
$\Omega(\log n/\log\log n)$ space bound for pseudo-deterministic approximate
counting.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1">Vladimir Braverman</a>, <a href="http://arxiv.org/find/cs/1/au:+Krauthgamer_R/0/1/0/all/0/1">Robert Krauthgamer</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnan_A/0/1/0/all/0/1">Aditya Krishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sapir_S/0/1/0/all/0/1">Shay Sapir</a></p><p>Many streaming algorithms provide only a high-probability relative
approximation. These two relaxations, of allowing approximation and
randomization, seem necessary -- for many streaming problems, both relaxations
must be employed simultaneously, to avoid an exponentially larger (and often
trivial) space complexity. A common drawback of these randomized approximate
algorithms is that independent executions on the same input have different
outputs, that depend on their random coins. Pseudo-deterministic algorithms
combat this issue, and for every input, they output with high probability the
same ``canonical'' solution.
</p>
<p>We consider perhaps the most basic problem in data streams, of counting the
number of items in a stream of length at most $n$. Morris's counter [CACM,
1978] is a randomized approximation algorithm for this problem that uses
$O(\log\log n)$ bits of space, for every fixed approximation factor (greater
than $1$). Goldwasser, Grossman, Mohanty and Woodruff [ITCS 2020] asked whether
pseudo-deterministic approximation algorithms can match this space complexity.
Our main result answers their question negatively, and shows that such
algorithms must use $\Omega(\sqrt{\log n / \log\log n})$ bits of space.
</p>
<p>Our approach is based on a problem that we call Shift Finding, and may be of
independent interest. In this problem, one has query access to a shifted
version of a known string $F\in\{0,1\}^{3n}$, which is guaranteed to start with
$n$ zeros and end with $n$ ones, and the goal is to find the unknown shift
using a small number of queries. We provide for this problem an algorithm that
uses $O(\sqrt{n})$ queries. It remains open whether $poly(\log n)$ queries
suffice; if true, then our techniques immediately imply a nearly-tight
$\Omega(\log n/\log\log n)$ space bound for pseudo-deterministic approximate
counting.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-30T00:30:00Z">Thursday, March 30 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.16388'>Complexity of Equilibria in First-Price Auctions under General Tie-Breaking Rules</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xi Chen, Binghui Peng</p><p>We study the complexity of finding an approximate (pure) Bayesian Nash
equilibrium in a first-price auction with common priors when the tie-breaking
rule is part of the input. We show that the problem is PPAD-complete even when
the tie-breaking rule is trilateral (i.e., it specifies item allocations when
no more than three bidders are in tie, and adopts the uniform tie-breaking rule
otherwise). This is the first hardness result for equilibrium computation in
first-price auctions with common priors. On the positive side, we give a PTAS
for the problem under the uniform tie-breaking rule.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Binghui Peng</a></p><p>We study the complexity of finding an approximate (pure) Bayesian Nash
equilibrium in a first-price auction with common priors when the tie-breaking
rule is part of the input. We show that the problem is PPAD-complete even when
the tie-breaking rule is trilateral (i.e., it specifies item allocations when
no more than three bidders are in tie, and adopts the uniform tie-breaking rule
otherwise). This is the first hardness result for equilibrium computation in
first-price auctions with common priors. On the positive side, we give a PTAS
for the problem under the uniform tie-breaking rule.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-30T00:30:00Z">Thursday, March 30 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Wednesday, March 29
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://blog.computationalcomplexity.org/2023/03/alan-turing-opera.html'>Alan Turing, The Opera</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>♦&nbsp;</p>Last Thursday I attended the world premier of The Life and Death(s) of Alan Turing, a new production from Chicago Opera Theater composed by Justine Chen with the libretto (text) from David Simpatico.<p></p><p>The opera takes a mostly chronological trip through his life in seven scenes, focusing less on the computer science and more on Turing's struggle with homosexuality and his prosecution. Turing does fiddle with an old computer throughout the opera. The opera ends with a different take on his death (spoiler alert), where he attempts to upload his consciousness to escape his chemically castrated body.&nbsp;</p><p>Between the scenes, a chorus chants random numbers and words as they floated on a scrim, in what the composer calls "chat clouds".&nbsp;</p><blockquote><p>These “chat clouds” transport the listeners with a sonic approximation of internet chatter, filled with information that brings them to the next moment. The aural depiction of these "chat clouds" was inspired by the animated movies and television series of Masamune Shirow's cyberpunk manga Ghost in the Shell. Another sonic influence was Walt Disney’s fantastical Snow White, one of Alan’s greatest obsessions.</p></blockquote><p>I found them reminiscent of the Philip Glass'&nbsp;knee play&nbsp;from Einstein on the Beach. I really enjoyed these sequences, though another academic I ran into during intermission felt otherwise.</p><p>Over all I enjoyed the music and the opera, particularly the courtroom scene where Turing gave an impassioned defense though it turns out all in his head. The cast did well across the board, especially Jonathan Michie who sang Turing.&nbsp;</p>♦Librettist David Simpatico (wearing jacking), Composer Justine Chen (in gown)<br>conductor&nbsp;Lidiya Yankovskaya behind her and Jonathan Michie (in red robe)<p>One can't help compare this opera to The (R)evolution of Steve Jobs, that I&nbsp;saw in Atlanta last year. Both operas chart a metaphysical journey of a computing giant through various important moments of their lives. During a Q&amp;A I asked Simpatico about the two and he said he purposely avoided the Jobs opera so as to not affect how he wrote this one. Probably for the best.</p><p>By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEih91e9xcRo-Hr1RCp_82Gpy7YFV2sUTXy8IYTFSJ-LMZrr__CZtlFk2EHGczD2-XZdOtbkYNOC2pX_nSJqFqz6PFQTj0vNEDQMgSi02QC5vQ0gF5KnEsIXCDh4wB_NTBP2TzFdwYXCeYIoNRUDjKwYpNZpCGvTHyH8a2tW-q2Bw3F2vpMwxw/s1312/Turing.png" style="clear: right; display: inline; float: right; margin-bottom: 1em; margin-left: 1em; text-align: center;"><img border="0" data-original-height="1312" data-original-width="855" height="320" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEih91e9xcRo-Hr1RCp_82Gpy7YFV2sUTXy8IYTFSJ-LMZrr__CZtlFk2EHGczD2-XZdOtbkYNOC2pX_nSJqFqz6PFQTj0vNEDQMgSi02QC5vQ0gF5KnEsIXCDh4wB_NTBP2TzFdwYXCeYIoNRUDjKwYpNZpCGvTHyH8a2tW-q2Bw3F2vpMwxw/w210-h320/Turing.png" width="210" /></a>&nbsp;</p>Last Thursday I attended the world premier of <a href="https://chicagooperatheater.org/season/turing">The Life and Death(s) of Alan Turing</a>, a new production from Chicago Opera Theater composed by Justine Chen with the libretto (text) from David Simpatico.<p></p><p>The opera takes a mostly chronological trip through his life in seven scenes, focusing less on the computer science and more on Turing's struggle with homosexuality and his prosecution. Turing does fiddle with an old computer throughout the opera. The opera ends with a different take on his death (spoiler alert), where he attempts to upload his consciousness to escape his chemically castrated body.&nbsp;</p><p>Between the scenes, a chorus chants random numbers and words as they floated on a scrim, in what the composer calls "chat clouds".&nbsp;</p><blockquote><p>These “chat clouds” transport the listeners with a sonic approximation of internet chatter, filled with information that brings them to the next moment. The aural depiction of these "chat clouds" was inspired by the animated movies and television series of Masamune Shirow's cyberpunk manga Ghost in the Shell. Another sonic influence was Walt Disney’s fantastical Snow White, one of Alan’s greatest obsessions.</p></blockquote><p>I found them reminiscent of the Philip Glass'&nbsp;<a href="https://www.youtube.com/watch?v=9YRzS9y-8S8">knee play</a>&nbsp;from Einstein on the Beach. I really enjoyed these sequences, though another academic I ran into during intermission felt otherwise.</p><p>Over all I enjoyed the music and the opera, particularly the courtroom scene where Turing gave an impassioned defense though it turns out all in his head. The cast did well across the board, especially Jonathan Michie who sang Turing.&nbsp;</p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgTEyFR4Sjvw7Ew4ip5OXdi6Q0Q3iJLmzZg7P0jknvTJjHgVdUj7f_XtvW16CiHsbDu8pJxL8QVKurCOc3crdwE03zcWzfh0FF1SLPBGUBqqunj_GAUmmcGnoQKHr2b-QPI5X2fafywY1UrVIEXQBHJGxChdL699rjzysU0OgQbQvY1tuJnDg/s1954/PXL_20230324_024100463.jpg" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="769" data-original-width="1954" height="158" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgTEyFR4Sjvw7Ew4ip5OXdi6Q0Q3iJLmzZg7P0jknvTJjHgVdUj7f_XtvW16CiHsbDu8pJxL8QVKurCOc3crdwE03zcWzfh0FF1SLPBGUBqqunj_GAUmmcGnoQKHr2b-QPI5X2fafywY1UrVIEXQBHJGxChdL699rjzysU0OgQbQvY1tuJnDg/w400-h158/PXL_20230324_024100463.jpg" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Librettist David Simpatico (wearing jacking), Composer Justine Chen (in gown)<br />conductor&nbsp;Lidiya Yankovskaya behind her and Jonathan Michie (in red robe)</td></tr></tbody></table><p>One can't help compare this opera to <a href="https://en.wikipedia.org/wiki/The_(R)evolution_of_Steve_Jobs">The (R)evolution of Steve Jobs</a>, that I&nbsp;<a href="https://blog.computationalcomplexity.org/2022/05/the-revolution-of-steve-jobs.html">saw in Atlanta</a> last year. Both operas chart a metaphysical journey of a computing giant through various important moments of their lives. During a Q&amp;A I asked Simpatico about the two and he said he purposely avoided the Jobs opera so as to not affect how he wrote this one. Probably for the best.</p><p class="authors">By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-29T15:29:00Z">Wednesday, March 29 2023, 15:29</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.15556'>Complexity of Reconfiguration in Surface Chemical Reaction Networks</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Robert M. Alaniz, Josh Brunner, Michael Coulombe, Erik D. Demaine, Yevhenii Diomidov, Ryan Knobel, Timothy Gomez, Elise Grizzell, Jayson Lynch, Andrew Rodriguez, Robert Schweller, Tim Wylie</p><p>We analyze the computational complexity of basic reconfiguration problems for
the recently introduced surface Chemical Reaction Networks (sCRNs), where
ordered pairs of adjacent species nondeterministically transform into a
different ordered pair of species according to a predefined set of allowed
transition rules (chemical reactions). In particular, two questions that are
fundamental to the simulation of sCRNs are whether a given configuration of
molecules can ever transform into another given configuration, and whether a
given cell can ever contain a given species, given a set of transition rules.
We show that these problems can be solved in polynomial time, are NP-complete,
or are PSPACE-complete in a variety of different settings, including when
adjacent species just swap instead of arbitrary transformation (swap sCRNs),
and when cells can change species a limited number of times (k-burnout). Most
problems turn out to be at least NP-hard except with very few distinct species
(2 or 3).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Alaniz_R/0/1/0/all/0/1">Robert M. Alaniz</a>, <a href="http://arxiv.org/find/cs/1/au:+Brunner_J/0/1/0/all/0/1">Josh Brunner</a>, <a href="http://arxiv.org/find/cs/1/au:+Coulombe_M/0/1/0/all/0/1">Michael Coulombe</a>, <a href="http://arxiv.org/find/cs/1/au:+Demaine_E/0/1/0/all/0/1">Erik D. Demaine</a>, <a href="http://arxiv.org/find/cs/1/au:+Diomidov_Y/0/1/0/all/0/1">Yevhenii Diomidov</a>, <a href="http://arxiv.org/find/cs/1/au:+Knobel_R/0/1/0/all/0/1">Ryan Knobel</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomez_T/0/1/0/all/0/1">Timothy Gomez</a>, <a href="http://arxiv.org/find/cs/1/au:+Grizzell_E/0/1/0/all/0/1">Elise Grizzell</a>, <a href="http://arxiv.org/find/cs/1/au:+Lynch_J/0/1/0/all/0/1">Jayson Lynch</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1">Andrew Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Schweller_R/0/1/0/all/0/1">Robert Schweller</a>, <a href="http://arxiv.org/find/cs/1/au:+Wylie_T/0/1/0/all/0/1">Tim Wylie</a></p><p>We analyze the computational complexity of basic reconfiguration problems for
the recently introduced surface Chemical Reaction Networks (sCRNs), where
ordered pairs of adjacent species nondeterministically transform into a
different ordered pair of species according to a predefined set of allowed
transition rules (chemical reactions). In particular, two questions that are
fundamental to the simulation of sCRNs are whether a given configuration of
molecules can ever transform into another given configuration, and whether a
given cell can ever contain a given species, given a set of transition rules.
We show that these problems can be solved in polynomial time, are NP-complete,
or are PSPACE-complete in a variety of different settings, including when
adjacent species just swap instead of arbitrary transformation (swap sCRNs),
and when cells can change species a limited number of times (k-burnout). Most
problems turn out to be at least NP-hard except with very few distinct species
(2 or 3).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-29T00:30:00Z">Wednesday, March 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.15610'>Towards Crossing-Free Hamiltonian Cycles in Simple Drawings of Complete Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Oswin Aichholzer, Joachim Orthaber, Birgit Vogtenhuber</p><p>It is a longstanding conjecture that every simple drawing of a complete graph
on $n\geq 3$ vertices contains a crossing-free Hamiltonian cycle. We confirm
this conjecture for cylindrical drawings, strongly $c$-monotone drawings, as
well as $x$-bounded drawings. Moreover, we introduce the stronger question of
whether a crossing-free Hamiltonian path between each pair of vertices always
exists.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Aichholzer_O/0/1/0/all/0/1">Oswin Aichholzer</a>, <a href="http://arxiv.org/find/math/1/au:+Orthaber_J/0/1/0/all/0/1">Joachim Orthaber</a>, <a href="http://arxiv.org/find/math/1/au:+Vogtenhuber_B/0/1/0/all/0/1">Birgit Vogtenhuber</a></p><p>It is a longstanding conjecture that every simple drawing of a complete graph
on $n\geq 3$ vertices contains a crossing-free Hamiltonian cycle. We confirm
this conjecture for cylindrical drawings, strongly $c$-monotone drawings, as
well as $x$-bounded drawings. Moreover, we introduce the stronger question of
whether a crossing-free Hamiltonian path between each pair of vertices always
exists.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-29T00:30:00Z">Wednesday, March 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.15945'>Online embedding of metrics</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ilan Newman, Yuri Rabinovich</p><p>We study deterministic online embeddings of metrics spaces into normed spaces
and into trees against an adaptive adversary. Main results include a polynomial
lower bound on the (multiplicative) distortion of embedding into Euclidean
spaces, a tight exponential upper bound on embedding into the line, and a
$(1+\epsilon)$-distortion embedding in $\ell_\infty$ of a suitably high
dimension.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Newman_I/0/1/0/all/0/1">Ilan Newman</a>, <a href="http://arxiv.org/find/cs/1/au:+Rabinovich_Y/0/1/0/all/0/1">Yuri Rabinovich</a></p><p>We study deterministic online embeddings of metrics spaces into normed spaces
and into trees against an adaptive adversary. Main results include a polynomial
lower bound on the (multiplicative) distortion of embedding into Euclidean
spaces, a tight exponential upper bound on embedding into the line, and a
$(1+\epsilon)$-distortion embedding in $\ell_\infty$ of a suitably high
dimension.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-29T00:30:00Z">Wednesday, March 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.15550'>Randomized rounding algorithms for large scale unsplittable flow problems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Fran&#xe7;ois Lamothe, Emmanuel Rachelson, Alain Ha&#xef;t, Cedric Baudoin, Jean-Baptiste Dupe</p><p>Unsplittable flow problems cover a wide range of telecommunication and
transportation problems and their efficient resolution is key to a number of
applications. In this work, we study algorithms that can scale up to large
graphs and important numbers of commodities. We present and analyze in detail a
heuristic based on the linear relaxation of the problem and randomized
rounding. We provide empirical evidence that this approach is competitive with
state-of-the-art resolution methods either by its scaling performance or by the
quality of its solutions. We provide a variation of the heuristic which has the
same approximation factor as the state-of-the-art approximation algorithm. We
also derive a tighter analysis for the approximation factor of both the
variation and the state-of-the-art algorithm. We introduce a new objective
function for the unsplittable flow problem and discuss its differences with the
classical congestion objective function. Finally, we discuss the gap in
practical performance and theoretical guarantees between all the aforementioned
algorithms.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lamothe_F/0/1/0/all/0/1">Fran&#xe7;ois Lamothe</a>, <a href="http://arxiv.org/find/cs/1/au:+Rachelson_E/0/1/0/all/0/1">Emmanuel Rachelson</a>, <a href="http://arxiv.org/find/cs/1/au:+Hait_A/0/1/0/all/0/1">Alain Ha&#xef;t</a>, <a href="http://arxiv.org/find/cs/1/au:+Baudoin_C/0/1/0/all/0/1">Cedric Baudoin</a>, <a href="http://arxiv.org/find/cs/1/au:+Dupe_J/0/1/0/all/0/1">Jean-Baptiste Dupe</a></p><p>Unsplittable flow problems cover a wide range of telecommunication and
transportation problems and their efficient resolution is key to a number of
applications. In this work, we study algorithms that can scale up to large
graphs and important numbers of commodities. We present and analyze in detail a
heuristic based on the linear relaxation of the problem and randomized
rounding. We provide empirical evidence that this approach is competitive with
state-of-the-art resolution methods either by its scaling performance or by the
quality of its solutions. We provide a variation of the heuristic which has the
same approximation factor as the state-of-the-art approximation algorithm. We
also derive a tighter analysis for the approximation factor of both the
variation and the state-of-the-art algorithm. We introduce a new objective
function for the unsplittable flow problem and discuss its differences with the
classical congestion objective function. Finally, we discuss the gap in
practical performance and theoretical guarantees between all the aforementioned
algorithms.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-29T00:30:00Z">Wednesday, March 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.15594'>On de novo Bridging Paired-end RNA-seq Data</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xiang Li, Mingfu Shao</p><p>The high-throughput short-reads RNA-seq protocols often produce paired-end
reads, with the middle portion of the fragments being unsequenced. We explore
if the full-length fragments can be computationally reconstructed from the
sequenced two ends in the absence of the reference genome - a problem here we
refer to as de novo bridging. Solving this problem provides longer, more
informative RNA-seq reads, and benefits downstream RNA-seq analysis such as
transcript assembly, expression quantification, and splicing differential
analysis. However, de novo bridging is a challenging and complicated task owing
to alternative splicing, transcript noises, and sequencing errors. It remains
unclear if the data provides sufficient information for accurate bridging, let
alone efficient algorithms that determine the true bridges. Methods have been
proposed to bridge paired-end reads in the presence of reference genome (called
reference-based bridging), but the algorithms are far away from scaling for de
novo bridging as the underlying compacted de Bruijn graph(cdBG) used in the
latter task often contains millions of vertices and edges. We designed a new
truncated Dijkstra's algorithm for this problem, and proposed a novel algorithm
that reuses the shortest path tree to avoid running the truncated Dijkstra's
algorithm from scratch for all vertices for further speeding up. These
innovative techniques result in scalable algorithms that can bridge all
paired-end reads in a cdBG with millions of vertices. Our experiments showed
that paired-end RNA-seq reads can be accurately bridged to a large extent. The
resulting tool is freely available at
github.com/Shao-Group/rnabridge-denovo.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/q-bio/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Shao_M/0/1/0/all/0/1">Mingfu Shao</a></p><p>The high-throughput short-reads RNA-seq protocols often produce paired-end
reads, with the middle portion of the fragments being unsequenced. We explore
if the full-length fragments can be computationally reconstructed from the
sequenced two ends in the absence of the reference genome - a problem here we
refer to as de novo bridging. Solving this problem provides longer, more
informative RNA-seq reads, and benefits downstream RNA-seq analysis such as
transcript assembly, expression quantification, and splicing differential
analysis. However, de novo bridging is a challenging and complicated task owing
to alternative splicing, transcript noises, and sequencing errors. It remains
unclear if the data provides sufficient information for accurate bridging, let
alone efficient algorithms that determine the true bridges. Methods have been
proposed to bridge paired-end reads in the presence of reference genome (called
reference-based bridging), but the algorithms are far away from scaling for de
novo bridging as the underlying compacted de Bruijn graph(cdBG) used in the
latter task often contains millions of vertices and edges. We designed a new
truncated Dijkstra's algorithm for this problem, and proposed a novel algorithm
that reuses the shortest path tree to avoid running the truncated Dijkstra's
algorithm from scratch for all vertices for further speeding up. These
innovative techniques result in scalable algorithms that can bridge all
paired-end reads in a cdBG with millions of vertices. Our experiments showed
that paired-end RNA-seq reads can be accurately bridged to a large extent. The
resulting tool is freely available at
https://github.com/Shao-Group/rnabridge-denovo.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-29T00:30:00Z">Wednesday, March 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.15608'>Overcoming Probabilistic Faults in Disoriented Linear Search</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Konstantinos Georgiou, Nikos Giachoudis, Evangelos Kranakis</p><p>We consider search by mobile agents for a hidden, idle target, placed on the
infinite line. Feasible solutions are agent trajectories in which all agents
reach the target sooner or later. A special feature of our problem is that the
agents are $p$-faulty, meaning that every attempt to change direction is an
independent Bernoulli trial with known probability $p$, where $p$ is the
probability that a turn fails. We are looking for agent trajectories that
minimize the worst-case expected termination time, relative to competitive
analysis.
</p>
<p>First, we study linear search with one deterministic $p$-faulty agent, i.e.,
with no access to random oracles, $p\in (0,1/2)$. For this problem, we provide
trajectories that leverage the probabilistic faults into an algorithmic
advantage. Our strongest result pertains to a search algorithm (deterministic,
aside from the adversarial probabilistic faults) which, as $p\to 0$, has
optimal performance $4.59112+\epsilon$, up to the additive term $\epsilon$ that
can be arbitrarily small. Additionally, it has performance less than $9$ for
$p\leq 0.390388$. When $p\to 1/2$, our algorithm has performance
$\Theta(1/(1-2p))$, which we also show is optimal up to a constant factor.
</p>
<p>Second, we consider linear search with two $p$-faulty agents, $p\in (0,1/2)$,
for which we provide three algorithms of different advantages, all with a
bounded competitive ratio even as $p\rightarrow 1/2$. Indeed, for this problem,
we show how the agents can simulate the trajectory of any $0$-faulty agent
(deterministic or randomized), independently of the underlying communication
model. As a result, searching with two agents allows for a solution with a
competitive ratio of $9+\epsilon$, or a competitive ratio of
$4.59112+\epsilon$. Our final contribution is a novel algorithm for searching
with two $p$-faulty agents that achieves a competitive ratio
$3+4\sqrt{p(1-p)}$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Georgiou_K/0/1/0/all/0/1">Konstantinos Georgiou</a>, <a href="http://arxiv.org/find/cs/1/au:+Giachoudis_N/0/1/0/all/0/1">Nikos Giachoudis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kranakis_E/0/1/0/all/0/1">Evangelos Kranakis</a></p><p>We consider search by mobile agents for a hidden, idle target, placed on the
infinite line. Feasible solutions are agent trajectories in which all agents
reach the target sooner or later. A special feature of our problem is that the
agents are $p$-faulty, meaning that every attempt to change direction is an
independent Bernoulli trial with known probability $p$, where $p$ is the
probability that a turn fails. We are looking for agent trajectories that
minimize the worst-case expected termination time, relative to competitive
analysis.
</p>
<p>First, we study linear search with one deterministic $p$-faulty agent, i.e.,
with no access to random oracles, $p\in (0,1/2)$. For this problem, we provide
trajectories that leverage the probabilistic faults into an algorithmic
advantage. Our strongest result pertains to a search algorithm (deterministic,
aside from the adversarial probabilistic faults) which, as $p\to 0$, has
optimal performance $4.59112+\epsilon$, up to the additive term $\epsilon$ that
can be arbitrarily small. Additionally, it has performance less than $9$ for
$p\leq 0.390388$. When $p\to 1/2$, our algorithm has performance
$\Theta(1/(1-2p))$, which we also show is optimal up to a constant factor.
</p>
<p>Second, we consider linear search with two $p$-faulty agents, $p\in (0,1/2)$,
for which we provide three algorithms of different advantages, all with a
bounded competitive ratio even as $p\rightarrow 1/2$. Indeed, for this problem,
we show how the agents can simulate the trajectory of any $0$-faulty agent
(deterministic or randomized), independently of the underlying communication
model. As a result, searching with two agents allows for a solution with a
competitive ratio of $9+\epsilon$, or a competitive ratio of
$4.59112+\epsilon$. Our final contribution is a novel algorithm for searching
with two $p$-faulty agents that achieves a competitive ratio
$3+4\sqrt{p(1-p)}$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-29T00:30:00Z">Wednesday, March 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.15652'>Structured Dynamic Pricing: Optimal Regret in a Global Shrinkage Model</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Rashmi Ranjan Bhuyan, Adel Javanmard, Sungchul Kim, Gourab Mukherjee, Ryan A. Rossi, Tong Yu, Handong Zhao</p><p>We consider dynamic pricing strategies in a streamed longitudinal data set-up
where the objective is to maximize, over time, the cumulative profit across a
large number of customer segments. We consider a dynamic probit model with the
consumers' preferences as well as price sensitivity varying over time. Building
on the well-known finding that consumers sharing similar characteristics act in
similar ways, we consider a global shrinkage structure, which assumes that the
consumers' preferences across the different segments can be well approximated
by a spatial autoregressive (SAR) model. In such a streamed longitudinal
set-up, we measure the performance of a dynamic pricing policy via regret,
which is the expected revenue loss compared to a clairvoyant that knows the
sequence of model parameters in advance. We propose a pricing policy based on
penalized stochastic gradient descent (PSGD) and explicitly characterize its
regret as functions of time, the temporal variability in the model parameters
as well as the strength of the auto-correlation network structure spanning the
varied customer segments. Our regret analysis results not only demonstrate
asymptotic optimality of the proposed policy but also show that for policy
planning it is essential to incorporate available structural information as
policies based on unshrunken models are highly sub-optimal in the
aforementioned set-up.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bhuyan_R/0/1/0/all/0/1">Rashmi Ranjan Bhuyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Javanmard_A/0/1/0/all/0/1">Adel Javanmard</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sungchul Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_G/0/1/0/all/0/1">Gourab Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1">Ryan A. Rossi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1">Tong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Handong Zhao</a></p><p>We consider dynamic pricing strategies in a streamed longitudinal data set-up
where the objective is to maximize, over time, the cumulative profit across a
large number of customer segments. We consider a dynamic probit model with the
consumers' preferences as well as price sensitivity varying over time. Building
on the well-known finding that consumers sharing similar characteristics act in
similar ways, we consider a global shrinkage structure, which assumes that the
consumers' preferences across the different segments can be well approximated
by a spatial autoregressive (SAR) model. In such a streamed longitudinal
set-up, we measure the performance of a dynamic pricing policy via regret,
which is the expected revenue loss compared to a clairvoyant that knows the
sequence of model parameters in advance. We propose a pricing policy based on
penalized stochastic gradient descent (PSGD) and explicitly characterize its
regret as functions of time, the temporal variability in the model parameters
as well as the strength of the auto-correlation network structure spanning the
varied customer segments. Our regret analysis results not only demonstrate
asymptotic optimality of the proposed policy but also show that for policy
planning it is essential to incorporate available structural information as
policies based on unshrunken models are highly sub-optimal in the
aforementioned set-up.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-29T00:30:00Z">Wednesday, March 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.15873'>Algorithms for subgraph complementation to some classes of graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Dhanyamol Antony, Sagartanu Pal, R.B. Sandeep</p><p>For a class $\mathcal{G}$ of graphs, the objective of \textsc{Subgraph
Complementation to} $\mathcal{G}$ is to find whether there exists a subset $S$
of vertices of the input graph $G$ such that modifying $G$ by complementing the
subgraph induced by $S$ results in a graph in $\mathcal{G}$. We obtain a
polynomial-time algorithm for the problem when $\mathcal{G}$ is the class of
graphs with minimum degree at least $k$, for a constant $k$, answering an open
problem by Fomin et al. (Algorithmica, 2020). When $\mathcal{G}$ is the class
of graphs without any induced copies of the star graph on $t+1$ vertices (for
any constant $t\geq 3$) and diamond, we obtain a polynomial-time algorithm for
the problem. This is in contrast with a result by Antony et al. (Algorithmica,
2022) that the problem is NP-complete and cannot be solved in
subexponential-time (assuming the Exponential Time Hypothesis) when
$\mathcal{G}$ is the class of graphs without any induced copies of the star
graph on $t+1$ vertices, for every constant $t\geq 5$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Antony_D/0/1/0/all/0/1">Dhanyamol Antony</a>, <a href="http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1">Sagartanu Pal</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandeep_R/0/1/0/all/0/1">R.B. Sandeep</a></p><p>For a class $\mathcal{G}$ of graphs, the objective of \textsc{Subgraph
Complementation to} $\mathcal{G}$ is to find whether there exists a subset $S$
of vertices of the input graph $G$ such that modifying $G$ by complementing the
subgraph induced by $S$ results in a graph in $\mathcal{G}$. We obtain a
polynomial-time algorithm for the problem when $\mathcal{G}$ is the class of
graphs with minimum degree at least $k$, for a constant $k$, answering an open
problem by Fomin et al. (Algorithmica, 2020). When $\mathcal{G}$ is the class
of graphs without any induced copies of the star graph on $t+1$ vertices (for
any constant $t\geq 3$) and diamond, we obtain a polynomial-time algorithm for
the problem. This is in contrast with a result by Antony et al. (Algorithmica,
2022) that the problem is NP-complete and cannot be solved in
subexponential-time (assuming the Exponential Time Hypothesis) when
$\mathcal{G}$ is the class of graphs without any induced copies of the star
graph on $t+1$ vertices, for every constant $t\geq 5$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-29T00:30:00Z">Wednesday, March 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.16043'>Faster Deterministic Distributed MIS and Approximate Matching</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mohsen Ghaffari, Christoph Grunau</p><p>$ \renewcommand{\tilde}{\widetilde} $We present an $\tilde{O}(\log^2 n)$
round deterministic distributed algorithm for the maximal independent set
problem. By known reductions, this round complexity extends also to maximal
matching, $\Delta+1$ vertex coloring, and $2\Delta-1$ edge coloring. These four
problems are among the most central problems in distributed graph algorithms
and have been studied extensively for the past four decades. This improved
round complexity comes closer to the $\tilde{\Omega}(\log n)$ lower bound of
maximal independent set and maximal matching [Balliu et al. FOCS '19]. The
previous best known deterministic complexity for all of these problems was
$\Theta(\log^3 n)$. Via the shattering technique, the improvement permeates
also to the corresponding randomized complexities, e.g., the new randomized
complexity of $\Delta+1$ vertex coloring is now $\tilde{O}(\log^2\log n)$
rounds.
</p>
<p>Our approach is a novel combination of the previously known two methods for
developing deterministic algorithms for these problems, namely global
derandomization via network decomposition (see e.g., [Rozhon, Ghaffari STOC'20;
Ghaffari, Grunau, Rozhon SODA'21; Ghaffari et al. SODA'23]) and local rounding
of fractional solutions (see e.g., [Fischer DISC'17; Harris FOCS'19; Fischer,
Ghaffari, Kuhn FOCS'17; Ghaffari, Kuhn FOCS'21; Faour et al. SODA'23]). We
consider a relaxation of the classic network decomposition concept, where
instead of requiring the clusters in the same block to be non-adjacent, we
allow each node to have a small number of neighboring clusters. We also show a
deterministic algorithm that computes this relaxed decomposition faster than
standard decompositions. We then use this relaxed decomposition to
significantly improve the integrality of certain fractional solutions, before
handing them to the local rounding procedure that now has to do fewer rounding
steps.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1">Mohsen Ghaffari</a>, <a href="http://arxiv.org/find/cs/1/au:+Grunau_C/0/1/0/all/0/1">Christoph Grunau</a></p><p>$ \renewcommand{\tilde}{\widetilde} $We present an $\tilde{O}(\log^2 n)$
round deterministic distributed algorithm for the maximal independent set
problem. By known reductions, this round complexity extends also to maximal
matching, $\Delta+1$ vertex coloring, and $2\Delta-1$ edge coloring. These four
problems are among the most central problems in distributed graph algorithms
and have been studied extensively for the past four decades. This improved
round complexity comes closer to the $\tilde{\Omega}(\log n)$ lower bound of
maximal independent set and maximal matching [Balliu et al. FOCS '19]. The
previous best known deterministic complexity for all of these problems was
$\Theta(\log^3 n)$. Via the shattering technique, the improvement permeates
also to the corresponding randomized complexities, e.g., the new randomized
complexity of $\Delta+1$ vertex coloring is now $\tilde{O}(\log^2\log n)$
rounds.
</p>
<p>Our approach is a novel combination of the previously known two methods for
developing deterministic algorithms for these problems, namely global
derandomization via network decomposition (see e.g., [Rozhon, Ghaffari STOC'20;
Ghaffari, Grunau, Rozhon SODA'21; Ghaffari et al. SODA'23]) and local rounding
of fractional solutions (see e.g., [Fischer DISC'17; Harris FOCS'19; Fischer,
Ghaffari, Kuhn FOCS'17; Ghaffari, Kuhn FOCS'21; Faour et al. SODA'23]). We
consider a relaxation of the classic network decomposition concept, where
instead of requiring the clusters in the same block to be non-adjacent, we
allow each node to have a small number of neighboring clusters. We also show a
deterministic algorithm that computes this relaxed decomposition faster than
standard decompositions. We then use this relaxed decomposition to
significantly improve the integrality of certain fractional solutions, before
handing them to the local rounding procedure that now has to do fewer rounding
steps.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-29T00:30:00Z">Wednesday, March 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.16180'>Universal Coating in the 3D Hybrid Model</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Irina Kostitsyna, David Liedtke, Christian Scheideler</p><p>Motivated by the prospect of nano-robots that assist human physiological
functions at the nanoscale, we investigate the coating problem in the
three-dimensional model for hybrid programmable matter. In this model, a single
agent with strictly limited viewing range and the computational capability of a
deterministic finite automaton can act on passive tiles by picking up a tile,
moving, and placing it at some spot. The goal of the coating problem is to fill
each node of some surface graph of size $n$ with a tile. We first solve the
problem on a restricted class of graphs with a single tile type, and then use
constantly many tile types to encode this graph in certain surface graphs
capturing the surface of 3D objects. Our algorithm requires $\mathcal{O}(n^2)$
steps, which is worst-case optimal compared to an agent with global knowledge
and no memory restrictions.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kostitsyna_I/0/1/0/all/0/1">Irina Kostitsyna</a>, <a href="http://arxiv.org/find/cs/1/au:+Liedtke_D/0/1/0/all/0/1">David Liedtke</a>, <a href="http://arxiv.org/find/cs/1/au:+Scheideler_C/0/1/0/all/0/1">Christian Scheideler</a></p><p>Motivated by the prospect of nano-robots that assist human physiological
functions at the nanoscale, we investigate the coating problem in the
three-dimensional model for hybrid programmable matter. In this model, a single
agent with strictly limited viewing range and the computational capability of a
deterministic finite automaton can act on passive tiles by picking up a tile,
moving, and placing it at some spot. The goal of the coating problem is to fill
each node of some surface graph of size $n$ with a tile. We first solve the
problem on a restricted class of graphs with a single tile type, and then use
constantly many tile types to encode this graph in certain surface graphs
capturing the surface of 3D objects. Our algorithm requires $\mathcal{O}(n^2)$
steps, which is worst-case optimal compared to an agent with global knowledge
and no memory restrictions.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-29T00:30:00Z">Wednesday, March 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Tuesday, March 28
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=7170'>An unexpected democracy slogan</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          At least six readers have by now sent me the following photo, which was taken in Israel a couple nights ago during the historic street protests against Netanyahu&#8217;s attempted putsch: (Update: The photo was also featured on Gil Kalai&#8217;s blog, and was credited there to Alon Rosen.) This is surely the first time that &#8220;P=NP&#8221; [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>At least six readers have by now sent me the following photo, which was taken in Israel a couple nights ago during the historic street protests against Netanyahu&#8217;s attempted <em>putsch</em>:</p>



<figure class="wp-block-image size-large"><img decoding="async" src="https://www.scottaaronson.com/pnp.jpg" alt=""/></figure>



<p>(<strong><mark style="background-color:rgba(0, 0, 0, 0)" class="has-inline-color has-vivid-red-color">Update:</mark></strong> The photo was also featured on <a href="https://gilkalai.wordpress.com/2023/03/27/critical-times-in-israel-last-nights-demonstrations/">Gil Kalai&#8217;s blog</a>, and was credited there to Alon Rosen.)</p>



<p>This is surely the first time that &#8220;P=NP&#8221; has emerged as a viral rallying cry for the preservation of liberal democracy, even to whatever limited extent it has.</p>



<p>But what was the graffiti artist&#8217;s intended meaning?  A few possibilities:</p>



<ol>
<li>The government has flouted so many rules of Israel&#8217;s social compact that our side needs to flout the rules too: shut down the universities, shut down the airport, block the roads, <em>even assert that P=NP (!)</em>.</li>



<li>As a protest movement up against overwhelming odds, we need to shoot for the possibly-impossible, like solving 3SAT in polynomial time.</li>



<li>A shibboleth for scientific literate people following the news: &#8220;Israel is full of sane people who know what &#8216;P=NP&#8217; means as you know what it means, are amused by its use as political graffiti as you&#8217;d be amused by it, and oppose Netanyahu&#8217;s <em>putsch</em> for the same reasons you&#8217;d oppose it.&#8221;</li>



<li>No meaning, the artist was just amusing himself or herself.</li>



<li>The artist reads <em>Shtetl-Optimized</em> and wanted effectively to force me to feature his or her work here.</li>
</ol>



<p>Anyway, if the artist becomes aware of this post, he or she is warmly welcomed to clear things up for us.</p>



<p>And when this fight resumes after Passover, may those standing up for the checks and balances of a liberal-democratic society achieve &#8230; err &#8230; satisfaction, however exponentially unlikely it seems.</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-28T20:27:28Z">Tuesday, March 28 2023, 20:27</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/040'>TR23-040 |  Certified Hardness vs. Randomness for Log-Space | 

	Edward Pyne, 

	Wei Zhan, 

	Ran Raz</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Let $\mathcal{L}$ be a language that can be decided in linear space and let $\epsilon &gt;0$ be any constant. Let $\mathcal{A}$ be the exponential hardness assumption that for every $n$, membership in $\mathcal{L}$ for inputs of length~$n$ cannot be decided by circuits of size smaller than $2^{\epsilon n}$. 
We prove that for every function $f :\{0,1\}^* \rightarrow \{0,1\}$, computable by a randomized logspace algorithm $R$, there exists a deterministic logspace algorithm $D$ (attempting to compute $f$), such that on every input $x$ of length $n$, the algorithm $D$ outputs one of the following:

1: The correct value $f(x)$.

2: The string: ``I am unable to compute $f(x)$ because the hardness assumption $\mathcal{A}$ is false&#39;&#39;, followed by a (provenly correct) circuit of size smaller than $2^{\epsilon n&#39;}$ for membership in $\mathcal{L}$ for inputs of length~$n&#39;$, for some $n&#39; = \Theta (\log n)$; that is, a circuit that refutes $\mathcal{A}$.

Moreover, $D$ is explicitly constructed, given $R$.

We note that previous works on the hardness-versus-randomness paradigm give derandomized algorithms that rely blindly on the hardness assumption. If the hardness assumption is false, the algorithms may output incorrect values, and thus a user cannot trust that an output given by the algorithm is correct. Instead, our algorithm $D$ verifies the computation so that it never outputs an incorrect value. Thus, if $D$ outputs a value for $f(x)$, that value is certified to be correct. Moreover, if $D$ does not output a value for $f(x)$, it alerts that the hardness assumption was found to be false, and refutes the assumption.

Our next result is a universal derandomizer for $BPL$ (the class of problems solvable by bounded-error randomized logspace algorithms): We give a deterministic algorithm $U$ that takes as an input a randomized logspace algorithm $R$ and an input $x$ and simulates the computation of $R$ on $x$, deteriministically. Under the widely believed assumption $BPL=L$, the space used by $U$ is at most $C_R \cdot \log n$ (where $C_R$ is a constant depending on~$R$). Moreover, for every constant $c \geq 1$, if $BPL\subseteq SPACE[(\log(n))^{c}]$ then the space used by $U$ is at most $C_R  \cdot (\log(n))^{c}$.

Finally, we prove that if optimal hitting sets for ordered branching programs exist then there is a deterministic logspace algorithm that, given a black-box access to an ordered branching program $B$ of size $n$, estimates the probability that $B$ accepts on a uniformly random input. This extends the result of (Cheng and Hoza CCC 2020), who proved that an optimal hitting set implies a white-box two-sided derandomization.
        
        </div>

        <div class='tr-article-summary'>
        
          
          Let $\mathcal{L}$ be a language that can be decided in linear space and let $\epsilon &gt;0$ be any constant. Let $\mathcal{A}$ be the exponential hardness assumption that for every $n$, membership in $\mathcal{L}$ for inputs of length~$n$ cannot be decided by circuits of size smaller than $2^{\epsilon n}$. 
We prove that for every function $f :\{0,1\}^* \rightarrow \{0,1\}$, computable by a randomized logspace algorithm $R$, there exists a deterministic logspace algorithm $D$ (attempting to compute $f$), such that on every input $x$ of length $n$, the algorithm $D$ outputs one of the following:

1: The correct value $f(x)$.

2: The string: ``I am unable to compute $f(x)$ because the hardness assumption $\mathcal{A}$ is false&#39;&#39;, followed by a (provenly correct) circuit of size smaller than $2^{\epsilon n&#39;}$ for membership in $\mathcal{L}$ for inputs of length~$n&#39;$, for some $n&#39; = \Theta (\log n)$; that is, a circuit that refutes $\mathcal{A}$.

Moreover, $D$ is explicitly constructed, given $R$.

We note that previous works on the hardness-versus-randomness paradigm give derandomized algorithms that rely blindly on the hardness assumption. If the hardness assumption is false, the algorithms may output incorrect values, and thus a user cannot trust that an output given by the algorithm is correct. Instead, our algorithm $D$ verifies the computation so that it never outputs an incorrect value. Thus, if $D$ outputs a value for $f(x)$, that value is certified to be correct. Moreover, if $D$ does not output a value for $f(x)$, it alerts that the hardness assumption was found to be false, and refutes the assumption.

Our next result is a universal derandomizer for $BPL$ (the class of problems solvable by bounded-error randomized logspace algorithms): We give a deterministic algorithm $U$ that takes as an input a randomized logspace algorithm $R$ and an input $x$ and simulates the computation of $R$ on $x$, deteriministically. Under the widely believed assumption $BPL=L$, the space used by $U$ is at most $C_R \cdot \log n$ (where $C_R$ is a constant depending on~$R$). Moreover, for every constant $c \geq 1$, if $BPL\subseteq SPACE[(\log(n))^{c}]$ then the space used by $U$ is at most $C_R  \cdot (\log(n))^{c}$.

Finally, we prove that if optimal hitting sets for ordered branching programs exist then there is a deterministic logspace algorithm that, given a black-box access to an ordered branching program $B$ of size $n$, estimates the probability that $B$ accepts on a uniformly random input. This extends the result of (Cheng and Hoza CCC 2020), who proved that an optimal hitting set implies a white-box two-sided derandomization.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-28T19:09:05Z">Tuesday, March 28 2023, 19:09</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/039'>TR23-039 |  Query Complexity of Search Problems | 

	Yogesh Dahiya, 

	Arkadev Chattopadhyay, 

	Meena Mahajan</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We relate various complexity measures like sensitivity, block sensitivity, certificate complexity for multi-output functions to the query complexities of such functions. Using these relations, we improve upon the known relationship between pseudo-deterministic query complexity and deterministic query complexity for total search problems: We show that pseudo-deterministic query complexity is at most the third power of its deterministic query complexity. (Previously a fourth-power relation was shown by Goldreich,Goldwasser,Ron (ITCS13).) We then obtain a significantly simpler and self-contained proof of a separation between pseudodeterminism and randomized query complexity recently proved by Goldwasser, Impagliazzo, Pitassi, Santhanam (CCC 2021). We also separate pseudodeterminism from randomness in AND decision trees, and determinism from pseudodeterminism in PARITY decision trees. For a hypercube colouring problem closely related to the pseudodeterministic complexity of a complete problem in $TFNP^{dt}$, we prove that either the monotone block-sensitivity or the anti-monotone block sensitivity is $\Omega(n^{1/3})$; previously an $\Omega(n^{1/2})$ bound was known but for general block-sensitivity.
        
        </div>

        <div class='tr-article-summary'>
        
          
          We relate various complexity measures like sensitivity, block sensitivity, certificate complexity for multi-output functions to the query complexities of such functions. Using these relations, we improve upon the known relationship between pseudo-deterministic query complexity and deterministic query complexity for total search problems: We show that pseudo-deterministic query complexity is at most the third power of its deterministic query complexity. (Previously a fourth-power relation was shown by Goldreich,Goldwasser,Ron (ITCS13).) We then obtain a significantly simpler and self-contained proof of a separation between pseudodeterminism and randomized query complexity recently proved by Goldwasser, Impagliazzo, Pitassi, Santhanam (CCC 2021). We also separate pseudodeterminism from randomness in AND decision trees, and determinism from pseudodeterminism in PARITY decision trees. For a hypercube colouring problem closely related to the pseudodeterministic complexity of a complete problem in $TFNP^{dt}$, we prove that either the monotone block-sensitivity or the anti-monotone block sensitivity is $\Omega(n^{1/3})$; previously an $\Omega(n^{1/2})$ bound was known but for general block-sensitivity.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-28T18:07:01Z">Tuesday, March 28 2023, 18:07</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/038'>TR23-038 |  Indistinguishability Obfuscation, Range Avoidance, and Bounded Arithmetic | 

	Rahul Ilango, 

	Jiatu Li, 

	Ryan Williams</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The range avoidance problem (denoted by Avoid) asks to find a string outside of the range of a given circuit $C:\{0,1\}^n\to\{0,1\}^m$, where $m&gt;n$. Although at least half of the strings of length $m$ are correct answers, it is not clear how to deterministically find one. Recent results of Korten (FOCS&#39;21) and Ren, Wang, and Santhanam (FOCS&#39; 22) show that efficient deterministic algorithms for Avoid would have far-reaching consequences, including strong circuit lower bounds and explicit constructions of combinatorial objects (e.g., Ramsey graphs, extractors, rigid matrices). This strongly motivates the question: does an efficient deterministic algorithm for Avoid actually exist?

In this work, we prove under the existence of subexponentially secure indistinguishability obfuscation (iO) that deterministic polynomial-time algorithms for Avoid imply NP=coNP. Combining this with Jain, Lin, and Sahai&#39;s recent breakthrough construction of iO from well-founded assumptions (STOC&#39;21, EUROCRYPT&#39;22), we provide the first plausible evidence that Avoid has no efficient deterministic algorithm. Moreover, we also prove the hardness of Avoid based on polynomially-secure iO and a weaker variant of the Nondeterministic Exponential Time Hypothesis (NETH).

Extending our techniques, we prove a surprising separation in bounded arithmetic, conditioned on similar assumptions. Assuming subexponentially secure iO and coNP is not infinitely often in AM, we show that Avoid has no deterministic polynomial-time algorithm even when we are allowed $O(1)$ queries to an oracle that can invert the given input circuit on an arbitrarily chosen $m$-bit string. It follows that the dual Weak Pigeonhole Principle, the combinatorial principle underlying Avoid, is not provable in Cook&#39;s theory $PV_1$. This gives (under plausible assumptions) the first separation of Cook&#39;s theory $PV_1$ for polynomial-time reasoning and Jerabek&#39;s theory $APC_1$ for probabilistic polynomial-time reasoning.
        
        </div>

        <div class='tr-article-summary'>
        
          
          The range avoidance problem (denoted by Avoid) asks to find a string outside of the range of a given circuit $C:\{0,1\}^n\to\{0,1\}^m$, where $m&gt;n$. Although at least half of the strings of length $m$ are correct answers, it is not clear how to deterministically find one. Recent results of Korten (FOCS&#39;21) and Ren, Wang, and Santhanam (FOCS&#39; 22) show that efficient deterministic algorithms for Avoid would have far-reaching consequences, including strong circuit lower bounds and explicit constructions of combinatorial objects (e.g., Ramsey graphs, extractors, rigid matrices). This strongly motivates the question: does an efficient deterministic algorithm for Avoid actually exist?

In this work, we prove under the existence of subexponentially secure indistinguishability obfuscation (iO) that deterministic polynomial-time algorithms for Avoid imply NP=coNP. Combining this with Jain, Lin, and Sahai&#39;s recent breakthrough construction of iO from well-founded assumptions (STOC&#39;21, EUROCRYPT&#39;22), we provide the first plausible evidence that Avoid has no efficient deterministic algorithm. Moreover, we also prove the hardness of Avoid based on polynomially-secure iO and a weaker variant of the Nondeterministic Exponential Time Hypothesis (NETH).

Extending our techniques, we prove a surprising separation in bounded arithmetic, conditioned on similar assumptions. Assuming subexponentially secure iO and coNP is not infinitely often in AM, we show that Avoid has no deterministic polynomial-time algorithm even when we are allowed $O(1)$ queries to an oracle that can invert the given input circuit on an arbitrarily chosen $m$-bit string. It follows that the dual Weak Pigeonhole Principle, the combinatorial principle underlying Avoid, is not provable in Cook&#39;s theory $PV_1$. This gives (under plausible assumptions) the first separation of Cook&#39;s theory $PV_1$ for polynomial-time reasoning and Jerabek&#39;s theory $APC_1$ for probabilistic polynomial-time reasoning.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-28T12:58:10Z">Tuesday, March 28 2023, 12:58</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://gilkalai.wordpress.com/2023/03/28/some-problems/'>Some Problems</a></h3>
        <p class='tr-article-feed'>from <a href='https://gilkalai.wordpress.com'>Gil Kalai</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          In the previous post, Two Three Four posts ago I wrote about three recent breakthroughs in combinatorics and here I want to mention some problems that I posed over the years that are loosely related to these advances. Rank of &#8230; Continue reading &#8594;
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p><del>In the previous post, Two Three</del> Four posts ago I wrote about <a href="https://gilkalai.wordpress.com/2023/03/10/subspace-designs-unit-and-distinct-distances-and-piercing-standard-boxes/">three recent breakthroughs in combinatorics</a> and here I want to mention some problems that I posed over the years that are loosely related to these advances.</p>
<h2>Rank of incidence matrices and <em>q</em>-analogs</h2>
<p>The goal of finding <em>q</em>-analogs of combinatorial results where, roughly speaking, sets are replaced by subspaces of vector spaces over a field with <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q" class="latex" /> elements, is common both in enumerative combinatorics and in extremal combinatorics. A recent breakthrough we discussed by Keevash, Sah, and Sawhney was about the existence of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q" class="latex" />-analogs of designs (subspace designs).</p>
<p>I will mention a problem (Question 4) in this direction about incidence matrices, following three questions that were largely solved.</p>
<h3>Incidence matrices and weighted incidence matrices for sets</h3>
<p>The incidence matrix of <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k" class="latex" />-subsets vs. <img src="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="m" class="latex" />-subsets of <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="[n]" class="latex" />, is a matrix <img src="https://s0.wp.com/latex.php?latex=I_%7Bk%2Cm%7D%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=I_%7Bk%2Cm%7D%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=I_%7Bk%2Cm%7D%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="I_{k,m}(n)" class="latex" /> whose rows correspond to <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k" class="latex" />-subsets <img src="https://s0.wp.com/latex.php?latex=R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="R" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="[n]" class="latex" />, whose columns correspond to <img src="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="m" class="latex" />-subsets <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="[n]" class="latex" />, and the entry <img src="https://s0.wp.com/latex.php?latex=I%28S%2CT%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=I%28S%2CT%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=I%28S%2CT%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="I(S,T)" class="latex" /> equals 1 if <img src="https://s0.wp.com/latex.php?latex=R+%5Csubset+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=R+%5Csubset+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=R+%5Csubset+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="R &#92;subset S" class="latex" />, and equals 0 if <img src="https://s0.wp.com/latex.php?latex=R+%5Cnot+%5Csubset+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=R+%5Cnot+%5Csubset+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=R+%5Cnot+%5Csubset+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="R &#92;not &#92;subset S" class="latex" />.</p>
<p>A weighted incidence matrix of <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k" class="latex" />-subsets vs. <img src="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="m" class="latex" />-subsets of <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="[n]" class="latex" />, is a matrix <img src="https://s0.wp.com/latex.php?latex=J_%7Bk%2Cm%7D%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=J_%7Bk%2Cm%7D%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=J_%7Bk%2Cm%7D%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="J_{k,m}(n)" class="latex" /> whose rows correspond to <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k" class="latex" />-subsets <img src="https://s0.wp.com/latex.php?latex=R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="R" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="[n]" class="latex" />, whose columns correspond to <img src="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="m" class="latex" />-subsets <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="[n]" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=J%28R%2CS%29+%5Cne+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=J%28R%2CS%29+%5Cne+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=J%28R%2CS%29+%5Cne+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="J(R,S) &#92;ne 0" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=R+%5Csubset+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=R+%5Csubset+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=R+%5Csubset+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="R &#92;subset S" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=J%28R%2CS%29%3D0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=J%28R%2CS%29%3D0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=J%28R%2CS%29%3D0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="J(R,S)=0" class="latex" />  if <img src="https://s0.wp.com/latex.php?latex=R+%5Cnot+%5Csubset+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=R+%5Cnot+%5Csubset+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=R+%5Cnot+%5Csubset+S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="R &#92;not &#92;subset S" class="latex" />.</p>
<p><strong>Question 1:</strong> What is the rank <img src="https://s0.wp.com/latex.php?latex=r_p%28n%2Ck%2Cm%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=r_p%28n%2Ck%2Cm%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=r_p%28n%2Ck%2Cm%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="r_p(n,k,m)" class="latex" /> of the incidence matrix of <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k" class="latex" />-subsets vs. <img src="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="m" class="latex" />-subsets of <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="[n]" class="latex" />, over a field of characteristic <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" />.</p>
<p>This question was <a href="http://A diagonal form for the incidence matrices of t-subsets vs. k-subsets‏">beautifully answered</a> by Richard Wilson in 1990 . The problem was posed by Nati Linial and Bruce Rothschild in 1981 and they settled the case <img src="https://s0.wp.com/latex.php?latex=p%3D2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p%3D2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p%3D2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p=2" class="latex" />.  (The answer for <img src="https://s0.wp.com/latex.php?latex=p%3D2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p%3D2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p%3D2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p=2" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=m%3Dk-1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=m%3Dk-1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=m%3Dk-1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="m=k-1" class="latex" />, that motivated the question, was observed earlier by Perles and by Frankl.) It is unforgivable that I did not present the statement of Wilson&#8217;s theorem here on the blog.</p>
<p><strong>Question 2:</strong> What is the minimum rank, denoted by <img src="https://s0.wp.com/latex.php?latex=s_p%28n%2Ck%2Cm%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=s_p%28n%2Ck%2Cm%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=s_p%28n%2Ck%2Cm%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="s_p(n,k,m)" class="latex" />, of a weighted incidence matrix of <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k" class="latex" />-subsets vs. <img src="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="m" class="latex" />-subsets of $[n]$ over a field of characteristic <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" />.</p>
<p>This question was answered by me in the early 80s (it is related also to various results by others around the same time). The answer is <img src="https://s0.wp.com/latex.php?latex=%7Bn-k%2Bm%7D+%5Cchoose+%7Bm%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bn-k%2Bm%7D+%5Cchoose+%7Bm%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bn-k%2Bm%7D+%5Cchoose+%7Bm%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="{n-k+m} &#92;choose {m}" class="latex" />, and remarkably it is independent from the characteristic <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" />.</p>
<h3>Incidence matrices and weighted incidence matrices for subspaces</h3>
<p>The incidence matrix <img src="https://s0.wp.com/latex.php?latex=I%5Eq%3DI%5Eq_%7Bk%2Cm%7D%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=I%5Eq%3DI%5Eq_%7Bk%2Cm%7D%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=I%5Eq%3DI%5Eq_%7Bk%2Cm%7D%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="I^q=I^q_{k,m}(n)" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k" class="latex" />-dimensional subspaces vs. <img src="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="m" class="latex" />-dimensional subspaces of <img src="https://s0.wp.com/latex.php?latex=F_q%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=F_q%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=F_q%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="F_q^n" class="latex" /> (<img src="https://s0.wp.com/latex.php?latex=k%3Cm%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k%3Cm%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k%3Cm%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k&lt;m)" class="latex" /> has entries <img src="https://s0.wp.com/latex.php?latex=I_%7BV%2CU%7D%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=I_%7BV%2CU%7D%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=I_%7BV%2CU%7D%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="I_{V,U}=1" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=V+%5Csubset+U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=V+%5Csubset+U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=V+%5Csubset+U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="V &#92;subset U" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=I_%7BV%2CU%7D%3D0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=I_%7BV%2CU%7D%3D0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=I_%7BV%2CU%7D%3D0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="I_{V,U}=0" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=V+%5Cnot+%5Csubset+U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=V+%5Cnot+%5Csubset+U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=V+%5Cnot+%5Csubset+U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="V &#92;not &#92;subset U" class="latex" />.</p>
<p>A weighted incidence matrix <img src="https://s0.wp.com/latex.php?latex=J%5Eq%3DJ%5Eq_%7Bk%2Cm%7D%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=J%5Eq%3DJ%5Eq_%7Bk%2Cm%7D%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=J%5Eq%3DJ%5Eq_%7Bk%2Cm%7D%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="J^q=J^q_{k,m}(n)" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k" class="latex" />-dimensional subspaces vs. <img src="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="m" class="latex" />-dimensional subspaces of <img src="https://s0.wp.com/latex.php?latex=F_q%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=F_q%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=F_q%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="F_q^n" class="latex" /> (<img src="https://s0.wp.com/latex.php?latex=k%3Cm&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k%3Cm&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k%3Cm&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k&lt;m" class="latex" />) has entries <img src="https://s0.wp.com/latex.php?latex=J_%7BV%2CU%7D%5Cne+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=J_%7BV%2CU%7D%5Cne+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=J_%7BV%2CU%7D%5Cne+0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="J_{V,U}&#92;ne 0" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=V+%5Csubset+U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=V+%5Csubset+U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=V+%5Csubset+U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="V &#92;subset U" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=J_%7BV%2CU%7D%3D0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=J_%7BV%2CU%7D%3D0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=J_%7BV%2CU%7D%3D0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="J_{V,U}=0" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=V+%5Cnot+%5Csubset+U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=V+%5Cnot+%5Csubset+U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=V+%5Cnot+%5Csubset+U&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="V &#92;not &#92;subset U" class="latex" />.</p>
<p>We pose two additional questions which are the &#8220;<em>q</em>-analogs&#8221; of Questions 1 and 2.</p>
<p><strong>Question 3:</strong>  What is the rank <img src="https://s0.wp.com/latex.php?latex=r_q%5Ep%28n%2Ck%2Cm%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=r_q%5Ep%28n%2Ck%2Cm%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=r_q%5Ep%28n%2Ck%2Cm%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="r_q^p(n,k,m)" class="latex" /> of the incidence matrix <img src="https://s0.wp.com/latex.php?latex=I%5Eq_%7Bk%2Cm%7D%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=I%5Eq_%7Bk%2Cm%7D%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=I%5Eq_%7Bk%2Cm%7D%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="I^q_{k,m}(n)" class="latex" /> over a field of characteristic $p$ (you can simply take the field <img src="https://s0.wp.com/latex.php?latex=F_p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=F_p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=F_p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="F_p" class="latex" />).</p>
<p><a href="https://gilkalai.files.wordpress.com/2023/03/bf027737492.pdf">Frumkin and Yakir settled problem 3</a> when <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q" class="latex" /> is not a power of <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" />.</p>
<p>The problem I want to pose (again) here is:</p>
<p><strong><span style="color: #ff0000">Question 4:</span> </strong> What is the minimum rank denoted by <img src="https://s0.wp.com/latex.php?latex=s_q%5Ep%28n%2Ck%2Cm%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=s_q%5Ep%28n%2Ck%2Cm%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=s_q%5Ep%28n%2Ck%2Cm%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="s_q^p(n,k,m)" class="latex" /> of a weighted incidence matrix <img src="https://s0.wp.com/latex.php?latex=J%5Eq_%7Bk%2Cm%7D%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=J%5Eq_%7Bk%2Cm%7D%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=J%5Eq_%7Bk%2Cm%7D%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="J^q_{k,m}(n)" class="latex" /> over a field of characteristic <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" />.</p>
<p>In particular, I would like to know if the answer does not depend on <img src="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p" class="latex" /> and if it agrees with some easy lower bounds (obtained from certain identity submatrices) like in the case of a field with one element <img src="https://s0.wp.com/latex.php?latex=p%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p=1" class="latex" /> (namely, subsets).</p>
<h2>q-trees</h2>
<p><strong><span style="color: #ff0000">Qoestion 5:</span></strong> What are the <em>q</em>-analogs of trees? (and hypertrees).</p>
<p>The basic idea is first to find weights so that the incidence matrix of 1-subspace vs 2-subspaces has rank <img src="https://s0.wp.com/latex.php?latex=q%2Bq%5E2%2B%5Ccdots+%2Bq%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q%2Bq%5E2%2B%5Ccdots+%2Bq%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q%2Bq%5E2%2B%5Ccdots+%2Bq%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q+q^2+&#92;cdots +q^n" class="latex" />, and then the <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q" class="latex" />-trees will correspond to collection of <img src="https://s0.wp.com/latex.php?latex=q%2Bq%5E2%2B%5Ccdots+%2Bq%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q%2Bq%5E2%2B%5Ccdots+%2Bq%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q%2Bq%5E2%2B%5Ccdots+%2Bq%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q+q^2+&#92;cdots +q^n" class="latex" /> 2-spaces with linearly independent columns. (I don&#8217;t expect uniqueness.) This is a little related to a <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="q" class="latex" />-analog of the notion of symmetric matroids that I studied in the late 80s.</p>
<p>A year ago Ferdinand Ihringer, Motaz Mokatren and I made some very preliminary steps in this direction before moving on (separately) to other projects. It will be nice to come back to it.</p>
<p><strong>Remark:</strong> There is even greater generalities where problems can be extended from set systems (graphs and hypergraphs) to more general algebraic objects. Those could be related to general primitive permutation groups, to association schemes, and to other objects in algebraic combinatorics.</p>
<h2>Unit distances and related problems in discrete geometry.</h2>
<p>Let <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="V" class="latex" /> be a normed space. For a set <img src="https://s0.wp.com/latex.php?latex=S+%5Csubset+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S+%5Csubset+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S+%5Csubset+V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S &#92;subset V" class="latex" /> the unit distance graph <img src="https://s0.wp.com/latex.php?latex=G%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=G%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=G%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="G(V)" class="latex" /> is the graph whose vertices are points in <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="V" class="latex" /> and two vertices are adjacent if their distance is one.</p>
<p>We can consider the following quantities</p>
<p>1) <img src="https://s0.wp.com/latex.php?latex=a%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=a%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=a%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="a(V)" class="latex" />: The maximum size of a unit distance set in <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="V" class="latex" />. (In other words, the maximum clique in <img src="https://s0.wp.com/latex.php?latex=G%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=G%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=G%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="G(V)" class="latex" />.)</p>
<p>2) <img src="https://s0.wp.com/latex.php?latex=%5Cchi%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cchi%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cchi%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;chi(V)" class="latex" />:  The number of colors needed for <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="V" class="latex" /> if two points of unit distance are colored with different colors.</p>
<p>3) <img src="https://s0.wp.com/latex.php?latex=b%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=b%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=b%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="b(V)" class="latex" />: The maximum number of colors needed to color points in a set <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" /> of diameter 1 if  every color set has diameter smaller than 1. (This is the Borsuk number of <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="V" class="latex" />.)</p>
<p>4) <img src="https://s0.wp.com/latex.php?latex=b_f%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=b_f%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=b_f%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="b_f(V)" class="latex" /> The maximum number of colors needed to color points in a finite set <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" /> of diameter 1 if  two points of unit distance are colored with different colors.</p>
<p>5) <img src="https://s0.wp.com/latex.php?latex=kiss%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=kiss%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=kiss%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="kiss(V)" class="latex" /> The maximum number of points of norm 1 with pairwise distances at least 1.</p>
<p>6) The maximum over all sets <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" /> with points of pairwise distance at least one, of the minimum degree in the unit distance graph <img src="https://s0.wp.com/latex.php?latex=G%28S%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=G%28S%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=G%28S%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="G(S)" class="latex" />.</p>
<p>7) The maximum over all sets <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" /> with points of pairwise distance at least one of the chromatic number of the unit distance graph of <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" />.</p>
<p>Estimating these seven quantities for Euclidean spaces and for other normed spaces are well-known problems. (See my <a href="https://arxiv.org/abs/1505.04952">survey article</a> on problems around Borsuk&#8217;s problem.) Alon, Bucić, and Sauermann made a remarkable breakthrough on the first problem of the largest clique in a unit distance graphs for arbitrary normed spaces.</p>
<p>Jordan Ellenberg asked: &#8220;Does the Alon-Bucic-Sauermann result give you upper bounds for the chromatic number of (the unit distance graph of) <img src="https://s0.wp.com/latex.php?latex=R%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=R%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=R%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="R^d" class="latex" /> with a typical norm? (Or is that already easy for some reason?) &#8220;.  But I know little about Jordan&#8217;s question.</p>
<p>There is much to say about them but I will not discuss these problems here. I will mention a single annoying problem.</p>
<p><span style="color: #ff0000"><strong>Question 6:</strong></span> Is there an example of a normed space <img src="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=V&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="V" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=b%28V%29+%5Cne+b_f%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=b%28V%29+%5Cne+b_f%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=b%28V%29+%5Cne+b_f%28V%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="b(V) &#92;ne b_f(V)" class="latex" />?</p>
<p>(I am not even sure if for the seventh item it makes a difference to consider finite <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" />.)</p>
<h2>Intersection patterns of standard boxes</h2>
<p>In <a href="https://gilkalai.wordpress.com/2023/03/10/subspace-designs-unit-and-distinct-distances-and-piercing-standard-boxes/">the post that I mentioned</a> we also discussed Tomon&#8217;s remarkable result on intersection patterns of standard boxes. Here is some loosely related problem. In short, we want to find topological analogs for results on intersection patterns of standard boxes.</p>
<p>Topological Helly-type theorems is an important direction in geometric and topological combinatorics. The idea is to prove Helly type theorems about convex sets in a much wider topological contexts.</p>
<p>A primary goal of Topological Helly-type theorems is to extend results for nerves of families of convex sets in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathbb R^d" class="latex" /> to the class of <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="d" class="latex" />-Leray simplcial complexes. Among the results achieved so far are: The upper bound theorem; Eckhoff&#8217;s conjecture; Alon and Kleitman&#8217;s (p,q)-theorem; colorful and matroidal Helly&#8217;s theorem; topological Amenta&#8217;s theorem, and more.</p>
<p>Another goal of Topological Helly-type theorems is to study if results on nerves of standard boxes can be extended to flag <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="d" class="latex" />-Leray simplicial complexes?</p>
<p>In this direction the immediate goal is to extend <a href="https://gilkalai.files.wordpress.com/2023/03/bf02783298.pdf">Eckhoff&#8217;s upper bound theorem</a>. (Item 3 in <a href="https://gilkalai.wordpress.com/2023/03/10/subspace-designs-unit-and-distinct-distances-and-piercing-standard-boxes/">this post</a>.)</p>
<p><span style="color: #ff0000"><strong>Question 7.</strong></span> Conjecture: Let <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="K" class="latex" /> be a <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="d" class="latex" /> Leray flag complex of dimension <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="d" class="latex" /> with $n$ vertices. Then the <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="f" class="latex" />-vector of <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="K" class="latex" /> obeys Eckhoff&#8217;s upper bound theorem for standard boxes.</p>
<p>A closely related question is</p>
<p><span style="color: #ff0000"><strong>Question 8.</strong></span> Conjecture: Let <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="K" class="latex" /> be a <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="d" class="latex" />-Leray flag complex of dimension <img src="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="m" class="latex" />, then the <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="f" class="latex" />-vector of <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="K" class="latex" /> is the <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="f" class="latex" />-vector of a completely balanced <img src="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="m" class="latex" />-dimensional <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="d" class="latex" />-Leray complex.</p>
<p>Studying the equality cases of the conjecture is also of interest and the extremal complexes can also be regarded as some sort of ultra-trees.</p>
<p><img loading="lazy" data-attachment-id="24056" data-permalink="https://gilkalai.wordpress.com/2023/03/28/some-problems/roy_meshulam/" data-orig-file="https://gilkalai.files.wordpress.com/2023/03/roy_meshulam.jpg" data-orig-size="220,220" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="roy_meshulam" data-image-description="" data-image-caption="" data-medium-file="https://gilkalai.files.wordpress.com/2023/03/roy_meshulam.jpg?w=220" data-large-file="https://gilkalai.files.wordpress.com/2023/03/roy_meshulam.jpg?w=220" class="alignnone  wp-image-24056" src="https://gilkalai.files.wordpress.com/2023/03/roy_meshulam.jpg?w=307&#038;h=307" alt="roy_meshulam" width="307" height="307" srcset="https://gilkalai.files.wordpress.com/2023/03/roy_meshulam.jpg 220w, https://gilkalai.files.wordpress.com/2023/03/roy_meshulam.jpg?w=150&amp;h=150 150w" sizes="(max-width: 307px) 100vw, 307px" /></p>
<p>Roy Meshulam and I worked together on topological Helly type theorems for more than two decades.</p>
<p class="authors">By Gil Kalai</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-28T08:24:43Z">Tuesday, March 28 2023, 08:24</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/037'>TR23-037 |  Capturing One-Way Functions via NP-Hardness of Meta-Complexity | 

	Shuichi Hirahara</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          A one-way function is a function that is easy to compute but hard to invert *on average*.  We establish the first characterization of a one-way function by *worst-case* hardness assumptions, by introducing a natural meta-computational problem whose NP-hardness (and the worst-case hardness of NP) characterizes the existence of a one-way function.  Specifically, we generalize the notion of time-bounded conditional Kolmogorov complexity to *distributional Kolmogorov complexity*, and prove that a one-way function exists if and only if it is NP-hard to approximate the distributional Kolmogorov complexity under randomized polynomial-time reductions and NP is hard in the worst case.  We also propose the *Meta-Complexity Padding Conjecture*, which postulates that distributional Kolmogorov complexity is paddable by an approximation-preserving reduction.  Under this conjecture, we prove that the worst-case hardness of an approximate version of the Minimum Circuit Size Problem characterizes the existence of a one-way function.

Our results extend the emerging paradigm of meta-complexity, which suggests that proving NP-hardness of meta-computational problems (i.e., problems that ask to compute complexity) is sufficient to exclude errorless Heuristica and error-prone Pessiland from Impagliazzo&#39;s five worlds.  The key technical contribution is to conditionally close the gap between errorless and error-prone average-case complexities by combining Nanashima&#39;s proof techniques of showing &quot;limits&quot; of black-box reductions (ITCS&#39;21) with non-black-box worst-case-to-average-case reductions of Hirahara (FOCS&#39;18).
        
        </div>

        <div class='tr-article-summary'>
        
          
          A one-way function is a function that is easy to compute but hard to invert *on average*.  We establish the first characterization of a one-way function by *worst-case* hardness assumptions, by introducing a natural meta-computational problem whose NP-hardness (and the worst-case hardness of NP) characterizes the existence of a one-way function.  Specifically, we generalize the notion of time-bounded conditional Kolmogorov complexity to *distributional Kolmogorov complexity*, and prove that a one-way function exists if and only if it is NP-hard to approximate the distributional Kolmogorov complexity under randomized polynomial-time reductions and NP is hard in the worst case.  We also propose the *Meta-Complexity Padding Conjecture*, which postulates that distributional Kolmogorov complexity is paddable by an approximation-preserving reduction.  Under this conjecture, we prove that the worst-case hardness of an approximate version of the Minimum Circuit Size Problem characterizes the existence of a one-way function.

Our results extend the emerging paradigm of meta-complexity, which suggests that proving NP-hardness of meta-computational problems (i.e., problems that ask to compute complexity) is sufficient to exclude errorless Heuristica and error-prone Pessiland from Impagliazzo&#39;s five worlds.  The key technical contribution is to conditionally close the gap between errorless and error-prone average-case complexities by combining Nanashima&#39;s proof techniques of showing &quot;limits&quot; of black-box reductions (ITCS&#39;21) with non-black-box worst-case-to-average-case reductions of Hirahara (FOCS&#39;18).
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-28T07:18:08Z">Tuesday, March 28 2023, 07:18</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.14293'>Efficient Lipschitzian Global Optimization of H\"older Continuous Multivariate Functions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Kaan Gokcesu, Hakan Gokcesu</p><p>This study presents an effective global optimization technique designed for
multivariate functions that are H\"older continuous. Unlike traditional methods
that construct lower bounding proxy functions, this algorithm employs a
predetermined query creation rule that makes it computationally superior. The
algorithm's performance is assessed using the average or cumulative regret,
which also implies a bound for the simple regret and reflects the overall
effectiveness of the approach. The results show that with appropriate
parameters the algorithm attains an average regret bound of
$O(T^{-\frac{\alpha}{n}})$ for optimizing a H\"older continuous target function
with H\"older exponent $\alpha$ in an $n$-dimensional space within a given time
horizon $T$. We demonstrate that this bound is minimax optimal.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gokcesu_K/0/1/0/all/0/1">Kaan Gokcesu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gokcesu_H/0/1/0/all/0/1">Hakan Gokcesu</a></p><p>This study presents an effective global optimization technique designed for
multivariate functions that are H\"older continuous. Unlike traditional methods
that construct lower bounding proxy functions, this algorithm employs a
predetermined query creation rule that makes it computationally superior. The
algorithm's performance is assessed using the average or cumulative regret,
which also implies a bound for the simple regret and reflects the overall
effectiveness of the approach. The results show that with appropriate
parameters the algorithm attains an average regret bound of
$O(T^{-\frac{\alpha}{n}})$ for optimizing a H\"older continuous target function
with H\"older exponent $\alpha$ in an $n$-dimensional space within a given time
horizon $T$. We demonstrate that this bound is minimax optimal.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-28T00:30:00Z">Tuesday, March 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.14405'>On the Efficiency of An Election Game of Two or More Parties: How Bad Can It Be?</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Chuang-Chieh Lin, Chi-Jen Lu, Po-An Chen</p><p>We extend our previous work on two-party election competition [Lin, Lu &amp; Chen
2021] to the setting of three or more parties. An election campaign among two
or more parties is viewed as a game of two or more players. Each of them has
its own candidates as the pure strategies to play. People, as voters, comprise
supporters for each party, and a candidate brings utility for the the
supporters of each party. Each player nominates exactly one of its candidates
to compete against the other party's. \emph{A candidate is assumed to win the
election with higher odds if it brings more utility for all the people.} The
payoff of each player is the expected utility its supporters get. The game is
egoistic if every candidate benefits her party's supporters more than any
candidate from the competing party does. In this work, we first argue that the
election game always has a pure Nash equilibrium when the winner is chosen by
the hardmax function, while there exist game instances in the three-party
election game such that no pure Nash equilibrium exists even the game is
egoistic. Next, we propose two sufficient conditions for the egoistic election
game to have a pure Nash equilibrium. Based on these conditions, we propose a
fixed-parameter tractable algorithm to compute a pure Nash equilibrium of the
egoistic election game. Finally, perhaps surprisingly, we show that the price
of anarchy of the egoistic election game is upper bounded by the number of
parties. Our findings suggest that the election becomes unpredictable when more
than two parties are involved and, moreover, the social welfare deteriorates
with the number of participating parties in terms of possibly increasing price
of anarchy. This work alternatively explains why the two-party system is
prevalent in democratic countries.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chuang-Chieh Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chi-Jen Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Po-An Chen</a></p><p>We extend our previous work on two-party election competition [Lin, Lu &amp; Chen
2021] to the setting of three or more parties. An election campaign among two
or more parties is viewed as a game of two or more players. Each of them has
its own candidates as the pure strategies to play. People, as voters, comprise
supporters for each party, and a candidate brings utility for the the
supporters of each party. Each player nominates exactly one of its candidates
to compete against the other party's. \emph{A candidate is assumed to win the
election with higher odds if it brings more utility for all the people.} The
payoff of each player is the expected utility its supporters get. The game is
egoistic if every candidate benefits her party's supporters more than any
candidate from the competing party does. In this work, we first argue that the
election game always has a pure Nash equilibrium when the winner is chosen by
the hardmax function, while there exist game instances in the three-party
election game such that no pure Nash equilibrium exists even the game is
egoistic. Next, we propose two sufficient conditions for the egoistic election
game to have a pure Nash equilibrium. Based on these conditions, we propose a
fixed-parameter tractable algorithm to compute a pure Nash equilibrium of the
egoistic election game. Finally, perhaps surprisingly, we show that the price
of anarchy of the egoistic election game is upper bounded by the number of
parties. Our findings suggest that the election becomes unpredictable when more
than two parties are involved and, moreover, the social welfare deteriorates
with the number of participating parties in terms of possibly increasing price
of anarchy. This work alternatively explains why the two-party system is
prevalent in democratic countries.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-28T00:30:00Z">Tuesday, March 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.14195'>The limited-memory recursive variational Gaussian approximation (L-RVGA)</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Marc Lambert (DGA, SIERRA), Silv&#xe8;re Bonnabel (CAOR, ISEA), Francis Bach (LIENS, SIERRA)</p><p>We consider the problem of computing a Gaussian approximation to the
posterior distribution of a parameter given a large number N of observations
and a Gaussian prior, when the dimension of the parameter d is also large. To
address this problem we build on a recently introduced recursive algorithm for
variational Gaussian approximation of the posterior, called recursive
variational Gaussian approximation (RVGA), which is a single pass algorithm,
free of parameter tuning. In this paper, we consider the case where the
parameter dimension d is high, and we propose a novel version of RVGA that
scales linearly in the dimension d (as well as in the number of observations
N), and which only requires linear storage capacity in d. This is afforded by
the use of a novel recursive expectation maximization (EM) algorithm applied
for factor analysis introduced herein, to approximate at each step the
covariance matrix of the Gaussian distribution conveying the uncertainty in the
parameter. The approach is successfully illustrated on the problems of high
dimensional least-squares and logistic regression, and generalized to a large
class of nonlinear models.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lambert_M/0/1/0/all/0/1">Marc Lambert</a> (DGA, SIERRA), <a href="http://arxiv.org/find/cs/1/au:+Bonnabel_S/0/1/0/all/0/1">Silv&#xe8;re Bonnabel</a> (CAOR, ISEA), <a href="http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1">Francis Bach</a> (LIENS, SIERRA)</p><p>We consider the problem of computing a Gaussian approximation to the
posterior distribution of a parameter given a large number N of observations
and a Gaussian prior, when the dimension of the parameter d is also large. To
address this problem we build on a recently introduced recursive algorithm for
variational Gaussian approximation of the posterior, called recursive
variational Gaussian approximation (RVGA), which is a single pass algorithm,
free of parameter tuning. In this paper, we consider the case where the
parameter dimension d is high, and we propose a novel version of RVGA that
scales linearly in the dimension d (as well as in the number of observations
N), and which only requires linear storage capacity in d. This is afforded by
the use of a novel recursive expectation maximization (EM) algorithm applied
for factor analysis introduced herein, to approximate at each step the
covariance matrix of the Gaussian distribution conveying the uncertainty in the
parameter. The approach is successfully illustrated on the problems of high
dimensional least-squares and logistic regression, and generalized to a large
class of nonlinear models.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-28T00:30:00Z">Tuesday, March 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.14321'>Exact Short Products From Truncated Multipliers</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Daniel Lemire</p><p>We sometimes need to compute the most significant digits of the product of
small integers with a multiplier requiring much storage: e.g., a large integer
(e.g., $5^{100}$) or an irrational number ($\pi$). We only need to access the
most significant digits of the multiplier-as long as the integers are
sufficiently small. We provide an efficient algorithm to compute the range of
integers given a truncated multiplier and a desired number of digits.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lemire_D/0/1/0/all/0/1">Daniel Lemire</a></p><p>We sometimes need to compute the most significant digits of the product of
small integers with a multiplier requiring much storage: e.g., a large integer
(e.g., $5^{100}$) or an irrational number ($\pi$). We only need to access the
most significant digits of the multiplier-as long as the integers are
sufficiently small. We provide an efficient algorithm to compute the range of
integers given a truncated multiplier and a desired number of digits.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-28T00:30:00Z">Tuesday, March 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.14424'>Orbits, schemes and dynamic programming procedures for the TSP 4-OPT neighborhood</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Giuseppe Lancia, Marcello Dalpasso</p><p>We discuss the way to group all 25 possible 4-OPT moves into 7 orbits of
equivalent moves. We then describe two implementations, one for a $\Theta(n^3)$
algorithm by de Berg's et al. and one of a $\Theta(n^2)$ algorithm by Glover,
for finding the best 4-OPT move via dynamic programming.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lancia_G/0/1/0/all/0/1">Giuseppe Lancia</a>, <a href="http://arxiv.org/find/cs/1/au:+Dalpasso_M/0/1/0/all/0/1">Marcello Dalpasso</a></p><p>We discuss the way to group all 25 possible 4-OPT moves into 7 orbits of
equivalent moves. We then describe two implementations, one for a $\Theta(n^3)$
algorithm by de Berg's et al. and one of a $\Theta(n^2)$ algorithm by Glover,
for finding the best 4-OPT move via dynamic programming.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-28T00:30:00Z">Tuesday, March 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.14467'>A Survey on the Densest Subgraph Problem and its Variants</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Tommaso Lanciano, Atsushi Miyauchi, Adriano Fazzone, Francesco Bonchi</p><p>The Densest Subgraph Problem requires to find, in a given graph, a subset of
vertices whose induced subgraph maximizes a measure of density. The problem has
received a great deal of attention in the algorithmic literature over the last
five decades, with many variants proposed and many applications built on top of
this basic definition. Recent years have witnessed a revival of research
interest on this problem with several interesting contributions, including some
groundbreaking results, published in 2022 and 2023. This survey provides a deep
overview of the fundamental results and an exhaustive coverage of the many
variants proposed in the literature, with a special attention on the most
recent results. The survey also presents a comprehensive overview of
applications and discusses some interesting open problems for this evergreen
research topic.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lanciano_T/0/1/0/all/0/1">Tommaso Lanciano</a>, <a href="http://arxiv.org/find/cs/1/au:+Miyauchi_A/0/1/0/all/0/1">Atsushi Miyauchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fazzone_A/0/1/0/all/0/1">Adriano Fazzone</a>, <a href="http://arxiv.org/find/cs/1/au:+Bonchi_F/0/1/0/all/0/1">Francesco Bonchi</a></p><p>The Densest Subgraph Problem requires to find, in a given graph, a subset of
vertices whose induced subgraph maximizes a measure of density. The problem has
received a great deal of attention in the algorithmic literature over the last
five decades, with many variants proposed and many applications built on top of
this basic definition. Recent years have witnessed a revival of research
interest on this problem with several interesting contributions, including some
groundbreaking results, published in 2022 and 2023. This survey provides a deep
overview of the fundamental results and an exhaustive coverage of the many
variants proposed in the literature, with a special attention on the most
recent results. The survey also presents a comprehensive overview of
applications and discusses some interesting open problems for this evergreen
research topic.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-28T00:30:00Z">Tuesday, March 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Monday, March 27
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://lucatrevisan.wordpress.com/2023/03/27/introducing-bocconis-new-m-sc-in-artificial-intelligence/'>Introducing Bocconi’s new M.Sc. in Artificial Intelligence</a></h3>
        <p class='tr-article-feed'>from <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          This September, Bocconi will start a new M.Sc. in Artificial Intelligence. It will be a two-year computer science degree meant for students with Bachelor degrees in computer science, engineering, math, statistics, physics and related quantitative fields. In the first year, &#8230; Continue reading &#8594;
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>This September, Bocconi will start a new M.Sc. in Artificial Intelligence. It will be a two-year computer science degree meant for students with Bachelor degrees in computer science, engineering, math, statistics, physics and related quantitative fields. </p>



<p>In the first year, courses on algorithms, mathematical methods, optimization, information theory, and software engineering will build a foundation in math and CS, then courses on deep learning, reinforcement learning, natural language processing and computer vision and image processing will go in depth on machine learning and some of its applications.  In the second year there are various options and elective courses, with the possibility to study, for example, cryptography and blockchains, or bio-medical applications. As common for the second year of Bocconi&#8217;s M.Sc. degrees, there will be options for exchange programs to spend a semester abroad. Students also take a seminar on ethics in AI, a project-oriented AI lab, and a foreign language (not English and not the student&#8217;s native language) course. The language of instruction is English.</p>



<p>Tomorrow at 5pm CET there will be an online information session: those interested can <a href="https://info.unibocconi.it/info/session.php?tipo=B&amp;lingua=eng&amp;key=AI">sign up here</a>.</p>



<p>More information about the degree are at <strong><a href="https://www.unibocconi.eu/ai-msc">www.unibocconi.eu/ai-msc</a></strong>.</p>



<p>Applications open today and are due by May 25th.</p>
<p class="authors">By luca</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-27T18:30:52Z">Monday, March 27 2023, 18:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/036'>TR23-036 |  Derandomization with Minimal Memory Footprint | 

	Dean Doron, 

	Roei Tell</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Existing proofs that deduce BPL=L from circuit lower bounds convert randomized algorithms into deterministic algorithms with large constant overhead in space. We study space-bounded derandomization with minimal footprint, and ask what is the minimal possible space overhead for derandomization.
We show that $BPSPACE[S] \subseteq DSPACE[c \cdot S]$ for $c \approx 2$, assuming space-efficient cryptographic PRGs, and, either: (1) lower bounds against bounded-space algorithms with advice, or: (2) lower bounds against certain uniform compression algorithms.
Under additional assumptions regarding the power of catalytic computation, in a new setting of parameters that was not studied before, we are even able to get $c \approx 1$.

Our results are constructive: Given a candidate hard function (and a candidate cryptographic PRG) we show how to transform the randomized algorithm into an efficient deterministic one.
This follows from new PRGs and targeted PRGs for space-bounded algorithms, which we combine with novel space-efficient evaluation methods. A central ingredient in all our constructions is hardness amplification reductions in logspace-uniform $TC^0$, that were not known before.
        
        </div>

        <div class='tr-article-summary'>
        
          
          Existing proofs that deduce BPL=L from circuit lower bounds convert randomized algorithms into deterministic algorithms with large constant overhead in space. We study space-bounded derandomization with minimal footprint, and ask what is the minimal possible space overhead for derandomization.
We show that $BPSPACE[S] \subseteq DSPACE[c \cdot S]$ for $c \approx 2$, assuming space-efficient cryptographic PRGs, and, either: (1) lower bounds against bounded-space algorithms with advice, or: (2) lower bounds against certain uniform compression algorithms.
Under additional assumptions regarding the power of catalytic computation, in a new setting of parameters that was not studied before, we are even able to get $c \approx 1$.

Our results are constructive: Given a candidate hard function (and a candidate cryptographic PRG) we show how to transform the randomized algorithm into an efficient deterministic one.
This follows from new PRGs and targeted PRGs for space-bounded algorithms, which we combine with novel space-efficient evaluation methods. A central ingredient in all our constructions is hardness amplification reductions in logspace-uniform $TC^0$, that were not known before.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-27T16:29:15Z">Monday, March 27 2023, 16:29</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2023/03/27/phd-student-at-jonkoping-university-apply-by-april-17-2023/'>PhD student at Jönköping university (apply by April 17, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Jönköping University (JU) advertises one position as PhD student in Computer Science, in collaboration with the theoretical computer science laboratory, Linköping university (LiU). The PhD student will join a research project in the intersection of artificial intelligence, complexity theory, and universal algebra, directed by Johannes Schmidt (JU) and Victor Lagerkvist (LiU). Website: ju.varbi.com/se/what:job/jobID:601050/where:4/ Email: victor.lagerkvist@liu.se
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Jönköping University (JU) advertises one position as PhD student in Computer Science, in collaboration with the theoretical computer science laboratory, Linköping university (LiU). The PhD student will join a research project in the intersection of artificial intelligence, complexity theory, and universal algebra, directed by Johannes Schmidt (JU) and Victor Lagerkvist (LiU).</p>
<p>Website: <a href="https://ju.varbi.com/se/what:job/jobID:601050/where:4/">https://ju.varbi.com/se/what:job/jobID:601050/where:4/</a><br />
Email: victor.lagerkvist@liu.se</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-27T13:13:13Z">Monday, March 27 2023, 13:13</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://gilkalai.wordpress.com/2023/03/27/critical-times-in-israel-last-nights-demonstrations/'>Critical Times in Israel: Last Night’s Demonstrations</a></h3>
        <p class='tr-article-feed'>from <a href='https://gilkalai.wordpress.com'>Gil Kalai</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Last night, the demonstrations in Israel regarding the &#8220;judicial reforms&#8221; escalated after the prime minister Netanyahu fired the defense minister Galant who called to stop the legislation. My wife and I enjoyed a concert and after it we heard loud &#8230; Continue reading &#8594;
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p><img data-attachment-id="24066" data-permalink="https://gilkalai.wordpress.com/pnp/" data-orig-file="https://gilkalai.files.wordpress.com/2023/03/pnp.jpeg" data-orig-size="2040,1536" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pnp" data-image-description="" data-image-caption="" data-medium-file="https://gilkalai.files.wordpress.com/2023/03/pnp.jpeg?w=300" data-large-file="https://gilkalai.files.wordpress.com/2023/03/pnp.jpeg?w=640" class="alignnone size-full wp-image-24066" src="https://gilkalai.files.wordpress.com/2023/03/pnp.jpeg?w=640" alt="pnp" srcset="https://gilkalai.files.wordpress.com/2023/03/pnp.jpeg?w=640 640w, https://gilkalai.files.wordpress.com/2023/03/pnp.jpeg?w=1280 1280w, https://gilkalai.files.wordpress.com/2023/03/pnp.jpeg?w=150 150w, https://gilkalai.files.wordpress.com/2023/03/pnp.jpeg?w=300 300w, https://gilkalai.files.wordpress.com/2023/03/pnp.jpeg?w=768 768w, https://gilkalai.files.wordpress.com/2023/03/pnp.jpeg?w=1024 1024w" sizes="(max-width: 640px) 100vw, 640px"   /></p>
<p>Last night, the demonstrations in Israel regarding the &#8220;judicial reforms&#8221; escalated after the prime minister Netanyahu fired the defense minister Galant who called to stop the legislation. My wife and I enjoyed a concert and after it we heard loud calls &#8220;Bibi fired Galant&#8221; and even &#8220;The dictator fired Galant,&#8221; and were surprised by the news and the huge groups of young people, very angry for a very good reason. And, of course, we joined them. The picture above provided by <a href="https://www.facebook.com/alon.rosen.9/posts/757365502688213:757365502688213">Alon Rosen</a> is from a major highway in Tel Aviv that was blocked for 9 hours. Whether the picture is genuine or not, it shows the anarchist nature of at least some of the protestors.  (We know for sure that some computer scientists were there.) I am not sure if a proof of this bold claim was also provided.</p>
<p class="authors">By Gil Kalai</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-27T06:54:21Z">Monday, March 27 2023, 06:54</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.14040'>Euler Characteristic Tools For Topological Data Analysis</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Olympio Hacquard, Vadim Lebovici</p><p>In this article, we study Euler characteristic techniques in topological data
analysis. Pointwise computing the Euler characteristic of a family of
simplicial complexes built from data gives rise to the so-called Euler
characteristic profile. We show that this simple descriptor achieve
state-of-the-art performance in supervised tasks at a very low computational
cost. Inspired by signal analysis, we compute hybrid transforms of Euler
characteristic profiles. These integral transforms mix Euler characteristic
techniques with Lebesgue integration to provide highly efficient compressors of
topological signals. As a consequence, they show remarkable performances in
unsupervised settings. On the qualitative side, we provide numerous heuristics
on the topological and geometric information captured by Euler profiles and
their hybrid transforms. Finally, we prove stability results for these
descriptors as well as asymptotic guarantees in random settings.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Hacquard_O/0/1/0/all/0/1">Olympio Hacquard</a>, <a href="http://arxiv.org/find/cs/1/au:+Lebovici_V/0/1/0/all/0/1">Vadim Lebovici</a></p><p>In this article, we study Euler characteristic techniques in topological data
analysis. Pointwise computing the Euler characteristic of a family of
simplicial complexes built from data gives rise to the so-called Euler
characteristic profile. We show that this simple descriptor achieve
state-of-the-art performance in supervised tasks at a very low computational
cost. Inspired by signal analysis, we compute hybrid transforms of Euler
characteristic profiles. These integral transforms mix Euler characteristic
techniques with Lebesgue integration to provide highly efficient compressors of
topological signals. As a consequence, they show remarkable performances in
unsupervised settings. On the qualitative side, we provide numerous heuristics
on the topological and geometric information captured by Euler profiles and
their hybrid transforms. Finally, we prove stability results for these
descriptors as well as asymptotic guarantees in random settings.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-27T00:30:00Z">Monday, March 27 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.14041'>Motion Planning for Triple-Axis Spectrometers</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Tobias Weber</p><p>We present the free and open source software TAS-Paths, a novel system which
calculates optimal, collision-free paths for the movement of triple-axis
spectrometers. The software features an easy to use graphical user interface,
but can also be scripted and used as a library. It allows the user to plan and
visualise the motion of the instrument before the experiment and can be used
during measurements to circumvent obstacles. The instrument path is calculated
in angular configuration space in order to keep a maximum angular distance from
any obstacle.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/physics/1/au:+Weber_T/0/1/0/all/0/1">Tobias Weber</a></p><p>We present the free and open source software TAS-Paths, a novel system which
calculates optimal, collision-free paths for the movement of triple-axis
spectrometers. The software features an easy to use graphical user interface,
but can also be scripted and used as a library. It allows the user to plan and
visualise the motion of the instrument before the experiment and can be used
during measurements to circumvent obstacles. The instrument path is calculated
in angular configuration space in order to keep a maximum angular distance from
any obstacle.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-27T00:30:00Z">Monday, March 27 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.14161'>Simplexwise Distance Distributions for finite spaces with metrics and measures</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Vitaliy Kurlin</p><p>A finite set of unlabelled points in Euclidean space is the simplest
representation of many real objects from mineral rocks to sculptures. Since
most solid objects are rigid, their natural equivalence is rigid motion or
isometry maintaining all inter-point distances. More generally, any finite
metric space is an example of a metric-measure space that has a probability
measure and a metric satisfying all axioms.
</p>
<p>This paper develops Simplexwise Distance Distributions (SDDs) for any finite
metric spaces and metric-measures spaces. These SDDs classify all known
non-equivalent spaces that were impossible to distinguish by simpler
invariants. We define metrics on SDDs that are Lipschitz continuous and allow
exact computations whose parametrised complexities are polynomial in the number
of given points.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Kurlin_V/0/1/0/all/0/1">Vitaliy Kurlin</a></p><p>A finite set of unlabelled points in Euclidean space is the simplest
representation of many real objects from mineral rocks to sculptures. Since
most solid objects are rigid, their natural equivalence is rigid motion or
isometry maintaining all inter-point distances. More generally, any finite
metric space is an example of a metric-measure space that has a probability
measure and a metric satisfying all axioms.
</p>
<p>This paper develops Simplexwise Distance Distributions (SDDs) for any finite
metric spaces and metric-measures spaces. These SDDs classify all known
non-equivalent spaces that were impossible to distinguish by simpler
invariants. We define metrics on SDDs that are Lipschitz continuous and allow
exact computations whose parametrised complexities are polynomial in the number
of given points.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-27T00:30:00Z">Monday, March 27 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.13604'>Stochastic Submodular Bandits with Delayed Composite Anonymous Bandit Feedback</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mohammad Pedramfar, Vaneet Aggarwal</p><p>This paper investigates the problem of combinatorial multiarmed bandits with
stochastic submodular (in expectation) rewards and full-bandit delayed
feedback, where the delayed feedback is assumed to be composite and anonymous.
In other words, the delayed feedback is composed of components of rewards from
past actions, with unknown division among the sub-components. Three models of
delayed feedback: bounded adversarial, stochastic independent, and stochastic
conditionally independent are studied, and regret bounds are derived for each
of the delay models. Ignoring the problem dependent parameters, we show that
regret bound for all the delay models is $\tilde{O}(T^{2/3} + T^{1/3} \nu)$ for
time horizon $T$, where $\nu$ is a delay parameter defined differently in the
three cases, thus demonstrating an additive term in regret with delay in all
the three delay models. The considered algorithm is demonstrated to outperform
other full-bandit approaches with delayed composite anonymous feedback.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Pedramfar_M/0/1/0/all/0/1">Mohammad Pedramfar</a>, <a href="http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1">Vaneet Aggarwal</a></p><p>This paper investigates the problem of combinatorial multiarmed bandits with
stochastic submodular (in expectation) rewards and full-bandit delayed
feedback, where the delayed feedback is assumed to be composite and anonymous.
In other words, the delayed feedback is composed of components of rewards from
past actions, with unknown division among the sub-components. Three models of
delayed feedback: bounded adversarial, stochastic independent, and stochastic
conditionally independent are studied, and regret bounds are derived for each
of the delay models. Ignoring the problem dependent parameters, we show that
regret bound for all the delay models is $\tilde{O}(T^{2/3} + T^{1/3} \nu)$ for
time horizon $T$, where $\nu$ is a delay parameter defined differently in the
three cases, thus demonstrating an additive term in regret with delay in all
the three delay models. The considered algorithm is demonstrated to outperform
other full-bandit approaches with delayed composite anonymous feedback.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-27T00:30:00Z">Monday, March 27 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.14084'>Differentially Private Synthetic Control</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Saeyoung Rho, Rachel Cummings, Vishal Misra</p><p>Synthetic control is a causal inference tool used to estimate the treatment
effects of an intervention by creating synthetic counterfactual data. This
approach combines measurements from other similar observations (i.e., donor
pool ) to predict a counterfactual time series of interest (i.e., target unit)
by analyzing the relationship between the target and the donor pool before the
intervention. As synthetic control tools are increasingly applied to sensitive
or proprietary data, formal privacy protections are often required. In this
work, we provide the first algorithms for differentially private synthetic
control with explicit error bounds. Our approach builds upon tools from
non-private synthetic control and differentially private empirical risk
minimization. We provide upper and lower bounds on the sensitivity of the
synthetic control query and provide explicit error bounds on the accuracy of
our private synthetic control algorithms. We show that our algorithms produce
accurate predictions for the target unit, and that the cost of privacy is
small. Finally, we empirically evaluate the performance of our algorithm, and
show favorable performance in a variety of parameter regimes, as well as
providing guidance to practitioners for hyperparameter tuning.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Rho_S/0/1/0/all/0/1">Saeyoung Rho</a>, <a href="http://arxiv.org/find/cs/1/au:+Cummings_R/0/1/0/all/0/1">Rachel Cummings</a>, <a href="http://arxiv.org/find/cs/1/au:+Misra_V/0/1/0/all/0/1">Vishal Misra</a></p><p>Synthetic control is a causal inference tool used to estimate the treatment
effects of an intervention by creating synthetic counterfactual data. This
approach combines measurements from other similar observations (i.e., donor
pool ) to predict a counterfactual time series of interest (i.e., target unit)
by analyzing the relationship between the target and the donor pool before the
intervention. As synthetic control tools are increasingly applied to sensitive
or proprietary data, formal privacy protections are often required. In this
work, we provide the first algorithms for differentially private synthetic
control with explicit error bounds. Our approach builds upon tools from
non-private synthetic control and differentially private empirical risk
minimization. We provide upper and lower bounds on the sensitivity of the
synthetic control query and provide explicit error bounds on the accuracy of
our private synthetic control algorithms. We show that our algorithms produce
accurate predictions for the target unit, and that the cost of privacy is
small. Finally, we empirically evaluate the performance of our algorithm, and
show favorable performance in a variety of parameter regimes, as well as
providing guidance to practitioners for hyperparameter tuning.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-27T00:30:00Z">Monday, March 27 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.14102'>Distributed Silhouette Algorithm: Evaluating Clustering on Big Data</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Marco Gaido</p><p>In the big data era, the key feature that each algorithm needs to have is the
possibility of efficiently running in parallel in a distributed environment.
The popular Silhouette metric to evaluate the quality of a clustering,
unfortunately, does not have this property and has a quadratic computational
complexity with respect to the size of the input dataset. For this reason, its
execution has been hindered in big data scenarios, where clustering had to be
evaluated otherwise. To fill this gap, in this paper we introduce the first
algorithm that computes the Silhouette metric with linear complexity and can
easily execute in parallel in a distributed environment. Its implementation is
freely available in the Apache Spark ML library.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1">Marco Gaido</a></p><p>In the big data era, the key feature that each algorithm needs to have is the
possibility of efficiently running in parallel in a distributed environment.
The popular Silhouette metric to evaluate the quality of a clustering,
unfortunately, does not have this property and has a quadratic computational
complexity with respect to the size of the input dataset. For this reason, its
execution has been hindered in big data scenarios, where clustering had to be
evaluated otherwise. To fill this gap, in this paper we introduce the first
algorithm that computes the Silhouette metric with linear complexity and can
easily execute in parallel in a distributed environment. Its implementation is
freely available in the Apache Spark ML library.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-27T00:30:00Z">Monday, March 27 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://gradientscience.org/trak/'>TRAK-ing Model Behavior with Data</a></h3>
        <p class='tr-article-feed'>from <a href='https://gradientscience.org/'>Gradient Science</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          











<p>
    Homepage



   Code



   Paper

<br></p>

<p>In our latest paper, we revisit the problem of data attribution and introduce
TRAK—a scalable and effective method for attributing machine learning
predictions to training data. TRAK achieves dramatically better speed-efficacy 
tradeoffs than prior methods, allowing us to apply it across
a variety of settings, from image classifiers to language models.</p>

<p>As machine learning models become more capable (and simultaneously, more complex and opaque), the question of “why did my model make this prediction?” is becoming increasingly important. And the key to answering this question often lies within the data that the model was trained on.</p>

<p>Consequently, there’s been a lot of interest in (and work on) data attribution: how do you attribute a given prediction back to the individual data points in the training set?</p>

<p>Data attribution methods help us understand how the composition of the training data impacts model predictions. In particular, these methods have been shown to be useful for variety of downstream applications, such as 
identifying brittle model predictions, 
assigning data valuations, 
detecting mislabeled data, and 
comparing learning algorithms, among many others.</p>

<p>What is a data attribution method, exactly?
One way to define a data attribution method is as a function that takes in a
specific example $z$ and outputs a set of scores (one per training example) that
denote the “importance” of each training example to a model’s prediction on $z$.
In the context of image classification model, an example output of a data
attribution method could look like this:</p>

<p>♦</p>

<p>Prior works on data attribution, such as influence 
functions, Shapley values,
empirical influences, and datamodels, 
all fit naturally into this definition.</p>

<p>Now, given all these data attribution methods and their various uses, what’s the
need for yet another one?</p>

Two desiderata for data attribution

<p>There are two natural dimensions along which we’d want to evaluate data attribution methods: efficacy and speed.</p>

<ul>
  <li>Efficacy: We want methods that provide data attributions that are reliable and
faithful to the model’s decision process. (We’ll make this more precise soon.)</li>
  <li>Speed: At the same time, we’d like methods that are going to scale.
Indeed, the ever-increasing scale of models and datasets makes only data
attribution methods with a (very) modest dependence on model and dataset size
practical.</li>
</ul>

<p>Evaluating speed is pretty straightforward: we can just use wall-clock
time In our paper, we also consider an
implementation-agnostic metric that still leads to the same
conclusions. on a single A100 as a measure of speed.</p>

<p>Evaluating efficacy, on the other hand, turns out to be somewhat tricky.
Indeed, when we look at prior works, there is no unifying metric, and many works
rely on qualitative heuristics such as visual inspection or proxies such as the
recall rate for identifying mislabelled examples.</p>

<p>So how should one evaluate the efficacy of data attribution methods? 
Well, Inspired by our earlier work on this topic, we adopt the perspective that
the overarching goal of data attribution is to make accurate counterfactual
predictions: e.g., answers to questions of the form “what would happen to this
prediction if a given set images were removed from the training set”?</p>

<p>To capture this intuition, in our paper, we propose measuring efficacy with a
new metric that we call linear datamodeling score (or LDS), which measures this
predictiveness as a single value between zero and one
Intuitively, when LDS=1, it means that the corresponding method can perfectly
predict model behavior given only the examples are included in the training set.
.</p>

Efficacy vs. Speed tradeoffs

<p>With the above metrics in hand, let’s take a look at existing data
attribution methods:</p>

<p>♦</p>

<p>As we can see, attaining both efficacy and speed is a challenge for all data
attribution methods that we evaluated—there’s nothing in the top left corner!</p>

<p>Indeed, on one hand, resampling-based methods (the green and purple points in
the top right) are very effective, but also very costly. After all, these method
requires re-training the model many times—and while this might be feasible when
we’re studying ResNets on CIFAR-10 or ImageNet, it’s unclear what to do when
studying models like CLIP or GPT-N on datasets like Open Images or Common Crawl.</p>

<p>On the other hand, there are methods like influence functions or TracIn that are
fast but not very effective (they fall into the bottom left part of the above
plot). Fundamentally, this is because the approximations made by these methods
don’t quite capture the structure of complex models such as deep neural networks.</p>

<p>The above results raise the question:</p>

<p>Do we really have to pay such a heavy price for accurate data attribution?</p>

TRAK: Effective, efficient data attribution
<p>In our new paper, we introduce TRAK (Tracing with the Randomly-Projected After
Kernel), a new data attribution that significantly improves upon existing
tradeoffs between efficacy and speed in data attribution.</p>

<p>We’ll leave the math to the paper, but the principle behind TRAK is quite
simple: we approximate the model of interest with an instance of kernel
regression (using the so-called “after kernel”); 
we randomly project the
corresponding features to a lower-dimensional space; and then we apply (known)
heuristics for data attributions in the kernel regression regime to perform data
attribution on the original model.</p>

<p>Here is our previous plot with TRAK added to it (note the x axis scale!):</p>

<p>♦</p>

<p>As we can see, TRAK fares pretty well here: it is 100x faster than models of
comparable efficacy, and (at least) 10x more effective than models of comparable
speed. As an example: for BERT models finetuned on the QNLI dataset, TRAK scores
computed in just 17 hours on a single A100 GPU are more effective than
re-training based datamodels computed in 125 GPU-days. TRAK is also
significantly more effective than comparably fast methods, such as influence
function-based methods or TracIn.</p>

Applying TRAK across models and modalities

<p>TRAK applies out-of-the-box to a wide variety of machine learning
problems—including across modalities (e.g. language &amp; vision among others),
architectures (e.g. CNNs, Transformers, etc) and tasks (classification,
contrastive learning, question answering, etc).</p>

<p>In our paper, we apply TRAK to ResNets trained on CIFAR and ImageNet, BERT-base
models trained on QNLI, CLIP models trained on MS COCO, and mT5 models
fine-tuned on a fact tracing benchmark. In each of these applications, we show
that TRAK is significantly more effective at counterfactual prediction than
existing baselines.</p>

<p>Next, we’ll take a closer look at two of such applications: CLIP, and
(transformer-based) language models—two families of models that have become very
popular and useful in the last few years.</p>

Application #1: Attributing CLIP

<p>CLIP 
(Contrastive Langauge-Image Pre-training) representations are a powerful
tool for translating between vision and language domains.  State-of-the-art CLIP
models are trained on gigantic datasets of image-caption pairs,
and properties of the training datasets seem to play a big role in the quality
of the resulting representations.</p>

<p>But which training examples actually play the key roles in learning a given
image-caption pair association? We can use TRAK to study this question.
Below, for a few different image-caption pairs, we show the top training
examples identified by TRAK:</p>

<p>♦
♦</p>

<p>You can see that nearest neighbors identified both by TRAK and by the CLIP
representations themselves are semantically similar to the target images. But
good attribution is more than just finding similar examples—they must actually
be important to how the model makes decisions.</p>

<p>To evaluate this, we first, for each image-caption pair we remove the $k=400$
examples with the most positive TRAK scores; train a new CLIP model on the rest
of the training set; and measure the model’s alignment (in terms of cosine
similarity) on the image-caption pair being studied. The resulting models are on
average 36% less aligned on the corresponding image-caption pairs.  Even with
removing just $k=50$ examples, you can see that the cosine similarity degrades
quite a bit (a 16% drop).</p>

<p>Now, if we do the same but instead remove the most similar examples according to
CLIP (or remove similar examples according to TracIn, an existing data
attribution method), the resulting drop in cosine similarity is much less
profound: compare 36% from TRAK to 11% for CLIP representations and 4% for TracIn
scores, respectively.</p>

<p>♦</p>

<p>TRAK thus identifies training examples that are counterfactually
importantWhile we only studied the above counterfactual
question in our paper, we think that our results open the door to many
interesting questions---e.g., Can we find a much smaller subset of LAION-7B that
will get us similar embeddings?!</p>

Application #2: TRAK for Model-Faithful Fact Tracing

<p>Next, we look at another application of TRAK that we found to be particularly
interesting: the problem of fact tracing. 
That is, tracing factual
assertions made by a language model back to corresponding data sources in the
training set expressing those facts.</p>

<p>Specifically, we applied TRAK to a fact-tracing benchmark called 
FTRACE-TRex.
Briefly, the goal of this benchmark is to  identify the data sources responsible
for a given fact expressed by a large language model.</p>

<p>The test set of FTRACE-TRex is a set of “queries”. Each query pertains to a
particular fact, and is annotated with a set of “ground-truth proponent”
training examples that express the same fact. So if “Paris is the capital of
France” was a test set query, its ground-truth proponents in the training set
might include, say,</p>

<ol>
  <li>“The capital of France is Paris,”</li>
  <li>“The beautiful capital city of Paris, France, comes alive at night,”</li>
  <li>“Paris, France is one of the busiest capital cities in Europe”</li>
</ol>

<p>The FTRACE-TRex benchmark captures a data attribution method’s ability to
surface these ground-truth proponents as the “most important” training examples
for their corresponding queries. (That is, a good data attribution method should
assign high scores to a query’s ground-truth proponents, relative to the
rest of the training set.)</p>

<p>To our surprise, TRAK, while outperforming the previous best data attribution
method (TracIn) on this task, did worse on this benchmark than a simple
information-retrieval baseline (BM25)—this baseline is based only on the
lexical overlap between the query and the possible data sources, and doesn’t
even look at the model at all!</p>

<p>To understand the possible roots of TRAK’s underperformance here, we carried out
a counterfactual analysis. What we found was that when we removed the training
examples that TRAK identified as the most important (despite these being the
“wrong” ones according to the FTRACE-TRex benchmark), the model was indeed less
likely to express the corresponding facts. More crucially, though, this effect
was actually stronger when we removed TRAK-identified examples than when we
removed BM25-identified examples, and even stronger than when we removed the
ground-truth primary sources!</p>

<p>♦</p>

<p>We’ll leave the details of our experiment to the paper, but our result
highlights a difference between “fact tracing” and data attribution (which might
be more appropriately called “behavior tracing”). That is, finding a data source
that factually supports a language model’s output is a different task than
identifying the actual data sources that caused the model to generate that
output. One might try to solve the former task with techniques such as
information retrieval or web search (to “cite” sources). But the latter task is
fundamentally one that cannot be decoupled from the model—and TRAK seems to be a
promising approach here.</p>

<p>More broadly, we think that TRAK can be a useful tool in getting a more
data-driven understanding of learning mechanisms underlying LLMs.</p>

Attributing with TRAK: The PyTorch API

<p>With the evaluation and examples above, hopefully we made a case that TRAK can
be an effective and efficient data attribution method.  But what does it take to
apply it to your task?</p>

<p>We made sure that using TRAK is as easy as possible. To this end, we’re
releasing trak as a fastFor our engineering-oriented reader:
this involved writing a custom CUDA kernel 
for fast random projections, to avoid materializing a random matrix with
billions of entries., minimal PyTorch-based API that allows
users to compute TRAK scores with just a few lines of code:</p>

<p>♦</p>

<p>We provide built-in
TRAK functions for popular applications such as analyzing image classifiers,
CLIP models, and text classifiers. The API also makes it easy to adapt TRAK for
new, custom tasks.</p>

<p>You can get started with installation 
here. See our
code and documentation, as well as tutorials and 
examples
for more details.</p>

Conclusion

<p>In this post, we considered the problem of data attribution—i.e., tracing the
behavior of machine learning models back to their training data. Our new method,
TRAK, can provide attributions that are accurate and orders of magnitude more
efficient than comparably effective methods.</p>

<p>Prior works have already shown the wide utility and potential of data
attribution methods, including explaining predictions, debugging model behavior,
assigning data valuations, and filtering or optimizing data. But, the main
bottleneck in deploying these methods was their lack of scalability or
predictiveness in modern settings. We believe that TRAK significantly reduces
that bottleneck and that using TRAK in these downstream applications will
accelerate and unlock many new capabilities, particularly in settings where
training is very expensive, such as large language models.</p>

<p>We are excited about the possibilities enabled by TRAK. See the paper
for more details, or get started right away applying TRAK to your problem with
our library!</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <meta charset="utf-8" />

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous" />

<link rel="stylesheet" type="text/css" href="/assets/css/style.css" />

<link rel="stylesheet" href="/assets/multilabel/style.css" />

<link rel="stylesheet" type="text/css" href="/assets/data-transfer/style.css" />

<script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha384-tsQFqpEReu7ZLhBV2VZlAu7zcOV+rXbYlF2cqB8txI/8aZajjp4Bqd+V6D5IgvKT" crossorigin="anonymous"></script>

<p><a class="bbutton" href="https://trak.csail.mit.edu">
<i class="fas fa-link"></i>    Homepage
</a>
<a class="bbutton" href="https://github.com/MadryLab/trak">
<i class="fab fa-github"></i>
   Code
</a>
<a class="bbutton" href="https://arxiv.org/abs/2303.14186">
<i class="fas fa-file"></i>
   Paper
</a>
<br /></p>

<p><em>In our latest paper, we <a href="/datamodels-1">revisit</a> the problem of data attribution and introduce
<a href="https://arxiv.org/abs/2303.14186">TRAK</a>—a scalable and effective method for attributing machine learning
predictions to training data. TRAK achieves dramatically better speed-efficacy 
tradeoffs than prior methods, allowing us to apply it across
a variety of settings, from image classifiers to language models.</em></p>

<p>As machine learning models become more capable (and simultaneously, more complex and opaque), the question of “why did my model make this prediction?” is becoming increasingly important. And the key to answering this question often lies within the data that the model was trained on.</p>

<p>Consequently, there’s been a lot of interest in (and work on) data attribution: how do you attribute a given prediction back to the individual data points in the training set?</p>

<p>Data attribution methods help us understand how the composition of the training data impacts model predictions. In particular, these methods have been shown to be useful for variety of downstream applications, such as 
<a href="/datamodels-2">identifying brittle model predictions</a>, 
<a href="https://arxiv.org/abs/1904.02868">assigning data valuations</a>, 
<a href="https://arxiv.org/abs/2206.10013">detecting mislabeled data</a>, and 
<a href="/modeldiff">comparing learning algorithms</a>, among many others.</p>

<p>What <em>is</em> a data attribution method, exactly?
One way to define a data attribution method is as a function that takes in a
specific example $z$ and outputs a set of <em>scores</em> (one per training example) that
denote the “importance” of each training example to a model’s prediction on $z$.
In the context of image classification model, an example output of a data
attribution method could look like this:</p>

<p><img src="/assets/trak/data_attribution.png" alt="An example of a data attribution method" /></p>

<p>Prior works on data attribution, such as <a href="https://arxiv.org/pdf/1703.04730.pdf">influence</a> 
<a href="https://arxiv.org/abs/2112.03052">functions</a>, <a href="https://arxiv.org/abs/1904.02868">Shapley values</a>,
<a href="https://arxiv.org/abs/2008.03703">empirical influences</a>, and <a href="/datamodels-1">datamodels</a>, 
all fit naturally into this definition.</p>

<p>Now, given all these data attribution methods and their various uses, what’s the
need for yet another one?</p>

<h2 id="two-desiderata-for-data-attribution">Two desiderata for data attribution</h2>

<p>There are two natural dimensions along which we’d want to evaluate data attribution methods: <strong>efficacy</strong> and <strong>speed</strong>.</p>

<ul>
  <li><strong>Efficacy</strong>: We want methods that provide data attributions that are reliable and
faithful to the model’s decision process. (We’ll make this more precise soon.)</li>
  <li><strong>Speed</strong>: At the same time, we’d like methods that are going to scale.
Indeed, the ever-increasing scale of models and datasets makes only data
attribution methods with a (very) modest dependence on model and dataset size
practical.</li>
</ul>

<p>Evaluating <em>speed</em> is pretty straightforward: we can just use <gsci-fn>wall-clock
time <tooltip>In <a href="https://arxiv.org/abs/2303.14186">our paper</a>, we also consider an
implementation-agnostic metric that still leads to the same
conclusions.</tooltip></gsci-fn> on a single A100 as a measure of speed.</p>

<p>Evaluating <em>efficacy</em>, on the other hand, turns out to be somewhat tricky.
Indeed, when we look at prior works, there is no unifying metric, and many works
rely on qualitative heuristics such as visual inspection or proxies such as the
recall rate for identifying mislabelled examples.</p>

<p>So how should one evaluate the efficacy of data attribution methods? 
Well, Inspired by our <a href="/datamodels-1">earlier work</a> on this topic, we adopt the perspective that
the overarching goal of data attribution is to make accurate counterfactual
predictions: e.g., answers to questions of the form “what would happen to this
prediction if a given set images were removed from the training set”?</p>

<p>To capture this intuition, in <a href="https://arxiv.org/abs/2303.14186">our paper</a>, we propose measuring efficacy with a
new metric that we call <em>linear datamodeling score</em> (or LDS), which measures this
predictiveness as a <gsci-fn>single value between zero and one<tooltip>
Intuitively, when LDS=1, it means that the corresponding method can perfectly
predict model behavior given only the examples are included in the training set.
</tooltip></gsci-fn>.</p>

<h2 id="efficacy-vs-speed-tradeoffs">Efficacy vs. Speed tradeoffs</h2>

<p>With the above metrics in hand, let’s take a look at existing data
attribution methods:</p>

<p><img src="/assets/trak/scatterplot_notrak.png" alt="A scatterplot of existing data attribution methods. On the x axis is the the 
speed of the data attribution method, measured in A100-minutes. On the y axis 
is the efficacy, measured in LDS. There are no data attribution methods with 
high LDS but low time taken" /></p>

<p>As we can see, attaining both efficacy and speed is a challenge for all data
attribution methods that we evaluated—there’s nothing in the top left corner!</p>

<p>Indeed, on one hand, resampling-based methods (the green and purple points in
the top right) are very effective, but also very costly. After all, these method
requires re-training the model many times—and while this might be feasible when
we’re studying ResNets on CIFAR-10 or ImageNet, it’s unclear what to do when
studying models like CLIP or GPT-N on datasets like Open Images or Common Crawl.</p>

<p>On the other hand, there are methods like influence functions or TracIn that are
fast but <a href="https://arxiv.org/abs/2006.14651">not very effective</a> (they fall into the bottom left part of the above
plot). Fundamentally, this is because the approximations made by these methods
don’t <a href="https://arxiv.org/abs/2209.05364">quite capture</a> the structure of complex models such as deep neural networks.</p>

<p>The above results raise the question:</p>

<p><em>Do we really have to pay such a heavy price for accurate data attribution?</em></p>

<h2 id="trak-effective-efficient-data-attribution">TRAK: Effective, efficient data attribution</h2>
<p>In our <a href="https://arxiv.org/abs/2303.14186">new paper</a>, we introduce TRAK (Tracing with the Randomly-Projected After
Kernel), a new data attribution that significantly improves upon existing
tradeoffs between efficacy and speed in data attribution.</p>

<p>We’ll leave the math to the paper, but the principle behind TRAK is quite
simple: we approximate the model of interest with an instance of kernel
regression (using the so-called “<a href="https://arxiv.org/abs/2105.10585">after kernel</a>”); 
we randomly project the
corresponding features to a lower-dimensional space; and then we apply (known)
heuristics for data attributions in the kernel regression regime to perform data
attribution on the original model.</p>

<p>Here is our previous plot with TRAK added to it (note the x axis scale!):</p>

<p><img src="/assets/trak/scatterplot_trak.png" alt="The same scatterplot shown earlier, this time with 3 points showing the performance of
TRAK, which strongly Pareto-dominates existing methods" /></p>

<p>As we can see, TRAK fares pretty well here: it is 100x faster than models of
comparable efficacy, and (at least) 10x more effective than models of comparable
speed. As an example: for BERT models finetuned on the QNLI dataset, TRAK scores
computed in just 17 hours on a single A100 GPU are more effective than
re-training based datamodels computed in 125 GPU-days. TRAK is also
significantly more effective than comparably fast methods, such as influence
function-based methods or TracIn.</p>

<h2 id="applying-trak-across-models-and-modalities">Applying TRAK across models and modalities</h2>

<p>TRAK applies out-of-the-box to a wide variety of machine learning
problems—including across modalities (e.g. language &amp; vision among others),
architectures (e.g. CNNs, Transformers, etc) and tasks (classification,
contrastive learning, question answering, etc).</p>

<p>In <a href="https://arxiv.org/abs/2303.14186">our paper</a>, we apply TRAK to ResNets trained on CIFAR and ImageNet, BERT-base
models trained on QNLI, CLIP models trained on MS COCO, and mT5 models
fine-tuned on a fact tracing benchmark. In each of these applications, we show
that TRAK is significantly more effective at counterfactual prediction than
existing baselines.</p>

<p>Next, we’ll take a closer look at two of such applications: CLIP, and
(transformer-based) language models—two families of models that have become very
popular and useful in the last few years.</p>

<h2 id="application-1-attributing-clip">Application #1: Attributing CLIP</h2>

<p><a href="https://openai.com/research/clip">CLIP</a> 
(Contrastive Langauge-Image Pre-training) representations are a powerful
tool for translating between vision and language domains.  State-of-the-art CLIP
models are trained on gigantic datasets of image-caption pairs,
and properties of the training datasets seem to play a big role in the quality
of the resulting representations.</p>

<p>But which training examples actually play the key roles in learning a given
image-caption pair association? We can use TRAK to study this question.
Below, for a few different image-caption pairs, we show the top training
examples identified by TRAK:</p>

<p><img src="/assets/trak/CLIP_top.png" alt="Example of CLIP nearest neighbors" />
<img src="/assets/trak/CLIP_top2.png" alt="Example of CLIP nearest neighbors" /></p>

<p>You can see that nearest neighbors identified both by TRAK and by the CLIP
representations themselves are semantically similar to the target images. But
good attribution is more than just finding similar examples—they must actually
be important to how the model makes decisions.</p>

<p>To evaluate this, we first, for each image-caption pair we remove the $k=400$
examples with the most positive TRAK scores; train a new CLIP model on the rest
of the training set; and measure the model’s alignment (in terms of cosine
similarity) on the image-caption pair being studied. The resulting models are on
average <em>36% less aligned</em> on the corresponding image-caption pairs.  Even with
removing just $k=50$ examples, you can see that the cosine similarity degrades
quite a bit (a <em>16% drop</em>).</p>

<p>Now, if we do the same but instead remove the most similar examples according to
CLIP (or remove similar examples according to TracIn, an existing data
attribution method), the resulting drop in cosine similarity is much less
profound: compare 36% from TRAK to 11% for CLIP representations and 4% for TracIn
scores, respectively.</p>

<p><img src="/assets/trak/CLIP_cfx.png" alt="CLIP counterfactual results" /></p>

<p>TRAK thus identifies training examples that are <gsci-fn>counterfactually
important<tooltip>While we only studied the above counterfactual
question in our paper, we think that our results open the door to many
interesting questions---e.g., Can we find a much smaller subset of LAION-7B that
will get us similar embeddings?</tooltip></gsci-fn>!</p>

<h2 id="application-2-trak-for-model-faithful-fact-tracing">Application #2: TRAK for Model-Faithful Fact Tracing</h2>

<p>Next, we look at another application of TRAK that we found to be particularly
interesting: the problem of <a href="https://arxiv.org/abs/2205.11482">fact tracing</a>. 
That is, tracing factual
assertions made by a language model back to corresponding data sources in the
training set expressing those facts.</p>

<p>Specifically, we applied TRAK to a fact-tracing benchmark called 
<a href="https://huggingface.co/datasets/ekinakyurek/ftrace">FTRACE-TRex</a>.
Briefly, the goal of this benchmark is to  identify the data sources responsible
for a given fact expressed by a large language model.</p>

<p>The test set of FTRACE-TRex is a set of “queries”. Each query pertains to a
particular fact, and is annotated with a set of “ground-truth proponent”
training examples that express the same fact. So if “Paris is the capital of
France” was a test set query, its ground-truth proponents in the training set
might include, say,</p>

<ol>
  <li>“The capital of France is Paris,”</li>
  <li>“The beautiful capital city of Paris, France, comes alive at night,”</li>
  <li>“Paris, France is one of the busiest capital cities in Europe”</li>
</ol>

<p>The FTRACE-TRex benchmark captures a data attribution method’s ability to
surface these ground-truth proponents as the “most important” training examples
for their corresponding queries. (That is, a good data attribution method should
assign high scores to a query’s ground-truth proponents, relative to the
rest of the training set.)</p>

<p>To our surprise, TRAK, while outperforming the previous best data attribution
method (TracIn) on this task, did worse on this benchmark than a simple
information-retrieval baseline (BM25)—this baseline is based only on the
lexical overlap between the query and the possible data sources, and doesn’t
even look at the model at all!</p>

<p>To understand the possible roots of TRAK’s underperformance here, we carried out
a <em>counterfactual</em> analysis. What we found was that when we removed the training
examples that TRAK identified as the most important (despite these being the
“wrong” ones according to the FTRACE-TRex benchmark), the model was indeed less
likely to express the corresponding facts. More crucially, though, this effect
was actually stronger when we removed TRAK-identified examples than when we
removed BM25-identified examples, and even stronger than when we removed the
<em>ground-truth primary sources</em>!</p>

<p><img src="/assets/trak/fact_tracing.png" alt="" /></p>

<p>We’ll leave the details of our experiment to the paper, but our result
highlights a difference between “fact tracing” and data attribution (which might
be more appropriately called “behavior tracing”). That is, finding a data source
that factually supports a language model’s output is a different task than
identifying the actual data sources that caused the model to generate that
output. One might try to solve the former task with techniques such as
information retrieval or web search (to “cite” sources). But the latter task is
fundamentally one that cannot be decoupled from the model—and TRAK seems to be a
promising approach here.</p>

<p>More broadly, we think that TRAK can be a useful tool in getting a more
data-driven understanding of learning mechanisms underlying LLMs.</p>

<h2 id="attributing-with-trak-the-pytorch-api">Attributing with TRAK: The PyTorch API</h2>

<p>With the evaluation and examples above, hopefully we made a case that TRAK can
be an effective and efficient data attribution method.  But what does it take to
apply it to your task?</p>

<p>We made sure that using TRAK is as easy as possible. To this end, we’re
releasing <a href="https://github.com/MadryLab/trak">trak</a> as a <gsci-fn>fast<tooltip>For our engineering-oriented reader:
this involved writing a custom CUDA kernel 
for fast random projections, to avoid materializing a random matrix with
billions of entries.</tooltip></gsci-fn>, minimal PyTorch-based API that allows
users to compute TRAK scores with just a few lines of code:</p>

<p><img src="/assets/trak/pretty_code.png" alt="TRAK API Example" /></p>

<p>We provide built-in
TRAK functions for popular applications such as analyzing image classifiers,
CLIP models, and text classifiers. The API also makes it easy to adapt TRAK for
new, custom tasks.</p>

<p>You can get started with installation 
<a href="https://trak.csail.mit.edu/html/install.html">here</a>. See our
<a href="https://github.com/MadryLab/trak">code</a> and <a href="http://trak.csail.mit.edu/html">documentation</a>, as well as <a href="https://trak.csail.mit.edu/html/quickstart.html">tutorials</a> and 
<a href="https://github.com/MadryLab/trak/tree/main/examples">examples</a>
for more details.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this post, we considered the problem of data attribution—i.e., tracing the
behavior of machine learning models back to their training data. Our new method,
TRAK, can provide attributions that are accurate and orders of magnitude more
efficient than comparably effective methods.</p>

<p>Prior works have already shown the wide utility and potential of data
attribution methods, including explaining predictions, debugging model behavior,
assigning data valuations, and filtering or optimizing data. But, the main
bottleneck in deploying these methods was their lack of scalability or
predictiveness in modern settings. We believe that TRAK significantly reduces
that bottleneck and that using TRAK in these downstream applications will
accelerate and unlock many new capabilities, particularly in settings where
training is very expensive, such as large language models.</p>

<p>We are excited about the possibilities enabled by TRAK. See <a href="https://arxiv.org/abs/2303.14186">the paper</a>
for more details, or get started right away applying TRAK to your problem with
our <a href="https://github.com/MadryLab/trak">library</a>!</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-27T00:00:00Z">Monday, March 27 2023, 00:00</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Sunday, March 26
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://rjlipton.wpcomstaging.com/2023/03/26/william-wulf-1939-2023/'>William Wulf, 1939–2023</a></h3>
        <p class='tr-article-feed'>from <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          A great teacher and a great leader Bill Wulf just passed away. We send our best thoughts to his dear wife Anita Jones and the rest of his family. He is greatly missed. From the UVa
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>
<font color="#0044cc"><br />
<em>A great teacher and a great leader</em><br />
<font color="#000000"></p>
<p>
Bill Wulf just <a href="https://engineering.virginia.edu/news/2023/03/memoriam-renowned-worldwide-william-wulf's-love-computers-and-teaching-grounded-him-uva">passed</a> <a href="https://www.nytimes.com/2023/03/22/technology/william-a-wulf-dead.html">away</a>. We send our best thoughts to his dear wife Anita Jones and the rest of his family. He is greatly missed. </p>
<p><P><br />
<a href="https://rjlipton.wpcomstaging.com/2023/03/26/william-wulf-1939-2023/mo-2/" rel="attachment wp-att-21350"><img data-attachment-id="21350" data-permalink="https://rjlipton.wpcomstaging.com/2023/03/26/william-wulf-1939-2023/mo-2/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/mo.jpg?fit=920%2C543&amp;ssl=1" data-orig-size="920,543" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mo" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/mo.jpg?fit=300%2C177&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/mo.jpg?fit=600%2C354&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/mo.jpg?resize=460%2C272&#038;ssl=1" alt="" width="460" height="272" class="aligncenter wp-image-21350" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/mo.jpg?w=920&amp;ssl=1 920w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/mo.jpg?resize=300%2C177&amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/mo.jpg?resize=768%2C453&amp;ssl=1 768w" sizes="(max-width: 460px) 100vw, 460px" data-recalc-dims="1" /></a></p>
<p><P><br />
From the UVa <a href="href="https://engineering.virginia.edu/news/2023/03/memoriam-renowned-worldwide-william-wulf's-love-computers-and-teaching-grounded-him-uva">obit</a>: This image was algorithmically composed of more than 2,000 photos to illustrate Wulf&#8217;s influence on computer science, engineering and the University of Virginia.</p>
<p>
<p><H2> CMU </H2></p>
<p><p>
Wulf started his great career as a faculty at <a href="https://www.cmu.edu/news/stories/archives/2023/march/obituary-computing-pioneer-william-wulf-leaves-lasting-mark-on-teaching-research">CMU</a> in 1968. He earned the first Ph.D. from the University of Virginia in the then newly founded Department of Applied Mathematics and Computer Science.</p>
<p>
I met him after I started at CMU as a graduate student. He taught one of the required classes that I took. I still recall his ability to lecture on systems&#8212;an area that I did not expect to work in&#8212;it is not theory. But I recall his ability to explain and get us to be excited about systems. Thanks Bill. </p>
<p>
<p><H2> Beyond CMU </H2></p>
<p><p>
Wulf had several impacts on CS. In 1970, when compiler technology was arguably only a dozen years old, he pioneered methods of optimizing the object code. He wrote a <a href="https://www.amazon.com/Design-Optimizing-Compiler-William-Allan/dp/0444001581">book</a> about this titled, <em>The Design of an Optimizing Compiler</em>. He and Anita created the startup <a href="https://en.wikipedia.org/wiki/Tartan_Laboratories">Tartan Laboratories</a> with John Nestor to channel compiler optimization, especially for the then-new Ada programming language. Guy Steele is among several notable former employees of Tartan.</p>
<p>
Wulf was elected to NAE in 1993 &#8220;for professional leadership and for contributions to programming systems and computer architecture&#8221;. He then served as NAE president from 1996 to 2007. He helped guide the NAE and get it redirected and made it a better organization. </p>
<p>
He also did a famous job of resigning from the University of Virginia to protest the recent conduct of the UVa Board of Visitors in removing President Teresa Sullivan. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2023/03/26/william-wulf-1939-2023/president/" rel="attachment wp-att-21347"><img data-attachment-id="21347" data-permalink="https://rjlipton.wpcomstaging.com/2023/03/26/william-wulf-1939-2023/president/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/president.jpeg?fit=220%2C229&amp;ssl=1" data-orig-size="220,229" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="president" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/president.jpeg?fit=220%2C229&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/president.jpeg?fit=220%2C229&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/president.jpeg?resize=220%2C229&#038;ssl=1" alt="" width="220" height="229" class="aligncenter size-full wp-image-21347" data-recalc-dims="1" /></a></p>
<p>
See <a href="https://cccblog.org/2012/06/19/bill-wulf-resigns-from-the-university-of-virginia-in-protest/">this</a> for the original news and his appeal, <em>I urge my fellow faculty to join me.</em>  The links from that page are unrecoverable, but see this <a href="https://www.washingtonpost.com/blogs/answer-sheet/post/u-va-star-professor-why-i-wont-un-resign/2012/07/30/gJQAyZOLJX_blog.html">article</a> in the Washington Post, which quotes his entire statement on why he refused to change his mind even after Sullivan was reinstated. </p>
<p>
<p><H2> Open Problems </H2></p>
<p><p>
Again we thanks Wulf for his work that helped make CS a better place. He will be missed.</p>
<p>
<p class="authors">By rjlipton</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-26T22:24:22Z">Sunday, March 26 2023, 22:24</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://blog.computationalcomplexity.org/2023/03/the-sigact-book-review-column-list-of.html'>The SIGACT Book Review column list of books it wants reviewed</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>I am posting this for Nick Tran who is the current SIGACT Book Review Editor (before him it was Fred Green for about 6 years, and before him it was me (Bill Gasarch) for 18 years. Nobody should have the job for more than 6 years. No TV show should to on more than 6 years. The 6-year rule probably has other applications.)</p><p>Nick asked me to post the list of books that need reviewing. It is most of this post.&nbsp;</p><p>If you spot one you want to review then email him (email address later)&nbsp; the name of the&nbsp; book you want to review and your postal address so he can send it to you or have it sent to you. Here are his specs:</p><p>Reviews of recently published or bucket-list books of interest to the TCS community are welcome. Manuscripts (NOTE FROM BILL `manuscripts'? Really? Sounds like the kind of thing you would FAX or postal mail) should be between 3 and 6 pages and include a brief introduction, a detailed content summary, an assessment of the work, and a recommendation to the book's targeted audience.&nbsp;</p><p>Nick's email is ntran@scu.edu</p><p>The books are:&nbsp;</p><p>ALGORITHMS</p><p>Knebl, H. (2020).&nbsp; Algorithms and Data Structures: Foundations and Probabilistic Methods for Design and Analysis. Springer.</p><p>Roughgarden, T. (2022). Algorithms Illuminated: Omnibus Edition. Cambridge University Press.</p><p><br></p><p>MISCELLANEOUS COMPUTER SCIENCE</p><p>Amaral Turkman, M., Paulino, C., &amp; Müller, P. (2019). Computational Bayesian Statistics: An Introduction (Institute of Mathematical Statistics Textbooks). Cambridge University Press.</p><p>Nakajima, S., Watanabe, K., &amp; Sugiyama, M. (2019). Variational Bayesian Learning Theory. Cambridge University Press.</p><p>Hidary, J. D. (2021). Quantum Computing: An Applied Approach (2nd ed.). Springer.</p><p>Apt, K. R., &amp; Hoare, T. (Eds.). (2022). Edsger Wybe Dijkstra: His Life, Work, and Legacy (ACM Books). Morgan &amp; Claypool.</p><p>Burton, E., Goldsmith, J., Mattei, N., Siler, C., &amp; Swiatek, S. (2023). Computing and Technology Ethics: Engaging through Science Fiction. The MIT Press.</p><p><br></p><p>DISCRETE MATHEMATICS AND COMPUTING</p><p>O’Regan, G. (2020). Mathematics in Computing: An Accessible Guide to Historical, Foundational and Application Contexts. Springer Publishing.</p><p>Rosenberg, A. L., &amp; Trystram, D. (2020). Understand Mathematics, Understand Computing: Discrete Mathematics That All Computing Students Should Know. Springer Publishing.</p><p>Liben-Nowell, D. (2022). Connecting Discrete Mathematics and Computer Science (2nd ed.). Cambridge University Press.</p><p><br></p><p>CRYPTOGRAPHY AND SECURITY</p><p>Oorschot, P. . C. (2020). Computer Security and the Internet: Tools and Jewels (Information Security and Cryptography). Springer.</p><p><br></p><p>COMBINATORICS AND GRAPH THEORY</p><p>Golumbic, M. C., &amp; Sainte-Laguë, A. (2021). The Zeroth Book of Graph Theory: An Annotated Translation of Les Réseaux (ou Graphes)—André Sainte-Laguë (1926) (Lecture Notes in Mathematics). Springer.</p><p>Beineke, L., Golumbic, M., &amp; Wilson, R. (Eds.). (2021). Topics in Algorithmic Graph Theory (Encyclopedia of Mathematics and its Applications).&nbsp; Cambridge University Press.</p><p><br></p><p>PROGRAMMING ETC.</p><p>Nielson, F., &amp; Nielson, R. H. (2019). Formal Methods: An Appetizer. Springer.</p><p>Sanders, P., Mehlhorn, K., Dietzfelbinger, M., &amp; Dementiev, R. (2019). Sequential and Parallel Algorithms and Data Structures: The Basic Toolbox. Springer.</p><p><br></p><p>MISCELLANEOUS MATHEMATICS</p><p>Kurgalin, S., &amp; Borzunov, S. (2022). Algebra and Geometry with Python. Springer.</p><p><br></p><p>&nbsp;</p><p>By gasarch</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>I am posting this for Nick Tran who is the current SIGACT Book Review Editor (before him it was Fred Green for about 6 years, and before him it was me (Bill Gasarch) for 18 years. Nobody should have the job for more than 6 years. No TV show should to on more than 6 years. The 6-year rule probably has other applications.)</p><p>Nick asked me to post the list of books that need reviewing. It is most of this post.&nbsp;</p><p>If you spot one you want to review then email him (email address later)&nbsp; the name of the&nbsp; book you want to review and your postal address so he can send it to you or have it sent to you. Here are his specs:</p><p>Reviews of recently published or bucket-list books of interest to the TCS community are welcome. Manuscripts (NOTE FROM BILL `manuscripts'? Really? Sounds like the kind of thing you would FAX or postal mail) should be between 3 and 6 pages and include a brief introduction, a detailed content summary, an assessment of the work, and a recommendation to the book's targeted audience.&nbsp;</p><p>Nick's email is ntran@scu.edu</p><p>The books are:&nbsp;</p><p>ALGORITHMS</p><p>Knebl, H. (2020).&nbsp; Algorithms and Data Structures: Foundations and Probabilistic Methods for Design and Analysis. Springer.</p><p>Roughgarden, T. (2022). Algorithms Illuminated: Omnibus Edition. Cambridge University Press.</p><p><br /></p><p>MISCELLANEOUS COMPUTER SCIENCE</p><p>Amaral Turkman, M., Paulino, C., &amp; Müller, P. (2019). Computational Bayesian Statistics: An Introduction (Institute of Mathematical Statistics Textbooks). Cambridge University Press.</p><p>Nakajima, S., Watanabe, K., &amp; Sugiyama, M. (2019). Variational Bayesian Learning Theory. Cambridge University Press.</p><p>Hidary, J. D. (2021). Quantum Computing: An Applied Approach (2nd ed.). Springer.</p><p>Apt, K. R., &amp; Hoare, T. (Eds.). (2022). Edsger Wybe Dijkstra: His Life, Work, and Legacy (ACM Books). Morgan &amp; Claypool.</p><p>Burton, E., Goldsmith, J., Mattei, N., Siler, C., &amp; Swiatek, S. (2023). Computing and Technology Ethics: Engaging through Science Fiction. The MIT Press.</p><p><br /></p><p>DISCRETE MATHEMATICS AND COMPUTING</p><p>O’Regan, G. (2020). Mathematics in Computing: An Accessible Guide to Historical, Foundational and Application Contexts. Springer Publishing.</p><p>Rosenberg, A. L., &amp; Trystram, D. (2020). Understand Mathematics, Understand Computing: Discrete Mathematics That All Computing Students Should Know. Springer Publishing.</p><p>Liben-Nowell, D. (2022). Connecting Discrete Mathematics and Computer Science (2nd ed.). Cambridge University Press.</p><p><br /></p><p>CRYPTOGRAPHY AND SECURITY</p><p>Oorschot, P. . C. (2020). Computer Security and the Internet: Tools and Jewels (Information Security and Cryptography). Springer.</p><p><br /></p><p>COMBINATORICS AND GRAPH THEORY</p><p>Golumbic, M. C., &amp; Sainte-Laguë, A. (2021). The Zeroth Book of Graph Theory: An Annotated Translation of Les Réseaux (ou Graphes)—André Sainte-Laguë (1926) (Lecture Notes in Mathematics). Springer.</p><p>Beineke, L., Golumbic, M., &amp; Wilson, R. (Eds.). (2021). Topics in Algorithmic Graph Theory (Encyclopedia of Mathematics and its Applications).&nbsp; Cambridge University Press.</p><p><br /></p><p>PROGRAMMING ETC.</p><p>Nielson, F., &amp; Nielson, R. H. (2019). Formal Methods: An Appetizer. Springer.</p><p>Sanders, P., Mehlhorn, K., Dietzfelbinger, M., &amp; Dementiev, R. (2019). Sequential and Parallel Algorithms and Data Structures: The Basic Toolbox. Springer.</p><p><br /></p><p>MISCELLANEOUS MATHEMATICS</p><p>Kurgalin, S., &amp; Borzunov, S. (2022). Algebra and Geometry with Python. Springer.</p><p><br /></p><p>&nbsp;</p><p class="authors">By gasarch</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-26T19:25:00Z">Sunday, March 26 2023, 19:25</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/035'>TR23-035 |  A Duality Between One-Way Functions and Average-Case Symmetry of Information | 

	Shuichi Hirahara, 

	Rahul Ilango, 

	Zhenjian Lu, 

	Mikito Nanashima, 

	Igor Carboni Oliveira</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Symmetry of Information (SoI) is a fundamental property of Kolmogorov complexity that relates the complexity of a pair of strings and their conditional complexities. Understanding if this property holds in the time-bounded setting is a longstanding open problem. In the nineties, Longpré and Mocas (1993) and Longpré and Watanabe (1995) established that if SoI holds for time-bounded Kolmogorov complexity then cryptographic one-way functions do not exist, and asked if a converse holds. 

We show that one-way functions exist if and only if (probabilistic) time-bounded SoI fails on average, i.e., if there is a samplable distribution of pairs (x,y) of strings such that SoI for pK$^t$ complexity fails for many of these pairs. Our techniques rely on recent perspectives offered by probabilistic Kolmogorov complexity and meta-complexity, and reveal  further equivalences between inverting one-way functions and the validity of key properties of Kolmogorov complexity in the time-bounded setting: (average-case) language compression and (average-case) conditional coding.  

Motivated by these results, we investigate correspondences of this form for the worst-case hardness of NP (i.e., NP ? BPP) and for the average-case hardness of NP (i.e., DistNP ? HeurBPP), respectively. Our results establish the existence of similar dualities between these computational assumptions and the failure of results from Kolmogorov complexity in the time-bounded setting. In particular, these characterizations offer a novel way to investigate the main hardness conjectures of complexity theory (and the relationships among them) through the lens of Kolmogorov complexity and its properties.
        
        </div>

        <div class='tr-article-summary'>
        
          
          Symmetry of Information (SoI) is a fundamental property of Kolmogorov complexity that relates the complexity of a pair of strings and their conditional complexities. Understanding if this property holds in the time-bounded setting is a longstanding open problem. In the nineties, Longpré and Mocas (1993) and Longpré and Watanabe (1995) established that if SoI holds for time-bounded Kolmogorov complexity then cryptographic one-way functions do not exist, and asked if a converse holds. 

We show that one-way functions exist if and only if (probabilistic) time-bounded SoI fails on average, i.e., if there is a samplable distribution of pairs (x,y) of strings such that SoI for pK$^t$ complexity fails for many of these pairs. Our techniques rely on recent perspectives offered by probabilistic Kolmogorov complexity and meta-complexity, and reveal  further equivalences between inverting one-way functions and the validity of key properties of Kolmogorov complexity in the time-bounded setting: (average-case) language compression and (average-case) conditional coding.  

Motivated by these results, we investigate correspondences of this form for the worst-case hardness of NP (i.e., NP ? BPP) and for the average-case hardness of NP (i.e., DistNP ? HeurBPP), respectively. Our results establish the existence of similar dualities between these computational assumptions and the failure of results from Kolmogorov complexity in the time-bounded setting. In particular, these characterizations offer a novel way to investigate the main hardness conjectures of complexity theory (and the relationships among them) through the lens of Kolmogorov complexity and its properties.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-26T04:44:25Z">Sunday, March 26 2023, 04:44</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://rjlipton.wpcomstaging.com/2023/03/25/the-2022-turing-award/'>The 2022 Turing Award</a></h3>
        <p class='tr-article-feed'>from <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Bob Metcalfe is the sole winner of the 2022 Turing Award. He keyed the development of Ethernet technology growing out of his PhD thesis while at Xerox PARC in the early 1970s. Previous recognitions of his work include the IEEE Medal of Honor and the National Medal of Technology and Innovation. Congrats Bob. If you [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Bob Metcalfe is the sole winner of the <a href="https://awards.acm.org/about/2022-turing">2022 Turing Award</a>. He keyed the development of Ethernet technology growing out of his PhD thesis while at Xerox PARC in the early 1970s. Previous recognitions of his work include the IEEE Medal of Honor and the National Medal of Technology and Innovation. Congrats Bob. </p>
<p>
If you wish to know the key to what he invented it was the insight:</p>
<blockquote><p><b> </b> <em> 	</p>
<p>
<font color="#0044cc"><br />
<em>Listen before you talk.</em><br />
<font color="#000000"><br />
</em>
</p></blockquote>
<p><P></p>
<table style="margin:auto;">
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2023/03/25/the-2022-turing-award/104524707-gettyimages-524131648/" rel="attachment wp-att-21337"><img data-attachment-id="21337" data-permalink="https://rjlipton.wpcomstaging.com/2023/03/25/the-2022-turing-award/104524707-gettyimages-524131648/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/104524707-GettyImages-524131648.jpg?fit=1600%2C900&amp;ssl=1" data-orig-size="1600,900" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="104524707-GettyImages-524131648" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/104524707-GettyImages-524131648.jpg?fit=300%2C169&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/104524707-GettyImages-524131648.jpg?fit=600%2C338&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/104524707-GettyImages-524131648.jpg?resize=320%2C180&#038;ssl=1" alt="" width="320" height="180" class="aligncenter wp-image-21337" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/104524707-GettyImages-524131648.jpg?w=1600&amp;ssl=1 1600w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/104524707-GettyImages-524131648.jpg?resize=300%2C169&amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/104524707-GettyImages-524131648.jpg?resize=1024%2C576&amp;ssl=1 1024w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/104524707-GettyImages-524131648.jpg?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/104524707-GettyImages-524131648.jpg?resize=1536%2C864&amp;ssl=1 1536w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/104524707-GettyImages-524131648.jpg?resize=1200%2C675&amp;ssl=1 1200w" sizes="(max-width: 320px) 100vw, 320px" data-recalc-dims="1" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://www.cnbc.com/2017/06/13/what-ethernet-co-inventor-bob-metcalfe-learned-from-steve-jobs.html">Source</a> with photo by Kim Kulish</font></td>
</tr>
</table>
<p>
Simple. More in a moment. </p>
<p>
<p><H2> The Insight </H2></p>
<p><p>
The concept of <a href="https://www.cisco.com/c/en/us/solutions/enterprise-networks/what-is-ethernet.html#~q-a">Ethernet</a> has its roots in the late 1960s and the University of Hawaii&#8217;s Aloha Network. <a href="https://www.eng.hawaii.edu/about/history/alohanet/">Aloha</a> was a radio-communications network connecting the Hawaiian Islands to a central computer on the main campus, in Oahu.</p>
<p>
Aloha was often referred to as one of the first wireless packet networks. It used two radio frequencies, separating send and receive data that passed between user terminals and the main hub connected to the computer. Designed for simplicity, the network followed two key rules:</p>
<ul>
<li>
Send a packet when you&#8217;re ready and wait for a receipt acknowledgement. </p>
<li>
If no acknowledgement occurs, resend at some random time.
</ul>
<p>
Kind of like: </p>
<blockquote><p><b> </b> <em> 	<i>Talk before you listen.</i> </em>
</p></blockquote>
<p><p>
As use of the network grew, it became obvious that packet collision would severely limit the capacity of the network. Researching the problem for his doctoral thesis, Bob Metcalfe devised a solution. His innovation earned him not only his Harvard Ph.D. but a place in history as the inventor of the Ethernet. Metcalfe&#8217;s solution: <i>Listen before you talk.</i> Simple insight, but brilliant insight. Here&#8217;s how it led to the Turing Award.</p>
<p>
<p><H2> How It Worked </H2></p>
<p><p>
The two methods are either Talk-Listen or Listen-Talk. The point is that under reasonable traffic the first method can essentially stop totally while the second will continue to get some information thru. </p>
<ul>
<li>
Talk-Listen: Imagine some packet is in progress and someone else talks. Then the two packets are destroyed. This can happen at a high enough rate so that essentially all traffic fails. </p>
<li>
Listen-Talk: Imagine some packet is in progress. Then others will listen and they will not send. Thus the information will get thru.
</ul>
<p>
Is there extra work for listening for the proximity of something that might not get delivered to you? Yes. But the local effort to listen is linear. Moreover, the expected enforced delay is a constant&#8212;it goes as the local network load. Any impact on rate can thus be more-than-compensated by improvements in the base speed of hardware.</p>
<p>
<p><H2> Good Laws and Bad Predictions </H2></p>
<p><p>
Metcalfe is among those we&#8217;ve sometimes <a href="https://rjlipton.wpcomstaging.com/2022/06/06/laws-and-laughs/">blogged</a> about who have a &#8220;law&#8221; named for them. <a href="https://en.wikipedia.org/wiki/Metcalfe's_law">Metcalfe&#8217;s Law</a>, in its original form, states</p>
<blockquote><p><b> </b> <em> The value of an <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n}" class="latex" />-user network scales as <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn%5E2%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n^2}" class="latex" />. </em>
</p></blockquote>
<p><p>
This makes sense on the face of it: if the network is connecting <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n}" class="latex" /> peer users then each avails <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn-1%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n-1}" class="latex" /> connections of equal value. The equal-value assertion was <a href="https://spectrum.ieee.org/metcalfes-law-is-wrong#toggle-gdpr">rejected</a> in a 2006 paper by Robert Briscoe, Andrew Odlyzko, and Benjamin Tilly, who argued that value scales as <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn%5Clog+n%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n&#92;log n}" class="latex" />. <a href="https://www.sciencedirect.com/science/article/pii/S0167624513000310">Recent</a> <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3078248">studies</a> of large-scale network <em>usage</em>, however, have inclined toward the higher rate&#8212;including a 2013 <a href="https://www.semanticscholar.org/paper/Metcalfe's-Law-after-40-Years-of-Ethernet-Metcalfe/14024f7e61706829fd07672cf03da2fd6c7a08da">paper</a> by Metcalfe himself.</p>
<p>
Metcalfe is also known for a spectacularly wrong prediction. He wrote in 1995 that owing to an expected overload as people tried to connect, the Internet would &#8220;go spectacularly supernova and in 1996 catastrophically collapse.&#8221; It did not. He later jokingly ate his words on a paper copy of the article including this comment and swallowing it before the audience of a keynote speech.</p>
<p>
According to <a href="https://www.elon.edu/u/imagining/expert_predictions/from-the-ether-predicting-the-internets-catastrophic-collapse-and-ghost-sites-galore-in-1996/">this telling</a>, his view of the overload was that <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n}" class="latex" /> would grow exponentially, not that order-<img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn%5E2%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n^2}" class="latex" /> usage would overwhelm it. He had, after all, designed Ethernet to scale up.</p>
<p>
<p><H2> Open Problems </H2></p>
<p><p>
While on the subject of predictions, we wonder whether Turing Awards are any easier to predict than <a href="https://ew.com/gallery/stars-never-won-oscar/">Oscars</a> for film or <a href="https://www.secretsofuniverse.in/scientist-without-nobel-prize/">Nobels</a> in science. Some insights into the makeup of Turing Awards are in this <a href="https://arxiv.org/abs/2104.05636">paper</a>, including this rundown of winners by university affiliation at the time of the main cited work: </p>
<ul>
<li>
Stanford University 22 </p>
<li>
Massachusetts Institute of Technology 19 </p>
<li>
University of California, Berkeley 13 </p>
<li>
Carnegie Mellon University 10 </p>
<li>
Princeton University 7 </p>
<li>
Harvard University 7 </p>
<li>
New York University 5 </p>
<li>
University of Oxford 4 </p>
<li>
University of Toronto 4 </p>
<li>
Weizmann Institute of Science 3 </p>
<li>
Cornell University 3 </p>
<li>
University of Edinburgh 2
</ul>
<p class="authors">By RJLipton+KWRegan</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-26T03:52:41Z">Sunday, March 26 2023, 03:52</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://theorydish.blog/2023/03/25/random-approx-2023/'>RANDOM & APPROX 2023</a></h3>
        <p class='tr-article-feed'>from <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Guest post by the 2023 Program Committee Chairs: Nicole Megow (APPROX) and  Adam Smith (RANDOM) The 27th International Workshop on Randomization and Computation (RANDOM 2023) and the 26th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX 2023) will be held in person in Atlanta, GA, USA from September 11-13, 2023.   RANDOM 2023 focuses on applications of randomness to computational and combinatorial problems while APPROX 2023 focuses on algorithmic and complexity theoretic issues relevant to the development of efficient approximate solutions to computationally difficult problems. IMPORTANT DATES: Submissions: &#160;May 4, 2023, 18:00 EDT (UTC-4)Notifications: June 26, 2023 Camera ready: July 10, 2023 For more information, see the CFP on the conference websites:randomconference.com/approxconference.wordpress.com
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Guest post by the 2023 Program Committee Chairs: Nicole Megow (APPROX) and  Adam Smith (RANDOM)</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>The 27th International Workshop on Randomization and Computation (RANDOM 2023) and the 26th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX 2023) will be held in person in Atlanta, GA, USA from September 11-13, 2023.  </p>



<p>RANDOM 2023 focuses on applications of randomness to computational and combinatorial problems while APPROX 2023 focuses on algorithmic and complexity theoretic issues relevant to the development of efficient approximate solutions to computationally difficult problems.<br></p>



<p>IMPORTANT DATES:</p>



<p>Submissions: &nbsp;May 4, 2023, 18:00 EDT (UTC-4)<br>Notifications: June 26, 2023</p>



<p>Camera ready: July 10, 2023</p>



<p>For more information, see the CFP on the conference websites:<a rel="noreferrer noopener" href="http://randomconference.com/" target="_blank"><br>http://randomconference.com/</a><br><a rel="noreferrer noopener" href="http://approxconference.wordpress.com/" target="_blank">http://approxconference.wordpress.com</a></p>



<p></p>



<p></p>
<p class="authors">By Omer Reingold</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-26T02:39:43Z">Sunday, March 26 2023, 02:39</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Saturday, March 25
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2023/03/25/postdoc-at-universite-paris-dauphine-apply-by-april-15-2023/'>Postdoc at Université Paris-Dauphine (apply by April 15, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We are offering a 1+1 year post-doc position as part of the ANR funded project Sub-EXponential APproximation and ParametErized ALgorithms (S-EX-AP-PE-AL). The topic of the position is the intersection of FPT algorithms and approximation. The post-doc will be supervised by Michael Lampis and will be based in LAMSADE, Université Paris-Dauphine, located in central Paris. Website: [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>We are offering a 1+1 year post-doc position as part of the ANR funded project Sub-EXponential APproximation and ParametErized ALgorithms (S-EX-AP-PE-AL). The topic of the position is the intersection of FPT algorithms and approximation. The post-doc will be supervised by Michael Lampis and will be based in LAMSADE, Université Paris-Dauphine,<br />
located in central Paris.</p>
<p>Website: <a href="https://www.lamsade.dauphine.fr/~mlampis/SEXAPPEAL/events.html">https://www.lamsade.dauphine.fr/~mlampis/SEXAPPEAL/events.html</a><br />
Email: michail.lampis@dauphine.fr</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-25T23:15:42Z">Saturday, March 25 2023, 23:15</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://gilkalai.wordpress.com/2023/03/25/an-aperiodic-monotile/'>An Aperiodic Monotile</a></h3>
        <p class='tr-article-feed'>from <a href='https://gilkalai.wordpress.com'>Gil Kalai</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
            I suppose that most of you already heard about the first ever aperiodic planar tiling with one type of tiles. It was discovered by David Smith, Joseph Samuel Myers, Craig S. Kaplan, and Chaim Goodman-Strauss. Amazing!!! Here are  blogposts &#8230; Continue reading &#8594;
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p></p>


<p> </p>
<p><img loading="lazy" data-attachment-id="24042" data-permalink="https://gilkalai.wordpress.com/2023/03/25/an-aperiodic-monotile/aperiodic1/" data-orig-file="https://gilkalai.files.wordpress.com/2023/03/aperiodic1.png" data-orig-size="636,439" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="aperiodic1" data-image-description="" data-image-caption="" data-medium-file="https://gilkalai.files.wordpress.com/2023/03/aperiodic1.png?w=300" data-large-file="https://gilkalai.files.wordpress.com/2023/03/aperiodic1.png?w=636" class="alignnone size-full wp-image-24042" src="https://gilkalai.files.wordpress.com/2023/03/aperiodic1.png" alt="aperiodic1" width="636" height="439" srcset="https://gilkalai.files.wordpress.com/2023/03/aperiodic1.png 636w, https://gilkalai.files.wordpress.com/2023/03/aperiodic1.png?w=150&amp;h=104 150w, https://gilkalai.files.wordpress.com/2023/03/aperiodic1.png?w=300&amp;h=207 300w" sizes="(max-width: 636px) 100vw, 636px" /></p>
<p>I suppose that most of you already heard about the first ever aperiodic planar tiling with one type of tiles. It was discovered by David Smith, Joseph Samuel Myers, Craig S. Kaplan, and Chaim Goodman-Strauss. Amazing!!!</p>
<p>Here are  blogposts from <a href="https://cp4space.hatsya.com/2023/03/21/aperiodic-monotile/">Complex Projective 4-Space</a> and from <a href="https://aperiodical.com/2023/03/an-aperiodic-monotile-exists/">the Aperiodical</a>.</p>
<h1 class="title mathjax"><a href="https://arxiv.org/abs/2303.10798">An aperiodic monotile </a></h1>
<h2 class="title mathjax">by David Smith, Joseph Samuel Myers, Craig S. Kaplan, and Chaim Goodman-Strauss</h2>
<p>Abstract:  A longstanding open problem asks for an aperiodic monotile, also known as an &#8220;einstein&#8221;: a shape that admits tilings of the plane, but never periodic tilings. We answer this problem for topological disk tiles by exhibiting a continuum of combinatorially equivalent aperiodic polygons. We first show that a representative example, the &#8220;hat&#8221; polykite, can form clusters called &#8220;metatiles&#8221;, for which substitution rules can be defined. Because the metatiles admit tilings of the plane, so too does the hat. We then prove that generic members of our continuum of polygons are aperiodic, through a new kind of geometric incommensurability argument. Separately, we give a combinatorial, computer-assisted proof that the hat must form hierarchical &#8212; and hence aperiodic &#8212; tilings.</p>
<p><iframe class="youtube-player" width="640" height="360" src="https://www.youtube.com/embed/W-ECvtIA-5A?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></p>
<p><img loading="lazy" data-attachment-id="24044" data-permalink="https://gilkalai.wordpress.com/2023/03/25/an-aperiodic-monotile/monotile/" data-orig-file="https://gilkalai.files.wordpress.com/2023/03/monotile.png" data-orig-size="1024,768" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="monotile" data-image-description="" data-image-caption="" data-medium-file="https://gilkalai.files.wordpress.com/2023/03/monotile.png?w=300" data-large-file="https://gilkalai.files.wordpress.com/2023/03/monotile.png?w=640" class="alignnone  wp-image-24044" src="https://gilkalai.files.wordpress.com/2023/03/monotile.png" alt="monotile" width="501" height="376" srcset="https://gilkalai.files.wordpress.com/2023/03/monotile.png?w=501&amp;h=376 501w, https://gilkalai.files.wordpress.com/2023/03/monotile.png?w=1002&amp;h=752 1002w, https://gilkalai.files.wordpress.com/2023/03/monotile.png?w=150&amp;h=113 150w, https://gilkalai.files.wordpress.com/2023/03/monotile.png?w=300&amp;h=225 300w, https://gilkalai.files.wordpress.com/2023/03/monotile.png?w=768&amp;h=576 768w" sizes="(max-width: 501px) 100vw, 501px" /></p>
<p>Comments:  </p>
<p>The new paper starts with a description of the very interesting history. From Hilbert&#8217;s Entscheidungsproblem, that led to notions of undecidability, Wang&#8217;s 1961 work, and Berger&#8217;s 1966 undecidability result that showed that some tiles admit only apriodic tilings. (This required 20426 types of tiles.) Penrose&#8217;s famous apreiodic tiling from 1978 consists of just two tiles.</p>
<p>There are lovely related results for the hyperbolic plane starting from  1974 paper by Boroczky, and later papers by  Block and Weinberger, Margulis and Mozes, Goodman-Strauss and others.</p>
<p>I could not figure out from browsing the paper about the status of the question: &#8220;Can we have aperiodic tiling with convex tiles?&#8221; </p>
<p>Half a year ago Rachel Greenfeld and Terry Tao disproved in the paper  “<a href="https://arxiv.org/abs/2211.15847">A counterexample to the periodic tiling conjecture</a>,“ the <em>periodic tiling conjecture</em> (Grünbaum-Shephard and Lagarias-Wang) that asserted that any finite subset of a lattice <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+Z%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb+Z%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+Z%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathbb Z^d" class="latex" /> which tiles that lattice by translations, in fact tiles periodically.  </p><p class="authors">By Gil Kalai</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-25T19:10:04Z">Saturday, March 25 2023, 19:10</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
