<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.4 (2022-04-12) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2022-11-22T14:34:44Z">Tuesday, November 22 2022, 14:34</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>Sébastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Tuesday, November 22
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://windowsontheory.org/2022/11/22/ai-will-change-the-world-but-wont-take-it-over-by-playing-3-dimensional-chess/'>AI will change the world, but won’t take it over by playing “3-dimensional chess”.</a></h3>
        <p class='tr-article-feed'>from <a href='https://windowsontheory.org'>Windows on Theory</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          By Boaz Barak and&#160;Ben Edelman [Cross-posted on Lesswrong (pending moderation); See also Boaz’s posts on longtermism and AGI via scaling , as well as other &#8220;philosophizing&#8221; posts.] [Disclaimer:&#160;Predictions are very hard, especially about the future. In fact, this is one of the points of this essay. Hence, while for concreteness, we phrase our claims as if we &#8230; Continue reading AI will change the world, but won’t take it over by playing “3-dimensional chess”.
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>By <a href="https://www.boazbarak.org/">Boaz Barak</a> and&nbsp;<a href="https://www.benjaminedelman.com/">Ben Edelman</a></p>



<p><em>[Cross-posted on <a href="https://www.lesswrong.com/">Lesswrong</a> (pending moderation);  See also Boaz’s posts on </em><a href="https://windowsontheory.org/2022/05/23/why-i-am-not-a-longtermist/"><em><u>longtermism</u></em></a><em> and </em><a href="https://windowsontheory.org/2022/06/27/injecting-some-numbers-into-the-agi-debate/"><em><u>AGI via scaling</u></em></a> , as well as other &#8220;<a href="https://windowsontheory.org/category/philosophizing/">philosophizing</a>&#8221; posts.<em>]</em></p>



<p><em>[Disclaimer:&nbsp;Predictions are very hard, especially about the future. In fact, this is one of the points of this essay. Hence, while for concreteness, we phrase our claims as if we are confident about them, these are not mathematically proven facts. However we do believe that the claims below are more likely to be true than false, and, even more confidently, believe some of the ideas herein are underrated in current discussions around risks from future AI systems.]</em></p>



<p>In the past, the word “computer” was used to denote a person that performs calculations. Such people were highly skilled and were crucial to scientific enterprises. As described in the book “<a href="https://en.wikipedia.org/wiki/Hidden_Figures_(book)"><u>Hidden Figures</u></a>”, until the 1960s, NASA still used human computers for the space mission. However, these days a $10 calculator can instantly perform calculations beyond the capabilities of every human on earth.</p>



<p>On a high level, the situation in Chess and other games is similar. Humans used to be the reigning champions in Chess and Go, but have now been surpassed by computers. Yet, while the success of computers in performing calculations has not engendered fears of them “taking over the world,” the growing powers of AI systems have more people increasingly worried about their long-term implications. Some reasons why the success of AI systems such as&nbsp;<a href="https://www.deepmind.com/blog/alphazero-shedding-new-light-on-chess-shogi-and-go"><u>AlphaZero</u></a> in Go and Chess is more concerning than the success of calculation programs include</p>



<ol>
<li>Unlike when working with numerical computation programs, it seems that in Chess and Go humans are entirely “unnecessary.” There is no need to have a “human in the loop”. Computer systems are so powerful that no meaningful competition is possible between even the best human players and software running on commodity laptops.<sup><a href="#footnotes">[1]</a></sup><br>&nbsp;</li>



<li>Unlike the numerical algorithms used for calculations, we do not understand the inner workings of AI chess systems, especially ones trained without any hand-designed knowledge. These systems are to a large extent “black boxes,” which even their creators do not fully understand and hence cannot fully predict or control.<br>&nbsp;</li>



<li>Moreover, AlphaZero was trained using a paradigm known as&nbsp;<a href="https://en.wikipedia.org/wiki/Reinforcement_learning"><u>reinforcement learning</u></a> or RL (see also this&nbsp;<a href="https://rltheorybook.github.io/"><u>book</u></a>). At a high level, RL can be described as training an agent to learn a&nbsp;<em>strategy</em> (i.e., a rule to decide on a move or action based on the history of all prior ones) in order to maximize a long-term reward (e.g., “win the game”). The result is a system that is capable of executing actions that may seem wrong in the short term (e.g., sacrificing a queen) but will help achieve the long-term goal.&nbsp;</li>
</ol>



<p>While RL so far has had very limited success outside specific realms such as games or low-complexity settings, the success of (non-RL) deep learning systems such as&nbsp;<a href="https://en.wikipedia.org/wiki/GPT-3"><u>GPT-3</u></a> or<a href="https://openai.com/dall-e-2/"><u> Dall-E</u></a> in open-ended text or image generation has raised fears of future AI systems that could both act in the real world, interacting with humans, physical, and digital systems, and do so in the pursuit of long term goals that may not be “aligned” with the interests of humanity. The fear is that such systems could become so powerful that they could end up destroying much or all of humanity. We refer to the above scenario as the&nbsp;<strong>loss of control</strong> scenario. It is distinct from other potential risks of Artificial Intelligence, including the risks of AI being used by humans to develop more lethal weapons, better ways for repressive regimes to surveil their population or more effective ways of spreading misinformation.</p>



<p>In this essay,&nbsp;<strong>we claim that the “loss of control” scenario rests on a few key assumptions that are not justified by our current understanding of artificial intelligence research</strong>. (This doesn’t mean the assumptions are necessarily wrong—just that we don’t believe the preponderance of the evidence supports them.)&nbsp; To be clear, we are not “AI skeptics” by any means. We fully believe that over the next few decades, AI will continue to make breakthrough advances, and AI systems will surpass current human performance in many creative and technical fields, including, but not limited to, software engineering, hacking, marketing, visual design, (at least some components of) scientific discovery, and more. We are also not “techno-optimists.” The world already faces risks, and even existential ones, from the actions of humans. People who have had control over nuclear weapons over the course of history include Joseph Stalin, Kim Jong-un, Vladimir Putin, and many others whose moral judgment is suspect, to say the least. Nuclear weapons are not the only way humans can and have caused suffering on a mass scale; whether it is biological, chemical, or even so-called “conventional” weapons, climate change, exploitation of resources and people, or others, humans have a long history of pain and destruction. Like any new technology, AI will be (and in fact already has been) used by humans for warfare, manipulations, and other illicit goals. These risks are real and should be studied, but are not the focus of this essay.</p>



<h2><strong>Our argument: an executive summary.</strong></h2>



<p>The loss of control scenario is typically described as a “battle” between AIs and humans, in which AIs would eventually win due to their superior abilities. However, unlike in Chess games, humans can and will use all the tools at their disposal, including many tools (e.g., code-completion engines, optimizers for protein folding, etc..) that are currently classified as “Artificial Intelligence”. So to understand the balance of power, we need to distinguish between systems or agents that have only&nbsp;<strong>short-term goals</strong>, versus systems that&nbsp;<strong>plan their own long-term strategies</strong>.&nbsp;</p>



<p>The distinction above applies not just to artificial systems but also to human occupations as well. As an example, software developers, architects, engineers, or artists have&nbsp;<em>short-term</em> goals, in the sense that they provide some particular&nbsp;<em>product</em> (piece of software, design for a bridge, artwork, scientific paper) that can stand and be evaluated on its own merits. In contrast, leaders of companies and countries set&nbsp;<em>long-term goals</em> in the sense that they need to come up with a strategy that will yield benefits in the long run and cannot be assessed with confidence until it is implemented.<sup><a href="#footnotes">[2</a><a href="#fnh8xhqamyb87">]</a></sup>&nbsp;</p>



<p>We already have at least partial “short-term AI”, even if not at the level of replacing e.g., human software engineers. The existence of successful “long-term AI” that can come up with strategies which are enacted over a scale of, say, years is still an open question, but for the sake of this essay we accept that assumption.</p>



<p>We believe that when evaluating the loss-of-control scenario, the relevant competition is not between humans and AI systems, but rather between humans aided with short-term AI systems and long-term AI systems (themselves possibly aided with short-term components). One thought experiment we have in mind is a competition between two firms: one with a human CEO, but with AI engineers and advisors, and the other a fully AI firm.</p>



<p>While it might seem “obvious” that eventually AI would be far superior to humans in all endeavors, including being a CEO, we argue that this is not so obviously the case. We agree that future AIs could possess superior information processing and cognitive skills &#8211; a.k.a. “intelligence” &#8211; compared to humans. But the evidence so far suggests the&nbsp;<strong>advantages of these skills would be much more significant in some fields than in others</strong>. We believe that this is uncontroversial &#8211; for example, it’s not far-fetched to claim that AI would make much better chess players than kindergarten teachers. Specifically, there are&nbsp;<strong>“diminishing returns”</strong> for superior information-processing capabilities in the context of setting<strong> longer-term goals or strategies</strong>. The long time horizon and the relevance of interactions among high numbers of agents (who are themselves often difficult to predict) make real-life large-scale systems&nbsp;<strong>“chaotic”</strong> in the sense that even with superior analytic abilities, they are still unpredictable (see Figure 1).</p>



<p>As a consequence, we believe the<strong> main fields where AI systems will yield advantages will be in short-term domains</strong>. An AI engineer will be much more useful than an AI CEO (see also Table 2). We do not claim that it would be impossible to build an AI system that can conceive and execute long-term plans; only that this would not be where AI would have a “competitive advantage”. Short-term goals that can be evaluated and graded also mesh much better with the current paradigm of training AI systems on vast amounts of data.</p>



<p>We believe it&nbsp;<strong>will be possible to construct very useful AIs with only short-term goals,</strong> and in fact that the vast majority of AI’s power will come from such short-term systems. Even if a long-term AI system is built, it will likely&nbsp;<strong>not have a significant advantage over humans assisted with short-term AIs</strong>. There can be many risks even from short-term AI systems, but such machines cannot by design have any long-term goals, including the goal of taking over the world and killing all humans.<sup><a href="#footnotes">[3</a><a href="#fnk2m16hjnqf">]</a></sup></p>



<p><strong>Perspective.&nbsp;</strong>Our analysis also has a lesson for AI safety research. Traditionally, approaches to mitigate the behavior of bad actors include</p>



<ul>
<li><strong>Prevention:&nbsp;</strong>We prevent break-ins by putting locks on our doors, we prevent hacks by securing our systems, etc…&nbsp;</li>



<li><strong>Deterrence:&nbsp;</strong>Another way we prevent bad actions is by ensuring that the negative consequences for these actions will outweigh benefits. This is one basis for the penal system, as well as the “mutually assured destruction” paradigm that has kept Russia and US from a nuclear war.</li>



<li><strong>Alignment:</strong> We try to educate children and adults and socialize them to our values, so they are not motivated to pursue the actions we consider as bad.</li>
</ul>



<p>Much of AI safety research (wrt to the “loss of control” scenario) has been focused on the third approach, with the expectation that these systems may be so powerful that prevention and deterrence will be impossible. However, it is unclear to us that this will be the case. For example, it may well be that humans, aided by short-term AI systems, could vastly expand the scope of formally verified secure systems, and so prevent hacking attacks against sensitive resources. A huge advantage of research on prevention is that it is highly relevant not just to protect against hypothetical future bad AI actors, but also against current malicious humans. Such research might greatly&nbsp;<em>benefit&nbsp;</em>from advances in AI code-completion engines and other tools, hence belying the notion that there is a “zero-sum game” between “AI safety” and “AI capabilities” research.&nbsp;</p>



<p>Furthermore, one advantage of studying AI systems, as opposed to other organisms, is that we can try to extract useful modules and representations for them. (Indeed, this is already done in “transfer learning.”) Hence, it may be possible to extract useful and beneficial “short-term AI” even from long-term systems. Such restricted systems would still give most of the utility, but with less risk. Once again, increasing the capabilities of short-term AI systems will empower humans that are assisted by such systems.<br>&nbsp;</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img src="https://lh5.googleusercontent.com/CrDF9kngAXpxxa1YLkXY1hdD7K2YMgMgvc8U8joLPJDX-PZyXIGMqR21cd7ebrszWUCK_VJEN2DcpHcb9NLbi6Y3Fi_HKlRzvirnv5wv_K2DW9md2s2WVpghmkoppI2Qros6HDeGRcwh7ZZHO9dWCAult2H-m1xjeiTgwy0O1GXtFShyNuk4EsopMR0rFQ" alt="" width="566" height="383" /></figure></div>


<p><strong>Figure 1:</strong> Cartoon of the feasibility of predicting future events and the level of ability (i.e., cognitive skill / compute / data) required to do so (approximately) optimally. As the horizon grows, events have more inherent uncertainty and also require more skills/data to predict. However, many realistic systems are&nbsp;<em>chaotic</em> and become unpredictable at some finite horizon.<sup><a href="#footnotes">[4</a><a href="#fnmrfsb0zx5hf">]</a></sup>&nbsp; At that point, even sophisticated agents cannot predict better than baseline heuristics, which require only a bounded level of skill.<br>&nbsp;</p>



<figure class="wp-block-table"><table><tbody><tr><td><em>Profession</em></td><td><em>Cognitive Score (standard deviations)</em></td><td><em>Annual Earnings&nbsp;</em></td></tr><tr><td><strong>Mayors</strong></td><td><strong>6.2 ( ≈ +0.6σ )</strong></td><td><strong>679K SEK</strong></td></tr><tr><td><strong>Parliamentarians</strong></td><td><strong>6.4 ( ≈ +0.7σ )</strong></td><td><strong>802K SEK</strong></td></tr><tr><td><strong>CEOs (10-24 employees)</strong></td><td><strong>5.8 ( ≈ +0.4σ )</strong></td><td><strong>675K SEK</strong></td></tr><tr><td><strong>CEOs (25-249 employees)</strong></td><td><strong>6.2 ( ≈ +0.6σ )</strong></td><td><strong>1,046K SEK</strong></td></tr><tr><td><strong>CEOs (≥ 250 employees)</strong></td><td><strong>6.7 ( ≈ +0.85σ )</strong></td><td><strong>1,926K SEK</strong></td></tr><tr><td>Medical Doctors</td><td>7.4 ( ≈ +1.2σ )</td><td>640K SEK</td></tr><tr><td>Lawyers and Judges</td><td>6.8 ( ≈ +0.9σ )</td><td>568K SEK</td></tr><tr><td>Economists</td><td>7 ( ≈ +1σ )</td><td>530K SEK</td></tr><tr><td>Political Scientists</td><td>6.8 ( ≈ +0.9σ )</td><td>513 SEK</td></tr></tbody></table></figure>



<p><strong>Table 2:</strong> Cognitive scores for Swedish men in various “elite” occupations, based on Swedish army entrance examinations, taken from&nbsp;<a href="https://academic.oup.com/qje/article-abstract/132/4/1877/3859758?redirectedFrom=fulltext"><u>Dal Bó et al</u></a> (Table II). Emphases ours: bold text corresponds to jobs that (in our view) require longer horizon decision-making across time or number of people. Note that despite being apparently less cognitively demanding, the “bold” professions are higher paying.</p>



<h2><strong>A digression: what is intelligence</strong></h2>



<p>Merriam-Webster&nbsp;<a href="https://www.merriam-webster.com/dictionary/intelligence"><u>defines</u></a> intelligence as “the skilled use of reason”, “the ability to learn or understand or to deal with new or trying situations”, or “to apply knowledge to manipulate one&#8217;s environment or to think abstractly.” Intelligence is similar to&nbsp;<em>computation</em>, in the sense that its main components are the ability to take in observations (aka “inputs”) and use reasoning (aka “algorithms”) to decide on actions (aka “outputs”). In fact, in the currently dominant paradigm of AI, performance is primarily determined by the amount of computation performed during learning, and AI systems consist of enormous homogeneous circuits executing a series of simple operations on (a large quantity of) inputs and learned knowledge.&nbsp;<a href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies"><u>Bostrom</u></a> (Chapter 3) defines three forms of “superintelligence”: “speed superintelligence”, “collective superintelligence” and “quality superintelligence”. In the language of computing, speed super-intelligence corresponds to clock speed of processors, while collective super-intelligence corresponds to massive parallelism. “Quality superintelligence” is not well defined, but is presumably some type of emergent phenomenon from passing some thresholds of speed and parallelism.</p>



<p><br>A fundamental phenomenon in computing is&nbsp;<em>universality:</em> there are many&nbsp;<em>restricted</em> computational models (finite state automata, context-free grammars, simply-typed lambda calculus), but once a computational model passes a certain threshold or&nbsp;<em>phase transition</em>, it becomes universal (a.k.a. “Turing complete”), and all universal models are equivalent to one another in computational power. For example, in a cellular automata, even though each cell is very restricted (can only store a constant amount of memory and process a finite rule based only on the state of its immediate neighbors), given enough cells we can simulate any arbitrarily complex machine.<sup><a href="#footnotes">[5]</a></sup>&nbsp; Once a system passes the universality transition, it is not bottlenecked any more by the complexity of an individual unit, but rather by the resources in the system as a whole.</p>



<p>In the animal kingdom, we seem to have undergone a similar phase transition, whereby humans are qualitatively more intelligent than any other animal or creature. It also seems to be the case that with the invention of language, the printing press, and the Internet, we (like cellular automata) are able to combine large numbers of humans to achieve feats of collective intelligence that are beyond any one individual. In particular, the fruits of the scientific revolution of the 1500-1600s increased the scale of GDP by 10,000-fold (to the extent such comparisons are meaningful) and the distance we can measure in space a trillion-fold, all with the same brains used by our hunter-gatherer ancestors (or&nbsp;<a href="https://www.frontiersin.org/articles/10.3389/fevo.2022.963568/full"><u>maybe</u></a> somewhat&nbsp;<a href="https://www.frontiersin.org/articles/10.3389/fevo.2021.742639/full"><u>smaller</u></a> ones).&nbsp;</p>



<p><a href="https://www.pnas.org/doi/full/10.1073/pnas.1100290108"><u>Arguably</u></a>, the fact humans are far better than chimpanzees at culturally transmitting knowledge is more significant than the gap in intelligence between individuals of the two species. Ever since the development of language, the intelligence of an individual human has&nbsp;<em>not</em> been a bottleneck for the achievements of humanity. The brilliance of individuals like Newton may have been crucial for speeding up the Scientific Revolution, but there have been brilliant individuals for millennia. The crucial difference between Newton and Archimedes is not that Newton was smarter, but rather that he lived at a later time and thus was able to stand on the shoulders of more giants. As another example, a collection of humans, aided by Internet-connected computers, can do much better at pretty much any intelligence feat (including but not limited to IQ exams) than any single human.&nbsp;<br>&nbsp;</p>



<figure class="wp-block-image"><img src="https://lh3.googleusercontent.com/ENKzULhufaFT8CI2c_l-YKNl5XldYKJA8CjBtw9TrV6NFao1pEhyk9pmyDHSsA1iY0aZG-zJxv1wVb7zaF6zaZnyptxKs2CXe5zaS-40ZR_m_Sf59X2HorWz1xZdGFd-aSfhdgpjPPRfYpj5OWBbZMqkpCZEj9sdeGIcRXN1tjjxucVZPUcgbKyHiGLmkA" alt="" /></figure>



<p><strong>Figure 3:</strong> Measures of human progress both in terms of GDP and the scale of objects we can measure. Taken from&nbsp;<a href="https://windowsontheory.org/2022/05/03/philosophy-of-science-and-the-blockchain-a-book-review/"><u>this blog post</u></a>, with the first figure from&nbsp;<a href="https://ourworldindata.org/grapher/world-gdp-over-the-last-two-millennia"><u>Our World in Data</u></a>, and data for second figure from Terence Tao’s&nbsp;<a href="https://terrytao.wordpress.com/2010/10/10/the-cosmic-distance-ladder-ver-4-1/"><u>cosmic ladder presentation</u></a>.</p>



<p><br>The “loss of control” scenario posits a&nbsp;<em>second phase transition</em>, whereby once AI systems become more powerful, they would not merely enable humans to achieve more objectives quicker but would themselves become as qualitatively superior to humans as humans are to other animals. We are suggesting an alternative future scenario, in which while AI would provide powerful new capabilities to human society that can (and unfortunately likely will) be used for ill as well as good, the AI systems themselves would not be the inevitable leaders of this society.</p>



<p>Indeed, our societies and firms do not currently select our leaders to be the top individuals in intellectual capacity. The evidence is very limited that “natural talent for leadership” (to the extent it exists) is as measurable and transferable as talent for chess, math, or athletics. There are many examples of leaders who have been extremely successful in one setting but failed in another which seems rather similar.<sup><a href="#footnotes">[6]</a></sup>&nbsp;</p>



<p>Whether or not an AI system should be considered an “individual” is a matter for debate, but regardless, it is not at all clear that such individuals would be the leaders of the society, rather than being employed in domains such as software development and scientific discovery, where their superior information-processing capabilities would provide the most competitive advantage. Bostrom (Table 8 in Chapter 6) lists several potential “cognitive superpowers” that an AI system might develop. One category is&nbsp;<em>“hacking”</em>,&nbsp;<em>“technology research”</em>, and&nbsp;<em>“economic productivity”</em>. These are skills that correspond to jobs that are not in the domain of CEOs or leaders, but rather engineers, middle managers, scientists, etc. AI systems may well be able to assist or even replace such individuals, but this does not mean such systems will be the leaders of companies or countries.</p>



<p>Another task Bostrom considers is&nbsp;<em>“intelligence amplification”</em> which is the ability to improve AI systems. Again, it is quite possible that AI systems would help in improving other or the same AI systems, but this on its own does not imply that they would become infinitely powerful. Specifically, if indeed stronger AI would arrive through “scaling” of massive computational resources, then there would be&nbsp;<a href="https://windowsontheory.org/2022/06/27/injecting-some-numbers-into-the-agi-debate/"><u>some hard limits</u></a> on the ability to improve AI’s power solely through software updates. It is not at all clear that in terms of energy efficiency, AI systems would be much better (if at all) than humans. If the gains from scaling are far more important than gains from improved algorithms/architectures, then intelligence amplification might be primarily a function of resource acquisition rather than algorithmic research.</p>



<p>A third task listed is&nbsp;<em>“social manipulation.”</em> Here we must admit we are skeptical. Anyone who has ever tried to convince a dog to part with a bone or a child with a toy could attest to the diminishing returns that an intelligence advantage has in such a situation.&nbsp;</p>



<p>Finally, Boston lists the cognitive superpower of&nbsp;<em>“strategizing”</em>, which is the ability to make long-term plans to achieve distant goals. This is the point we focus on in this essay. In short, our belief is that the chaotic nature of the real world implies diminishing returns to “three-dimensional chess” strategies that are beyond the comprehension of mere humans. Hence we do not believe that this would be a domain where AI systems have a strong competitive advantage.</p>



<h2><br><strong>A thought experiment: “The AI CEO vs. the AI advisor”</strong></h2>



<p>Before we delve into the technical(-ish) analysis, let us consider a thought experiment. At its heart, our argument is that the power of AI systems, present and future, will not come from the ability to make long-term strategic plans (“three-dimensional chess”) but rather from the ability to produce pieces of work that can be evaluated on their own terms. In short, we believe that even if a long-term malicious AI system is constructed, it will not have an insurmountable advantage over humans that are assisted with short-term AIs. To examine this, let us imagine two possible scenarios for how future AI could assist humans in making strategic decisions, such as running a company:</p>



<ul>
<li>In the&nbsp;<strong>“AI Advisor”</strong> model, leaders could use AI to come up with simulations of the impact of decisions and possibly make some suggestions. However, humans would ultimately make the decision and evaluate their results. Key for this is that an AI would be able not just to produce a recommendation for a decision but explain how this decision would lead to improvement in some interpretable metric (e.g., revenue, market share, etc..). For example, a decision might be “let’s sell this product at a loss so we can increase our market share.”<br>&nbsp;</li>



<li>In the&nbsp;<strong>“AI CEO”</strong> model, AIs could use their superior powers to choose an optimal long-term&nbsp;<em>strategy</em> as opposed to an individual decision. The strategy would not be “greedy”, in the sense of a sequence of steps each making progress on measurable goals, and it would not have any compact analysis of why it is good. Also, the only way to accrue the benefits of the strategy would be to continue pursuing it in the long term. Hence users would have to trust the AI and follow its recommendations blindly. For example, think of the case in Chess where an AI figures out that the best move is to sacrifice the queen because for any one of the possible opponent’s moves, there is a countermove, and so on and so forth. The only explanation for why this strategy is a good one may consist of an exponentially big game tree up to a certain depth.</li>
</ul>



<p>Our sense is that there is strong evidence that AI would be incredibly useful for making low-level decisions (i.e., optimizing objectives under constraints) once the high-level strategy was set. Indeed, by far the most exciting advances for deep learning have&nbsp;<em>not</em> been through reinforcement learning, but rather through techniques such as supervised and unsupervised learning. (With the major exception being games like Chess and Go, though even there, given the&nbsp;<a href="https://lichess.org/broadcast/tcec-season-21-superfinal--stockfish-vs-leela-chess-zero/rounds-1-100/Mtixisaw"><u>success</u></a> of non-RL engines such as<a href="https://towardsdatascience.com/dissecting-stockfish-part-2-in-depth-look-at-a-chess-engine-2643cdc35c9a"><u> Stockfish&nbsp;</u></a>versions 12 and later, it is not clear RL is needed.) There is less evidence that “AI advisors” would be useful for setting high-level strategies, but it is certainly plausible. In particular, the power of prompt-based generative models suggests that AI could be useful for generating realistic simulations that can help better convey the impact of various decisions and events. So, while “AI engineers” might be more useful than “AI advisors”, the latter might well have their role as well.&nbsp;</p>



<p>In contrast, we believe that there is little to no evidence for the benefits of “three-dimensional chess” strategies of the type required for the “AI CEO” scenario. The real world (unlike the game of chess or even poker), involves a significant amount of unpredictability and chaos, which makes highly elaborate strategies depending on complex branching trees of moves and counter-moves far less useful. We also find it unlikely that savvy corporate boards would place blind trust in an AI CEO given that (as mentioned above) evaluation of even human CEOs tends to be controversial.&nbsp;</p>



<p>There is an alternative viewpoint, which is that an AI CEO would basically be equivalent to a human CEO but with superhuman “intuition” or “gut feeling” that they cannot explain but somehow leads to decisions that yield enormous benefits in the long term. While this viewpoint cannot be ruled out, there is no evidence in current deep learning successes to support it. Moreover, often great CEO’s “gut feelings” are less about particular decisions, but more about the relative importance of particular metrics (e.g., prioritizing market share or user experience over short-term profits).&nbsp;</p>



<p>In any case, even if one does not agree with our judgment of the relative likelihoods of the above scenarios, we hope that this essay will help sharpen the questions that need to be studied, as well as what lessons can we draw about them from the progress so far of AI systems.</p>



<h1><strong>Technical Analysis</strong></h1>



<h2><br><strong>1. Key hypotheses behind the “Loss of Control” Scenario</strong></h2>



<p>For the sake of the discussion below, let’s assume that at some future time there exists an artificial intelligence system that in a unified way achieves performance far superior to that achieved by all humans today across many fields. This is a necessary assumption for the “loss of control” scenario and an assumption we accept in this essay. For the sake of simplicity, below we refer to such AI systems as “powerful”.</p>



<p>We will also assume that powerful AI will be constructed following the general paradigm that has been so successful in the last decade of machine learning. Specifically, the system will be obtained by going through a large amount of data and computational steps to find some instantiation (a.k.a. “parameters” or “weights”) of it that optimizes some chosen objective. Depending on the choice of the objective, this paradigm includes supervised learning (“classify this image”), unsupervised learning (“predict the next token”), reinforcement learning (“win the game”), and more.</p>



<p><br>For the loss of control scenario to occur, the following two hypotheses must be true:<br>&nbsp;</p>



<blockquote class="wp-block-quote">
<p><strong>Loss-of-Control Hypothesis 1:</strong> There will exist a powerful AI that has long-term goals.</p>
</blockquote>



<p>For an AI to have misaligned long-term goals, it needs to have some long-term goals in the first place. There is a question of how to define the “goals” of an AI system or even a human for that matter. In this essay, we say that an agent has a goal X if, looking retrospectively at the history of the agent’s actions, the most parsimonious explanation for its actions was that it was attempting to achieve X, subject to other constraints or objectives. For example, while chess experts often find it hard to understand why an engine such as AlphaZero makes a specific move, by the end of the game, they often understand the reasoning retrospectively and the sub-goals it was pursuing.</p>



<p>In our parlance, a goal is “long-term” if it has a similar horizon to goals such as&nbsp;<em>“take over the world and kill all the humans”</em> —requiring planning over large scales of time, complexity, and number of agents involved.<sup><a href="#footnotes">[7]</a></sup>&nbsp;&nbsp;</p>



<p>In contrast, we consider goals such as “win a chess game”, “come up with a plan for a bridge that minimizes cost and can carry X traffic”, or “write a piece of software that meets the requirements Y”, as short-term goals.&nbsp; As another example, “come up with a mix of stocks to invest today that will maximize return next week” is a short-term goal, while “come up with a strategy for our company that will maximize our market cap over the next decade” or “come up with a strategy for our country that will maximize our GDP for the next generation” would be long-term goals. The distinction between “short-term goals AI” and “long-term goals AI” is somewhat similar to the distinction between “Tool AI” and “Agent AI” (see&nbsp;<a href="https://www.gwern.net/Tool-AI"><u>here</u></a>). However, what we call “short-term AI” encompasses much more than “Tool AI”, and absolutely includes systems that can take actions such as driving cars, executing trading actions, and so on and so forth.</p>



<p>We claim that for the “loss of control” scenario to materialize, we need not only Hypothesis 1 but also the following stronger hypothesis:</p>



<blockquote class="wp-block-quote">
<p><strong>Loss-of-Control Hypothesis 2:&nbsp;</strong>In several key domains,&nbsp;<em>only</em> AIs with long-term goals will be powerful.</p>
</blockquote>



<p><br>By this, we mean that AIs with long-term goals would completely dominate other AIs, in that they would be much more useful for any user (or for furthering their own goals). In particular,&nbsp; a country, company or organization that restricts itself to only using AIs with short term goals would be at a severe competitive disadvantage compared to one that uses AIs with long-term goals.</p>



<p>Why is Hypothesis 2 necessary for the “loss of control” scenario? The reason is that this scenario requires the “misaligned long-term powerful AI” to be not merely more powerful than humanity as it exists today, but more powerful than humanity in the future. Future humans will have at their disposal the assistance of short-term AIs.</p>



<h2><strong>2. Understanding the validity of the hypotheses</strong></h2>



<p>We now make the following claims, which we believe cast significant doubt on Hypothesis 2.</p>



<p><br><strong>Claim 1: There are diminishing returns to information-processing skills with longer horizons.</strong></p>



<p>Consider the task of predicting the consequences of a particular action in the future. In any sufficiently complex real-life scenario, the further away we attempt to predict, the more there is inherent uncertainty. For example, we can use advanced methods to predict the weather over a short time frame, but the further away the prediction, the more the system “regresses to the mean”, and&nbsp;<a href="https://www.globalagtechinitiative.com/digital-farming/data-management/weather-forecasting-how-does-it-work-and-how-reliable-is-it/"><u>the less advantage</u></a> that highly complex models have over simpler ones (see Figure 4). As in meteorology, this story seems to play out similarly in&nbsp;<a href="https://www.federalreserve.gov/econres/feds/the-accuracy-of-forecasts-prepared-for-the-federal-open-market-committee.htm"><u>macroeconomic forecasting</u></a>.&nbsp; In general, we expect prediction success to behave like Figure 1 below—the error increases with the horizon until it plateaus to a baseline level of some simple heuristic(s). Hence while initially highly sophisticated models can beat simpler ones by a wide margin, this advantage eventually diminishes with the time horizon.</p>



<p>Tetlock’s&nbsp;<a href="https://www.lesswrong.com/posts/dvYeSKDRd68GcrWoe/ten-commandments-for-aspiring-superforecasters"><u>first commandment</u></a> to potential superforecasters is to triage: “Don’t waste time either on “clocklike” questions (where simple rules of thumb can get you close to the right answer) or on impenetrable “cloud-like” questions (where even fancy statistical models can’t beat the dart-throwing chimp). Concentrate on questions in the Goldilocks zone of difficulty, where effort pays off the most.”&nbsp; Another way to say it is that outside of the Goldilocks zone, more effort or cognitive power does not give much returns.&nbsp;</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2022/11/image.png"><img data-attachment-id="8474" data-permalink="https://windowsontheory.org/2022/11/22/ai-will-change-the-world-but-wont-take-it-over-by-playing-3-dimensional-chess/image-9/" data-orig-file="https://windowsontheory.files.wordpress.com/2022/11/image.png" data-orig-size="1224,440" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://windowsontheory.files.wordpress.com/2022/11/image.png?w=300" data-large-file="https://windowsontheory.files.wordpress.com/2022/11/image.png?w=656" src="https://windowsontheory.files.wordpress.com/2022/11/image.png?w=1024" alt="" class="wp-image-8474" srcset="https://windowsontheory.files.wordpress.com/2022/11/image.png?w=1024 1024w, https://windowsontheory.files.wordpress.com/2022/11/image.png?w=150 150w, https://windowsontheory.files.wordpress.com/2022/11/image.png?w=300 300w, https://windowsontheory.files.wordpress.com/2022/11/image.png?w=768 768w, https://windowsontheory.files.wordpress.com/2022/11/image.png 1224w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></figure>



<p><br><strong>Figure 4:&nbsp;</strong>&nbsp;Left: Historical weather prediction accuracy data taken from a&nbsp;<a href="https://qr.ae/pvVKIJ"><u>Quora answer of Mikko Strahlendorff</u></a>. With technological advances, accuracy has improved significantly, but prediction accuracy sharply decays with time. Right: Figure on relative applicability of different methods from&nbsp;<a href="https://www.globalagtechinitiative.com/digital-farming/data-management/weather-forecasting-how-does-it-work-and-how-reliable-is-it/"><u>Brent Shaw</u></a>. Computationally intensive numerical prediction applies in a “goldilocks zone” of days to weeks.</p>



<p>In a variety of human endeavors, it seems that the cognitive skills needed to make decisions display a similar phenomenon. Occupations involving making decisions on the mid-range horizon, such as engineering, law, and medicine, require higher cognitive skills than those requiring long-term decisions such as CEOs or Politicians (see Table 3).</p>



<p>One argument people make is that intelligence is not just about IQ or “<a href="https://www.lesswrong.com/posts/aiQabnugDhcrFtr9n/the-power-of-intelligence"><u>booksmarts</u></a>”. We do not dispute this. However, we do believe that the key potential advantage of AI systems over their human counterparts would be the ability to quickly process large amounts of information, which in humans is approximated by scores such as IQ. If that skill were key to successful leadership of companies or countries, then we would expect CEOs and leaders to come from the top 0.1% (≈ +3σ)&nbsp; of the distribution of such scores. The data does not bear this out.<sup><a href="#footnotes">[8]</a></sup>&nbsp;</p>



<p><strong>Claim 2:</strong>&nbsp;<strong>It may be possible to extract powerful short-term modules from long-term systems.</strong></p>



<p>For Hypothesis 2 to be true, it should not be possible to take a powerful AI system with long-term goals, and extract from it modules that would be just as powerful in the key domains, but would have short-term goals. However, a nascent body of work identifies and extracts useful representations and sub-modules in deep neural networks. See, for example, this recent investigation of&nbsp;<a href="https://arxiv.org/abs/2111.09259"><u>AlphaZero</u></a>. We remark that some components of AlphaZero also inspired advances to the Stockfish Chess Engine (which is not trained using RL and involves a lot of hand-coded features), and whose latest version&nbsp;<a href="https://ccrl.chessdom.com/ccrl/4040/"><u>does in fact beat</u></a> RL trained methods a-la AlphaZero.</p>



<p>A related issue is that a consistent theme of theoretical computer science is that verification is easier than solving or proving. Hence even a complex system could explain its reasoning to a simple verifier, even if that reasoning required a significant effort to discover. There are similar examples in human affairs: e.g., even though the discovery of quantum mechanics took thousands of years and multiple scientific revolutions, we can still teach it to undergraduates today whose brains are no better than those of the ancient Greeks.&nbsp;</p>



<h2><br><strong>2.1 The impact of the deep learning paradigm on Hypothesis 2</strong></h2>



<p><br>The following claims have to do with the way we believe advanced AI systems will be constructed. We believe it is fair to assume that the paradigm of using massive data and computation to create such systems, by optimizing with respect to a certain objective, will continue to be used. Indeed, it is the success of this paradigm that has caused the rise in concerns about AI in the first place.&nbsp; In particular, we want to make a clear distinction between the&nbsp;<em>training objective</em>, which the system is designed to optimize, versus the goals that the system appears to follow during its&nbsp;<em>deployment</em>.</p>



<p><strong>Claim 3: There may be fundamental “scaling laws” governing the amount of performance AI systems can achieve as a function of the data and computational resources.</strong></p>



<p>One of the original worries in the AI risk literature is the “<a href="https://edoras.sdsu.edu/~vinge/misc/singularity.html"><u>singularity</u></a>” scenario, by which an AI system continuously improves its own performance without limit. However, this assumes that a system can improve itself by rewriting its code, without requiring additional hardware resources.&nbsp; If there are hard limits to what can be achieved with a certain level of resources, then such self-improvements will also hit diminishing returns. There has been significant evidence for&nbsp;<a href="https://arxiv.org/abs/1909.12673"><u>the</u></a> “<a href="https://arxiv.org/abs/2001.08361"><u>scaling</u></a>&nbsp;<a href="https://arxiv.org/abs/2203.15556"><u>laws</u></a>” hypothesis in recent years.</p>



<p><br><br><img src="https://lh4.googleusercontent.com/b_3ZFahE15wnxQCDcRBEnnbGytqGJayIVfztpee7Ff3X8DkyqQT6VCpLABMnBUMy0dRaNpbQdpiWrwYGz_C-NSxovBP9E48nRixKyFT0PgnqhYdaxEUAeXMVxCtsT9ib4RSvhd7fvYgS3ZUJjg5HuQVpfwke49K8Ytys3mVX7hdCUbDugfkL42FnEZYi5Q" style="width:450px;"><br><strong>Figure 5:</strong> Scaling laws as computed by&nbsp;<a href="https://arxiv.org/abs/2203.15556"><u>Hoffman et al</u></a> (“Chinchilla”), see Figure A4 there. While the scaling laws are shaped differently from those of&nbsp;<a href="https://arxiv.org/abs/2001.08361"><u>Kaplan et al</u></a>, the qualitative point we make remains the same.</p>



<p><strong>Claim 4: When training with reinforcement learning, the gradient signal may decrease exponentially with the length of the horizon.</strong></p>



<p>Consider training a system that chooses a sequence of actions, and only gets a reward after H steps (where H is known as the “horizon”). If at any step there is some probability of an action leading to a “dead end” then the chances of getting a meaningful signal decrease&nbsp;<em>exponentially</em> with H. This is a fundamental obstacle to reinforcement learning and its applicability in open-ended situations with a very large space of actions, and a non-trivial cost for any interaction. In particular, one reason AlphaZero was successful was that in games such as chess, the space of legal moves is very constrained, and in the artificial context of a game it is possible to “reset” to a particular position: that is, one can try out different actions and see what their consequences are, and then go back to the same position. This is not possible when interacting in the real world.</p>



<p><br>&nbsp;As a corollary of Claim 4, we claim the following:</p>



<p><strong>Claim 5: There will be powerful AI systems that are trained with short-term objective functions.</strong></p>



<p>By this, we mean models that are trained on a reward/loss function that only depends on a relatively short span of actions/outputs. A canonical example of this is next-token prediction. That is, even if the eventual&nbsp;<em>deployment</em> of the model will involve it making actions and decisions over a long time horizon, its&nbsp;<em>training</em> will involve optimizing short-term rewards.</p>



<p>&nbsp;One might think that the model&#8217;s training does not matter as much, since once it is deployed in the real world, much of what it will learn will be “on the job”. However, this is not at all clear. Suppose the average worker reads/hears about 10 pages per day, which is roughly 5K tokens, leading to roughly 2M tokens per year. In contrast, future AIs will likely be trained on a trillion tokens or so, corresponding to the amount a worker will see in 5 million years! This means that while “fine-tuning” or “in context” learning can and will occur, many of the fundamental capabilities of the systems will be fixed at the time of training (as appears to be the case for pre-trained language models that are fine-tuned with human feedback).</p>



<p><strong>Claim 6: For a long-term goal to necessarily emerge from a system trained with a short-term objective, it must be correlated or causally related to that objective.</strong></p>



<p>If we assume that powerful AIs will be trained with short-term objectives, then Hypothesis 2 requires that (in several key domains)&nbsp;<em>every</em> such system will develop long-term goals. In fact, for the loss-of-control scenario to hold, every such system should develop more-or-less the same sort of goal (e.g., “take over the world”).</p>



<p>While it is certainly possible for systems that evolve from simple rules to develop complex behavior (e.g.,&nbsp;<a href="https://en.wikipedia.org/wiki/Cellular_automaton"><u>cellular automata</u></a>), for a long-term goal to&nbsp;<em>consistently emerge</em> from mere short-term training, there should be some causal relation (or at least persistent correlation)&nbsp; between the long-term goal and the short-term training objective. This is because an AI system can be modeled as a maximizer of the objective on which it was trained. Thus for such a system to&nbsp;<em>always</em> pursue a particular long-term goal, that goal should be correlated with maximizing the training objective.</p>



<p>We illustrate this with an example. Consider an AI software developer which is trained to receive a specification of a software task (say, given by some unit tests) and then come up with a module implementing it, obtaining a reward if the module passes the tests. Now suppose that in actual deployment, the system is also writing the tests that would be used to check its future outputs. We might worry that the system would develop a “long-term” goal to maximize total reward by writing one faulty test, taking the “hit” on it, and receiving a low reward, but then getting high rewards on future tasks. However, that worry would be unfounded, since the AI software developer system is trained to maximize the reward for each task separately, as opposed to maximizing the sum of rewards over time over adaptively chosen inputs of its own making.</p>



<p>Indeed, this situation can already happen today. Next-token prediction models such as GPT-3 are trained on the reward of the perplexity over a single token, but when they are deployed, we typically generate a long sequence of tokens. Now consider a model that simply outputs an endless repetition of the word “blah”. The first few repetitions would get very low rewards, since they are completely unexpected, but once n is large enough (e.g. 10 or so), if you’ve already seen n “blah”s then the probability that the n+1 st word is also “blah” is very high.&nbsp; So if the model were to be maximizing total reward, it may well be worth “taking the hit” by outputting a few blahs. The key point is that GPT-3 does&nbsp;<em>not</em> do that. Since it is trained on predicting the next token for human-generated (as opposed to the text generated by itself), it will optimize for this short-term objective rather than the long-term one.</p>



<p>We believe the example above generalizes to many other cases. An AI system trained in the current paradigm is, by default, a maximizer of the objective it was trained on, rather than an autonomous agent that pursues goals of its own design. The shorter the horizon and more well-defined the objective is, the less likely that optimizing it will lead to systems that appear to take elaborate plans to pursue far-reaching (good or bad) long-term goals.&nbsp;</p>



<h1><br><strong>Summary</strong></h1>



<p>Given the above, we believe that while AI will continue to yield breakthroughs in many areas of human endeavor, we will not see a unitary nigh-omnipotent AI system that acts autonomously to pursue long-term goals. Concretely, even if a successful long-term AI system could be constructed, we believe that this is not a domain where AI will have a significant “competitive advantage” over humans.</p>



<p>Rather, based on what we know, it is likely that AI systems will have a “sweet spot” of a not-too-long horizon in which they can provide significant benefits. For strategic and long-term decisions that are far beyond this sweet spot, the superior information processing skills of AIs will give diminishing returns. (Although AIs will likely supply valuable input and analysis to the decision makers.).&nbsp; An AI engineer may well dominate a human engineer (or at least one that is not aided by AI tools), but an AI CEO’s advantage will be much more muted, if any, over its human counterpart. Like our world, such a world will still involve much conflict and competition, with all sides aided by advanced technology, but without one system that dominates all others.</p>



<p>If our analysis holds, then it also suggests different approaches to mitigating AI risk than have been considered in the “AI safety” community. Currently, the prevailing wisdom in that community is that AI systems with long-term goals are a given, and hence the approach to mitigate their risk is to “align” these goals with human values. However, perhaps more evidence should be placed on building just-as-powerful AI systems that are restricted to short time horizons. Such systems could also be used to monitor and control other AIs, whether autonomous or directed by humans. This includes monitoring and hardening systems against hacking, detecting misinformation, and more. Regardless, we believe that more research needs to be done on understanding the internal representations of deep learning systems, and what features and strategies emerge from the training process (so we are happy that the AI safety community is putting increasing resources into “interpretability” research). There is&nbsp;<a href="https://arxiv.org/abs/2106.07682"><u>some evidence</u></a> that the same internal representations emerge regardless of the choices made in training.</p>



<p>There are also some technical research directions that would affect whether our argument is correct. For instance, we are interested in seeing work on the impacts of noise and unpredictability on the performance of reinforcement learning algorithms; in particular, on the&nbsp;<em>relative</em> performance of models of varying complexity (i.e.&nbsp;<a href="https://arxiv.org/abs/2104.03113"><u>scaling</u></a>&nbsp;<a href="https://arxiv.org/abs/2210.00849"><u>laws</u></a> for RL).</p>



<p><strong>Acknowledgments: </strong>Thanks to&nbsp;<a href="mailto:jedelman@g.harvard.edu"><u>Yafah Edelman</u></a> for comments on an earlier version of this essay.</p>



<h2 id="footnotes"><strong>Footnotes</strong></h2>



<ol>
<li>During the 90s-2000s, human-engine teams were able to consistently beat engines in&nbsp;<a href="https://en.wikipedia.org/wiki/Advanced_chess"><u>“advanced chess”</u></a> tournaments, but no major advanced chess tournament seems to have taken place since the release of AlphaZero and the resulting jump in engine strength, presumably because the human half of each team would be superfluous.</li>



<li>The success of a bridge does hinge on its long-term stability, but stability can be tested before the bridge is built, and coming up with measures for load-bearing and other desiderata is standard practice in the engineering profession. An AI trained using such a short-term evaluation suite as its reward function may still “<a href="https://arxiv.org/abs/2210.10760"><u>overoptimize</u></a>” against the metric, a la Goodhart’s Law, but this can likely be addressed with regularization techniques.</li>



<li>It may be the case that, for subtle reasons, if we try to train an AI with only short-term goals—e.g. by training in a series of short episodes—we could accidentally end up with an AI that has long-term goals. See Claim 6 below. But avoiding this pitfall seems like an easier problem than “aligning” the goals of an AI that is explicitly meant to care about the long-term.</li>



<li>We don’t mean that they satisfy&nbsp;<a href="https://en.wikipedia.org/wiki/Chaos_theory#Chaotic_dynamics"><u>all the formal requirements</u></a> to be defined as a chaotic system; though sensitivity to initial conditions is crucial.</li>



<li>For a nice illustration, see Sam Trajtenberg’s construction of&nbsp;<a href="https://twitter.com/boazbaraktcs/status/1586798286463270914?s=20&amp;t=bEhHVwKP7N6yyp_s2K2HRw"><u>Minecraft in Minecraft</u></a>, or this construction of&nbsp;<a href="https://www.youtube.com/watch?v=xP5-iIeKXE8"><u>Life in Life</u></a>.</li>



<li>Steve Jobs at Apple vs NeXT is one such example; success and failure can themselves be difficult to distinguish even with the benefit of hindsight, as in the case of&nbsp;<a href="https://www.newyorker.com/magazine/2022/11/07/was-jack-welch-the-greatest-ceo-of-his-day-or-the-worst"><u>Jack Welch</u></a>.</li>



<li>For example, such planning might require setting up many companies to earn large amounts of funds, conducting successful political campaigns in several countries, constructing laboratories without being detected, etc. Some such “take-over scenarios” are listed by Bostrom, as well as&nbsp;<a href="https://forum.effectivealtruism.org/posts/zzFbZyGP6iz8jLe9n/agi-ruin-a-list-of-lethalities"><u>Yudkowski</u></a> and&nbsp;<a href="https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html"><u>Urban</u></a>.</li>



<li>It is hypothetically possible that companies would be better off en masse if they hired smarter CEOs than they currently do, but given the high compensation CEOs receive this doesn’t seem like a particularly plausible equilibrium.</li>
</ol>
<p class="authors">By Boaz Barak</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-22T14:05:27Z">Tuesday, November 22 2022, 14:05</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10669'>Littlewood-Richardson coefficients and Kostka number</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sagar Shrivastava</p><p>Littlewood-Richardson (LR) coefficients and Kostka Numbers appear in
representation theory and combinatorics related to GLn . It is known that
Kostka numbers can be represented as special Littlewood-Rischardson
coefficient. In this paper, we show how one can represent LR coefficient in
terms of Kostka numbers, and use the formulation to give a polynomial time
algorithm for the same, hence showing that they belong to the same class of
decision problems.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Shrivastava_S/0/1/0/all/0/1">Sagar Shrivastava</a></p><p>Littlewood-Richardson (LR) coefficients and Kostka Numbers appear in
representation theory and combinatorics related to GLn . It is known that
Kostka numbers can be represented as special Littlewood-Rischardson
coefficient. In this paper, we show how one can represent LR coefficient in
terms of Kostka numbers, and use the formulation to give a polynomial time
algorithm for the same, hence showing that they belong to the same class of
decision problems.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-22T01:30:00Z">Tuesday, November 22 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10471'>Prophet-Inequalities over Time</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Andreas Abels, Elias Pitschmann, Daniel Schmand</p><p>In this paper, we introduce an over-time variant of the well-known
prophet-inequality with i.i.d. random variables. Instead of stopping with one
realized value at some point in the process, we decide for each step how long
we select the value. Then we cannot select another value until this period is
over. The goal is to maximize the expectation of the sum of selected values. We
describe the structure of the optimal stopping rule and give upper and lower
bounds on the prophet-inequality. - Which, in online algorithms terminology,
corresponds to bounds on the competitive ratio of an online algorithm.
</p>
<p>We give a surprisingly simple algorithm with a single threshold that results
in a prophet-inequality of $\approx 0.396$ for all input lengths $n$.
Additionally, as our main result, we present a more advanced algorithm
resulting in a prophet-inequality of $\approx 0.598$ when the number of steps
tends to infinity. We complement our results by an upper bound that shows that
the best possible prophet-inequality is at most $1/\varphi \approx 0.618$,
where $\varphi$ denotes the golden ratio. As part of the proof, we give an
advanced bound on the weighted mediant.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Abels_A/0/1/0/all/0/1">Andreas Abels</a>, <a href="http://arxiv.org/find/cs/1/au:+Pitschmann_E/0/1/0/all/0/1">Elias Pitschmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmand_D/0/1/0/all/0/1">Daniel Schmand</a></p><p>In this paper, we introduce an over-time variant of the well-known
prophet-inequality with i.i.d. random variables. Instead of stopping with one
realized value at some point in the process, we decide for each step how long
we select the value. Then we cannot select another value until this period is
over. The goal is to maximize the expectation of the sum of selected values. We
describe the structure of the optimal stopping rule and give upper and lower
bounds on the prophet-inequality. - Which, in online algorithms terminology,
corresponds to bounds on the competitive ratio of an online algorithm.
</p>
<p>We give a surprisingly simple algorithm with a single threshold that results
in a prophet-inequality of $\approx 0.396$ for all input lengths $n$.
Additionally, as our main result, we present a more advanced algorithm
resulting in a prophet-inequality of $\approx 0.598$ when the number of steps
tends to infinity. We complement our results by an upper bound that shows that
the best possible prophet-inequality is at most $1/\varphi \approx 0.618$,
where $\varphi$ denotes the golden ratio. As part of the proof, we give an
advanced bound on the weighted mediant.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-22T01:30:00Z">Tuesday, November 22 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10507'>Efficient Determinant Maximization for All Matroids</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Adam Brown, Aditi Laddha, Madhusudhan Pittu, Mohit Singh</p><p>Determinant maximization provides an elegant generalization of problems in
many areas, including convex geometry, statistics, machine learning, fair
allocation of goods, and network design. In an instance of the determinant
maximization problem, we are given a collection of vectors $v_1,\ldots, v_n \in
\mathbb{R}^d$, and the goal is to pick a subset $S\subseteq [n]$ of given
vectors to maximize the determinant of the matrix $\sum_{i \in S} v_iv_i^\top$,
where the picked set of vectors $S$ must satisfy some combinatorial constraint
such as cardinality constraint ($|S| \leq k$) or matroid constraint ($S$ is a
basis of a matroid defined on $[n]$).
</p>
<p>In this work, we give a combinatorial algorithm for the determinant
maximization problem under a matroid constraint that achieves
$O(d^{O(d)})$-approximation for any matroid of rank $r\geq d$. This complements
the recent result of~\cite{BrownLPST22} that achieves a similar bound for
matroids of rank $r\leq d$, relying on a geometric interpretation of the
determinant. Our result matches the best-known estimation
algorithms~\cite{madan2020maximizing} for the problem, which could estimate the
objective value but could not give an approximate solution with a similar
guarantee. Our work follows the framework developed by~\cite{BrownLPST22} of
using matroid intersection based algorithms for determinant maximization. To
overcome the lack of a simple geometric interpretation of the objective when $r
\geq d$, our approach combines ideas from combinatorial optimization with
algebraic properties of the determinant. We also critically use the properties
of a convex programming relaxation of the problem introduced
by~\cite{madan2020maximizing}.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1">Adam Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Laddha_A/0/1/0/all/0/1">Aditi Laddha</a>, <a href="http://arxiv.org/find/cs/1/au:+Pittu_M/0/1/0/all/0/1">Madhusudhan Pittu</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1">Mohit Singh</a></p><p>Determinant maximization provides an elegant generalization of problems in
many areas, including convex geometry, statistics, machine learning, fair
allocation of goods, and network design. In an instance of the determinant
maximization problem, we are given a collection of vectors $v_1,\ldots, v_n \in
\mathbb{R}^d$, and the goal is to pick a subset $S\subseteq [n]$ of given
vectors to maximize the determinant of the matrix $\sum_{i \in S} v_iv_i^\top$,
where the picked set of vectors $S$ must satisfy some combinatorial constraint
such as cardinality constraint ($|S| \leq k$) or matroid constraint ($S$ is a
basis of a matroid defined on $[n]$).
</p>
<p>In this work, we give a combinatorial algorithm for the determinant
maximization problem under a matroid constraint that achieves
$O(d^{O(d)})$-approximation for any matroid of rank $r\geq d$. This complements
the recent result of~\cite{BrownLPST22} that achieves a similar bound for
matroids of rank $r\leq d$, relying on a geometric interpretation of the
determinant. Our result matches the best-known estimation
algorithms~\cite{madan2020maximizing} for the problem, which could estimate the
objective value but could not give an approximate solution with a similar
guarantee. Our work follows the framework developed by~\cite{BrownLPST22} of
using matroid intersection based algorithms for determinant maximization. To
overcome the lack of a simple geometric interpretation of the objective when $r
\geq d$, our approach combines ideas from combinatorial optimization with
algebraic properties of the determinant. We also critically use the properties
of a convex programming relaxation of the problem introduced
by~\cite{madan2020maximizing}.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-22T01:30:00Z">Tuesday, November 22 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10516'>PIM-tree: A Skew-resistant Index for Processing-in-Memory</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Hongbo Kang, Yiwei Zhao, Guy E. Blelloch, Laxman Dhulipala, Yan Gu, Charles McGuffey, Phillip B. Gibbons</p><p>The performance of today's in-memory indexes is bottlenecked by the memory
latency/bandwidth wall. Processing-in-memory (PIM) is an emerging approach that
potentially mitigates this bottleneck, by enabling low-latency memory access
whose aggregate memory bandwidth scales with the number of PIM nodes. There is
an inherent tension, however, between minimizing inter-node communication and
achieving load balance in PIM systems, in the presence of workload skew. This
paper presents PIM-tree, an ordered index for PIM systems that achieves both
low communication and high load balance, regardless of the degree of skew in
the data and the queries. Our skew-resistant index is based on a novel division
of labor between the multi-core host CPU and the PIM nodes, which leverages the
strengths of each. We introduce push-pull search, which dynamically decides
whether to push queries to a PIM-tree node (CPU -&gt; PIM-node) or pull the node's
keys back to the CPU (PIM-node -&gt; CPU) based on workload skew. Combined with
other PIM-friendly optimizations (shadow subtrees and chunked skip lists), our
PIM-tree provides high-throughput, (guaranteed) low communication, and
(guaranteed) high load balance, for batches of point queries, updates, and
range scans.
</p>
<p>We implement the PIM-tree structure, in addition to prior proposed PIM
indexes, on the latest PIM system from UPMEM, with 32 CPU cores and 2048 PIM
nodes. On workloads with 500 million keys and batches of one million queries,
the throughput using PIM-trees is up to 69.7x and 59.1x higher than the two
best prior methods. As far as we know these are the first implementations of an
ordered index on a real PIM system.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1">Hongbo Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yiwei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Blelloch_G/0/1/0/all/0/1">Guy E. Blelloch</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhulipala_L/0/1/0/all/0/1">Laxman Dhulipala</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yan Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+McGuffey_C/0/1/0/all/0/1">Charles McGuffey</a>, <a href="http://arxiv.org/find/cs/1/au:+Gibbons_P/0/1/0/all/0/1">Phillip B. Gibbons</a></p><p>The performance of today's in-memory indexes is bottlenecked by the memory
latency/bandwidth wall. Processing-in-memory (PIM) is an emerging approach that
potentially mitigates this bottleneck, by enabling low-latency memory access
whose aggregate memory bandwidth scales with the number of PIM nodes. There is
an inherent tension, however, between minimizing inter-node communication and
achieving load balance in PIM systems, in the presence of workload skew. This
paper presents PIM-tree, an ordered index for PIM systems that achieves both
low communication and high load balance, regardless of the degree of skew in
the data and the queries. Our skew-resistant index is based on a novel division
of labor between the multi-core host CPU and the PIM nodes, which leverages the
strengths of each. We introduce push-pull search, which dynamically decides
whether to push queries to a PIM-tree node (CPU -&gt; PIM-node) or pull the node's
keys back to the CPU (PIM-node -&gt; CPU) based on workload skew. Combined with
other PIM-friendly optimizations (shadow subtrees and chunked skip lists), our
PIM-tree provides high-throughput, (guaranteed) low communication, and
(guaranteed) high load balance, for batches of point queries, updates, and
range scans.
</p>
<p>We implement the PIM-tree structure, in addition to prior proposed PIM
indexes, on the latest PIM system from UPMEM, with 32 CPU cores and 2048 PIM
nodes. On workloads with 500 million keys and batches of one million queries,
the throughput using PIM-trees is up to 69.7x and 59.1x higher than the two
best prior methods. As far as we know these are the first implementations of an
ordered index on a real PIM system.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-22T01:30:00Z">Tuesday, November 22 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10556'>A Distanced Matching Game, Decremental APSP in Expanders, and Faster Deterministic Algorithms for Graph Cut Problems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Julia Chuzhoy</p><p>Expander graphs play a central role in graph theory and algorithms. With a
number of powerful algorithmic tools developed around them, such as the
Cut-Matching game, expander pruning, expander decomposition, and algorithms for
decremental All-Pairs Shortest Paths (APSP) in expanders, to name just a few,
the use of expanders in the design of graph algorithms has become ubiquitous.
Specific applications of interest to us are fast deterministic algorithms for
cut problems in static graphs, and algorithms for dynamic distance-based graph
problems, such as APSP.
</p>
<p>Unfortunately, the use of expanders in these settings incurs a number of
drawbacks. For example, the best currently known algorithm for decremental APSP
in constant-degree expanders can only achieve a $(\log
n)^{O(1/\epsilon^2)}$-approximation with $n^{1+O(\epsilon)}$ total update time
for any $\epsilon$. All currently known algorithms for the Cut Player in the
Cut-Matching game are either randomized, or provide rather weak guarantees.
This, in turn, leads to somewhat weak algorithmic guarantees for several
central cut problems: for example, the best current almost linear time
deterministic algorithm for Sparsest Cut can only achieve approximation factor
$(\log n)^{\omega(1)}$. Lastly, when relying on expanders in distance-based
problems, such as dynamic APSP, via current methods, it seems inevitable that
one has to settle for approximation factors that are at least $\Omega(\log n)$.
</p>
<p>In this paper we propose the use of well-connected graphs, and introduce a
new algorithmic toolkit for such graphs that, in a sense, mirrors the above
mentioned algorithmic tools for expanders. One of these new tools is the
Distanced Matching game, an analogue of the Cut-Matching game for
well-connected graphs. We demonstrate the power of these new tools by obtaining
better results for several of the problems mentioned above.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chuzhoy_J/0/1/0/all/0/1">Julia Chuzhoy</a></p><p>Expander graphs play a central role in graph theory and algorithms. With a
number of powerful algorithmic tools developed around them, such as the
Cut-Matching game, expander pruning, expander decomposition, and algorithms for
decremental All-Pairs Shortest Paths (APSP) in expanders, to name just a few,
the use of expanders in the design of graph algorithms has become ubiquitous.
Specific applications of interest to us are fast deterministic algorithms for
cut problems in static graphs, and algorithms for dynamic distance-based graph
problems, such as APSP.
</p>
<p>Unfortunately, the use of expanders in these settings incurs a number of
drawbacks. For example, the best currently known algorithm for decremental APSP
in constant-degree expanders can only achieve a $(\log
n)^{O(1/\epsilon^2)}$-approximation with $n^{1+O(\epsilon)}$ total update time
for any $\epsilon$. All currently known algorithms for the Cut Player in the
Cut-Matching game are either randomized, or provide rather weak guarantees.
This, in turn, leads to somewhat weak algorithmic guarantees for several
central cut problems: for example, the best current almost linear time
deterministic algorithm for Sparsest Cut can only achieve approximation factor
$(\log n)^{\omega(1)}$. Lastly, when relying on expanders in distance-based
problems, such as dynamic APSP, via current methods, it seems inevitable that
one has to settle for approximation factors that are at least $\Omega(\log n)$.
</p>
<p>In this paper we propose the use of well-connected graphs, and introduce a
new algorithmic toolkit for such graphs that, in a sense, mirrors the above
mentioned algorithmic tools for expanders. One of these new tools is the
Distanced Matching game, an analogue of the Cut-Matching game for
well-connected graphs. We demonstrate the power of these new tools by obtaining
better results for several of the problems mentioned above.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-22T01:30:00Z">Tuesday, November 22 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10887'>Differential Privacy from Locally Adjustable Graph Algorithms: $k$-Core Decomposition, Low Out-Degree Ordering, and Densest Subgraphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Laxman Dhulipala, Quanquan C. Liu, Sofya Raskhodnikova, Jessica Shi, Julian Shun, Shangdi Yu</p><p>Differentially private algorithms allow large-scale data analytics while
preserving user privacy. Designing such algorithms for graph data is gaining
importance with the growth of large networks that model various (sensitive)
relationships between individuals. While there exists a rich history of
important literature in this space, to the best of our knowledge, no results
formalize a relationship between certain parallel and distributed graph
algorithms and differentially private graph analysis. In this paper, we define
\emph{locally adjustable} graph algorithms and show that algorithms of this
type can be transformed into differentially private algorithms.
</p>
<p>Our formalization is motivated by a set of results that we present in the
central and local models of differential privacy for a number of problems,
including $k$-core decomposition, low out-degree ordering, and densest
subgraphs. First, we design an $\varepsilon$-edge differentially private (DP)
algorithm that returns a subset of nodes that induce a subgraph of density at
least $\frac{D^*}{1+\eta} - O\left(\text{poly}(\log n)/\varepsilon\right),$
where $D^*$ is the density of the densest subgraph in the input graph (for any
constant $\eta &gt; 0$). This algorithm achieves a two-fold improvement on the
multiplicative approximation factor of the previously best-known private
densest subgraph algorithms while maintaining a near-linear runtime.
</p>
<p>Then, we present an $\varepsilon$-locally edge differentially private (LEDP)
algorithm for $k$-core decompositions. Our LEDP algorithm provides approximates
the core numbers (for any constant $\eta &gt; 0$) with $(2+\eta)$ multiplicative
and $O\left(\text{poly}\left(\log n\right)/\varepsilon\right)$ additive error.
This is the first differentially private algorithm that outputs private
$k$-core decomposition statistics.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dhulipala_L/0/1/0/all/0/1">Laxman Dhulipala</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Quanquan C. Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Raskhodnikova_S/0/1/0/all/0/1">Sofya Raskhodnikova</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jessica Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shun_J/0/1/0/all/0/1">Julian Shun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Shangdi Yu</a></p><p>Differentially private algorithms allow large-scale data analytics while
preserving user privacy. Designing such algorithms for graph data is gaining
importance with the growth of large networks that model various (sensitive)
relationships between individuals. While there exists a rich history of
important literature in this space, to the best of our knowledge, no results
formalize a relationship between certain parallel and distributed graph
algorithms and differentially private graph analysis. In this paper, we define
\emph{locally adjustable} graph algorithms and show that algorithms of this
type can be transformed into differentially private algorithms.
</p>
<p>Our formalization is motivated by a set of results that we present in the
central and local models of differential privacy for a number of problems,
including $k$-core decomposition, low out-degree ordering, and densest
subgraphs. First, we design an $\varepsilon$-edge differentially private (DP)
algorithm that returns a subset of nodes that induce a subgraph of density at
least $\frac{D^*}{1+\eta} - O\left(\text{poly}(\log n)/\varepsilon\right),$
where $D^*$ is the density of the densest subgraph in the input graph (for any
constant $\eta &gt; 0$). This algorithm achieves a two-fold improvement on the
multiplicative approximation factor of the previously best-known private
densest subgraph algorithms while maintaining a near-linear runtime.
</p>
<p>Then, we present an $\varepsilon$-locally edge differentially private (LEDP)
algorithm for $k$-core decompositions. Our LEDP algorithm provides approximates
the core numbers (for any constant $\eta &gt; 0$) with $(2+\eta)$ multiplicative
and $O\left(\text{poly}\left(\log n\right)/\varepsilon\right)$ additive error.
This is the first differentially private algorithm that outputs private
$k$-core decomposition statistics.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-22T01:30:00Z">Tuesday, November 22 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Monday, November 21
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2022/11/21/straight-line-through.html'>A straight line through every face</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          While updating my online publications list for something else I noticed that I had neglected to discuss one of my papers from earlier this fall: “Geodesic paths passing through all faces on a polyhedron” (with Demaine, Demaine, Ito, Katayama, Maruyama, and Uno), in the booklet of abstracts from JCDCG3 2022, the Japanese Conference on Discrete and Computational Geometry, Graphs, and Games.
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>While updating my online publications list for something else I noticed that I had neglected to discuss one of my papers from earlier this fall: “Geodesic paths passing through all faces on a polyhedron” (with Demaine, Demaine, Ito, Katayama, Maruyama, and Uno), in the <a href="https://www.rs.tus.ac.jp/jcdcggg/JCDCG3-2022Proceedings(r2).pdf">booklet of abstracts from JCDCG<sup>3</sup> 2022</a>, the Japanese Conference on Discrete and Computational Geometry, Graphs, and Games.</p>

<p>The paper is kind of telegraphic, but the question it considers is easily stated. On the surface of a polyhedron, the analogue of a straight line is a geodesic, the shortest curve between two points. Which polyhedra have geodesics that cross through all of their faces? Maybe the 2d version is easier to explain: any two points on a convex polygon split the polygon into two arcs, and a geodesic is the shorter of the two. Which polygons have at least one geodesic that includes a segment from each edge?</p>

<p style="text-align:center"><img src="/blog/assets/2022/2d-univ-geodesics.svg" alt="Geodesics through all edges of a kite and a trapezoid" style="width:100%;max-width:600px" /></p>

<p>The endpoints of such a geodesic \(A\) must be in different edges, because if they were in the same edge then the complementary arc \(\bar A\) would be a straight line segment, shorter than any other arc. Those two edges must be adjacent, because otherwise \(\bar A\) would include an edge missed by \(A\). And these two adjacent edges must be longer than the sum of all the other edges, so that \(A\) (a superset of the other edges) can be the shorter than \(\bar A\) (a subset of the two adjacent edges). That turns out to be an exact characterization: a convex polygon has two points whose geodesic passes through all edges, if and only if it has two adjacent edges that together have more than half the perimeter. For these polygons, the arc \(A\) can be chosen to have its endpoints near the outer vertices of the two long adjacent edges. So this is possible for all triangles, for any quadrilateral that is not a parallelogram, and for many other polygons of arbitrarily many sides. But it does not work for any centrally symmetric polygon, because each two adjacent sides are at least matched in length by the two opposite sides.</p>

<p>When we first discussed this problem (five years ago at a Barbados workshop), we started with the idea that no polyhedron with two parallel faces can have a geodesic through all faces. In particular, this would imply that the only regular polyhedron with a geodesic through all faces is a regular tetrahedron. But it’s not true! Instead, if \(P\) is any polygon with a geodesic through all edges, then long-enough right prisms over \(P\) have geodesics through all faces.</p>

<p>Geodesics on the surface of a convex polyhedron may be easier to understand by unfolding the polyhedron into a <a href="https://en.wikipedia.org/wiki/Net_(polyhedron)">net</a>, a flat system of polygons in the plane, drawing the line segment between the endpoints of the geodesics in the net, and then folding it back up. The complication is that the line segment needs to stay inside the net, and there may be many different nets with different line segments.</p>

<p>Suppose \(P\) is a polygon with a geodesic \(A\) through all edges, like the yellow kite above. The prism over \(P\) has two copies of \(P\), connected by rectangles. It can be unfolded by unrolling the rectangles into one long rectangular strip, and connecting the two copies of \(P\) to the top and bottom of the strip, as shown below. (The lightly shaded copy of \(P\) is an alternative placement on the top of the strip; you should only keep one of the two top copies.) To make a curve through all faces on a prism over \(P\),
attach each copy of \(P\) along one of its two adjacent long edges, and arrange the rectangular strip with these two attached copies at opposite ends. Then, connect a point on the top copy of \(P\), near the start of \(A\) on that copy, to another point on the bottom copy of \(P\), near the end of \(A\) on that copy. The resulting curve is shown below as the red segment on its net.</p>

<p style="text-align:center"><img src="/blog/assets/2022/prism-univ-geodesic.svg" alt="A geodesic through all faces of an unfolded prism over a kite (red) and a different curve that is not a geodesic (blue)" style="width:100%;max-width:600px" /></p>

<p>The red curve is definitely shorter than the curve that you would get by applying the same construction to \(\bar A\), which would be drawn in its unfolding as a segment with the same height but greater width. But what we have to worry more about is the blue curve in the figure, which cuts through one of the rectangular sides of the prism before cutting straight across one of the copies of \(P\). Could such a curve be shorter? It isn’t in the figure (I measured), but what about more generally?</p>

<p>When the height of the prism is small, the blue curve can be shorter. But in the limit as the height of the prism gets much larger than the size of \(P\), it cannot. The length of curves like the blue one, in the limit, approaches the height of the prism plus the height of the endpoint above the central rectangular region. Instead, in the limit, the length of curves like the red one approaches the height of the prism: the horizontal component of the curve contributes negligibly to its length. So for tall enough prisms the red curve is shorter than any curve like the blue one, and we have a universal geodesic. (You might think that attaching the top and bottom face in the middle of the rectangular strip would produce a shorter geodesic, and for the illustrated prism maybe it does, but as long as the endpoints are much closer to the edge of \(P\) than to the corner of \(P\), the same argument also applies to these alternative curves.)</p>

<p>Wataru Maruyama, a student of Hiro Ito and a coauthor of the paper, succeeded in proving that the other regular polyhedra indeed do not have universal geodesics. We could also prove that every tetrahedron or right prism over a triangle does have one. On the other hand, much more remains unknown. In particular, we do not have an answer to the following question: is there a centrally symmetric polyhedron with a universal geodesic?</p>

<p>(<a href="https://mathstodon.xyz/@11011110/109386054076782254">Discuss on Mastodon</a>)</p><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T22:08:00Z">Monday, November 21 2022, 22:08</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/21/faculty-position-at-cs-department-boston-university-apply-by-december-2-2022/'>Faculty position at CS  department,   Boston University  (apply by December 2, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Two tenure-track assistant professorships beginning July 1, 2023. Strong applicants in all areas of computer science are encouraged to apply, particularly in theory of computation, algorithms, and systems. Applicants working on foundational, methodological, or use-inspired AI research are encouraged to apply to the AI cluster hire initiative. Website: www.bu.edu/cs/2022/10/04/bu-cs-invites-applications-for-new-faculty-members-2022/ Email: canetti@bu.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Two tenure-track assistant professorships beginning July 1, 2023. Strong applicants in all areas of computer science are encouraged to apply, particularly in theory of computation, algorithms, and systems. Applicants working on foundational, methodological, or use-inspired AI research are encouraged to apply to the AI cluster hire initiative.</p>
<p>Website: <a href="https://www.bu.edu/cs/2022/10/04/bu-cs-invites-applications-for-new-faculty-members-2022/">https://www.bu.edu/cs/2022/10/04/bu-cs-invites-applications-for-new-faculty-members-2022/</a><br />
Email: canetti@bu.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T21:26:41Z">Monday, November 21 2022, 21:26</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/21/faculty-at-rutgers-university-new-brunswick-apply-by-january-3-2023/'>Faculty at Rutgers University (New Brunswick) (apply by January 3, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The Computer Science Department at Rutgers University, New Brunswick NJ, invites applications for multiple tenure-track/tenured. We invite applications from candidates specializing in any area of CS, and welcome applicants with interdisciplinary approaches. We are especially interested in Algorithms, Machine Learning and Data Science, High-performance Computing and Scalable Systems. Website: jobs.rutgers.edu/postings/183703 Email: hiring-committee@cs.rutgers.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The Computer Science Department at Rutgers University, New Brunswick NJ, invites applications for multiple tenure-track/tenured. We invite applications from candidates specializing in any area of CS, and welcome applicants with interdisciplinary approaches. We are especially interested in Algorithms, Machine Learning and Data Science, High-performance Computing and Scalable Systems.</p>
<p>Website: <a href="https://jobs.rutgers.edu/postings/183703">https://jobs.rutgers.edu/postings/183703</a><br />
Email: hiring-committee@cs.rutgers.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T19:14:14Z">Monday, November 21 2022, 19:14</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://blog.computationalcomplexity.org/2022/11/a-celebration-of-juris.html'>A Celebration of Juris</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p></p>♦<br>On November 4th I travelled to my undergraduate alma mater Cornell for a Celebration of the Life and Career of Juris Hartmanis&nbsp;who passed away in July. The workshop attracted many Cornell faculty and students, many of Hartmanis' former colleague and students, grad and undergrad, as well as his family. For the most part, the talks did not focus on technical content but rather memories of the great man.&nbsp;<p></p><p>I talked about&nbsp;how Hartmanis founded the field of Computational Complexity and brought me into it. Herbert Lin told the story behind Computing the Future, a 1992 agenda for the future of computer science led by Hartmanis and the challenge to the report by John McCarthy, one of the founders of AI. Should the agenda of computer science be solely in the hands of academic computer scientists, or should it take into account its role in the larger scientific and world-wide community? We still face these questions today.</p><p>Ryan Williams gave a powerful talk&nbsp;about how Hartmanis personally intervened to ensure Ryan had a future in complexity. We are all better off for that.</p><p>After the workshop, Ryan and I walked around the campus and Collegetown reminiscing on how things have changed in the two decades since Ryan was an undergrad and the four decades (!) since I was. Most of the bars and restaurants have disappeared. The Arts quad is mostly the same, while the engineering building have been mostly rebuilt. There's a new computer science building with another on the way.&nbsp;</p><p>I stayed in town to catch the Cornell football game the next day, as I once was on that field playing tuba for the marching band. They tore down the west stands to put up a parking lot and the east stands were sparsely filled watching Penn dominate the game.</p><p>Good bye Juris. You created a discipline, started one of the first CS departments, and plotted the future of both computational complexity and computer science as a whole. A master and commander indeed.</p><p>By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p></p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxWze7urg3GUKNZRP8vI4maCWwNHi1JQkVcFCRfLas3dZQbmvYZz4jLurkuCXLAiLOFmKXjg7QAFHh5iNwt2vCR4ONgOSIHSBRpw1cC0rQjCvm9bKSSREiDMmPRLK8N3xtg9la8HCM7yr1iOTwP9v4FWv1eYhwGAQoF6JrbXlEZvw8LR-kug/s4080/PXL_20221104_140728676.jpg" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="3072" data-original-width="4080" height="241" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxWze7urg3GUKNZRP8vI4maCWwNHi1JQkVcFCRfLas3dZQbmvYZz4jLurkuCXLAiLOFmKXjg7QAFHh5iNwt2vCR4ONgOSIHSBRpw1cC0rQjCvm9bKSSREiDMmPRLK8N3xtg9la8HCM7yr1iOTwP9v4FWv1eYhwGAQoF6JrbXlEZvw8LR-kug/s320/PXL_20221104_140728676.jpg" width="320" /></a></div><br />On November 4th I travelled to my undergraduate alma mater Cornell for a <a href="https://cis.cornell.edu/bowers-cis-community-celebrates-life-juris-hartmanis">Celebration of the Life and Career of Juris Hartmanis</a>&nbsp;who <a href="https://blog.computationalcomplexity.org/2022/08/the-godfather-of-complexity.html">passed away</a> in July. The workshop attracted many Cornell faculty and students, many of Hartmanis' former colleague and students, grad and undergrad, as well as his family. For the most part, the talks did not focus on technical content but rather memories of the great man.&nbsp;<p></p><p>I <a href="https://www.youtube.com/watch?v=ACxU-90O-ag&amp;t=2768s">talked about</a>&nbsp;how Hartmanis founded the field of Computational Complexity and brought me into it. Herbert Lin <a href="https://www.youtube.com/watch?v=QKW_GalI31o&amp;t=2900s">told the story</a> behind <a href="https://www.google.com/books/edition/Computing_the_Future/tYBQAAAAMAAJ">Computing the Future</a>, a 1992 agenda for the future of computer science led by Hartmanis and the challenge to the report by John McCarthy, one of the founders of AI. Should the agenda of computer science be solely in the hands of academic computer scientists, or should it take into account its role in the larger scientific and world-wide community? We still face these questions today.</p><p>Ryan Williams gave <a href="https://www.youtube.com/watch?v=ACxU-90O-ag&amp;t=10350s">a powerful talk</a>&nbsp;about how Hartmanis personally intervened to ensure Ryan had a future in complexity. We are all better off for that.</p><p>After the workshop, Ryan and I walked around the campus and Collegetown reminiscing on how things have changed in the two decades since Ryan was an undergrad and the four decades (!) since I was. Most of the bars and restaurants have disappeared. The Arts quad is mostly the same, while the engineering building have been mostly rebuilt. There's a <a href="https://www.engineering.cornell.edu/magazine/features/gates-hall-new-home-cis">new computer science building</a> with <a href="https://ithacavoice.com/2022/05/cornell-plans-new-computer-science-building-on-hoy-field/">another on the way</a>.&nbsp;</p><p>I stayed in town to catch the Cornell football game the next day, as I once was on that field playing tuba for the marching band. They tore down the west stands to put up a parking lot and the east stands were sparsely filled watching Penn dominate the game.</p><p>Good bye Juris. You created a discipline, started one of the first CS departments, and plotted the future of both computational complexity and computer science as a whole. A master and commander indeed.</p><p class="authors">By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T14:52:00Z">Monday, November 21 2022, 14:52</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10144'>Computational Short Cuts in Infinite Domain Constraint Satisfaction</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Peter Jonsson, Victor Lagerkvist, Sebastian Ordyniak</p><p>A backdoor in a finite-domain CSP instance is a set of variables where each
possible instantiation moves the instance into a polynomial-time solvable
class. Backdoors have found many applications in artificial intelligence and
elsewhere, and the algorithmic problem of finding such backdoors has
consequently been intensively studied. Sioutis and Janhunen (Proc. 42nd German
Conference on AI (KI-2019)) have proposed a generalised backdoor concept
suitable for infinite-domain CSP instances over binary constraints. We
generalise their concept into a large class of CSPs that allow for higher-arity
constraints. We show that this kind of infinite-domain backdoors have many of
the positive computational properties that finite-domain backdoors have: the
associated computational problems are fixed-parameter tractable whenever the
underlying constraint language is finite. On the other hand, we show that
infinite languages make the problems considerably harder: the general backdoor
detection problem is W[2]-hard and fixed-parameter tractability is ruled out
under standard complexity-theoretic assumptions. We demonstrate that backdoors
may have suboptimal behaviour on binary constraints -- this is detrimental from
an AI perspective where binary constraints are predominant in, for instance,
spatiotemporal applications. In response to this, we introduce sidedoors as an
alternative to backdoors. The fundamental computational problems for sidedoors
remain fixed-parameter tractable for finite constraint language (possibly also
containing non-binary relations). Moreover, the sidedoor approach has appealing
computational properties that sometimes leads to faster algorithms than the
backdoor approach.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jonsson_P/0/1/0/all/0/1">Peter Jonsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Lagerkvist_V/0/1/0/all/0/1">Victor Lagerkvist</a>, <a href="http://arxiv.org/find/cs/1/au:+Ordyniak_S/0/1/0/all/0/1">Sebastian Ordyniak</a></p><p>A backdoor in a finite-domain CSP instance is a set of variables where each
possible instantiation moves the instance into a polynomial-time solvable
class. Backdoors have found many applications in artificial intelligence and
elsewhere, and the algorithmic problem of finding such backdoors has
consequently been intensively studied. Sioutis and Janhunen (Proc. 42nd German
Conference on AI (KI-2019)) have proposed a generalised backdoor concept
suitable for infinite-domain CSP instances over binary constraints. We
generalise their concept into a large class of CSPs that allow for higher-arity
constraints. We show that this kind of infinite-domain backdoors have many of
the positive computational properties that finite-domain backdoors have: the
associated computational problems are fixed-parameter tractable whenever the
underlying constraint language is finite. On the other hand, we show that
infinite languages make the problems considerably harder: the general backdoor
detection problem is W[2]-hard and fixed-parameter tractability is ruled out
under standard complexity-theoretic assumptions. We demonstrate that backdoors
may have suboptimal behaviour on binary constraints -- this is detrimental from
an AI perspective where binary constraints are predominant in, for instance,
spatiotemporal applications. In response to this, we introduce sidedoors as an
alternative to backdoors. The fundamental computational problems for sidedoors
remain fixed-parameter tractable for finite constraint language (possibly also
containing non-binary relations). Moreover, the sidedoor approach has appealing
computational properties that sometimes leads to faster algorithms than the
backdoor approach.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T01:30:00Z">Monday, November 21 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09904'>Crossing and intersecting families of geometric graphs on point sets</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jos&#xe9; Luis &#xc1;lvarez-Rebollar (1), Jorge Cravioto-Lagos (2), Nestaly Mar&#xed;n (2), Oriol Sol&#xe9;-Pi (3), Jorge Urrutia (4) ((1) Posgrado en Ciencias Matem&#xe1;ticas, UNAM and Departamento de Ciencias B&#xe1;sicas, Instituto Tecnol&#xf3;gico de Zit&#xe1;cuaro, (2) Posgrado en Ciencia e Ingenier&#xed;a de la Computaci&#xf3;n, UNAM, (3) Facultad de Ciencias, UNAM, (4) Instituto de Matem&#xe1;ticas, UNAM)</p><p>Let $S$ be a set of $n$ points in the plane in general position. Two line
segments connecting pairs of points of $S$ cross if they have an interior point
in common. Two vertex disjoint geometric graphs with vertices in $S$ cross if
there are two edges, one from each graph, which cross. A set of vertex disjoint
geometric graphs with vertices in $S$ is called mutually crossing if any two of
them cross.
</p>
<p>We show that there exists a constant $c$ such that from any family of $n$
mutually crossing triangles, one can always obtain a family of at least $n^c$
mutually crossing $2$-paths (each of which is the result of deleting an edge
from one of the triangles) and then provide an example that implies that $c$
cannot be taken to be larger than $2/3$. For every $n$ we determine the maximum
number of crossings that a Hamiltonian cycle on a set of $n$ points might have.
Next, we construct a point set whose longest perfect matching contains no
crossings. We also consider edges consisting of a horizontal and a vertical
line segment joining pairs of points of $S$, which we call elbows, and prove
that in any point set $S$ there exists a family of $\lfloor n/4 \rfloor$ vertex
disjoint mutually crossing elbows. Additionally, we show a point set that
admits no more than $n/3$ mutually crossing elbows.
</p>
<p>Finally we study intersecting families of graphs, which are not necessarily
vertex disjoint. A set of edge disjoint graphs with vertices in $S$ is called
an intersecting family if for any two graphs in the set we can choose an edge
in each of them such that they cross. We prove a conjecture by Lara and
Rubio-Montiel, namely, that any set $S$ of $n$ points in general position
admits a family of intersecting triangles with a quadratic number of elements.
</p>
<p>Some other results are obtained throughout this work.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Alvarez_Rebollar_J/0/1/0/all/0/1">Jos&#xe9; Luis &#xc1;lvarez-Rebollar</a> (1), <a href="http://arxiv.org/find/math/1/au:+Cravioto_Lagos_J/0/1/0/all/0/1">Jorge Cravioto-Lagos</a> (2), <a href="http://arxiv.org/find/math/1/au:+Marin_N/0/1/0/all/0/1">Nestaly Mar&#xed;n</a> (2), <a href="http://arxiv.org/find/math/1/au:+Sole_Pi_O/0/1/0/all/0/1">Oriol Sol&#xe9;-Pi</a> (3), <a href="http://arxiv.org/find/math/1/au:+Urrutia_J/0/1/0/all/0/1">Jorge Urrutia</a> (4) ((1) Posgrado en Ciencias Matem&#xe1;ticas, UNAM and Departamento de Ciencias B&#xe1;sicas, Instituto Tecnol&#xf3;gico de Zit&#xe1;cuaro, (2) Posgrado en Ciencia e Ingenier&#xed;a de la Computaci&#xf3;n, UNAM, (3) Facultad de Ciencias, UNAM, (4) Instituto de Matem&#xe1;ticas, UNAM)</p><p>Let $S$ be a set of $n$ points in the plane in general position. Two line
segments connecting pairs of points of $S$ cross if they have an interior point
in common. Two vertex disjoint geometric graphs with vertices in $S$ cross if
there are two edges, one from each graph, which cross. A set of vertex disjoint
geometric graphs with vertices in $S$ is called mutually crossing if any two of
them cross.
</p>
<p>We show that there exists a constant $c$ such that from any family of $n$
mutually crossing triangles, one can always obtain a family of at least $n^c$
mutually crossing $2$-paths (each of which is the result of deleting an edge
from one of the triangles) and then provide an example that implies that $c$
cannot be taken to be larger than $2/3$. For every $n$ we determine the maximum
number of crossings that a Hamiltonian cycle on a set of $n$ points might have.
Next, we construct a point set whose longest perfect matching contains no
crossings. We also consider edges consisting of a horizontal and a vertical
line segment joining pairs of points of $S$, which we call elbows, and prove
that in any point set $S$ there exists a family of $\lfloor n/4 \rfloor$ vertex
disjoint mutually crossing elbows. Additionally, we show a point set that
admits no more than $n/3$ mutually crossing elbows.
</p>
<p>Finally we study intersecting families of graphs, which are not necessarily
vertex disjoint. A set of edge disjoint graphs with vertices in $S$ is called
an intersecting family if for any two graphs in the set we can choose an edge
in each of them such that they cross. We prove a conjecture by Lara and
Rubio-Montiel, namely, that any set $S$ of $n$ points in general position
admits a family of intersecting triangles with a quadratic number of elements.
</p>
<p>Some other results are obtained throughout this work.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T01:30:00Z">Monday, November 21 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09964'>Optimal Algorithms for Linear Algebra in the Current Matrix Multiplication Time</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yeshwanth Cherapanamjeri, Sandeep Silwal, David P. Woodruff, Samson Zhou</p><p>We study fundamental problems in linear algebra, such as finding a maximal
linearly independent subset of rows or columns (a basis), solving linear
regression, or computing a subspace embedding. For these problems, we consider
input matrices $\mathbf{A}\in\mathbb{R}^{n\times d}$ with $n &gt; d$. The input
can be read in $\text{nnz}(\mathbf{A})$ time, which denotes the number of
nonzero entries of $\mathbf{A}$. In this paper, we show that beyond the time
required to read the input matrix, these fundamental linear algebra problems
can be solved in $d^{\omega}$ time, i.e., where $\omega \approx 2.37$ is the
current matrix-multiplication exponent.
</p>
<p>To do so, we introduce a constant-factor subspace embedding with the optimal
$m=\mathcal{O}(d)$ number of rows, and which can be applied in time
$\mathcal{O}\left(\frac{\text{nnz}(\mathbf{A})}{\alpha}\right) + d^{2 +
\alpha}\text{poly}(\log d)$ for any trade-off parameter $\alpha&gt;0$, tightening
a recent result by Chepurko et. al. [SODA 2022] that achieves an
$\exp(\text{poly}(\log\log n))$ distortion with $m=d\cdot\text{poly}(\log\log
d)$ rows in
$\mathcal{O}\left(\frac{\text{nnz}(\mathbf{A})}{\alpha}+d^{2+\alpha+o(1)}\right)$
time. Our subspace embedding uses a recently shown property of stacked
Subsampled Randomized Hadamard Transforms (SRHT), which actually increase the
input dimension, to "spread" the mass of an input vector among a large number
of coordinates, followed by random sampling. To control the effects of random
sampling, we use fast semidefinite programming to reweight the rows. We then
use our constant-factor subspace embedding to give the first optimal runtime
algorithms for finding a maximal linearly independent subset of columns,
regression, and leverage score sampling. To do so, we also introduce a novel
subroutine that iteratively grows a set of independent rows, which may be of
independent interest.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cherapanamjeri_Y/0/1/0/all/0/1">Yeshwanth Cherapanamjeri</a>, <a href="http://arxiv.org/find/cs/1/au:+Silwal_S/0/1/0/all/0/1">Sandeep Silwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1">David P. Woodruff</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Samson Zhou</a></p><p>We study fundamental problems in linear algebra, such as finding a maximal
linearly independent subset of rows or columns (a basis), solving linear
regression, or computing a subspace embedding. For these problems, we consider
input matrices $\mathbf{A}\in\mathbb{R}^{n\times d}$ with $n &gt; d$. The input
can be read in $\text{nnz}(\mathbf{A})$ time, which denotes the number of
nonzero entries of $\mathbf{A}$. In this paper, we show that beyond the time
required to read the input matrix, these fundamental linear algebra problems
can be solved in $d^{\omega}$ time, i.e., where $\omega \approx 2.37$ is the
current matrix-multiplication exponent.
</p>
<p>To do so, we introduce a constant-factor subspace embedding with the optimal
$m=\mathcal{O}(d)$ number of rows, and which can be applied in time
$\mathcal{O}\left(\frac{\text{nnz}(\mathbf{A})}{\alpha}\right) + d^{2 +
\alpha}\text{poly}(\log d)$ for any trade-off parameter $\alpha&gt;0$, tightening
a recent result by Chepurko et. al. [SODA 2022] that achieves an
$\exp(\text{poly}(\log\log n))$ distortion with $m=d\cdot\text{poly}(\log\log
d)$ rows in
$\mathcal{O}\left(\frac{\text{nnz}(\mathbf{A})}{\alpha}+d^{2+\alpha+o(1)}\right)$
time. Our subspace embedding uses a recently shown property of stacked
Subsampled Randomized Hadamard Transforms (SRHT), which actually increase the
input dimension, to "spread" the mass of an input vector among a large number
of coordinates, followed by random sampling. To control the effects of random
sampling, we use fast semidefinite programming to reweight the rows. We then
use our constant-factor subspace embedding to give the first optimal runtime
algorithms for finding a maximal linearly independent subset of columns,
regression, and leverage score sampling. To do so, we also introduce a novel
subroutine that iteratively grows a set of independent rows, which may be of
independent interest.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T01:30:00Z">Monday, November 21 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10022'>Listing 4-Cycles</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Amir Abboud, Seri Khoury, Oree Leibowitz, Ron Safier</p><p>In this note we present an algorithm that lists all $4$-cycles in a graph in
time $\tilde{O}(\min(n^2,m^{4/3})+t)$ where $t$ is their number. Notably, this
separates $4$-cycle listing from triangle-listing, since the latter has a
$(\min(n^3,m^{3/2})+t)^{1-o(1)}$ lower bound under the $3$-SUM Conjecture.
</p>
<p>Our upper bound is conditionally tight because (1) $O(n^2,m^{4/3})$ is the
best known bound for detecting if the graph has any $4$-cycle, and (2) it
matches a recent $(\min(n^3,m^{3/2})+t)^{1-o(1)}$ $3$-SUM lower bound for
enumeration algorithms.
</p>
<p>The latter lower bound was proved very recently by Abboud, Bringmann, and
Fischer [arXiv, 2022] and independently by Jin and Xu [arXiv, 2022].
</p>
<p>In an independent work, Jin and Xu [arXiv, 2022] also present an algorithm
with the same time bound.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Abboud_A/0/1/0/all/0/1">Amir Abboud</a>, <a href="http://arxiv.org/find/cs/1/au:+Khoury_S/0/1/0/all/0/1">Seri Khoury</a>, <a href="http://arxiv.org/find/cs/1/au:+Leibowitz_O/0/1/0/all/0/1">Oree Leibowitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Safier_R/0/1/0/all/0/1">Ron Safier</a></p><p>In this note we present an algorithm that lists all $4$-cycles in a graph in
time $\tilde{O}(\min(n^2,m^{4/3})+t)$ where $t$ is their number. Notably, this
separates $4$-cycle listing from triangle-listing, since the latter has a
$(\min(n^3,m^{3/2})+t)^{1-o(1)}$ lower bound under the $3$-SUM Conjecture.
</p>
<p>Our upper bound is conditionally tight because (1) $O(n^2,m^{4/3})$ is the
best known bound for detecting if the graph has any $4$-cycle, and (2) it
matches a recent $(\min(n^3,m^{3/2})+t)^{1-o(1)}$ $3$-SUM lower bound for
enumeration algorithms.
</p>
<p>The latter lower bound was proved very recently by Abboud, Bringmann, and
Fischer [arXiv, 2022] and independently by Jin and Xu [arXiv, 2022].
</p>
<p>In an independent work, Jin and Xu [arXiv, 2022] also present an algorithm
with the same time bound.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T01:30:00Z">Monday, November 21 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10041'>The communication cost of security and privacy in federated frequency estimation</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Wei-Ning Chen, Ayfer &#xd6;zg&#xfc;r, Graham Cormode, Akash Bharadwaj</p><p>We consider the federated frequency estimation problem, where each user holds
a private item $X_i$ from a size-$d$ domain and a server aims to estimate the
empirical frequency (i.e., histogram) of $n$ items with $n \ll d$. Without any
security and privacy considerations, each user can communicate its item to the
server by using $\log d$ bits. A naive application of secure aggregation
protocols would, however, require $d\log n$ bits per user. Can we reduce the
communication needed for secure aggregation, and does security come with a
fundamental cost in communication?
</p>
<p>In this paper, we develop an information-theoretic model for secure
aggregation that allows us to characterize the fundamental cost of security and
privacy in terms of communication. We show that with security (and without
privacy) $\Omega\left( n \log d \right)$ bits per user are necessary and
sufficient to allow the server to compute the frequency distribution. This is
significantly smaller than the $d\log n$ bits per user needed by the naive
scheme, but significantly higher than the $\log d$ bits per user needed without
security. To achieve differential privacy, we construct a linear scheme based
on a noisy sketch which locally perturbs the data and does not require a
trusted server (a.k.a. distributed differential privacy). We analyze this
scheme under $\ell_2$ and $\ell_\infty$ loss. By using our
information-theoretic framework, we show that the scheme achieves the optimal
accuracy-privacy trade-off with optimal communication cost, while matching the
performance in the centralized case where data is stored in the central server.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wei-Ning Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozgur_A/0/1/0/all/0/1">Ayfer &#xd6;zg&#xfc;r</a>, <a href="http://arxiv.org/find/cs/1/au:+Cormode_G/0/1/0/all/0/1">Graham Cormode</a>, <a href="http://arxiv.org/find/cs/1/au:+Bharadwaj_A/0/1/0/all/0/1">Akash Bharadwaj</a></p><p>We consider the federated frequency estimation problem, where each user holds
a private item $X_i$ from a size-$d$ domain and a server aims to estimate the
empirical frequency (i.e., histogram) of $n$ items with $n \ll d$. Without any
security and privacy considerations, each user can communicate its item to the
server by using $\log d$ bits. A naive application of secure aggregation
protocols would, however, require $d\log n$ bits per user. Can we reduce the
communication needed for secure aggregation, and does security come with a
fundamental cost in communication?
</p>
<p>In this paper, we develop an information-theoretic model for secure
aggregation that allows us to characterize the fundamental cost of security and
privacy in terms of communication. We show that with security (and without
privacy) $\Omega\left( n \log d \right)$ bits per user are necessary and
sufficient to allow the server to compute the frequency distribution. This is
significantly smaller than the $d\log n$ bits per user needed by the naive
scheme, but significantly higher than the $\log d$ bits per user needed without
security. To achieve differential privacy, we construct a linear scheme based
on a noisy sketch which locally perturbs the data and does not require a
trusted server (a.k.a. distributed differential privacy). We analyze this
scheme under $\ell_2$ and $\ell_\infty$ loss. By using our
information-theoretic framework, we show that the scheme achieves the optimal
accuracy-privacy trade-off with optimal communication cost, while matching the
performance in the centralized case where data is stored in the central server.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T01:30:00Z">Monday, November 21 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10137'>Identifying Correlation in Stream of Samples</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Zhenhao Gu, Hao Zhang</p><p>Identifying independence between two random variables or correlated given
their samples has been a fundamental problem in Statistics. However, how to do
so in a space-efficient way if the number of states is large is not quite
well-studied.
</p>
<p>We propose a new, simple counter matrix algorithm, which utilize hash
functions and a compressed counter matrix to give an unbiased estimate of the
$\ell_2$ independence metric. With $\mathcal{O}(\epsilon^{-4}\log\delta^{-1})$
(very loose bound) space, we can guarantee $1\pm\epsilon$ multiplicative error
with probability at least $1-\delta$. We also provide a comparison of our
algorithm with the state-of-the-art sketching of sketches algorithm and show
that our algorithm is effective, and actually faster and at least 2 times more
space-efficient.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1">Zhenhao Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hao Zhang</a></p><p>Identifying independence between two random variables or correlated given
their samples has been a fundamental problem in Statistics. However, how to do
so in a space-efficient way if the number of states is large is not quite
well-studied.
</p>
<p>We propose a new, simple counter matrix algorithm, which utilize hash
functions and a compressed counter matrix to give an unbiased estimate of the
$\ell_2$ independence metric. With $\mathcal{O}(\epsilon^{-4}\log\delta^{-1})$
(very loose bound) space, we can guarantee $1\pm\epsilon$ multiplicative error
with probability at least $1-\delta$. We also provide a comparison of our
algorithm with the state-of-the-art sketching of sketches algorithm and show
that our algorithm is effective, and actually faster and at least 2 times more
space-efficient.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T01:30:00Z">Monday, November 21 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10398'>Improved Approximations for Unrelated Machine Scheduling</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sungjin Im, Shi Li</p><p>We revisit two well-studied scheduling problems in the unrelated machines
setting where each job can have a different processing time on each machine.
For minimizing total weighted completion time we give a 1.45-approximation,
which improves upon the previous 1.488-approximation [Im and Shadloo SODA
2020]. The key technical ingredient in this improvement lies in a new rounding
scheme that gives strong negative correlation with less restrictions. For
minimizing $L_k$-norms of machine loads, inspired by [Kalaitzis et al. SODA
2017], we give better approximation algorithms. In particular we give a $\sqrt
{4/3}$-approximation for the $L_2$-norm which improves upon the former $\sqrt
2$-approximations due to [Azar-Epstein STOC 2005] and [Kumar et al. JACM 2009].
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1">Sungjin Im</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shi Li</a></p><p>We revisit two well-studied scheduling problems in the unrelated machines
setting where each job can have a different processing time on each machine.
For minimizing total weighted completion time we give a 1.45-approximation,
which improves upon the previous 1.488-approximation [Im and Shadloo SODA
2020]. The key technical ingredient in this improvement lies in a new rounding
scheme that gives strong negative correlation with less restrictions. For
minimizing $L_k$-norms of machine loads, inspired by [Kalaitzis et al. SODA
2017], we give better approximation algorithms. In particular we give a $\sqrt
{4/3}$-approximation for the $L_2$-norm which improves upon the former $\sqrt
2$-approximations due to [Azar-Epstein STOC 2005] and [Kumar et al. JACM 2009].
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T01:30:00Z">Monday, November 21 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Sunday, November 20
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=6821'>Reform AI Alignment</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Nearly halfway into my year at OpenAI, still reeling from the FTX collapse, I feel like it&#8217;s finally time to start blogging my AI safety thoughts&#8212;starting with a little appetizer course today, more substantial fare to come. Many people claim that AI alignment is little more a modern eschatological religion&#8212;with prophets, an end-times prophecy, sacred scriptures, and [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Nearly halfway into my year at OpenAI, still reeling from the FTX collapse, I feel like it&#8217;s finally time to start blogging my AI safety thoughts&#8212;starting with a little appetizer course today, more substantial fare to come.</p>



<p>Many people claim that AI alignment is little more a modern eschatological religion&#8212;with prophets, an end-times prophecy, sacred scriptures, and even a god (albeit, one who doesn&#8217;t exist quite yet).  The obvious response to that claim is that, while there&#8217;s some truth to it, &#8220;religions&#8221; based around technology are a little different from the old kind, because technological progress <em>actually happens</em> regardless of whether you believe in it.</p>



<p>I mean, the Internet is sort of like the old concept of the collective unconscious, except that it actually exists and you&#8217;re using it right now.  Airplanes and spacecraft are kind of like the ancient dream of Icarus&#8212;except, again, for the actually existing part.  Today GPT-3 and DALL-E2 and LaMDA and AlphaTensor exist, as they didn&#8217;t two years ago, and one has to try to project forward to what their vastly-larger successors will be doing a decade from now.  Though some of my colleagues are still in denial about it, I regard the fact that such systems will have transformative effects on civilization, comparable to or greater than those of the Internet itself, as &#8220;already baked in&#8221;&#8212;as just the mainstream position, not even a question anymore.  That doesn&#8217;t mean that future AIs are going to convert the earth into paperclips, or give us eternal life in a simulated utopia.  But their story <em>will</em> be a central part of the story of this century.</p>



<p>Which brings me to a second response.  If AI alignment is a religion, it’s now large and established enough to have a thriving &#8220;Reform&#8221; branch, in addition to the original &#8220;Orthodox&#8221; branch epitomized by Eliezer Yudkowsky and <a href="https://intelligence.org/">MIRI</a>.  As far as I can tell, this Reform branch now counts among its members a large fraction of the AI safety researchers now working in academia and industry.  (I’ll leave the formation of a Conservative branch of AI alignment, which reacts against the Reform branch by moving <em>slightly</em> back in the direction of the Orthodox branch, as a problem for the future — to say nothing of Reconstructionist or Marxist branches.)</p>



<p>Here’s an incomplete but hopefully representative list of the differences in doctrine between Orthodox and Reform AI Risk:</p>



<p>(1) Orthodox AI-riskers tend to believe that humanity will survive or be destroyed based on the actions of a few elite engineers over the next decade or two.  Everything else&#8212;climate change, droughts, the future of US democracy, war over Ukraine and maybe Taiwan&#8212;fades into insignificance except insofar as it affects those engineers.</p>



<p>We Reform AI-riskers, by contrast, believe that AI might well pose civilizational risks in the coming century, but so does all the other stuff, and it&#8217;s all tied together.  An invasion of Taiwan might change which world power gets access to TSMC GPUs.  Almost everything affects which entities pursue the AI scaling frontier and whether they&#8217;re cooperating or competing to be first.</p>



<p>(2) Orthodox AI-riskers believe that public outreach has limited value: most people can&#8217;t understand this issue anyway, and will need to be saved from AI despite themselves.</p>



<p>We Reform AI-riskers believe that trying to get a broad swath of the public on board with one&#8217;s preferred AI policy is something close to a deontological imperative.</p>



<p>(3) Orthodox AI-riskers worry almost entirely about an agentic, misaligned AI that deceives humans while it works to destroy them, along the way to maximizing its strange utility function.</p>



<p>We Reform&nbsp;AI-riskers entertain that possibility, but we worry at least as much about powerful AIs that are weaponized by bad humans, which we expect to pose existential risks much earlier in any case.</p>



<p>(4) Orthodox AI-riskers have limited interest in AI safety research applicable to actually-existing systems (LaMDA, GPT-3, DALL-E2, etc.), seeing the dangers posed by those systems as basically trivial compared to the looming danger of a misaligned agentic AI.</p>



<p>We Reform&nbsp;AI-riskers see research on actually-existing systems as one of the only ways to get feedback from the world about which&nbsp;AI&nbsp;safety&nbsp;ideas are or aren&#8217;t promising.</p>



<p>(5) Orthodox AI-riskers worry most about the &#8220;FOOM&#8221; scenario, where some AI might cross a threshold from innocuous-looking to plotting to kill all humans in the space of hours or days.</p>



<p>We Reform&nbsp;AI-riskers worry most about the &#8220;slow-moving trainwreck&#8221; scenario, where (just like with climate change) well-informed people can see the writing on the wall decades ahead, but just can&#8217;t line up everyone&#8217;s incentives to prevent it.</p>



<p>(6) Orthodox AI-riskers talk a lot about a &#8220;pivotal act&#8221; to prevent a misaligned AI from ever being developed, which might involve (e.g.) using an aligned AI to impose a worldwide surveillance regime.</p>



<p>We Reform&nbsp;AI-riskers worry more about such an act causing the very calamity that it was intended to prevent.</p>



<p>(7) Orthodox AI-riskers feel a strong need to repudiate the norms of mainstream science, seeing them as too slow-moving to react in time to the existential danger of AI.</p>



<p>We Reform&nbsp;AI-riskers feel a strong need to get mainstream science on board with the&nbsp;AI&nbsp;safety&nbsp;program.</p>



<p>(8) Orthodox AI-riskers are maximalists about the power of pure, unaided superintelligence to just figure out how to commandeer whatever physical resources it needs to take over the world (for example, by messaging some lab over the Internet, and tricking it into manufacturing nanobots that will do the superintelligence&#8217;s bidding).</p>



<p>We Reform AI-riskers believe that, here just like in high school, there are limits to the power of pure intelligence to achieve one&#8217;s goals.  We&#8217;d expect even an agentic, misaligned AI, if such existed, to need a stable power source, robust interfaces to the physical world, and probably allied humans before it posed much of an existential threat.</p>



<p>What have I missed?</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-20T20:44:33Z">Sunday, November 20 2022, 20:44</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/164'>TR22-164 |  Learning versus Pseudorandom Generators in Constant Parallel Time | 

	Shuichi Hirahara, 

	Mikito Nanashima</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          A polynomial-stretch pseudorandom generator (PPRG) in NC$^0$ (i.e., constant parallel time) is one of the most important cryptographic primitives, especially for constructing highly efficient cryptography and indistinguishability obfuscation. The celebrated work (Applebaum, Ishai, and Kushilevitz, SIAM Journal on Computing, 2006) on randomized encodings yields the characterization of sublinear-stretch pseudorandom generators in NC$^0$ by the existence of logspace-computable one-way functions, but characterizing PPRGs in NC$^0$ seems out of reach at present. Therefore, it is natural to ask which sort of hardness notion is essential for constructing PPRGs in NC$^0$. Particularly, to the best of our knowledge, all the previously known candidates for PPRGs in NC$^0$ follow only one framework based on Goldreich&#39;s one-way function. 
		
In this paper, we present a new learning-theoretic characterization for PPRGs in NC$^0$ and related classes. Specifically, we consider the average-case hardness of learning for well-studied classes in parameterized settings, where the number of samples is restricted to fixed-parameter tractable (FPT), and show that the following are equivalent:
	(i) The existence of (a collection of) PPRGs in NC$^0$.
	(ii) The average-case hardness of learning sparse $\mathbb{F}_2$-polynomials on a sparse example distribution and an NC$^0$-samplable target distribution (i.e., a distribution on target functions).
	(iii) The average-case hardness of learning Fourier-sparse functions on a sparse example distribution and an NC$^0$-samplable target distribution.
	(iv) The average-case hardness of learning constant-depth parity decision trees on a sparse example distribution and an NC$^0$-samplable target distribution.
Furthermore, we characterize a (single) PPRG in $\oplus$-NC$^0$ by the average-case hardness of learning constant-degree $\mathbb{F}_2$-polynomials on a uniform example distribution with FPT samples. Based on our results, we propose new candidates for PPRGs in NC$^0$ and related classes under a hardness assumption on a natural learning problem. An important property of PPRGs in NC$^0$ constructed in our framework is that the output bits are computed by various predicates; thus, it seems to resist an attack that depends on a specific property of one fixed predicate.
	
Conceptually, the main contribution of this study is to formalize a theory of FPT dualization of concept classes, which yields a meta-theorem for the first result. For the second result on PPRGs in $\oplus$-NC$^0$, we use a different technique of pseudorandom $\mathbb{F}_2$-polynomials.
        
        </div>

        <div class='tr-article-summary'>
        
          
          A polynomial-stretch pseudorandom generator (PPRG) in NC$^0$ (i.e., constant parallel time) is one of the most important cryptographic primitives, especially for constructing highly efficient cryptography and indistinguishability obfuscation. The celebrated work (Applebaum, Ishai, and Kushilevitz, SIAM Journal on Computing, 2006) on randomized encodings yields the characterization of sublinear-stretch pseudorandom generators in NC$^0$ by the existence of logspace-computable one-way functions, but characterizing PPRGs in NC$^0$ seems out of reach at present. Therefore, it is natural to ask which sort of hardness notion is essential for constructing PPRGs in NC$^0$. Particularly, to the best of our knowledge, all the previously known candidates for PPRGs in NC$^0$ follow only one framework based on Goldreich&#39;s one-way function. 
		
In this paper, we present a new learning-theoretic characterization for PPRGs in NC$^0$ and related classes. Specifically, we consider the average-case hardness of learning for well-studied classes in parameterized settings, where the number of samples is restricted to fixed-parameter tractable (FPT), and show that the following are equivalent:
	(i) The existence of (a collection of) PPRGs in NC$^0$.
	(ii) The average-case hardness of learning sparse $\mathbb{F}_2$-polynomials on a sparse example distribution and an NC$^0$-samplable target distribution (i.e., a distribution on target functions).
	(iii) The average-case hardness of learning Fourier-sparse functions on a sparse example distribution and an NC$^0$-samplable target distribution.
	(iv) The average-case hardness of learning constant-depth parity decision trees on a sparse example distribution and an NC$^0$-samplable target distribution.
Furthermore, we characterize a (single) PPRG in $\oplus$-NC$^0$ by the average-case hardness of learning constant-degree $\mathbb{F}_2$-polynomials on a uniform example distribution with FPT samples. Based on our results, we propose new candidates for PPRGs in NC$^0$ and related classes under a hardness assumption on a natural learning problem. An important property of PPRGs in NC$^0$ constructed in our framework is that the output bits are computed by various predicates; thus, it seems to resist an attack that depends on a specific property of one fixed predicate.
	
Conceptually, the main contribution of this study is to formalize a theory of FPT dualization of concept classes, which yields a meta-theorem for the first result. For the second result on PPRGs in $\oplus$-NC$^0$, we use a different technique of pseudorandom $\mathbb{F}_2$-polynomials.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-20T11:26:40Z">Sunday, November 20 2022, 11:26</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/163'>TR22-163 |  Random Walks on Rotating Expanders | 

	Gil Cohen, 

	Gal Maor</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Random walks on expanders are a powerful tool which found applications in many areas of theoretical computer science, and beyond. However, they come with an inherent cost -- the spectral expansion of the corresponding power graph deteriorates at a rate that is exponential in the length of the walk. As an example, when $G$ is a $d$-regular Ramanujan graph, the power graph $G^t$ has spectral expansion $2^{\Omega(t)} \sqrt{D}$, where $D = d^t$ is the regularity of $G^t$, thus, $G^t$ is $2^{\Omega(t)}$ away from being Ramanujan. This exponential blowup manifests itself in many applications.

In this work we bypass this barrier by permuting the vertices of the given graph after each random step. We prove that there exists a sequence of permutations for which the spectral expansion deteriorates by only a linear factor in $t$. In the Ramanujan case this yields an expansion of $O(t \sqrt{D})$. We stress that the permutations are tailor-made to the graph at hand and require no randomness to generate.

Our proof, which holds for all sufficiently high girth graphs, makes heavy use of the powerful framework of finite free probability and interlacing families that was developed in a seminal sequence of works by Marcus, Spielman and Srivastava.
        
        </div>

        <div class='tr-article-summary'>
        
          
          Random walks on expanders are a powerful tool which found applications in many areas of theoretical computer science, and beyond. However, they come with an inherent cost -- the spectral expansion of the corresponding power graph deteriorates at a rate that is exponential in the length of the walk. As an example, when $G$ is a $d$-regular Ramanujan graph, the power graph $G^t$ has spectral expansion $2^{\Omega(t)} \sqrt{D}$, where $D = d^t$ is the regularity of $G^t$, thus, $G^t$ is $2^{\Omega(t)}$ away from being Ramanujan. This exponential blowup manifests itself in many applications.

In this work we bypass this barrier by permuting the vertices of the given graph after each random step. We prove that there exists a sequence of permutations for which the spectral expansion deteriorates by only a linear factor in $t$. In the Ramanujan case this yields an expansion of $O(t \sqrt{D})$. We stress that the permutations are tailor-made to the graph at hand and require no randomness to generate.

Our proof, which holds for all sufficiently high girth graphs, makes heavy use of the powerful framework of finite free probability and interlacing families that was developed in a seminal sequence of works by Marcus, Spielman and Srivastava.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-20T09:05:18Z">Sunday, November 20 2022, 09:05</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://decentralizedthoughts.github.io/2022-11-20-pbft-via-locked-braodcast/'>On PBFT from Locked Broadcast</a></h3>
        <p class='tr-article-feed'>from <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We describe a variation of the authenticated version of PBFT using Locked Broadcast that follows a similar path as our previous post on Paxos using Recoverable Broadcast. I call this protocol linear PBFT and variants of it are used by SBFT and Tusk. A later post will show how to...
        
        </div>

        <div class='tr-article-summary'>
        
          
          We describe a variation of the authenticated version of PBFT using Locked Broadcast that follows a similar path as our previous post on Paxos using Recoverable Broadcast. I call this protocol linear PBFT and variants of it are used by SBFT and Tusk. A later post will show how to...
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-20T09:00:00Z">Sunday, November 20 2022, 09:00</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/162'>TR22-162 |  Directed Isoperimetric Theorems for Boolean Functions on the Hypergrid and an $\widetilde{O}(n\sqrt{d})$ Monotonicity Tester | 

	Hadley Black, 

	Deeparnab Chakrabarty, 

	C. Seshadhri</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The problem of testing monotonicity for Boolean functions on the hypergrid, $f:[n]^d \to \{0,1\}$ is a classic topic in property testing. When $n=2$, the domain is the hypercube. For the hypercube case, a breakthrough result of Khot-Minzer-Safra (FOCS 2015) gave a non-adaptive, one-sided tester making $\widetilde{O}(\varepsilon^{-2}\sqrt{d})$ queries. Up to polylog $d$ and $\varepsilon$ factors, this bound matches the $\widetilde{\Omega}(\sqrt{d})$-query non-adaptive lower bound (Chen-De-Servedio-Tan (STOC 2015), Chen-Waingarten-Xie (STOC 2017)). For any $n &gt; 2$, the optimal non-adaptive complexity was unknown. A previous result of the authors achieves a $\widetilde{O}(d^{5/6})$-query upper bound (SODA 2020), quite far from the $\sqrt{d}$ bound for the hypercube.

In this paper, we resolve the non-adaptive complexity of monotonicity testing for all constant $n$, up to $\text{poly}(\varepsilon^{-1}\log d)$ factors. Specifically, we give a non-adaptive, one-sided monotonicity tester making $\widetilde{O}(\varepsilon^{-2}n\sqrt{d})$ queries. From a technical standpoint, we prove new directed isoperimetric theorems over the hypergrid $[n]^d$. These results generalize the celebrated directed Talagrand inequalities that were only known for the hypercube.
        
        </div>

        <div class='tr-article-summary'>
        
          
          The problem of testing monotonicity for Boolean functions on the hypergrid, $f:[n]^d \to \{0,1\}$ is a classic topic in property testing. When $n=2$, the domain is the hypercube. For the hypercube case, a breakthrough result of Khot-Minzer-Safra (FOCS 2015) gave a non-adaptive, one-sided tester making $\widetilde{O}(\varepsilon^{-2}\sqrt{d})$ queries. Up to polylog $d$ and $\varepsilon$ factors, this bound matches the $\widetilde{\Omega}(\sqrt{d})$-query non-adaptive lower bound (Chen-De-Servedio-Tan (STOC 2015), Chen-Waingarten-Xie (STOC 2017)). For any $n &gt; 2$, the optimal non-adaptive complexity was unknown. A previous result of the authors achieves a $\widetilde{O}(d^{5/6})$-query upper bound (SODA 2020), quite far from the $\sqrt{d}$ bound for the hypercube.

In this paper, we resolve the non-adaptive complexity of monotonicity testing for all constant $n$, up to $\text{poly}(\varepsilon^{-1}\log d)$ factors. Specifically, we give a non-adaptive, one-sided monotonicity tester making $\widetilde{O}(\varepsilon^{-2}n\sqrt{d})$ queries. From a technical standpoint, we prove new directed isoperimetric theorems over the hypergrid $[n]^d$. These results generalize the celebrated directed Talagrand inequalities that were only known for the hypercube.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-20T05:41:10Z">Sunday, November 20 2022, 05:41</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/161'>TR22-161 |  Towards Multi-Pass Streaming Lower Bounds for Optimal Approximation of Max-Cut | 

	Raghuvansh Saxena, 

	Lijie Chen, 

	Gillat Kol, 

	Dmitry Paramonov, 

	Zhao Song, 

	Huacheng Yu</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We consider the Max-Cut problem, asking how much space is needed by a streaming algorithm in order to estimate the value of the maximum cut in a graph. This problem has been extensively studied over the last decade, and we now have a near-optimal lower bound for one-pass streaming algorithms, showing that they require linear space to guarantee a better-than-$2$ approximation [KKS15, KK19]. The result relies on a lower bound for the cycle-finding problem, showing that it is hard for a one-pass streaming algorithm to find a cycle in a union of matchings.

The end-goal of our research is to prove a similar lower for multi-pass streaming algorithms that guarantee a better-than-$2$ approximation for Max-Cut, a highly challenging open problem. In this paper, we take a significant step in this direction, showing that even $o(\log n)$-pass streaming algorithms need $n^{\Omega(1)}$ space to solve the cycle-finding problem. Our proof is quite involved, dividing the cycles in the graph into &quot;short&quot; and &quot;long&quot; cycles, and using tailor-made lower bound techniques to handle each case.
        
        </div>

        <div class='tr-article-summary'>
        
          
          We consider the Max-Cut problem, asking how much space is needed by a streaming algorithm in order to estimate the value of the maximum cut in a graph. This problem has been extensively studied over the last decade, and we now have a near-optimal lower bound for one-pass streaming algorithms, showing that they require linear space to guarantee a better-than-$2$ approximation [KKS15, KK19]. The result relies on a lower bound for the cycle-finding problem, showing that it is hard for a one-pass streaming algorithm to find a cycle in a union of matchings.

The end-goal of our research is to prove a similar lower for multi-pass streaming algorithms that guarantee a better-than-$2$ approximation for Max-Cut, a highly challenging open problem. In this paper, we take a significant step in this direction, showing that even $o(\log n)$-pass streaming algorithms need $n^{\Omega(1)}$ space to solve the cycle-finding problem. Our proof is quite involved, dividing the cycles in the graph into &quot;short&quot; and &quot;long&quot; cycles, and using tailor-made lower bound techniques to handle each case.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-20T05:39:49Z">Sunday, November 20 2022, 05:39</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/160'>TR22-160 |  The Geometry of Rounding | 

	Jason Vander Woude, 

	Peter Dixon, 

	A.  Pavan, 

	Jamie Radcliffe, 

	N. V. Vinodchandran</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Rounding has proven to be a fundamental tool in theoretical computer science. By observing that rounding and partitioning of $\mathbb{R}^d$ are equivalent, we introduce the following natural partition problem which we call the secluded hypercube partition problem: Given $k\in\mathbb{N}$ (ideally small) and $\epsilon&gt;0$ (ideally large), is there a partition of $\mathbb{R}^d$ with unit hypercubes such that for every point $\vec{p} \in \mathbb{R}^d$, its closed $\epsilon$-neighborhood (in the $\ell_{\infty}$  norm) intersects at most $k$ hypercubes?

We undertake a comprehensive study of this partition problem. We prove that for every $d\in\mathbb{N}$, there is an explicit (and efficiently computable) hypercube partition of $\mathbb{R}^d$ with $k = d+1$ and $\epsilon = \frac{1}{2d}$. We complement this construction by proving that the value of $k=d+1$ is the best possible (for any $\epsilon$) for a broad class of &quot;reasonable&quot; partitions including hypercube partitions. We also investigate the optimality of the parameter $\epsilon$ and prove that any partition in this broad class that has $k=d+1$, must have $\epsilon\leq\frac{1}{2\sqrt{d}}$. These bounds imply limitations of certain deterministic rounding schemes existing in the literature. Furthermore, this general bound is based on the currently known lower bounds for the dissection number of the cube, and improvements to this bound will yield improvements to our bounds.

While our work is motivated by the desire to understand rounding algorithms, one of our main conceptual contributions is the introduction of the secluded hypercube partition problem, which fits well with a long history of investigations by mathematicians on various hypercube partitions/tilings of Euclidean  space.
        
        </div>

        <div class='tr-article-summary'>
        
          
          Rounding has proven to be a fundamental tool in theoretical computer science. By observing that rounding and partitioning of $\mathbb{R}^d$ are equivalent, we introduce the following natural partition problem which we call the secluded hypercube partition problem: Given $k\in\mathbb{N}$ (ideally small) and $\epsilon&gt;0$ (ideally large), is there a partition of $\mathbb{R}^d$ with unit hypercubes such that for every point $\vec{p} \in \mathbb{R}^d$, its closed $\epsilon$-neighborhood (in the $\ell_{\infty}$  norm) intersects at most $k$ hypercubes?

We undertake a comprehensive study of this partition problem. We prove that for every $d\in\mathbb{N}$, there is an explicit (and efficiently computable) hypercube partition of $\mathbb{R}^d$ with $k = d+1$ and $\epsilon = \frac{1}{2d}$. We complement this construction by proving that the value of $k=d+1$ is the best possible (for any $\epsilon$) for a broad class of &quot;reasonable&quot; partitions including hypercube partitions. We also investigate the optimality of the parameter $\epsilon$ and prove that any partition in this broad class that has $k=d+1$, must have $\epsilon\leq\frac{1}{2\sqrt{d}}$. These bounds imply limitations of certain deterministic rounding schemes existing in the literature. Furthermore, this general bound is based on the currently known lower bounds for the dissection number of the cube, and improvements to this bound will yield improvements to our bounds.

While our work is motivated by the desire to understand rounding algorithms, one of our main conceptual contributions is the introduction of the secluded hypercube partition problem, which fits well with a long history of investigations by mathematicians on various hypercube partitions/tilings of Euclidean  space.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-20T05:38:52Z">Sunday, November 20 2022, 05:38</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Saturday, November 19
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://decentralizedthoughts.github.io/2022-11-19-from-single-shot-to-smr/'>From Single-Shot Consensus to State Machine Replication</a></h3>
        <p class='tr-article-feed'>from <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          In this post we explore the path from Single-Shot Consensus, via Write-Once Registers, to Log Replication, and finally to State Machine Replication. We begin by defining all four problems assuming minority omission failures and partial synchrony. This post continues our previous post on Paxos from Recoverable Broadcast. (Single-Shot) Consensus In...
        
        </div>

        <div class='tr-article-summary'>
        
          
          In this post we explore the path from Single-Shot Consensus, via Write-Once Registers, to Log Replication, and finally to State Machine Replication. We begin by defining all four problems assuming minority omission failures and partial synchrony. This post continues our previous post on Paxos from Recoverable Broadcast. (Single-Shot) Consensus In...
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-19T09:00:00Z">Saturday, November 19 2022, 09:00</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Friday, November 18
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/159'>TR22-159 |  Deep Neural Networks: The Missing Complexity Parameter | 

	Songhua He, 

	Periklis Papakonstantinou</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Deep neural networks are the dominant machine learning model. We show that this model is missing a crucial complexity parameter. Today, the standard neural network (NN) model is a circuit whose gates (neurons) are ReLU units. The complexity of a NN is quantified by the depth (number of layers) and the size (number of neurons = depth times width). This work shows that this alone is insufficient, resulting in NNs with unreasonable computing power. We show that the correct way to talk about the size complexity of a NN is beside the number of neurons to consider the precision (or magnitude) of the weights of the ReLU units. The main message of this work is that if the precision of the weights is not considered in the complexity of the NN then one can engineer weights to &quot;buy&quot; exponentially many neurons for free. In summary, we make three theoretical contributions, potentially affecting many theoretical works on NNs.

1. Every function $f:\{0,1\}^n\to\{0,1\}$ can be computed with $O(\sqrt{2^n})$ many neurons and constant fan-in per neuron; i.e.~exponential times less than Shannon&#39;s classic lower bound for usual combinatorial circuits. 

2. We give a new definition of circuit size that takes into account the precision/magnitude of the weights. Under this new definition of size we asymptotically match Shannon&#39;s bound for NNs.

3. We complement the above results showing that P-uniform NNs decide exactly P.
        
        </div>

        <div class='tr-article-summary'>
        
          
          Deep neural networks are the dominant machine learning model. We show that this model is missing a crucial complexity parameter. Today, the standard neural network (NN) model is a circuit whose gates (neurons) are ReLU units. The complexity of a NN is quantified by the depth (number of layers) and the size (number of neurons = depth times width). This work shows that this alone is insufficient, resulting in NNs with unreasonable computing power. We show that the correct way to talk about the size complexity of a NN is beside the number of neurons to consider the precision (or magnitude) of the weights of the ReLU units. The main message of this work is that if the precision of the weights is not considered in the complexity of the NN then one can engineer weights to &quot;buy&quot; exponentially many neurons for free. In summary, we make three theoretical contributions, potentially affecting many theoretical works on NNs.

1. Every function $f:\{0,1\}^n\to\{0,1\}$ can be computed with $O(\sqrt{2^n})$ many neurons and constant fan-in per neuron; i.e.~exponential times less than Shannon&#39;s classic lower bound for usual combinatorial circuits. 

2. We give a new definition of circuit size that takes into account the precision/magnitude of the weights. Under this new definition of size we asymptotically match Shannon&#39;s bound for NNs.

3. We complement the above results showing that P-uniform NNs decide exactly P.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T19:13:20Z">Friday, November 18 2022, 19:13</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/158'>TR22-158 |  Query Complexity of Inversion Minimization on Trees | 

	Ivan Hu, 

	Dieter van Melkebeek, 

	Andrew Morgan</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We consider the following computational problem: Given a rooted tree and a ranking of its leaves, what is the minimum number of inversions of the leaves that can be attained by ordering the tree? This variation of the well-known problem of counting inversions in arrays originated in mathematical psychology. It has the evaluation of the Mann-Whitney statistic for detecting differences between distributions as a special case. 

We study the complexity of the problem in the comparison-query model, the standard model for problems like sorting, selection, and heap construction. The complexity depends heavily on the shape of the tree: for trees of unit depth, the problem is trivial; for many other shapes, we establish lower bounds close to the strongest known in the model, namely the lower bound of $\log_2(n!)$ for sorting $n$ items. For trees with $n$ leaves we show, in increasing order of closeness to the sorting lower bound:
(a) $\log_2((\alpha(1-\alpha)n)!) - O(\log n)$ queries are needed whenever the tree has a subtree that contains a fraction $\alpha$ of the leaves. This implies a lower bound of $\log_2((\frac{k}{(k+1)^2}n)!) - O(\log n)$ for trees of degree $k$.
(b) $\log_2(n!) - O(\log n)$ queries are needed in case the tree is binary. 
(c) $\log_2(n!) - O(k \log k)$ queries are needed for certain classes of trees of degree $k$, including perfect trees with even $k$.

The lower bounds are obtained by developing two novel techniques for a generic problem $\Pi$ in the comparison-query model and applying them to inversion minimization on trees. Both techniques can be described in terms of the Cayley graph of the symmetric group with adjacent-rank transpositions as the generating set, or equivalently, in terms of the edge graph of the permutahedron, the polytope spanned by all permutations of the vector $(1,2,\dots,n)$. Consider the subgraph consisting of the edges between vertices with the same value under $\Pi$. We show that the size of any decision tree for $\Pi$ must be at least:
(i) the number of connected components of this subgraph, and
(ii) the factorial of the average degree of the complementary subgraph, divided by $n$.

Lower bounds on query complexity then follow by taking the base-2 logarithm. Technique (i) represents a discrete analog of a classical technique in algebraic complexity and allows us to establish (c) and a tight lower bound for counting cross inversions, as well as unify several of the known lower bounds in the comparison-query model. Technique (ii) represents an analog of sensitivity arguments in Boolean complexity and allows us to establish (a) and (b). 

Along the way to proving (b), we derive a tight upper bound on the maximum probability of the distribution of cross inversions, which is the distribution of the Mann-Whitney statistic in the case of the null hypothesis. Up to normalization the probabilities alternately appear in the literature as the coefficients of polynomials formed by the Gaussian binomial coefficients, also known as Gaussian polynomials.
        
        </div>

        <div class='tr-article-summary'>
        
          
          We consider the following computational problem: Given a rooted tree and a ranking of its leaves, what is the minimum number of inversions of the leaves that can be attained by ordering the tree? This variation of the well-known problem of counting inversions in arrays originated in mathematical psychology. It has the evaluation of the Mann-Whitney statistic for detecting differences between distributions as a special case. 

We study the complexity of the problem in the comparison-query model, the standard model for problems like sorting, selection, and heap construction. The complexity depends heavily on the shape of the tree: for trees of unit depth, the problem is trivial; for many other shapes, we establish lower bounds close to the strongest known in the model, namely the lower bound of $\log_2(n!)$ for sorting $n$ items. For trees with $n$ leaves we show, in increasing order of closeness to the sorting lower bound:
(a) $\log_2((\alpha(1-\alpha)n)!) - O(\log n)$ queries are needed whenever the tree has a subtree that contains a fraction $\alpha$ of the leaves. This implies a lower bound of $\log_2((\frac{k}{(k+1)^2}n)!) - O(\log n)$ for trees of degree $k$.
(b) $\log_2(n!) - O(\log n)$ queries are needed in case the tree is binary. 
(c) $\log_2(n!) - O(k \log k)$ queries are needed for certain classes of trees of degree $k$, including perfect trees with even $k$.

The lower bounds are obtained by developing two novel techniques for a generic problem $\Pi$ in the comparison-query model and applying them to inversion minimization on trees. Both techniques can be described in terms of the Cayley graph of the symmetric group with adjacent-rank transpositions as the generating set, or equivalently, in terms of the edge graph of the permutahedron, the polytope spanned by all permutations of the vector $(1,2,\dots,n)$. Consider the subgraph consisting of the edges between vertices with the same value under $\Pi$. We show that the size of any decision tree for $\Pi$ must be at least:
(i) the number of connected components of this subgraph, and
(ii) the factorial of the average degree of the complementary subgraph, divided by $n$.

Lower bounds on query complexity then follow by taking the base-2 logarithm. Technique (i) represents a discrete analog of a classical technique in algebraic complexity and allows us to establish (c) and a tight lower bound for counting cross inversions, as well as unify several of the known lower bounds in the comparison-query model. Technique (ii) represents an analog of sensitivity arguments in Boolean complexity and allows us to establish (a) and (b). 

Along the way to proving (b), we derive a tight upper bound on the maximum probability of the distribution of cross inversions, which is the distribution of the Mann-Whitney statistic in the case of the null hypothesis. Up to normalization the probabilities alternately appear in the literature as the coefficients of polynomials formed by the Gaussian binomial coefficients, also known as Gaussian polynomials.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T15:57:57Z">Friday, November 18 2022, 15:57</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=6818'>WINNERS of the Scott Aaronson Grant for Advanced Precollege STEM Education!</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          I&#8217;m thrilled to be able to interrupt your regular depressing programming for 100% happy news. Some readers will remember that, back in September, I announced that an unnamed charitable foundation had asked my advice on how best to donate $250,000 for advanced precollege STEM education. So, just like the previous time I got such a [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>I&#8217;m thrilled to be able to interrupt your regular depressing programming for 100% happy news.</p>



<p>Some readers will remember that, back in September, I <a href="https://scottaaronson.blog/?p=6678">announced</a> that an unnamed charitable foundation had asked my advice on how best to donate $250,000 for advanced precollege STEM education.  So, just like the <a href="https://scottaaronson.blog/?p=6232">previous time</a> I got such a request, from Jaan Tallinn&#8217;s <a href="https://survivalandflourishing.fund/">Survival and Flourishing Fund</a>, I decided to do a call for proposals on <em>Shtetl-Optimized</em> before passing along my recommendations.</p>



<p>I can now reveal that the generous foundation, this time around, was the <a href="https://www.packard.org/">Packard Foundation</a>.  Indeed, the idea and initial inquiries to me came directly from <a href="https://www.packard.org/about-the-foundation/our-people/bio/david-orr/">Dave Orr</a>: the chair of the foundation, grandson of Hewlett-Packard cofounder <a href="https://en.wikipedia.org/wiki/David_Packard">David Packard</a>, and (so I learned) longtime <em>Shtetl-Optimized</em> reader.</p>



<p>I can <em>also</em> now reveal the results.  I was honored to get more than a dozen excellent applications.  After carefully considering all of them, I passed along four finalists to the Packard Foundation, which preferred to award the entire allotment to a single program if possible.  After more discussion and research, the Foundation then actually decided on <em>two</em> winners:</p>



<ul>
<li>$225,000 for general support to <a href="https://promys.org/">PROMYS</a>: the long-running, world-renowned summer math camp for high-school students, which (among other things) is in the process of launching a new branch in India.  While I ended up at <a href="https://www.mathcamp.org/">Canada/USA Mathcamp</a> (which I supported in my <a href="https://scottaaronson.blog/?p=6256">first grant round</a>) rather than PROMYS, I knew all about and admired PROMYS even back when I was the right age to attend it.  I&#8217;m thrilled to be able to play a small role in its expansion.</li>
</ul>



<ul>
<li>$30,000 for general support to <a href="https://www.addiscoder.com/">AddisCoder</a>: the phenomenal program that introduces Ethiopian high-schoolers to programming and algorithms.  AddisCoder was founded by UC Berkeley theoretical computer science professor and longtime friend-of-the-blog <a href="https://people.eecs.berkeley.edu/~minilek/">Jelani Nelson</a>, and <em>also</em> received $30,000 in my <a href="https://scottaaronson.blog/?p=6256">first grant round</a>.  Jelani and his co-organizers will be pressing ahead with AddisCoder despite political conflict in Ethiopia including a recently-concluded <a href="https://en.wikipedia.org/wiki/Tigray_War">civil war</a>.  I&#8217;m humbled if I can make even the tiniest difference.</li>
</ul>



<p>Thanks so much to the Packard Foundation, and to Packard&#8217;s talented program officers, directors, and associates&#8212;especially Laura Sullivan, Jean Ries, and Prithi Trivedi&#8212;for their hard work to make this happen.  Thanks so much also to everyone who applied.  While I wish we could&#8217;ve funded everyone, I&#8217;ve learned a lot about programs to which I&#8217;d like to steer future support <strong>(other prospective benefactors: please email me!!)</strong>, <em>and</em> to which I&#8217;d like to steer kids: my own, once they&#8217;re old enough, and other kids of my acquaintance.</p>



<p>I feel good that, in the tiny, underfunded world of accelerated STEM education, the $255,000 that Packard is donating will already make a difference.  But of course, $255,000 is only a thousandth of $255 million, which is a thousandth of $255 billion.  Perhaps I could earn the latter sort of sums, to donate to STEM education or any other cause, by (for example) starting my own cryptocurrency exchange.  I hope my readers will forgive me for not having chosen that route, expected-utility-maximization arguments be damned.</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T08:01:17Z">Friday, November 18 2022, 08:01</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09229'>Improved Monotonicity Testers via Hypercube Embeddings</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mark Braverman, Subhash Khot, Guy Kindler, Dor Minzer</p><p>We show improved monotonicity testers for the Boolean hypercube under the
$p$-biased measure, as well as over the hypergrid $[m]^n$. Our results are:
</p>
<p>1. For any $p\in (0,1)$, for the $p$-biased hypercube we show a non-adaptive
tester that makes $\tilde{O}(\sqrt{n}/\varepsilon^2)$ queries, accepts monotone
functions with probability $1$ and rejects functions that are $\varepsilon$-far
from monotone with probability at least $2/3$.
</p>
<p>2. For all $m\in\mathbb{N}$, we show an
$\tilde{O}(\sqrt{n}m^3/\varepsilon^2)$ query monotonicity tester over $[m]^n$.
</p>
<p>We also establish corresponding directed isoperimetric inequalities in these
domains. Previously, the best known tester due to Black, Chakrabarty and
Seshadhri had $\Omega(n^{5/6})$ query complexity. Our results are optimal up to
poly-logarithmic factors and the dependency on $m$.
</p>
<p>Our proof uses a notion of monotone embeddings of measures into the Boolean
hypercube that can be used to reduce the problem of monotonicity testing over
an arbitrary product domains to the Boolean cube. The embedding maps a function
over a product domain of dimension $n$ into a function over a Boolean cube of a
larger dimension $n'$, while preserving its distance from being monotone; an
embedding is considered efficient if $n'$ is not much larger than $n$, and we
show how to construct efficient embeddings in the above mentioned settings.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Braverman_M/0/1/0/all/0/1">Mark Braverman</a>, <a href="http://arxiv.org/find/cs/1/au:+Khot_S/0/1/0/all/0/1">Subhash Khot</a>, <a href="http://arxiv.org/find/cs/1/au:+Kindler_G/0/1/0/all/0/1">Guy Kindler</a>, <a href="http://arxiv.org/find/cs/1/au:+Minzer_D/0/1/0/all/0/1">Dor Minzer</a></p><p>We show improved monotonicity testers for the Boolean hypercube under the
$p$-biased measure, as well as over the hypergrid $[m]^n$. Our results are:
</p>
<p>1. For any $p\in (0,1)$, for the $p$-biased hypercube we show a non-adaptive
tester that makes $\tilde{O}(\sqrt{n}/\varepsilon^2)$ queries, accepts monotone
functions with probability $1$ and rejects functions that are $\varepsilon$-far
from monotone with probability at least $2/3$.
</p>
<p>2. For all $m\in\mathbb{N}$, we show an
$\tilde{O}(\sqrt{n}m^3/\varepsilon^2)$ query monotonicity tester over $[m]^n$.
</p>
<p>We also establish corresponding directed isoperimetric inequalities in these
domains. Previously, the best known tester due to Black, Chakrabarty and
Seshadhri had $\Omega(n^{5/6})$ query complexity. Our results are optimal up to
poly-logarithmic factors and the dependency on $m$.
</p>
<p>Our proof uses a notion of monotone embeddings of measures into the Boolean
hypercube that can be used to reduce the problem of monotonicity testing over
an arbitrary product domains to the Boolean cube. The embedding maps a function
over a product domain of dimension $n$ into a function over a Boolean cube of a
larger dimension $n'$, while preserving its distance from being monotone; an
embedding is considered efficient if $n'$ is not much larger than $n$, and we
show how to construct efficient embeddings in the above mentioned settings.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09341'>Approaching the Soundness Barrier: A Near Optimal Analysis of the Cube versus Cube Test</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Dor Minzer, Kai Zheng</p><p>The Cube versus Cube test is a variant of the well-known Plane versus Plane
test of Raz and Safra, in which to each $3$-dimensional affine subspace $C$ of
$\mathbb{F}_q^n$, a polynomial of degree at most $d$, $T(C)$, is assigned in a
somewhat locally consistent manner: taking two cubes $C_1, C_2$ that intersect
in a plane uniformly at random, the probability that $T(C_1)$ and $T(C_2)$
agree on $C_1\cap C_2$ is at least some $\epsilon$. An element of interest is
the soundness threshold of this test, i.e. the smallest value of $\epsilon$,
such that this amount of local consistency implies a global structure; namely,
that there is a global degree $d$ function $g$ such that $g|_{C} \equiv T(C)$
for at least $\Omega(\epsilon)$ fraction of the cubes.
</p>
<p>We show that the cube versus cube low degree test has soundness ${\sf
poly}(d)/q$. This result achieves the optimal dependence on $q$ for soundness
in low degree testing and improves upon previous soundness results of ${\sf
poly}(d)/q^{1/2}$ due to Bhangale, Dinur and Navon.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Minzer_D/0/1/0/all/0/1">Dor Minzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1">Kai Zheng</a></p><p>The Cube versus Cube test is a variant of the well-known Plane versus Plane
test of Raz and Safra, in which to each $3$-dimensional affine subspace $C$ of
$\mathbb{F}_q^n$, a polynomial of degree at most $d$, $T(C)$, is assigned in a
somewhat locally consistent manner: taking two cubes $C_1, C_2$ that intersect
in a plane uniformly at random, the probability that $T(C_1)$ and $T(C_2)$
agree on $C_1\cap C_2$ is at least some $\epsilon$. An element of interest is
the soundness threshold of this test, i.e. the smallest value of $\epsilon$,
such that this amount of local consistency implies a global structure; namely,
that there is a global degree $d$ function $g$ such that $g|_{C} \equiv T(C)$
for at least $\Omega(\epsilon)$ fraction of the cubes.
</p>
<p>We show that the cube versus cube low degree test has soundness ${\sf
poly}(d)/q$. This result achieves the optimal dependence on $q$ for soundness
in low degree testing and improves upon previous soundness results of ${\sf
poly}(d)/q^{1/2}$ due to Bhangale, Dinur and Navon.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09482'>Unique-Neighbor-Like Expansion and Group-Independent Cosystolic Expansion</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Tali Kaufman, David Mass</p><p>In recent years, high dimensional expanders have been found to have a variety
of applications in theoretical computer science, such as efficient CSPs
approximations, improved sampling and list-decoding algorithms, and more.
Within that, an important high dimensional expansion notion is \emph{cosystolic
expansion}, which has found applications in the construction of efficiently
decodable quantum codes and in proving lower bounds for CSPs.
</p>
<p>Cosystolic expansion is considered with systems of equations over a group
where the variables and equations correspond to faces of the complex. Previous
works that studied cosystolic expansion were tailored to the specific group
$\mathbb{F}_2$. In particular, Kaufman, Kazhdan and Lubotzky (FOCS 2014), and
Evra and Kaufman (STOC 2016) in their breakthrough works, who solved a famous
open question of Gromov, have studied a notion which we term ``parity''
expansion for small sets. They showed that small sets of $k$-faces have
proportionally many $(k+1)$-faces that contain \emph{an odd number} of
$k$-faces from the set. Parity expansion for small sets could be used to imply
cosystolic expansion only over $\mathbb{F}_2$.
</p>
<p>In this work we introduce a stronger \emph{unique-neighbor-like} expansion
for small sets. We show that small sets of $k$-faces have proportionally many
$(k+1)$-faces that contain \emph{exactly one} $k$-face from the set. This
notion is fundamentally stronger than parity expansion and cannot be implied by
previous works.
</p>
<p>We then show, utilizing the new unique-neighbor-like expansion notion
introduced in this work, that cosystolic expansion can be made
\emph{group-independent}, i.e., unique-neighbor-like expansion for small sets
implies cosystolic expansion \emph{over any group}.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kaufman_T/0/1/0/all/0/1">Tali Kaufman</a>, <a href="http://arxiv.org/find/cs/1/au:+Mass_D/0/1/0/all/0/1">David Mass</a></p><p>In recent years, high dimensional expanders have been found to have a variety
of applications in theoretical computer science, such as efficient CSPs
approximations, improved sampling and list-decoding algorithms, and more.
Within that, an important high dimensional expansion notion is \emph{cosystolic
expansion}, which has found applications in the construction of efficiently
decodable quantum codes and in proving lower bounds for CSPs.
</p>
<p>Cosystolic expansion is considered with systems of equations over a group
where the variables and equations correspond to faces of the complex. Previous
works that studied cosystolic expansion were tailored to the specific group
$\mathbb{F}_2$. In particular, Kaufman, Kazhdan and Lubotzky (FOCS 2014), and
Evra and Kaufman (STOC 2016) in their breakthrough works, who solved a famous
open question of Gromov, have studied a notion which we term ``parity''
expansion for small sets. They showed that small sets of $k$-faces have
proportionally many $(k+1)$-faces that contain \emph{an odd number} of
$k$-faces from the set. Parity expansion for small sets could be used to imply
cosystolic expansion only over $\mathbb{F}_2$.
</p>
<p>In this work we introduce a stronger \emph{unique-neighbor-like} expansion
for small sets. We show that small sets of $k$-faces have proportionally many
$(k+1)$-faces that contain \emph{exactly one} $k$-face from the set. This
notion is fundamentally stronger than parity expansion and cannot be implied by
previous works.
</p>
<p>We then show, utilizing the new unique-neighbor-like expansion notion
introduced in this work, that cosystolic expansion can be made
\emph{group-independent}, i.e., unique-neighbor-like expansion for small sets
implies cosystolic expansion \emph{over any group}.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09485'>Double Balanced Sets in High Dimensional Expanders</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Tali Kaufman, David Mass</p><p>Recent works have shown that expansion of pseudorandom sets is of great
importance. However, all current works on pseudorandom sets are limited only to
product (or approximate product) spaces, where Fourier Analysis methods could
be applied. In this work we ask the natural question whether pseudorandom sets
are relevant in domains where Fourier Analysis methods cannot be applied, e.g.,
one-sided local spectral expanders.
</p>
<p>We take the first step in the path of answering this question. We put forward
a new definition for pseudorandom sets, which we call ``double balanced sets''.
We demonstrate the strength of our new definition by showing that small double
balanced sets in one-sided local spectral expanders have very strong expansion
properties, such as unique-neighbor-like expansion. We further show that
cohomologies in cosystolic expanders are double balanced, and use the newly
derived strong expansion properties of double balanced sets in order to obtain
an exponential improvement over the current state of the art lower bound on
their minimal distance.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kaufman_T/0/1/0/all/0/1">Tali Kaufman</a>, <a href="http://arxiv.org/find/cs/1/au:+Mass_D/0/1/0/all/0/1">David Mass</a></p><p>Recent works have shown that expansion of pseudorandom sets is of great
importance. However, all current works on pseudorandom sets are limited only to
product (or approximate product) spaces, where Fourier Analysis methods could
be applied. In this work we ask the natural question whether pseudorandom sets
are relevant in domains where Fourier Analysis methods cannot be applied, e.g.,
one-sided local spectral expanders.
</p>
<p>We take the first step in the path of answering this question. We put forward
a new definition for pseudorandom sets, which we call ``double balanced sets''.
We demonstrate the strength of our new definition by showing that small double
balanced sets in one-sided local spectral expanders have very strong expansion
properties, such as unique-neighbor-like expansion. We further show that
cohomologies in cosystolic expanders are double balanced, and use the newly
derived strong expansion properties of double balanced sets in order to obtain
an exponential improvement over the current state of the art lower bound on
their minimal distance.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09198'>Cooperative 2D Reconfiguration using Spatio-Temporal Planning and Load Transferring</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Javier Garcia, Michael Yannuzzi, Peter Kramer, Christian Rieck, S&#xe1;ndor P. Fekete, Aaron T. Becker</p><p>We present progress on the problem of reconfiguring a 2D arrangement of
building material by a cooperative set of robots. These robots are subjected to
the constraints of avoiding obstacles and maintaining connectivity of the
structure. We develop two reconfiguration methods, one based on spatio-temporal
planning, and one based on target swapping. Both methods achieve coordinated
motion of robots by avoiding deadlocks and maintaining all constraints. Both
methods also increase efficiency by reducing the amount of waiting times and
lowering combined travel costs. The resulting progress is validated by
simulations that also scale the number of robots.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1">Javier Garcia</a>, <a href="http://arxiv.org/find/cs/1/au:+Yannuzzi_M/0/1/0/all/0/1">Michael Yannuzzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kramer_P/0/1/0/all/0/1">Peter Kramer</a>, <a href="http://arxiv.org/find/cs/1/au:+Rieck_C/0/1/0/all/0/1">Christian Rieck</a>, <a href="http://arxiv.org/find/cs/1/au:+Fekete_S/0/1/0/all/0/1">S&#xe1;ndor P. Fekete</a>, <a href="http://arxiv.org/find/cs/1/au:+Becker_A/0/1/0/all/0/1">Aaron T. Becker</a></p><p>We present progress on the problem of reconfiguring a 2D arrangement of
building material by a cooperative set of robots. These robots are subjected to
the constraints of avoiding obstacles and maintaining connectivity of the
structure. We develop two reconfiguration methods, one based on spatio-temporal
planning, and one based on target swapping. Both methods achieve coordinated
motion of robots by avoiding deadlocks and maintaining all constraints. Both
methods also increase efficiency by reducing the amount of waiting times and
lowering combined travel costs. The resulting progress is validated by
simulations that also scale the number of robots.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09328'>Covering and packing with homothets of limited capacity</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Oriol Sol&#xe9; Pi</p><p>This work revolves around the two following questions: Given a convex body
$C\subset\mathbb{R}^d$, a positive integer $k$ and a finite set
$S\subset\mathbb{R}^d$ (or a finite $\mu$ Borel measure in $\mathbb{R}^d$), how
many homothets of $C$ are required to cover $S$ if no homothet is allowed to
cover more than $k$ points of $S$ (or have measure more than $k$)? how many
homothets of $C$ can be packed if each of them must cover at least $k$ points
of $S$ (or have measure at least $k$)? We prove that, so long as $S$ is not too
degenerate, the answer to both questions is $\Theta_d(\frac{|S|}{k})$, where
the hidden constant is independent of $d$, this is clearly best possible up to
a multiplicative constant. Analogous results hold in the case of measures. Then
we introduce a generalization of the standard covering and packing densities of
a convex body $C$ to Borel measure spaces in $\mathbb{R}^d$ and, using the
aforementioned bounds, we show that they are bounded from above and below,
respectively, by functions of $d$. As an intermediate result, we give a simple
proof the existence of weak $\epsilon$-nets of size $O(\frac{1}{\epsilon})$ for
the range space induced by all homothets of $C$. Following some recent work in
discrete geometry, we investigate the case $d=k=2$ in greater detail. We also
provide polynomial time algorithms for constructing a packing/covering
exhibiting the $\Theta_d(\frac{|S|}{k})$ bound mentioned above in the case that
$C$ is an Euclidean ball. Finally, it is shown that if $C$ is a square then it
is NP-hard to decide whether $S$ can be covered by $\frac{|S|}{4}$ squares
containing $4$ points each.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Pi_O/0/1/0/all/0/1">Oriol Sol&#xe9; Pi</a></p><p>This work revolves around the two following questions: Given a convex body
$C\subset\mathbb{R}^d$, a positive integer $k$ and a finite set
$S\subset\mathbb{R}^d$ (or a finite $\mu$ Borel measure in $\mathbb{R}^d$), how
many homothets of $C$ are required to cover $S$ if no homothet is allowed to
cover more than $k$ points of $S$ (or have measure more than $k$)? how many
homothets of $C$ can be packed if each of them must cover at least $k$ points
of $S$ (or have measure at least $k$)? We prove that, so long as $S$ is not too
degenerate, the answer to both questions is $\Theta_d(\frac{|S|}{k})$, where
the hidden constant is independent of $d$, this is clearly best possible up to
a multiplicative constant. Analogous results hold in the case of measures. Then
we introduce a generalization of the standard covering and packing densities of
a convex body $C$ to Borel measure spaces in $\mathbb{R}^d$ and, using the
aforementioned bounds, we show that they are bounded from above and below,
respectively, by functions of $d$. As an intermediate result, we give a simple
proof the existence of weak $\epsilon$-nets of size $O(\frac{1}{\epsilon})$ for
the range space induced by all homothets of $C$. Following some recent work in
discrete geometry, we investigate the case $d=k=2$ in greater detail. We also
provide polynomial time algorithms for constructing a packing/covering
exhibiting the $\Theta_d(\frac{|S|}{k})$ bound mentioned above in the case that
$C$ is an Euclidean ball. Finally, it is shown that if $C$ is a square then it
is NP-hard to decide whether $S$ can be covered by $\frac{|S|}{4}$ squares
containing $4$ points each.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09729'>Rounding via Low Dimensional Embeddings</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mark Braverman, Dor Minzer</p><p>A regular graph $G = (V,E)$ is an $(\varepsilon,\gamma)$ small-set expander
if for any set of vertices of fractional size at most $\varepsilon$, at least
$\gamma$ of the edges that are adjacent to it go outside. In this paper, we
give a unified approach to several known complexity-theoretic results on
small-set expanders. In particular, we show:
</p>
<p>1. Max-Cut: we show that if a regular graph $G = (V,E)$ is an
$(\varepsilon,\gamma)$ small-set expander that contains a cut of fractional
size at least $1-\delta$, then one can find in $G$ a cut of fractional size at
least $1-O\left(\frac{\delta}{\varepsilon\gamma^6}\right)$ in polynomial time.
</p>
<p>2. Improved spectral partitioning, Cheeger's inequality and the parallel
repetition theorem over small-set expanders. The general form of each one of
these results involves square-root loss that comes from certain rounding
procedure, and we show how this can be avoided over small set expanders.
</p>
<p>Our main idea is to project a high dimensional vector solution into a
low-dimensional space while roughly maintaining $\ell_2^2$ distances, and then
perform a pre-processing step using low-dimensional geometry and the properties
of $\ell_2^2$ distances over it. This pre-processing leverages the small-set
expansion property of the graph to transform a vector valued solution to a
different vector valued solution with additional structural properties, which
give rise to more efficient integral-solution rounding schemes.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Braverman_M/0/1/0/all/0/1">Mark Braverman</a>, <a href="http://arxiv.org/find/cs/1/au:+Minzer_D/0/1/0/all/0/1">Dor Minzer</a></p><p>A regular graph $G = (V,E)$ is an $(\varepsilon,\gamma)$ small-set expander
if for any set of vertices of fractional size at most $\varepsilon$, at least
$\gamma$ of the edges that are adjacent to it go outside. In this paper, we
give a unified approach to several known complexity-theoretic results on
small-set expanders. In particular, we show:
</p>
<p>1. Max-Cut: we show that if a regular graph $G = (V,E)$ is an
$(\varepsilon,\gamma)$ small-set expander that contains a cut of fractional
size at least $1-\delta$, then one can find in $G$ a cut of fractional size at
least $1-O\left(\frac{\delta}{\varepsilon\gamma^6}\right)$ in polynomial time.
</p>
<p>2. Improved spectral partitioning, Cheeger's inequality and the parallel
repetition theorem over small-set expanders. The general form of each one of
these results involves square-root loss that comes from certain rounding
procedure, and we show how this can be avoided over small set expanders.
</p>
<p>Our main idea is to project a high dimensional vector solution into a
low-dimensional space while roughly maintaining $\ell_2^2$ distances, and then
perform a pre-processing step using low-dimensional geometry and the properties
of $\ell_2^2$ distances over it. This pre-processing leverages the small-set
expansion property of the graph to transform a vector valued solution to a
different vector valued solution with additional structural properties, which
give rise to more efficient integral-solution rounding schemes.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09603'>(Re)packing Equal Disks into Rectangle</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Fedor V. Fomin, Petr A. Golovach, Tanmay Inamdar, Saket Saurabh, Meirav Zehavi</p><p>The problem of packing of equal disks (or circles) into a rectangle is a
fundamental geometric problem. (By a packing here we mean an arrangement of
disks in a rectangle without overlapping.) We consider the following
algorithmic generalization of the equal disk packing problem. In this problem,
for a given packing of equal disks into a rectangle, the question is whether by
changing positions of a small number of disks, we can allocate space for
packing more disks. More formally, in the repacking problem, for a given set of
$n$ equal disks packed into a rectangle and integers $k$ and $h$, we ask
whether it is possible by changing positions of at most $h$ disks to pack $n+k$
disks. Thus the problem of packing equal disks is the special case of our
problem with $n=h=0$.
</p>
<p>While the computational complexity of packing equal disks into a rectangle
remains open, we prove that the repacking problem is NP-hard already for $h=0$.
Our main algorithmic contribution is an algorithm that solves the repacking
problem in time $(h+k)^{O(h+k)}\cdot |I|^{O(1)}$, where $I$ is the input size.
That is, the problem is fixed-parameter tractable parameterized by $k$ and $h$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Fomin_F/0/1/0/all/0/1">Fedor V. Fomin</a>, <a href="http://arxiv.org/find/cs/1/au:+Golovach_P/0/1/0/all/0/1">Petr A. Golovach</a>, <a href="http://arxiv.org/find/cs/1/au:+Inamdar_T/0/1/0/all/0/1">Tanmay Inamdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Saurabh_S/0/1/0/all/0/1">Saket Saurabh</a>, <a href="http://arxiv.org/find/cs/1/au:+Zehavi_M/0/1/0/all/0/1">Meirav Zehavi</a></p><p>The problem of packing of equal disks (or circles) into a rectangle is a
fundamental geometric problem. (By a packing here we mean an arrangement of
disks in a rectangle without overlapping.) We consider the following
algorithmic generalization of the equal disk packing problem. In this problem,
for a given packing of equal disks into a rectangle, the question is whether by
changing positions of a small number of disks, we can allocate space for
packing more disks. More formally, in the repacking problem, for a given set of
$n$ equal disks packed into a rectangle and integers $k$ and $h$, we ask
whether it is possible by changing positions of at most $h$ disks to pack $n+k$
disks. Thus the problem of packing equal disks is the special case of our
problem with $n=h=0$.
</p>
<p>While the computational complexity of packing equal disks into a rectangle
remains open, we prove that the repacking problem is NP-hard already for $h=0$.
Our main algorithmic contribution is an algorithm that solves the repacking
problem in time $(h+k)^{O(h+k)}\cdot |I|^{O(1)}$, where $I$ is the input size.
That is, the problem is fixed-parameter tractable parameterized by $k$ and $h$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09133'>On the complexity of implementing Trotter steps</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Guang Hao Low, Yuan Su, Yu Tong, Minh C. Tran</p><p>Quantum dynamics can be simulated on a quantum computer by exponentiating
elementary terms from the Hamiltonian in a sequential manner. However, such an
implementation of Trotter steps has gate complexity depending on the total
Hamiltonian term number, comparing unfavorably to algorithms using more
advanced techniques. We develop methods to perform faster Trotter steps with
complexity sublinear in the number of terms. We achieve this for a class of
Hamiltonians whose interaction strength decays with distance according to power
law. Our methods include one based on a recursive block encoding and one based
on an average-cost simulation, overcoming the normalization-factor barrier of
these advanced quantum simulation techniques. We also realize faster Trotter
steps when certain blocks of Hamiltonian coefficients have low rank. Combining
with a tighter error analysis, we show that it suffices to use
$\left(\eta^{1/3}n^{1/3}+\frac{n^{2/3}}{\eta^{2/3}}\right)n^{1+o(1)}$ gates to
simulate uniform electron gas with $n$ spin orbitals and $\eta$ electrons in
second quantization in real space, asymptotically improving over the best
previous work. We obtain an analogous result when the external potential of
nuclei is introduced under the Born-Oppenheimer approximation. We prove a
circuit lower bound when the Hamiltonian coefficients take a continuum range of
values, showing that generic $n$-qubit $2$-local Hamiltonians with commuting
terms require at least $\Omega(n^2)$ gates to evolve with accuracy
$\epsilon=\Omega(1/poly(n))$ for time $t=\Omega(\epsilon)$. Our proof is based
on a gate-efficient reduction from the approximate synthesis of diagonal
unitaries within the Hamming weight-$2$ subspace, which may be of independent
interest. Our result thus suggests the use of Hamiltonian structural properties
as both necessary and sufficient to implement Trotter steps with lower gate
complexity.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Low_G/0/1/0/all/0/1">Guang Hao Low</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Su_Y/0/1/0/all/0/1">Yuan Su</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Tong_Y/0/1/0/all/0/1">Yu Tong</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Tran_M/0/1/0/all/0/1">Minh C. Tran</a></p><p>Quantum dynamics can be simulated on a quantum computer by exponentiating
elementary terms from the Hamiltonian in a sequential manner. However, such an
implementation of Trotter steps has gate complexity depending on the total
Hamiltonian term number, comparing unfavorably to algorithms using more
advanced techniques. We develop methods to perform faster Trotter steps with
complexity sublinear in the number of terms. We achieve this for a class of
Hamiltonians whose interaction strength decays with distance according to power
law. Our methods include one based on a recursive block encoding and one based
on an average-cost simulation, overcoming the normalization-factor barrier of
these advanced quantum simulation techniques. We also realize faster Trotter
steps when certain blocks of Hamiltonian coefficients have low rank. Combining
with a tighter error analysis, we show that it suffices to use
$\left(\eta^{1/3}n^{1/3}+\frac{n^{2/3}}{\eta^{2/3}}\right)n^{1+o(1)}$ gates to
simulate uniform electron gas with $n$ spin orbitals and $\eta$ electrons in
second quantization in real space, asymptotically improving over the best
previous work. We obtain an analogous result when the external potential of
nuclei is introduced under the Born-Oppenheimer approximation. We prove a
circuit lower bound when the Hamiltonian coefficients take a continuum range of
values, showing that generic $n$-qubit $2$-local Hamiltonians with commuting
terms require at least $\Omega(n^2)$ gates to evolve with accuracy
$\epsilon=\Omega(1/poly(n))$ for time $t=\Omega(\epsilon)$. Our proof is based
on a gate-efficient reduction from the approximate synthesis of diagonal
unitaries within the Hamming weight-$2$ subspace, which may be of independent
interest. Our result thus suggests the use of Hamiltonian structural properties
as both necessary and sufficient to implement Trotter steps with lower gate
complexity.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09251'>On the Power of Learning-Augmented BSTs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jingbang Chen, Li Chen</p><p>We present the first Learning-Augmented Binary Search Tree(BST) that attains
Static Optimality and Working-Set Bound given rough predictions. Following the
recent studies in algorithms with predictions and learned index structures,
Lin, Luo, and Woodruff (ICML 2022) introduced the concept of Learning-Augmented
BSTs, which aim to improve BSTs with learned advice. Unfortunately, their
construction gives only static optimality under strong assumptions on the
input.
</p>
<p>In this paper, we present a simple BST maintenance scheme that benefits from
learned advice. With proper predictions, the scheme achieves Static Optimality
and Working-Set Bound, respectively, which are important performance measures
for BSTs. Moreover, the scheme is robust to prediction errors and makes no
assumption on the input.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jingbang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Li Chen</a></p><p>We present the first Learning-Augmented Binary Search Tree(BST) that attains
Static Optimality and Working-Set Bound given rough predictions. Following the
recent studies in algorithms with predictions and learned index structures,
Lin, Luo, and Woodruff (ICML 2022) introduced the concept of Learning-Augmented
BSTs, which aim to improve BSTs with learned advice. Unfortunately, their
construction gives only static optimality under strong assumptions on the
input.
</p>
<p>In this paper, we present a simple BST maintenance scheme that benefits from
learned advice. With proper predictions, the scheme achieves Static Optimality
and Working-Set Bound, respectively, which are important performance measures
for BSTs. Moreover, the scheme is robust to prediction errors and makes no
assumption on the input.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09415'>Near-Optimal Distributed Computation of Small Vertex Cuts</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Merav Parter, Asaf Petruschka</p><p>We present near-optimal algorithms for detecting small vertex cuts in the
CONGEST model of distributed computing. Despite extensive research in this
area, our understanding of the vertex connectivity of a graph is still
incomplete, especially in the distributed setting. To this date, all
distributed algorithms for detecting cut vertices suffer from an inherent
dependency in the maximum degree of the graph, $\Delta$. Hence, in particular,
there is no truly sub-linear time algorithm for this problem, not even for
detecting a single cut vertex. We take a new algorithmic approach for vertex
connectivity which allows us to bypass the existing $\Delta$ barrier. As a
warm-up to our approach, we show a simple $\widetilde{O}(D)$-round randomized
algorithm for computing all cut vertices in a $D$-diameter $n$-vertex graph.
This improves upon the $O(D+\Delta/\log n)$-round algorithm of [Pritchard and
Thurimella, ICALP 2008]. Our key technical contribution is an
$\widetilde{O}(D)$-round randomized algorithm for computing all cut pairs in
the graph, improving upon the state-of-the-art $O(\Delta \cdot D)^4$-round
algorithm by [Parter, DISC '19]. Note that even for the considerably simpler
setting of edge cuts, currently $\widetilde{O}(D)$-round algorithms are known
only for detecting pairs of cut edges. Our approach is based on employing the
well-known linear graph sketching technique [Ahn, Guha and McGregor, SODA 2012]
along with the heavy-light tree decomposition of [Sleator and Tarjan, STOC
1981]. Combining this with a careful characterization of the survivable
subgraphs, allows us to determine the connectivity of $G \setminus \{x,y\}$ for
every pair $x,y \in V$, using $\widetilde{O}(D)$-rounds. We believe that the
tools provided in this paper are useful for omitting the $\Delta$-dependency
even for larger cut values.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Parter_M/0/1/0/all/0/1">Merav Parter</a>, <a href="http://arxiv.org/find/cs/1/au:+Petruschka_A/0/1/0/all/0/1">Asaf Petruschka</a></p><p>We present near-optimal algorithms for detecting small vertex cuts in the
CONGEST model of distributed computing. Despite extensive research in this
area, our understanding of the vertex connectivity of a graph is still
incomplete, especially in the distributed setting. To this date, all
distributed algorithms for detecting cut vertices suffer from an inherent
dependency in the maximum degree of the graph, $\Delta$. Hence, in particular,
there is no truly sub-linear time algorithm for this problem, not even for
detecting a single cut vertex. We take a new algorithmic approach for vertex
connectivity which allows us to bypass the existing $\Delta$ barrier. As a
warm-up to our approach, we show a simple $\widetilde{O}(D)$-round randomized
algorithm for computing all cut vertices in a $D$-diameter $n$-vertex graph.
This improves upon the $O(D+\Delta/\log n)$-round algorithm of [Pritchard and
Thurimella, ICALP 2008]. Our key technical contribution is an
$\widetilde{O}(D)$-round randomized algorithm for computing all cut pairs in
the graph, improving upon the state-of-the-art $O(\Delta \cdot D)^4$-round
algorithm by [Parter, DISC '19]. Note that even for the considerably simpler
setting of edge cuts, currently $\widetilde{O}(D)$-round algorithms are known
only for detecting pairs of cut edges. Our approach is based on employing the
well-known linear graph sketching technique [Ahn, Guha and McGregor, SODA 2012]
along with the heavy-light tree decomposition of [Sleator and Tarjan, STOC
1981]. Combining this with a careful characterization of the survivable
subgraphs, allows us to determine the connectivity of $G \setminus \{x,y\}$ for
every pair $x,y \in V$, using $\widetilde{O}(D)$-rounds. We believe that the
tools provided in this paper are useful for omitting the $\Delta$-dependency
even for larger cut values.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09606'>Incremental Approximate Maximum Flow in $m^{1/2+o(1)}$ update time</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Gramoz Goranci, Monika Henzinger</p><p>We show an $(1+\epsilon)$-approximation algorithm for maintaining maximum
$s$-$t$ flow under $m$ edge insertions in $m^{1/2+o(1)} \epsilon^{-1/2}$
amortized update time for directed, unweighted graphs. This constitutes the
first sublinear dynamic maximum flow algorithm in general sparse graphs with
arbitrarily good approximation guarantee.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Goranci_G/0/1/0/all/0/1">Gramoz Goranci</a>, <a href="http://arxiv.org/find/cs/1/au:+Henzinger_M/0/1/0/all/0/1">Monika Henzinger</a></p><p>We show an $(1+\epsilon)$-approximation algorithm for maintaining maximum
$s$-$t$ flow under $m$ edge insertions in $m^{1/2+o(1)} \epsilon^{-1/2}$
amortized update time for directed, unweighted graphs. This constitutes the
first sublinear dynamic maximum flow algorithm in general sparse graphs with
arbitrarily good approximation guarantee.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09618'>A (simple) classical algorithm for estimating Betti numbers</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Simon Apers, Sayantan Sen, D&#xe1;niel Szab&#xf3;</p><p>We describe a simple algorithm for estimating the $k$-th normalized Betti
number of a simplicial complex over $n$ elements using the path integral Monte
Carlo method. For a general simplicial complex, the running time of our
algorithm is $n^{O(\frac{1}{\gamma}\log\frac{1}{\varepsilon})}$ with $\gamma$
measuring the spectral gap of the combinatorial Laplacian and $\varepsilon \in
(0,1)$ the additive precision. In the case of a clique complex, the running
time of our algorithm improves to
$(n/\lambda_{\max})^{O(\frac{1}{\gamma}\log\frac{1}{\varepsilon})}$ with
$\lambda_{\max} \geq k$ the maximum eigenvalue of the combinatorial Laplacian.
Our algorithm provides a classical benchmark for a line of quantum algorithms
for estimating Betti numbers, and it matches their running time on clique
complexes when the spectral gap is constant and $k \in \Omega(n)$ or
$\lambda_{\max} \in \Omega(n)$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Apers_S/0/1/0/all/0/1">Simon Apers</a>, <a href="http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1">Sayantan Sen</a>, <a href="http://arxiv.org/find/cs/1/au:+Szabo_D/0/1/0/all/0/1">D&#xe1;niel Szab&#xf3;</a></p><p>We describe a simple algorithm for estimating the $k$-th normalized Betti
number of a simplicial complex over $n$ elements using the path integral Monte
Carlo method. For a general simplicial complex, the running time of our
algorithm is $n^{O(\frac{1}{\gamma}\log\frac{1}{\varepsilon})}$ with $\gamma$
measuring the spectral gap of the combinatorial Laplacian and $\varepsilon \in
(0,1)$ the additive precision. In the case of a clique complex, the running
time of our algorithm improves to
$(n/\lambda_{\max})^{O(\frac{1}{\gamma}\log\frac{1}{\varepsilon})}$ with
$\lambda_{\max} \geq k$ the maximum eigenvalue of the combinatorial Laplacian.
Our algorithm provides a classical benchmark for a line of quantum algorithms
for estimating Betti numbers, and it matches their running time on clique
complexes when the spectral gap is constant and $k \in \Omega(n)$ or
$\lambda_{\max} \in \Omega(n)$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09659'>Minimum Path Cover in Parameterized Linear Time</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Manuel Caceres, Massimo Cairo, Brendan Mumey, Romeo Rizzi, Alexandru I. Tomescu</p><p>A minimum path cover (MPC) of a directed acyclic graph (DAG) $G = (V,E)$ is a
minimum-size set of paths that together cover all the vertices of the DAG.
Computing an MPC is a basic polynomial problem, dating back to Dilworth's and
Fulkerson's results in the 1950s. Since the size $k$ of an MPC (also known as
the width) can be small in practical applications, research has also studied
algorithms whose running time is parameterized on $k$.
</p>
<p>We obtain a new MPC parameterized algorithm for DAGs running in time
$O(k^2|V| + |E|)$. Our algorithm is the first solving the problem in
parameterized linear time. Additionally, we obtain an edge sparsification
algorithm preserving the width of a DAG but reducing $|E|$ to less than $2|V|$.
This algorithm runs in time $O(k^2|V|)$ and requires an MPC of a DAG as input,
thus its total running time is the same as the running time of our MPC
algorithm.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Caceres_M/0/1/0/all/0/1">Manuel Caceres</a>, <a href="http://arxiv.org/find/cs/1/au:+Cairo_M/0/1/0/all/0/1">Massimo Cairo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mumey_B/0/1/0/all/0/1">Brendan Mumey</a>, <a href="http://arxiv.org/find/cs/1/au:+Rizzi_R/0/1/0/all/0/1">Romeo Rizzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomescu_A/0/1/0/all/0/1">Alexandru I. Tomescu</a></p><p>A minimum path cover (MPC) of a directed acyclic graph (DAG) $G = (V,E)$ is a
minimum-size set of paths that together cover all the vertices of the DAG.
Computing an MPC is a basic polynomial problem, dating back to Dilworth's and
Fulkerson's results in the 1950s. Since the size $k$ of an MPC (also known as
the width) can be small in practical applications, research has also studied
algorithms whose running time is parameterized on $k$.
</p>
<p>We obtain a new MPC parameterized algorithm for DAGs running in time
$O(k^2|V| + |E|)$. Our algorithm is the first solving the problem in
parameterized linear time. Additionally, we obtain an edge sparsification
algorithm preserving the width of a DAG but reducing $|E|$ to less than $2|V|$.
This algorithm runs in time $O(k^2|V|)$ and requires an MPC of a DAG as input,
thus its total running time is the same as the running time of our MPC
algorithm.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09665'>Features for the 0-1 knapsack problem based on inclusionwise maximal solutions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jorik Jooken, Pieter Leyman, Patrick De Causmaecker</p><p>Decades of research on the 0-1 knapsack problem led to very efficient
algorithms that are able to quickly solve large problem instances to
optimality. This prompted researchers to also investigate whether relatively
small problem instances exist that are hard for existing solvers and
investigate which features characterize their hardness. Previously the authors
proposed a new class of hard 0-1 knapsack problem instances and demonstrated
that the properties of so-called inclusionwise maximal solutions (IMSs) can be
important hardness indicators for this class. In the current paper, we
formulate several new computationally challenging problems related to the IMSs
of arbitrary 0-1 knapsack problem instances. Based on generalizations of
previous work and new structural results about IMSs, we formulate polynomial
and pseudopolynomial time algorithms for solving these problems. From this we
derive a set of 14 computationally expensive features, which we calculate for
two large datasets on a supercomputer in approximately 540 CPU-hours. We show
that the proposed features contain important information related to the
empirical hardness of a problem instance that was missing in earlier features
from the literature by training machine learning models that can accurately
predict the empirical hardness of a wide variety of 0-1 knapsack problem
instances. Using the instance space analysis methodology, we also show that
hard 0-1 knapsack problem instances are clustered together around a relatively
dense region of the instance space and several features behave differently in
the easy and hard parts of the instance space.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jooken_J/0/1/0/all/0/1">Jorik Jooken</a>, <a href="http://arxiv.org/find/cs/1/au:+Leyman_P/0/1/0/all/0/1">Pieter Leyman</a>, <a href="http://arxiv.org/find/cs/1/au:+Causmaecker_P/0/1/0/all/0/1">Patrick De Causmaecker</a></p><p>Decades of research on the 0-1 knapsack problem led to very efficient
algorithms that are able to quickly solve large problem instances to
optimality. This prompted researchers to also investigate whether relatively
small problem instances exist that are hard for existing solvers and
investigate which features characterize their hardness. Previously the authors
proposed a new class of hard 0-1 knapsack problem instances and demonstrated
that the properties of so-called inclusionwise maximal solutions (IMSs) can be
important hardness indicators for this class. In the current paper, we
formulate several new computationally challenging problems related to the IMSs
of arbitrary 0-1 knapsack problem instances. Based on generalizations of
previous work and new structural results about IMSs, we formulate polynomial
and pseudopolynomial time algorithms for solving these problems. From this we
derive a set of 14 computationally expensive features, which we calculate for
two large datasets on a supercomputer in approximately 540 CPU-hours. We show
that the proposed features contain important information related to the
empirical hardness of a problem instance that was missing in earlier features
from the literature by training machine learning models that can accurately
predict the empirical hardness of a wide variety of 0-1 knapsack problem
instances. Using the instance space analysis methodology, we also show that
hard 0-1 knapsack problem instances are clustered together around a relatively
dense region of the instance space and several features behave differently in
the easy and hard parts of the instance space.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09747'>Extensions of the $(p,q)$-Flexible-Graph-Connectivity model</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ishan Bansal, Joseph Cheriyan, Logan Grout, Sharat Ibrahimpur</p><p>We present approximation algorithms for network design problems in some
models related to the $(p,q)$-FGC model. Adjiashvili, Hommelsheim and
M\"uhlenthaler introduced the model of Flexible Graph Connectivity that we
denote by FGC. Boyd, Cheriyan, Haddadan and Ibrahimpur introduced a
generalization of FGC. Let $p\geq 1$ and $q\geq 0$ be integers. In an instance
of the $(p,q)$-Flexible Graph Connectivity problem, denoted $(p,q)$-FGC, we
have an undirected connected graph $G = (V,E)$, a partition of $E$ into a set
of safe edges and a set of unsafe edges, and nonnegative costs
$c\in\mathbb{R}_{\geq0}^E$ on the edges. A subset $F \subseteq E$ of edges is
feasible for the $(p,q)$-FGC problem if for any set of unsafe edges, $F'$, with
$|F'|\leq q$, the subgraph $(V, F \setminus F')$ is $p$-edge connected. The
algorithmic goal is to find a feasible edge-set $F$ that minimizes $c(F) =
\sum_{e \in F} c_e$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bansal_I/0/1/0/all/0/1">Ishan Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheriyan_J/0/1/0/all/0/1">Joseph Cheriyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Grout_L/0/1/0/all/0/1">Logan Grout</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahimpur_S/0/1/0/all/0/1">Sharat Ibrahimpur</a></p><p>We present approximation algorithms for network design problems in some
models related to the $(p,q)$-FGC model. Adjiashvili, Hommelsheim and
M\"uhlenthaler introduced the model of Flexible Graph Connectivity that we
denote by FGC. Boyd, Cheriyan, Haddadan and Ibrahimpur introduced a
generalization of FGC. Let $p\geq 1$ and $q\geq 0$ be integers. In an instance
of the $(p,q)$-Flexible Graph Connectivity problem, denoted $(p,q)$-FGC, we
have an undirected connected graph $G = (V,E)$, a partition of $E$ into a set
of safe edges and a set of unsafe edges, and nonnegative costs
$c\in\mathbb{R}_{\geq0}^E$ on the edges. A subset $F \subseteq E$ of edges is
feasible for the $(p,q)$-FGC problem if for any set of unsafe edges, $F'$, with
$|F'|\leq q$, the subgraph $(V, F \setminus F')$ is $p$-edge connected. The
algorithmic goal is to find a feasible edge-set $F$ that minimizes $c(F) =
\sum_{e \in F} c_e$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09776'>Cheeger Inequalities for Directed Graphs and Hypergraphs Using Reweighted Eigenvalues</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Lap Chi Lau, Kam Chuen Tung, Robert Wang</p><p>We derive Cheeger inequalities for directed graphs and hypergraphs using the
reweighted eigenvalue approach that was recently developed for vertex expansion
in undirected graphs [OZ22,KLT22,JPV22]. The goal is to develop a new spectral
theory for directed graphs and an alternative spectral theory for hypergraphs.
</p>
<p>The first main result is a Cheeger inequality relating the vertex expansion
$\vec{\psi}(G)$ of a directed graph $G$ to the vertex-capacitated maximum
reweighted second eigenvalue $\vec{\lambda}_2^{v*}$: \[ \vec{\lambda}_2^{v*}
\lesssim \vec{\psi}(G) \lesssim \sqrt{\vec{\lambda}_2^{v*} \cdot \log
(\Delta/\vec{\lambda}_2^{v*})}. \] This provides a combinatorial
characterization of the fastest mixing time of a directed graph by vertex
expansion, and builds a new connection between reweighted eigenvalued, vertex
expansion, and fastest mixing time for directed graphs.
</p>
<p>The second main result is a stronger Cheeger inequality relating the edge
conductance $\vec{\phi}(G)$ of a directed graph $G$ to the edge-capacitated
maximum reweighted second eigenvalue $\vec{\lambda}_2^{e*}$: \[
\vec{\lambda}_2^{e*} \lesssim \vec{\phi}(G) \lesssim \sqrt{\vec{\lambda}_2^{e*}
\cdot \log (1/\vec{\lambda}_2^{e*})}. \] This provides a certificate for a
directed graph to be an expander and a spectral algorithm to find a sparse cut
in a directed graph, playing a similar role as Cheeger's inequality in
certifying graph expansion and in the spectral partitioning algorithm for
undirected graphs.
</p>
<p>We also use this reweighted eigenvalue approach to derive the improved
Cheeger inequality for directed graphs, and furthermore to derive several
Cheeger inequalities for hypergraphs that match and improve the existing
results in [Lou15,CLTZ18]. These are supporting results that this provides a
unifying approach to lift the spectral theory for undirected graphs to more
general settings.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lau_L/0/1/0/all/0/1">Lap Chi Lau</a>, <a href="http://arxiv.org/find/cs/1/au:+Tung_K/0/1/0/all/0/1">Kam Chuen Tung</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Robert Wang</a></p><p>We derive Cheeger inequalities for directed graphs and hypergraphs using the
reweighted eigenvalue approach that was recently developed for vertex expansion
in undirected graphs [OZ22,KLT22,JPV22]. The goal is to develop a new spectral
theory for directed graphs and an alternative spectral theory for hypergraphs.
</p>
<p>The first main result is a Cheeger inequality relating the vertex expansion
$\vec{\psi}(G)$ of a directed graph $G$ to the vertex-capacitated maximum
reweighted second eigenvalue $\vec{\lambda}_2^{v*}$: \[ \vec{\lambda}_2^{v*}
\lesssim \vec{\psi}(G) \lesssim \sqrt{\vec{\lambda}_2^{v*} \cdot \log
(\Delta/\vec{\lambda}_2^{v*})}. \] This provides a combinatorial
characterization of the fastest mixing time of a directed graph by vertex
expansion, and builds a new connection between reweighted eigenvalued, vertex
expansion, and fastest mixing time for directed graphs.
</p>
<p>The second main result is a stronger Cheeger inequality relating the edge
conductance $\vec{\phi}(G)$ of a directed graph $G$ to the edge-capacitated
maximum reweighted second eigenvalue $\vec{\lambda}_2^{e*}$: \[
\vec{\lambda}_2^{e*} \lesssim \vec{\phi}(G) \lesssim \sqrt{\vec{\lambda}_2^{e*}
\cdot \log (1/\vec{\lambda}_2^{e*})}. \] This provides a certificate for a
directed graph to be an expander and a spectral algorithm to find a sparse cut
in a directed graph, playing a similar role as Cheeger's inequality in
certifying graph expansion and in the spectral partitioning algorithm for
undirected graphs.
</p>
<p>We also use this reweighted eigenvalue approach to derive the improved
Cheeger inequality for directed graphs, and furthermore to derive several
Cheeger inequalities for hypergraphs that match and improve the existing
results in [Lou15,CLTZ18]. These are supporting results that this provides a
unifying approach to lift the spectral theory for undirected graphs to more
general settings.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Thursday, November 17
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://gilkalai.wordpress.com/2022/11/17/amazing-justin-gilmer-gave-a-constant-lower-bound-for-the-union-closed-sets-conjecture/'>Amazing: Justin Gilmer gave a constant lower bound for the union-closed sets conjecture</a></h3>
        <p class='tr-article-feed'>from <a href='https://gilkalai.wordpress.com'>Gil Kalai</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Frankl&#8217;s conjecture (aka the union closed sets conjecture) asserts that if is a family of subsets of [n] (=: ) which is closed under union then there is an element such that Justin Gilmer just proved an amazing weaker form &#8230; Continue reading &#8594;
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Frankl&#8217;s conjecture (aka the union closed sets conjecture) asserts that if <img src="https://s0.wp.com/latex.php?latex=%5Ccal+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Ccal+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ccal+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;cal F" class="latex" /> is a family of subsets of [n] (=: <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C2%2C%5Cdots%2Cn+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5C%7B1%2C2%2C%5Cdots%2Cn+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5C%7B1%2C2%2C%5Cdots%2Cn+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;{1,2,&#92;dots,n &#92;}" class="latex" />) which is closed under union then there is an element <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k" class="latex" /> such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%7C%5C%7BS+%5Cin+%7B%5Ccal+F%7D%3A+k+%5Cin+S%5C%7D%7C+%5Cge+%5Cfrac+%7B1%7D%7B2%7D%7C%7B%5Ccal+F%7D%7C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7C%5C%7BS+%5Cin+%7B%5Ccal+F%7D%3A+k+%5Cin+S%5C%7D%7C+%5Cge+%5Cfrac+%7B1%7D%7B2%7D%7C%7B%5Ccal+F%7D%7C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7C%5C%7BS+%5Cin+%7B%5Ccal+F%7D%3A+k+%5Cin+S%5C%7D%7C+%5Cge+%5Cfrac+%7B1%7D%7B2%7D%7C%7B%5Ccal+F%7D%7C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="|&#92;{S &#92;in {&#92;cal F}: k &#92;in S&#92;}| &#92;ge &#92;frac {1}{2}|{&#92;cal F}|." class="latex" /></p>
<p>Justin Gilmer just proved an amazing weaker form of the conjecture asserting that there always exists an element <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k" class="latex" /> such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%7C%5C%7BS+%5Cin+%7B%5Ccal+F%7D%3A+k+%5Cin+S%5C%7D%7C+%5Cge%C2%A0+0.01+%7C%7B%5Ccal+F%7D%7C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7C%5C%7BS+%5Cin+%7B%5Ccal+F%7D%3A+k+%5Cin+S%5C%7D%7C+%5Cge%C2%A0+0.01+%7C%7B%5Ccal+F%7D%7C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7C%5C%7BS+%5Cin+%7B%5Ccal+F%7D%3A+k+%5Cin+S%5C%7D%7C+%5Cge%C2%A0+0.01+%7C%7B%5Ccal+F%7D%7C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="|&#92;{S &#92;in {&#92;cal F}: k &#92;in S&#92;}| &#92;ge  0.01 |{&#92;cal F}|." class="latex" /></p>
<p>This is am amazing progress! Congratulations, Justin.</p>
<p>The breakthrough paper, just posted on the arXiv is:</p>
<p><a href="https://arxiv.org/abs/2211.09055">A constant lower bound for the union-closed sets conjecture</a> by Justin Gilmer</p>
<p><strong>Abstract:</strong> We show that for any union-closed family  <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D+%5Csubseteq+2%5E%7B%5Bn%5D%7D%2C+%5Cmathcal%7BF%7D+%5Cneq+%5C%7B%5Cemptyset%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D+%5Csubseteq+2%5E%7B%5Bn%5D%7D%2C+%5Cmathcal%7BF%7D+%5Cneq+%5C%7B%5Cemptyset%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D+%5Csubseteq+2%5E%7B%5Bn%5D%7D%2C+%5Cmathcal%7BF%7D+%5Cneq+%5C%7B%5Cemptyset%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathcal{F} &#92;subseteq 2^{[n]}, &#92;mathcal{F} &#92;neq &#92;{&#92;emptyset&#92;}" class="latex" /> there exists an <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=i+%5Cin+%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=i+%5Cin+%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="i &#92;in [n]" class="latex" />  which is contained in a <span id="MathJax-Element-3-Frame" class="MathJax"><span id="MathJax-Span-29" class="math"><span id="MathJax-Span-30" class="mrow"><span id="MathJax-Span-31" class="mn">0.01</span></span></span></span> fraction of the sets in <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathcal+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathcal F" class="latex" />.</p>
<p>This is the first known constant lower bound, and improves upon the <img src="https://s0.wp.com/latex.php?latex=%5COmega%28%5Clog_2%28%5Cmathcal%7BF%7D%7C%29%5E%7B-1%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5COmega%28%5Clog_2%28%5Cmathcal%7BF%7D%7C%29%5E%7B-1%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5COmega%28%5Clog_2%28%5Cmathcal%7BF%7D%7C%29%5E%7B-1%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;Omega(&#92;log_2(&#92;mathcal{F}|)^{-1})" class="latex" /> bounds of Knill and Wójick.</p>
<p>Our result follows from an information theoretic strengthening of the conjecture. Specifically, we show that if <img src="https://s0.wp.com/latex.php?latex=A%2CB&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=A%2CB&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A%2CB&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="A,B" class="latex" /> are independent samples from a distribution over subsets of <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="[n]" class="latex" />  such that <img src="https://s0.wp.com/latex.php?latex=Pr%5Bi+%5Cin+A%5D+%3C+0.01&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=Pr%5Bi+%5Cin+A%5D+%3C+0.01&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=Pr%5Bi+%5Cin+A%5D+%3C+0.01&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="Pr[i &#92;in A] &lt; 0.01" class="latex" /> for all <span id="MathJax-Element-9-Frame" class="MathJax"><span id="MathJax-Span-83" class="math"><span id="MathJax-Span-84" class="mrow"><span id="MathJax-Span-85" class="mi"><img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="i" class="latex" /></span></span></span></span> and <img src="https://s0.wp.com/latex.php?latex=H%28A%29%3E0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=H%28A%29%3E0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=H%28A%29%3E0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="H(A)&gt;0" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=H%28A+%5Ccup+B%29%3E+H%28A%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=H%28A+%5Ccup+B%29%3E+H%28A%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=H%28A+%5Ccup+B%29%3E+H%28A%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="H(A &#92;cup B)&gt; H(A)" class="latex" />.</p>
<p>______</p>
<p>Mike Saks who first told me about the breakthrough wrote &#8220;the bound comes from a simple clever idea (using information theory) and 5 pages of gentle technical calculations.&#8221; (I thank Mike, Ryan <span class="gI"><span class="qu" role="gridcell"><span class="gD">Alweiss, and Nati Linial who wrote me about it.) </span></span></span></p>
<p>We mentioned Frankl&#8217;s conjecture several times including <a href="https://gilkalai.wordpress.com/2008/04/29/hello-world/">here</a>, <a href="https://gilkalai.wordpress.com/2017/12/26/ilam-karpas-frankls-conjecture-for-large-families/">here</a>, <a href="https://gilkalai.wordpress.com/2018/03/09/frankls-conjecture-for-large-families-ilan-karpas-proof/">here</a>, and <a href="https://gilkalai.wordpress.com/2021/01/29/possible-future-polymath-projects-2009-2021/">here</a>. <a href="https://gowers.wordpress.com/category/polymath11/">Polymath11</a> on Tim Gowers&#8217;s blog was devoted to the conjecture. Below the fold: What it will take to prove the conjecture in its full strength and another beautiful conjecture by Peter Frankl.</p>
<p><span id="more-23540"></span></p>
<h3>What is the limit of Gilmer&#8217;s method and what it will take to prove the Frankl conjecture</h3>
<p>Justin Gilmer&#8217;s mentions that proving a tight bout for Lemma 1 in the paper will push the 0.01 bound to <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B3-%5Csqrt+5%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cfrac%7B3-%5Csqrt+5%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B3-%5Csqrt+5%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;frac{3-&#92;sqrt 5}{2}" class="latex" />=<span id="cwos" class="qv3Wpe" dir="ltr">0.381966&#8230; . He also presents an appealing information-theoretic strengthening of the conjecture which may consist of a path toward a proof. </span></p>
<h3>Another beautiful conjecture by Peter Frankl</h3>
<p>To face a possible risk that Frankl&#8217;s &#8220;union closed&#8221; conjecture will be solved here is another beautiful conjecture by Peter Frankl.</p>
<p>A family of sets is convex if whenever <img src="https://s0.wp.com/latex.php?latex=A+%5Csubset+B+%5Csubset+C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=A+%5Csubset+B+%5Csubset+C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A+%5Csubset+B+%5Csubset+C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="A &#92;subset B &#92;subset C" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=A%2CC+%5Cin+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=A%2CC+%5Cin+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A%2CC+%5Cin+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="A,C &#92;in {&#92;cal F}" class="latex" /> then also <img src="https://s0.wp.com/latex.php?latex=B+%5Cin+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=B+%5Cin+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=B+%5Cin+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="B &#92;in {&#92;cal F}" class="latex" />.</p>
<p>Conjecture (<strong>P. Frankl</strong>):  Let <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="{&#92;cal F}" class="latex" /> be a convex family of subsets of <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="[n]" class="latex" />. Then there exists an antichain <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+G%7D+%5Csubset+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ccal+G%7D+%5Csubset+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+G%7D+%5Csubset+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="{&#92;cal G} &#92;subset {&#92;cal F}" class="latex" /> such that</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%7C%7B%5Ccal+G%7D%7C%2F%7C%7B%5Ccal+F%7D%7C+%5Cge+%7B%7Bn%7D+%5Cchoose+%7B%5Bn%2F2%5D%7D%7D%2F2%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7C%7B%5Ccal+G%7D%7C%2F%7C%7B%5Ccal+F%7D%7C+%5Cge+%7B%7Bn%7D+%5Cchoose+%7B%5Bn%2F2%5D%7D%7D%2F2%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7C%7B%5Ccal+G%7D%7C%2F%7C%7B%5Ccal+F%7D%7C+%5Cge+%7B%7Bn%7D+%5Cchoose+%7B%5Bn%2F2%5D%7D%7D%2F2%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="|{&#92;cal G}|/|{&#92;cal F}| &#92;ge {{n} &#92;choose {[n/2]}}/2^n." class="latex" /></p>
<p class="authors">By Gil Kalai</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T20:47:56Z">Thursday, November 17 2022, 20:47</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://blog.computationalcomplexity.org/2022/11/fall-jobs-post-2022.html'>Fall Jobs Post 2022</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>In the fall I try to make my predictions on the faculty job market for the spring. The outlook this year is hazy as we have two forces pushing in opposite directions.&nbsp;</p><p>Most of the largest tech companies are having layoffs and hiring freezes amidst a recession, higher expenses and a drop in revenue from cloud and advertising. Meanwhile computing has never had a more exciting (or scary) year of advances, particularly in generative AI. I can't remember such a dichotomy in the past. In the downturn after the 2008 financial crisis computing wasn't particularly exciting as the cloud, smart phones and machine learning were then just nascent technologies.</p><p>We'll probably have more competition in the academic job market as many new PhDs may decide to look at academic positions because of limited opportunities in large tech companies. We might even see a reverse migration from industry to academia from those who now might see universities as a safe haven.</p><p>What about the students? Will they still come in droves driven by the excitement in computing or get scared off by the downturn in the tech industry. They shouldn't worry--the market should turn around by the time they graduate and even today there are plenty of tech jobs in smaller and midsize tech companies as well as companies that deal with data, which is pretty much every company.</p><p>But perception matters more than reality. If students do stay away that might reduce pressure to grow CS departments.</p><p>Onto my usual advice. Give yourself a good virtual face. Have a well-designed web page with access to all your job materials and papers. Maintain your Google Scholar page. Add yourself to the CRA's&nbsp;CV database. Find a way to stand out, perhaps a short video describing your research.&nbsp;</p><p>Best source for finding jobs are the ads from the&nbsp;CRA&nbsp;and the&nbsp;ACM. For theoretical computer science specific postdoc and faculty positions check out&nbsp;TCS Jobs&nbsp;and&nbsp;Theory Announcements. If you have jobs to announce, please post to the above and/or feel free to leave a comment on this post. Even if you don't see an ad for a specific school they may still be hiring, check out their website or email someone at the department. You'll never know if you don't ask.</p><p>By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>In the fall I try to make my predictions on the faculty job market for the spring. The outlook this year is hazy as we have two forces pushing in opposite directions.&nbsp;</p><p>Most of the largest tech companies are having layoffs and hiring freezes amidst a recession, higher expenses and a drop in revenue from cloud and advertising. Meanwhile computing has never had a more exciting (or scary) year of advances, particularly in generative AI. I can't remember such a dichotomy in the past. In the downturn after the 2008 financial crisis computing wasn't particularly exciting as the cloud, smart phones and machine learning were then just nascent technologies.</p><p>We'll probably have more competition in the academic job market as many new PhDs may decide to look at academic positions because of limited opportunities in large tech companies. We might even see a reverse migration from industry to academia from those who now might see universities as a safe haven.</p><p>What about the students? Will they still come in droves driven by the excitement in computing or get scared off by the downturn in the tech industry. They shouldn't worry--the market should turn around by the time they graduate and even today there are plenty of tech jobs in smaller and midsize tech companies as well as companies that deal with data, which is pretty much every company.</p><p>But perception matters more than reality. If students do stay away that might reduce pressure to grow CS departments.</p><p>Onto my usual advice. Give yourself a good virtual face. Have a well-designed web page with access to all your job materials and papers. Maintain your Google Scholar page. Add yourself to the CRA's&nbsp;<a href="https://cra.org/cv-database/">CV database</a>. Find a way to stand out, perhaps a short video describing your research.&nbsp;</p><p>Best source for finding jobs are the ads from the&nbsp;<a href="https://cra.org/ads/">CRA</a>&nbsp;and the&nbsp;<a href="https://jobs.acm.org/">ACM</a>. For theoretical computer science specific postdoc and faculty positions check out&nbsp;<a href="https://cstheory-jobs.org/">TCS Jobs</a>&nbsp;and&nbsp;<a href="http://dmatheorynet.blogspot.com/">Theory Announcements</a>. If you have jobs to announce, please post to the above and/or feel free to leave a comment on this post. Even if you don't see an ad for a specific school they may still be hiring, check out their website or email someone at the department. You'll never know if you don't ask.</p><p class="authors">By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T14:44:00Z">Thursday, November 17 2022, 14:44</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/17/senior-faculty-position-at-williams-college-apply-by-december-1-2022/'>Senior Faculty Position at Williams College (apply by December 1, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The Department of Computer Science at Williams College invites applications for a tenured faculty position at the associate or full professor level beginning July 1, 2023. We welcome candidates from all areas of computer science who can contribute to the vibrancy of our academic community through their research, teaching, and service. Website: apply.interfolio.com/111662 Email: cshiring@williams.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The Department of Computer Science at Williams College invites applications for a tenured faculty position at the associate or full professor level beginning July 1, 2023. We welcome candidates from all areas of computer science who can contribute to the vibrancy of our academic community through their research, teaching, and service.</p>
<p>Website: <a href="https://apply.interfolio.com/111662">https://apply.interfolio.com/111662</a><br />
Email: cshiring@williams.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T14:11:06Z">Thursday, November 17 2022, 14:11</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/17/postdocs-at-max-planck-institute-for-informatics-apply-by-december-31-2022/'>Postdocs at Max Planck Institute for Informatics (apply by December 31, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We are looking for applicants from all areas of algorithms and complexity, including related areas like mathematical optimization, distributed computing, and algorithms engineering. Postdoctoral fellowships are available at the algorithms and complexity department for two years through the Guest Program of our institute. Website: www.mpi-inf.mpg.de/d1postdoc Email: d1office@mpi-inf.mpg.de
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>We are looking for applicants from all areas of algorithms and complexity, including related areas like mathematical optimization, distributed computing, and algorithms engineering. Postdoctoral fellowships are available at the algorithms and complexity department for two years through the Guest Program of our institute.</p>
<p>Website: <a href="http://www.mpi-inf.mpg.de/d1postdoc">http://www.mpi-inf.mpg.de/d1postdoc</a><br />
Email: d1office@mpi-inf.mpg.de</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T13:29:17Z">Thursday, November 17 2022, 13:29</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
