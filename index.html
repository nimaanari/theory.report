<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.4 (2022-04-12) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2022-11-08T11:32:27Z">Tuesday, November 08 2022, 11:32</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>Sébastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Tuesday, November 08
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.02694'>Geometry of Rounding</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jason Vander Woude, Peter Dixon, A. Pavan, Jamie Radcliffe, N. V. Vinodchandran</p><p>Rounding has proven to be a fundamental tool in theoretical computer science.
By observing that rounding and partitioning of $\mathbb{R}^d$ are equivalent,
we introduce the following natural partition problem which we call the {\em
secluded hypercube partition problem}: Given $k\in \mathbb{N}$ (ideally small)
and $\epsilon&gt;0$ (ideally large), is there a partition of $\mathbb{R}^d$ with
unit hypercubes such that for every point $p \in \mathbb{R}^d$, its closed
$\epsilon$-neighborhood (in the $\ell_{\infty}$ norm) intersects at most $k$
hypercubes?
</p>
<p>We undertake a comprehensive study of this partition problem. We prove that
for every $d\in \mathbb{N}$, there is an explicit (and efficiently computable)
hypercube partition of $\mathbb{R}^d$ with $k = d+1$ and $\epsilon =
\frac{1}{2d}$. We complement this construction by proving that the value of
$k=d+1$ is the best possible (for any $\epsilon$) for a broad class of
``reasonable'' partitions including hypercube partitions. We also investigate
the optimality of the parameter $\epsilon$ and prove that any partition in this
broad class that has $k=d+1$, must have $\epsilon\leq\frac{1}{2\sqrt{d}}$.
These bounds imply limitations of certain deterministic rounding schemes
existing in the literature. Furthermore, this general bound is based on the
currently known lower bounds for the dissection number of the cube, and
improvements to this bound will yield improvements to our bounds.
</p>
<p>While our work is motivated by the desire to understand rounding algorithms,
one of our main conceptual contributions is the introduction of the {\em
secluded hypercube partition problem}, which fits well with a long history of
investigations by mathematicians on various hypercube partitions/tilings of
Euclidean space.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Woude_J/0/1/0/all/0/1">Jason Vander Woude</a>, <a href="http://arxiv.org/find/cs/1/au:+Dixon_P/0/1/0/all/0/1">Peter Dixon</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavan_A/0/1/0/all/0/1">A. Pavan</a>, <a href="http://arxiv.org/find/cs/1/au:+Radcliffe_J/0/1/0/all/0/1">Jamie Radcliffe</a>, <a href="http://arxiv.org/find/cs/1/au:+Vinodchandran_N/0/1/0/all/0/1">N. V. Vinodchandran</a></p><p>Rounding has proven to be a fundamental tool in theoretical computer science.
By observing that rounding and partitioning of $\mathbb{R}^d$ are equivalent,
we introduce the following natural partition problem which we call the {\em
secluded hypercube partition problem}: Given $k\in \mathbb{N}$ (ideally small)
and $\epsilon&gt;0$ (ideally large), is there a partition of $\mathbb{R}^d$ with
unit hypercubes such that for every point $p \in \mathbb{R}^d$, its closed
$\epsilon$-neighborhood (in the $\ell_{\infty}$ norm) intersects at most $k$
hypercubes?
</p>
<p>We undertake a comprehensive study of this partition problem. We prove that
for every $d\in \mathbb{N}$, there is an explicit (and efficiently computable)
hypercube partition of $\mathbb{R}^d$ with $k = d+1$ and $\epsilon =
\frac{1}{2d}$. We complement this construction by proving that the value of
$k=d+1$ is the best possible (for any $\epsilon$) for a broad class of
``reasonable'' partitions including hypercube partitions. We also investigate
the optimality of the parameter $\epsilon$ and prove that any partition in this
broad class that has $k=d+1$, must have $\epsilon\leq\frac{1}{2\sqrt{d}}$.
These bounds imply limitations of certain deterministic rounding schemes
existing in the literature. Furthermore, this general bound is based on the
currently known lower bounds for the dissection number of the cube, and
improvements to this bound will yield improvements to our bounds.
</p>
<p>While our work is motivated by the desire to understand rounding algorithms,
one of our main conceptual contributions is the introduction of the {\em
secluded hypercube partition problem}, which fits well with a long history of
investigations by mathematicians on various hypercube partitions/tilings of
Euclidean space.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-08T01:30:00Z">Tuesday, November 08 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.02926'>Parity Games of Bounded Tree-Depth</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Konrad Staniszewski (University of Warsaw, IDEAS NCBR Sp. z o.o.)</p><p>The exact complexity of solving parity games is a major open problem. Several
authors have searched for efficient algorithms over specific classes of graphs.
In particular, Obdr\v{z}\'{a}lek showed that for graphs of bounded tree-width
or clique-width, the problem is in $\mathrm{P}$, which was later improved by
Ganardi, who showed that it is even in $\mathrm{LOGCFL}$ (with an additional
assumption for clique-width case). Here we extend this line of research by
showing that for graphs of bounded tree-depth the problem of solving parity
games is in logspace uniform $\text{AC}^0$. We achieve this by first
considering a parameter that we obtain from a modification of clique-width,
which we call shallow clique-width. We subsequently provide a suitable
reduction.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Staniszewski_K/0/1/0/all/0/1">Konrad Staniszewski</a> (University of Warsaw, IDEAS NCBR Sp. z o.o.)</p><p>The exact complexity of solving parity games is a major open problem. Several
authors have searched for efficient algorithms over specific classes of graphs.
In particular, Obdr\v{z}\'{a}lek showed that for graphs of bounded tree-width
or clique-width, the problem is in $\mathrm{P}$, which was later improved by
Ganardi, who showed that it is even in $\mathrm{LOGCFL}$ (with an additional
assumption for clique-width case). Here we extend this line of research by
showing that for graphs of bounded tree-depth the problem of solving parity
games is in logspace uniform $\text{AC}^0$. We achieve this by first
considering a parameter that we obtain from a modification of clique-width,
which we call shallow clique-width. We subsequently provide a suitable
reduction.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-08T01:30:00Z">Tuesday, November 08 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.03168'>Approximate Graph Colouring and the Hollow Shadow</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Lorenzo Ciardo, Stanislav &#x17d;ivn&#xfd;</p><p>We show that approximate graph colouring is not solved by constantly many
levels of the lift-and-project hierarchy for the combined basic linear
programming and affine integer programming relaxation. The proof involves a
construction of tensors whose fixed-dimensional projections are equal up to
reflection and satisfy a sparsity condition, which may be of independent
interest.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ciardo_L/0/1/0/all/0/1">Lorenzo Ciardo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zivny_S/0/1/0/all/0/1">Stanislav &#x17d;ivn&#xfd;</a></p><p>We show that approximate graph colouring is not solved by constantly many
levels of the lift-and-project hierarchy for the combined basic linear
programming and affine integer programming relaxation. The proof involves a
construction of tensors whose fixed-dimensional projections are equal up to
reflection and satisfy a sparsity condition, which may be of independent
interest.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-08T01:30:00Z">Tuesday, November 08 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.02835'>Approximation of Discs by Octagons on Pixel-Plane via Jaccards Proximity Criterion: Theoretical Approach and Experimental Results Analysis</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Irakli Dochviri, Alexander Gamkrelidze, Revaz Kurdiani</p><p>In the present paper we study approximation of discs by octagons on the pixel
plane. To decide which octagon approximates better the given disc we use
Jaccard's distance. The table of Jaccard's distances (calculated by a software
created for these purposes) are presented at the end of the paper. The results
for proximity are given in the form of a graph. Some properties of considered
octagons are also studied
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dochviri_I/0/1/0/all/0/1">Irakli Dochviri</a>, <a href="http://arxiv.org/find/cs/1/au:+Gamkrelidze_A/0/1/0/all/0/1">Alexander Gamkrelidze</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurdiani_R/0/1/0/all/0/1">Revaz Kurdiani</a></p><p>In the present paper we study approximation of discs by octagons on the pixel
plane. To decide which octagon approximates better the given disc we use
Jaccard's distance. The table of Jaccard's distances (calculated by a software
created for these purposes) are presented at the end of the paper. The results
for proximity are given in the form of a graph. Some properties of considered
octagons are also studied
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-08T01:30:00Z">Tuesday, November 08 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.02905'>Density of triangulated ternary disc packings</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Thomas Fernique, Daria Pchelina</p><p>We consider ternary disc packings of the plane, i.e. the packings using discs
of three different radii. Packings in which each ''hole'' is bounded by three
pairwise tangent discs are called triangulated. There are 164 pairs $(r,s)$,
$1{&gt;}r{&gt;}s$, allowing triangulated packings by discs of radii 1, $r$ and $s$.
In this paper, we enhance existing methods of dealing with maximal-density
packings in order to find ternary triangulated packings which maximize the
density among all the packings with the same disc radii. We showed for 16 pairs
that the density is maximized by a triangulated ternary packing; for 15 other
pairs, we proved the density to be maximized by a triangulated packing using
only two sizes of discs; for 40 pairs, we found non-triangulated packings
strictly denser than any triangulated one; finally, we classified the remaining
cases where our methods are not applicable.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Fernique_T/0/1/0/all/0/1">Thomas Fernique</a>, <a href="http://arxiv.org/find/cs/1/au:+Pchelina_D/0/1/0/all/0/1">Daria Pchelina</a></p><p>We consider ternary disc packings of the plane, i.e. the packings using discs
of three different radii. Packings in which each ''hole'' is bounded by three
pairwise tangent discs are called triangulated. There are 164 pairs $(r,s)$,
$1{&gt;}r{&gt;}s$, allowing triangulated packings by discs of radii 1, $r$ and $s$.
In this paper, we enhance existing methods of dealing with maximal-density
packings in order to find ternary triangulated packings which maximize the
density among all the packings with the same disc radii. We showed for 16 pairs
that the density is maximized by a triangulated ternary packing; for 15 other
pairs, we proved the density to be maximized by a triangulated packing using
only two sizes of discs; for 40 pairs, we found non-triangulated packings
strictly denser than any triangulated one; finally, we classified the remaining
cases where our methods are not applicable.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-08T01:30:00Z">Tuesday, November 08 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.02951'>Map matching queries on realistic input graphs under the Fr\'echet distance</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Joachim Gudmundsson, Martin P. Seybold, Sampson Wong</p><p>Map matching is a common preprocessing step for analysing vehicle
trajectories. In the theory community, the most popular approach for map
matching is to compute a path on the road network that is the most spatially
similar to the trajectory, where spatial similarity is measured using the
Fr\'echet distance. A shortcoming of existing map matching algorithms under the
Fr\'echet distance is that every time a trajectory is matched, the entire road
network needs to be reprocessed from scratch. An open problem is whether one
can preprocess the road network into a data structure, so that map matching
queries can be answered in sublinear time.
</p>
<p>In this paper, we investigate map matching queries under the Fr\'echet
distance. We provide a negative result for geometric planar graphs. We show
that, unless SETH fails, there is no data structure that can be constructed in
polynomial time that answers map matching queries in $O((pq)^{1-\delta})$ query
time for any $\delta &gt; 0$, where $p$ and $q$ are the complexities of the
geometric planar graph and the query trajectory, respectively. We provide a
positive result for realistic input graphs, which we regard as the main result
of this paper. We show that for $c$-packed graphs, one can construct a data
structure of $\tilde O(cp)$ size that can answer $(1+\varepsilon)$-approximate
map matching queries in $\tilde O(c^4 q \log^4 p)$ time, where $\tilde
O(\cdot)$ hides lower-order factors and dependence of $\varepsilon$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gudmundsson_J/0/1/0/all/0/1">Joachim Gudmundsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Seybold_M/0/1/0/all/0/1">Martin P. Seybold</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_S/0/1/0/all/0/1">Sampson Wong</a></p><p>Map matching is a common preprocessing step for analysing vehicle
trajectories. In the theory community, the most popular approach for map
matching is to compute a path on the road network that is the most spatially
similar to the trajectory, where spatial similarity is measured using the
Fr\'echet distance. A shortcoming of existing map matching algorithms under the
Fr\'echet distance is that every time a trajectory is matched, the entire road
network needs to be reprocessed from scratch. An open problem is whether one
can preprocess the road network into a data structure, so that map matching
queries can be answered in sublinear time.
</p>
<p>In this paper, we investigate map matching queries under the Fr\'echet
distance. We provide a negative result for geometric planar graphs. We show
that, unless SETH fails, there is no data structure that can be constructed in
polynomial time that answers map matching queries in $O((pq)^{1-\delta})$ query
time for any $\delta &gt; 0$, where $p$ and $q$ are the complexities of the
geometric planar graph and the query trajectory, respectively. We provide a
positive result for realistic input graphs, which we regard as the main result
of this paper. We show that for $c$-packed graphs, one can construct a data
structure of $\tilde O(cp)$ size that can answer $(1+\varepsilon)$-approximate
map matching queries in $\tilde O(c^4 q \log^4 p)$ time, where $\tilde
O(\cdot)$ hides lower-order factors and dependence of $\varepsilon$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-08T01:30:00Z">Tuesday, November 08 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.02713'>A degree 4 sum-of-squares lower bound for the clique number of the Paley graph</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Dmitriy Kunisky, Xifan Yu</p><p>We prove that the degree 4 sum-of-squares (SOS) relaxation of the clique
number of the Paley graph on a prime number $p$ of vertices has value at least
$\Omega(p^{1/3})$. This is in contrast to the widely believed conjecture that
the actual clique number of the Paley graph is $O(\mathrm{polylog}(p))$. Our
result may be viewed as a derandomization of that of Deshpande and Montanari
(2015), who showed the same lower bound (up to $\mathrm{polylog}(p)$ terms)
with high probability for the Erd\H{o}s-R\'{e}nyi random graph on $p$ vertices,
whose clique number is with high probability $O(\log(p))$. We also show that
our lower bound is optimal for the Feige-Krauthgamer construction of
pseudomoments, derandomizing an argument of Kelner. Finally, we present
numerical experiments indicating that the value of the degree 4 SOS relaxation
of the Paley graph may scale as $O(p^{1/2 - \epsilon})$ for some $\epsilon &gt;
0$, and give a matrix norm calculation indicating that the pseudocalibration
proof strategy for SOS lower bounds for random graphs will not immediately
transfer to the Paley graph. Taken together, our results suggest that degree 4
SOS may break the "$\sqrt{p}$ barrier" for upper bounds on the clique number of
Paley graphs, but prove that it can at best improve the exponent from $1/2$ to
$1/3$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kunisky_D/0/1/0/all/0/1">Dmitriy Kunisky</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xifan Yu</a></p><p>We prove that the degree 4 sum-of-squares (SOS) relaxation of the clique
number of the Paley graph on a prime number $p$ of vertices has value at least
$\Omega(p^{1/3})$. This is in contrast to the widely believed conjecture that
the actual clique number of the Paley graph is $O(\mathrm{polylog}(p))$. Our
result may be viewed as a derandomization of that of Deshpande and Montanari
(2015), who showed the same lower bound (up to $\mathrm{polylog}(p)$ terms)
with high probability for the Erd\H{o}s-R\'{e}nyi random graph on $p$ vertices,
whose clique number is with high probability $O(\log(p))$. We also show that
our lower bound is optimal for the Feige-Krauthgamer construction of
pseudomoments, derandomizing an argument of Kelner. Finally, we present
numerical experiments indicating that the value of the degree 4 SOS relaxation
of the Paley graph may scale as $O(p^{1/2 - \epsilon})$ for some $\epsilon &gt;
0$, and give a matrix norm calculation indicating that the pseudocalibration
proof strategy for SOS lower bounds for random graphs will not immediately
transfer to the Paley graph. Taken together, our results suggest that degree 4
SOS may break the "$\sqrt{p}$ barrier" for upper bounds on the clique number of
Paley graphs, but prove that it can at best improve the exponent from $1/2$ to
$1/3$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-08T01:30:00Z">Tuesday, November 08 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.02717'>A Framework for Approximation Schemes on Disk Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Daniel Lokshtanov, Fahad Panolan, Saket Saurabh, Jie Xue, Meirav Zehavi</p><p>We initiate a systematic study of approximation schemes for fundamental
optimization problems on disk graphs, a common generalization of both planar
graphs and unit-disk graphs. Our main contribution is a general framework for
designing efficient polynomial-time approximation schemes (EPTASes) for
vertex-deletion problems on disk graphs, which results in EPTASes for many
problems including Vertex Cover, Feedback Vertex Set, Small Cycle Hitting (in
particular, Triangle Hitting), $P_k$-Hitting for $k\in\{3,4,5\}$, Path
Deletion, Pathwidth $1$-Deletion, Component Order Connectivity, Bounded Degree
Deletion, Pseudoforest Deletion, Finite-Type Component Deletion, etc. All
EPTASes obtained using our framework are robust in the sense that they do not
require a realization of the input graph. To the best of our knowledge, prior
to this work, the only problems known to admit (E)PTASes on disk graphs are
Maximum Clique, Independent Set, Dominating set, and Vertex Cover, among which
the existing PTAS [Erlebach et al., SICOMP'05] and EPTAS [Leeuwen, SWAT'06] for
Vertex Cover require a realization of the input disk graph (while ours does
not).
</p>
<p>The core of our framework is a reduction for a broad class of (approximation)
vertex-deletion problems from (general) disk graphs to disk graphs of bounded
local radius, which is a new invariant of disk graphs introduced in this work.
Disk graphs of bounded local radius can be viewed as a mild generalization of
planar graphs, which preserves certain nice properties of planar graphs.
Specifically, we prove that disk graphs of bounded local radius admit the
Excluded Grid Minor property and have locally bounded treewidth. This allows
existing techniques for designing approximation schemes on planar graphs (e.g.,
bidimensionality and Baker's technique) to be directly applied to disk graphs
of bounded local radius.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lokshtanov_D/0/1/0/all/0/1">Daniel Lokshtanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Panolan_F/0/1/0/all/0/1">Fahad Panolan</a>, <a href="http://arxiv.org/find/cs/1/au:+Saurabh_S/0/1/0/all/0/1">Saket Saurabh</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1">Jie Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Zehavi_M/0/1/0/all/0/1">Meirav Zehavi</a></p><p>We initiate a systematic study of approximation schemes for fundamental
optimization problems on disk graphs, a common generalization of both planar
graphs and unit-disk graphs. Our main contribution is a general framework for
designing efficient polynomial-time approximation schemes (EPTASes) for
vertex-deletion problems on disk graphs, which results in EPTASes for many
problems including Vertex Cover, Feedback Vertex Set, Small Cycle Hitting (in
particular, Triangle Hitting), $P_k$-Hitting for $k\in\{3,4,5\}$, Path
Deletion, Pathwidth $1$-Deletion, Component Order Connectivity, Bounded Degree
Deletion, Pseudoforest Deletion, Finite-Type Component Deletion, etc. All
EPTASes obtained using our framework are robust in the sense that they do not
require a realization of the input graph. To the best of our knowledge, prior
to this work, the only problems known to admit (E)PTASes on disk graphs are
Maximum Clique, Independent Set, Dominating set, and Vertex Cover, among which
the existing PTAS [Erlebach et al., SICOMP'05] and EPTAS [Leeuwen, SWAT'06] for
Vertex Cover require a realization of the input disk graph (while ours does
not).
</p>
<p>The core of our framework is a reduction for a broad class of (approximation)
vertex-deletion problems from (general) disk graphs to disk graphs of bounded
local radius, which is a new invariant of disk graphs introduced in this work.
Disk graphs of bounded local radius can be viewed as a mild generalization of
planar graphs, which preserves certain nice properties of planar graphs.
Specifically, we prove that disk graphs of bounded local radius admit the
Excluded Grid Minor property and have locally bounded treewidth. This allows
existing techniques for designing approximation schemes on planar graphs (e.g.,
bidimensionality and Baker's technique) to be directly applied to disk graphs
of bounded local radius.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-08T01:30:00Z">Tuesday, November 08 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.03161'>4D Range Reporting in the Pointer Machine Model in Almost-Optimal Time</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yakov Nekrich, Saladi Rahul</p><p>In the orthogonal range reporting problem we must pre-process a set $P$ of
multi-dimensional points, so that for any axis-parallel query rectangle $q$ all
points from $q\cap P$ can be reported efficiently. In this paper we study the
query complexity of multi-dimensional orthogonal range reporting in the pointer
machine model. We present a data structure that answers four-dimensional
orthogonal range reporting queries in almost-optimal time $O(\log n\log\log n +
k)$ and uses $O(n\log^4 n)$ space, where $n$ is the number of points in $P$ and
$k$ is the number of points in $q\cap P$ . This is the first data structure
with nearly-linear space usage that achieves almost-optimal query time in 4d.
This result can be immediately generalized to $d\ge 4$ dimensions: we show that
there is a data structure supporting $d$-dimensional range reporting queries in
time $O(\log^{d-3} n\log\log n+k)$ for any constant $d\ge 4$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Nekrich_Y/0/1/0/all/0/1">Yakov Nekrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahul_S/0/1/0/all/0/1">Saladi Rahul</a></p><p>In the orthogonal range reporting problem we must pre-process a set $P$ of
multi-dimensional points, so that for any axis-parallel query rectangle $q$ all
points from $q\cap P$ can be reported efficiently. In this paper we study the
query complexity of multi-dimensional orthogonal range reporting in the pointer
machine model. We present a data structure that answers four-dimensional
orthogonal range reporting queries in almost-optimal time $O(\log n\log\log n +
k)$ and uses $O(n\log^4 n)$ space, where $n$ is the number of points in $P$ and
$k$ is the number of points in $q\cap P$ . This is the first data structure
with nearly-linear space usage that achieves almost-optimal query time in 4d.
This result can be immediately generalized to $d\ge 4$ dimensions: we show that
there is a data structure supporting $d$-dimensional range reporting queries in
time $O(\log^{d-3} n\log\log n+k)$ for any constant $d\ge 4$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-08T01:30:00Z">Tuesday, November 08 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.02681'>Deep Distance Sensitivity Oracles</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Davin Jeong, Chau Pham, Arnav Bhakta, Sarel Cohen, Maximilian Katzmann, Tobias Friedrich, Sang Chin</p><p>One of the most fundamental graph problems is finding a shortest path from a
source to a target node. While in its basic forms the problem has been studied
extensively and efficient algorithms are known, it becomes significantly harder
as soon as parts of the graph are susceptible to failure. Although one can
recompute a shortest replacement path after every outage, this is rather
inefficient both in time and/or storage. One way to overcome this problem is to
shift computational burden from the queries into a pre-processing step, where a
data structure is computed that allows for fast querying of replacement paths,
typically referred to as a Distance Sensitivity Oracle (DSO). While DSOs have
been extensively studied in the theoretical computer science community, to the
best of our knowledge this is the first work to construct DSOs using deep
learning techniques. We show how to use deep learning to utilize a
combinatorial structure of replacement paths. More specifically, we utilize the
combinatorial structure of replacement paths as a concatenation of shortest
paths and use deep learning to find the pivot nodes for stitching shortest
paths into replacement paths.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jeong_D/0/1/0/all/0/1">Davin Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1">Chau Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhakta_A/0/1/0/all/0/1">Arnav Bhakta</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1">Sarel Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Katzmann_M/0/1/0/all/0/1">Maximilian Katzmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Friedrich_T/0/1/0/all/0/1">Tobias Friedrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Chin_S/0/1/0/all/0/1">Sang Chin</a></p><p>One of the most fundamental graph problems is finding a shortest path from a
source to a target node. While in its basic forms the problem has been studied
extensively and efficient algorithms are known, it becomes significantly harder
as soon as parts of the graph are susceptible to failure. Although one can
recompute a shortest replacement path after every outage, this is rather
inefficient both in time and/or storage. One way to overcome this problem is to
shift computational burden from the queries into a pre-processing step, where a
data structure is computed that allows for fast querying of replacement paths,
typically referred to as a Distance Sensitivity Oracle (DSO). While DSOs have
been extensively studied in the theoretical computer science community, to the
best of our knowledge this is the first work to construct DSOs using deep
learning techniques. We show how to use deep learning to utilize a
combinatorial structure of replacement paths. More specifically, we utilize the
combinatorial structure of replacement paths as a concatenation of shortest
paths and use deep learning to find the pivot nodes for stitching shortest
paths into replacement paths.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-08T01:30:00Z">Tuesday, November 08 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.02703'>Online Learning and Bandits with Queried Hints</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Aditya Bhaskara, Sreenivas Gollapudi, Sungjin Im, Kostas Kollias, Kamesh Munagala</p><p>We consider the classic online learning and stochastic multi-armed bandit
(MAB) problems, when at each step, the online policy can probe and find out
which of a small number ($k$) of choices has better reward (or loss) before
making its choice. In this model, we derive algorithms whose regret bounds have
exponentially better dependence on the time horizon compared to the classic
regret bounds. In particular, we show that probing with $k=2$ suffices to
achieve time-independent regret bounds for online linear and convex
optimization. The same number of probes improve the regret bound of stochastic
MAB with independent arms from $O(\sqrt{nT})$ to $O(n^2 \log T)$, where $n$ is
the number of arms and $T$ is the horizon length. For stochastic MAB, we also
consider a stronger model where a probe reveals the reward values of the probed
arms, and show that in this case, $k=3$ probes suffice to achieve
parameter-independent constant regret, $O(n^2)$. Such regret bounds cannot be
achieved even with full feedback after the play, showcasing the power of
limited ``advice'' via probing before making the play. We also present
extensions to the setting where the hints can be imperfect, and to the case of
stochastic MAB where the rewards of the arms can be correlated.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bhaskara_A/0/1/0/all/0/1">Aditya Bhaskara</a>, <a href="http://arxiv.org/find/cs/1/au:+Gollapudi_S/0/1/0/all/0/1">Sreenivas Gollapudi</a>, <a href="http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1">Sungjin Im</a>, <a href="http://arxiv.org/find/cs/1/au:+Kollias_K/0/1/0/all/0/1">Kostas Kollias</a>, <a href="http://arxiv.org/find/cs/1/au:+Munagala_K/0/1/0/all/0/1">Kamesh Munagala</a></p><p>We consider the classic online learning and stochastic multi-armed bandit
(MAB) problems, when at each step, the online policy can probe and find out
which of a small number ($k$) of choices has better reward (or loss) before
making its choice. In this model, we derive algorithms whose regret bounds have
exponentially better dependence on the time horizon compared to the classic
regret bounds. In particular, we show that probing with $k=2$ suffices to
achieve time-independent regret bounds for online linear and convex
optimization. The same number of probes improve the regret bound of stochastic
MAB with independent arms from $O(\sqrt{nT})$ to $O(n^2 \log T)$, where $n$ is
the number of arms and $T$ is the horizon length. For stochastic MAB, we also
consider a stronger model where a probe reveals the reward values of the probed
arms, and show that in this case, $k=3$ probes suffice to achieve
parameter-independent constant regret, $O(n^2)$. Such regret bounds cannot be
achieved even with full feedback after the play, showcasing the power of
limited ``advice'' via probing before making the play. We also present
extensions to the setting where the hints can be imperfect, and to the case of
stochastic MAB where the rewards of the arms can be correlated.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-08T01:30:00Z">Tuesday, November 08 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.02755'>Extension of Simple Algorithms to the Matroid Secretary Problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Simon Park</p><p>Whereas there are simple algorithms that are proven to be optimal for the
Classical and the Multiple Choice Secretary Problem, the Matroid Secretary
Problem is less thoroughly understood. This paper proposes the generalization
of some simple algorithms from the Classical and Multiple Choice versions on
the Matroid Secretary Problem. Out of two algorithms that make decisions based
on samples, like the Dynkin's algorithm, one is proven to be an instance of
Greedy Algorithm (Bahrani et al., 2022), while the other is not. A generalized
version of the Virtual Algorithm (Babaioff et al., 2018) obtains a constant
competitive ratio for the Hat Graph, the adversarial example for Greedy
Algorithms, but fails to do so when a slight modificiation is introduced to the
graph. We show that there is no algorithm with Strong Forbidden Sets (Soto et
al., 2021) of size 1 on all graphic matroids.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Simon Park</a></p><p>Whereas there are simple algorithms that are proven to be optimal for the
Classical and the Multiple Choice Secretary Problem, the Matroid Secretary
Problem is less thoroughly understood. This paper proposes the generalization
of some simple algorithms from the Classical and Multiple Choice versions on
the Matroid Secretary Problem. Out of two algorithms that make decisions based
on samples, like the Dynkin's algorithm, one is proven to be an instance of
Greedy Algorithm (Bahrani et al., 2022), while the other is not. A generalized
version of the Virtual Algorithm (Babaioff et al., 2018) obtains a constant
competitive ratio for the Hat Graph, the adversarial example for Greedy
Algorithms, but fails to do so when a slight modificiation is introduced to the
graph. We show that there is no algorithm with Strong Forbidden Sets (Soto et
al., 2021) of size 1 on all graphic matroids.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-08T01:30:00Z">Tuesday, November 08 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.03077'>Online Nash Welfare Maximization Without Predictions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Zhiyi Huang, Minming Li, Xinkai Shu, Tianze Wei</p><p>Nash welfare maximization is widely studied because it balances efficiency
and fairness in resource allocation problems. Banerjee, Gkatzelis, Gorokh, and
Jin (2022) recently introduced the model of online Nash welfare maximization
with predictions for $T$ divisible items and $N$ agents with additive
utilities. They gave online algorithms whose competitive ratios are
logarithmic. We initiate the study of online Nash welfare maximization
\emph{without predictions}, assuming either that the agents' utilities for
receiving all items differ by a bounded ratio, or that their utilities for the
Nash welfare maximizing allocation differ by a bounded ratio. We design online
algorithms whose competitive ratios only depend on the logarithms of the
aforementioned ratios of agents' utilities and the number of agents.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhiyi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Minming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1">Xinkai Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1">Tianze Wei</a></p><p>Nash welfare maximization is widely studied because it balances efficiency
and fairness in resource allocation problems. Banerjee, Gkatzelis, Gorokh, and
Jin (2022) recently introduced the model of online Nash welfare maximization
with predictions for $T$ divisible items and $N$ agents with additive
utilities. They gave online algorithms whose competitive ratios are
logarithmic. We initiate the study of online Nash welfare maximization
\emph{without predictions}, assuming either that the agents' utilities for
receiving all items differ by a bounded ratio, or that their utilities for the
Nash welfare maximizing allocation differ by a bounded ratio. We design online
algorithms whose competitive ratios only depend on the logarithms of the
aforementioned ratios of agents' utilities and the number of agents.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-08T01:30:00Z">Tuesday, November 08 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.03146'>Balancing graph Voronoi diagrams with one more vertex</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Guillaume Ducoffe</p><p>Let $G=(V,E)$ be a graph with unit-length edges and nonnegative costs
assigned to its vertices. Being given a list of pairwise different vertices
$S=(s_1,s_2,\ldots,s_p)$, the {\em prioritized Voronoi diagram} of $G$ with
respect to $S$ is the partition of $G$ in $p$ subsets $V_1,V_2,\ldots,V_p$ so
that, for every $i$ with $1 \leq i \leq p$, a vertex $v$ is in $V_i$ if and
only if $s_i$ is a closest vertex to $v$ in $S$ and there is no closest vertex
to $v$ in $S$ within the subset $\{s_1,s_2,\ldots,s_{i-1}\}$. For every $i$
with $1 \leq i \leq p$, the {\em load} of vertex $s_i$ equals the sum of the
costs of all vertices in $V_i$. The load of $S$ equals the maximum load of a
vertex in $S$. We study the problem of adding one more vertex $v$ at the end of
$S$ in order to minimize the load. This problem occurs in the context of
optimally locating a new service facility ({\it e.g.}, a school or a hospital)
while taking into account already existing facilities, and with the goal of
minimizing the maximum congestion at a site. There is a brute-force algorithm
for solving this problem in ${\cal O}(nm)$ time on $n$-vertex $m$-edge graphs.
We prove a matching time lower bound for the special case where $m=n^{1+o(1)}$
and $p=1$, assuming the so called Hitting Set Conjecture of Abboud et al. On
the positive side, we present simple linear-time algorithms for this problem on
cliques, paths and cycles, and almost linear-time algorithms for trees, proper
interval graphs and (assuming $p$ to be a constant) bounded-treewidth graphs.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ducoffe_G/0/1/0/all/0/1">Guillaume Ducoffe</a></p><p>Let $G=(V,E)$ be a graph with unit-length edges and nonnegative costs
assigned to its vertices. Being given a list of pairwise different vertices
$S=(s_1,s_2,\ldots,s_p)$, the {\em prioritized Voronoi diagram} of $G$ with
respect to $S$ is the partition of $G$ in $p$ subsets $V_1,V_2,\ldots,V_p$ so
that, for every $i$ with $1 \leq i \leq p$, a vertex $v$ is in $V_i$ if and
only if $s_i$ is a closest vertex to $v$ in $S$ and there is no closest vertex
to $v$ in $S$ within the subset $\{s_1,s_2,\ldots,s_{i-1}\}$. For every $i$
with $1 \leq i \leq p$, the {\em load} of vertex $s_i$ equals the sum of the
costs of all vertices in $V_i$. The load of $S$ equals the maximum load of a
vertex in $S$. We study the problem of adding one more vertex $v$ at the end of
$S$ in order to minimize the load. This problem occurs in the context of
optimally locating a new service facility ({\it e.g.}, a school or a hospital)
while taking into account already existing facilities, and with the goal of
minimizing the maximum congestion at a site. There is a brute-force algorithm
for solving this problem in ${\cal O}(nm)$ time on $n$-vertex $m$-edge graphs.
We prove a matching time lower bound for the special case where $m=n^{1+o(1)}$
and $p=1$, assuming the so called Hitting Set Conjecture of Abboud et al. On
the positive side, we present simple linear-time algorithms for this problem on
cliques, paths and cycles, and almost linear-time algorithms for trees, proper
interval graphs and (assuming $p$ to be a constant) bounded-treewidth graphs.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-08T01:30:00Z">Tuesday, November 08 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.03206'>On Vertex Bisection Width of Random $d$-Regular Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Josep D&#xed;az, &#xd6;znur Ya&#x15f;ar Diner, Maria Serna, Oriol Serra</p><p>Vertex bisection is a graph partitioning problem in which the aim is to find
a partition into two equal parts that minimizes the number of vertices in one
partition set that have a neighbor in the other set. We are interested in
giving upper bounds on the vertex bisection width of random $d$-regular graphs
for constant values of $d$. Our approach is based on analyzing a greedy
algorithm by using the Differential Equations Method. In this way, we obtain
the first known upper bounds for the vertex bisection width in random regular
graphs. The results are compared with experimental ones and with lower bounds
obtained by Kolesnik and Wormald, (Lower Bounds for the Isoperimetric Numbers
of Random Regular Graphs, SIAM J. on Disc. Math. 28(1), 553-575, 2014).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Diaz_J/0/1/0/all/0/1">Josep D&#xed;az</a>, <a href="http://arxiv.org/find/cs/1/au:+Diner_O/0/1/0/all/0/1">&#xd6;znur Ya&#x15f;ar Diner</a>, <a href="http://arxiv.org/find/cs/1/au:+Serna_M/0/1/0/all/0/1">Maria Serna</a>, <a href="http://arxiv.org/find/cs/1/au:+Serra_O/0/1/0/all/0/1">Oriol Serra</a></p><p>Vertex bisection is a graph partitioning problem in which the aim is to find
a partition into two equal parts that minimizes the number of vertices in one
partition set that have a neighbor in the other set. We are interested in
giving upper bounds on the vertex bisection width of random $d$-regular graphs
for constant values of $d$. Our approach is based on analyzing a greedy
algorithm by using the Differential Equations Method. In this way, we obtain
the first known upper bounds for the vertex bisection width in random regular
graphs. The results are compared with experimental ones and with lower bounds
obtained by Kolesnik and Wormald, (Lower Bounds for the Isoperimetric Numbers
of Random Regular Graphs, SIAM J. on Disc. Math. 28(1), 553-575, 2014).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-08T01:30:00Z">Tuesday, November 08 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://blog.computationalcomplexity.org/2022/11/euclidean-tsp-is-np-hard-but-not-known.html'>Euclidean TSP is NP-hard but not known to be in NP. Why not known?</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>BILL: Lance, I was looking into TSP, metric TSP, Euclidean TSP since I am going to teach about P, NP, NPC, and approximating NPC problems and I came across the following from Lipton's book The P=NP Question and Godel's Letter (I paraphrase):</p><p>Graham proved that Euclidean TSP was NP-hard. But it is open if it is in NP. The difficulty hinges on a beautiful problem that is still open over thirty years later: Can we tell, given naturals a1,...,an,k if</p><p><br></p><p>\sqrt{a1} + ... + \sqrt{an} \le k</p><p>What I want to know is, is it still open? Lipton's book was written in 2010, so that was. uh, uh...</p><p>LANCE: 11 years ago.</p><p>BILL:&nbsp; Which is a long time ago- so has it been cracked?</p><p>LANCE: No it has not. And, by the way, I blogged on it&nbsp;in 2003.</p><p><br></p><p>This raises some questions:</p><p>1) Is&nbsp; the sqrt problem above in P? NP? (I have seen it stated that the problem is in PSPACE.)&nbsp;</p><p>2) Where do people think the problem is?</p><p>3) Why is it still open? Some options (I am sure there are more.)</p><p>a) Not that many people are working on it. But if not, why not?</p><p>b) The problem is just dang hard! That's probably why P vs NP is still unsolved and why FLT took so long, and why my proof of the Riemann hypothesis was so long in coming.) I am reminded of Erdos' quote on The Collatz Conjecture: Mathematics may not be ready for such problems. And you all know what Erdos said about R(5).&nbsp;</p><p>c) Reasons a and b above may lead to a death spiral: People THINK its hard so they don't work on it, then since nobody works on it no progress is made, reinforcing that its hard.&nbsp;</p><p><br></p><p><br></p><p>By gasarch</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>BILL: Lance, I was looking into TSP, metric TSP, Euclidean TSP since I am going to teach about P, NP, NPC, and approximating NPC problems and I came across the following from Lipton's book <i>The P=NP</i> <i>Question and Godel's Letter (</i>I paraphrase<i>):</i></p><p>Graham proved that Euclidean TSP was NP-hard. But it is open if it is in NP. The difficulty hinges on a beautiful problem that is still open over thirty years later: Can we tell, given naturals a1,...,an,k if</p><p><br /></p><p>\sqrt{a1} + ... + \sqrt{an} \le k</p><p>What I want to know is, is it still open? Lipton's book was written in 2010, so that was. uh, uh...</p><p>LANCE: 11 years ago.</p><p>BILL:&nbsp; Which is a long time ago- so has it been cracked?</p><p>LANCE: No it has not. And, by the way, I blogged on it&nbsp;<a href="https://blog.computationalcomplexity.org/2003/02/traveling-salesman-on-plane.html">in 2003</a>.</p><p><br /></p><p>This raises some questions:</p><p>1) Is&nbsp; the sqrt problem above in P? NP? (I have seen it stated that the problem is in PSPACE.)&nbsp;</p><p>2) Where do people think the problem is?</p><p>3) Why is it still open? Some options (I am sure there are more.)</p><p>a) Not that many people are working on it. But if not, why not?</p><p>b) The problem is just dang hard! That's probably why P vs NP is still unsolved and why FLT took so long, and why my proof of the Riemann hypothesis was so long in coming.) I am reminded of Erdos' quote on The Collatz Conjecture: <i>Mathematics may not be ready for such problems. </i>And you all know what Erdos said about R(5).&nbsp;</p><p>c) Reasons a and b above may lead to a death spiral: People THINK its hard so they don't work on it, then since nobody works on it no progress is made, reinforcing that its hard.&nbsp;</p><p><br /></p><p><br /></p><p class="authors">By gasarch</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-08T00:43:00Z">Tuesday, November 08 2022, 00:43</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Monday, November 07
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/144'>TR22-144 |  Streaming beyond sketching for Maximum Directed Cut | 

	Raghuvansh Saxena, 

	Noah Singer, 

	Madhu Sudan, 

	Santhoshini Velusamy</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We give an $\widetilde{O}(\sqrt{n})$-space single-pass $0.483$-approximation streaming algorithm for estimating the maximum directed cut size (Max-DICUT) in a directed graph on $n$ vertices. This improves over an $O(\log n)$-space $4/9 &lt; 0.45$ approximation algorithm  due to Chou, Golovnev, Velusamy (FOCS 2020), which was known to be optimal for $o(\sqrt{n})$-space algorithms.

Max-DICUT is a special case of a constraint satisfaction problem (CSP). In this broader context, our work gives the first CSP for which algorithms with $\widetilde{O}(\sqrt{n})$ space can provably outperform $o(\sqrt{n})$-space algorithms on general instances. Previously, this was shown in the restricted case of bounded-degree graphs in a previous work of the authors (SODA 2023). Prior to that work, the only algorithms for any CSP were based on generalizations of the $O(\log n)$-space algorithm for MAX-DICUT, and were in particular so-called &quot;sketching&quot;&quot; algorithms. In this work, we demonstrate that more sophisticated streaming algorithms can outperform these algorithms even on general instances.

Our algorithm constructs a &quot;snapshot&quot; of the graph and then applies a result of Feige and Jozeph (Algorithmica, 2015) to approximately estimate the Max-DICUT value from this snapshot. Constructing this snapshot is easy for bounded-degree graphs and the main contribution of our work is to construct this snapshot in the general setting. This involves some delicate sampling methods as well as a host of &quot;continuity&quot; results on the Max-DICUT behaviour in graphs.
        
        </div>

        <div class='tr-article-summary'>
        
          
          We give an $\widetilde{O}(\sqrt{n})$-space single-pass $0.483$-approximation streaming algorithm for estimating the maximum directed cut size (Max-DICUT) in a directed graph on $n$ vertices. This improves over an $O(\log n)$-space $4/9 &lt; 0.45$ approximation algorithm  due to Chou, Golovnev, Velusamy (FOCS 2020), which was known to be optimal for $o(\sqrt{n})$-space algorithms.

Max-DICUT is a special case of a constraint satisfaction problem (CSP). In this broader context, our work gives the first CSP for which algorithms with $\widetilde{O}(\sqrt{n})$ space can provably outperform $o(\sqrt{n})$-space algorithms on general instances. Previously, this was shown in the restricted case of bounded-degree graphs in a previous work of the authors (SODA 2023). Prior to that work, the only algorithms for any CSP were based on generalizations of the $O(\log n)$-space algorithm for MAX-DICUT, and were in particular so-called &quot;sketching&quot;&quot; algorithms. In this work, we demonstrate that more sophisticated streaming algorithms can outperform these algorithms even on general instances.

Our algorithm constructs a &quot;snapshot&quot; of the graph and then applies a result of Feige and Jozeph (Algorithmica, 2015) to approximately estimate the Max-DICUT value from this snapshot. Constructing this snapshot is easy for bounded-degree graphs and the main contribution of our work is to construct this snapshot in the general setting. This involves some delicate sampling methods as well as a host of &quot;continuity&quot; results on the Max-DICUT behaviour in graphs.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-07T21:43:30Z">Monday, November 07 2022, 21:43</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/143'>TR22-143 |  Certificate games | 

	Anna Gal, 

	Sourav Chakraborty, 

	Sophie Laplante, 

	Rajat Mittal, 

	Anupa Sunny</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We introduce and study Certificate Game complexity, a measure of complexity based on the probability of winning a game where two players are given inputs with different function values and are asked to output some index $i$ such that $x_i\neq y_i$, in a zero-communication setting.

We give upper and lower bounds for private coin, public coin, shared entanglement and non-signaling strategies, and give some separations.
We show that complexity in the public coin model is upper bounded by  Randomized query and Certificate complexity. On the other hand, it is lower bounded by fractional and randomized certificate complexity, making it a good candidate to prove strong lower bounds on randomized query complexity.
Complexity in the private coin model is bounded from below by zero-error randomized query complexity.  The quantum measure highlights an interesting and surprising difference between classical and quantum query models. Whereas the public coin certificate game complexity is bounded from above by randomized query complexity, the quantum certificate game complexity can be quadratically larger than  quantum query complexity. We use non-signaling, a notion from quantum information, to give a lower bound of $n$ on the quantum certificate game complexity of the OR function, whose quantum query complexity is  $\Theta(\sqrt{n})$, then go on to show that this ``non-signaling bottleneck&#39;&#39; applies to all functions with high sensitivity, block sensitivity or fractional block sensitivity. 

We also consider the single-bit version of certificate games, where the inputs of the two players are restricted to having Hamming distance $1$. We prove that the single-bit version of certificate game complexity with shared randomness is equal to sensitivity up to constant factors, thus giving a new characterization of sensitivity.  On the other hand, the single-bit version of certificate game complexity with private randomness is equal to $\lambda^2$, where $\lambda$ is the spectral sensitivity.
        
        </div>

        <div class='tr-article-summary'>
        
          
          We introduce and study Certificate Game complexity, a measure of complexity based on the probability of winning a game where two players are given inputs with different function values and are asked to output some index $i$ such that $x_i\neq y_i$, in a zero-communication setting.

We give upper and lower bounds for private coin, public coin, shared entanglement and non-signaling strategies, and give some separations.
We show that complexity in the public coin model is upper bounded by  Randomized query and Certificate complexity. On the other hand, it is lower bounded by fractional and randomized certificate complexity, making it a good candidate to prove strong lower bounds on randomized query complexity.
Complexity in the private coin model is bounded from below by zero-error randomized query complexity.  The quantum measure highlights an interesting and surprising difference between classical and quantum query models. Whereas the public coin certificate game complexity is bounded from above by randomized query complexity, the quantum certificate game complexity can be quadratically larger than  quantum query complexity. We use non-signaling, a notion from quantum information, to give a lower bound of $n$ on the quantum certificate game complexity of the OR function, whose quantum query complexity is  $\Theta(\sqrt{n})$, then go on to show that this ``non-signaling bottleneck&#39;&#39; applies to all functions with high sensitivity, block sensitivity or fractional block sensitivity. 

We also consider the single-bit version of certificate games, where the inputs of the two players are restricted to having Hamming distance $1$. We prove that the single-bit version of certificate game complexity with shared randomness is equal to sensitivity up to constant factors, thus giving a new characterization of sensitivity.  On the other hand, the single-bit version of certificate game complexity with private randomness is equal to $\lambda^2$, where $\lambda$ is the spectral sensitivity.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-07T07:52:43Z">Monday, November 07 2022, 07:52</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.02525'>Steiner connectivity problems in hypergraphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Florian H&#xf6;rsch, Zolt&#xe1;n Szigeti</p><p>We say that a tree $T$ is an $S$-Steiner tree if $S \subseteq V(T)$ and a
hypergraph is an $S$-Steiner hypertree if it can be trimmed to an $S$-Steiner
tree. We prove that it is NP-hard to decide, given a hypergraph $\mathcal{H}$
and some $S \subseteq V(\mathcal{H})$, whether there is a subhypergraph of
$\mathcal{H}$ which is an $S$-Steiner hypertree. As corollaries, we give two
negative results for two Steiner orientation problems in hypergraphs. Firstly,
we show that it is NP-hard to decide, given a hypergraph $\mathcal{H}$, some $r
\in V(\mathcal{H})$ and some $S \subseteq V(\mathcal{H})$, whether this
hypergraph has an orientation in which every vertex of $S$ is reachable from
$r$. Secondly, we show that it is NP-hard to decide, given a hypergraph
$\mathcal{H}$ and some $S \subseteq V(\mathcal{H})$, whether this hypergraph
has an orientation in which any two vertices in $S$ are mutually reachable from
each other. This answers a longstanding open question of the Egerv\'ary
Research group. On the positive side, we show that the problem of finding a
Steiner hypertree and the first orientation problem can be solved in polynomial
time if the number of terminals $|S|$ is fixed.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Horsch_F/0/1/0/all/0/1">Florian H&#xf6;rsch</a>, <a href="http://arxiv.org/find/math/1/au:+Szigeti_Z/0/1/0/all/0/1">Zolt&#xe1;n Szigeti</a></p><p>We say that a tree $T$ is an $S$-Steiner tree if $S \subseteq V(T)$ and a
hypergraph is an $S$-Steiner hypertree if it can be trimmed to an $S$-Steiner
tree. We prove that it is NP-hard to decide, given a hypergraph $\mathcal{H}$
and some $S \subseteq V(\mathcal{H})$, whether there is a subhypergraph of
$\mathcal{H}$ which is an $S$-Steiner hypertree. As corollaries, we give two
negative results for two Steiner orientation problems in hypergraphs. Firstly,
we show that it is NP-hard to decide, given a hypergraph $\mathcal{H}$, some $r
\in V(\mathcal{H})$ and some $S \subseteq V(\mathcal{H})$, whether this
hypergraph has an orientation in which every vertex of $S$ is reachable from
$r$. Secondly, we show that it is NP-hard to decide, given a hypergraph
$\mathcal{H}$ and some $S \subseteq V(\mathcal{H})$, whether this hypergraph
has an orientation in which any two vertices in $S$ are mutually reachable from
each other. This answers a longstanding open question of the Egerv\'ary
Research group. On the positive side, we show that the problem of finding a
Steiner hypertree and the first orientation problem can be solved in polynomial
time if the number of terminals $|S|$ is fixed.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-07T01:30:00Z">Monday, November 07 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.02257'>Certification with an NP Oracle</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Guy Blanc, Caleb Koch, Jane Lange, Carmen Strassle, Li-Yang Tan</p><p>In the certification problem, the algorithm is given a function $f$ with
certificate complexity $k$ and an input $x^\star$, and the goal is to find a
certificate of size $\le \text{poly}(k)$ for $f$'s value at $x^\star$. This
problem is in $\mathsf{NP}^{\mathsf{NP}}$, and assuming $\mathsf{P} \ne
\mathsf{NP}$, is not in $\mathsf{P}$. Prior works, dating back to Valiant in
1984, have therefore sought to design efficient algorithms by imposing
assumptions on $f$ such as monotonicity.
</p>
<p>Our first result is a $\mathsf{BPP}^{\mathsf{NP}}$ algorithm for the general
problem. The key ingredient is a new notion of the balanced influence of
variables, a natural variant of influence that corrects for the bias of the
function. Balanced influences can be accurately estimated via uniform
generation, and classic $\mathsf{BPP}^{\mathsf{NP}}$ algorithms are known for
the latter task.
</p>
<p>We then consider certification with stricter instance-wise guarantees: for
each $x^\star$, find a certificate whose size scales with that of the smallest
certificate for $x^\star$. In sharp contrast with our first result, we show
that this problem is $\mathsf{NP}^{\mathsf{NP}}$-hard even to approximate. We
obtain an optimal inapproximability ratio, adding to a small handful of
problems in the higher levels of the polynomial hierarchy for which optimal
inapproximability is known. Our proof involves the novel use of bit-fixing
dispersers for gap amplification.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Blanc_G/0/1/0/all/0/1">Guy Blanc</a>, <a href="http://arxiv.org/find/cs/1/au:+Koch_C/0/1/0/all/0/1">Caleb Koch</a>, <a href="http://arxiv.org/find/cs/1/au:+Lange_J/0/1/0/all/0/1">Jane Lange</a>, <a href="http://arxiv.org/find/cs/1/au:+Strassle_C/0/1/0/all/0/1">Carmen Strassle</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1">Li-Yang Tan</a></p><p>In the certification problem, the algorithm is given a function $f$ with
certificate complexity $k$ and an input $x^\star$, and the goal is to find a
certificate of size $\le \text{poly}(k)$ for $f$'s value at $x^\star$. This
problem is in $\mathsf{NP}^{\mathsf{NP}}$, and assuming $\mathsf{P} \ne
\mathsf{NP}$, is not in $\mathsf{P}$. Prior works, dating back to Valiant in
1984, have therefore sought to design efficient algorithms by imposing
assumptions on $f$ such as monotonicity.
</p>
<p>Our first result is a $\mathsf{BPP}^{\mathsf{NP}}$ algorithm for the general
problem. The key ingredient is a new notion of the balanced influence of
variables, a natural variant of influence that corrects for the bias of the
function. Balanced influences can be accurately estimated via uniform
generation, and classic $\mathsf{BPP}^{\mathsf{NP}}$ algorithms are known for
the latter task.
</p>
<p>We then consider certification with stricter instance-wise guarantees: for
each $x^\star$, find a certificate whose size scales with that of the smallest
certificate for $x^\star$. In sharp contrast with our first result, we show
that this problem is $\mathsf{NP}^{\mathsf{NP}}$-hard even to approximate. We
obtain an optimal inapproximability ratio, adding to a small handful of
problems in the higher levels of the polynomial hierarchy for which optimal
inapproximability is known. Our proof involves the novel use of bit-fixing
dispersers for gap amplification.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-07T01:30:00Z">Monday, November 07 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.02107'>Variable Parameter Analysis for Scheduling One Machine</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Nodari Vakhania</p><p>In contrast to the fixed parameter analysis (FPA), in the variable parameter
analysis (VPA) the value of the target problem parameter is not fixed, it
rather depends on the structure of a given problem instance and tends to have a
favorable asymptotic behavior when the size of the input increases. While
applying the VPA to an intractable optimization problem with $n$ objects, the
exponential-time dependence in enumeration of the feasible solution set is
attributed solely to the variable parameter $\nu$, $\nu&lt;&lt;n$. As opposed to the
FPA, the VPA does not imply any restriction on some problem parameters, it
rather takes an advantage of a favorable nature of the problem, which permits
to reduce the cost of enumeration of the solution space. Our main technical
contribution is a variable parameter algorithm for a strongly
$\mathsf{NP}$-hard single-machine scheduling problem to minimize maximum job
lateness. The target variable parameter $\nu$ is the number of jobs with some
specific characteristics, the ``emerging'' ones. The solution process is
separated in two phases. At phase 1 a partial solution including $n-\nu$
non-emerging jobs is constructed in a low degree polynomial time. At phase 2
less than $\nu!$ permutations of the $\nu$ emerging jobs are considered. Each
of them are incorporated into the partial schedule of phase 1. Doe to the
results of an earlier conducted experimental study, $\nu/n$ varied from $1/4$
for small problem instances to $1/10$ for the largest tested problem instances,
so that that the ratio becomes closer to 0 for large $n$s.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Vakhania_N/0/1/0/all/0/1">Nodari Vakhania</a></p><p>In contrast to the fixed parameter analysis (FPA), in the variable parameter
analysis (VPA) the value of the target problem parameter is not fixed, it
rather depends on the structure of a given problem instance and tends to have a
favorable asymptotic behavior when the size of the input increases. While
applying the VPA to an intractable optimization problem with $n$ objects, the
exponential-time dependence in enumeration of the feasible solution set is
attributed solely to the variable parameter $\nu$, $\nu&lt;&lt;n$. As opposed to the
FPA, the VPA does not imply any restriction on some problem parameters, it
rather takes an advantage of a favorable nature of the problem, which permits
to reduce the cost of enumeration of the solution space. Our main technical
contribution is a variable parameter algorithm for a strongly
$\mathsf{NP}$-hard single-machine scheduling problem to minimize maximum job
lateness. The target variable parameter $\nu$ is the number of jobs with some
specific characteristics, the ``emerging'' ones. The solution process is
separated in two phases. At phase 1 a partial solution including $n-\nu$
non-emerging jobs is constructed in a low degree polynomial time. At phase 2
less than $\nu!$ permutations of the $\nu$ emerging jobs are considered. Each
of them are incorporated into the partial schedule of phase 1. Doe to the
results of an earlier conducted experimental study, $\nu/n$ varied from $1/4$
for small problem instances to $1/10$ for the largest tested problem instances,
so that that the ratio becomes closer to 0 for large $n$s.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-07T01:30:00Z">Monday, November 07 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.02161'>Privacy-preserving Deep Learning based Record Linkage</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Thilina Ranbaduge, Dinusha Vatsalan, Ming Ding</p><p>Deep learning-based linkage of records across different databases is becoming
increasingly useful in data integration and mining applications to discover new
insights from multiple sources of data. However, due to privacy and
confidentiality concerns, organisations often are not willing or allowed to
share their sensitive data with any external parties, thus making it
challenging to build/train deep learning models for record linkage across
different organizations' databases. To overcome this limitation, we propose the
first deep learning-based multi-party privacy-preserving record linkage (PPRL)
protocol that can be used to link sensitive databases held by multiple
different organisations. In our approach, each database owner first trains a
local deep learning model, which is then uploaded to a secure environment and
securely aggregated to create a global model. The global model is then used by
a linkage unit to distinguish unlabelled record pairs as matches and
non-matches. We utilise differential privacy to achieve provable privacy
protection against re-identification attacks. We evaluate the linkage quality
and scalability of our approach using several large real-world databases,
showing that it can achieve high linkage quality while providing sufficient
privacy protection against existing attacks.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ranbaduge_T/0/1/0/all/0/1">Thilina Ranbaduge</a>, <a href="http://arxiv.org/find/cs/1/au:+Vatsalan_D/0/1/0/all/0/1">Dinusha Vatsalan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Ming Ding</a></p><p>Deep learning-based linkage of records across different databases is becoming
increasingly useful in data integration and mining applications to discover new
insights from multiple sources of data. However, due to privacy and
confidentiality concerns, organisations often are not willing or allowed to
share their sensitive data with any external parties, thus making it
challenging to build/train deep learning models for record linkage across
different organizations' databases. To overcome this limitation, we propose the
first deep learning-based multi-party privacy-preserving record linkage (PPRL)
protocol that can be used to link sensitive databases held by multiple
different organisations. In our approach, each database owner first trains a
local deep learning model, which is then uploaded to a secure environment and
securely aggregated to create a global model. The global model is then used by
a linkage unit to distinguish unlabelled record pairs as matches and
non-matches. We utilise differential privacy to achieve provable privacy
protection against re-identification attacks. We evaluate the linkage quality
and scalability of our approach using several large real-world databases,
showing that it can achieve high linkage quality while providing sufficient
privacy protection against existing attacks.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-07T01:30:00Z">Monday, November 07 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.02176'>Connected k-Center and k-Diameter Clustering</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Lukas Drexler, Jan Eube, Kelin Luo, Heiko R&#xf6;glin, Melanie Schmidt, Julian Wargalla</p><p>Motivated by an application from geodesy, we introduce a novel clustering
problem which is a $k$-center (or k-diameter) problem with a side constraint.
For the side constraint, we are given an undirected connectivity graph $G$ on
the input points, and a clustering is now only feasible if every cluster
induces a connected subgraph in $G$. We call the resulting problems the
connected $k$-center problem and the connected $k$-diameter problem.
</p>
<p>We prove several results on the complexity and approximability of these
problems. Our main result is an $O(\log^2{k})$-approximation algorithm for the
connected $k$-center and the connected $k$-diameter problem. For Euclidean
metrics and metrics with constant doubling dimension, the approximation factor
of this algorithm improves to $O(1)$. We also consider the special cases that
the connectivity graph is a line or a tree. For the line we give optimal
polynomial-time algorithms and for the case that the connectivity graph is a
tree, we either give an optimal polynomial-time algorithm or a
$2$-approximation algorithm for all variants of our model. We complement our
upper bounds by several lower bounds.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Drexler_L/0/1/0/all/0/1">Lukas Drexler</a>, <a href="http://arxiv.org/find/cs/1/au:+Eube_J/0/1/0/all/0/1">Jan Eube</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1">Kelin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Roglin_H/0/1/0/all/0/1">Heiko R&#xf6;glin</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidt_M/0/1/0/all/0/1">Melanie Schmidt</a>, <a href="http://arxiv.org/find/cs/1/au:+Wargalla_J/0/1/0/all/0/1">Julian Wargalla</a></p><p>Motivated by an application from geodesy, we introduce a novel clustering
problem which is a $k$-center (or k-diameter) problem with a side constraint.
For the side constraint, we are given an undirected connectivity graph $G$ on
the input points, and a clustering is now only feasible if every cluster
induces a connected subgraph in $G$. We call the resulting problems the
connected $k$-center problem and the connected $k$-diameter problem.
</p>
<p>We prove several results on the complexity and approximability of these
problems. Our main result is an $O(\log^2{k})$-approximation algorithm for the
connected $k$-center and the connected $k$-diameter problem. For Euclidean
metrics and metrics with constant doubling dimension, the approximation factor
of this algorithm improves to $O(1)$. We also consider the special cases that
the connectivity graph is a line or a tree. For the line we give optimal
polynomial-time algorithms and for the case that the connectivity graph is a
tree, we either give an optimal polynomial-time algorithm or a
$2$-approximation algorithm for all variants of our model. We complement our
upper bounds by several lower bounds.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-07T01:30:00Z">Monday, November 07 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.02394'>Online Matching with Set Delay</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Lindsey Deryckere, Seeun William Umboh</p><p>We initiate the study of online problems with set delay, where the delay cost
at any given time is an arbitrary function of the set of pending requests. In
particular, we study the online min-cost perfect matching with set delay
(MPMD-Set) problem, which generalises the online min-cost perfect matching with
delay (MPMD) problem introduced by Emek et al. (STOC 2016). In MPMD, $m$
requests arrive over time in a metric space of $n$ points. When a request
arrives the algorithm must choose to either match or delay the request. The
goal is to create a perfect matching of all requests while minimising the sum
of distances between matched requests, and the total delay costs incurred by
each of the requests. In contrast to previous work we study MPMD-Set in the
non-clairvoyant setting, where the algorithm does not know the future delay
costs. We first show no algorithm is competitive in $n$ or $m$. We then study
the natural special case of size-based delay where the delay is a
non-decreasing function of the number of unmatched requests. Our main result is
the first non-clairvoyant algorithms for online min-cost perfect matching with
size-based delay that are competitive in terms of $m$. In fact, these are the
first non-clairvoyant algorithms for any variant of MPMD. Furthermore, we prove
a lower bound of $\Omega(n)$ for any deterministic algorithm and $\Omega(\log
n)$ for any randomised algorithm. These lower bounds also hold for clairvoyant
algorithms.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Deryckere_L/0/1/0/all/0/1">Lindsey Deryckere</a>, <a href="http://arxiv.org/find/cs/1/au:+Umboh_S/0/1/0/all/0/1">Seeun William Umboh</a></p><p>We initiate the study of online problems with set delay, where the delay cost
at any given time is an arbitrary function of the set of pending requests. In
particular, we study the online min-cost perfect matching with set delay
(MPMD-Set) problem, which generalises the online min-cost perfect matching with
delay (MPMD) problem introduced by Emek et al. (STOC 2016). In MPMD, $m$
requests arrive over time in a metric space of $n$ points. When a request
arrives the algorithm must choose to either match or delay the request. The
goal is to create a perfect matching of all requests while minimising the sum
of distances between matched requests, and the total delay costs incurred by
each of the requests. In contrast to previous work we study MPMD-Set in the
non-clairvoyant setting, where the algorithm does not know the future delay
costs. We first show no algorithm is competitive in $n$ or $m$. We then study
the natural special case of size-based delay where the delay is a
non-decreasing function of the number of unmatched requests. Our main result is
the first non-clairvoyant algorithms for online min-cost perfect matching with
size-based delay that are competitive in terms of $m$. In fact, these are the
first non-clairvoyant algorithms for any variant of MPMD. Furthermore, we prove
a lower bound of $\Omega(n)$ for any deterministic algorithm and $\Omega(\log
n)$ for any randomised algorithm. These lower bounds also hold for clairvoyant
algorithms.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-07T01:30:00Z">Monday, November 07 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Saturday, November 05
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://gilkalai.wordpress.com/2022/11/05/inaugural-address-at-the-hungarian-academy-of-science-the-quantum-computer-a-miracle-or-mirage/'>Inaugural address at the Hungarian Academy of Science: The Quantum Computer – A Miracle or Mirage</a></h3>
        <p class='tr-article-feed'>from <a href='https://gilkalai.wordpress.com'>Gil Kalai</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          (Picture: János Pach) The Quantum Computer – A Miracle or Mirage inaugural address of Gil Kalai honorary member of the MTA, Budapest, 15 June, 2022, 15:00 Abstract: On February 12, 2002, Michel Devoret&#8217;s lecture entitled &#8220;The Quantum Computers: Miracle or &#8230; Continue reading &#8594;
        
        </div>

        <div class='tr-article-summary'>
        
          
          <h2><a href="https://gilkalai.files.wordpress.com/2022/11/1655299431168.jpg"><img loading="lazy" data-attachment-id="23493" data-permalink="https://gilkalai.wordpress.com/2022/11/05/inaugural-address-at-the-hungarian-academy-of-science-the-quantum-computer-a-miracle-or-mirage/attachment/1655299431168/" data-orig-file="https://gilkalai.files.wordpress.com/2022/11/1655299431168.jpg" data-orig-size="3533,2603" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.79&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Mi 9 Lite&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1655306336&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.74&quot;,&quot;iso&quot;:&quot;567&quot;,&quot;shutter_speed&quot;:&quot;0.03030303030303&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="1655299431168" data-image-description="" data-image-caption="" data-medium-file="https://gilkalai.files.wordpress.com/2022/11/1655299431168.jpg?w=300" data-large-file="https://gilkalai.files.wordpress.com/2022/11/1655299431168.jpg?w=640" class="alignnone  wp-image-23493" src="https://gilkalai.files.wordpress.com/2022/11/1655299431168.jpg?w=390&#038;h=287" alt="1655299431168" width="390" height="287" srcset="https://gilkalai.files.wordpress.com/2022/11/1655299431168.jpg?w=390&amp;h=287 390w, https://gilkalai.files.wordpress.com/2022/11/1655299431168.jpg?w=780&amp;h=574 780w, https://gilkalai.files.wordpress.com/2022/11/1655299431168.jpg?w=150&amp;h=111 150w, https://gilkalai.files.wordpress.com/2022/11/1655299431168.jpg?w=300&amp;h=221 300w, https://gilkalai.files.wordpress.com/2022/11/1655299431168.jpg?w=768&amp;h=566 768w" sizes="(max-width: 390px) 100vw, 390px" /></a></h2>
<p><span style="color:#ff0000;">(Picture: János Pach)</span></p>
<h2><strong>The Quantum Computer – A Miracle or Mirage</strong></h2>
<p>inaugural address of</p>
<h3>Gil Kalai</h3>
<p>honorary member of the MTA,</p>
<p>Budapest, 15 June, 2022, 15:00</p>
<blockquote><p><span style="color:#993366;"><em><strong>Abstract</strong><strong>:</strong> On February 12, 2002, Michel Devoret&#8217;s lecture entitled &#8220;The Quantum Computers: Miracle or Mirage&#8221; kicked off the 150th Anniversary Celebration of the Yale School of Engineering. In his abstract, Devoret asserted that while quantum mechanics had often been viewed as limiting the information one could extract from a physical system, &#8220;recent discoveries show that fundamental properties of quantum mechanics could actually be used to perform computations that would be impossible on a standard &#8216;classical&#8217; computer.&#8221; Devoret&#8217;s question of whether the quantum computer is a miracle or a mirage is, to this day, one of the fascinating clear-cut open scientific questions of our time. In my inaugural lecture I will explain the question and the discoveries from the 1990s that suggested that quantum computers could perform miracles, and I will present my theory as to why the quantum computer is a mirage.</em></span></p></blockquote>
<p>Janos Pach told me that the name of the room were my lecture was given, &#8220;Felolvaso terem&#8221; means something like &#8220;Hall for reading papers&#8221;. This is a rarely used, somewhat archaic, expression that really refers to the act of reading out papers loudly for an audience. Quite untypically for mathematical lectures, this was precisely the style of my lecture.  Here is the text of my lecture:</p>
<p>_________________________________________________</p>
<p>I was very honored and happy for being elected as an honorary member of the Hungarian Academy of Sciences and I am especially happy to come to the beautiful city of Budapest to give this inaugural address and the Turán memorial lectures and I am thankful for the opportunity to meet friends and colleagues of many decades as well as a new generation of wonderful mathematicians.</p>
<p><strong><span style="color:#0000ff;">Our mathematical life and the inherent difficulties and uncertainties of mathematics and science are for us islands of stability and solace in times of great concerns and difficulties.</span></strong></p>
<h2>Preface</h2>
<p>Quantum computers are new type of computers based on quantum physics. When it comes to certain computational objectives, the computational ability of quantum computers is tens, and even hundreds of orders of magnitude faster than that of the digital computers we are familiar with, and their construction will enable us to break most of the current cryptosystems. While quantum computers represent a future technology, which captivates the hearts and imaginations of many, there is also an ongoing dispute over the very possibility of their existence.</p>
<p>My theory asserts that quantum computers are inherently noisy. Their robust components represent a low-level (classical) computational ability, and they necessarily exhibit chaotic behavior.</p>
<h2>Classical Computers</h2>
<p><span style="color:#0000ff;">The classical computer can be seen as a device that contains n “bits” where every bit is in one of two positions of either “zero” or “one.” The computer operates through “gates” that perform logical operations on either a single or two bits. Two simple types of gates are sufficient to describe all forms of computation: the NOT gate, which reverses the value of a single bit, and the AND gate, which receives two bits as input and produces “one” if and only if the value of each of the bits in the input is “one.” The model presented here for computers is called the Boolean circuit model and is close to the 1940s models of Church, Turing, and others, which preceded the advent of the digital computer and the computer revolution in the second half of the twentieth century.</span></p>
<p><span style="color:#0000ff;">In the 1960s and 1970s, computer science researchers came to an important insight regarding the inherent limitations of computers. There are certain computational tasks that are very easy to formulate (and even to check whether a suggested answer to them is correct), but which are nonetheless impossible to perform because they require too many computational steps. For example, if the number of computational steps for a problem described by an input of <em>n</em> bits is <em>2<sup>n</sup></em>, then this computation will not be feasible for the most powerful computers, even when n = 100. To describe feasible (or “efficient”) computations, that is, computations that can actually be performed, an important assumption was added, according to which the number of computational steps (i.e., the number of gates) does not exceed a constant power (e.g., n<sup>3</sup>) in the number <em>n</em> of bits describing the input. This definition represents an important step in the theory of computational complexity, while providing the central tool for the analysis of the computational power of computational models, algorithms, and physical computational systems.</span></p>
<p><span style="color:#0000ff;">The main lens through which we analyze the difficulty of computation is the asymptotic lens. For most computational tasks we cannot hope for a full understanding of the number of required computational steps as a function of the number of bits of the input, but we can gain understanding (at times rough and at times just conjectural) of the computational difficulty by studying how the number of computational steps asymptotically depends on the input size. Experience gained in recent decades indicates that asymptotic analysis provides a good understanding of the practical difficulty of computational tasks.</span></p>
<p><span style="color:#0000ff;">We will now enrich the picture by adding probability. A single bit has two possible values, “zero” and “one,” and it is possible to extend the Boolean circle model by allowing the bit to have the value “zero” with probability <em>p</em> and the value “one” with probability <em>1-p</em>. In this way, the classical computer equipped with probabilistic bits can describe a sample from a probability space of sequences of zeros and ones. The model of probabilistic classical computers is an important model in the theory of computational complexity and it also constitutes a convenient introduction to the concept of quantum computers. Probabilistic Boolean circuits can be realized rather easily by digital computers.</span></p>
<h2>Quantum Computers</h2>
<p>The model of quantum computers is a computational model based on quantum mechanics and was first introduced in the 1980s. In quantum computers, the classical bit is replaced by the basic computational element called a qubit.</p>
<p>The state of a single qubit is described by a unit vector in a complex two-dimensional vector space (namely, it is described by four real numbers whose sum of squares equals 1). The uncertainty principle asserts that we cannot identify the precise state of the qubit, but rather we can measure it and obtain a probabilistic bit. One way to think about it is as follows: there are two basic states for the qubit and we denote them by <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C0%5Cright%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cleft%7C0%5Cright%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cleft%7C0%5Cright%5Crangle+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;left|0&#92;right&#92;rangle " class="latex" />  and <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C1%5Cright%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cleft%7C1%5Cright%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cleft%7C1%5Cright%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;left|1&#92;right&#92;rangle" class="latex" /> while the general state of the qubit is a “superposition” of these two basic states, namely, the general state of a qubit is a linear combination of the form</p>
<p><img src="https://s0.wp.com/latex.php?latex=z_1%5Cleft%7C0%5Cright%5Crangle+%2Bz_2%5Cleft%7C1%5Cright%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=z_1%5Cleft%7C0%5Cright%5Crangle+%2Bz_2%5Cleft%7C1%5Cright%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=z_1%5Cleft%7C0%5Cright%5Crangle+%2Bz_2%5Cleft%7C1%5Cright%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="z_1&#92;left|0&#92;right&#92;rangle +z_2&#92;left|1&#92;right&#92;rangle" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=z_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=z_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=z_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="z_1" class="latex" />  and latex z_2$  are complex numbers  satisfying</p>
<p><img src="https://s0.wp.com/latex.php?latex=%7Cz_1%7C%5E2%2B%7Cz_2%7C%5E2%3D1.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Cz_1%7C%5E2%2B%7Cz_2%7C%5E2%3D1.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Cz_1%7C%5E2%2B%7Cz_2%7C%5E2%3D1.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="|z_1|^2+|z_2|^2=1." class="latex" /></p>
<p>The state of the qubit can be measured and when a qubit in this state is measured, we obtain a probabilistic bit with a value 0 with probability  and 1 with probability. One possible physical realization of the two basic states of a single qubit is with the two energy levels of a Hydrogen atom, with quantum physics allowing the atom to be in a superposition of these basic states as described above.</p>
<p>The computer acts on <em>n</em> qubits through “quantum gates,” which are the basic quantum computation operations. (The gates are described mathematically by “unitary operators.”) At the end of the computation process, the state of the computer is measured, and this yields a single sample from a probability distribution of 0-1 strings of length n.  In this case too, the assumption is that for each efficient computation the number of gates is at most polynomial in <em>n</em>. The crucial fact about quantum computers is that they would allow us to efficiently achieve samples that cannot be efficiently achieved by classical computers.</p>
<p>In the 1990s, Peter Shor discovered that quantum computers would allow the execution of a certain computational tasks – factoring an integer to its prime components – hundreds of orders of magnitude faster than regular computers and would enable us to break most current cryptosystems.</p>
<h2>Quantum Computers and Noise</h2>
<p><span style="color:#0000ff;">Quantum systems are by nature “noisy” and unstable. The term “noise” refers to a deviation of the computer from the planned program, and in the case of a quantum computer any unwanted interaction or leakage of information leads to such a deviation. The model of noisy quantum computer adds a component of noise to the description of the quantum computer. Noisy intermediate-scale quantum (NISQ) computers are simply noisy quantum circuits with at most 200 (say) qubits.</span></p>
<p><span style="color:#0000ff;">It was around that time that early doubts concerning the model also surfaced: quantum systems are by nature “noisy” and unstable. The term “noise” refers to a deviation of the computer from the planned program, and in the case of a quantum computer any unwanted interaction or leakage of information leads to such a deviation. Therefore, in order to reduce the noise level, quantum computers must be isolated and protected from their environment. The key to a possible solution of the noise problem is quantum error-correcting codes, which will enable noisy quantum computers to perform the same computations conducted by the abstract noiseless model, provided that the level of noise can be reduced to below a certain threshold denoted by <strong>α</strong>.</span></p>
<p><span style="color:#0000ff;">It is a common opinion that the construction of quantum computers is possible, that the remaining challenge is mostly engineering-related, that such computers will be constructed in the next few decades, and that quantum error-correcting codes of the required quality will be developed in labs in the years to come. My standpoint is that it will be fundamentally impossible to reduce the noise level to below the required threshold. As a result, it will be impossible to develop the quantum codes required for quantum computation, nor will it be possible to reach the target of “quantum computation supremacy,” whereby a quantum computer performs computation that is extremely hard or even impossible for a classical computer.</span></p>
<h2>The argument against quantum computers</h2>
<p>My argument for the impossibility of quantum computers lies within the scope of quantum mechanics and does not deviate from its principles. In essence, the argument is based on computational complexity and its interpretation, and it is discussed in-depth in my papers which also include a discussion of general conclusions that derive from my argument and relate to quantum physics, alongside suggestions of general laws of nature that express the impossibility of quantum computation.</p>
<p>My argument mostly deals with understanding quantum computers on the intermediate scale (known as NISQ computers, an abbreviation of Noisy Intermediate Scale Quantum), that is, quantum computers of up to at most several hundreds of qubits. It is expected that on this scale we will be able to construct quantum codes of a quality sufficient for the construction of bigger quantum computers. It is further expected that on this scale the quantum computer will achieve computations far beyond the ability of powerful classical computers, that is, will achieve quantum computational supremacy. The Google’s Sycamore computer is an example of a noisy intermediate-scale quantum computer.</p>
<p>As specified later, it is my argument that NISQ computers cannot be controlled. Hence:</p>
<ol>
<li>Such systems cannot demonstrate significant quantum computational advantage.</li>
<li>Such systems cannot be used for the creation of quantum error-correcting codes.</li>
<li>Such systems lead to non-stationary and even chaotic distributions.</li>
</ol>
<p>Regarding the first item, let me remind the audience that computational complexity theory provides tools for studying the computational power of models and physical computational devices. The reason NISQ computers cannot support quantum supremacy is that when we use computational complexity tools to understand the computational power of NISQ computers, we discover that they describe a very low-level computational class. This low-level computational class does not allow for any complicated computations, much less computational supremacy.  My analysis draws computational conclusions for NISQ computers based on their mathematical model’s asymptotic behavior.</p>
<p>Regarding the second item, the reason it is impossible to build quantum error-correcting codes is that it requires an even lower noise level than that required for demonstrating quantum supremacy. The meaning of the infeasibility of quantum error-correcting codes is that even a quantum computer operating on a single qubit is inherently noisy. It is to be noted that the argument that the noise level required for error-correcting codes is lower than the level required for quantum supremacy is generally accepted by both theoreticians and experimental physicists.</p>
<p><a href="https://gilkalai.wordpress.com/2022/10/14/alefs-corner-it-wont-work-sorry/"><span style="color:#ff0000;">A picture by Alef</span></a></p>
<h2>Weiner Chaos and Lorenz Chaos</h2>
<p><span style="color:#0000ff;">When I speak here of chaotic behavior, I refer to a system (either deterministic, probabilistic, or quantum) that is so sensitive to its defining parameters that its behavior (or a large component of its behavior) cannot be determined, not even probabilistically. This notion is related to the mathematical theory of “noise sensitivity” (Benjamini, Kalai, and Schramm 1999) that we mentioned before, and to the related mathematical theory of “black noise” (Tsirelson and Vershik 1998). Both these theories have their early roots in Weiner’s chaos expansion (Weiner 1938). Chaos in this sense is sometimes called “Knightian uncertainty”, a term that originates in economics. The first use of the theory of noise-sensitivity to quantum computers was in my 2014 paper with Guy Kindler on “Boson Sampling”. (There we use Hermite-Fourier expansion.)</span></p>
<p><span style="color:#0000ff;">The term “chaotic system” in mathematical chaos theory (Lorenz 1963) refers to nonlinear classical deterministic systems, whose development very much depends on their initial conditions, and since these conditions are not known precisely, the system’s development in the long run cannot be predicted.</span></p>
<p><span style="color:#0000ff;">It is plausible that natural chaotic phenomenon (like the weather) reflects both the Lorenz chaos and the Weiner chaos.</span></p>
<p><span style="color:#0000ff;">Here I thank I Bernard Chazelle for raising an appealing argument for why Lorenz chaos and traditional dynamic theoretic notions of chaos are insufficient to describe chaos in nature.</span></p>
<h2>A Brief Look at the Mathematical Analysis: The Fourier Expansion</h2>
<p>Following is a brief look at a central technical analytic tool that applies to all three components of the argument, namely, the Fourier expansion of functions. In our case we consider functions, whose values are real numbers that are described on sequences of length <em>n</em> of zeros and ones, and every such function can be written (as a linear combination) by means of a special set of functions, known as Fourier–Walsh functions. Fourier–Walsh functions can be sorted according to their degree, which is a natural number between 0 and <em>n</em>. Much as the regular Fourier expansion makes it possible to describe a musical sound as a combination of pure high and low tones, in Fourier–Walsh functions, too, the degrees can be seen as analogous to the heights of the pure tones.</p>
<p>Our first step is to study the Walsh–Fourier transform of the probability distribution of the 0-1 sequences in an ideal noiseless quantum computer, and the second step is to study the effect of the noise. The main technical component of my argument is that the noise will exponentially lower the coefficients of the higher-degree Fourier–Walsh functions. This leads to a mathematical distinction (Benjamini, Kalai, and Schramm 1999) between distributions that can be described by low-degree Fourier coefficients and are referred to as “noise stable” and distributions that are supported by high-degree Fourier coefficients and are referred to as “noise sensitive.” A general distribution can have both noise-stable and noise-sensitive components.  This is the reason that, on the one hand, the stable component in the noisy distribution is described by low Fourier–Walsh levels, and hence this component represents an extremely low computational power, and that, on the other hand, if the unstable part, namely, the contributions of the higher-degree Fourier–Walsh functions remain substantial, this contribution will be chaotic.<strong> </strong></p>
<h2>On Classical Information and Classical Computation</h2>
<p>The question “why does this argument fail for classical computers?” is an appropriate critique of any argument asserting that quantum computers are not possible. Regarding my argument, the answer to this question is as follows: the path to large-scale quantum computers requires building quantum error-correcting codes on NISQ systems, and my theory asserts that this is impossible. At the same time, the primitive computational power represented by NISQ systems still allows for stable classical information, and subsequently even for classical computation.</p>
<h2>The Weak Link in My Argument</h2>
<p>The mathematical analysis that leads to the conclusion that noisy intermediate-scale quantum computers have primitive computational power is asymptotic. That is, a mathematical model of these computers is described and analyzed when the noise level is fixed, and the number of qubits increases. The conclusion I draw from this asymptotic behavior concerns the lowest noise level engineers can reach. I argue that it would be impossible to lower the noise level in such a way that enables us to create computers whose computational ability is low in an asymptotic analysis, but very high – indeed beyond the ability of classical computers – for several dozen qubits.  The move from an asymptotic argument regarding the model’s computational complexity to a concrete claim regarding engineering-related limitations of intermediate-scale computers is hardly standard, and my argument clashes with the strong intuition of experts in the field, according to whom an engineering effort would make it possible in principle (as well as in practice) to lower the noise level as much as we would like. Indeed, the multiple resources invested in building quantum computers, especially noisy intermediate-scale quantum computers, are based on the common position that there is no fundamental obstacle obstructing this effort.</p>
<h2><span style="color:#ff0000;">Recent experimental results</span></h2>
<p>In 2019 Researchers from <strong><span style="color:#0000ff;">G</span><span style="color:#ff0000;">o</span><span style="color:#ffcc00;">o</span><span style="color:#0000ff;">g</span><span style="color:#00ff00;">l</span><span style="color:#ff0000;">e</span></strong> claimed to achieve in 300 seconds a sampling task that requires 10,000 years for a supercomputer.</p>
<p><span style="color:#ff0000;">However, since the Google paper appeared the number “10,000” years was replaced by (roughly) 300 second by better classical algorithms. </span></p>
<p>In a joint work with Yosi Rinott and Tomer Shoham we also examine other aspects of the Google methods and claims.</p>
<p>In 2020 researchers from <strong>USTC</strong>, China claimed to achieve in 300 seconds a sampling task that requires a billion years for a supercomputer.</p>
<p><span style="color:#ff0000;">However, my 2014 paper with Guy Kindler on “Boson Sampling” (and subsequent works) largely refute these fantastic claims.</span></p>
<h2><a href="https://gilkalai.files.wordpress.com/2022/10/1655299431197.jpg"><img loading="lazy" data-attachment-id="23492" data-permalink="https://gilkalai.wordpress.com/2022/11/05/inaugural-address-at-the-hungarian-academy-of-science-the-quantum-computer-a-miracle-or-mirage/attachment/1655299431197/" data-orig-file="https://gilkalai.files.wordpress.com/2022/10/1655299431197.jpg" data-orig-size="2992,4000" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.79&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Mi 9 Lite&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1655306285&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.74&quot;,&quot;iso&quot;:&quot;687&quot;,&quot;shutter_speed&quot;:&quot;0.03030303030303&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="1655299431197" data-image-description="" data-image-caption="" data-medium-file="https://gilkalai.files.wordpress.com/2022/10/1655299431197.jpg?w=224" data-large-file="https://gilkalai.files.wordpress.com/2022/10/1655299431197.jpg?w=640" class="alignnone  wp-image-23492" src="https://gilkalai.files.wordpress.com/2022/10/1655299431197.jpg?w=289&#038;h=386" alt="1655299431197" width="289" height="386" srcset="https://gilkalai.files.wordpress.com/2022/10/1655299431197.jpg?w=289&amp;h=386 289w, https://gilkalai.files.wordpress.com/2022/10/1655299431197.jpg?w=578&amp;h=772 578w, https://gilkalai.files.wordpress.com/2022/10/1655299431197.jpg?w=112&amp;h=150 112w, https://gilkalai.files.wordpress.com/2022/10/1655299431197.jpg?w=224&amp;h=300 224w" sizes="(max-width: 289px) 100vw, 289px" /></a></h2>
<p><span style="color:#ff0000;">(Picture: János Pach)</span></p>
<h2>Conclusion</h2>
<p>Quantum computers are among the most important scientific developments of our time. In my judgement it is a serious possibility that quantum computers are not possible. This is what I expect, and I have been studying this possibility since 2005.</p>
<p>Understanding noisy quantum systems and potentially even the failure of quantum computers is related to the fascinating mathematics of noise stability and noise sensitivity and its connections to the theory of computing. Exploring this avenue may have important implications to various areas of quantum physics.</p>
<p>Evaluating the recent fantastic experimental claims is an exciting scientific matter on its own.</p>
<h3>______________________________________________</h3>
<h3>Some references</h3>
<p>For the readers of the blog let me add some references to my papers:</p>
<p><span style="color:#993366;">The argument against quantum computers</span></p>
<p>(i) G. Kalai, <a href="https://arxiv.org/abs/1908.02499">The argument against quantum computers</a>, <em>Quantum, Probability, Logic: Itamar Pitowsky’s Work and Influence</em> (M. Hemmo and O. Shenker, eds.), pp. 399–422, Springer, 2019.</p>
<p>(ii) G. Kalai, <a href="https://www.ams.org/journals/notices/201605/rnoti-p508.pdf">The quantum computer puzzle</a>, Notices AMS, May 2016</p>
<p>(iii). G. Kalai, <a href="https://gilkalai.files.wordpress.com/2020/08/laws-blog2.pdf">The argument against quantum computers, the quantum laws of nature, and Google’s supremacy claims,</a><em> The Intercontinental Academia Laws: Rigidity and Dynamics </em>(M. J. Hannon and E. Z. Rabinovici, eds.), World Scientific, 2020. arXiv:2008.05188.</p>
<p>(iv). G. Kalai, <a href="https://arxiv.org/abs/2209.01648">Conjecture C still stands,</a> arXiv. 2022</p>
<p>This recent paper responds to a <a href="https://arxiv.org/abs/1204.3404">2012 paper</a> of Steve Flammia and Aram Harrow (see this post) regarding a certain &#8220;Conjecture C&#8221; discussed in my 2011 debate with Aram.</p>
<p><span style="color:#993366;">In the wider context of &#8220;mathematical computer science: theory and practice&#8221;</span></p>
<p>(v). G. Kalai, <a href="https://gilkalai.files.wordpress.com/2019/09/main-pr.pdf">Three puzzles on mathematics computations, and games,</a> Proc. ICM2018;</p>
<p><span style="color:#993366;">Boson sampling</span></p>
<p>(vi). G. Kalai and G. Kindler, <a href="https://arxiv.org/abs/1409.3093" rel="nofollow ugc">Gaussian Noise Sensitivity and BosonSampling, 2014. </a></p>
<p><em>Our study of Boson Sampling gives the basis to understanding of general NISQ systems.</em></p>
<p>(vii) G. Kalai and G. Kindler, <a href="https://gilkalai.files.wordpress.com/2022/11/bskkr2.pdf">Concerns about recent claims of a huge quantum computational advantage via Gaussian boson sampling</a>,</p>
<p>This was a discussion paper for the 2020-2021 email exchange with the USTC photonic team and other researchers regarding the boson sampling claims.</p>
<p><span style="color:#993366;">On the Google 2019 experiment</span></p>
<p>(viii) Y. Rinott, T. Shoham, and G. Kalai, <a href="https://gilkalai.files.wordpress.com/2022/08/sts836.pdf">Statistical Aspects of the Quantum Supremacy Demonstration,</a> Statistical Science  (2022)</p>
<p>(ix) G. Kalai, Y. Rinott and T. Shoham, <a href="https://gilkalai.files.wordpress.com/2022/10/cc22a19.pdf"><span dir="ltr" role="presentation">Google’s 2019 “Quantum Supremacy” Claims: </span><span dir="ltr" role="presentation">Data, Documentation, &amp; Discussion</span></a></p>
<p>See also Sections 5 and 6 in reference (iii).</p>
<p>A lecture in Bandung, Indonesia with the same title</p>
<p><iframe class="youtube-player" width="640" height="360" src="https://www.youtube.com/embed/zlsNsEaL-jY?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></p>
<p>And my lecture in Lahore, Pakistan now on youtube</p>
<p><iframe class="youtube-player" width="640" height="360" src="https://www.youtube.com/embed/zOs1CoPrnuY?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></p>
<p class="authors">By Gil Kalai</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-05T17:27:17Z">Saturday, November 05 2022, 17:27</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/05/postdoc-at-technical-university-of-denmark-copenhagen-apply-by-december-15-2022/'>postdoc at Technical University of Denmark, Copenhagen (apply by December 15, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Join us in the pursuit of new provably efficient algorithms for graphs, dynamic graphs, discrete computational geometry, and related topics. We are a collaborative group of researchers with an array of algorithmic hypotheses, as well as broad interests and open minds, and we are looking for a postdoc to join our group. You are very [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Join us in the pursuit of new provably efficient algorithms for graphs, dynamic graphs, discrete computational geometry, and related topics.<br />
We are a collaborative group of researchers with an array of algorithmic hypotheses, as well as broad interests and open minds, and we are looking for a postdoc to join our group.<br />
You are very welcome to contact us with any questions you may have.</p>
<p>Website: <a href="https://www.dtu.dk/english/about/job-and-career/vacant-positions/job?id=ef0a4d3f-d74e-4a6e-b6ac-dc37848e04ca">https://www.dtu.dk/english/about/job-and-career/vacant-positions/job?id=ef0a4d3f-d74e-4a6e-b6ac-dc37848e04ca</a><br />
Email: erot@dtu.dk</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-05T09:38:27Z">Saturday, November 05 2022, 09:38</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Friday, November 04
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/04/postdoc-at-ideal-northwestern-uchicago-ttic-uic-iit-apply-by-december-15-2022/'>postdoc at IDEAL (Northwestern, UChicago, TTIC, UIC, IIT) (apply by December 15, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The Institute for Data, Econometrics, Algorithms, and Learning (IDEAL) invites applications for two postdoctoral positions with an anticipated start date in the Fall of 2023. IDEAL is an interdisciplinary institute organized by Northwestern University, the University of Chicago, Toyota Technological Institute at Chicago, the University of Illinois Chicago, and Illinois Institute of Technology. Website: academicjobsonline.org/ajo/jobs/23590 [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The Institute for Data, Econometrics, Algorithms, and Learning (IDEAL) invites applications for two postdoctoral positions with an anticipated start date in the Fall of 2023. IDEAL is an interdisciplinary institute organized by Northwestern University, the University of Chicago, Toyota Technological Institute at Chicago, the University of Illinois Chicago, and Illinois Institute of Technology.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/23590">https://academicjobsonline.org/ajo/jobs/23590</a><br />
Email: idealpostdoc@ttic.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T16:45:56Z">Friday, November 04 2022, 16:45</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://dstheory.wordpress.com/2022/11/04/tuesday-nov-8th-2022-edith-cohen-from-google-research-tel-aviv-university/'>Tuesday, Nov 8th, 2022 — Edith Cohen from Google Research (& Tel-Aviv University)</a></h3>
        <p class='tr-article-feed'>from <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The next Foundations of Data Science virtual talk series on recent advances in adversarially robust streaming will take place on Tuesday, November 8th at 1:00 PM Pacific Time (16:00 Eastern Time, 22:00 Central European Time, 20:00 UTC). Edith Cohen from Google Research will talk about “On Robustness to Adaptive Inputs: A Case Study of CountSketch.”Continue reading "Tuesday, Nov 8th, 2022 — Edith Cohen from Google Research (&#38; Tel-Aviv&#160;University)"
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The next <a rel="noreferrer noopener" href="https://sites.google.com/view/dstheory/home" target="_blank">Foundations of Data Science</a> virtual talk series on recent advances in adversarially robust streaming will take place on <strong>Tuesday, November 8th</strong> at <strong>1:00 PM Pacific Time</strong> (16:00 Eastern Time, 22:00 Central European Time, 20:00 UTC). <a href="http://www.cohenwang.com/edith/">Edith Cohen</a> from<strong> Google Research</strong> will talk about “On Robustness to Adaptive Inputs: A Case Study of CountSketch.<em>”</em></p>



<p><a href="https://sites.google.com/view/dstheory">Details of the talk (Zoom link) are available here.</a></p>



<p class="has-text-align-justify"><strong>Abstract</strong>:  The performance of randomized algorithms can be considered both in adaptive settings, where inputs may depend on prior outputs of the algorithm, and in non-adaptive settings. Adaptive settings arise when the algorithm is a component of a system with feedback or when an adversary attempts to construct an input on which the algorithm fails. The last decade saw impressive progress that included the development of black-box methods of obtaining algorithms that are robust to adaptive inputs from non-robust counterparts, most notably using differential privacy, and established lower bounds through &#8220;attacks.&#8221; But intriguing questions remain on the practical implications of adaptivity and the potential to develop tailored algorithms for specific problems and &#8220;nicer&#8221; inputs. In my talk, I will review key ideas from the literature and then present our recent work that explored these questions for CountSketch, a popular dimensionality reduction method (joint with Xin Lyu, Jelani Nelson, Tamas Sarlos, Moshe Shechner, and Uri Stemmer).</p>



<p>&nbsp;The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p>
<p class="authors">By dstheory</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T15:35:20Z">Friday, November 04 2022, 15:35</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/04/faculty-at-kth-royal-institute-of-technology-apply-by-november-28-2022/'>Faculty at KTH Royal Institute of Technology (apply by November 28, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Assistant Professor in Computer science with specialization in algorithms and complexity theory. The area is foundational theoretical computer science. This covers research in all areas of algorithms and complexity theory such as circuit complexity, combinatorial optimization, communication complexity, distributed algorithms, graph algorithms, quantum algorithms and proof complexity. Website: www.kth.se/en/om/work-at-kth/lediga-jobb/what:job/jobID:508144/type:job/where:4/apply:1 Email: johanh@kth.se,karlm@kth.se
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Assistant Professor in Computer science with specialization in algorithms and complexity theory.</p>
<p>The area is foundational theoretical computer science. This covers research in all areas of algorithms and complexity theory such as circuit complexity, combinatorial optimization, communication complexity, distributed algorithms, graph algorithms, quantum algorithms and proof complexity.</p>
<p>Website: <a href="https://www.kth.se/en/om/work-at-kth/lediga-jobb/what:job/jobID:508144/type:job/where:4/apply:1">https://www.kth.se/en/om/work-at-kth/lediga-jobb/what:job/jobID:508144/type:job/where:4/apply:1</a><br />
Email: johanh@kth.se,karlm@kth.se</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T09:36:27Z">Friday, November 04 2022, 09:36</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://decentralizedthoughts.github.io/2022-11-04-paxos-via-recoverable-broadcast/'>On Paxos from Recoverable Broadcast</a></h3>
        <p class='tr-article-feed'>from <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          There are many ways to learn about the Paxos protocol, this post is one more way. This posts has embedded a set of simple exercise - try to go over them! The model is Partial Synchrony with $f&lt;n/2$ omission failures and the goal is consensus (see below for exact details)....
        
        </div>

        <div class='tr-article-summary'>
        
          
          There are many ways to learn about the Paxos protocol, this post is one more way. This posts has embedded a set of simple exercise - try to go over them! The model is Partial Synchrony with $f&lt;n/2$ omission failures and the goal is consensus (see below for exact details)....
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T09:00:00Z">Friday, November 04 2022, 09:00</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://ptreview.sublinear.info/2022/11/news-for-october-2022/'>News for October 2022</a></h3>
        <p class='tr-article-feed'>from <a href='https://ptreview.sublinear.info'>Property Testing Review</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Yet another month that is kind of quiet! If we missed any work, please let us know in the comments. Gaussian Mean Testing Made Simple, by Ilias Diakonikolas, Daniel Kane and Ankit Pensia (arXiv). Consider an unknown distribution distribution \(p\) over \(\mathbb{R}^d\) that we have sample access to. The paper studies the problem of determining [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Yet another month that is kind of quiet! If we missed any work, please let us know in the comments.</p>



<p><strong>Gaussian Mean Testing Made Simple</strong>, by Ilias Diakonikolas, Daniel Kane and Ankit Pensia (<a rel="noreferrer noopener" href="https://arxiv.org/pdf/2210.13706.pdf" target="_blank">arXiv</a>). Consider an unknown distribution distribution \(p\) over \(\mathbb{R}^d\) that we have sample access to. The paper studies the problem of determining whether \(p\) is a standard Gaussian with zero mean or whether it is a Gaussian with large mean. More formally, the task is to distinguish between the case that \(p\) is \(\mathcal{N}(0, I_d)\) and the case that \(p\) is a Gaussian of the form \(\mathcal{N}(\mu, \Sigma)\), where \(||\mu||_2 \geq \epsilon\) and \(\Sigma\) is an unknown covariance matrix. Canonne, Chen, Kamath, Levi and Weingarten (2021) gave a sample-optimal algorithm for this problem with sample complexity \(\Theta(\sqrt{d}/\epsilon^2)\) sample complexity. The current paper gives another sample-optimal algorithm for the same problem with a simpler analysis. In addition to being sample-optimal, the algorithm in the current paper also runs in time linear in the total sample size, which is an improvement over the work of Canonne et al. </p>



<p><strong>Superpolynomial lower bounds for decision tree learning and testing</strong>, by Caleb Koch, Carmen Strassle and Li-Yang Tan (<a rel="noreferrer noopener" href="https://arxiv.org/pdf/2210.06375.pdf" target="_blank">arXiv</a>). Roughly speaking, the paper studies the problems of testing if a function has a low-depth decision tree and learning a low-depth decision tree approximating a function (provided that one such tree exists). In what follows, we summarize the testing results in the paper. Given an explicit representation of a function \(f:\{0,1\}^n \to \{0,1\}\) and access to samples from a known distribution \(\mathcal{D}\) over \(\{0,1\}^n\), one can aim to determine, with probability at least \(2/3\), if \(f\) has a decision tree of depth at most \(d\) or whether \(f\) is \(\epsilon\)-far from having a decision tree of depth at most \(d\log d\), where the distance is measured with respect to \(\mathcal{D}\). The paper shows that, under the randomized exponential time hypothesis, this problem cannot be solved in time \(\exp(d^{\Omega(1)})\). An immediate corollary is that the same lower bound holds for the problem of distribution-free testing of the property of having depth-\(d\) decision trees. The bound in the current paper is an improvement over the recent work of Blais, Ferreira Pinto Jr., and Harms (2021), who give a lower bound of \(\tilde{\Omega}(2^d)\) on the query complexity of testers for the same problem. However, the advantage of the latter result is that it is unconditional, as opposed to the result in the current paper. </p>
<p class="authors">By Nithin Varma</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T05:07:40Z">Friday, November 04 2022, 05:07</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.01443'>Improved Inapproximability of VC Dimension and Littlestone's Dimension via (Unbalanced) Biclique</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Pasin Manurangsi</p><p>We study the complexity of computing (and approximating) VC Dimension and
Littlestone's Dimension when we are given the concept class explicitly. We give
a simple reduction from Maximum (Unbalanced) Biclique problem to approximating
VC Dimension and Littlestone's Dimension. With this connection, we derive a
range of hardness of approximation results and running time lower bounds. For
example, under the (randomized) Gap-Exponential Time Hypothesis or the
Strongish Planted Clique Hypothesis, we show a tight inapproximability result:
both dimensions are hard to approximate to within a factor of $o(\log n)$ in
polynomial-time. These improve upon constant-factor inapproximability results
from [Manurangsi and Rubinstein, COLT 2017].
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Manurangsi_P/0/1/0/all/0/1">Pasin Manurangsi</a></p><p>We study the complexity of computing (and approximating) VC Dimension and
Littlestone's Dimension when we are given the concept class explicitly. We give
a simple reduction from Maximum (Unbalanced) Biclique problem to approximating
VC Dimension and Littlestone's Dimension. With this connection, we derive a
range of hardness of approximation results and running time lower bounds. For
example, under the (randomized) Gap-Exponential Time Hypothesis or the
Strongish Planted Clique Hypothesis, we show a tight inapproximability result:
both dimensions are hard to approximate to within a factor of $o(\log n)$ in
polynomial-time. These improve upon constant-factor inapproximability results
from [Manurangsi and Rubinstein, COLT 2017].
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T01:30:00Z">Friday, November 04 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.01444'>Pseudorandom (Function-Like) Quantum State Generators: New Definitions and Applications</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Prabhanjan Ananth, Aditya Gulati, Lower Qian, Henry Yuen</p><p>Pseudorandom quantum states (PRS) are efficiently constructible states that
are computationally indistinguishable from being Haar-random, and have recently
found cryptographic applications. We explore new definitions, new properties
and applications of pseudorandom states, and present the following
contributions:
</p>
<p>1. New Definitions: We study variants of pseudorandom function-like state
(PRFS) generators, introduced by Ananth, Qian, and Yuen (CRYPTO'22), where the
pseudorandomness property holds even when the generator can be queried
adaptively or in superposition. We show feasibility of these variants assuming
the existence of post-quantum one-way functions.
</p>
<p>2. Classical Communication: We show that PRS generators with logarithmic
output length imply commitment and encryption schemes with classical
communication. Previous constructions of such schemes from PRS generators
required quantum communication.
</p>
<p>3. Simplified Proof: We give a simpler proof of the Brakerski--Shmueli
(TCC'19) result that polynomially-many copies of uniform superposition states
with random binary phases are indistinguishable from Haar-random states.
</p>
<p>4. Necessity of Computational Assumptions: We also show that a secure PRS
with output length logarithmic, or larger, in the key length necessarily
requires computational assumptions.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Ananth_P/0/1/0/all/0/1">Prabhanjan Ananth</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Gulati_A/0/1/0/all/0/1">Aditya Gulati</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Qian_L/0/1/0/all/0/1">Lower Qian</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Yuen_H/0/1/0/all/0/1">Henry Yuen</a></p><p>Pseudorandom quantum states (PRS) are efficiently constructible states that
are computationally indistinguishable from being Haar-random, and have recently
found cryptographic applications. We explore new definitions, new properties
and applications of pseudorandom states, and present the following
contributions:
</p>
<p>1. New Definitions: We study variants of pseudorandom function-like state
(PRFS) generators, introduced by Ananth, Qian, and Yuen (CRYPTO'22), where the
pseudorandomness property holds even when the generator can be queried
adaptively or in superposition. We show feasibility of these variants assuming
the existence of post-quantum one-way functions.
</p>
<p>2. Classical Communication: We show that PRS generators with logarithmic
output length imply commitment and encryption schemes with classical
communication. Previous constructions of such schemes from PRS generators
required quantum communication.
</p>
<p>3. Simplified Proof: We give a simpler proof of the Brakerski--Shmueli
(TCC'19) result that polynomially-many copies of uniform superposition states
with random binary phases are indistinguishable from Haar-random states.
</p>
<p>4. Necessity of Computational Assumptions: We also show that a secure PRS
with output length logarithmic, or larger, in the key length necessarily
requires computational assumptions.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T01:30:00Z">Friday, November 04 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.01905'>The Complexity of Pattern Counting in Directed Graphs, Parameterised by the Outdegree</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Marco Bressan, Matthias Lanzinger, Marc Roth</p><p>We study the fixed-parameter tractability of the following fundamental
problem: given two directed graphs $\vec H$ and $\vec G$, count the number of
copies of $\vec H$ in $\vec G$. The standard setting, where the tractability is
well understood, uses only $|\vec H|$ as a parameter. In this paper we take a
step forward, and adopt as a parameter $|\vec H|+d(\vec G)$, where $d(\vec G)$
is the maximum outdegree of $|\vec G|$. Under this parameterization, we
completely characterize the fixed-parameter tractability of the problem in both
its non-induced and induced versions through two novel structural parameters,
the fractional cover number $\rho^*$ and the source number $\alpha_s$. On the
one hand we give algorithms with running time $f(|\vec H|,d(\vec G)) \cdot
|\vec G|^{\rho^*\!(\vec H)+O(1)}$ and $f(|\vec H|,d(\vec G)) \cdot |\vec
G|^{\alpha_s(\vec H)+O(1)}$ for counting respectively the copies and induced
copies of $\vec H$ in $\vec G$; on the other hand we show that, unless the
Exponential Time Hypothesis fails, for any class $\vec C$ of directed graphs
the (induced) counting problem is fixed-parameter tractable if and only if
$\rho^*(\vec C)$ ($\alpha_s(\vec C)$) is bounded. These results explain how the
orientation of the pattern can make counting easy or hard, and prove that a
classic algorithm by Chiba and Nishizeki and its extensions (Chiba, Nishizeki
SICOMP 85; Bressan Algorithmica 21) are optimal unless ETH fails.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bressan_M/0/1/0/all/0/1">Marco Bressan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lanzinger_M/0/1/0/all/0/1">Matthias Lanzinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_M/0/1/0/all/0/1">Marc Roth</a></p><p>We study the fixed-parameter tractability of the following fundamental
problem: given two directed graphs $\vec H$ and $\vec G$, count the number of
copies of $\vec H$ in $\vec G$. The standard setting, where the tractability is
well understood, uses only $|\vec H|$ as a parameter. In this paper we take a
step forward, and adopt as a parameter $|\vec H|+d(\vec G)$, where $d(\vec G)$
is the maximum outdegree of $|\vec G|$. Under this parameterization, we
completely characterize the fixed-parameter tractability of the problem in both
its non-induced and induced versions through two novel structural parameters,
the fractional cover number $\rho^*$ and the source number $\alpha_s$. On the
one hand we give algorithms with running time $f(|\vec H|,d(\vec G)) \cdot
|\vec G|^{\rho^*\!(\vec H)+O(1)}$ and $f(|\vec H|,d(\vec G)) \cdot |\vec
G|^{\alpha_s(\vec H)+O(1)}$ for counting respectively the copies and induced
copies of $\vec H$ in $\vec G$; on the other hand we show that, unless the
Exponential Time Hypothesis fails, for any class $\vec C$ of directed graphs
the (induced) counting problem is fixed-parameter tractable if and only if
$\rho^*(\vec C)$ ($\alpha_s(\vec C)$) is bounded. These results explain how the
orientation of the pattern can make counting easy or hard, and prove that a
classic algorithm by Chiba and Nishizeki and its extensions (Chiba, Nishizeki
SICOMP 85; Bressan Algorithmica 21) are optimal unless ETH fails.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T01:30:00Z">Friday, November 04 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.01990'>Membership in moment cones, quiver semi-invariants, and generic semi-stability for bipartite quivers</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Calin Chindris, Brett Collins, Daniel Kline</p><p>Let $Q$ be a bipartite quiver with vertex set $Q_0$ such that the number of
arrows between any two source and sink vertices is constant. Let
$\beta=(\beta(x))_{x \in Q_0}$ be a dimension vector of $Q$ with positive
integer coordinates, and let $\Delta(Q, \beta)$ be the moment cone associated
to $(Q, \beta)$. We show that the membership problem for $\Delta(Q, \beta)$ can
be solved in strongly polynomial time.
</p>
<p>As a key step in our approach, we first solve the polytopal problem for
semi-invariants of $Q$ and its flag-extensions. Specifically, let $Q_{\beta}$
be the flag-extension of $Q$ obtained by attaching a flag $\mathcal{F}(x)$ of
length $\beta(x)-1$ at every vertex $x$ of $Q$, and let $\widetilde{\beta}$ be
the extension of $\beta$ to $Q_{\beta}$ that takes values $1, \ldots, \beta(x)$
along the vertices of the flag $\mathcal{F}(x)$ for every vertex $x$ of $Q$.
For an integral weight $\widetilde{\sigma}$ of $Q_{\beta}$, let
$K_{\widetilde{\sigma}}$ be the dimension of the space of semi-invariants of
weight $\widetilde{\sigma}$ on the representation space of
$\widetilde{\beta}$-dimensional complex representations of $Q_{\beta}$.
</p>
<p>We show that $K_{\widetilde{\sigma}}$ can be expressed as the number of
lattice points of a certain hive-type polytope. This polytopal description
together with Derksen-Weyman's Saturation Theorem for quiver semi-invariants
allows us to use Tardos's algorithm to solve the membership problem for
$\Delta(Q,\beta)$ in strongly polynomial time. In particular, this yields a
strongly polynomial time algorithm for solving the generic semi-stability
problem for representations of $Q$ and $Q_\beta$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Chindris_C/0/1/0/all/0/1">Calin Chindris</a>, <a href="http://arxiv.org/find/math/1/au:+Collins_B/0/1/0/all/0/1">Brett Collins</a>, <a href="http://arxiv.org/find/math/1/au:+Kline_D/0/1/0/all/0/1">Daniel Kline</a></p><p>Let $Q$ be a bipartite quiver with vertex set $Q_0$ such that the number of
arrows between any two source and sink vertices is constant. Let
$\beta=(\beta(x))_{x \in Q_0}$ be a dimension vector of $Q$ with positive
integer coordinates, and let $\Delta(Q, \beta)$ be the moment cone associated
to $(Q, \beta)$. We show that the membership problem for $\Delta(Q, \beta)$ can
be solved in strongly polynomial time.
</p>
<p>As a key step in our approach, we first solve the polytopal problem for
semi-invariants of $Q$ and its flag-extensions. Specifically, let $Q_{\beta}$
be the flag-extension of $Q$ obtained by attaching a flag $\mathcal{F}(x)$ of
length $\beta(x)-1$ at every vertex $x$ of $Q$, and let $\widetilde{\beta}$ be
the extension of $\beta$ to $Q_{\beta}$ that takes values $1, \ldots, \beta(x)$
along the vertices of the flag $\mathcal{F}(x)$ for every vertex $x$ of $Q$.
For an integral weight $\widetilde{\sigma}$ of $Q_{\beta}$, let
$K_{\widetilde{\sigma}}$ be the dimension of the space of semi-invariants of
weight $\widetilde{\sigma}$ on the representation space of
$\widetilde{\beta}$-dimensional complex representations of $Q_{\beta}$.
</p>
<p>We show that $K_{\widetilde{\sigma}}$ can be expressed as the number of
lattice points of a certain hive-type polytope. This polytopal description
together with Derksen-Weyman's Saturation Theorem for quiver semi-invariants
allows us to use Tardos's algorithm to solve the membership problem for
$\Delta(Q,\beta)$ in strongly polynomial time. In particular, this yields a
strongly polynomial time algorithm for solving the generic semi-stability
problem for representations of $Q$ and $Q_\beta$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T01:30:00Z">Friday, November 04 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.01776'>Complexity of Simon's problem in classical sense</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Hans Zantema</p><p>Simon's problem is a standard example of a problem that is exponential in
classical sense, while it admits a polynomial solution in quantum computing. It
is about a function $f$ for which it is given that a unique non-zero vector $s$
exists for which $f(x) = f(x \oplus s)$ for all $x$, where $\oplus$ is the
exclusive or operator. The goal is to find $s$. The exponential lower bound for
the classical sense assumes that $f$ only admits black box access. In this
paper we investigate classical complexity when $f$ is given by a standard
representation like a circuit. We focus on finding the vector space of all
vectors $s$ for which $f(x) = f(x \oplus s)$ for all $x$, for any given $f$.
Two main results are: (1) if $f$ is given by any circuit, then checking whether
this vector space contains a non-zero element is NP-hard, and (2) if $f$ is
given by any ordered BDD, then a basis of this vector space can be computed in
polynomial time.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Zantema_H/0/1/0/all/0/1">Hans Zantema</a></p><p>Simon's problem is a standard example of a problem that is exponential in
classical sense, while it admits a polynomial solution in quantum computing. It
is about a function $f$ for which it is given that a unique non-zero vector $s$
exists for which $f(x) = f(x \oplus s)$ for all $x$, where $\oplus$ is the
exclusive or operator. The goal is to find $s$. The exponential lower bound for
the classical sense assumes that $f$ only admits black box access. In this
paper we investigate classical complexity when $f$ is given by a standard
representation like a circuit. We focus on finding the vector space of all
vectors $s$ for which $f(x) = f(x \oplus s)$ for all $x$, for any given $f$.
Two main results are: (1) if $f$ is given by any circuit, then checking whether
this vector space contains a non-zero element is NP-hard, and (2) if $f$ is
given by any ordered BDD, then a basis of this vector space can be computed in
polynomial time.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T01:30:00Z">Friday, November 04 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.01468'>A New Approach to Estimating Effective Resistances and Counting Spanning Trees in Expander Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Lawrence Li, Sushant Sachdeva</p><p>We demonstrate that for expander graphs, for all $\epsilon &gt; 0,$ there exists
a data structure of size $\widetilde{O}(n\epsilon^{-1})$ which can be used to
return $(1 + \epsilon)$-approximations to effective resistances in
$\widetilde{O}(1)$ time per query. Short of storing all effective resistances,
previous best approaches could achieve $\widetilde{O}(n\epsilon^{-2})$ size and
$\widetilde{O}(\epsilon^{-2})$ time per query by storing Johnson-Lindenstrauss
vectors for each vertex, or $\widetilde{O}(n\epsilon^{-1})$ size and
$\widetilde{O}(n\epsilon^{-1})$ time per query by storing a spectral sketch.
</p>
<p>Our construction is based on two key ideas: 1) $\epsilon^{-1}$-sparse,
$\epsilon$-additive approximations to $DL^+1_u$ for all $u,$ can be used to
recover $(1 + \epsilon)$-approximations to the effective resistances, 2) In
expander graphs, only $\widetilde{O}(\epsilon^{-1})$ coordinates of a vector
similar to $DL^+1_u$ are larger than $\epsilon.$ We give an efficient
construction for such a data structure in $\widetilde{O}(m + n\epsilon^{-2})$
time via random walks. This results in an algorithm for computing
$(1+\epsilon)$-approximate effective resistances for $s$ vertex pairs in
expanders that runs in $\widetilde{O}(m + n\epsilon^{-2} + s)$ time, improving
over the previously best known running time of $m^{1 + o(1)} + (n +
s)n^{o(1)}\epsilon^{-1.5}$ for $s = \omega(n\epsilon^{-0.5}).$
</p>
<p>We employ the above algorithm to compute a $(1+\delta)$-approximation to the
number of spanning trees in an expander graph, or equivalently, approximating
the (pseudo)determinant of its Laplacian in $\widetilde{O}(m +
n^{1.5}\delta^{-1})$ time. This improves on the previously best known result of
$m^{1+o(1)} + n^{1.875+o(1)}\delta^{-1.75}$ time, and matches the best known
size of determinant sparsifiers.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lawrence Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sachdeva_S/0/1/0/all/0/1">Sushant Sachdeva</a></p><p>We demonstrate that for expander graphs, for all $\epsilon &gt; 0,$ there exists
a data structure of size $\widetilde{O}(n\epsilon^{-1})$ which can be used to
return $(1 + \epsilon)$-approximations to effective resistances in
$\widetilde{O}(1)$ time per query. Short of storing all effective resistances,
previous best approaches could achieve $\widetilde{O}(n\epsilon^{-2})$ size and
$\widetilde{O}(\epsilon^{-2})$ time per query by storing Johnson-Lindenstrauss
vectors for each vertex, or $\widetilde{O}(n\epsilon^{-1})$ size and
$\widetilde{O}(n\epsilon^{-1})$ time per query by storing a spectral sketch.
</p>
<p>Our construction is based on two key ideas: 1) $\epsilon^{-1}$-sparse,
$\epsilon$-additive approximations to $DL^+1_u$ for all $u,$ can be used to
recover $(1 + \epsilon)$-approximations to the effective resistances, 2) In
expander graphs, only $\widetilde{O}(\epsilon^{-1})$ coordinates of a vector
similar to $DL^+1_u$ are larger than $\epsilon.$ We give an efficient
construction for such a data structure in $\widetilde{O}(m + n\epsilon^{-2})$
time via random walks. This results in an algorithm for computing
$(1+\epsilon)$-approximate effective resistances for $s$ vertex pairs in
expanders that runs in $\widetilde{O}(m + n\epsilon^{-2} + s)$ time, improving
over the previously best known running time of $m^{1 + o(1)} + (n +
s)n^{o(1)}\epsilon^{-1.5}$ for $s = \omega(n\epsilon^{-0.5}).$
</p>
<p>We employ the above algorithm to compute a $(1+\delta)$-approximation to the
number of spanning trees in an expander graph, or equivalently, approximating
the (pseudo)determinant of its Laplacian in $\widetilde{O}(m +
n^{1.5}\delta^{-1})$ time. This improves on the previously best known result of
$m^{1+o(1)} + n^{1.875+o(1)}\delta^{-1.75}$ time, and matches the best known
size of determinant sparsifiers.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T01:30:00Z">Friday, November 04 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.01612'>Computing a many-to-many matching with demands and capacities between two sets using the Hungarian algorithm</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Fatemeh Rajabi-Alni, Alireza Bagheri</p><p>Given two sets A={a_1,a_2,...,a_s} and {b_1,b_2,...,b_t}, a many-to-many
matching with demands and capacities (MMDC) between A and B matches each
element a_i in A to at least \alpha_i and at most \alpha'_i elements in B, and
each element b_j in B to at least \beta_j and at most \beta'_j elements in A
for all 1=&lt;i&lt;=s and 1=&lt;j&lt;=t. In this paper, we present an algorithm for finding
a minimum-cost MMDC between A and B using the well-known Hungarian algorithm.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Rajabi_Alni_F/0/1/0/all/0/1">Fatemeh Rajabi-Alni</a>, <a href="http://arxiv.org/find/cs/1/au:+Bagheri_A/0/1/0/all/0/1">Alireza Bagheri</a></p><p>Given two sets A={a_1,a_2,...,a_s} and {b_1,b_2,...,b_t}, a many-to-many
matching with demands and capacities (MMDC) between A and B matches each
element a_i in A to at least \alpha_i and at most \alpha'_i elements in B, and
each element b_j in B to at least \beta_j and at most \beta'_j elements in A
for all 1=&lt;i&lt;=s and 1=&lt;j&lt;=t. In this paper, we present an algorithm for finding
a minimum-cost MMDC between A and B using the well-known Hungarian algorithm.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T01:30:00Z">Friday, November 04 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.01661'>Pairing optimization via statistics: Algebraic structure in pairing problems and its application to performance enhancement</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Naoki Fujita, Andr&#xe9; R&#xf6;hm, Takatomo Mihana, Ryoichi Horisaki, Aohan Li, Mikio Hasegawa, Makoto Naruse</p><p>Fully pairing all elements of a set while attempting to maximize the total
benefit is a combinatorically difficult problem. Such pairing problems
naturally appear in various situations in science, technology, economics, and
other fields. In our previous study, we proposed an efficient method to infer
the underlying compatibilities among the entities, under the constraint that
only the total compatibility is observable. Furthermore, by transforming the
pairing problem into a traveling salesman problem with a multi-layer
architecture, a pairing optimization algorithm was successfully demonstrated to
derive a high-total-compatibility pairing. However, there is substantial room
for further performance enhancement by further exploiting the underlying
mathematical properties. In this study, we prove the existence of algebraic
structures in the pairing problem. We transform the initially estimated
compatibility information into an equivalent form where the variance of the
individual compatibilities is minimized. We then demonstrate that the total
compatibility obtained when using the heuristic pairing algorithm on the
transformed problem is significantly higher compared to the previous method.
With this improved perspective on the pairing problem using fundamental
mathematical properties, we can contribute to practical applications such as
wireless communications beyond 5G, where efficient pairing is of critical
importance.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Fujita_N/0/1/0/all/0/1">Naoki Fujita</a>, <a href="http://arxiv.org/find/cs/1/au:+Rohm_A/0/1/0/all/0/1">Andr&#xe9; R&#xf6;hm</a>, <a href="http://arxiv.org/find/cs/1/au:+Mihana_T/0/1/0/all/0/1">Takatomo Mihana</a>, <a href="http://arxiv.org/find/cs/1/au:+Horisaki_R/0/1/0/all/0/1">Ryoichi Horisaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Aohan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasegawa_M/0/1/0/all/0/1">Mikio Hasegawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Naruse_M/0/1/0/all/0/1">Makoto Naruse</a></p><p>Fully pairing all elements of a set while attempting to maximize the total
benefit is a combinatorically difficult problem. Such pairing problems
naturally appear in various situations in science, technology, economics, and
other fields. In our previous study, we proposed an efficient method to infer
the underlying compatibilities among the entities, under the constraint that
only the total compatibility is observable. Furthermore, by transforming the
pairing problem into a traveling salesman problem with a multi-layer
architecture, a pairing optimization algorithm was successfully demonstrated to
derive a high-total-compatibility pairing. However, there is substantial room
for further performance enhancement by further exploiting the underlying
mathematical properties. In this study, we prove the existence of algebraic
structures in the pairing problem. We transform the initially estimated
compatibility information into an equivalent form where the variance of the
individual compatibilities is minimized. We then demonstrate that the total
compatibility obtained when using the heuristic pairing algorithm on the
transformed problem is significantly higher compared to the previous method.
With this improved perspective on the pairing problem using fundamental
mathematical properties, we can contribute to practical applications such as
wireless communications beyond 5G, where efficient pairing is of critical
importance.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T01:30:00Z">Friday, November 04 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.01699'>A Round and Bipartize Approximation Algorithm for Vertex Cover</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Danish Kashaev, Guido Sch&#xe4;fer</p><p>The vertex cover problem is a fundamental and widely studied combinatorial
optimization problem. It is known that its standard linear programming
relaxation is integral for bipartite graphs and half-integral for general
graphs. As a consequence, the natural rounding algorithm based on this
relaxation computes an optimal solution for bipartite graphs and a
$2$-approximation for general graphs. This raises the question of whether one
can obtain improved bounds on the approximation ratio, depending on how close
the graph is to being bipartite.
</p>
<p>In this paper, we consider a round-and-bipartize algorithm that exploits the
knowledge of an induced bipartite subgraph to attain improved approximation
ratios. Equivalently, we suppose that we have access to a subset of vertices
$S$ whose removal bipartizes the graph.
</p>
<p>If $S$ is an independent set, we prove an approximation ratio of $1 +
1/\rho$, where $2\rho -1$ denotes the odd girth of the contracted graph
$\tilde{\mathcal{G}} := \mathcal{G} /S$ and thus satisfies $\rho \geq 2$. We
show that this is tight for any graph and independent set by providing a family
of weight functions for which this bound is attained. In addition, we give
tight upper bounds for the fractional chromatic number and the integrality gap
of such graphs, both of which also depend on the odd girth.
</p>
<p>If $S$ is an arbitrary set, we prove a tight approximation ratio of
$\left(1+1/\rho \right) (1 - \alpha) + 2 \alpha$, where $\alpha \in [0,1]$
denotes the total normalized dual sum of the edges lying inside of the set $S$.
As an algorithmic application, we show that for any efficiently $k$-colorable
graph with $k \geq 4$ we can find a bipartizing set satisfying $\alpha \leq 1 -
4/k$. This provides an approximation algorithm recovering the bound of $2 -
2/k$ in the worst case (i.e., when $\rho = 2$), which is best possible for this
setting when using the standard relaxation.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kashaev_D/0/1/0/all/0/1">Danish Kashaev</a>, <a href="http://arxiv.org/find/cs/1/au:+Schafer_G/0/1/0/all/0/1">Guido Sch&#xe4;fer</a></p><p>The vertex cover problem is a fundamental and widely studied combinatorial
optimization problem. It is known that its standard linear programming
relaxation is integral for bipartite graphs and half-integral for general
graphs. As a consequence, the natural rounding algorithm based on this
relaxation computes an optimal solution for bipartite graphs and a
$2$-approximation for general graphs. This raises the question of whether one
can obtain improved bounds on the approximation ratio, depending on how close
the graph is to being bipartite.
</p>
<p>In this paper, we consider a round-and-bipartize algorithm that exploits the
knowledge of an induced bipartite subgraph to attain improved approximation
ratios. Equivalently, we suppose that we have access to a subset of vertices
$S$ whose removal bipartizes the graph.
</p>
<p>If $S$ is an independent set, we prove an approximation ratio of $1 +
1/\rho$, where $2\rho -1$ denotes the odd girth of the contracted graph
$\tilde{\mathcal{G}} := \mathcal{G} /S$ and thus satisfies $\rho \geq 2$. We
show that this is tight for any graph and independent set by providing a family
of weight functions for which this bound is attained. In addition, we give
tight upper bounds for the fractional chromatic number and the integrality gap
of such graphs, both of which also depend on the odd girth.
</p>
<p>If $S$ is an arbitrary set, we prove a tight approximation ratio of
$\left(1+1/\rho \right) (1 - \alpha) + 2 \alpha$, where $\alpha \in [0,1]$
denotes the total normalized dual sum of the edges lying inside of the set $S$.
As an algorithmic application, we show that for any efficiently $k$-colorable
graph with $k \geq 4$ we can find a bipartizing set satisfying $\alpha \leq 1 -
4/k$. This provides an approximation algorithm recovering the bound of $2 -
2/k$ in the worst case (i.e., when $\rho = 2$), which is best possible for this
setting when using the standard relaxation.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T01:30:00Z">Friday, November 04 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.01701'>Efficient Branch-and-Bound Algorithms for Finding Triangle-Constrained 2-Clubs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Niels Gr&#xfc;ttemeier, Christian Komusiewicz, Philipp Heinrich Ke&#xdf;ler, Frank Sommer</p><p>In the Vertex Triangle 2-Club problem, we are given an undirected graph $G$
and aim to find a maximum-vertex subgraph of $G$ that has diameter at most 2
and in which every vertex is contained in at least $\ell$ triangles in the
subgraph. So far, the only algorithm for solving Vertex Triangle 2-Club relies
on an ILP formulation [Almeida and Br\'as, Comput. Oper. Res. 2019]. In this
work, we develop a combinatorial branch-and-bound algorithm that, coupled with
a set of data reduction rules, outperforms the existing implementation and is
able to find optimal solutions on sparse real-world graphs with more than
100,000 vertices in a few minutes. We also extend our algorithm to the Edge
Triangle 2-Club problem where the triangle constraint is imposed on all edges
of the subgraph.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gruttemeier_N/0/1/0/all/0/1">Niels Gr&#xfc;ttemeier</a>, <a href="http://arxiv.org/find/cs/1/au:+Komusiewicz_C/0/1/0/all/0/1">Christian Komusiewicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kessler_P/0/1/0/all/0/1">Philipp Heinrich Ke&#xdf;ler</a>, <a href="http://arxiv.org/find/cs/1/au:+Sommer_F/0/1/0/all/0/1">Frank Sommer</a></p><p>In the Vertex Triangle 2-Club problem, we are given an undirected graph $G$
and aim to find a maximum-vertex subgraph of $G$ that has diameter at most 2
and in which every vertex is contained in at least $\ell$ triangles in the
subgraph. So far, the only algorithm for solving Vertex Triangle 2-Club relies
on an ILP formulation [Almeida and Br\'as, Comput. Oper. Res. 2019]. In this
work, we develop a combinatorial branch-and-bound algorithm that, coupled with
a set of data reduction rules, outperforms the existing implementation and is
able to find optimal solutions on sparse real-world graphs with more than
100,000 vertices in a few minutes. We also extend our algorithm to the Edge
Triangle 2-Club problem where the triangle constraint is imposed on all edges
of the subgraph.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T01:30:00Z">Friday, November 04 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.01723'>Model-Checking for First-Order Logic with Disjoint Paths Predicates in Proper Minor-Closed Graph Classes</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Petr A. Golovach, Giannos Stamoulis, Dimitrios M. Thilikos</p><p>The disjoint paths logic, FOL+DP, is an extension of First-Order Logic (FOL)
with the extra atomic predicate ${\sf dp}_k(x_1,y_1,\ldots,x_k,y_k),$
expressing the existence of internally vertex-disjoint paths between $x_i$ and
$y_i,$ for $i\in\{1,\ldots, k\}$. This logic can express a wide variety of
problems that escape the expressibility potential of FOL. We prove that for
every proper minor-closed graph class, model-checking for FOL+DP can be done in
quadratic time. We also introduce an extension of FOL+DP, namely the scattered
disjoint paths logic, FOL+SDP, where we further consider the atomic predicate
$s{\sf -sdp}_k(x_1,y_1,\ldots,x_k,y_k),$ demanding that the disjoint paths are
within distance bigger than some fixed value $s$. Using the same technique we
prove that model-checking for FOL+SDP can be done in quadratic time on classes
of graphs with bounded Euler genus.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Golovach_P/0/1/0/all/0/1">Petr A. Golovach</a>, <a href="http://arxiv.org/find/cs/1/au:+Stamoulis_G/0/1/0/all/0/1">Giannos Stamoulis</a>, <a href="http://arxiv.org/find/cs/1/au:+Thilikos_D/0/1/0/all/0/1">Dimitrios M. Thilikos</a></p><p>The disjoint paths logic, FOL+DP, is an extension of First-Order Logic (FOL)
with the extra atomic predicate ${\sf dp}_k(x_1,y_1,\ldots,x_k,y_k),$
expressing the existence of internally vertex-disjoint paths between $x_i$ and
$y_i,$ for $i\in\{1,\ldots, k\}$. This logic can express a wide variety of
problems that escape the expressibility potential of FOL. We prove that for
every proper minor-closed graph class, model-checking for FOL+DP can be done in
quadratic time. We also introduce an extension of FOL+DP, namely the scattered
disjoint paths logic, FOL+SDP, where we further consider the atomic predicate
$s{\sf -sdp}_k(x_1,y_1,\ldots,x_k,y_k),$ demanding that the disjoint paths are
within distance bigger than some fixed value $s$. Using the same technique we
prove that model-checking for FOL+SDP can be done in quadratic time on classes
of graphs with bounded Euler genus.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T01:30:00Z">Friday, November 04 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.01725'>Distributed Reconfiguration of Spanning Trees</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Siddharth Gupta, Manish Kumar, Shreyas Pai</p><p>In a reconfiguration problem, given a problem and two feasible solutions of
the problem, the task is to find a sequence of transformations to reach from
one solution to the other such that every intermediate state is also a feasible
solution to the problem. In this paper, we study the distributed spanning tree
reconfiguration problem and we define a new reconfiguration step, called
$k$-simultaneous add and delete, in which every node is allowed to add at most
$k$ edges and delete at most $k$ edges such that multiple nodes do not add or
delete the same edge.
</p>
<p>We first observe that, if the two input spanning trees are rooted, then we
can do the reconfiguration using a single $1$-simultaneous add and delete step
in one round in the CONGEST model. Therefore, we focus our attention towards
unrooted spanning trees and show that transforming an unrooted spanning tree
into another using a single $1$-simultaneous add and delete step requires
$\Omega(n)$ rounds in the LOCAL model. We additionally show that transforming
an unrooted spanning tree into another using a single $2$-simultaneous add and
delete step can be done in $O(\log n)$ rounds in the CONGEST model.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1">Siddharth Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1">Manish Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Pai_S/0/1/0/all/0/1">Shreyas Pai</a></p><p>In a reconfiguration problem, given a problem and two feasible solutions of
the problem, the task is to find a sequence of transformations to reach from
one solution to the other such that every intermediate state is also a feasible
solution to the problem. In this paper, we study the distributed spanning tree
reconfiguration problem and we define a new reconfiguration step, called
$k$-simultaneous add and delete, in which every node is allowed to add at most
$k$ edges and delete at most $k$ edges such that multiple nodes do not add or
delete the same edge.
</p>
<p>We first observe that, if the two input spanning trees are rooted, then we
can do the reconfiguration using a single $1$-simultaneous add and delete step
in one round in the CONGEST model. Therefore, we focus our attention towards
unrooted spanning trees and show that transforming an unrooted spanning tree
into another using a single $1$-simultaneous add and delete step requires
$\Omega(n)$ rounds in the LOCAL model. We additionally show that transforming
an unrooted spanning tree into another using a single $2$-simultaneous add and
delete step can be done in $O(\log n)$ rounds in the CONGEST model.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T01:30:00Z">Friday, November 04 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.01726'>SQUID: Faster Analytics via Sampled Quantiles Data-structure</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ran Ben-Basat, Gil Einziger, Wenchen Han, Bilal Tayh</p><p>Measurement is a fundamental enabler of network applications such as load
balancing, attack detection and mitigation, and traffic engineering. A key
building block in many critical measurement tasks is \emph{q-MAX}, where we
wish to find the largest $q$ values in a number stream. A standard approach of
maintaining a heap of the largest $q$ values ordered results in logarithmic
runtime, which is too slow for large measurements. Modern approaches attain a
constant runtime by removing small items in bulk and retaining the largest $q$
items at all times. Yet, these approaches are bottlenecked by an expensive
quantile calculation method.
</p>
<p>We propose SQUID, a method that redesigns q-MAX to allow the use of
\emph{approximate quantiles}, which we can compute efficiently, thereby
accelerating the solution and, subsequently, many measurement tasks. We
demonstrate the benefit of our approach by designing a novel weighted heavy
hitters data structure that is faster and more accurate than the existing
alternatives. Here, we combine our previous techniques with a lazy deletion of
small entries, which expiates the maintenance process and increases the
accuracy. We also demonstrate the applicability of our algorithmic approach in
a general algorithmic scope by implementing the LRFU cache policy with a
constant update time. Furthermore, we also show the practicality of SQUID for
improving real-world networked systems, by implementing a P4 prototype of SQUID
for in-network caching and demonstrating how SQUID enables a wide spectrum of
score-based caching policies directly on a P4 switch.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ben_Basat_R/0/1/0/all/0/1">Ran Ben-Basat</a>, <a href="http://arxiv.org/find/cs/1/au:+Einziger_G/0/1/0/all/0/1">Gil Einziger</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1">Wenchen Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Tayh_B/0/1/0/all/0/1">Bilal Tayh</a></p><p>Measurement is a fundamental enabler of network applications such as load
balancing, attack detection and mitigation, and traffic engineering. A key
building block in many critical measurement tasks is \emph{q-MAX}, where we
wish to find the largest $q$ values in a number stream. A standard approach of
maintaining a heap of the largest $q$ values ordered results in logarithmic
runtime, which is too slow for large measurements. Modern approaches attain a
constant runtime by removing small items in bulk and retaining the largest $q$
items at all times. Yet, these approaches are bottlenecked by an expensive
quantile calculation method.
</p>
<p>We propose SQUID, a method that redesigns q-MAX to allow the use of
\emph{approximate quantiles}, which we can compute efficiently, thereby
accelerating the solution and, subsequently, many measurement tasks. We
demonstrate the benefit of our approach by designing a novel weighted heavy
hitters data structure that is faster and more accurate than the existing
alternatives. Here, we combine our previous techniques with a lazy deletion of
small entries, which expiates the maintenance process and increases the
accuracy. We also demonstrate the applicability of our algorithmic approach in
a general algorithmic scope by implementing the LRFU cache policy with a
constant update time. Furthermore, we also show the practicality of SQUID for
improving real-world networked systems, by implementing a P4 prototype of SQUID
for in-network caching and demonstrating how SQUID enables a wide spectrum of
score-based caching policies directly on a P4 switch.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T01:30:00Z">Friday, November 04 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.01912'>Matching Augmentation via Simultaneous Contractions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mohit Garg, Felix Hommelsheim, Nicole Megow</p><p>We consider the matching augmentation problem (MAP), where a matching of a
graph needs to be extended into a $2$-edge-connected spanning subgraph by
adding the minimum number of edges to it. We present a polynomial-time
algorithm with an approximation ratio of $13/8 = 1.625$ improving upon an
earlier $5/3$-approximation. The improvement builds on a new
$\alpha$-approximation preserving reduction for any $\alpha\geq 3/2$ from
arbitrary MAP instances to well-structured instances that do not contain
certain forbidden structures like parallel edges, small separators, and
contractible subgraphs. We further introduce, as key ingredients, the technique
of repeated simultaneous contractions and provide improved lower bounds for
instances that cannot be contracted.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1">Mohit Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Hommelsheim_F/0/1/0/all/0/1">Felix Hommelsheim</a>, <a href="http://arxiv.org/find/cs/1/au:+Megow_N/0/1/0/all/0/1">Nicole Megow</a></p><p>We consider the matching augmentation problem (MAP), where a matching of a
graph needs to be extended into a $2$-edge-connected spanning subgraph by
adding the minimum number of edges to it. We present a polynomial-time
algorithm with an approximation ratio of $13/8 = 1.625$ improving upon an
earlier $5/3$-approximation. The improvement builds on a new
$\alpha$-approximation preserving reduction for any $\alpha\geq 3/2$ from
arbitrary MAP instances to well-structured instances that do not contain
certain forbidden structures like parallel edges, small separators, and
contractible subgraphs. We further introduce, as key ingredients, the technique
of repeated simultaneous contractions and provide improved lower bounds for
instances that cannot be contracted.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T01:30:00Z">Friday, November 04 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.01945'>Distributed Maximal Matching and Maximal Independent Set on Hypergraphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Alkida Balliu, Sebastian Brandt, Fabian Kuhn, Dennis Olivetti</p><p>We investigate the distributed complexity of maximal matching and maximal
independent set (MIS) in hypergraphs in the LOCAL model. A maximal matching of
a hypergraph $H=(V_H,E_H)$ is a maximal disjoint set $M\subseteq E_H$ of
hyperedges and an MIS $S\subseteq V_H$ is a maximal set of nodes such that no
hyperedge is fully contained in $S$. Both problems can be solved by a simple
sequential greedy algorithm, which can be implemented naively in $O(\Delta r +
\log^* n)$ rounds, where $\Delta$ is the maximum degree, $r$ is the rank, and
$n$ is the number of nodes.
</p>
<p>We show that for maximal matching, this naive algorithm is optimal in the
following sense. Any deterministic algorithm for solving the problem requires
$\Omega(\min\{\Delta r, \log_{\Delta r} n\})$ rounds, and any randomized one
requires $\Omega(\min\{\Delta r, \log_{\Delta r} \log n\})$ rounds. Hence, for
any algorithm with a complexity of the form $O(f(\Delta, r) + g(n))$, we have
$f(\Delta, r) \in \Omega(\Delta r)$ if $g(n)$ is not too large, and in
particular if $g(n) = \log^* n$ (which is the optimal asymptotic dependency on
$n$ due to Linial's lower bound [FOCS'87]). Our lower bound proof is based on
the round elimination framework, and its structure is inspired by a new round
elimination fixed point that we give for the $\Delta$-vertex coloring problem
in hypergraphs.
</p>
<p>For the MIS problem on hypergraphs, we show that for $\Delta\ll r$, there are
significant improvements over the naive $O(\Delta r + \log^* n)$-round
algorithm. We give two deterministic algorithms for the problem. We show that a
hypergraph MIS can be computed in $O(\Delta^2\cdot\log r + \Delta\cdot\log
r\cdot \log^* r + \log^* n)$ rounds. We further show that at the cost of a
worse dependency on $\Delta$, the dependency on $r$ can be removed almost
entirely, by giving an algorithm with complexity $\Delta^{O(\Delta)}\cdot\log^*
r + O(\log^* n)$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Balliu_A/0/1/0/all/0/1">Alkida Balliu</a>, <a href="http://arxiv.org/find/cs/1/au:+Brandt_S/0/1/0/all/0/1">Sebastian Brandt</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuhn_F/0/1/0/all/0/1">Fabian Kuhn</a>, <a href="http://arxiv.org/find/cs/1/au:+Olivetti_D/0/1/0/all/0/1">Dennis Olivetti</a></p><p>We investigate the distributed complexity of maximal matching and maximal
independent set (MIS) in hypergraphs in the LOCAL model. A maximal matching of
a hypergraph $H=(V_H,E_H)$ is a maximal disjoint set $M\subseteq E_H$ of
hyperedges and an MIS $S\subseteq V_H$ is a maximal set of nodes such that no
hyperedge is fully contained in $S$. Both problems can be solved by a simple
sequential greedy algorithm, which can be implemented naively in $O(\Delta r +
\log^* n)$ rounds, where $\Delta$ is the maximum degree, $r$ is the rank, and
$n$ is the number of nodes.
</p>
<p>We show that for maximal matching, this naive algorithm is optimal in the
following sense. Any deterministic algorithm for solving the problem requires
$\Omega(\min\{\Delta r, \log_{\Delta r} n\})$ rounds, and any randomized one
requires $\Omega(\min\{\Delta r, \log_{\Delta r} \log n\})$ rounds. Hence, for
any algorithm with a complexity of the form $O(f(\Delta, r) + g(n))$, we have
$f(\Delta, r) \in \Omega(\Delta r)$ if $g(n)$ is not too large, and in
particular if $g(n) = \log^* n$ (which is the optimal asymptotic dependency on
$n$ due to Linial's lower bound [FOCS'87]). Our lower bound proof is based on
the round elimination framework, and its structure is inspired by a new round
elimination fixed point that we give for the $\Delta$-vertex coloring problem
in hypergraphs.
</p>
<p>For the MIS problem on hypergraphs, we show that for $\Delta\ll r$, there are
significant improvements over the naive $O(\Delta r + \log^* n)$-round
algorithm. We give two deterministic algorithms for the problem. We show that a
hypergraph MIS can be computed in $O(\Delta^2\cdot\log r + \Delta\cdot\log
r\cdot \log^* r + \log^* n)$ rounds. We further show that at the cost of a
worse dependency on $\Delta$, the dependency on $r$ can be removed almost
entirely, by giving an algorithm with complexity $\Delta^{O(\Delta)}\cdot\log^*
r + O(\log^* n)$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T01:30:00Z">Friday, November 04 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.02004'>Truthful Matching with Online Items and Offline Agents</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Michal Feldman, Federico Fusco, Stefano Leonardi, Simon Mauras, Rebecca Reiffenh&#xe4;user</p><p>We study truthful mechanisms for welfare maximization in online bipartite
matching. In our (multi-parameter) setting, every buyer is associated with a
(possibly private) desired set of items, and has a private value for being
assigned an item in her desired set. Unlike most online matching settings,
where agents arrive online, in our setting the items arrive online in an
adversarial order while the buyers are present for the entire duration of the
process. This poses a significant challenge to the design of truthful
mechanisms, due to the ability of buyers to strategize over future rounds. We
provide an almost full picture of the competitive ratios in different
scenarios, including myopic vs. non-myopic agents, tardy vs. prompt payments,
and private vs. public desired sets. Among other results, we identify the
frontier for which the celebrated $e/(e-1)$ competitive ratio for the
vertex-weighted online matching of Karp, Vazirani and Vazirani extends to
truthful agents and online items.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Feldman_M/0/1/0/all/0/1">Michal Feldman</a>, <a href="http://arxiv.org/find/cs/1/au:+Fusco_F/0/1/0/all/0/1">Federico Fusco</a>, <a href="http://arxiv.org/find/cs/1/au:+Leonardi_S/0/1/0/all/0/1">Stefano Leonardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mauras_S/0/1/0/all/0/1">Simon Mauras</a>, <a href="http://arxiv.org/find/cs/1/au:+Reiffenhauser_R/0/1/0/all/0/1">Rebecca Reiffenh&#xe4;user</a></p><p>We study truthful mechanisms for welfare maximization in online bipartite
matching. In our (multi-parameter) setting, every buyer is associated with a
(possibly private) desired set of items, and has a private value for being
assigned an item in her desired set. Unlike most online matching settings,
where agents arrive online, in our setting the items arrive online in an
adversarial order while the buyers are present for the entire duration of the
process. This poses a significant challenge to the design of truthful
mechanisms, due to the ability of buyers to strategize over future rounds. We
provide an almost full picture of the competitive ratios in different
scenarios, including myopic vs. non-myopic agents, tardy vs. prompt payments,
and private vs. public desired sets. Among other results, we identify the
frontier for which the celebrated $e/(e-1)$ competitive ratio for the
vertex-weighted online matching of Karp, Vazirani and Vazirani extends to
truthful agents and online items.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T01:30:00Z">Friday, November 04 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.02044'>Competitive Kill-and-Restart Strategies for Non-Clairvoyant Scheduling</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sven J&#xe4;ger, Guillaume Sagnol, Daniel Schmidt genannt Waldschmidt, Philipp Warode</p><p>We consider the fundamental scheduling problem of minimizing the sum of
weighted completion times on a single machine in the non-clairvoyant setting.
While no non-preemptive algorithm is constant competitive, Motwani, Phillips,
and Torng (SODA '93) proved that the simple preemptive round robin procedure is
$2$-competitive and that no better competitive ratio is possible, initiating a
long line of research focused on preemptive algorithms for generalized variants
of the problem. As an alternative model, Shmoys, Wein, and Williamson (FOCS
'91) introduced kill-and-restart schedules, where running jobs may be killed
and restarted from scratch later, and analyzed then for the makespan objective.
However, to the best of our knowledge, this concept has never been considered
for the total completion time objective in the non-clairvoyant model.
</p>
<p>We contribute to both models: First we give for any $b &gt; 1$ a tight analysis
for the natural $b$-scaling kill-and-restart strategy for scheduling jobs
without release dates, as well as for a randomized variant of it. This implies
a performance guarantee of $(1+3\sqrt{3})\approx 6.197$ for the deterministic
algorithm and of $\approx 3.032$ for the randomized version. Second, we show
that the preemptive Weighted Shortest Elapsed Time First (WSETF) rule is
$2$-competitive for jobs released in an online fashion over time, matching the
lower bound by Motwani et al. Using this result as well as the competitiveness
of round robin for multiple machines, we prove performance guarantees of
adaptions of the $b$-scaling algorithm to online release dates and unweighted
jobs on identical parallel machines.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jager_S/0/1/0/all/0/1">Sven J&#xe4;ger</a>, <a href="http://arxiv.org/find/cs/1/au:+Sagnol_G/0/1/0/all/0/1">Guillaume Sagnol</a>, <a href="http://arxiv.org/find/cs/1/au:+Waldschmidt_D/0/1/0/all/0/1">Daniel Schmidt genannt Waldschmidt</a>, <a href="http://arxiv.org/find/cs/1/au:+Warode_P/0/1/0/all/0/1">Philipp Warode</a></p><p>We consider the fundamental scheduling problem of minimizing the sum of
weighted completion times on a single machine in the non-clairvoyant setting.
While no non-preemptive algorithm is constant competitive, Motwani, Phillips,
and Torng (SODA '93) proved that the simple preemptive round robin procedure is
$2$-competitive and that no better competitive ratio is possible, initiating a
long line of research focused on preemptive algorithms for generalized variants
of the problem. As an alternative model, Shmoys, Wein, and Williamson (FOCS
'91) introduced kill-and-restart schedules, where running jobs may be killed
and restarted from scratch later, and analyzed then for the makespan objective.
However, to the best of our knowledge, this concept has never been considered
for the total completion time objective in the non-clairvoyant model.
</p>
<p>We contribute to both models: First we give for any $b &gt; 1$ a tight analysis
for the natural $b$-scaling kill-and-restart strategy for scheduling jobs
without release dates, as well as for a randomized variant of it. This implies
a performance guarantee of $(1+3\sqrt{3})\approx 6.197$ for the deterministic
algorithm and of $\approx 3.032$ for the randomized version. Second, we show
that the preemptive Weighted Shortest Elapsed Time First (WSETF) rule is
$2$-competitive for jobs released in an online fashion over time, matching the
lower bound by Motwani et al. Using this result as well as the competitiveness
of round robin for multiple machines, we prove performance guarantees of
adaptions of the $b$-scaling algorithm to online release dates and unweighted
jobs on identical parallel machines.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-04T01:30:00Z">Friday, November 04 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Thursday, November 03
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/142'>TR22-142 |  Correlation bounds against polynomials | 

	Emanuele Viola</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          A survey on correlation bounds against polynomials.
        
        </div>

        <div class='tr-article-summary'>
        
          
          A survey on correlation bounds against polynomials.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-03T15:00:46Z">Thursday, November 03 2022, 15:00</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://blog.computationalcomplexity.org/2022/11/should-you-quit-twitter-and-texas.html'>Should you quit Twitter and Texas?</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Generally with some exceptions, I use Facebook for personal stuff, LinkedIn for Illinois Tech stuff and Twitter&nbsp;and this blog for CS stuff. Many of you got to this post through the Twitter link. Now that Elon Musk has bought the social media company, should I and the rest of the academic twitterverse move on to something else?</p><p>I'd say not yet. Let's see what Elon does to the place. Maybe he can allow more points of view, without turning it into a cesspool. Or maybe he ruins it. It'll be a network effect--if too many academics leave Twitter, I'd have to follow or I'd have few followers. I wonder where they will go. I hope it isn't TikTok.</p><p>On a similar vein, I often here of those who suggest we don't hold conferences in certain jurisdictions for political reasons, for example Texas, because of its laws against abortion and transgender rights. I don't believe computer science, as a field, should be making decisions based on politics. Academics who live in these states don't generally hold the same views as the political leaders in those states.</p><p>Should we not have meetings in Illinois because some in our field might be opposed to abortion? Or do we just assume everyone has the same political views in the field. Individuals can make their own choices as to whether to attend, but it's best when politics is left out of academics. FOCS 2022 is wrapping up today in Denver. Seems like a safe choice--perhaps all US conferences in the future should be in Colorado.&nbsp;</p><p>There are limits--I wouldn't attend or organize a conference in Russia in the near future. But if we start eliminating locations based on politics, we'll only be able to meet up in the metaverse, and we won't have social media to tell us how to get there.</p><p>By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Generally with some exceptions, I use <a href="https://facebook.com/fortnow">Facebook</a> for personal stuff, <a href="https://www.linkedin.com/in/fortnow/">LinkedIn</a> for Illinois Tech stuff and <a href="https://twitter.com/fortnow">Twitter</a>&nbsp;and this blog for CS stuff. Many of you got to this post through the <a href="https://twitter.com/fortnow/status/1588144625755328514">Twitter link</a>. Now that Elon Musk has bought the social media company, should I and the rest of the academic twitterverse move on to something else?</p><p>I'd say not yet. Let's see what Elon does to the place. Maybe he can allow more points of view, without turning it into a cesspool. Or maybe he ruins it. It'll be a network effect--if too many academics leave Twitter, I'd have to follow or I'd have few followers. I wonder where they will go. I hope it isn't TikTok.</p><p>On a similar vein, I often here of those who suggest we don't hold conferences in certain jurisdictions for political reasons, for example Texas, because of its laws against abortion and transgender rights. I don't believe computer science, as a field, should be making decisions based on politics. Academics who live in these states don't generally hold the same views as the political leaders in those states.</p><p>Should we not have meetings in Illinois because some in our field might be opposed to abortion? Or do we just assume everyone has the same political views in the field. Individuals can make their own choices as to whether to attend, but it's best when politics is left out of academics. <a href="https://focs2022.eecs.berkeley.edu/">FOCS 2022</a> is wrapping up today in Denver. Seems like a safe choice--perhaps all US conferences in the future should be in Colorado.&nbsp;</p><p>There are limits--I wouldn't attend or organize a conference in Russia in the near future. But if we start eliminating locations based on politics, we'll only be able to meet up in the metaverse, and we won't have social media to tell us how to get there.</p><p class="authors">By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-03T12:18:00Z">Thursday, November 03 2022, 12:18</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
