<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.6 (2023-03-30) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2023-06-24T18:34:38Z">Saturday, June 24 2023, 18:34</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://minorfree.github.io/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://minorfree.github.io'>Hung Le</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>SÃ©bastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Saturday, June 24
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://blog.computationalcomplexity.org/2023/06/can-you-put-n-pennies-on-n-x-n.html'>Can you put n pennies on an n x n chessboard so that all of the distances are distinct/how to look that up?</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>&nbsp;In Jan 2023 I went to the Joint Math Meeting of the AMS and the MAA and took notes on things to look up later. In one of the talks they discussed a problem and indicated that the answer was known, but did not give a reference or a proof. I emailed the authors and got no response. I tried to search the web but could not find it. SO I use this blog post to see if someone either knows the reference or can solve it outright, and either leave the answer in the comments, point to a paper that has the answer in the comments, or email me personally.&nbsp;</p><p>--------------------------------------------------------------------</p><p>A chessboard has squares that are 1 by 1.&nbsp;</p><p>Pennies have diameter 1.</p><p>QUESTION:&nbsp;</p><p>For which n is there a way to place n pennies on squares of the n x n chessboard so that all of the distances between centers of the pennies are DIFFERENT?</p><p>-----------------------------------------------------------</p><p>I have figured out that you CAN do this for n=3,4,5. I THINK the talk said&nbsp; it cannot be done for n=6. If&nbsp; you know or find a proof or disproof then please tell me. I am looking for human-readable proofs, not computer proofs.&nbsp; Similar for higher n.</p><p>I have a writeup of the n=3,4,5 cases&nbsp;here</p><p>----------------------------------------------------------------------</p><p>With technology and search engines it SHOULD be easier to find out answers to questions then it was in a prior era. And I think it is. But there are times when you are still better off asking&nbsp; someone, or in my case blog about it, to find the answer. Here is hoping it works!</p><p>ADDED LATER: Within 30 minutes of posting this one of my readers wrote a program and found tha tyou CAN do it for n=6 and gives the answer. Another commenter pointed to a website with the related quetion of putting as many pawns as you can on an 8x8 board.</p><p><br></p><p>By gasarch</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>&nbsp;In Jan 2023 I went to the Joint Math Meeting of the AMS and the MAA and took notes on things to look up later. In one of the talks they discussed a problem and indicated that the answer was known, but did not give a reference or a proof. I emailed the authors and got no response. I tried to search the web but could not find it. SO I use this blog post to see if someone either knows the reference or can solve it outright, and either leave the answer in the comments, point to a paper that has the answer in the comments, or email me personally.&nbsp;</p><p>--------------------------------------------------------------------</p><p>A chessboard has squares that are 1 by 1.&nbsp;</p><p>Pennies have diameter 1.</p><p>QUESTION:&nbsp;</p><p>For which n is there a way to place n pennies on squares of the n x n chessboard so that all of the distances between centers of the pennies are DIFFERENT?</p><p>-----------------------------------------------------------</p><p>I have figured out that you CAN do this for n=3,4,5. I THINK the talk said&nbsp; it cannot be done for n=6. If&nbsp; you know or find a proof or disproof then please tell me. I am looking for human-readable proofs, not computer proofs.&nbsp; Similar for higher n.</p><p>I have a writeup of the n=3,4,5 cases&nbsp;<a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/pennychess.pdf">here</a></p><p>----------------------------------------------------------------------</p><p>With technology and search engines it SHOULD be easier to find out answers to questions then it was in a prior era. And I think it is. But there are times when you are still better off asking&nbsp; someone, or in my case blog about it, to find the answer. Here is hoping it works!</p><p>ADDED LATER: Within 30 minutes of posting this one of my readers wrote a program and found tha tyou CAN do it for n=6 and gives the answer. Another commenter pointed to a website with the related quetion of putting as many pawns as you can on an 8x8 board.</p><p><br /></p><p class="authors">By gasarch</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-24T14:35:00Z">Saturday, June 24 2023, 14:35</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Friday, June 23
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12440'>Sleptsov Nets are Turing-complete</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Bernard Berthomieu, Dmitry A. Zaitsev</p><p>The present paper proves that a Sleptsov net (SN) is Turing-complete, that
considerably improves, with a brief construct, the previous result that a
strong SN is Turing-complete. Remind that, unlike Petri nets, an SN always
fires enabled transitions at their maximal firing multiplicity, as a single
step, leaving for a nondeterministic choice of which fireable transitions to
fire. A strong SN restricts nondeterministic choice to firing only the
transitions having the highest firing multiplicity.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Berthomieu_B/0/1/0/all/0/1">Bernard Berthomieu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaitsev_D/0/1/0/all/0/1">Dmitry A. Zaitsev</a></p><p>The present paper proves that a Sleptsov net (SN) is Turing-complete, that
considerably improves, with a brief construct, the previous result that a
strong SN is Turing-complete. Remind that, unlike Petri nets, an SN always
fires enabled transitions at their maximal firing multiplicity, as a single
step, leaving for a nondeterministic choice of which fireable transitions to
fire. A strong SN restricts nondeterministic choice to firing only the
transitions having the highest firing multiplicity.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13073'>Unitary Complexity and the Uhlmann Transformation Problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: John Bostanci, Yuval Efron, Tony Metger, Alexander Poremba, Luowen Qian, Henry Yuen</p><p>State transformation problems such as compressing quantum information or
breaking quantum commitments are fundamental quantum tasks. However, their
computational difficulty cannot easily be characterized using traditional
complexity theory, which focuses on tasks with classical inputs and outputs.
</p>
<p>To study the complexity of such state transformation tasks, we introduce a
framework for unitary synthesis problems, including notions of reductions and
unitary complexity classes. We use this framework to study the complexity of
transforming one entangled state into another via local operations. We
formalize this as the Uhlmann Transformation Problem, an algorithmic version of
Uhlmann's theorem. Then, we prove structural results relating the complexity of
the Uhlmann Transformation Problem, polynomial space quantum computation, and
zero knowledge protocols.
</p>
<p>The Uhlmann Transformation Problem allows us to characterize the complexity
of a variety of tasks in quantum information processing, including decoding
noisy quantum channels, breaking falsifiable quantum cryptographic assumptions,
implementing optimal prover strategies in quantum interactive proofs, and
decoding the Hawking radiation of black holes. Our framework for unitary
complexity thus provides new avenues for studying the computational complexity
of many natural quantum information processing tasks.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Bostanci_J/0/1/0/all/0/1">John Bostanci</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Efron_Y/0/1/0/all/0/1">Yuval Efron</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Metger_T/0/1/0/all/0/1">Tony Metger</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Poremba_A/0/1/0/all/0/1">Alexander Poremba</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Qian_L/0/1/0/all/0/1">Luowen Qian</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Yuen_H/0/1/0/all/0/1">Henry Yuen</a></p><p>State transformation problems such as compressing quantum information or
breaking quantum commitments are fundamental quantum tasks. However, their
computational difficulty cannot easily be characterized using traditional
complexity theory, which focuses on tasks with classical inputs and outputs.
</p>
<p>To study the complexity of such state transformation tasks, we introduce a
framework for unitary synthesis problems, including notions of reductions and
unitary complexity classes. We use this framework to study the complexity of
transforming one entangled state into another via local operations. We
formalize this as the Uhlmann Transformation Problem, an algorithmic version of
Uhlmann's theorem. Then, we prove structural results relating the complexity of
the Uhlmann Transformation Problem, polynomial space quantum computation, and
zero knowledge protocols.
</p>
<p>The Uhlmann Transformation Problem allows us to characterize the complexity
of a variety of tasks in quantum information processing, including decoding
noisy quantum channels, breaking falsifiable quantum cryptographic assumptions,
implementing optimal prover strategies in quantum interactive proofs, and
decoding the Hawking radiation of black holes. Our framework for unitary
complexity thus provides new avenues for studying the computational complexity
of many natural quantum information processing tasks.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12611'>Geometric Graphs with Unbounded Flip-Width</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: David Eppstein, Rose McCarty</p><p>We consider the flip-width of geometric graphs, a notion of graph width
recently introduced by Toru\'nczyk. We prove that many different types of
geometric graphs have unbounded flip-width. These include interval graphs,
permutation graphs, circle graphs, intersection graphs of axis-aligned line
segments or axis-aligned unit squares, unit distance graphs, unit disk graphs,
visibility graphs of simple polygons, $\beta$-skeletons, 4-polytopes, rectangle
of influence graphs, and 3d Delaunay triangulations.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Eppstein_D/0/1/0/all/0/1">David Eppstein</a>, <a href="http://arxiv.org/find/cs/1/au:+McCarty_R/0/1/0/all/0/1">Rose McCarty</a></p><p>We consider the flip-width of geometric graphs, a notion of graph width
recently introduced by Toru\'nczyk. We prove that many different types of
geometric graphs have unbounded flip-width. These include interval graphs,
permutation graphs, circle graphs, intersection graphs of axis-aligned line
segments or axis-aligned unit squares, unit distance graphs, unit disk graphs,
visibility graphs of simple polygons, $\beta$-skeletons, 4-polytopes, rectangle
of influence graphs, and 3d Delaunay triangulations.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12508'>Polynomial Logical Zonotopes: A Set Representation for Reachability Analysis of Logical Systems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Amr Alanwar, Frank J. Jiang, Karl H. Johansson</p><p>In this paper, we introduce a set representation called polynomial logical
zonotopes for performing exact and computationally efficient reachability
analysis on logical systems. Polynomial logical zonotopes are a generalization
of logical zonotopes, which are able to represent up to 2^n binary vectors
using only n generators. Due to their construction, logical zonotopes are only
able to support exact computations of some logical operations (XOR, NOT, XNOR),
while other operations (AND, NAND, OR, NOR) result in over-approximations. In
order to perform all fundamental logical operations exactly, we formulate a
generalization of logical zonotopes that is constructed by additional dependent
generators and exponent matrices. We prove that through this polynomial-like
construction, we are able to perform all of the fundamental logical operations
(XOR, NOT, XNOR, AND, NAND, OR, NOR) exactly. While we are able to perform all
of the logical operations exactly, this comes with a slight increase in
computational complexity compared to logical zonotopes. We show that we can use
polynomial logical zonotopes to perform exact reachability analysis while
retaining a low computational complexity. To illustrate and showcase the
computational benefits of polynomial logical zonotopes, we present the results
of performing reachability analysis on two use cases: (1) safety verification
of an intersection crossing protocol, (2) and reachability analysis on a
high-dimensional Boolean function. Moreover, to highlight the extensibility of
logical zonotopes, we include an additional use case where we perform a
computationally tractable exhaustive search for the key of a linear-feedback
shift register.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Alanwar_A/0/1/0/all/0/1">Amr Alanwar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1">Frank J. Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Johansson_K/0/1/0/all/0/1">Karl H. Johansson</a></p><p>In this paper, we introduce a set representation called polynomial logical
zonotopes for performing exact and computationally efficient reachability
analysis on logical systems. Polynomial logical zonotopes are a generalization
of logical zonotopes, which are able to represent up to 2^n binary vectors
using only n generators. Due to their construction, logical zonotopes are only
able to support exact computations of some logical operations (XOR, NOT, XNOR),
while other operations (AND, NAND, OR, NOR) result in over-approximations. In
order to perform all fundamental logical operations exactly, we formulate a
generalization of logical zonotopes that is constructed by additional dependent
generators and exponent matrices. We prove that through this polynomial-like
construction, we are able to perform all of the fundamental logical operations
(XOR, NOT, XNOR, AND, NAND, OR, NOR) exactly. While we are able to perform all
of the logical operations exactly, this comes with a slight increase in
computational complexity compared to logical zonotopes. We show that we can use
polynomial logical zonotopes to perform exact reachability analysis while
retaining a low computational complexity. To illustrate and showcase the
computational benefits of polynomial logical zonotopes, we present the results
of performing reachability analysis on two use cases: (1) safety verification
of an intersection crossing protocol, (2) and reachability analysis on a
high-dimensional Boolean function. Moreover, to highlight the extensibility of
logical zonotopes, we include an additional use case where we perform a
computationally tractable exhaustive search for the key of a linear-feedback
shift register.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12655'>Preprocessing Complexity for Some Graph Problems Parameterized by Structural Parameters</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Manuel Lafond, Weidong Luo</p><p>Structural graph parameters play an important role in parameterized
complexity, including in kernelization. Notably, vertex cover, neighborhood
diversity, twin-cover, and modular-width have been studied extensively in the
last few years. However, there are many fundamental problems whose
preprocessing complexity is not fully understood under these parameters.
Indeed, the existence of polynomial kernels or polynomial Turing kernels for
famous problems such as Clique, Chromatic Number, and Steiner Tree has only
been established for a subset of structural parameters. In this work, we use
several techniques to obtain a complete preprocessing complexity landscape for
over a dozen of fundamental algorithmic problems.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lafond_M/0/1/0/all/0/1">Manuel Lafond</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1">Weidong Luo</a></p><p>Structural graph parameters play an important role in parameterized
complexity, including in kernelization. Notably, vertex cover, neighborhood
diversity, twin-cover, and modular-width have been studied extensively in the
last few years. However, there are many fundamental problems whose
preprocessing complexity is not fully understood under these parameters.
Indeed, the existence of polynomial kernels or polynomial Turing kernels for
famous problems such as Clique, Chromatic Number, and Steiner Tree has only
been established for a subset of structural parameters. In this work, we use
several techniques to obtain a complete preprocessing complexity landscape for
over a dozen of fundamental algorithmic problems.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12534'>Memory-Query Tradeoffs for Randomized Convex Optimization</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xi Chen, Binghui Peng</p><p>We show that any randomized first-order algorithm which minimizes a
$d$-dimensional, $1$-Lipschitz convex function over the unit ball must either
use $\Omega(d^{2-\delta})$ bits of memory or make $\Omega(d^{1+\delta/6-o(1)})$
queries, for any constant $\delta\in (0,1)$ and when the precision $\epsilon$
is quasipolynomially small in $d$. Our result implies that cutting plane
methods, which use $\tilde{O}(d^2)$ bits of memory and $\tilde{O}(d)$ queries,
are Pareto-optimal among randomized first-order algorithms, and quadratic
memory is required to achieve optimal query complexity for convex optimization.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Binghui Peng</a></p><p>We show that any randomized first-order algorithm which minimizes a
$d$-dimensional, $1$-Lipschitz convex function over the unit ball must either
use $\Omega(d^{2-\delta})$ bits of memory or make $\Omega(d^{1+\delta/6-o(1)})$
queries, for any constant $\delta\in (0,1)$ and when the precision $\epsilon$
is quasipolynomially small in $d$. Our result implies that cutting plane
methods, which use $\tilde{O}(d^2)$ bits of memory and $\tilde{O}(d)$ queries,
are Pareto-optimal among randomized first-order algorithms, and quadratic
memory is required to achieve optimal query complexity for convex optimization.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12549'>On Differentially Private Sampling from Gaussian and Product Distributions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Badih Ghazi, Xiao Hu, Ravi Kumar, Pasin Manurangsi</p><p>Given a dataset of $n$ i.i.d. samples from an unknown distribution $P$, we
consider the problem of generating a sample from a distribution that is close
to $P$ in total variation distance, under the constraint of differential
privacy (DP). We study the problem when $P$ is a multi-dimensional Gaussian
distribution, under different assumptions on the information available to the
DP mechanism: known covariance, unknown bounded covariance, and unknown
unbounded covariance. We present new DP sampling algorithms, and show that they
achieve near-optimal sample complexity in the first two settings. Moreover,
when $P$ is a product distribution on the binary hypercube, we obtain a pure-DP
algorithm whereas only an approximate-DP algorithm (with slightly worse sample
complexity) was previously known.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ghazi_B/0/1/0/all/0/1">Badih Ghazi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1">Ravi Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Manurangsi_P/0/1/0/all/0/1">Pasin Manurangsi</a></p><p>Given a dataset of $n$ i.i.d. samples from an unknown distribution $P$, we
consider the problem of generating a sample from a distribution that is close
to $P$ in total variation distance, under the constraint of differential
privacy (DP). We study the problem when $P$ is a multi-dimensional Gaussian
distribution, under different assumptions on the information available to the
DP mechanism: known covariance, unknown bounded covariance, and unknown
unbounded covariance. We present new DP sampling algorithms, and show that they
achieve near-optimal sample complexity in the first two settings. Moreover,
when $P$ is a product distribution on the binary hypercube, we obtain a pure-DP
algorithm whereas only an approximate-DP algorithm (with slightly worse sample
complexity) was previously known.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12667'>The Power of Menus in Contract Design</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Guru Guruganesh, Jon Schneider, Joshua Wang, Junyao Zhao</p><p>We study the power of menus of contracts in principal-agent problems with
adverse selection (agents can be one of several types) and moral hazard (we
cannot observe agent actions directly). For principal-agent problems with $T$
types and $n$ actions, we show that the best menu of contracts can obtain a
factor $\Omega(\max(n, \log T))$ more utility for the principal than the best
individual contract, partially resolving an open question of Guruganesh et al.
(2021). We then turn our attention to randomized menus of linear contracts,
where we likewise show that randomized linear menus can be $\Omega(T)$ better
than the best single linear contract. As a corollary, we show this implies an
analogous gap between deterministic menus of (general) contracts and randomized
menus of contracts (as introduced by Castiglioni et al. (2022)).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Guruganesh_G/0/1/0/all/0/1">Guru Guruganesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1">Jon Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Joshua Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Junyao Zhao</a></p><p>We study the power of menus of contracts in principal-agent problems with
adverse selection (agents can be one of several types) and moral hazard (we
cannot observe agent actions directly). For principal-agent problems with $T$
types and $n$ actions, we show that the best menu of contracts can obtain a
factor $\Omega(\max(n, \log T))$ more utility for the principal than the best
individual contract, partially resolving an open question of Guruganesh et al.
(2021). We then turn our attention to randomized menus of linear contracts,
where we likewise show that randomized linear menus can be $\Omega(T)$ better
than the best single linear contract. As a corollary, we show this implies an
analogous gap between deterministic menus of (general) contracts and randomized
menus of contracts (as introduced by Castiglioni et al. (2022)).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12682'>Counting occurrences of patterns in permutations</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Andrew R Conway, Anthony J Guttmann</p><p>We develop a new, powerful method for counting elements in a {\em multiset.}
As a first application, we use this algorithm to study the number of
occurrences of patterns in a permutation. For patterns of length 3 there are
two Wilf classes, and the general behaviour of these is reasonably well-known.
We slightly extend some of the known results in that case, and exhaustively
study the case of patterns of length 4, about which there is little previous
knowledge. For such patterns, there are seven Wilf classes, and based on
extensive enumerations and careful series analysis, we have conjectured the
asymptotic behaviour for all classes.
</p>
<p>Finally, we investigate a proposal of Blitvi\'c and Steingr\'imsson as to the
range of a parameter for which a particular generating function formed from the
occurrence sequences is itself a Stieltjes moment sequence.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Conway_A/0/1/0/all/0/1">Andrew R Conway</a>, <a href="http://arxiv.org/find/math/1/au:+Guttmann_A/0/1/0/all/0/1">Anthony J Guttmann</a></p><p>We develop a new, powerful method for counting elements in a {\em multiset.}
As a first application, we use this algorithm to study the number of
occurrences of patterns in a permutation. For patterns of length 3 there are
two Wilf classes, and the general behaviour of these is reasonably well-known.
We slightly extend some of the known results in that case, and exhaustively
study the case of patterns of length 4, about which there is little previous
knowledge. For such patterns, there are seven Wilf classes, and based on
extensive enumerations and careful series analysis, we have conjectured the
asymptotic behaviour for all classes.
</p>
<p>Finally, we investigate a proposal of Blitvi\'c and Steingr\'imsson as to the
range of a parameter for which a particular generating function formed from the
occurrence sequences is itself a Stieltjes moment sequence.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12771'>Faster Compression of Deterministic Finite Automata</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Philip Bille, Inge Li G&#xf8;rtz, Max Rish&#xf8;j Pedersen</p><p>Deterministic finite automata (DFA) are a classic tool for high throughput
matching of regular expressions, both in theory and practice.
</p>
<p>Due to their high space consumption, extensive research has been devoted to
compressed representations of DFAs that still support efficient pattern
matching queries.
</p>
<p>Kumar~et~al.~[SIGCOMM 2006] introduced the \emph{delayed deterministic finite
automaton} (\ddfa{}) which exploits the large redundancy between inter-state
transitions in the automaton.
</p>
<p>They showed it to obtain up to two orders of magnitude compression of
real-world DFAs, and their work formed the basis of numerous subsequent
results.
</p>
<p>Their algorithm, as well as later algorithms based on their idea, have an
inherent quadratic-time bottleneck, as they consider every pair of states to
compute the optimal compression.
</p>
<p>In this work we present a simple, general framework based on
locality-sensitive hashing for speeding up these algorithms to achieve
sub-quadratic construction times for \ddfa{}s.
</p>
<p>We apply the framework to speed up several algorithms to near-linear time,
and experimentally evaluate their performance on real-world regular expression
sets extracted from modern intrusion detection systems.
</p>
<p>We find an order of magnitude improvement in compression times, with either
little or no loss of compression, or even significantly better compression in
some cases.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bille_P/0/1/0/all/0/1">Philip Bille</a>, <a href="http://arxiv.org/find/cs/1/au:+Gortz_I/0/1/0/all/0/1">Inge Li G&#xf8;rtz</a>, <a href="http://arxiv.org/find/cs/1/au:+Pedersen_M/0/1/0/all/0/1">Max Rish&#xf8;j Pedersen</a></p><p>Deterministic finite automata (DFA) are a classic tool for high throughput
matching of regular expressions, both in theory and practice.
</p>
<p>Due to their high space consumption, extensive research has been devoted to
compressed representations of DFAs that still support efficient pattern
matching queries.
</p>
<p>Kumar~et~al.~[SIGCOMM 2006] introduced the \emph{delayed deterministic finite
automaton} (\ddfa{}) which exploits the large redundancy between inter-state
transitions in the automaton.
</p>
<p>They showed it to obtain up to two orders of magnitude compression of
real-world DFAs, and their work formed the basis of numerous subsequent
results.
</p>
<p>Their algorithm, as well as later algorithms based on their idea, have an
inherent quadratic-time bottleneck, as they consider every pair of states to
compute the optimal compression.
</p>
<p>In this work we present a simple, general framework based on
locality-sensitive hashing for speeding up these algorithms to achieve
sub-quadratic construction times for \ddfa{}s.
</p>
<p>We apply the framework to speed up several algorithms to near-linear time,
and experimentally evaluate their performance on real-world regular expression
sets extracted from modern intrusion detection systems.
</p>
<p>We find an order of magnitude improvement in compression times, with either
little or no loss of compression, or even significantly better compression in
some cases.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12934'>On boundedness of zeros of the independence polynomial of tor</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: David de Boer, Pjotr Buys, Han Peters, Guus Regts</p><p>We study boundedness of zeros of the independence polynomial of tori for
sequences of tori converging to the integer lattice. We prove that zeros are
bounded for sequences of balanced tori, but unbounded for sequences of highly
unbalanced tori. Here balanced means that the size of the torus is at most
exponential in the shortest side length, while highly unbalanced means that the
longest side length of the torus is super exponential in the product over the
other side lengths cubed. We discuss implications of our results to the
existence of efficient algorithms for approximating the independence polynomial
on tori.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Boer_D/0/1/0/all/0/1">David de Boer</a>, <a href="http://arxiv.org/find/math/1/au:+Buys_P/0/1/0/all/0/1">Pjotr Buys</a>, <a href="http://arxiv.org/find/math/1/au:+Peters_H/0/1/0/all/0/1">Han Peters</a>, <a href="http://arxiv.org/find/math/1/au:+Regts_G/0/1/0/all/0/1">Guus Regts</a></p><p>We study boundedness of zeros of the independence polynomial of tori for
sequences of tori converging to the integer lattice. We prove that zeros are
bounded for sequences of balanced tori, but unbounded for sequences of highly
unbalanced tori. Here balanced means that the size of the torus is at most
exponential in the shortest side length, while highly unbalanced means that the
longest side length of the torus is super exponential in the product over the
other side lengths cubed. We discuss implications of our results to the
existence of efficient algorithms for approximating the independence polynomial
on tori.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13057'>SQ Lower Bounds for Learning Bounded Covariance GMMs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, Nikos Zarifis</p><p>We study the complexity of learning mixtures of separated Gaussians with
common unknown bounded covariance matrix. Specifically, we focus on learning
Gaussian mixture models (GMMs) on $\mathbb{R}^d$ of the form $P= \sum_{i=1}^k
w_i \mathcal{N}(\boldsymbol \mu_i,\mathbf \Sigma_i)$, where $\mathbf \Sigma_i =
\mathbf \Sigma \preceq \mathbf I$ and $\min_{i \neq j} \| \boldsymbol \mu_i -
\boldsymbol \mu_j\|_2 \geq k^\epsilon$ for some $\epsilon&gt;0$. Known learning
algorithms for this family of GMMs have complexity $(dk)^{O(1/\epsilon)}$. In
this work, we prove that any Statistical Query (SQ) algorithm for this problem
requires complexity at least $d^{\Omega(1/\epsilon)}$. In the special case
where the separation is on the order of $k^{1/2}$, we additionally obtain
fine-grained SQ lower bounds with the correct exponent. Our SQ lower bounds
imply similar lower bounds for low-degree polynomial tests. Conceptually, our
results provide evidence that known algorithms for this problem are nearly best
possible.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1">Ilias Diakonikolas</a>, <a href="http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1">Daniel M. Kane</a>, <a href="http://arxiv.org/find/cs/1/au:+Pittas_T/0/1/0/all/0/1">Thanasis Pittas</a>, <a href="http://arxiv.org/find/cs/1/au:+Zarifis_N/0/1/0/all/0/1">Nikos Zarifis</a></p><p>We study the complexity of learning mixtures of separated Gaussians with
common unknown bounded covariance matrix. Specifically, we focus on learning
Gaussian mixture models (GMMs) on $\mathbb{R}^d$ of the form $P= \sum_{i=1}^k
w_i \mathcal{N}(\boldsymbol \mu_i,\mathbf \Sigma_i)$, where $\mathbf \Sigma_i =
\mathbf \Sigma \preceq \mathbf I$ and $\min_{i \neq j} \| \boldsymbol \mu_i -
\boldsymbol \mu_j\|_2 \geq k^\epsilon$ for some $\epsilon&gt;0$. Known learning
algorithms for this family of GMMs have complexity $(dk)^{O(1/\epsilon)}$. In
this work, we prove that any Statistical Query (SQ) algorithm for this problem
requires complexity at least $d^{\Omega(1/\epsilon)}$. In the special case
where the separation is on the order of $k^{1/2}$, we additionally obtain
fine-grained SQ lower bounds with the correct exponent. Our SQ lower bounds
imply similar lower bounds for low-degree polynomial tests. Conceptually, our
results provide evidence that known algorithms for this problem are nearly best
possible.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Thursday, June 22
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://blog.computationalcomplexity.org/2023/06/dont-negotiate-with-logic.html'>Don't Negotiate with Logic</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Computer science and mathematicians often try to use logic to negotiate whether it be at a university or life in general. I've tried it myself and it doesn't usually work. Even if you have that (rare) perfect argument, remember Upton Sinclair's words:&nbsp;It is difficult to get a man to understand something, when his salary depends on his not understanding it.</p><p>So make sure their salary depends on them understanding it. Or more to the point, in a world with limited resources, why it makes sense for them to help you.&nbsp;</p><ol><li>Ideally go for the win-win. Why a certain decision helps the department/college/university as well as yourself. Asking for a small investment as a seed towards a large grant for example.</li><li>How would the decision make you or your students more successful? The success of a department is measured by the success of the faculty and students. On the other hand, why would a different decision hold you and your students back.</li></ol>Even outside the university, make your objectives in line with the objectives of the person you are negotiating with to lead to a better outcome.<br>Of course sometimes you are haggling over a price or a salary when it really is a zero-sum game. There it's good to know the BATNA, Best Alternative To a Negotiated Agreement, for yourself and the other entity. In other words, if they aren't selling to you what other options do you and they have?<br>There are whole books written about negotiating strategies. Mostly it comes down to making it work for both parties. That's what matters, not the logic.<p></p><p>By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Computer science and mathematicians often try to use logic to negotiate whether it be at a university or life in general. I've tried it myself and it doesn't usually work. Even if you have that (rare) perfect argument, remember <a href="https://www.goodreads.com/quotes/21810-it-is-difficult-to-get-a-man-to-understand-something">Upton Sinclair's words</a>:&nbsp;<i>It is difficult to get a man to understand something, when his salary depends on his not understanding it</i>.</p><p>So make sure their salary depends on them understanding it. Or more to the point, in a world with limited resources, why it makes sense for them to help you.&nbsp;</p><ol style="text-align: left;"><li>Ideally go for the win-win. Why a certain decision helps the department/college/university as well as yourself. Asking for a small investment as a seed towards a large grant for example.</li><li>How would the decision make you or your students more successful? The success of a department is measured by the success of the faculty and students. On the other hand, why would a different decision hold you and your students back.</li></ol><div>Even outside the university, make your objectives in line with the objectives of the person you are negotiating with to lead to a better outcome.</div><div><br /></div><div>Of course sometimes you are haggling over a price or a salary when it really is a zero-sum game. There it's good to know the <a href="https://www.investopedia.com/terms/b/best-alternative-to-a-negotiated-agreement-batna.asp">BATNA</a>, Best Alternative To a Negotiated Agreement, for yourself and the other entity. In other words, if they aren't selling to you what other options do you and they have?</div><div><br /></div><div>There are whole books written about negotiating strategies. Mostly it comes down to making it work for both parties. That's what matters, not the logic.</div><p></p><p class="authors">By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T17:15:00Z">Thursday, June 22 2023, 17:15</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12103'>Quantum and classical query complexities for determining connectedness of matroids</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xiaowei Huang, Shiguang Feng, Lvzhou Li</p><p>Connectivity is a fundamental structural property of matroids, and has been
studied algorithmically over 50 years. In 1974, Cunningham proposed a
deterministic algorithm consuming $O(n^{2})$ queries to the independence oracle
to determine whether a matroid is connected. Since then, no algorithm, not even
a random one, has worked better. To the best of our knowledge, the classical
query complexity lower bound and the quantum complexity for this problem have
not been considered. Thus, in this paper we are devoted to addressing these
issues, and our contributions are threefold as follows: (i) First, we prove
that the randomized query complexity of determining whether a matroid is
connected is $\Omega(n^2)$ and thus the algorithm proposed by Cunningham is
optimal in classical computing. (ii) Second, we present a quantum algorithm
with $O(n^{3/2})$ queries, which exhibits provable quantum speedups over
classical ones. (iii) Third, we prove that any quantum algorithm requires
$\Omega(n)$ queries, which indicates that quantum algorithms can achieve at
most a quadratic speedup over classical ones. Therefore, we have a relatively
comprehensive understanding of the potential of quantum computing in
determining the connectedness of matroids.\
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Huang_X/0/1/0/all/0/1">Xiaowei Huang</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Feng_S/0/1/0/all/0/1">Shiguang Feng</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Li_L/0/1/0/all/0/1">Lvzhou Li</a></p><p>Connectivity is a fundamental structural property of matroids, and has been
studied algorithmically over 50 years. In 1974, Cunningham proposed a
deterministic algorithm consuming $O(n^{2})$ queries to the independence oracle
to determine whether a matroid is connected. Since then, no algorithm, not even
a random one, has worked better. To the best of our knowledge, the classical
query complexity lower bound and the quantum complexity for this problem have
not been considered. Thus, in this paper we are devoted to addressing these
issues, and our contributions are threefold as follows: (i) First, we prove
that the randomized query complexity of determining whether a matroid is
connected is $\Omega(n^2)$ and thus the algorithm proposed by Cunningham is
optimal in classical computing. (ii) Second, we present a quantum algorithm
with $O(n^{3/2})$ queries, which exhibits provable quantum speedups over
classical ones. (iii) Third, we prove that any quantum algorithm requires
$\Omega(n)$ queries, which indicates that quantum algorithms can achieve at
most a quadratic speedup over classical ones. Therefore, we have a relatively
comprehensive understanding of the potential of quantum computing in
determining the connectedness of matroids.\
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12377'>Geometric Algorithms for $k$-NN Poisoning</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Diego Ihara Centurion, Karine Chubarian, Bohan Fan, Francesco Sgherzi, Thiruvenkadam S Radhakrishnan, Anastasios Sidiropoulos, Angelo Straight</p><p>We propose a label poisoning attack on geometric data sets against
$k$-nearest neighbor classification. We provide an algorithm that can compute
an $\varepsilon n$-additive approximation of the optimal poisoning in $n\cdot
2^{2^{O(d+k/\varepsilon)}}$ time for a given data set $X \in \mathbb{R}^d$,
where $|X| = n$. Our algorithm achieves its objectives through the application
of multi-scale random partitions.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Centurion_D/0/1/0/all/0/1">Diego Ihara Centurion</a>, <a href="http://arxiv.org/find/cs/1/au:+Chubarian_K/0/1/0/all/0/1">Karine Chubarian</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1">Bohan Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sgherzi_F/0/1/0/all/0/1">Francesco Sgherzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_T/0/1/0/all/0/1">Thiruvenkadam S Radhakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sidiropoulos_A/0/1/0/all/0/1">Anastasios Sidiropoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Straight_A/0/1/0/all/0/1">Angelo Straight</a></p><p>We propose a label poisoning attack on geometric data sets against
$k$-nearest neighbor classification. We provide an algorithm that can compute
an $\varepsilon n$-additive approximation of the optimal poisoning in $n\cdot
2^{2^{O(d+k/\varepsilon)}}$ time for a given data set $X \in \mathbb{R}^d$,
where $|X| = n$. Our algorithm achieves its objectives through the application
of multi-scale random partitions.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.11802'>Fast quantum algorithm for differential equations</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mohsen Bagherimehrab, Kouhei Nakaji, Nathan Wiebe, Al&#xe1;n Aspuru-Guzik</p><p>Partial differential equations (PDEs) are ubiquitous in science and
engineering. Prior quantum algorithms for solving the system of linear
algebraic equations obtained from discretizing a PDE have a computational
complexity that scales at least linearly with the condition number $\kappa$ of
the matrices involved in the computation. For many practical applications,
$\kappa$ scales polynomially with the size $N$ of the matrices, rendering a
polynomial-in-$N$ complexity for these algorithms. Here we present a quantum
algorithm with a complexity that is polylogarithmic in $N$ but is independent
of $\kappa$ for a large class of PDEs. Our algorithm generates a quantum state
that enables extracting features of the solution. Central to our methodology is
using a wavelet basis as an auxiliary system of coordinates in which the
condition number of associated matrices is independent of $N$ by a simple
diagonal preconditioner. We present numerical simulations showing the effect of
the wavelet preconditioner for several differential equations. Our work could
provide a practical way to boost the performance of quantum-simulation
algorithms where standard methods are used for discretization.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Bagherimehrab_M/0/1/0/all/0/1">Mohsen Bagherimehrab</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Nakaji_K/0/1/0/all/0/1">Kouhei Nakaji</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Wiebe_N/0/1/0/all/0/1">Nathan Wiebe</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Aspuru_Guzik_A/0/1/0/all/0/1">Al&#xe1;n Aspuru-Guzik</a></p><p>Partial differential equations (PDEs) are ubiquitous in science and
engineering. Prior quantum algorithms for solving the system of linear
algebraic equations obtained from discretizing a PDE have a computational
complexity that scales at least linearly with the condition number $\kappa$ of
the matrices involved in the computation. For many practical applications,
$\kappa$ scales polynomially with the size $N$ of the matrices, rendering a
polynomial-in-$N$ complexity for these algorithms. Here we present a quantum
algorithm with a complexity that is polylogarithmic in $N$ but is independent
of $\kappa$ for a large class of PDEs. Our algorithm generates a quantum state
that enables extracting features of the solution. Central to our methodology is
using a wavelet basis as an auxiliary system of coordinates in which the
condition number of associated matrices is independent of $N$ by a simple
diagonal preconditioner. We present numerical simulations showing the effect of
the wavelet preconditioner for several differential equations. Our work could
provide a practical way to boost the performance of quantum-simulation
algorithms where standard methods are used for discretization.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.11939'>A Parameterized Algorithm for Flat Folding</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: David Eppstein</p><p>We prove that testing the flat foldability of an origami crease pattern
(either labeled with mountain and valley folds, or unlabeled) is
fixed-parameter tractable when parameterized by the ply of the flat-folded
state and by the treewidth of an associated planar graph, the cell adjacency
graph of an arrangement of polygons formed by the flat-folded state. For flat
foldings of bounded ply, our algorithm is single-exponential in the treewidth;
this dependence on treewidth is necessary under the exponential time
hypothesis.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Eppstein_D/0/1/0/all/0/1">David Eppstein</a></p><p>We prove that testing the flat foldability of an origami crease pattern
(either labeled with mountain and valley folds, or unlabeled) is
fixed-parameter tractable when parameterized by the ply of the flat-folded
state and by the treewidth of an associated planar graph, the cell adjacency
graph of an arrangement of polygons formed by the flat-folded state. For flat
foldings of bounded ply, our algorithm is single-exponential in the treewidth;
this dependence on treewidth is necessary under the exponential time
hypothesis.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.11828'>Near-Optimal Dynamic Rounding of Fractional Matchings in Bipartite Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sayan Bhattacharya, Peter Kiss, Aaron Sidford, David Wajc</p><p>We study dynamic $(1-\epsilon)$-approximate rounding of fractional matchings
-- a key ingredient in numerous breakthroughs in the dynamic graph algorithms
literature. Our first contribution is a surprisingly simple deterministic
rounding algorithm in bipartite graphs with amortized update time
$O(\epsilon^{-1} \log^2 (\epsilon^{-1} \cdot n))$, matching an (unconditional)
recourse lower bound of $\Omega(\epsilon^{-1})$ up to logarithmic factors.
Moreover, this algorithm's update time improves provided the minimum (non-zero)
weight in the fractional matching is lower bounded throughout. Combining this
algorithm with novel dynamic \emph{partial rounding} algorithms to increase
this minimum weight, we obtain several algorithms that improve this dependence
on $n$. For example, we give a high-probability randomized algorithm with
$\tilde{O}(\epsilon^{-1}\cdot (\log\log n)^2)$-update time against adaptive
adversaries. (We use Soft-Oh notation, $\tilde{O}$, to suppress polylogarithmic
factors in the argument, i.e., $\tilde{O}(f)=O(f\cdot \mathrm{poly}(\log f))$.)
Using our rounding algorithms, we also round known $(1-\epsilon)$-decremental
fractional bipartite matching algorithms with no asymptotic overhead, thus
improving on state-of-the-art algorithms for the decremental bipartite matching
problem. Further, we provide extensions of our results to general graphs and to
maintaining almost-maximal matchings.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1">Sayan Bhattacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Kiss_P/0/1/0/all/0/1">Peter Kiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Sidford_A/0/1/0/all/0/1">Aaron Sidford</a>, <a href="http://arxiv.org/find/cs/1/au:+Wajc_D/0/1/0/all/0/1">David Wajc</a></p><p>We study dynamic $(1-\epsilon)$-approximate rounding of fractional matchings
-- a key ingredient in numerous breakthroughs in the dynamic graph algorithms
literature. Our first contribution is a surprisingly simple deterministic
rounding algorithm in bipartite graphs with amortized update time
$O(\epsilon^{-1} \log^2 (\epsilon^{-1} \cdot n))$, matching an (unconditional)
recourse lower bound of $\Omega(\epsilon^{-1})$ up to logarithmic factors.
Moreover, this algorithm's update time improves provided the minimum (non-zero)
weight in the fractional matching is lower bounded throughout. Combining this
algorithm with novel dynamic \emph{partial rounding} algorithms to increase
this minimum weight, we obtain several algorithms that improve this dependence
on $n$. For example, we give a high-probability randomized algorithm with
$\tilde{O}(\epsilon^{-1}\cdot (\log\log n)^2)$-update time against adaptive
adversaries. (We use Soft-Oh notation, $\tilde{O}$, to suppress polylogarithmic
factors in the argument, i.e., $\tilde{O}(f)=O(f\cdot \mathrm{poly}(\log f))$.)
Using our rounding algorithms, we also round known $(1-\epsilon)$-decremental
fractional bipartite matching algorithms with no asymptotic overhead, thus
improving on state-of-the-art algorithms for the decremental bipartite matching
problem. Further, we provide extensions of our results to general graphs and to
maintaining almost-maximal matchings.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.11951'>On the Optimal Bounds for Noisy Computing</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Banghua Zhu, Ziao Wang, Nadim Ghaddar, Jiantao Jiao, Lele Wang</p><p>We revisit the problem of computing with noisy information considered in
Feige et al. 1994, which includes computing the OR function from noisy queries,
and computing the MAX, SEARCH and SORT functions from noisy pairwise
comparisons. For $K$ given elements, the goal is to correctly recover the
desired function with probability at least $1-\delta$ when the outcome of each
query is flipped with probability $p$. We consider both the adaptive sampling
setting where each query can be adaptively designed based on past outcomes, and
the non-adaptive sampling setting where the query cannot depend on past
outcomes. The prior work provides tight bounds on the worst-case query
complexity in terms of the dependence on $K$. However, the upper and lower
bounds do not match in terms of the dependence on $\delta$ and $p$. We improve
the lower bounds for all the four functions under both adaptive and
non-adaptive query models. Most of our lower bounds match the upper bounds up
to constant factors when either $p$ or $\delta$ is bounded away from $0$, while
the ratio between the best prior upper and lower bounds goes to infinity when
$p\rightarrow 0$ or $p\rightarrow 1/2$. On the other hand, we also provide
matching upper and lower bounds for the number of queries in expectation,
improving both the upper and lower bounds for the variable-length query model.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Banghua Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghaddar_N/0/1/0/all/0/1">Nadim Ghaddar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1">Jiantao Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lele Wang</a></p><p>We revisit the problem of computing with noisy information considered in
Feige et al. 1994, which includes computing the OR function from noisy queries,
and computing the MAX, SEARCH and SORT functions from noisy pairwise
comparisons. For $K$ given elements, the goal is to correctly recover the
desired function with probability at least $1-\delta$ when the outcome of each
query is flipped with probability $p$. We consider both the adaptive sampling
setting where each query can be adaptively designed based on past outcomes, and
the non-adaptive sampling setting where the query cannot depend on past
outcomes. The prior work provides tight bounds on the worst-case query
complexity in terms of the dependence on $K$. However, the upper and lower
bounds do not match in terms of the dependence on $\delta$ and $p$. We improve
the lower bounds for all the four functions under both adaptive and
non-adaptive query models. Most of our lower bounds match the upper bounds up
to constant factors when either $p$ or $\delta$ is bounded away from $0$, while
the ratio between the best prior upper and lower bounds goes to infinity when
$p\rightarrow 0$ or $p\rightarrow 1/2$. On the other hand, we also provide
matching upper and lower bounds for the number of queries in expectation,
improving both the upper and lower bounds for the variable-length query model.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.11964'>Sampling Individually-Fair Rankings that are Always Group Fair</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sruthi Gorantla, Anay Mehrotra, Amit Deshpande, Anand Louis</p><p>Rankings on online platforms help their end-users find the relevant
information -- people, news, media, and products -- quickly. Fair ranking
tasks, which ask to rank a set of items to maximize utility subject to
satisfying group-fairness constraints, have gained significant interest in the
Algorithmic Fairness, Information Retrieval, and Machine Learning literature.
Recent works, however, identify uncertainty in the utilities of items as a
primary cause of unfairness and propose introducing randomness in the output.
This randomness is carefully chosen to guarantee an adequate representation of
each item (while accounting for the uncertainty). However, due to this
randomness, the output rankings may violate group fairness constraints. We give
an efficient algorithm that samples rankings from an individually-fair
distribution while ensuring that every output ranking is group fair. The
expected utility of the output ranking is at least $\alpha$ times the utility
of the optimal fair solution. Here, $\alpha$ depends on the utilities,
position-discounts, and constraints -- it approaches 1 as the range of
utilities or the position-discounts shrinks, or when utilities satisfy
distributional assumptions. Empirically, we observe that our algorithm achieves
individual and group fairness and that Pareto dominates the state-of-the-art
baselines.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gorantla_S/0/1/0/all/0/1">Sruthi Gorantla</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehrotra_A/0/1/0/all/0/1">Anay Mehrotra</a>, <a href="http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1">Amit Deshpande</a>, <a href="http://arxiv.org/find/cs/1/au:+Louis_A/0/1/0/all/0/1">Anand Louis</a></p><p>Rankings on online platforms help their end-users find the relevant
information -- people, news, media, and products -- quickly. Fair ranking
tasks, which ask to rank a set of items to maximize utility subject to
satisfying group-fairness constraints, have gained significant interest in the
Algorithmic Fairness, Information Retrieval, and Machine Learning literature.
Recent works, however, identify uncertainty in the utilities of items as a
primary cause of unfairness and propose introducing randomness in the output.
This randomness is carefully chosen to guarantee an adequate representation of
each item (while accounting for the uncertainty). However, due to this
randomness, the output rankings may violate group fairness constraints. We give
an efficient algorithm that samples rankings from an individually-fair
distribution while ensuring that every output ranking is group fair. The
expected utility of the output ranking is at least $\alpha$ times the utility
of the optimal fair solution. Here, $\alpha$ depends on the utilities,
position-discounts, and constraints -- it approaches 1 as the range of
utilities or the position-discounts shrinks, or when utilities satisfy
distributional assumptions. Empirically, we observe that our algorithm achieves
individual and group fairness and that Pareto dominates the state-of-the-art
baselines.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12042'>Block-Wise Index Modulation and Receiver Design for High-Mobility OTFS Communications</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mi Qian, Fei Ji, Yao Ge, Miaowen Wen, Xiang Cheng, H. Vincent Poor</p><p>As a promising technique for high-mobility wireless communications,
orthogonal time frequency space (OTFS) has been proved to enjoy excellent
advantages with respect to traditional orthogonal frequency division
multiplexing (OFDM). Although multiple studies have considered index modulation
(IM) based OTFS (IM-OTFS) schemes to further improve system performance, a
challenging and open problem is the development of effective IM schemes and
efficient receivers for practical OTFS systems that must operate in the
presence of channel delays and Doppler shifts. In this paper, we propose two
novel block-wise IM schemes for OTFS systems, named delay-IM with OTFS
(DeIM-OTFS) and Doppler-IM with OTFS (DoIM-OTFS), where a block of
delay/Doppler resource bins are activated simultaneously. Based on a maximum
likelihood (ML) detector, we analyze upper bounds on the average bit error
rates for the proposed DeIM-OTFS and DoIM-OTFS schemes, and verify their
performance advantages over the existing IM-OTFS systems. We also develop a
multi-layer joint symbol and activation pattern detection (MLJSAPD) algorithm
and a customized message passing detection (CMPD) algorithm for our proposed
DeIMOTFS and DoIM-OTFS systems with low complexity. Simulation results
demonstrate that our proposed MLJSAPD and CMPD algorithms can achieve desired
performance with robustness to the imperfect channel state information (CSI).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Qian_M/0/1/0/all/0/1">Mi Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_F/0/1/0/all/0/1">Fei Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1">Miaowen Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xiang Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1">H. Vincent Poor</a></p><p>As a promising technique for high-mobility wireless communications,
orthogonal time frequency space (OTFS) has been proved to enjoy excellent
advantages with respect to traditional orthogonal frequency division
multiplexing (OFDM). Although multiple studies have considered index modulation
(IM) based OTFS (IM-OTFS) schemes to further improve system performance, a
challenging and open problem is the development of effective IM schemes and
efficient receivers for practical OTFS systems that must operate in the
presence of channel delays and Doppler shifts. In this paper, we propose two
novel block-wise IM schemes for OTFS systems, named delay-IM with OTFS
(DeIM-OTFS) and Doppler-IM with OTFS (DoIM-OTFS), where a block of
delay/Doppler resource bins are activated simultaneously. Based on a maximum
likelihood (ML) detector, we analyze upper bounds on the average bit error
rates for the proposed DeIM-OTFS and DoIM-OTFS schemes, and verify their
performance advantages over the existing IM-OTFS systems. We also develop a
multi-layer joint symbol and activation pattern detection (MLJSAPD) algorithm
and a customized message passing detection (CMPD) algorithm for our proposed
DeIMOTFS and DoIM-OTFS systems with low complexity. Simulation results
demonstrate that our proposed MLJSAPD and CMPD algorithms can achieve desired
performance with robustness to the imperfect channel state information (CSI).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12071'>Optimal (degree+1)-Coloring in Congested Clique</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sam Coy, Artur Czumaj, Peter Davies, Gopinath Mishra</p><p>We consider the distributed complexity of the (degree+1)-list coloring
problem, in which each node $u$ of degree $d(u)$ is assigned a palette of
$d(u)+1$ colors, and the goal is to find a proper coloring using these color
palettes. The (degree+1)-list coloring problem is a natural generalization of
the classical $(\Delta+1)$-coloring and $(\Delta+1)$-list coloring problems,
both being benchmark problems extensively studied in distributed and parallel
computing. In this paper we settle the complexity of the (degree+1)-list
coloring problem in the Congested Clique model by showing that it can be solved
deterministically in a constant number of rounds.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Coy_S/0/1/0/all/0/1">Sam Coy</a>, <a href="http://arxiv.org/find/cs/1/au:+Czumaj_A/0/1/0/all/0/1">Artur Czumaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Davies_P/0/1/0/all/0/1">Peter Davies</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1">Gopinath Mishra</a></p><p>We consider the distributed complexity of the (degree+1)-list coloring
problem, in which each node $u$ of degree $d(u)$ is assigned a palette of
$d(u)+1$ colors, and the goal is to find a proper coloring using these color
palettes. The (degree+1)-list coloring problem is a natural generalization of
the classical $(\Delta+1)$-coloring and $(\Delta+1)$-list coloring problems,
both being benchmark problems extensively studied in distributed and parallel
computing. In this paper we settle the complexity of the (degree+1)-list
coloring problem in the Congested Clique model by showing that it can be solved
deterministically in a constant number of rounds.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12282'>Online Resource Allocation with Convex-set Machine-Learned Advice</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Negin Golrezaei, Patrick Jaillet, Zijie Zhou</p><p>Decision-makers often have access to a machine-learned prediction about
demand, referred to as advice, which can potentially be utilized in online
decision-making processes for resource allocation. However, exploiting such
advice poses challenges due to its potential inaccuracy. To address this issue,
we propose a framework that enhances online resource allocation decisions with
potentially unreliable machine-learned (ML) advice. We assume here that this
advice is represented by a general convex uncertainty set for the demand
vector.
</p>
<p>We introduce a parameterized class of Pareto optimal online resource
allocation algorithms that strike a balance between consistent and robust
ratios. The consistent ratio measures the algorithm's performance (compared to
the optimal hindsight solution) when the ML advice is accurate, while the
robust ratio captures performance under an adversarial demand process when the
advice is inaccurate. Specifically, in a C-Pareto optimal setting, we maximize
the robust ratio while ensuring that the consistent ratio is at least C. Our
proposed C-Pareto optimal algorithm is an adaptive protection level algorithm,
which extends the classical fixed protection level algorithm introduced in
Littlewood (2005) and Ball and Queyranne (2009). Solving a complex non-convex
continuous optimization problem characterizes the adaptive protection level
algorithm. To complement our algorithms, we present a simple method for
computing the maximum achievable consistent ratio, which serves as an estimate
for the maximum value of the ML advice. Additionally, we present numerical
studies to evaluate the performance of our algorithm in comparison to benchmark
algorithms. The results demonstrate that by adjusting the parameter C, our
algorithms effectively strike a balance between worst-case and average
performance, outperforming the benchmark algorithms.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Golrezaei_N/0/1/0/all/0/1">Negin Golrezaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaillet_P/0/1/0/all/0/1">Patrick Jaillet</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zijie Zhou</a></p><p>Decision-makers often have access to a machine-learned prediction about
demand, referred to as advice, which can potentially be utilized in online
decision-making processes for resource allocation. However, exploiting such
advice poses challenges due to its potential inaccuracy. To address this issue,
we propose a framework that enhances online resource allocation decisions with
potentially unreliable machine-learned (ML) advice. We assume here that this
advice is represented by a general convex uncertainty set for the demand
vector.
</p>
<p>We introduce a parameterized class of Pareto optimal online resource
allocation algorithms that strike a balance between consistent and robust
ratios. The consistent ratio measures the algorithm's performance (compared to
the optimal hindsight solution) when the ML advice is accurate, while the
robust ratio captures performance under an adversarial demand process when the
advice is inaccurate. Specifically, in a C-Pareto optimal setting, we maximize
the robust ratio while ensuring that the consistent ratio is at least C. Our
proposed C-Pareto optimal algorithm is an adaptive protection level algorithm,
which extends the classical fixed protection level algorithm introduced in
Littlewood (2005) and Ball and Queyranne (2009). Solving a complex non-convex
continuous optimization problem characterizes the adaptive protection level
algorithm. To complement our algorithms, we present a simple method for
computing the maximum achievable consistent ratio, which serves as an estimate
for the maximum value of the ML advice. Additionally, we present numerical
studies to evaluate the performance of our algorithm in comparison to benchmark
algorithms. The results demonstrate that by adjusting the parameter C, our
algorithms effectively strike a balance between worst-case and average
performance, outperforming the benchmark algorithms.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12344'>An efficient, provably exact algorithm for the 0-1 loss linear classification problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xi He, Max A. Little</p><p>Algorithms for solving the linear classification problem have a long history,
dating back at least to 1936 with linear discriminant analysis. For linearly
separable data, many algorithms can obtain the exact solution to the
corresponding 0-1 loss classification problem efficiently, but for data which
is not linearly separable, it has been shown that this problem, in full
generality, is NP-hard. Alternative approaches all involve approximations of
some kind, including the use of surrogates for the 0-1 loss (for example, the
hinge or logistic loss) or approximate combinatorial search, none of which can
be guaranteed to solve the problem exactly. Finding efficient algorithms to
obtain an exact i.e. globally optimal solution for the 0-1 loss linear
classification problem with fixed dimension, remains an open problem. In
research we report here, we detail the construction of a new algorithm,
incremental cell enumeration (ICE), that can solve the 0-1 loss classification
problem exactly in polynomial time. To our knowledge, this is the first,
rigorously-proven polynomial time algorithm for this long-standing problem.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xi He</a>, <a href="http://arxiv.org/find/cs/1/au:+Little_M/0/1/0/all/0/1">Max A. Little</a></p><p>Algorithms for solving the linear classification problem have a long history,
dating back at least to 1936 with linear discriminant analysis. For linearly
separable data, many algorithms can obtain the exact solution to the
corresponding 0-1 loss classification problem efficiently, but for data which
is not linearly separable, it has been shown that this problem, in full
generality, is NP-hard. Alternative approaches all involve approximations of
some kind, including the use of surrogates for the 0-1 loss (for example, the
hinge or logistic loss) or approximate combinatorial search, none of which can
be guaranteed to solve the problem exactly. Finding efficient algorithms to
obtain an exact i.e. globally optimal solution for the 0-1 loss linear
classification problem with fixed dimension, remains an open problem. In
research we report here, we detail the construction of a new algorithm,
incremental cell enumeration (ICE), that can solve the 0-1 loss classification
problem exactly in polynomial time. To our knowledge, this is the first,
rigorously-proven polynomial time algorithm for this long-standing problem.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Wednesday, June 21
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2023/06/21/flat-folding-map.html'>Flat folding and map folding</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Another day, another new arXiv preprint. Todayâs is âA parameterized algorithm for flat foldingâ (arXiv:2306.11939, to appear at CCCG 2023), a paper I mentioned in the last slide of my recent talk on graph width parameters for parameterized geometric algorithms.
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Another day, another new arXiv preprint. Todayâs is âA parameterized algorithm for flat foldingâ (<a href="https://arxiv.org/abs/2306.11939">arXiv:2306.11939</a>, to appear at CCCG 2023), a paper I mentioned in the last slide of my recent talk on <a href="https://www.ics.uci.edu/~eppstein/pubs/Epp-WPAGP-23.pdf">graph width parameters for parameterized geometric algorithms</a>.</p>

<p>Itâs been known since the work of Marshall Bern and Barry Hayes in SODA 1996 that itâs NP-complete to test whether a given origami pattern (with or without an assignment of a mountain or valley fold to each crease) can be folded flat. Of course, many origami structures are not intended to be flat, but they still often start with a flat base, and testing flatness is a prerequisite for any more complicated foldability question you might have. On the other hand, if a pattern does fold flat, itâs very easy to determine where in plane each piece of paper folds to; the hard part is determining the above-below relation between different pieces of paper that fold to the same points.</p>

<p>The new paper defines a graph from any folding pattern, whose vertices represent connected regions of the plane that should be covered by the same pieces of paper in the folded state and whose edges represent adjacencies between those regions. An example is shown below.</p>

<p style="text-align:center"><img src="/blog/assets/2023/folding-graph.svg" alt="A folding pattern and its graph" /></p>

<p>The left side is the folding pattern (blue for valley folds and red for mountain folds): first, fold the right edge of the paper on top of the rest, along the vertical blue crease, and then second, fold the top right corner down, along the two diagonal creases. The right side is how it should map to a folded state (without an assignment of an above-below relation to the layers in that state) and the resulting graph. The goal of the algorithm is to figure out that the regions of the folding pattern should be layered (bottom to top) in the order: big pentagon, lower right trapezoid, upper right trapezoid, triangle. It determines whether a folded state like that exists, but not the sequence of moves you would need to make to get there. But some care is needed in defining the layer order: it has to be local, because there exist folding patterns where different regions are layered differently in different parts of the folded state.</p>

<p style="text-align:center"><img src="/blog/assets/2023/over-under.svg" alt="A folding pattern where two regions have different over-under relations at two different parts of the folded state" style="width:100%;max-width:720px" /></p>

<p>The paper shows that finding a valid above-below relation for each region can be done using a dynamic programming algorithm in time exponential in the treewidth of this graph, where the base of the exponential is factorial in the <em>ply</em> of the folding pattern, the maximum number of pieces of paper that all fold on top of each other at any single point. As it also shows, even for patterns of bounded ply, the exponential dependence on treewidth is necessary under standard complexity-theoretic assumptions.</p>

<p>But what about the dependence on ply? Is that necessary? Why should it be difficult to fold patterns that have many layers, but simple region adjacency graphs?</p>

<p>Unlike for treewidth, I donât have a proof that high ply makes the problem hard. But there is a natural class of problems for which the ply is very high, the treewidth is tiny, and the complexity of finding a flat folding is a famous open problem. This is the <a href="https://en.wikipedia.org/wiki/Map_folding">map folding problem</a>, where the input is just a rectangular grid with labeled creases, and the desired output is a flat-folded state respecting the given labeling. Its region adjacency graph is just a single vertex (everything should fold onto a single square), but its ply can be big (the number of squares in the entire grid). Even for a very simple case, a \(2\times 2\) grid, Wikipedia provides the following illustration of the many vertical orderings among the squares of the grid that are possible. Each square is shown with a different color, visible on both of its sides:</p>

<p style="text-align:center"><img src="/blog/assets/2023/map-foldings.png" alt="Eight solutions to the 2x2 map folding problem" title="CC-BY-SA 3.0 image File:MapFoldings-2x2.png by Robert Dickau from Wikimedia commons" /></p>

<p>The \(1\times n\) map folding problem is easy: just fold over one square from the end of a strip of \(n\) squares, according to the label of its crease, and then solve the remaining \(1\times (n-1)\) problem recursively, treating the folded-over square and the next square that it is folded onto as a single unit. In a 2012 MIT masterâs thesis supervised by Erik Demaine, Thomas Morgan <a href="http://dspace.mit.edu/handle/1721.1/77030">announced a solution to the \(2\times n\) map folding problem</a>, although I donât know that it was ever published in any other form. The algorithm is complicated, with time bound \(O(n^9)\), and the writing is not always clear.</p>

<p>The techniques from Morganâs thesis are very specific to the \(2\times n\) case. Basically, everything you need to know can be recovered from a one-dimensional flat folding of the central crease of the \(2\times n\) grid. At the end of the thesis there is a single paragraph on higher order grids, which (after removing unnecessary complications) boils down to testing all orderings of the squares that are consistent with the above-below relations forced by the crease labels of adjacent squares. The number of orderings can be very small (for instance, if you accordion-fold in one direction and then the other, there is only one consistent ordering) and when it is the algorithm is fast. Beyond that observation, the map folding problem for grids larger than \(2\times n\) appears to be wide open.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/110585322158385767">Discuss on Mastodon</a>)</p><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-21T18:34:00Z">Wednesday, June 21 2023, 18:34</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Tuesday, June 20
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2023/06/20/all-but-clique.html'>All-but-clique-universal graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          When he visited UCI last May, Noga Alon gave two talks: a technical seminar on universal graphs and a more general talk on some of his work in the theory of machine learning. The seminar talk was based in part on his paper âAsymptotically optimal induced universal graphsâ (Geom. Funct. Anal. 2017) in which he proves that the smallest graphs containing all \(k\)-vertex graphs as induced subgraphs have size
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>When he visited UCI last May, Noga Alon gave two talks: a technical seminar on <a href="https://en.wikipedia.org/wiki/Universal_graph">universal graphs</a> and a more general talk on some of his work in the theory of machine learning. The seminar talk was based in part on his paper âAsymptotically optimal induced universal graphsâ (<a href="https://doi.org/10.1007/s00039-017-0396-9"><em>Geom. Funct. Anal.</em> 2017</a>) in which he proves that the smallest graphs containing all \(k\)-vertex graphs as induced subgraphs have size</p>

\[2^{(k-1)/2}\bigl(1+o(1)\bigr),\]

<p>tight except possibly for the \(o(1)\) term. At the talk I asked him whether the same bound also worked for graphs that contain all <span style="white-space:nowrap">\(k\)-vertex</span> graphs except for the <span style="white-space:nowrap">\(k\)-clique,</span> but do not contain the <span style="white-space:nowrap">\(k\)-clique.</span> He came back a day later with the answer: Yes.</p>

<p>The reason for my question was that I needed these all-but-clique-universal graphs for a paper, now on arXiv: âQuasipolynomiality of the smallest missing induced subgraphâ, with Andrea Lincoln and Virginia Vassilevska Williams, <a href="https://arxiv.org/abs/2306.11185">arXiv:2306.11185</a>, to appear in <em>J. Graph Alg. Appl.</em> This is a followup to <a href="/blog/2019/05/27/shattering-quasipolynomiality.html">an earlier blog post on quasipolynomiality</a> in which I observed that the smallest graph that is not an induced subgraph of some given graph could be found in quasipolynomial time. It necessarily has at most logarithmic size, so you can just do a brute force search of all subgraphs of that size to see which ones are missing. The new paper proves that, under the <a href="https://en.wikipedia.org/wiki/Exponential_time_hypothesis">exponential time hypothesis</a>, this sort of quasipolynomial time bound is necessary: you cannot solve the problem faster than the input size to a logarithmic power. The proof is an easy reduction from clique-finding in which one adds an all-but-clique-universal graph to the input to force the smallest missing graph to be a clique.</p>

<p>Another way of describing our paper is that it finds tight time bounds for determining the largest subgraph size for which a given graph is induced-universal. This adds to the small but growing repertoire of problems for which a quasipolynomial time bound is the correct time bound, under standard complexity theory assumptions. Others cited in the paper include finding approximate Nash equilibria, and finding maximum independent sets in hyperbolic unit disk graphs.</p>

<p>In our paper we use a recursive construction for all-but-clique-universal graphs that produces bigger graphs than Alonâs, of size <span style="white-space:nowrap">roughly \(2^k\),</span> for two reasons: first, because we need a deterministic construction (Alonâs is randomized), and second, because our paper was almost completely through the reviewing process when I found out about Alonâs construction. For proving quasipolynomiality, the bigger exponent in our construction isnât important. But I thought this post would be a good place to describe Alonâs construction instead. Keep in mind that this is based on my faulty recollection of a conversation several weeks ago, so this will read more like half-baked research notes than like a completed result. Iâve probably introduced multiple mistakes into the construction: these are my fault, not Alonâs.</p>

<p>The main idea of Alonâs paper is to cover most induced subgraphs of size at most \(k\) by a big random graph, chosen with the specified number of vertices and with each edge present or absent independently with <span style="white-space:nowrap">probability \(\tfrac12\).</span> As he proves, this is (with high probability) a universal graph for all of the subgraphs that have few symmetries. But it may miss some highly symmetric subgraphs. To cover those, he shows that (regardless of its actual symmetries) every symmetric subgraph has three moderately-large ordered sets of vertices \(A\), \(B\), and \(C\) such that the <span style="white-space:nowrap">\(A\)â\(B\)</span> adjacencies have exactly the same pattern as the <span style="white-space:nowrap">\(A\)â\(C\)</span> adjacencies. Storing only one of these two sets of adjacencies saves enough information that he can use a less-efficient construction for a universal graph for the symmetric subgraphs. The result has size \(O(1/k)\) times the size of the big random part. Because itâs so much smaller, adding it to the big random graph only increases the \(o(1)\) term in their combined size bound.</p>

<p>Modifying this to produce an all-but-clique-universal graph requires only a few more ideas.</p>

<ul>
  <li>
    <p>First, the big random part can contain a <span style="white-space:nowrap">\(k\)-clique.</span> But the expected number of copies of any graph in this part is inversely proportional to its number of symmetries. The size of this part is specifically chosen by a formula (equation 1 of Alonâs paper) that makes the expected number of copies equal to one when the number of symmetries is moderate, <span style="white-space:nowrap">\(k^{16\sqrt{k\log k}}\).</span> In this way, there are many expected copies for graphs with few symmetries <span style="white-space:nowrap">(\(\le k^{8\sqrt{k\log k}}\)),</span> which with some care leads to a proof that all such graphs are covered with high probability. But because the number of symmetries of a clique <span style="white-space:nowrap">is \(k!\),</span> much larger than the moderate size bound above, the expected number of cliques is much smaller than one. So choosing a random graph conditioned to have no clique doesnât really change the high probability of covering all the few-symmetry graphs.</p>
  </li>
  <li>
    <p>Second, the part \(S\) of Alonâs construction that covers the high-symmetry subgraphs will also contain a <span style="white-space:nowrap">\(k\)-clique,</span> but one can fix that by using the graph product \(S\times (K_{k}-e)\) with a clique minus an edge. Any <span style="white-space:nowrap">\(k\)-vertex</span> subgraph of this product either uses one copy of each clique vertex, and is missing the same edge, or it has two vertices coming from the same clique vertex, which cannot be adjacent in the product. Therefore, there are no <span style="white-space:nowrap">\(k\)-cliques</span> in the product, but all other <span style="white-space:nowrap">\(k\)-vertex</span> induced subgraphs remain present.</p>
  </li>
  <li>
    <p>Third, the product blows up the symmetric part by a factor <span style="white-space:nowrap">of \(k\),</span> big enough to increase the multiplier in the size of the overall graph from \(1+o(1)\) <span style="white-space:nowrap">to \(O(1)\).</span> To avoid this increase, we need to compensate by being more careful in the construction of \(S\) to make it even smaller. I didnât hear about the details of this part.</p>
  </li>
</ul>

<p>Putting these ideas together should give an all-but-clique-universal graph of size</p>

\[2^{(k-1)/2}\bigl(1+o(1)\bigr),\]

<p>the same bound as for the induced-universal graphs. If thatâs all too sketchy to be relied on, you could always just use the product of any induced-universal graph <span style="white-space:nowrap">with \(K_k-e\).</span> Itâs bigger by a factor <span style="white-space:nowrap">of \(k\),</span> but it has the right exponent and doesnât have any gaps in my recollection of its construction.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/110580932664089904">Discuss on Mastodon</a>)</p><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-20T23:49:00Z">Tuesday, June 20 2023, 23:49</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://rjlipton.wpcomstaging.com/2023/06/20/computer-science-marches-on/'>Computer Science Marches On</a></h3>
        <p class='tr-article-feed'>from <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          With a note on the death of someone who tried to stop it Arnold the Allosaurus is moving to new digs. All during my time at Princeton, he held sway in cavernous Guyot Hall, the home of several earth and environmental science departments. Thanks to a huge gift from alumnus Eric Schmidt and his wife [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>
<font color="#0044cc"><br />
<em>With a note on the death of someone who tried to stop it</em><br />
<font color="#000000"></p>
<p>
Arnold the Allosaurus is moving to new digs. All during my time at Princeton, he held sway in cavernous Guyot Hall, the home of several earth and environmental science departments. Thanks to a huge gift from alumnus Eric Schmidt and his wife Wendy, Guyot Hall will be <a href="https://engineering.princeton.edu/news/2019/05/29/gift-eric-and-wendy-schmidt-create-new-home-computer-science-princeton-university">rebuilt and expanded</a> as the new home for the Department of Computer Science and several affiliated centers.</p>
<p>
&nbsp;</p>
<p><P></p>
<table style="margin:auto;">
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2023/06/20/computer-science-marches-on/allosauruscropped/" rel="attachment wp-att-21792"><img data-attachment-id="21792" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/20/computer-science-marches-on/allosauruscropped/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/AllosaurusCropped.jpg?fit=736%2C625&amp;ssl=1" data-orig-size="736,625" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="AllosaurusCropped" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/AllosaurusCropped.jpg?fit=300%2C255&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/AllosaurusCropped.jpg?fit=600%2C510&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/AllosaurusCropped.jpg?resize=368%2C312&#038;ssl=1" alt="" width="368" height="312" class="aligncenter wp-image-21792" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/AllosaurusCropped.jpg?w=736&amp;ssl=1 736w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/AllosaurusCropped.jpg?resize=300%2C255&amp;ssl=1 300w" sizes="(max-width: 368px) 100vw, 368px" data-recalc-dims="1" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><FONT size="-2">Tongue-in-cheek <a href="https://www.dailyprincetonian.com/article/2023/05/princeton-guyot-hall-dinosaur-to-switch-major-to-cos">source</a></FONT>
</td>
</tr>
</table>
<p>
Arnold will travel to the new Environmental Studies and Schools of Engineering and Applied Sciences <a href="https://facilities.princeton.edu/projects/es-seas">complex</a> being built along Ivy Lane. The Guyot name will travel with him.</p>
<p>
<p><H2> The Old New Building </H2></p>
<p><p>
I remember when the current Computer Science building was new. It was built in 1989 after I arrived. One striking fact in retrospect is that it isn&#8217;t named for anyone. Its name is the &#8220;Computer Science Building.&#8221; Another fact is that it has our favorite open problem encoded into its brickwork:</p>
<p>
&nbsp;</p>
<p><P></p>
<table style="margin:auto;">
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2023/06/20/computer-science-marches-on/cs_bricks/" rel="attachment wp-att-21793"><img data-attachment-id="21793" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/20/computer-science-marches-on/cs_bricks/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/CS_bricks.jpg?fit=706%2C179&amp;ssl=1" data-orig-size="706,179" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="CS_bricks" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/CS_bricks.jpg?fit=300%2C76&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/CS_bricks.jpg?fit=600%2C152&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/CS_bricks.jpg?resize=530%2C135&#038;ssl=1" alt="" width="530" height="135" class="aligncenter wp-image-21793" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/CS_bricks.jpg?w=706&amp;ssl=1 706w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/CS_bricks.jpg?resize=300%2C76&amp;ssl=1 300w" sizes="(max-width: 530px) 100vw, 530px" data-recalc-dims="1" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><FONT size="-2">Composite crop of <a href="https://www.cs.princeton.edu/news/article/princeton-cs-30-years-and-still-roll">src1</a>, <a href="https://www.cs.princeton.edu/general/bricks">src2</a></FONT>
</td>
</tr>
</table>
<p>
At the dedication, Bob Sedgewick <a href="https://www.cs.princeton.edu/news/article/princeton-cs-30-years-and-still-roll">called</a> this &#8220;a gargoyle for the 1990s.&#8221; A <a href="https://theprince.princeton.edu/princetonperiodicals/cgi-bin/princetonperiodicals?a=d&#038;d=WeeklyBulletin19891113-01.2.4&#038;srpos=2&#038;e=01-09-1989-01-01-1990--en-20--1--txt-txIN-computer+science+building------">story</a> at that time quoted that as a &#8220;20th century gargoyle.&#8221; Well, we are almost a quarter way into the 21st century and no resolution is in sight. Will the problem outlive the building?&#8212;it will almost certainly outlive CS occupancy of the building.</p>
<p>
Before 1985, we were part of the Electrical Engineering and Computer Science and occupied one floor of one wing of the Engineering Quadrangle. That&#8217;s all Ken remembers from his time in 1977&#8211;81 as an undergrad. He just went back for an off-year reunion and marveled at vast amount of construction in progress all over the campus. The 1989 building was outgrown by the field itself: right now CS-affiliated faculty and staff are housed in nine locations and 25&#37; of undergraduates choose the CS major.</p>
<p>
<p><H2> Other Gifts </H2></p>
<p><p>
Large gifts for computing have been made all around the country at an increasing pace. At Georgia Tech I was among the first occupants of the <a href="https://en.wikipedia.org/wiki/Klaus_Advanced_Computing_Building">Christopher W. Klaus Advanced Computing Building</a>. The building was announced in 2000 and finished in 2006. </p>
<p>
Some gifts are from people I know well. Mike Fischer and his wife Alice recently <a href="https://www.newhaven.edu/news/releases/2022/alice-fischer-gift.php">gave</a> &#36;2 million to the Computer Science Endowed Fund at the University of New Haven. Alice started Computer Science as a program at New Haven while Mike has been at Yale all the same time. Here they are looking like when I knew them from my own time at Yale:</p>
<p><P><br />
<a href="https://rjlipton.wpcomstaging.com/2023/06/20/computer-science-marches-on/mikeandalicefischer/" rel="attachment wp-att-21794"><img data-attachment-id="21794" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/20/computer-science-marches-on/mikeandalicefischer/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/MikeAndAliceFischer.png?fit=335%2C188&amp;ssl=1" data-orig-size="335,188" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="MikeAndAliceFischer" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/MikeAndAliceFischer.png?fit=300%2C168&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/MikeAndAliceFischer.png?fit=335%2C188&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/MikeAndAliceFischer.png?resize=335%2C188&#038;ssl=1" alt="" width="335" height="188" class="aligncenter size-full wp-image-21794" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/MikeAndAliceFischer.png?w=335&amp;ssl=1 335w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/MikeAndAliceFischer.png?resize=300%2C168&amp;ssl=1 300w" sizes="(max-width: 335px) 100vw, 335px" data-recalc-dims="1" /></a></p>
<p><P><br />
Mike&#8217;s late brother Patrick was at Penn State and then Vanderbilt when I knew him best. Vanderbilt in 1998 <a href="https://www.nytimes.com/1998/12/01/us/vanderbilt-u-receives-a-gift-of-300-million.html">received</a> perhaps the largest university gift ever in real dollar terms. It was from the family that owns Ingram Micro but was not specifically for computing. </p>
<p>
<p><H2> Coda </H2></p>
<p><p>
I say this because Patrick was a target of someone who tried to stop the progress of computing. His secretary opened the package addressed to him and was severely injured, needing hospitalization for three weeks. Patrick later speculated that he was targeted because he &#8220;went from pure math to theoretical computer science.&#8221; </p>
<p><P></p>
<table style="margin:auto;">
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2023/06/20/computer-science-marches-on/pf-2/" rel="attachment wp-att-21795"><img data-attachment-id="21795" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/20/computer-science-marches-on/pf-2/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/pf.jpeg?fit=188%2C268&amp;ssl=1" data-orig-size="188,268" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pf" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/pf.jpeg?fit=188%2C268&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/pf.jpeg?fit=188%2C268&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/pf.jpeg?resize=188%2C268&#038;ssl=1" alt="" width="188" height="268" class="aligncenter size-full wp-image-21795" data-recalc-dims="1" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><FONT size="-2">Vanderbilt memorial <a href="https://news.vanderbilt.edu/2011/08/26/patrick-fischer-former-engineering-chair-dies/">source</a></FONT>
</td>
</tr>
</table>
<p><P><br />
Other targets were more and less fortunate. Three were killed. David Gelernter opened the package himself in the Yale Computer Science mail room and was left with permanent damage to his right eye and the loss of four fingers of his right hand. The sender of those bombs died by his own hand in prison ten days ago. We will not say any more about him.</p>
<p>
<p><H2> Open Problems </H2></p>
<p><p>
One of William Shakespeare&#8217;s best known lines is, &#8220;Not marble nor the gilded monuments of princes shall outlive this powerful rhyme.&#8221; Will the new buildings and endowments in computer science outlive &#8220;P=NP?&#8221;</p>
<p>
<p class="authors">By RJLipton+KWRegan</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-20T18:12:18Z">Tuesday, June 20 2023, 18:12</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Monday, June 19
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.09542'>Finite state verifiers with both private and public coins</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: M. Utkan Gezer, A. C. Cem Say</p><p>We consider the effects of allowing a finite state verifier in an interactive
proof system to use a bounded number of private coins, in addition to "public"
coins whose outcomes are visible to the prover. Although swapping between
private and public-coin machines does not change the class of verifiable
languages when the verifiers are given reasonably large time and space bounds,
this distinction has well known effects for the capabilities of constant space
verifiers. We show that a constant private-coin "budget" (independent of the
length of the input) increases the power of public-coin interactive proofs with
finite state verifiers considerably, and provide a new characterization of the
complexity class $\rm P$ as the set of languages that are verifiable by such
machines with arbitrarily small error in expected polynomial time.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gezer_M/0/1/0/all/0/1">M. Utkan Gezer</a>, <a href="http://arxiv.org/find/cs/1/au:+Say_A/0/1/0/all/0/1">A. C. Cem Say</a></p><p>We consider the effects of allowing a finite state verifier in an interactive
proof system to use a bounded number of private coins, in addition to "public"
coins whose outcomes are visible to the prover. Although swapping between
private and public-coin machines does not change the class of verifiable
languages when the verifiers are given reasonably large time and space bounds,
this distinction has well known effects for the capabilities of constant space
verifiers. We show that a constant private-coin "budget" (independent of the
length of the input) increases the power of public-coin interactive proofs with
finite state verifiers considerably, and provide a new characterization of the
complexity class $\rm P$ as the set of languages that are verifiable by such
machines with arbitrarily small error in expected polynomial time.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-19T00:30:00Z">Monday, June 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.09550'>Minimizing an Uncrossed Collection of Drawings</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Petr Hlin&#x11b;n&#xfd;, Tom&#xe1;&#x161; Masa&#x159;&#xed;k</p><p>In this paper, we introduce the following new concept in graph drawing. Our
task is to find a small collection of drawings such that they all together
satisfy some property that is useful for graph visualization. We propose
investigating a property where each edge is not crossed in at least one drawing
in the collection. We call such collection uncrossed. Such property is
motivated by a quintessential problem of the crossing number, where one asks
for a plane drawing where the number of edge crossings is minimum. Indeed, if
we are allowed to visualize only one drawing, then the one which minimizes the
number of crossings is probably the neatest for the first orientation. However,
a collection of drawings where each highlights a different aspect of a graph
without any crossings could shed even more light on the graph's structure.
</p>
<p>We propose two definitions. First, the uncrossed number, minimizes the number
of graph drawings in a collection, satisfying the uncrossed property. Second,
the uncrossed crossing number, minimizes the total number of crossings in the
collection that satisfy the uncrossed property. For both definitions, we
establish initial results. We prove that the uncrossed crossing number is
NP-hard, but there is an FPT algorithm parameterized by the solution size.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Hlineny_P/0/1/0/all/0/1">Petr Hlin&#x11b;n&#xfd;</a>, <a href="http://arxiv.org/find/cs/1/au:+Masarik_T/0/1/0/all/0/1">Tom&#xe1;&#x161; Masa&#x159;&#xed;k</a></p><p>In this paper, we introduce the following new concept in graph drawing. Our
task is to find a small collection of drawings such that they all together
satisfy some property that is useful for graph visualization. We propose
investigating a property where each edge is not crossed in at least one drawing
in the collection. We call such collection uncrossed. Such property is
motivated by a quintessential problem of the crossing number, where one asks
for a plane drawing where the number of edge crossings is minimum. Indeed, if
we are allowed to visualize only one drawing, then the one which minimizes the
number of crossings is probably the neatest for the first orientation. However,
a collection of drawings where each highlights a different aspect of a graph
without any crossings could shed even more light on the graph's structure.
</p>
<p>We propose two definitions. First, the uncrossed number, minimizes the number
of graph drawings in a collection, satisfying the uncrossed property. Second,
the uncrossed crossing number, minimizes the total number of crossings in the
collection that satisfy the uncrossed property. For both definitions, we
establish initial results. We prove that the uncrossed crossing number is
NP-hard, but there is an FPT algorithm parameterized by the solution size.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-19T00:30:00Z">Monday, June 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.09870'>An Efficient Algorithm for Power Dominating Set</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Thomas Bl&#xe4;sius, Max G&#xf6;ttlicher</p><p>The problem Power Dominating Set (PDS) is motivated by the placement of
phasor measurement units to monitor electrical networks. It asks for a minimum
set of vertices in a graph that observes all remaining vertices by exhaustively
applying two observation rules. Our contribution is twofold. First, we
determine the parameterized complexity of PDS by proving it is $W[P]$-complete
when parameterized with respect to the solution size. We note that it was only
known to be $W[2]$-hard before. Our second and main contribution is a new
algorithm for PDS that efficiently solves practical instances.
</p>
<p>Our algorithm consists of two complementary parts. The first is a set of
reduction rules for PDS that can also be used in conjunction with previously
existing algorithms. The second is an algorithm for solving the remaining
kernel based on the implicit hitting set approach. Our evaluation on a set of
power grid instances from the literature shows that our solver outperforms
previous state-of-the-art solvers for PDS by more than one order of magnitude
on average. Furthermore, our algorithm can solve previously unsolved instances
of continental scale within a few minutes.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Blasius_T/0/1/0/all/0/1">Thomas Bl&#xe4;sius</a>, <a href="http://arxiv.org/find/cs/1/au:+Gottlicher_M/0/1/0/all/0/1">Max G&#xf6;ttlicher</a></p><p>The problem Power Dominating Set (PDS) is motivated by the placement of
phasor measurement units to monitor electrical networks. It asks for a minimum
set of vertices in a graph that observes all remaining vertices by exhaustively
applying two observation rules. Our contribution is twofold. First, we
determine the parameterized complexity of PDS by proving it is $W[P]$-complete
when parameterized with respect to the solution size. We note that it was only
known to be $W[2]$-hard before. Our second and main contribution is a new
algorithm for PDS that efficiently solves practical instances.
</p>
<p>Our algorithm consists of two complementary parts. The first is a set of
reduction rules for PDS that can also be used in conjunction with previously
existing algorithms. The second is an algorithm for solving the remaining
kernel based on the implicit hitting set approach. Our evaluation on a set of
power grid instances from the literature shows that our solver outperforms
previous state-of-the-art solvers for PDS by more than one order of magnitude
on average. Furthermore, our algorithm can solve previously unsolved instances
of continental scale within a few minutes.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-19T00:30:00Z">Monday, June 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.09396'>Private Federated Frequency Estimation: Adapting to the Hardness of the Instance</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jingfeng Wu, Wennan Zhu, Peter Kairouz, Vladimir Braverman</p><p>In federated frequency estimation (FFE), multiple clients work together to
estimate the frequencies of their collective data by communicating with a
server that respects the privacy constraints of Secure Summation (SecSum), a
cryptographic multi-party computation protocol that ensures that the server can
only access the sum of client-held vectors. For single-round FFE, it is known
that count sketching is nearly information-theoretically optimal for achieving
the fundamental accuracy-communication trade-offs [Chen et al., 2022]. However,
we show that under the more practical multi-round FEE setting, simple
adaptations of count sketching are strictly sub-optimal, and we propose a novel
hybrid sketching algorithm that is provably more accurate. We also address the
following fundamental question: how should a practitioner set the sketch size
in a way that adapts to the hardness of the underlying problem? We propose a
two-phase approach that allows for the use of a smaller sketch size for simpler
problems (e.g. near-sparse or light-tailed distributions). We conclude our work
by showing how differential privacy can be added to our algorithm and verifying
its superior performance through extensive experiments conducted on large-scale
datasets.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jingfeng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wennan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1">Peter Kairouz</a>, <a href="http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1">Vladimir Braverman</a></p><p>In federated frequency estimation (FFE), multiple clients work together to
estimate the frequencies of their collective data by communicating with a
server that respects the privacy constraints of Secure Summation (SecSum), a
cryptographic multi-party computation protocol that ensures that the server can
only access the sum of client-held vectors. For single-round FFE, it is known
that count sketching is nearly information-theoretically optimal for achieving
the fundamental accuracy-communication trade-offs [Chen et al., 2022]. However,
we show that under the more practical multi-round FEE setting, simple
adaptations of count sketching are strictly sub-optimal, and we propose a novel
hybrid sketching algorithm that is provably more accurate. We also address the
following fundamental question: how should a practitioner set the sketch size
in a way that adapts to the hardness of the underlying problem? We propose a
two-phase approach that allows for the use of a smaller sketch size for simpler
problems (e.g. near-sparse or light-tailed distributions). We conclude our work
by showing how differential privacy can be added to our algorithm and verifying
its superior performance through extensive experiments conducted on large-scale
datasets.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-19T00:30:00Z">Monday, June 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.09666'>A Smooth Binary Mechanism for Efficient Private Continual Observation</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Joel Daniel Andersson, Rasmus Pagh</p><p>In privacy under continual observation we study how to release differentially
private estimates based on a dataset that evolves over time. The problem of
releasing private prefix sums of $x_1,x_2,x_3,\dots \in\{0,1\}$ (where the
value of each $x_i$ is to be private) is particularly well-studied, and a
generalized form is used in state-of-the-art methods for private stochastic
gradient descent (SGD). The seminal binary mechanism privately releases the
first $t$ prefix sums with noise of variance polylogarithmic in $t$. Recently,
Henzinger et al. and Denisov et al. showed that it is possible to improve on
the binary mechanism in two ways: The variance of the noise can be reduced by a
(large) constant factor, and also made more even across time steps. However,
their algorithms for generating the noise distribution are not as efficient as
one would like in terms of computation time and (in particular) space. We
address the efficiency problem by presenting a simple alternative to the binary
mechanism in which 1) generating the noise takes constant average time per
value, 2) the variance is reduced by a factor about 4 compared to the binary
mechanism, and 3) the noise distribution at each step is identical.
Empirically, a simple Python implementation of our approach outperforms the
running time of the approach of Henzinger et al., as well as an attempt to
improve their algorithm using high-performance algorithms for multiplication
with Toeplitz matrices.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Andersson_J/0/1/0/all/0/1">Joel Daniel Andersson</a>, <a href="http://arxiv.org/find/cs/1/au:+Pagh_R/0/1/0/all/0/1">Rasmus Pagh</a></p><p>In privacy under continual observation we study how to release differentially
private estimates based on a dataset that evolves over time. The problem of
releasing private prefix sums of $x_1,x_2,x_3,\dots \in\{0,1\}$ (where the
value of each $x_i$ is to be private) is particularly well-studied, and a
generalized form is used in state-of-the-art methods for private stochastic
gradient descent (SGD). The seminal binary mechanism privately releases the
first $t$ prefix sums with noise of variance polylogarithmic in $t$. Recently,
Henzinger et al. and Denisov et al. showed that it is possible to improve on
the binary mechanism in two ways: The variance of the noise can be reduced by a
(large) constant factor, and also made more even across time steps. However,
their algorithms for generating the noise distribution are not as efficient as
one would like in terms of computation time and (in particular) space. We
address the efficiency problem by presenting a simple alternative to the binary
mechanism in which 1) generating the noise takes constant average time per
value, 2) the variance is reduced by a factor about 4 compared to the binary
mechanism, and 3) the noise distribution at each step is identical.
Empirically, a simple Python implementation of our approach outperforms the
running time of the approach of Henzinger et al., as well as an attempt to
improve their algorithm using high-performance algorithms for multiplication
with Toeplitz matrices.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-19T00:30:00Z">Monday, June 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.09783'>MementoHash: A Stateful, Minimal Memory, Best Performing Consistent Hash Algorithm</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Massimo Coluzzi, Amos Brocco, Alessandro Antonucci, Tiziano Leidi</p><p>Consistent hashing is used in distributed systems and networking applications
to spread data evenly and efficiently across a cluster of nodes. In this paper,
we present MementoHash, a novel consistent hashing algorithm that eliminates
known limitations of state-of-the-art algorithms while keeping optimal
performance and minimal memory usage. We describe the algorithm in detail,
provide a pseudo-code implementation, and formally establish its solid
theoretical guarantees. To measure the efficacy of MementoHash, we compare its
performance, in terms of memory usage and lookup time, to that of
state-of-the-art algorithms, namely, AnchorHash, DxHash, and JumpHash. Unlike
JumpHash, MementoHash can handle random failures. Moreover, MementoHash does
not require fixing the overall capacity of the cluster (as AnchorHash and
DxHash do), allowing it to scale indefinitely. The number of removed nodes
affects the performance of all the considered algorithms. Therefore, we conduct
experiments considering three different scenarios: stable (no removed nodes),
one-shot removals (90% of the nodes removed at once), and incremental removals.
We report experimental results that averaged a varying number of nodes from ten
to one million. Results indicate that our algorithm shows optimal lookup
performance and minimal memory usage in its best-case scenario. It behaves
better than AnchorHash and DxHash in its average-case scenario and at least as
well as those two algorithms in its worst-case scenario. However, the
worst-case scenario for MementoHash occurs when more than 70% of the nodes
fail, which describes a unlikely scenario. Therefore, MementoHash shows the
best performance during the regular life cycle of a cluster.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Coluzzi_M/0/1/0/all/0/1">Massimo Coluzzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Brocco_A/0/1/0/all/0/1">Amos Brocco</a>, <a href="http://arxiv.org/find/cs/1/au:+Antonucci_A/0/1/0/all/0/1">Alessandro Antonucci</a>, <a href="http://arxiv.org/find/cs/1/au:+Leidi_T/0/1/0/all/0/1">Tiziano Leidi</a></p><p>Consistent hashing is used in distributed systems and networking applications
to spread data evenly and efficiently across a cluster of nodes. In this paper,
we present MementoHash, a novel consistent hashing algorithm that eliminates
known limitations of state-of-the-art algorithms while keeping optimal
performance and minimal memory usage. We describe the algorithm in detail,
provide a pseudo-code implementation, and formally establish its solid
theoretical guarantees. To measure the efficacy of MementoHash, we compare its
performance, in terms of memory usage and lookup time, to that of
state-of-the-art algorithms, namely, AnchorHash, DxHash, and JumpHash. Unlike
JumpHash, MementoHash can handle random failures. Moreover, MementoHash does
not require fixing the overall capacity of the cluster (as AnchorHash and
DxHash do), allowing it to scale indefinitely. The number of removed nodes
affects the performance of all the considered algorithms. Therefore, we conduct
experiments considering three different scenarios: stable (no removed nodes),
one-shot removals (90% of the nodes removed at once), and incremental removals.
We report experimental results that averaged a varying number of nodes from ten
to one million. Results indicate that our algorithm shows optimal lookup
performance and minimal memory usage in its best-case scenario. It behaves
better than AnchorHash and DxHash in its average-case scenario and at least as
well as those two algorithms in its worst-case scenario. However, the
worst-case scenario for MementoHash occurs when more than 70% of the nodes
fail, which describes a unlikely scenario. Therefore, MementoHash shows the
best performance during the regular life cycle of a cluster.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-19T00:30:00Z">Monday, June 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.09835'>Subset Selection Based On Multiple Rankings in the Presence of Bias: Effectiveness of Fairness Constraints for Multiwinner Voting Score Functions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Niclas Boehmer, L. Elisa Celis, Lingxiao Huang, Anay Mehrotra, Nisheeth K. Vishnoi</p><p>We consider the problem of subset selection where one is given multiple
rankings of items and the goal is to select the highest ``quality'' subset.
Score functions from the multiwinner voting literature have been used to
aggregate rankings into quality scores for subsets. We study this setting of
subset selection problems when, in addition, rankings may contain systemic or
unconscious biases toward a group of items. For a general model of input
rankings and biases, we show that requiring the selected subset to satisfy
group fairness constraints can improve the quality of the selection with
respect to unbiased rankings. Importantly, we show that for fairness
constraints to be effective, different multiwinner score functions may require
a drastically different number of rankings: While for some functions, fairness
constraints need an exponential number of rankings to recover a
close-to-optimal solution, for others, this dependency is only polynomial. This
result relies on a novel notion of ``smoothness'' of submodular functions in
this setting that quantifies how well a function can ``correctly'' assess the
quality of items in the presence of bias. The results in this paper can be used
to guide the choice of multiwinner score functions for the subset selection
setting considered here; we additionally provide a tool to empirically enable
this.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Boehmer_N/0/1/0/all/0/1">Niclas Boehmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Celis_L/0/1/0/all/0/1">L. Elisa Celis</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lingxiao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehrotra_A/0/1/0/all/0/1">Anay Mehrotra</a>, <a href="http://arxiv.org/find/cs/1/au:+Vishnoi_N/0/1/0/all/0/1">Nisheeth K. Vishnoi</a></p><p>We consider the problem of subset selection where one is given multiple
rankings of items and the goal is to select the highest ``quality'' subset.
Score functions from the multiwinner voting literature have been used to
aggregate rankings into quality scores for subsets. We study this setting of
subset selection problems when, in addition, rankings may contain systemic or
unconscious biases toward a group of items. For a general model of input
rankings and biases, we show that requiring the selected subset to satisfy
group fairness constraints can improve the quality of the selection with
respect to unbiased rankings. Importantly, we show that for fairness
constraints to be effective, different multiwinner score functions may require
a drastically different number of rankings: While for some functions, fairness
constraints need an exponential number of rankings to recover a
close-to-optimal solution, for others, this dependency is only polynomial. This
result relies on a novel notion of ``smoothness'' of submodular functions in
this setting that quantifies how well a function can ``correctly'' assess the
quality of items in the presence of bias. The results in this paper can be used
to guide the choice of multiwinner score functions for the subset selection
setting considered here; we additionally provide a tool to empirically enable
this.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-19T00:30:00Z">Monday, June 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.09950'>Nearly-Optimal Hierarchical Clustering for Well-Clustered Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Steinar Laenen, Bogdan-Adrian Manghiuc, He Sun</p><p>This paper presents two efficient hierarchical clustering (HC) algorithms
with respect to Dasgupta's cost function. For any input graph $G$ with a clear
cluster-structure, our designed algorithms run in nearly-linear time in the
input size of $G$, and return an $O(1)$-approximate HC tree with respect to
Dasgupta's cost function. We compare the performance of our algorithm against
the previous state-of-the-art on synthetic and real-world datasets and show
that our designed algorithm produces comparable or better HC trees with much
lower running time.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Laenen_S/0/1/0/all/0/1">Steinar Laenen</a>, <a href="http://arxiv.org/find/cs/1/au:+Manghiuc_B/0/1/0/all/0/1">Bogdan-Adrian Manghiuc</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">He Sun</a></p><p>This paper presents two efficient hierarchical clustering (HC) algorithms
with respect to Dasgupta's cost function. For any input graph $G$ with a clear
cluster-structure, our designed algorithms run in nearly-linear time in the
input size of $G$, and return an $O(1)$-approximate HC tree with respect to
Dasgupta's cost function. We compare the performance of our algorithm against
the previous state-of-the-art on synthetic and real-world datasets and show
that our designed algorithm produces comparable or better HC trees with much
lower running time.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-19T00:30:00Z">Monday, June 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Sunday, June 18
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://blog.computationalcomplexity.org/2023/06/ted-kaczynski-was-at-one-time-worlds.html'>Ted Kaczynski was at one time the worlds most famous living mathematician</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>(This post is inspired by the death of Ted Kaczynski who died on June 10, 2023.)&nbsp;</p><p>From 1978 until 1995 23 mailbombs were sent to various people. 3 caused deaths, the rest caused injuries. The culprit was nicknamed The Unabomber (I wonder if he liked that nickname.) For more on his story see&nbsp;here.</p><p>The culprit was Ted Kaczynski. He had a BS in Math from Harvard in Mathematics in 1962, and a PhD in Math from The Univ of Michigan in 1967. He got a job in the Berkeley math dept but resigned in 1969. He soon thereafter moved to a shack in the woods (I wonder if his prison accommodations were better) and began sending out the mailbombs.&nbsp;</p><p>When he was caught in 1995 he was (a) famous and (b) a mathematician. That last point is debatable in that I doubt he was doing math while living in his shack. But we will ignore that point for now. Would you call him a famous mathematician? If so then he was, in 1995, the most famous living mathematician.&nbsp;</p><p>In&nbsp; 1995 Andrew Wiles proved Fermat's Last theorem (this is not quite right- there was a bug and it was fixed with help from Richard Taylor) and he was, for a brief time, the world's most famous living mathematician, though perhaps Wiles and Kaczinski were tied. Wiles made People magazine's 25 most intriguing people of the year! (NOTE- I originally had, incorrectly that Wiles had proven it in 1986. A comment alerted me to the error which makes the story MORE interesting since Ted and Andrew were competing for Most Famous Living Mathemticians!)</p><p>Terry Tao won the Fields medal (2006) AND the MacArthur Genius award (2006) AND the breakthrough award (2015). The last one got him a spot on&nbsp;&nbsp;The Colbert Report (2014) (See&nbsp;here,) For those 15 minutes he might have been the most famous living mathematician. He did not have much competition for the honor.&nbsp;&nbsp;</p><p>And then there is Grigori Perelman who solved the Ponicare Conjecture and declined the Fields Medal and the Millennium prize (Colbert commented on this, see&nbsp;here.) For a very brief time Perelman may have been the most famous living mathematician. He did not have much competition for the honor.&nbsp;</p><p>The most famous mathematicians of all time: Pythagoras of Samos, Euclid, Lewis Carroll.&nbsp;</p><p>1) Pythagoras might not count since its not clear how much he had to do with his theorem.</p><p>2) Lewis Carroll is the most interesting case. He IS famous. He DID do Mathematics. He DID mathematics while he wrote the books that made him famous. So he is a famous mathematician but he is not famous for his math. But that does not quite seem right.&nbsp;</p><p>3) The Math version of AND and the English version of AND are different. Lewis Carroll is FAMOUS and Lewis Caroll is A MATHEMATICIAN but it doesn't seem quite right to call him a FAMOUS MATHEMATICIAN. Same for Ted K.&nbsp; Andrew W was, for a short time, a legit FAMOUS MATHEMATICIAN.&nbsp;</p><p>3) Stephen Hawkings has appeared on ST:TNG and his voice on The Simpsons, Futurama, The Big Bang Theory. He is famous for a combination of his disability, his expository work, and his Physics. Is he a famous actor?&nbsp;</p><p>4) Science expositors like Carl Sagan and Neil deGrasse Tyson are famous for being expositors of science, not quite for their science. How do Professor Proton and Bill Nye the Science Guy fit into this?</p><p>5) Looking at Ted K, Andrew W, Terry T, Grigori P one other point comes up: All of them were famous for a short time but it faded QUICKLY. So- fame is fleeting!</p><p><br></p><p>By gasarch</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>(This post is inspired by the death of Ted Kaczynski who died on June 10, 2023.)&nbsp;</p><p>From 1978 until 1995 23 mailbombs were sent to various people. 3 caused deaths, the rest caused injuries. The culprit was nicknamed <i>The Unabomber</i> (I wonder if he liked that nickname.) For more on his story see&nbsp;<a href="https://en.wikipedia.org/wiki/Ted_Kaczynski">here</a>.</p><p>The culprit was Ted Kaczynski. He had a BS in Math from Harvard in Mathematics in 1962, and a PhD in Math from The Univ of Michigan in 1967. He got a job in the Berkeley math dept but resigned in 1969. He soon thereafter moved to a shack in the woods (I wonder if his prison accommodations were better) and began sending out the mailbombs.&nbsp;</p><p>When he was caught in 1995 he was (a) famous and (b) a mathematician. That last point is debatable in that I doubt he was doing math while living in his shack. But we will ignore that point for now. Would you call him a famous mathematician? If so then he was, in 1995, the most famous living mathematician.&nbsp;</p><p>In&nbsp; 1995 Andrew Wiles proved Fermat's Last theorem (this is not quite right- there was a bug and it was fixed with help from Richard Taylor) and he was, for a brief time, the world's most famous living mathematician, though perhaps Wiles and Kaczinski were tied. Wiles made People magazine's 25 most intriguing people of the year! (NOTE- I originally had, incorrectly that Wiles had proven it in 1986. A comment alerted me to the error which makes the story MORE interesting since Ted and Andrew were competing for Most Famous Living Mathemticians!)</p><p>Terry Tao won the Fields medal (2006) AND the MacArthur Genius award (2006) AND the breakthrough award (2015). The last one got him a spot on&nbsp;&nbsp;<i>The Colbert Report (2014)</i> (See&nbsp;<a href="https://www.cc.com/video/6wtwlg/the-colbert-report-terence-tao">here</a>,) For those 15 minutes he might have been the most famous living mathematician. He did not have much competition for the honor.&nbsp;&nbsp;</p><p>And then there is Grigori Perelman who solved the Ponicare Conjecture and declined the Fields Medal and the Millennium prize (Colbert commented on this, see&nbsp;<a href="https://www.cc.com/video/xr6owy/the-colbert-report-cheating-death-fields-medal">here</a>.) For a very brief time Perelman may have been the most famous living mathematician. He did not have much competition for the honor.&nbsp;</p><p>The most famous mathematicians of all time: Pythagoras of Samos, Euclid, Lewis Carroll.&nbsp;</p><p>1) Pythagoras might not count since its not clear how much he had to do with his theorem.</p><p>2) Lewis Carroll is the most interesting case. He IS famous. He DID do Mathematics. He DID mathematics while he wrote the books that made him famous. So he is a <i>famous mathematician </i>but he is not famous for his math. But that does not quite seem right.&nbsp;</p><p>3) The Math version of AND and the English version of AND are different. Lewis Carroll is FAMOUS and Lewis Caroll is A MATHEMATICIAN but it doesn't seem quite right to call him a FAMOUS MATHEMATICIAN. Same for Ted K.&nbsp; Andrew W was, for a short time, a legit FAMOUS MATHEMATICIAN.&nbsp;</p><p>3) Stephen Hawkings has appeared on ST:TNG and his voice on The Simpsons, Futurama, The Big Bang Theory. He is famous for a combination of his disability, his expository work, and his Physics. Is he a <i>famous actor</i>?&nbsp;</p><p>4) Science expositors like Carl Sagan and Neil deGrasse Tyson are famous for being expositors of science, not quite for their science. How do Professor Proton and Bill Nye the Science Guy fit into this?</p><p>5) Looking at Ted K, Andrew W, Terry T, Grigori P one other point comes up: All of them were famous for a short time but it faded QUICKLY. So- fame is fleeting!</p><p><br /></p><p class="authors">By gasarch</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-18T13:37:00Z">Sunday, June 18 2023, 13:37</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://ptreview.sublinear.info/2023/06/news-for-may-2023/'>News for May 2023</a></h3>
        <p class='tr-article-feed'>from <a href='https://ptreview.sublinear.info'>Property Testing Review</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Apologies, dear readers for the delay in getting out this month&#8217;s post. This month we had three papers &#8212; all on testing properties of graphs! (EDIT: Updated later) four papers: three on property testing problems on graphs and the last one on testing convexity. One of the featured papers this month revisits the problem of [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Apologies, dear readers for the delay in getting out this month&#8217;s post. This month we had <s>three papers &#8212; all on testing properties of graphs!</s> (<em><strong>EDIT:</strong> Updated later</em>) four papers: three on property testing problems on graphs and the last one on testing convexity. One of the featured papers this month revisits the problem of testing the properties of directed graphs and comes back with a happy progress report. Alright, let&#8217;s dig in.</p>



<p></p>



<p><strong>A Distributed Conductance Tester Without Global Information Collection</strong> by Tugkan Batu, Chhaya Trehan (<a href="https://arxiv.org/abs/2305.14178">arXiv</a>) One of the classic questions in property testing considers the task of testing expansion. Here, you are interested in knowing whether the input graph has conductance at least \(\alpha\) or it is far from having conductance at most \(\alpha^2\). On a parallel track, we recall that thanks to the classic work of Parnas and Ron, we know there are connections between distributed algorithms and graph property testing. Meditating on these connections led to the emergence of distributed graph property testing as an active area of research. The featured paper considers the task of testing expansion in the distributed framework. The algorithms presented give a distributed implementation of multiple random walks from all vertices, and controls the congestion of the implementation. In particular, this leads to a \(O(\log n/\alpha^2)\) round expansion-tester. In a first attempt at such an implementation, you might note that you need to track how well short random walks mix when started from a bunch of randomly chosen vertices. This seems to require maintaining some global state/global aggregate information. One of the important features of their algorithm (as mentioned in the title) does away with the need to maintain such global states. As a closing remark, I note the algorithm presented in this paper does not require the graph to be bounded degree.</p>



<p></p>



<p><strong>Testing versus estimation of graph properties, revisited</strong> by Lior Gishboliner, Nick Kushnir, Asaf Shapira (<a href="https://arxiv.org/abs/2305.05487">arXiv</a>) This paper considers the task of additively estimating the distance to a property \(\mathcal{P}\) of a dense graph. Let me set up some context to view the results in the featured paper by summarizing what was known before. One of the early important results in this area is the original result of Fischer and Newman which shows that any testable graph property admits a \(\pm \varepsilon\) distance approximation algorithm, which follows from the regularity lemma. However, the query complexity of the resulting algorithm is a Wowzer-style bound. Later, Hoppen et al., building upon tools from the classic work of Conlon and Fox, demonstrated that this bound of \(twr(poly(1/\varepsilon))\) also holds for testable hereditary properties. Fiat and Ron introduced a decomposition machinery that allowed them to decompose a &#8220;complex&#8221; property into a collection of simpler properties. They used this decomposition to estimate distances to a vast collection of graph properties. They also asked if it was possible to find a transformation using which one can bypass the blowup in query complexity incurred by Fischer and Newman for some rich class of graph properties. The featured paper proves that for a hereditary graph property, you can in fact get algorithms where the query complexity for distance estimation only grows as \(\exp(1/\varepsilon)\). Additionally, for every testable graph property, you can get distance estimators for that property whose query complexity only grows doubly exponentially in \(1/\varepsilon\) (as opposed to the tower bound earlier).  </p>



<p></p>



<p><strong>An Optimal Separation between Two Property Testing Models for Bounded Degree Directed Graphs</strong> by Pan Peng, Yuyang Wang (<a href="https://arxiv.org/abs/2305.13089">arXiv</a>) Unlike undirected graphs, directed graph properties have not received as much attention in the property testing community. In a classic work, Bender and Ron considered two natural models for studying property testing on directed graphs. The first model is one where you can only follow the &#8220;out&#8221; edges or the so-called <em>unidirectional model</em>. In the other model, you are allowed to follow both the &#8220;out&#8221; edges and the &#8220;in&#8221; edges incident on the vertex which is also called the <em>bidirectional model</em>. The featured paper considers directed graphs where the in-degree and the out-degrees are both bounded in both of the models mentioned above. The graph is presented to you in the adjacency list format (tailored for the model you consider). The paper shows that even for the fundamental task of subgraph-freeness, the directed world has some interesting behavior with respect to the two models above. Let me showcase one of the catchy results from the paper which illustrates this separation nicely. Take a connected directed graph \(H\) with \(k\) source components. The paper shows that for sufficiently small \(\varepsilon\), testing whether a directed graph \(G\) is \(H\)-free or \(\varepsilon\)-far from being \(H\)-free requires \(\Omega(n^{1-1/k})\) unidirectional queries. On the other hand, in the bidirectional model, this can be done with a mere \(O_{\varepsilon, d, k}(1)\) number of queries.</p>



<p>(<em><strong>EDIT:</strong> Added later</em>)</p>



<p><strong>Testing Convexity of Discrete Sets in High Dimensions</strong> by Hadley Black, Eric Blais, Nathaniel Harms (<a href="https://arxiv.org/abs/2305.03194">arXiv</a>) As the title suggests, this paper explores the problem of testing convexity. To understand the notion of convexity explored in the paper, consider the following setup: You call a set \(S \subseteq \{-1, 0, 1\}^n\) <em>convex</em> if there exists a convex set \(S&#8217; \subseteq \mathbb{R}^n\) such that \(S = S&#8217; \cap \{-1,0,1\}^n\). And you call a set \(S \subseteq \{-1,0,1\}^n\) <em>far from being convex</em> if for every convex \(T \subseteq \{-1,0,1\}^n\), you have \(|S \oplus T| \geq \varepsilon 3^n\). The paper shows that when you are allowed membership queries, you can test convexity non-adaptively with a one-sided error by issuing \(3^{O(\sqrt{n \log(1/\varepsilon)})}\) queries. Also, they prove an almost matching lower bound. Finally, with a lower bound of \(3^{\Omega(n)}\) when confined to using sample-based testers, authors provably demonstrate that membership queries indeed buy you some undeniable power for testing convexity in high dimensions. </p>
<p class="authors">By Akash</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-18T07:14:46Z">Sunday, June 18 2023, 07:14</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/090'>TR23-090 |  HDX Condensers | 

	Itay Cohen, 

	Roy Roth, 

	Amnon Ta-Shma</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          More than twenty years ago, Capalbo, Reingold, Vadhan and Wigderson gave the first (and up to date only) explicit construction of a bipartite expander with almost full combinatorial expansion. The construction incorporates zig-zag ideas together with extractor technology, and is rather complicated. We give an alternative construction that builds upon recent constructions of hyper-regular, high-dimensional expanders. The new construction is, in our opinion, simple and elegant.
Beyond demonstrating a new, surprising, and intriguing, application of high-dimensional expanders, the construction employs totally new ideas which we hope may lead to progress on the still remaining open problems in the area.
        
        </div>

        <div class='tr-article-summary'>
        
          
          More than twenty years ago, Capalbo, Reingold, Vadhan and Wigderson gave the first (and up to date only) explicit construction of a bipartite expander with almost full combinatorial expansion. The construction incorporates zig-zag ideas together with extractor technology, and is rather complicated. We give an alternative construction that builds upon recent constructions of hyper-regular, high-dimensional expanders. The new construction is, in our opinion, simple and elegant.
Beyond demonstrating a new, surprising, and intriguing, application of high-dimensional expanders, the construction employs totally new ideas which we hope may lead to progress on the still remaining open problems in the area.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-18T05:48:51Z">Sunday, June 18 2023, 05:48</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Saturday, June 17
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://tcsplus.wordpress.com/2023/06/17/guest-post-stocial-activities-graduating-bits-and-junior-senior-lunch-at-stoc-2023/'>Guest post: STOCial activities, Graduating Bits, and Junior/Senior Lunch at STOC 2023</a></h3>
        <p class='tr-article-feed'>from <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          If you are attending STOC&#8217;23 as part of the TheoryFest next week, don&#8217;t forget to have a look at the STOCial Program! In particular, and to get to the point of this announcement: if you are graduating or will be on the job market soon, consider participating to the Graduating Bits Session on Wednesday: 1-2 [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>If you are attending <a href="http://acm-stoc.org/stoc2023/">STOC&#8217;23</a> as part of the TheoryFest next week, don&#8217;t forget to have a look at the <a href="https://sites.google.com/view/stocial2023/home">STOCial Program</a>! </p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><a href="https://tcsplus.files.wordpress.com/2023/06/it-is-fun-its-enjoyable.gif"><img data-attachment-id="699" data-permalink="https://tcsplus.wordpress.com/2023/06/17/guest-post-stocial-activities-graduating-bits-and-junior-senior-lunch-at-stoc-2023/it-is-fun-its-enjoyable/" data-orig-file="https://tcsplus.files.wordpress.com/2023/06/it-is-fun-its-enjoyable.gif" data-orig-size="498,289" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="it-is-fun-its-enjoyable" data-image-description="" data-image-caption="" data-medium-file="https://tcsplus.files.wordpress.com/2023/06/it-is-fun-its-enjoyable.gif?w=300" data-large-file="https://tcsplus.files.wordpress.com/2023/06/it-is-fun-its-enjoyable.gif?w=498" src="https://tcsplus.files.wordpress.com/2023/06/it-is-fun-its-enjoyable.gif?w=498" alt="GIF of a man with a beret saying &quot;It is fun!&quot;" class="wp-image-699" /></a></figure></div>


<p></p>



<p>In particular, and to get to the point of this announcement: if you are graduating or will be on the job market soon, consider participating to the <em>Graduating Bits Session</em> on Wednesday: 1-2 slides, 2 minutes, entirely yours to present and pitch your research to the STOC attendees! To register, <a href="https://docs.google.com/forms/d/e/1FAIpQLScgDDosoozV6rJ7kemMolHQJGgt3vO7ihjfpKhaeVMc_7IDVw/viewform">fill this form</a> before Tuesday evening <em>(up to 25 slots, first-come-first-served)</em>.</p>



<p>Following the by-now-well-established tradition, there will also be a &#8220;senior/junior lunch&#8221; on Thursday, during the 12:30-2pm time slot: <a href="https://docs.google.com/spreadsheets/d/1nmPaZdjC0y0FbJgwXDAzaHSp6FjOtSoRM_SCjqbA9rE/edit">put your name here</a> as a &#8220;senior&#8221; (broadly construed) or a &#8220;junior&#8221; academic, for informal discussions, academic advice, and general questions over lunch. </p>



<p>See you next week at the TheoryFest!</p>



<p><em>ClÃ©ment, on behalf of the STOCial Committee (Barna Saha, ClÃ©ment Canonne, Elena Grigorescu, and Raghu Meka)</em></p>



<p></p>
<p class="authors">By plustcs</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-17T12:34:16Z">Saturday, June 17 2023, 12:34</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Thursday, June 15
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2023/06/15/linkage.html'>Linkage</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Marmor Soorten, or The Book of Marble (\(\mathbb{M}\)), an Enlightenment-era full-color catalog of types of marble, to be reprinted by Taschen.
        
        </div>

        <div class='tr-article-summary'>
        
          
          <ul>
  <li>
    <p><a href="https://www.thisiscolossal.com/2023/04/book-of-marble/"><em>Marmor Soorten</em>, or <em>The Book of Marble</em></a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@colossal@mastodon.art/110349972015054620">\(\mathbb{M}\)</a>),</span> an Enlightenment-era full-color catalog of types of marble, to be reprinted by Taschen.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/how-math-has-changed-the-shape-of-gerrymandering-20230601/">How Math Has Changed the Shape of Gerrymandering</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@JeanneClelland/110475855814528525">\(\mathbb{M}\)</a>),</span> <em>Quanta</em> on the ReCom method for generating random ensembles of redistricting plans with the probability of a plan proportional to its number of spanning forests, via Markov Chain Monte Carlo, and using them as a basis of comparison to test the fairness of real redistricting plans. Moon Duchin recently spoke on the same thing at SoCG.</p>
  </li>
  <li>
    <p><a href="https://www.erdosproblems.com/">ErdÅs problems</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@sioum/110469648800130541">\(\mathbb{M}\)</a>),</span> collected and tracked by Thomas Bloom.</p>
  </li>
  <li>
    <p><a href="https://meta.stackexchange.com/questions/389811/moderation-strike-stack-overflow-inc-cannot-consistently-ignore-mistreat-an">Moderation Strike on Stack Overflow</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@highergeometer/110489672001944513">\(\mathbb{M}\)</a>)</span> triggered by a combination of corporate insistance on allowing AI content and lying to users about what they are requiring of their moderators. This affects both <a href="https://mathoverflow.net/">MathOverflow</a> and the <a href="https://cstheory.stackexchange.com/">TCS Stack Exchange</a>, although MathOverflow at least is merely operated by StackExchange, not actually owned by it.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.09534">Hyperbolic Minesweeper is in \(\mathsf{P}\)</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110499653895105776">\(\mathbb{M}\)</a>),</span> paper from FUN 2021 by Eryk KopczyÅski. As the paper shows, finite subsets of hyperbolic tilings form graphs with only logarithmic treewidth. As a consequence, problems that can be solved by dynamic programming on these graphs, such as testing the safety of moves in hyperbolic minesweeper, take polynomial time.</p>
  </li>
  <li>
    <p><a href="https://www.cambridge.org/files/9216/8487/6990/matching_book_pw.pdf"><em>Online and Matching-Based Market Design</em></a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@fortnow@fediscience.org/110498942774330068">\(\mathbb{M}\)</a>),</span> new book edited by Federico Echenique, Nicole Immorlica, and Vijay Vazirani. The editors gave up any royalties to make it free online, but in a gratuitously annoying format where the online copy is password-locked with a publicly-announced password. The password is OMBMD_CUP. I put it into the filename of my downloaded copy so I wouldnât lose one without the other.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2306.04007">Sam Mattheus and Jacques Verstraete determine the asymptotics of the Ramsey numbers \(r(4,n)\) to within a polylog</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@anuragbishnoi/110508895802423231">\(\mathbb{M}\)</a>).</span> See also <a href="https://anuragbishnoi.wordpress.com/2023/06/08/determining-ramsey-numbers-using-finite-geometry/">Anurag Bishnoiâs blog post on the proof</a>.</p>
  </li>
  <li>
    <p><a href="https://www.techcityng.com/wikipedia-says-it-will-not-comply-with-uk-bill-on-age-checks/">âWikipedia has announced that it will not comply with the age verification requirements of the UKâs Online Safety Billâ</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110518781331781604">\(\mathbb{M}\)</a>,</span> <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-06-05/In_the_media">via</a>). For more about the bill, see <a href="https://en.wikipedia.org/wiki/Online_Safety_Bill">Wikipediaâs article on it</a> â the issues are whether Wikipediaâs articles on sexual topics could be classified as pornography, which would trigger mandatory age verification according to the bill, and incompatibility of both age restriction and reader identification with the goals and purposes of Wikipedia.</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/news/faculty-issues/diversity-equity/2023/06/09/mathfest-florida-some-lgbtq-mathematicians-arent"><em>Inside Higher Education</em> on MAAâs insistence on holding MathFest in Florida this August</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110524346620592147">\(\mathbb{M}\)</a>,</span> <a href="https://web.archive.org/web/20230609042324/https://www.insidehighered.com/news/faculty-issues/diversity-equity/2023/06/09/mathfest-florida-some-lgbtq-mathematicians-arent">archive</a>), in-person only, over the objections of LGBTQ+ mathematicians targeted by recently passed state anti-gay laws that prohibit people from using gender-appropriate bathrooms, forbid the mention of homosexuality, sexism, or racism in schools, and have been used to shut down parades and concerts for including people being trans in public. See also <a href="http://digitaleditions.walsworthprintgroup.com/publication/?i=793202&amp;article_id=4588328">âMathfest in Tampa: A Discussionâ, <em>MAA Focus</em></a>. These sorts of events take years in advance to set up, and would incur a huge financial penalty to cancel at this late date, but the MAA already started having recriminations in 2021.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@tonwood/110524333213370377">Anton Sherwood asks for monohedral tilings of the sphere with no symmetries</a>. Itâs possible, but whether it can be done with arbitrarily large numbers of tiles is unclear.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Cantor%27s_isomorphism_theorem">Newly promoted Wikipedia Good Article: Cantorâs isomorphism theorem</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110533545897925388">\(\mathbb{M}\)</a>).</span> This is the one saying that when an infinite linear ordering looks like the ordering on the rational numbers, it is the same ordering. âLooks likeâ means:</p>

    <ul>
      <li>
        <p>It has countably many elements</p>
      </li>
      <li>
        <p>It has no minimum and no maximum</p>
      </li>
      <li>
        <p>It has another element between any two of its elements</p>
      </li>
    </ul>

    <p>So the rationals, the dyadic rationals, the algebraic numbers, their intersections with the open unit interval, etc. all have the same ordering.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@logicalelegance@mastodon.online/110503412274553258">Origami snail shells and companion octopodes</a>.</p>
  </li>
  <li>
    <p><a href="https://www.wired.com/story/dont-want-students-to-rely-on-chatgpt-have-them-use-it/">According to <em>Wired</em>, giving students assignments using ChatGPT can help them become more confident in their own abilities to do better than it</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@TammyKolda/110497708715756714">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2301.10191">New simple streaming algorithm for estimating the number of distinct elements in a stream</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@fortnow@fediscience.org/110537617395576272">\(\mathbb{M}\)</a>),</span> âDistinct Elements in Streams: An Algorithm for the (Text) Bookâ, by Sourav Chakraborty, N. V. Vinodchandran, and Kuldeep S. Meel, updated from their ESA 2022 paper. <a href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog</a> can do the same thing but the new one is maybe more suitable for teaching, and doesnât depend on hashing. See also writeups by <a href="https://cs.stanford.edu/~knuth/papers/cvm-note.pdf">Knuth</a> and <a href="https://justinjaffray.com/a-charming-algorithm-for-count-distinct/">Justin Jaffray</a>.</p>
  </li>
</ul><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-15T23:00:00Z">Thursday, June 15 2023, 23:00</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://blog.computationalcomplexity.org/2023/06/randomized-acceptances.html'>Randomized Acceptances</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>NeurIPS recently released their 2021 consistency report, a sequel to the 2014 experiment. While the conference has grown dramatically, the results remain "consistent", about 23% disagreement from two separated program committee groups. As before I don't find this too surprising--different committee members have different tastes.</p><p>Roughly conference submissions fall into three categories</p><p></p><ol><li>Clearly strong papers</li><li>Clear rejects</li><li>A bunch that could go either way.</li></ol>A typical program committee quickly sorts out the first two groups and then painfully spends considerable time arguing over the others.<p></p><p>What if instead we took a different approach. Accept all the strong papers and reject the weak ones. Choose the rest randomly, either with a uniform or weighted distribution based on the ranking. Maybe reduce the probability of those who submit multiple papers.</p><p>Choosing randomly reduces biases and can increase diversity, if there is diversity in submissions. Knowing there is randomness in the process allows those with rejected papers to blame the randomness and those whose papers gets in claim they were in the first group. Randomness encourages more submissions and is fair over time.</p><p>Note we're just acknowledging the randomness in the process instead of pretending there is a perfect linear order to the papers that only a lengthy program committee discussion can suss out.</p><p>We should do the same for grant proposals--all worthy proposals should get a chance to be funded.</p><p>I doubt any of this will ever happen. People would rather trust human decisions with all their inconsistencies over pure randomness.&nbsp;</p><p>By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>NeurIPS recently released their <a href="https://arxiv.org/abs/2306.03262">2021 consistency report</a>, a sequel to the <a href="https://inverseprobability.com/2014/12/16/the-nips-experiment">2014 experiment</a>. While the conference has grown dramatically, the results remain "consistent", about 23% disagreement from two separated program committee groups. As <a href="https://blog.computationalcomplexity.org/2014/12/the-nips-experiment.html">before</a> I don't find this too surprising--different committee members have different tastes.</p><p>Roughly conference submissions fall into three categories</p><p></p><ol style="text-align: left;"><li>Clearly strong papers</li><li>Clear rejects</li><li>A bunch that could go either way.</li></ol>A typical program committee quickly sorts out the first two groups and then painfully spends considerable time arguing over the others.<p></p><p>What if instead we took a different approach. Accept all the strong papers and reject the weak ones. Choose the rest randomly, either with a uniform or weighted distribution based on the ranking. Maybe reduce the probability of those who submit multiple papers.</p><p>Choosing randomly reduces biases and can increase diversity, if there is diversity in submissions. Knowing there is randomness in the process allows those with rejected papers to blame the randomness and those whose papers gets in claim they were in the first group. Randomness encourages more submissions and is fair over time.</p><p>Note we're just acknowledging the randomness in the process instead of pretending there is a perfect linear order to the papers that only a lengthy program committee discussion can suss out.</p><p>We should do the same for grant proposals--all worthy proposals should get a chance to be funded.</p><p>I doubt any of this will ever happen. People would rather trust human decisions with all their inconsistencies over pure randomness.&nbsp;</p><p class="authors">By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-15T21:04:00Z">Thursday, June 15 2023, 21:04</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/089'>TR23-089 |  New Explicit Constant-Degree Lossless Expanders | 

	Louis Golowich</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We present a new explicit construction of onesided bipartite lossless expanders of constant degree, with arbitrary constant ratio between the sizes of the two vertex sets. Our construction is simpler to state and analyze than the prior construction of Capalbo, Reingold, Vadhan, and Wigderson (2002).

  We construct our lossless expanders by imposing the structure of a constant-sized lossless expander &quot;gadget&quot; within the neighborhoods of a large bipartite spectral expander; similar constructions were previously used to obtain the weaker notion of unique-neighbor expansion. Our analysis simply consists of elementary counting arguments and an application of the expander mixing lemma.
        
        </div>

        <div class='tr-article-summary'>
        
          
          We present a new explicit construction of onesided bipartite lossless expanders of constant degree, with arbitrary constant ratio between the sizes of the two vertex sets. Our construction is simpler to state and analyze than the prior construction of Capalbo, Reingold, Vadhan, and Wigderson (2002).

  We construct our lossless expanders by imposing the structure of a constant-sized lossless expander &quot;gadget&quot; within the neighborhoods of a large bipartite spectral expander; similar constructions were previously used to obtain the weaker notion of unique-neighbor expansion. Our analysis simply consists of elementary counting arguments and an application of the expander mixing lemma.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-15T06:20:23Z">Thursday, June 15 2023, 06:20</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Wednesday, June 14
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://rjlipton.wpcomstaging.com/2023/06/14/a-little-noise-makes-quantum-factoring-fail/'>A Little Noise Makes Quantum Factoring Fail</a></h3>
        <p class='tr-article-feed'>from <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Jin-Yi Cai is one of the top theory experts in the world. Both Ken and I have had the pleasure to work with him and interact with him over the years. We have discussed some of his previous work here and here. Today we will talk about his new work on quantum computing. Quantum Factoring [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>
Jin-Yi Cai is one of the top theory experts in the world. Both Ken and I have had the pleasure to work with him and interact with him over the years. We have discussed some of his previous work <a href="https://rjlipton.wpcomstaging.com/2022/02/23/simons-awards-for-2022/">here</a> and <a href="https://rjlipton.wpcomstaging.com/2017/11/20/a-magic-madison-visit/">here</a>.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2023/06/14/a-little-noise-makes-quantum-factoring-fail/jcai/" rel="attachment wp-att-21763"><img data-attachment-id="21763" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/14/a-little-noise-makes-quantum-factoring-fail/jcai/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/jcai.jpeg?fit=150%2C190&amp;ssl=1" data-orig-size="150,190" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="jcai" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/jcai.jpeg?fit=150%2C190&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/jcai.jpeg?fit=150%2C190&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/jcai.jpeg?resize=150%2C190&#038;ssl=1" alt="" width="150" height="190" class="aligncenter size-full wp-image-21763" data-recalc-dims="1" /></a></p>
<p><P><br />
Today we will talk about his new work on quantum computing.</p>
<p>
<p><H2> Quantum Factoring </H2></p>
<p><p>
Peter Shor invented <a href="https://en.wikipedia.org/wiki/Shor&#37;27s_algorithm">the</a> quantum algorithm for finding the prime factors of an integer in 1994.</p>
<p><P><br />
<a href="https://rjlipton.wpcomstaging.com/2023/06/14/a-little-noise-makes-quantum-factoring-fail/pshor/" rel="attachment wp-att-21764"><img data-attachment-id="21764" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/14/a-little-noise-makes-quantum-factoring-fail/pshor/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/pshor.jpeg?fit=241%2C241&amp;ssl=1" data-orig-size="241,241" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pshor" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/pshor.jpeg?fit=241%2C241&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/pshor.jpeg?fit=241%2C241&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/pshor.jpeg?resize=150%2C150&#038;ssl=1" alt="" width="150" height="150" class="aligncenter wp-image-21764" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/pshor.jpeg?w=241&amp;ssl=1 241w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/pshor.jpeg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/pshor.jpeg?resize=200%2C200&amp;ssl=1 200w" sizes="(max-width: 150px) 100vw, 150px" data-recalc-dims="1" /></a></p>
<p><P><br />
This is one of the great algorithms of all time. It shows at least in theory that quantum algorithms can be much more efficient than classical algorithms. The algorithm shows that the integer factorization problem can be efficiently solved on an idealized quantum computer and is consequently in the complexity class <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BBQP%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;mathsf{BQP}}" class="latex" />. This is almost exponentially faster than the most efficient known classical factoring algorithm.</p>
<p>
<p><H2> Quantum Factoring Possible? </H2></p>
<p>Is it practically feasible to use Shor&#8217;s factoring method to break RSA? This leads to a major question: </p>
<blockquote><p>
<em>Can cryptography survive quantum methods?</em>
</p></blockquote>
<p>
A <a href="https://eprint.iacr.org/2017/351.pdf">paper</a> by Daniel Bernstein, Nadia Heninger, Paul Lou, and Luke Valenta titled &#8220;Post-Quantum RSA&#8221; is a key one. They consider further systems including elliptic curve cryptography (ECC) and say:</p>
<blockquote><p><b> </b> <em> The conventional wisdom among researchers in post-quantum cryptography is that quantum computers will kill RSA and ECC but will not kill hash-based cryptography, code-based cryptography, lattice-based cryptography, or multivariate- quadratic-equations cryptography.<br />
&#8230;<br />
Shor&#8217;s algorithm easily breaks RSA as used on the Internet today. The question is whether RSA parameters can be adjusted so that all known quantum attack algorithms are infeasible while encryption and decryption remain feasible. </em>
</p></blockquote>
<p><p>
See also <a href="https://math.mit.edu/~apost/courses/18.204-2016/18.204_Jeremy_Wohlwend_final_paper.pdf">this</a>. A 2019 <a href="https://arxiv.org/abs/1905.09749">paper</a> by Craig Gidney and Martin Eker&aring; argues that implementations of Shor on 2,048-bit integers <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BN%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{N}" class="latex" /> is within reach of current technology using noisy qubits&#8212;needing only some millions of them. However, this presumes an error-free implementation of the Quantum Fourier Transform (QFT). They say:</p>
<blockquote><p><b> </b> <em> Note furthermore that when we analyze the success probabilities of Shorâs algorithms, and the various derivatives, we assume the use of an ideal QFT even though the implemented QFT is technically an approximation. </em>
</p></blockquote>
<p>
[<b>Added 6/19:</b> This quotation is taken somewhat out of context, because the paper&#8217;s main concern is optimizing and dealing with the much greater noise and precision issues in the superposed modular exponentiation step.  See Craig Gidney&#8217;s <a href="https://rjlipton.wpcomstaging.com/2023/06/14/a-little-noise-makes-quantum-factoring-fail/#comment-120009">comment</a> below for more information on that and on how the QFT step is executed.]</p>
<p>
<p><H2> Quantum Factoring Impossible? </H2></p>
<p>Now enter Jin-Yi. He has a new <a href="https://pages.cs.wisc.edu/~jyc/Shor-Algorithm-with-Noise.pdf">paper</a> that says:</p>
<blockquote><p><b> </b> <em> We consider Shor&#8217;s quantum factoring algorithm in the setting of noisy quantum gates. Under a generic model of random noise for rotation gates, we prove that the algorithm does not factor integers of the form <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bpq%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{pq}" class="latex" /> when the noise exceeds a vanishingly small level in terms of <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n}" class="latex" /> (the number of bits of the integer to be factored), where <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{p}" class="latex" /> and <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{q}" class="latex" /> are chosen from a set of primes of positive density. </em>
</p></blockquote>
<p><p>
Jin-Yi essentially is saying that quantum algorithms fail to break RSA in the presence of noisy gates. He argues that they will <b>not</b> be able to work when quantum gates are not perfect. </p>
<p>
<i>This seems to contradict the previous section.</i> Can it be that quantum algorithms break RSA in theory, but are not practically realizable? See these <a href="https://usa.kaspersky.com/blog/quantum-computers-and-rsa-2023/27605/">three</a> <a href="https://www.lawfareblog.com/retrospective-post-quantum-policy-problem">recent</a> <a href="https://www.forbes.com/sites/forbestechcouncil/2022/08/08/rsas-quantum-proof-successor-may-not-be-safe-for-long/?sh=55c0555475ff">discussions</a>.</p>
<p>
To our knowledge, this is the first hard-and-fast negative result about Shor&#8217;s algorithm. Let&#8217;s take a closer look.</p>
<p>
<p><H2> Angles on Shor&#8217;s Algorithm </H2></p>
<p><p>
Given <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BN+%3D+pq%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{N = pq}" class="latex" /> to factor, Shor&#8217;s algorithm starts by choosing <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{a}" class="latex" /> relatively prime to <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BN%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{N}" class="latex" />. The algorithm extends the domain of the function <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29+%3D+a%5Ex+%5Cpmod%7BN%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{f(x) = a^x &#92;pmod{N}}" class="latex" /> to all <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx+%3C+M%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x &lt; M}" class="latex" /> where <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BM+%3D+2%5Em%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{M = 2^m}" class="latex" />, <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bm+%3D+2n%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{m = 2n}" class="latex" />, and <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B2%5En%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{2^n}" class="latex" /> is the next power of <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B2%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{2}" class="latex" /> after <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BN%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{N}" class="latex" />, so that <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BM+%5Capprox+N%5E2%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{M &#92;approx N^2}" class="latex" />. The quantum engine of Shor&#8217;s algorithm has just two main components:</p>
<ol>
<li>
A routine that computes the quantum state </p>
<p align=center><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7BM%7D%7D%5Csum_x+%7Cx%5Crangle%7Cf%28x%29%5Crangle.+&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="&#92;displaystyle  &#92;Phi = &#92;frac{1}{&#92;sqrt{M}}&#92;sum_x |x&#92;rangle|f(x)&#92;rangle. " class="latex" /></p>
<p>Put another way without the Dirac angle-bracket notation, <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5CPhi%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;Phi}" class="latex" /> is a state of <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bm%2Bn%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{m+n}" class="latex" /> qubits that has equal nonzero amplitude only on those components <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bxy%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{xy}" class="latex" /> where <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7By+%3D+f%28x%29%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{y = f(x)}" class="latex" />. </p>
<li>
The QFT (or its inverse) on <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{m}" class="latex" /> qubits.
</ol>
<p>
Quantum gates of the form <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BR_k+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%5C%5C+0+%26+%5Comega_k+%5Cend%7Bbmatrix%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{R_k = &#92;begin{bmatrix} 1 &amp; 0 &#92;&#92; 0 &amp; &#92;omega_k &#92;end{bmatrix}}" class="latex" /> where <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Comega_k+%3D+%5Cexp%28%5Cfrac%7B2%5Cpi+i%7D%7B2%5Ek%7D%29%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;omega_k = &#92;exp(&#92;frac{2&#92;pi i}{2^k})}" class="latex" />, when <em>controlled</em> from another qubit, are used in the &#8220;textbook&#8221; way to compute the QFT. The diagram with <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bm%3D4%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{m=4}" class="latex" /> suffices for the general pattern:</p>
<p><P></p>
<table style="margin:auto;">
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2023/06/14/a-little-noise-makes-quantum-factoring-fail/qft4a/" rel="attachment wp-att-21765"><img data-attachment-id="21765" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/14/a-little-noise-makes-quantum-factoring-fail/qft4a/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/QFT4A.jpg?fit=666%2C178&amp;ssl=1" data-orig-size="666,178" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;KWRegan&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1686570454&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="QFT4A" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/QFT4A.jpg?fit=300%2C80&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/QFT4A.jpg?fit=600%2C160&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/QFT4A.jpg?resize=555%2C150&#038;ssl=1" alt="" width="555" height="150" class="aligncenter wp-image-21765" data-recalc-dims="1" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><FONT size="-2"><a href="http://math.utoledo.edu/~codenth/Spring_15/4350/HW/fourier.pdf">source</a></FONT>
</td>
</tr>
</table>
<p>
For all but a few small values of <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{k}" class="latex" />, the rotation angle in <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BR_k%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{R_k}" class="latex" /> is tinier than theoretical minimum units of space, let alone the smallest precision of angular or spatial resolution we have achieved in experiments such as LIGO. Call a circuit family using <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BR_k%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{R_k}" class="latex" /> for unbounded <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{k}" class="latex" /> &#8220;idealistic.&#8221; </p>
<p>
Donald Coppersmith <a href="https://arxiv.org/abs/quant-ph/0201067">showed</a> that Shor&#8217;s algorithm still works if <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BR_k%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{R_k}" class="latex" /> is replaced by the identity operator for <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bk+%3E+b%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{k &gt; b}" class="latex" />, where the threshold <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{b}" class="latex" /> equals <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bc%5Clog_2+n%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{c&#92;log_2 n}" class="latex" /> for a constant <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{c}" class="latex" /> slightly above <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B1%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{1}" class="latex" />. The resulting circuits are still &#8220;idealistic&#8221; but at least not exponentially so. Coppersmith&#8217;s analysis is referenced in Shor&#8217;s original <a href="https://arxiv.org/abs/quant-ph/9508027">paper</a> but not expounded further there.</p>
<p>
Jin-Yi shows that Shor&#8217;s and Coppersmith&#8217;s circuits cannot tolerate a natural kind of noise that operates close to Coppersmith&#8217;s level of scaling. It stands concretely against any asymptotic claims of power via Shor&#8217;s algorithm that involve idealistic circuits. At the end we will discuss its implications also for circuits that implement Shor&#8217;s algorithm without using <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BR_k%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{R_k}" class="latex" /> gates.</p>
<p>
<p><H2> The Noise </H2></p>
<p><p>
Call <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BC%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{C}" class="latex" /> a <em>Shor circuit</em> if it uses controlled <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BR_k%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{R_k}" class="latex" /> gates to compute the QFT (or its inverse) and can be sampled by a classical procedure <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+A%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{{&#92;cal A}}" class="latex" /> to infer the period <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;rho}" class="latex" /> of <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29+%3D+a%5Ex+%5Cpmod%7BN%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{f(x) = a^x &#92;pmod{N}}" class="latex" /> in <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7BO%281%29%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n^{O(1)}}" class="latex" /> expected time. </p>
<p>
Jin-Yi&#8217;s noise operation has parameters <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;epsilon}" class="latex" /> and <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bb%27%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{b&#039;}" class="latex" /> and maps a Shor circuit <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BC%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{C}" class="latex" /> to a distribution of circuits <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde%7BC%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;tilde{C}}" class="latex" /> defined as follows: For each controlled <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BR_k%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{R_k}" class="latex" /> gate in <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BC%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{C}" class="latex" /> with <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+b%27%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{k &#92;geq b&#039;}" class="latex" /> (alternatively, <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+b%27%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{k = b&#039;}" class="latex" />), replace it by </p>
<p align=center><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctilde%7BR_k%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%5C%5C+0+%26+%5Ctilde%7B%5Comega%7D+%5Cend%7Bbmatrix%7D%5Cquad%5Ctext%7Bwhere%7D%5Cquad+%5Ctilde%7B%5Comega%7D+%3D+%5Cexp%5Cleft%28%5Cfrac%7B2%5Cpi+i%281+%2B+r%5Cepsilon%29%7D%7B2%5Ek%7D%5Cright%29+&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="&#92;displaystyle  &#92;tilde{R_k} = &#92;begin{bmatrix} 1 &amp; 0 &#92;&#92; 0 &amp; &#92;tilde{&#92;omega} &#92;end{bmatrix}&#92;quad&#92;text{where}&#92;quad &#92;tilde{&#92;omega} = &#92;exp&#92;left(&#92;frac{2&#92;pi i(1 + r&#92;epsilon)}{2^k}&#92;right) " class="latex" /></p>
<p>with the same control qubit and with an independent draw of Gaussian noise <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Br+%5Csim+%7B%5Ccal+N%7D%280%2C1%29%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{r &#92;sim {&#92;cal N}(0,1)}" class="latex" />. The echo of Coppersmith&#8217;s &#8220;<img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{b}" class="latex" />&#8221; is on purpose, because he establishes the following fact, which we first state loosely:</p>
<blockquote><p><b> </b> <em> Provided <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bb%27+%2B+%5Clog_2%281%2F%5Cepsilon%29+%5Csim+%5Cfrac%7B1%7D%7B3%7D%28%5Clog_2+n%29%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{b&#039; + &#92;log_2(1/&#92;epsilon) &#92;sim &#92;frac{1}{3}(&#92;log_2 n)}" class="latex" />, the circuits <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde%7BC%7D%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;tilde{C}}" class="latex" /> lose the Shor property, meaning that <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+A%7D%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{{&#92;cal A}}" class="latex" /> sampling <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde%7BC%7D%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;tilde{C}}" class="latex" /> cannot find <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;rho}" class="latex" />. </em>
</p></blockquote>
<p><p>
This says that the noise range brushes against the Coppersmith upper bound for the precision needed to implement Shor&#8217;s algorithm. Since <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{k}" class="latex" /> is exponentiated, one can say that noise on the order of the cube of the precision needed for Shor&#8217;s algorithm is enough to destroy it. </p>
<p>
The estimates in the paper allow replacing <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B3%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;frac{1}{3}}" class="latex" /> by <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%2B%5Cdelta%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;frac{1}{2+&#92;delta}}" class="latex" /> with greater attention to additive constants, so lower noise approaching the square root of the Coppersmith precision suffices to destroy the Shor property. This may be improvable to almost linear. Exactly what does the noise attack? That&#8217;s next.</p>
<p>
<p><H2> Long Periods </H2></p>
<p><p>
The noise most strongly affects cases <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BN+%3D+pq%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{N = pq}" class="latex" /> where <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bp-1%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{p-1}" class="latex" /> and <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bq-1%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{q-1}" class="latex" /> have a large prime factor. The most extreme such case is <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Bp+-+1%7D%7B2%7D+%3D+p%27%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;frac{p - 1}{2} = p&#039;}" class="latex" /> being prime. Then <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bp%27%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{p&#039;}" class="latex" /> is called a <a href="https://en.wikipedia.org/wiki/Safe_and_Sophie_Germain_primes">Sophie Germain prime</a>. Ironically, <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bp+%3D+2p%27%2B1%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{p = 2p&#039;+1}" class="latex" /> is called a &#8220;safe prime&#8221; but those are the most unsafe under Jin-Yi&#8217;s noise.</p>
<p>
It remains unknown whether infinitely many Sophie Germain primes exist, despite the quest <a href="https://en.wikipedia.org/wiki/Proof_(play)">winning</a> a Tony Award and Pulitzer Prize. But a less-heralded property suffices. &Eacute;tienne Fouvry <a href="https://eudml.org/doc/143202">proved</a> in 1985 that the set of primes <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{p}" class="latex" /> for which <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bp+-+1%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{p - 1}" class="latex" /> has a factor <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bp%27+%3E+p%5E%7B2%2F3%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{p&#039; &gt; p^{2/3}}" class="latex" /> is not only infinite, but has positive density in the set of primes. It follows that cases <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BN+%3D+pq%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{N = pq}" class="latex" /> where both <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{p}" class="latex" /> and <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{q}" class="latex" /> have this &#8220;Fouvry property&#8221; have positive density among products of two primes. There can be only one prime factor <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bp%27+%3E+p%5E%7B2%2F3%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{p&#039; &gt; p^{2/3}}" class="latex" />, likewise <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bq%27+%3E+q%5E%7B2%2F3%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{q&#039; &gt; q^{2/3}}" class="latex" />. </p>
<p>
The upshot for such <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{p}" class="latex" /> and <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{q}" class="latex" /> is that most <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{a}" class="latex" /> have exponentially long periods <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;rho}" class="latex" /> modulo <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BN%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{N}" class="latex" />. The geometric sums that concentrate amplitudes on multiples of <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;rho}" class="latex" /> in the ideal situation, when the circuit is sampled via quantum measurement, have norm-squared proportional to <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;rho}" class="latex" />. In the noisy situation, such length maximizes the perturbative effect of the noise so as to level out the amplitude. This destroys the ability to infer <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;rho}" class="latex" />.</p>
<p>
We cut a few corners in the statements of Jin-Yi&#8217;s theorems, but they are reasonably close and the paper has full details. They hold also under the <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+b%27%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{k = b&#039;}" class="latex" /> variant and with-or-without removing controlled <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BR_k%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{R_k}" class="latex" /> gates for <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bk+%3E+b%27%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{k &gt; b&#039;}" class="latex" />.</p>
<blockquote><p><b>Theorem 1</b> <em><a name="Fouvry"></a> Asymptotically as <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Crightarrow+%5Cinfty%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n &#92;rightarrow &#92;infty}" class="latex" />, if <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BN+%3D+pq%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{N = pq}" class="latex" /> is an <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n}" class="latex" />-bit product of two Fouvry primes, and <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bb%27+%2B+%5Clog%281%2F%5Cepsilon%29+%3C+%5Cfrac%7B1%7D%7B3%7D%5Clog_2+n+-+%5CTheta%281%29%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{b&#039; + &#92;log(1/&#92;epsilon) &lt; &#92;frac{1}{3}&#92;log_2 n - &#92;Theta(1)}" class="latex" />, then the probability that <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+A%7D%28%5Ctilde%7BC%7D%29%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{{&#92;cal A}(&#92;tilde{C})}" class="latex" /> infers <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;rho}" class="latex" /> is exponentially small. </em>
</p></blockquote>
<p>
<blockquote><p><b>Theorem 2</b> <em><a name="random"></a> Asymptotically as <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Crightarrow+%5Cinfty%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n &#92;rightarrow &#92;infty}" class="latex" />, for all but a vanishing fraction of <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n}" class="latex" />-bit primes and with <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bb%27+%2B+%5Clog%281%2F%5Cepsilon%29+%3C+%5Cfrac%7B1%7D%7B3%7D%5Clog_2+n+-+%5CTheta%281%29%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{b&#039; + &#92;log(1/&#92;epsilon) &lt; &#92;frac{1}{3}&#92;log_2 n - &#92;Theta(1)}" class="latex" />, the probability over <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{a}" class="latex" /> and noisy <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde%7BC%7D%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;tilde{C}}" class="latex" /> that <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+A%7D%28%5Ctilde%7BC%7D%29%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{{&#92;cal A}(&#92;tilde{C})}" class="latex" /> infers <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;rho}" class="latex" /> is exponentially small. </em>
</p></blockquote>
<p><p>
Theorem <a href="#random">2</a>, whose proof is in the paper&#8217;s appendix, says that Shor&#8217;s algorithm fails to survive the noise in all but a vanishing fraction of instances. It applies also under certain restrictions of the primes, such as <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{p}" class="latex" /> and <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{q}" class="latex" /> both being congruent to 3 modulo 4. Theorem <a href="#Fouvry">1</a> gives a substantial explicit set of cases <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BN+%3D+pq%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{N = pq}" class="latex" /> on which the algorithm fails.</p>
<p>
<p><H2> How General Is This? </H2></p>
<p><p>
The theorems are carefully stated in terms of the period-inferencing component of Shor&#8217;s algorithm. And they are asymptotic. They do not rule out:</p>
<ul>
<li>
possible quantum improvements on input sizes in the finite range of conceivable practical crypto; </p>
<li>
quantum circuits <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BD%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{D}" class="latex" /> that might factor by other means; nor </p>
<li>
that error correction might restore the Shor property.
</ul>
<p>
In particular, they do not define a general-purpose noise model that could apply to any quantum circuit <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BD%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{D}" class="latex" />. </p>
<p>
Now we discuss two means to implement Shor&#8217;s algorithm without using gates beyond <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BR_3%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{R_3}" class="latex" />:</p>
<ol>
<li>
The Hadamard gate <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BH%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{H}" class="latex" />, the controlled-not gate CNOT, and the gate <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BT+%3D+R_3%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{T = R_3}" class="latex" /> form a <em>complete set</em> that (by the Solovay-Kitaev <a href="https://en.wikipedia.org/wiki/Solovay-Kitaev_theorem">theorem</a>) can feasibly approximate the state produced by any feasible quantum circuit plus QFT. Then the minimum angle of any individual operation is <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%2F8%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;pi/8}" class="latex" />. </p>
<li>
The Hadamard and <a href="https://en.wikipedia.org/wiki/Toffoli_gate">Toffoli</a> gates form a universal set in the weaker sense of encoding real and imaginary parts of quantum amplitudes separately. This suffices to compute the factoring function via polynomial-size circuits using only real entries.
</ol>
<p>
Idea 1 may only mask the issue, insofar as the resulting circuits must still approximate angles down to Coppersmith&#8217;s unboundedly small magnitude <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B-b%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{2^{-b}}" class="latex" />. Both <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BH%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{H}" class="latex" /> and <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BT%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{T}" class="latex" /> are rotations of the <a href="https://en.wikipedia.org/wiki/Bloch_sphere">Bloch sphere</a> of periods 2 and 8, respectively. As such, each may be exactly physically realizable, along with their controlled versions and CNOT in higher-dimensional Bloch spheres. </p>
<p>
However, <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BH%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{H}" class="latex" /> and <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BT%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{T}" class="latex" /> together generate an infinite subgroup of SU(2). The group has members that rotate through arbitrarily small angles. Jin-Yi says in his speculative concluding section:</p>
<blockquote><p><b> </b> <em> It is true that using a fixed finite set of rotations of reasonable angles such as <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%2F8%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;pi/8}" class="latex" /> along various axes <b>can</b> compose to rotations of arbitrarily small angles. But my view is just that these compositional rules as specified by the group SU(2) must not be exact for physical reality. </em>
</p></blockquote>
<p><p>
Most in particular, let <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BA+%3D+HT%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{A = HT}" class="latex" />. If <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BA%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{A}" class="latex" /> can be exactly realized, then any power <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BAA%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{AA}" class="latex" />, <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BAAA%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{AAA}" class="latex" />, &#8230; should be. But the angle of <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BA%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{A}" class="latex" /> is not a rational multiple of <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;pi}" class="latex" />, so the powers alone form an infinite state space and include arbitrarily tiny rotations. Please see Jin-Yi&#8217;s <a href="https://pages.cs.wisc.edu/~jyc/Shor-Algorithm-with-Noise.pdf">paper</a> for other context and justifications on these points, plus related <a href="https://spectrum.ieee.org/the-case-against-quantum-computing">contentions</a> <a href="https://arxiv.org/ftp/arxiv/papers/1401/1401.3629.pdf">by</a> Mikhail Dyakonov.</p>
<p>
The circuits in idea 2 cannot approximate any (feasible) quantum state metrically, but they can emulate Shor&#8217;s algorithm using only <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B0%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{0}" class="latex" /> and <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;pi}" class="latex" /> as &#8220;angles.&#8221; They may, however, still involve quantum states with filigrees beyond physically realizable precision. In the coda to our own textbook, we speculate this already for the deterministic &#8220;functional superposition&#8221; component of Shor&#8217;s algorithm.</p>
<p>
All this and more was discussed already twenty-plus years ago in the &#8220;<a href="http://www.scottaaronson.com/papers/mlinsiam.pdf">Sure/Shor</a> <a href="https://www.scottaaronson.com/democritus/lec14.html">separator</a>&#8221; <a href="https://scottaaronson.blog/?p=124">debate</a>. The difference now is having Jin-Yi&#8217;s new work as a linchpin for the skeptical side. Non-robustness to noise in the &#8220;Coppersmith range&#8221; may be a wider phenomenon than his current results show.</p>
<p>
In his last paragraph, Jin-Yi argues that quantum computing makes a fundamental departure from Alan Turing&#8217;s condition that primitive steps are finite and fixed independent of the data size <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n}" class="latex" />. He mentions the free use of SU(2) but his point may apply as well to the step of placing a Toffoli gate anywhere in an <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n}" class="latex" />-qubit quantum circuit. This point is separate from issues of noise models, about which we have heard much from Gil Kalai including <a href="https://rjlipton.wpcomstaging.com/2023/02/11/are-we-nuts/">recently</a>.</p>
<p>
<p><H2> Open Problems </H2></p>
<p><p>
The issue is simple: Can quantum algorithms be made to work in the presence of gates that are making errors at Jin-Yi&#8217;s scaling? The obvious interesting open question is: As in classical computation, can we build circuits that can handle errors? See <a href="https://e3s-center.berkeley.edu/wp-content/uploads/2017/07/PEB2012_12_TSzkopek_Webfinal.pdf">this</a> and <a href="https://cosmosmagazine.com/technology/error-free-quantum-computer/">this</a> on error-free computation. </p>
<p>
This seems to be a wonderful question. Will the new results reshape debates on quantum computing and the polynomial Church-Turing thesis, or are they subsumed in <a href="https://windowsontheory.org/2017/10/30/the-different-forms-of-quantum-computing-skepticism/">matters</a> <a href="https://cacm.acm.org/magazines/2019/5/236426-quantum-hype-and-quantum-skepticism/fulltext?mobile=false">already</a> <a href="https://unfashionable.blog/p/quantum">recently</a> <a href="https://www.technologyreview.com/2022/03/28/1048355/quantum-computing-has-a-hype-problem/">much</a> <a href="https://www.hpcwire.com/2023/05/02/microsoft-eth-take-aim-at-quantum-computings-hype-and-promise/">discussed</a>?</p>
<p><P><br />
[added update about Gidney-Eker&aring; paper in third section]</p>
<p class="authors">By RJLipton+KWRegan</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-14T04:16:28Z">Wednesday, June 14 2023, 04:16</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.07583'>Invertible Bloom Lookup Tables with Less Memory and Less Randomness</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Nils Fleischhacker, Kasper Green Larsen, Maciej Obremski, Mark Simkin</p><p>In this work we study Invertible Bloom Lookup Tables (IBLTs) with small
failure probabilities. IBLTs are highly versatile data structures that have
found applications in set reconciliation protocols, error-correcting codes, and
even the design of advanced cryptographic primitives. For storing $n$ elements
and ensuring correctness with probability at least $1 - \delta$, existing IBLT
constructions require $\Omega(n(\frac{\log(1/\delta)}{\log(n)}+1))$ space and
they crucially rely on fully random hash functions.
</p>
<p>We present new constructions of IBLTs that are simultaneously more space
efficient and require less randomness. For storing $n$ elements with a failure
probability of at most $\delta$, our data structure only requires
$\mathcal{O}(n + \log(1/\delta)\log\log(1/\delta))$ space and
$\mathcal{O}(\log(\log(n)/\delta))$-wise independent hash functions.
</p>
<p>As a key technical ingredient we show that hashing $n$ keys with any $k$-wise
independent hash function $h:U \to [Cn]$ for some sufficiently large constant
$C$ guarantees with probability $1 - 2^{-\Omega(k)}$ that at least $n/2$ keys
will have a unique hash value. Proving this is highly non-trivial as $k$
approaches $n$. We believe that the techniques used to prove this statement may
be of independent interest.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Fleischhacker_N/0/1/0/all/0/1">Nils Fleischhacker</a>, <a href="http://arxiv.org/find/cs/1/au:+Larsen_K/0/1/0/all/0/1">Kasper Green Larsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Obremski_M/0/1/0/all/0/1">Maciej Obremski</a>, <a href="http://arxiv.org/find/cs/1/au:+Simkin_M/0/1/0/all/0/1">Mark Simkin</a></p><p>In this work we study Invertible Bloom Lookup Tables (IBLTs) with small
failure probabilities. IBLTs are highly versatile data structures that have
found applications in set reconciliation protocols, error-correcting codes, and
even the design of advanced cryptographic primitives. For storing $n$ elements
and ensuring correctness with probability at least $1 - \delta$, existing IBLT
constructions require $\Omega(n(\frac{\log(1/\delta)}{\log(n)}+1))$ space and
they crucially rely on fully random hash functions.
</p>
<p>We present new constructions of IBLTs that are simultaneously more space
efficient and require less randomness. For storing $n$ elements with a failure
probability of at most $\delta$, our data structure only requires
$\mathcal{O}(n + \log(1/\delta)\log\log(1/\delta))$ space and
$\mathcal{O}(\log(\log(n)/\delta))$-wise independent hash functions.
</p>
<p>As a key technical ingredient we show that hashing $n$ keys with any $k$-wise
independent hash function $h:U \to [Cn]$ for some sufficiently large constant
$C$ guarantees with probability $1 - 2^{-\Omega(k)}$ that at least $n/2$ keys
will have a unique hash value. Proving this is highly non-trivial as $k$
approaches $n$. We believe that the techniques used to prove this statement may
be of independent interest.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-14T00:30:00Z">Wednesday, June 14 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.07674'>Differentially Private One Permutation Hashing and Bin-wise Consistent Weighted Sampling</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xiaoyun Li, Ping Li</p><p>Minwise hashing (MinHash) is a standard algorithm widely used in the
industry, for large-scale search and learning applications with the binary
(0/1) Jaccard similarity. One common use of MinHash is for processing massive
n-gram text representations so that practitioners do not have to materialize
the original data (which would be prohibitive). Another popular use of MinHash
is for building hash tables to enable sub-linear time approximate near neighbor
(ANN) search. MinHash has also been used as a tool for building large-scale
machine learning systems. The standard implementation of MinHash requires
applying $K$ random permutations. In comparison, the method of one permutation
hashing (OPH), is an efficient alternative of MinHash which splits the data
vectors into $K$ bins and generates hash values within each bin. OPH is
substantially more efficient and also more convenient to use.
</p>
<p>In this paper, we combine the differential privacy (DP) with OPH (as well as
MinHash), to propose the DP-OPH framework with three variants: DP-OPH-fix,
DP-OPH-re and DP-OPH-rand, depending on which densification strategy is adopted
to deal with empty bins in OPH. A detailed roadmap to the algorithm design is
presented along with the privacy analysis. An analytical comparison of our
proposed DP-OPH methods with the DP minwise hashing (DP-MH) is provided to
justify the advantage of DP-OPH. Experiments on similarity search confirm the
merits of DP-OPH, and guide the choice of the proper variant in different
practical scenarios. Our technique is also extended to bin-wise consistent
weighted sampling (BCWS) to develop a new DP algorithm called DP-BCWS for
non-binary data. Experiments on classification tasks demonstrate that DP-BCWS
is able to achieve excellent utility at around $\epsilon = 5\sim 10$, where
$\epsilon$ is the standard parameter in the language of $(\epsilon,
\delta)$-DP.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1">Xiaoyun Li</a>, <a href="http://arxiv.org/find/stat/1/au:+Li_P/0/1/0/all/0/1">Ping Li</a></p><p>Minwise hashing (MinHash) is a standard algorithm widely used in the
industry, for large-scale search and learning applications with the binary
(0/1) Jaccard similarity. One common use of MinHash is for processing massive
n-gram text representations so that practitioners do not have to materialize
the original data (which would be prohibitive). Another popular use of MinHash
is for building hash tables to enable sub-linear time approximate near neighbor
(ANN) search. MinHash has also been used as a tool for building large-scale
machine learning systems. The standard implementation of MinHash requires
applying $K$ random permutations. In comparison, the method of one permutation
hashing (OPH), is an efficient alternative of MinHash which splits the data
vectors into $K$ bins and generates hash values within each bin. OPH is
substantially more efficient and also more convenient to use.
</p>
<p>In this paper, we combine the differential privacy (DP) with OPH (as well as
MinHash), to propose the DP-OPH framework with three variants: DP-OPH-fix,
DP-OPH-re and DP-OPH-rand, depending on which densification strategy is adopted
to deal with empty bins in OPH. A detailed roadmap to the algorithm design is
presented along with the privacy analysis. An analytical comparison of our
proposed DP-OPH methods with the DP minwise hashing (DP-MH) is provided to
justify the advantage of DP-OPH. Experiments on similarity search confirm the
merits of DP-OPH, and guide the choice of the proper variant in different
practical scenarios. Our technique is also extended to bin-wise consistent
weighted sampling (BCWS) to develop a new DP algorithm called DP-BCWS for
non-binary data. Experiments on classification tasks demonstrate that DP-BCWS
is able to achieve excellent utility at around $\epsilon = 5\sim 10$, where
$\epsilon$ is the standard parameter in the language of $(\epsilon,
\delta)$-DP.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-14T00:30:00Z">Wednesday, June 14 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.07872'>Expanding the Scope of DAWN: A Novel Version for Weighted Shortest Path Problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yelai Feng</p><p>The shortest path problem is a typical problem in graph theory with wide
potential applications. The state-of-the-art single-source shortest paths
algorithm on the weight graph is the $\Delta$-stepping algorithm, which can
efficiently process weighted graphs in parallel. DAWN is an algorithm that
addresses the shortest path problem on unweighted graphs, and we propose a
weighted version that can handle graphs with weights edges, while maintaining
the high scalability and parallelism features as DAWN. The novel version
requires $O(\mu m)$ and $O(\mu \cdot E_{wcc})$ times on the connected and
unconnected graphs for SSSP problems, respectively. $E_{wcc}$ denote the number
of edges included in the largest weakly connected component, and $\mu$ is a
constant denoting the average number of path transformations in the tasks. We
tested the weighted version on the real graphs from Stanford Network Analysis
Platform and SuiteSparse Matrix Collection, which outperformed the solution of
$\Delta$-stepping algorithm from Gunrock, achieving a speedup of
43.163$\times$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yelai Feng</a></p><p>The shortest path problem is a typical problem in graph theory with wide
potential applications. The state-of-the-art single-source shortest paths
algorithm on the weight graph is the $\Delta$-stepping algorithm, which can
efficiently process weighted graphs in parallel. DAWN is an algorithm that
addresses the shortest path problem on unweighted graphs, and we propose a
weighted version that can handle graphs with weights edges, while maintaining
the high scalability and parallelism features as DAWN. The novel version
requires $O(\mu m)$ and $O(\mu \cdot E_{wcc})$ times on the connected and
unconnected graphs for SSSP problems, respectively. $E_{wcc}$ denote the number
of edges included in the largest weakly connected component, and $\mu$ is a
constant denoting the average number of path transformations in the tasks. We
tested the weighted version on the real graphs from Stanford Network Analysis
Platform and SuiteSparse Matrix Collection, which outperformed the solution of
$\Delta$-stepping algorithm from Gunrock, achieving a speedup of
43.163$\times$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-14T00:30:00Z">Wednesday, June 14 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.07884'>Continual Release of Differentially Private Synthetic Data</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mark Bun, Marco Gaboardi, Marcel Neunhoeffer, Wanrong Zhang</p><p>Motivated by privacy concerns in long-term longitudinal studies in medical
and social science research, we study the problem of continually releasing
differentially private synthetic data. We introduce a model where, in every
time step, each individual reports a new data element, and the goal of the
synthesizer is to incrementally update a synthetic dataset to capture a rich
class of statistical properties. We give continual synthetic data generation
algorithms that preserve two basic types of queries: fixed time window queries
and cumulative time queries. We show nearly tight upper bounds on the error
rates of these algorithms and demonstrate their empirical performance on
realistically sized datasets from the U.S. Census Bureau's Survey of Income and
Program Participation.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bun_M/0/1/0/all/0/1">Mark Bun</a>, <a href="http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1">Marco Gaboardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Neunhoeffer_M/0/1/0/all/0/1">Marcel Neunhoeffer</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wanrong Zhang</a></p><p>Motivated by privacy concerns in long-term longitudinal studies in medical
and social science research, we study the problem of continually releasing
differentially private synthetic data. We introduce a model where, in every
time step, each individual reports a new data element, and the goal of the
synthesizer is to incrementally update a synthetic dataset to capture a rich
class of statistical properties. We give continual synthetic data generation
algorithms that preserve two basic types of queries: fixed time window queries
and cumulative time queries. We show nearly tight upper bounds on the error
rates of these algorithms and demonstrate their empirical performance on
realistically sized datasets from the U.S. Census Bureau's Survey of Income and
Program Participation.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-14T00:30:00Z">Wednesday, June 14 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.07891'>Online Matching in Geometric Random Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Flore Sentenac, Nathan Noiry, Matthieu Lerasle, Laurent M&#xe9;nard, Vianney Perchet</p><p>In online advertisement, ad campaigns are sequentially displayed to users.
Both users and campaigns have inherent features, and the former is eligible to
the latter if they are ``similar enough''. We model these interactions as a
bipartite geometric random graph: the features of the $2N$ vertices ($N$ users
and $N$ campaigns) are drawn independently in a metric space and an edge is
present between a campaign and a user node if the distance between their
features is smaller than $c/N$, where $c&gt;0$ is the parameter of the model. Our
contributions are two-fold. In the one-dimensional case, with uniform
distribution over the segment $[0,1]$, we derive the size of the optimal
offline matching in these bipartite random geometric graphs, and we build an
algorithm achieving it (as a benchmark), and analyze precisely its performance.
We then turn to the online setting where one side of the graph is known at the
beginning while the other part is revealed sequentially. We study the number of
matches of the online algorithm closest, which matches any incoming point to
its closest available neighbor. We show that its performances can be compared
to its fluid limit, completely described as the solution of an explicit PDE.
From the latter, we can compute the competitive ratio of closest.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Sentenac_F/0/1/0/all/0/1">Flore Sentenac</a>, <a href="http://arxiv.org/find/cs/1/au:+Noiry_N/0/1/0/all/0/1">Nathan Noiry</a>, <a href="http://arxiv.org/find/cs/1/au:+Lerasle_M/0/1/0/all/0/1">Matthieu Lerasle</a>, <a href="http://arxiv.org/find/cs/1/au:+Menard_L/0/1/0/all/0/1">Laurent M&#xe9;nard</a>, <a href="http://arxiv.org/find/cs/1/au:+Perchet_V/0/1/0/all/0/1">Vianney Perchet</a></p><p>In online advertisement, ad campaigns are sequentially displayed to users.
Both users and campaigns have inherent features, and the former is eligible to
the latter if they are ``similar enough''. We model these interactions as a
bipartite geometric random graph: the features of the $2N$ vertices ($N$ users
and $N$ campaigns) are drawn independently in a metric space and an edge is
present between a campaign and a user node if the distance between their
features is smaller than $c/N$, where $c&gt;0$ is the parameter of the model. Our
contributions are two-fold. In the one-dimensional case, with uniform
distribution over the segment $[0,1]$, we derive the size of the optimal
offline matching in these bipartite random geometric graphs, and we build an
algorithm achieving it (as a benchmark), and analyze precisely its performance.
We then turn to the online setting where one side of the graph is known at the
beginning while the other part is revealed sequentially. We study the number of
matches of the online algorithm closest, which matches any incoming point to
its closest available neighbor. We show that its performances can be compared
to its fluid limit, completely described as the solution of an explicit PDE.
From the latter, we can compute the competitive ratio of closest.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-14T00:30:00Z">Wednesday, June 14 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.07892'>Robustly Learning a Single Neuron via Sharpness</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Puqian Wang, Nikos Zarifis, Ilias Diakonikolas, Jelena Diakonikolas</p><p>We study the problem of learning a single neuron with respect to the
$L_2^2$-loss in the presence of adversarial label noise. We give an efficient
algorithm that, for a broad family of activations including ReLUs, approximates
the optimal $L_2^2$-error within a constant factor. Our algorithm applies under
much milder distributional assumptions compared to prior work. The key
ingredient enabling our results is a novel connection to local error bounds
from optimization theory.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Puqian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zarifis_N/0/1/0/all/0/1">Nikos Zarifis</a>, <a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1">Ilias Diakonikolas</a>, <a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_J/0/1/0/all/0/1">Jelena Diakonikolas</a></p><p>We study the problem of learning a single neuron with respect to the
$L_2^2$-loss in the presence of adversarial label noise. We give an efficient
algorithm that, for a broad family of activations including ReLUs, approximates
the optimal $L_2^2$-error within a constant factor. Our algorithm applies under
much milder distributional assumptions compared to prior work. The key
ingredient enabling our results is a novel connection to local error bounds
from optimization theory.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-14T00:30:00Z">Wednesday, June 14 2023, 00:30</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
