<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.5 (2022-11-24) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2022-12-20T19:30:35Z">Tuesday, December 20 2022, 19:30</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://minorfree.github.io/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://minorfree.github.io'>Hung Le</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>Sébastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Tuesday, December 20
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/12/20/arc-postdoctoral-fellowship-at-georgia-institute-of-technology-apply-by-january-6-2023/'>ARC Postdoctoral Fellowship at Georgia Institute of Technology (apply by January 6, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The Algorithms and Randomness Center at Georgia Tech is seeking multiple postdoctoral fellows starting August 1, 2023. Qualified applicants must possess a PhD in Computer Science, Mathematics, Operations Research or a related field. Please send a CV, research statement, and contacts for three references to arc-postdoc@cc.gatech.edu by January 6, 2023 for full consideration. Website: arc.gatech.edu/post-doc2023 [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The Algorithms and Randomness Center at Georgia Tech is seeking multiple postdoctoral fellows starting August 1, 2023. Qualified applicants must possess a PhD in Computer Science, Mathematics, Operations Research or a related field. Please send a CV, research statement, and contacts for three references to arc-postdoc@cc.gatech.edu by January 6, 2023 for full consideration.</p>
<p>Website: <a href="https://arc.gatech.edu/post-doc2023">https://arc.gatech.edu/post-doc2023</a><br />
Email: arc-postdoc@cc.gatech.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-20T18:42:45Z">Tuesday, December 20 2022, 18:42</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/12/20/ibm-2023-24-herman-goldstine-memorial-postdoctoral-fellowship-at-ibm-research-apply-by-december-31-2022/'>IBM 2023-24 Herman Goldstine Memorial Postdoctoral Fellowship at IBM Research (apply by December 31, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The Mathematical Sciences department of IBM Research invites applications for its 2023–2024 Herman Goldstine Memorial Postdoctoral Fellowship for research in the mathematical and computer sciences. The department provides an atmosphere in which basic research is combined with work on practical applications. Areas of interest include theoretical computer science and optimization. Website: research.ibm.com/goldstine/ Email: gpfellow@ibm.com
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The Mathematical Sciences department of IBM Research invites applications for its 2023–2024 Herman Goldstine Memorial Postdoctoral Fellowship for research in the mathematical and computer sciences. The department provides an atmosphere in which basic research is combined with work on practical applications. Areas of interest include theoretical computer science and optimization.</p>
<p>Website: <a href="https://research.ibm.com/goldstine/">https://research.ibm.com/goldstine/</a><br />
Email: gpfellow@ibm.com</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-20T15:33:22Z">Tuesday, December 20 2022, 15:33</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.08678'>A super-polynomial quantum advantage for combinatorial optimization problems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Niklas Pirnay, Vincent Ulitzsch, Frederik Wilde, Jens Eisert, Jean-Pierre Seifert</p><p>Combinatorial optimization - a field of research addressing problems that
feature strongly in a wealth of practical and industrial contexts - has been
identified as one of the core potential fields of applicability of near-term
quantum computers. It is still unclear, however, to what extent variational
quantum algorithms can actually outperform classical algorithms for this type
of problems. In this work, by resorting to computational learning theory and
cryptographic notions, we prove that fault-tolerant quantum computers feature a
super-polynomial advantage over classical computers in approximating solutions
to combinatorial optimization problems. Specifically, building on seminal work
of Kearns and Valiant, we construct special instances of the integer
programming problem (which in its most general form is NP-complete) that we
prove to be hard-to-approximate classically but give an efficient quantum
algorithm to approximate the optimal solution of those instances, hence showing
a super-polynomial quantum advantage. This result shows that quantum devices
have the power to approximate combinatorial optimization solutions beyond the
reach of classical efficient algorithms.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Pirnay_N/0/1/0/all/0/1">Niklas Pirnay</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Ulitzsch_V/0/1/0/all/0/1">Vincent Ulitzsch</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Wilde_F/0/1/0/all/0/1">Frederik Wilde</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Eisert_J/0/1/0/all/0/1">Jens Eisert</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Seifert_J/0/1/0/all/0/1">Jean-Pierre Seifert</a></p><p>Combinatorial optimization - a field of research addressing problems that
feature strongly in a wealth of practical and industrial contexts - has been
identified as one of the core potential fields of applicability of near-term
quantum computers. It is still unclear, however, to what extent variational
quantum algorithms can actually outperform classical algorithms for this type
of problems. In this work, by resorting to computational learning theory and
cryptographic notions, we prove that fault-tolerant quantum computers feature a
super-polynomial advantage over classical computers in approximating solutions
to combinatorial optimization problems. Specifically, building on seminal work
of Kearns and Valiant, we construct special instances of the integer
programming problem (which in its most general form is NP-complete) that we
prove to be hard-to-approximate classically but give an efficient quantum
algorithm to approximate the optimal solution of those instances, hence showing
a super-polynomial quantum advantage. This result shows that quantum devices
have the power to approximate combinatorial optimization solutions beyond the
reach of classical efficient algorithms.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-20T01:30:00Z">Tuesday, December 20 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.08709'>On the Complexities of Understanding Matching Mechanisms</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yannai A. Gonczarowski, Clayton Thomas</p><p>We study various novel complexity measures for two-sided matching mechanisms,
applied to the popular real-world school choice mechanisms of Deferred
Acceptance (DA) and Top Trading Cycles (TTC). In contrast to typical bounds in
computer science, our metrics are not aimed to capture how hard the mechanisms
are to compute. Rather, they aim to capture certain aspects of the difficulty
of understanding or explaining the mechanisms and their properties.
</p>
<p>First, we study a set of questions regarding the complexity of how one
agent's report can affect other facets of the mechanism. We show that in both
DA and TTC, one agent's report can have a structurally complex effect on the
final matching. Considering how one agent's report can affect another agent's
set of obtainable options, we show that this effect has high complexity for
TTC, but low complexity for DA, showing that one agent can only affect another
in DA in a quantitatively controlled way.
</p>
<p>Second, we study a set of questions about the complexity of communicating
various facets of the outcome matching, after calculating it. We find that when
there are many more students than schools, it is provably harder to
concurrently describe to each student her match in TTC than in DA. In contrast,
we show that the outcomes of TTC and DA are equally hard to jointly verify, and
that all agents' sets of obtainable options are equally hard to describe,
showcasing ways in which the two mechanisms are comparably complex.
</p>
<p>Our results uncover new lenses into how TTC may be more complex than DA. This
stands in contrast with recent results under different models, emphasizing the
richness of the landscape of complexities of matching mechanisms. Our proofs
uncover novel structural properties of TTC and DA, which may be of independent
interest.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gonczarowski_Y/0/1/0/all/0/1">Yannai A. Gonczarowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomas_C/0/1/0/all/0/1">Clayton Thomas</a></p><p>We study various novel complexity measures for two-sided matching mechanisms,
applied to the popular real-world school choice mechanisms of Deferred
Acceptance (DA) and Top Trading Cycles (TTC). In contrast to typical bounds in
computer science, our metrics are not aimed to capture how hard the mechanisms
are to compute. Rather, they aim to capture certain aspects of the difficulty
of understanding or explaining the mechanisms and their properties.
</p>
<p>First, we study a set of questions regarding the complexity of how one
agent's report can affect other facets of the mechanism. We show that in both
DA and TTC, one agent's report can have a structurally complex effect on the
final matching. Considering how one agent's report can affect another agent's
set of obtainable options, we show that this effect has high complexity for
TTC, but low complexity for DA, showing that one agent can only affect another
in DA in a quantitatively controlled way.
</p>
<p>Second, we study a set of questions about the complexity of communicating
various facets of the outcome matching, after calculating it. We find that when
there are many more students than schools, it is provably harder to
concurrently describe to each student her match in TTC than in DA. In contrast,
we show that the outcomes of TTC and DA are equally hard to jointly verify, and
that all agents' sets of obtainable options are equally hard to describe,
showcasing ways in which the two mechanisms are comparably complex.
</p>
<p>Our results uncover new lenses into how TTC may be more complex than DA. This
stands in contrast with recent results under different models, emphasizing the
richness of the landscape of complexities of matching mechanisms. Our proofs
uncover novel structural properties of TTC and DA, which may be of independent
interest.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-20T01:30:00Z">Tuesday, December 20 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.09285'>Localizability of the approximation method</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jan Pich</p><p>We use the approximation method of Razborov to analyze the locality barrier
which arose from the investigation of the hardness magnification approach to
complexity lower bounds. Adapting a limitation of the approximation method
obtained by Razborov, we show that in many cases it is not possible to combine
the approximation method with typical (localizable) hardness magnification
theorems to derive strong circuit lower bounds. In particular, one cannot use
the approximation method to derive an extremely strong constant-depth circuit
lower bound and then magnify it to an $NC^1$ lower bound for an explicit
function.
</p>
<p>To prove this we show that lower bounds obtained by the approximation method
are in many cases localizable in the sense that they imply lower bounds for
circuits which are allowed to use arbitrarily powerful oracles with small
fan-in.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Pich_J/0/1/0/all/0/1">Jan Pich</a></p><p>We use the approximation method of Razborov to analyze the locality barrier
which arose from the investigation of the hardness magnification approach to
complexity lower bounds. Adapting a limitation of the approximation method
obtained by Razborov, we show that in many cases it is not possible to combine
the approximation method with typical (localizable) hardness magnification
theorems to derive strong circuit lower bounds. In particular, one cannot use
the approximation method to derive an extremely strong constant-depth circuit
lower bound and then magnify it to an $NC^1$ lower bound for an explicit
function.
</p>
<p>To prove this we show that lower bounds obtained by the approximation method
are in many cases localizable in the sense that they imply lower bounds for
circuits which are allowed to use arbitrarily powerful oracles with small
fan-in.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-20T01:30:00Z">Tuesday, December 20 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.09029'>SurfaceVoronoi: Efficiently Computing Voronoi Diagrams over Mesh Surfaces with Arbitrary Distance Solvers</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Shiqing Xin, Pengfei Wang, Rui Xu, Dongming Yan, Shuangmin Chen, Wenping Wang, Caiming Zhang, Changhe Tu,</p><p>In this paper, we propose to compute Voronoi diagrams over mesh surfaces
driven by an arbitrary geodesic distance solver, assuming that the input is a
triangle mesh as well as a collection of sites $P=\{p_i\}_{i=1}^m$ on the
surface. We propose two key techniques to solve this problem. First, as the
partition is determined by minimizing the $m$ distance fields, each of which
rooted at a source site, we suggest keeping one or more distance triples, for
each triangle, that may help determine the Voronoi bisectors when one uses a
mark-and-sweep geodesic algorithm to predict the multi-source distance field.
Second, rather than keep the distance itself at a mesh vertex, we use the
squared distance to characterize the linear change of distance field restricted
in a triangle, which is proved to induce an exact VD when the base surface
reduces to a planar triangle mesh. Specially, our algorithm also supports the
Euclidean distance, which can handle thin-sheet models (e.g. leaf) and runs
faster than the traditional restricted Voronoi diagram~(RVD) algorithm. It is
very extensible to deal with various variants of surface-based Voronoi diagrams
including (1)surface-based power diagram, (2)constrained Voronoi diagram with
curve-type breaklines, and (3)curve-type generators. We conduct extensive
experimental results to validate the ability to approximate the exact VD in
different distance-driven scenarios.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Xin_S/0/1/0/all/0/1">Shiqing Xin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Pengfei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Rui Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1">Dongming Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shuangmin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Caiming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_C/0/1/0/all/0/1">Changhe Tu</a>,</p><p>In this paper, we propose to compute Voronoi diagrams over mesh surfaces
driven by an arbitrary geodesic distance solver, assuming that the input is a
triangle mesh as well as a collection of sites $P=\{p_i\}_{i=1}^m$ on the
surface. We propose two key techniques to solve this problem. First, as the
partition is determined by minimizing the $m$ distance fields, each of which
rooted at a source site, we suggest keeping one or more distance triples, for
each triangle, that may help determine the Voronoi bisectors when one uses a
mark-and-sweep geodesic algorithm to predict the multi-source distance field.
Second, rather than keep the distance itself at a mesh vertex, we use the
squared distance to characterize the linear change of distance field restricted
in a triangle, which is proved to induce an exact VD when the base surface
reduces to a planar triangle mesh. Specially, our algorithm also supports the
Euclidean distance, which can handle thin-sheet models (e.g. leaf) and runs
faster than the traditional restricted Voronoi diagram~(RVD) algorithm. It is
very extensible to deal with various variants of surface-based Voronoi diagrams
including (1)surface-based power diagram, (2)constrained Voronoi diagram with
curve-type breaklines, and (3)curve-type generators. We conduct extensive
experimental results to validate the ability to approximate the exact VD in
different distance-driven scenarios.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-20T01:30:00Z">Tuesday, December 20 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.08664'>On the Optimum Scenarios for Single Row Equidistant Facility Layout Problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Shrouq Gamal, Ahmed A. Hawam, Ahmed M. El-Kassas</p><p>Single Row Equidistant Facility Layout Problem SREFLP is with an NP-Hard
nature to mimic material handling costs along with equally spaced straight-line
facilities layout. Based on literature, it is obvious that efforts of
researchers for solving SREFLP turn from exact methods into release the running
time tracing the principle of the approximate methods in time race, regardless
searching their time complexity release in conjunction with a provable quality
of solutions. This study focuses on Lower bounding LB techniques as an
independent potential solution tool for SREFLP. In particular, Best-known
SREFLP LBs are reported from literature and significantly LBs optimum scenarios
are highlighted. Initially, one gap of the SREFLP bidirectional LB is enhanced.
From the integration between the enhanced LB and the best-known Gilmore-Lawler
GL bounding, a new SREFLP optimum scenario is provided. Further improvements to
GLB lead to guarantee an exact Shipping/Receiving Facility assignment and
propose a conjecture of at most 4/3 approximation scheme for SREFLP.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gamal_S/0/1/0/all/0/1">Shrouq Gamal</a>, <a href="http://arxiv.org/find/cs/1/au:+Hawam_A/0/1/0/all/0/1">Ahmed A. Hawam</a>, <a href="http://arxiv.org/find/cs/1/au:+El_Kassas_A/0/1/0/all/0/1">Ahmed M. El-Kassas</a></p><p>Single Row Equidistant Facility Layout Problem SREFLP is with an NP-Hard
nature to mimic material handling costs along with equally spaced straight-line
facilities layout. Based on literature, it is obvious that efforts of
researchers for solving SREFLP turn from exact methods into release the running
time tracing the principle of the approximate methods in time race, regardless
searching their time complexity release in conjunction with a provable quality
of solutions. This study focuses on Lower bounding LB techniques as an
independent potential solution tool for SREFLP. In particular, Best-known
SREFLP LBs are reported from literature and significantly LBs optimum scenarios
are highlighted. Initially, one gap of the SREFLP bidirectional LB is enhanced.
From the integration between the enhanced LB and the best-known Gilmore-Lawler
GL bounding, a new SREFLP optimum scenario is provided. Further improvements to
GLB lead to guarantee an exact Shipping/Receiving Facility assignment and
propose a conjecture of at most 4/3 approximation scheme for SREFLP.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-20T01:30:00Z">Tuesday, December 20 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.08964'>GPU Load Balancing</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Muhammad Osama</p><p>Fine-grained workload and resource balancing is the key to high performance
for regular and irregular computations on the GPUs. In this dissertation, we
conduct an extensive survey of existing load-balancing techniques to build an
abstraction that addresses the difficulty of scheduling computations on the
GPU.
</p>
<p>We propose a GPU fine-grained load-balancing abstraction that decouples load
balancing from work processing and aims to support both static and dynamic
schedules with a programmable interface to implement new load-balancing
schedules. Prior to our work, the only way to unleash the GPU's potential on
irregular problems has been to workload-balance through application-specific,
tightly coupled load-balancing techniques. With our open-source framework for
load-balancing, we hope to improve programmers' productivity when developing
irregular-parallel algorithms on the GPU, and also improve the overall
performance characteristics for such applications by allowing a quick path to
experimentation with a variety of existing load-balancing techniques.
</p>
<p>Using our insights from load-balancing irregular workloads, we build
Stream-K, a work-centric parallelization of matrix multiplication (GEMM) and
related computations in dense linear algebra. Whereas contemporary
decompositions are primarily tile-based, our method operates by partitioning an
even share of the aggregate inner loop iterations among physical processing
elements. This provides a near-perfect utilization of computing resources,
regardless of how efficiently the output tiling for any given problem quantizes
across the underlying processing elements. On GPU processors, our Stream-K
parallelization of GEMM produces a peak speedup of up to 14x and 6.7x, and an
average performance response that is both higher and more consistent across 32K
GEMM problem geometries than state-of-the-art math libraries such as CUTLASS
and cuBLAS.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Osama_M/0/1/0/all/0/1">Muhammad Osama</a></p><p>Fine-grained workload and resource balancing is the key to high performance
for regular and irregular computations on the GPUs. In this dissertation, we
conduct an extensive survey of existing load-balancing techniques to build an
abstraction that addresses the difficulty of scheduling computations on the
GPU.
</p>
<p>We propose a GPU fine-grained load-balancing abstraction that decouples load
balancing from work processing and aims to support both static and dynamic
schedules with a programmable interface to implement new load-balancing
schedules. Prior to our work, the only way to unleash the GPU's potential on
irregular problems has been to workload-balance through application-specific,
tightly coupled load-balancing techniques. With our open-source framework for
load-balancing, we hope to improve programmers' productivity when developing
irregular-parallel algorithms on the GPU, and also improve the overall
performance characteristics for such applications by allowing a quick path to
experimentation with a variety of existing load-balancing techniques.
</p>
<p>Using our insights from load-balancing irregular workloads, we build
Stream-K, a work-centric parallelization of matrix multiplication (GEMM) and
related computations in dense linear algebra. Whereas contemporary
decompositions are primarily tile-based, our method operates by partitioning an
even share of the aggregate inner loop iterations among physical processing
elements. This provides a near-perfect utilization of computing resources,
regardless of how efficiently the output tiling for any given problem quantizes
across the underlying processing elements. On GPU processors, our Stream-K
parallelization of GEMM produces a peak speedup of up to 14x and 6.7x, and an
average performance response that is both higher and more consistent across 32K
GEMM problem geometries than state-of-the-art math libraries such as CUTLASS
and cuBLAS.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-20T01:30:00Z">Tuesday, December 20 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.09005'>High-Performance Filters For GPUs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Hunter McCoy, Steven Hofmeyr, Katherine Yelick, Prashant Pandey</p><p>Filters approximately store a set of items while trading off accuracy for
space-efficiency and can address the limited memory on accelerators, such as
GPUs. However, there is a lack of high-performance and feature-rich GPU filters
as most advancements in filter research has focused on CPUs.
</p>
<p>In this paper, we explore the design space of filters with a goal to develop
massively parallel, high performance, and feature rich filters for GPUs. We
evaluate various filter designs in terms of performance, usability, and
supported features and identify two filter designs that offer the right trade
off in terms of performance, features, and usability.
</p>
<p>We present two new GPU-based filters, the TCF and GQF, that can be employed
in various high performance data analytics applications. The TCF is a set
membership filter and supports faster inserts and queries, whereas the GQF
supports counting which comes at an additional performance cost. Both the GQF
and TCF provide point and bulk insertion API and are designed to exploit the
massive parallelism in the GPU without sacrificing usability and necessary
features. The TCF and GQF are up to $4.4\times$ and $1.4\times$ faster than the
previous GPU filters in our benchmarks and at the same time overcome the
fundamental constraints in performance and usability in current GPU filters.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+McCoy_H/0/1/0/all/0/1">Hunter McCoy</a>, <a href="http://arxiv.org/find/cs/1/au:+Hofmeyr_S/0/1/0/all/0/1">Steven Hofmeyr</a>, <a href="http://arxiv.org/find/cs/1/au:+Yelick_K/0/1/0/all/0/1">Katherine Yelick</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1">Prashant Pandey</a></p><p>Filters approximately store a set of items while trading off accuracy for
space-efficiency and can address the limited memory on accelerators, such as
GPUs. However, there is a lack of high-performance and feature-rich GPU filters
as most advancements in filter research has focused on CPUs.
</p>
<p>In this paper, we explore the design space of filters with a goal to develop
massively parallel, high performance, and feature rich filters for GPUs. We
evaluate various filter designs in terms of performance, usability, and
supported features and identify two filter designs that offer the right trade
off in terms of performance, features, and usability.
</p>
<p>We present two new GPU-based filters, the TCF and GQF, that can be employed
in various high performance data analytics applications. The TCF is a set
membership filter and supports faster inserts and queries, whereas the GQF
supports counting which comes at an additional performance cost. Both the GQF
and TCF provide point and bulk insertion API and are designed to exploit the
massive parallelism in the GPU without sacrificing usability and necessary
features. The TCF and GQF are up to $4.4\times$ and $1.4\times$ faster than the
previous GPU filters in our benchmarks and at the same time overcome the
fundamental constraints in performance and usability in current GPU filters.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-20T01:30:00Z">Tuesday, December 20 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.09165'>Unconstrained Traveling Tournament Problem is APX-complete</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Salomon Bendayan, Joseph Cheriyan, Kevin K.H. Cheung</p><p>We show that the Unconstrained Traveling Tournament Problem (UTTP) is
APX-complete by presenting an L-reduction from a version of metric (1,2)-TSP to
UTTP.
</p>
<p>Keywords: Traveling Tournament Problem, APX-complete, Approximation
algorithms, Traveling Salesman Problem
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bendayan_S/0/1/0/all/0/1">Salomon Bendayan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheriyan_J/0/1/0/all/0/1">Joseph Cheriyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheung_K/0/1/0/all/0/1">Kevin K.H. Cheung</a></p><p>We show that the Unconstrained Traveling Tournament Problem (UTTP) is
APX-complete by presenting an L-reduction from a version of metric (1,2)-TSP to
UTTP.
</p>
<p>Keywords: Traveling Tournament Problem, APX-complete, Approximation
algorithms, Traveling Salesman Problem
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-20T01:30:00Z">Tuesday, December 20 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.09270'>The One-Inclusion Graph Algorithm is not Always Optimal</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ishaq Aden-Ali, Yeshwanth Cherapanamjeri, Abhishek Shetty, Nikita Zhivotovskiy</p><p>The one-inclusion graph algorithm of Haussler, Littlestone, and Warmuth
achieves an optimal in-expectation risk bound in the standard PAC
classification setup. In one of the first COLT open problems, Warmuth
conjectured that this prediction strategy always implies an optimal high
probability bound on the risk, and hence is also an optimal PAC algorithm. We
refute this conjecture in the strongest sense: for any practically interesting
Vapnik-Chervonenkis class, we provide an in-expectation optimal one-inclusion
graph algorithm whose high probability risk bound cannot go beyond that implied
by Markov's inequality. Our construction of these poorly performing
one-inclusion graph algorithms uses Varshamov-Tenengolts error correcting
codes.
</p>
<p>Our negative result has several implications. First, it shows that the same
poor high-probability performance is inherited by several recent prediction
strategies based on generalizations of the one-inclusion graph algorithm.
Second, our analysis shows yet another statistical problem that enjoys an
estimator that is provably optimal in expectation via a leave-one-out argument,
but fails in the high-probability regime. This discrepancy occurs despite the
boundedness of the binary loss for which arguments based on concentration
inequalities often provide sharp high probability risk bounds.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Aden_Ali_I/0/1/0/all/0/1">Ishaq Aden-Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+Cherapanamjeri_Y/0/1/0/all/0/1">Yeshwanth Cherapanamjeri</a>, <a href="http://arxiv.org/find/cs/1/au:+Shetty_A/0/1/0/all/0/1">Abhishek Shetty</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhivotovskiy_N/0/1/0/all/0/1">Nikita Zhivotovskiy</a></p><p>The one-inclusion graph algorithm of Haussler, Littlestone, and Warmuth
achieves an optimal in-expectation risk bound in the standard PAC
classification setup. In one of the first COLT open problems, Warmuth
conjectured that this prediction strategy always implies an optimal high
probability bound on the risk, and hence is also an optimal PAC algorithm. We
refute this conjecture in the strongest sense: for any practically interesting
Vapnik-Chervonenkis class, we provide an in-expectation optimal one-inclusion
graph algorithm whose high probability risk bound cannot go beyond that implied
by Markov's inequality. Our construction of these poorly performing
one-inclusion graph algorithms uses Varshamov-Tenengolts error correcting
codes.
</p>
<p>Our negative result has several implications. First, it shows that the same
poor high-probability performance is inherited by several recent prediction
strategies based on generalizations of the one-inclusion graph algorithm.
Second, our analysis shows yet another statistical problem that enjoys an
estimator that is provably optimal in expectation via a leave-one-out argument,
but fails in the high-probability regime. This discrepancy occurs despite the
boundedness of the binary loss for which arguments based on concentration
inequalities often provide sharp high probability risk bounds.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-20T01:30:00Z">Tuesday, December 20 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.09316'>Lower bound on the running time of Pop-Stack Sorting on a random permutation</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Lyuben Lichev</p><p>Pop-Stack Sorting is an algorithm that takes a permutation as an input and
sorts its elements. It consists of several steps. At one step, the algorithm
reads the permutation it has to process from left to right and reverses each of
its maximal decreasing subsequences of consecutive elements. It terminates at
the first step that outputs the identity permutation.
</p>
<p>In this note, we answer a question of Defant on the running time of Pop-Stack
Sorting on the uniform random permutation $\sigma_n$. More precisely, we show
that there is a constant $c &gt; 0.5$ such that asymptotically almost surely, the
algorithm needs at least $cn$ steps to terminate on $\sigma_n$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Lichev_L/0/1/0/all/0/1">Lyuben Lichev</a></p><p>Pop-Stack Sorting is an algorithm that takes a permutation as an input and
sorts its elements. It consists of several steps. At one step, the algorithm
reads the permutation it has to process from left to right and reverses each of
its maximal decreasing subsequences of consecutive elements. It terminates at
the first step that outputs the identity permutation.
</p>
<p>In this note, we answer a question of Defant on the running time of Pop-Stack
Sorting on the uniform random permutation $\sigma_n$. More precisely, we show
that there is a constant $c &gt; 0.5$ such that asymptotically almost surely, the
algorithm needs at least $cn$ steps to terminate on $\sigma_n$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-20T01:30:00Z">Tuesday, December 20 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.09348'>Excluding Single-Crossing Matching Minors in Bipartite Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Archontia C. Giannopoulou, Dimitrios M. Thilikos, Sebastian Wiederrecht</p><p>\noindent By a seminal result of Valiant, computing the permanent of
$(0,1)$-matrices is, in general, $\#\mathsf{P}$-hard. In 1913 P\'olya asked for
which $(0,1)$-matrices $A$ it is possible to change some signs such that the
permanent of $A$ equals the determinant of the resulting matrix. In 1975,
Little showed these matrices to be exactly the biadjacency matrices of
bipartite graphs excluding $K_{3,3}$ as a \{matching minor}. This was turned
into a polynomial time algorithm by McCuaig, Robertson, Seymour, and Thomas in
1999. However, the relation between the exclusion of some matching minor in a
bipartite graph and the tractability of the permanent extends beyond $K_{3,3}.$
Recently it was shown that the exclusion of any planar bipartite graph as a
matching minor yields a class of bipartite graphs on which the {permanent} of
the corresponding $(0,1)$-matrices can be computed efficiently. In this paper
we unify the two results above into a single, more general result in the style
of the celebrated structure theorem for single-crossing-minor-free graphs. We
identify a class of bipartite graphs strictly generalising planar bipartite
graphs and $K_{3,3}$ which includes infinitely many non-Pfaffian graphs. The
exclusion of any member of this class as a matching minor yields a structure
that allows for the efficient evaluation of the permanent. Moreover, we show
that the evaluation of the permanent remains $\#\mathsf{P}$-hard on bipartite
graphs which exclude $K_{5,5}$ as a matching minor. This establishes a first
computational lower bound for the problem of counting perfect matchings on
matching minor closed classes.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Giannopoulou_A/0/1/0/all/0/1">Archontia C. Giannopoulou</a>, <a href="http://arxiv.org/find/math/1/au:+Thilikos_D/0/1/0/all/0/1">Dimitrios M. Thilikos</a>, <a href="http://arxiv.org/find/math/1/au:+Wiederrecht_S/0/1/0/all/0/1">Sebastian Wiederrecht</a></p><p>\noindent By a seminal result of Valiant, computing the permanent of
$(0,1)$-matrices is, in general, $\#\mathsf{P}$-hard. In 1913 P\'olya asked for
which $(0,1)$-matrices $A$ it is possible to change some signs such that the
permanent of $A$ equals the determinant of the resulting matrix. In 1975,
Little showed these matrices to be exactly the biadjacency matrices of
bipartite graphs excluding $K_{3,3}$ as a \{matching minor}. This was turned
into a polynomial time algorithm by McCuaig, Robertson, Seymour, and Thomas in
1999. However, the relation between the exclusion of some matching minor in a
bipartite graph and the tractability of the permanent extends beyond $K_{3,3}.$
Recently it was shown that the exclusion of any planar bipartite graph as a
matching minor yields a class of bipartite graphs on which the {permanent} of
the corresponding $(0,1)$-matrices can be computed efficiently. In this paper
we unify the two results above into a single, more general result in the style
of the celebrated structure theorem for single-crossing-minor-free graphs. We
identify a class of bipartite graphs strictly generalising planar bipartite
graphs and $K_{3,3}$ which includes infinitely many non-Pfaffian graphs. The
exclusion of any member of this class as a matching minor yields a structure
that allows for the efficient evaluation of the permanent. Moreover, we show
that the evaluation of the permanent remains $\#\mathsf{P}$-hard on bipartite
graphs which exclude $K_{5,5}$ as a matching minor. This establishes a first
computational lower bound for the problem of counting perfect matchings on
matching minor closed classes.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-20T01:30:00Z">Tuesday, December 20 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Monday, December 19
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/183'>TR22-183 |  New Lower Bounds and Derandomization for ACC, and a Derandomization-centric View on the Algorithmic Method | 

	Lijie Chen</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          In this paper, we obtain several new results on lower bounds and derandomization for ACC^0 circuits (constant-depth circuits consisting of AND/OR/MOD_m gates for a fixed constant m, a frontier class in circuit complexity):
		
1. We prove that any polynomial-time Merlin-Arthur proof system with an ACC^0 verifier (denoted by MA_{ACC^0}) can be simulated by a nondeterministic proof system with quasi-polynomial running time and polynomial proof length on infinitely many input lengths. This improves the previous simulation by [Chen, Lyu, and Williams, FOCS 2020], which requires both quasi-polynomial running time and proof length.
			
2. We show that MA_{ACC^0} cannot be computed by fixed-polynomial-size ACC^0 circuits, and our hard languages are hard on a sufficiently dense set of input lengths.
			
3. We show that NEXP (nondeterministic exponential-time) does not have ACC^0 circuits of sub-half-exponential size, improving the previous sub-third-exponential size lower bound for NEXP against ACC^0 by [Williams, J. ACM 2014].
	
Combining our first and second results gives a conceptually simpler and derandomization-centric proof of the recent breakthrough result NQP = \NTIME[2^{polylog(n)}] is not in ACC^0 by [Murray and Williams, SICOMP 2020]: Instead of going through an easy witness lemma as they did, we first prove an ACC^0 lower bound for a subclass of MA, and then derandomize that subclass into NQP, while retaining its hardness against ACC^0. 
		
Moreover, since our derandomization of MA_{ACC^0} achieves a polynomial proof length, we indeed prove that nondeterministic quasi-polynomial-time with n^{omega(1)} nondeterminism bits (denoted as NTIMEGUESS[2^{polylog(n)},n^{omega(1)}]) has no poly(n)-size ACC^0 circuits, giving a new proof of a result by Vyas. Combining with a win-win argument based on randomized encodings from [Chen and Ren, STOC 2020], we also prove that NTIMEGUESS[2^{polylog(n)},n^{omega(1)}] cannot be (1/2+1/poly(n))-approximated by poly(n)-size ACC^0 circuits, improving the recent strongly average-case lower bounds for NQP against ACC^0 by [Chen and Ren, STOC 2020].
		
One interesting technical ingredient behind our second result is the construction of a PSPACE-complete language that is paddable, downward self-reducible, same-length checkable, and weakly error correctable. Moreover, all its reducibility properties have corresponding AC^0[2] non-adaptive oracle circuits. Our construction builds and improves upon similar constructions from [Trevisan and Vadhan, Complexity 2007] and [Chen, FOCS 2019], which all require at least TC^0 oracle circuits for implementing these properties.
        
        </div>

        <div class='tr-article-summary'>
        
          
          In this paper, we obtain several new results on lower bounds and derandomization for ACC^0 circuits (constant-depth circuits consisting of AND/OR/MOD_m gates for a fixed constant m, a frontier class in circuit complexity):
		
1. We prove that any polynomial-time Merlin-Arthur proof system with an ACC^0 verifier (denoted by MA_{ACC^0}) can be simulated by a nondeterministic proof system with quasi-polynomial running time and polynomial proof length on infinitely many input lengths. This improves the previous simulation by [Chen, Lyu, and Williams, FOCS 2020], which requires both quasi-polynomial running time and proof length.
			
2. We show that MA_{ACC^0} cannot be computed by fixed-polynomial-size ACC^0 circuits, and our hard languages are hard on a sufficiently dense set of input lengths.
			
3. We show that NEXP (nondeterministic exponential-time) does not have ACC^0 circuits of sub-half-exponential size, improving the previous sub-third-exponential size lower bound for NEXP against ACC^0 by [Williams, J. ACM 2014].
	
Combining our first and second results gives a conceptually simpler and derandomization-centric proof of the recent breakthrough result NQP = \NTIME[2^{polylog(n)}] is not in ACC^0 by [Murray and Williams, SICOMP 2020]: Instead of going through an easy witness lemma as they did, we first prove an ACC^0 lower bound for a subclass of MA, and then derandomize that subclass into NQP, while retaining its hardness against ACC^0. 
		
Moreover, since our derandomization of MA_{ACC^0} achieves a polynomial proof length, we indeed prove that nondeterministic quasi-polynomial-time with n^{omega(1)} nondeterminism bits (denoted as NTIMEGUESS[2^{polylog(n)},n^{omega(1)}]) has no poly(n)-size ACC^0 circuits, giving a new proof of a result by Vyas. Combining with a win-win argument based on randomized encodings from [Chen and Ren, STOC 2020], we also prove that NTIMEGUESS[2^{polylog(n)},n^{omega(1)}] cannot be (1/2+1/poly(n))-approximated by poly(n)-size ACC^0 circuits, improving the recent strongly average-case lower bounds for NQP against ACC^0 by [Chen and Ren, STOC 2020].
		
One interesting technical ingredient behind our second result is the construction of a PSPACE-complete language that is paddable, downward self-reducible, same-length checkable, and weakly error correctable. Moreover, all its reducibility properties have corresponding AC^0[2] non-adaptive oracle circuits. Our construction builds and improves upon similar constructions from [Trevisan and Vadhan, Complexity 2007] and [Chen, FOCS 2019], which all require at least TC^0 oracle circuits for implementing these properties.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-19T21:47:54Z">Monday, December 19 2022, 21:47</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://lucatrevisan.wordpress.com/2022/12/19/postdoc-positions-for-2023-24/'>Postdoc Positions for 2023-24</a></h3>
        <p class='tr-article-feed'>from <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          I am looking for three postdoctoral fellows for the next academic year to work with me at Bocconi. The positions offer an internationally competitive salary (up to 65,000 Euro per year, tax-free, plus relocation assistance and travel allowance), in a &#8230; Continue reading &#8594;
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>I am looking for three postdoctoral fellows for the next academic year to work with me at Bocconi. </p>



<p>The positions offer an internationally competitive salary (up to 65,000 Euro per year, tax-free, plus relocation assistance and travel allowance), in a wonderful location. The <strong>strict</strong> application deadline is <strong>January 31, 2023</strong>. Each position is for one year, renewable to a second year.</p>



<p>Among the topics that I am interested in are spectral graph theory, average-case complexity, “applications” of semidefinite programming, random processes on networks, approximation algorithms, pseudorandomness and combinatorial constructions.</p>



<p>Please contact me if you are interested in these positions and you would like more information.</p>



<p>To apply, go to <strong><a href="https://jobmarket.unibocconi.eu/?type=a&amp;urlBack=/wps/wcm/connect/Bocconi/SitoPubblico_IT/Albero+di+navigazione/Home/docenti+e+ricerca/docenti/Reclutamento+docenti/Concorsi/Assegni+di+Ricerca/">this link</a></strong>, and then look for the opening for 3 positions dated December 6, 2022, like the one below (unfortunately there isn&#8217;t a perma-link to the application form):</p>



<p> </p>



<figure class="wp-block-image size-large"><a href="https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023.png"><img data-attachment-id="4657" data-permalink="https://lucatrevisan.wordpress.com/2022/12/19/postdoc-positions-for-2023-24/postdoc-call-2023/" data-orig-file="https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023.png" data-orig-size="988,889" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="postdoc-call-2023" data-image-description="" data-image-caption="" data-medium-file="https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023.png?w=300" data-large-file="https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023.png?w=584" src="https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023.png?w=988" alt="" class="wp-image-4657" srcset="https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023.png 988w, https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023.png?w=150 150w, https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023.png?w=300 300w, https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023.png?w=768 768w" sizes="(max-width: 988px) 100vw, 988px" /></a></figure>



<p>Bocconi&#8217;s Computing Sciences department has a sizable theory group, that includes <a href="https://laurasanita.github.io/">Laura Sanità</a>, who works on optimization and approximation algorithms, <a href="https://www.alonrosen.net/">Alon Rosen</a>, who works on the foundations of cryptography, <a href="https://elias.ba30.eu/">Marek Elias</a>, who works on online optimization, and <a href="https://andcelli.github.io/">Andrea Celli</a>, who works on algorithmic game theory. Next year, <a href="https://adampolak.github.io/">Adam Polak</a> who works on fine-grained complexity and analysis of algorithms, will also join us.</p>



<p>Speaking of Alon Rosen, he is also recruiting postdocs for the next academic year, and he has two open positions, that you can find at the same link looking for two positions dated December 14 with an application deadline of February 28:</p>



<figure class="wp-block-image size-large"><a href="https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023-alon.png"><img data-attachment-id="4659" data-permalink="https://lucatrevisan.wordpress.com/2022/12/19/postdoc-positions-for-2023-24/postdoc-call-2023-alon/" data-orig-file="https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023-alon.png" data-orig-size="2079,1395" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="postdoc-call-2023-alon" data-image-description="" data-image-caption="" data-medium-file="https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023-alon.png?w=300" data-large-file="https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023-alon.png?w=584" src="https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023-alon.png?w=1024" alt="" class="wp-image-4659" srcset="https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023-alon.png?w=1024 1024w, https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023-alon.png?w=2048 2048w, https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023-alon.png?w=150 150w, https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023-alon.png?w=300 300w, https://lucatrevisan.files.wordpress.com/2022/12/postdoc-call-2023-alon.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></figure>
<p class="authors">By luca</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-19T17:18:05Z">Monday, December 19 2022, 17:18</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://gilkalai.wordpress.com/2022/12/19/alefs-corner-2/'>Alef’s Corner</a></h3>
        <p class='tr-article-feed'>from <a href='https://gilkalai.wordpress.com'>Gil Kalai</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
           
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p></p>


<p><img loading="lazy" data-attachment-id="23657" data-permalink="https://gilkalai.wordpress.com/alef88/" data-orig-file="https://gilkalai.files.wordpress.com/2022/12/alef88.png" data-orig-size="2100,2100" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="alef88" data-image-description="" data-image-caption="" data-medium-file="https://gilkalai.files.wordpress.com/2022/12/alef88.png?w=300" data-large-file="https://gilkalai.files.wordpress.com/2022/12/alef88.png?w=640" class="alignnone size-full wp-image-23657" src="https://gilkalai.files.wordpress.com/2022/12/alef88.png" alt="alef88" width="2100" height="2100" srcset="https://gilkalai.files.wordpress.com/2022/12/alef88.png 2100w, https://gilkalai.files.wordpress.com/2022/12/alef88.png?w=150&amp;h=150 150w, https://gilkalai.files.wordpress.com/2022/12/alef88.png?w=300&amp;h=300 300w, https://gilkalai.files.wordpress.com/2022/12/alef88.png?w=768&amp;h=768 768w, https://gilkalai.files.wordpress.com/2022/12/alef88.png?w=1024&amp;h=1024 1024w" sizes="(max-width: 2100px) 100vw, 2100px" /></p>
<p> </p><p class="authors">By Gil Kalai</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-19T16:19:19Z">Monday, December 19 2022, 16:19</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/12/19/junior-professor-at-university-of-cologne-apply-by-january-31-2023/'>Junior professor at University of Cologne (apply by January 31, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The department Mathematics/Computer Science seeks to strengthen its competences in the field of efficient algorithms. Possible but not exclusive research areas include randomized algorithms, approximation algorithms, sublinear algorithms and computational geometry. The position is for 3 years with a possibility of extension of another 3 years after a positive evaluation. Website: berufungen.uni-koeln.de/?lang=en Email: mnf-berufungen@uni-koeln.de
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The department Mathematics/Computer Science seeks to strengthen its competences in the field of efficient algorithms. Possible but not exclusive research areas include randomized algorithms, approximation algorithms, sublinear algorithms and computational geometry.</p>
<p>The position is for 3 years with a possibility of extension of another 3 years after a positive evaluation.</p>
<p>Website: <a href="https://berufungen.uni-koeln.de/?lang=en">https://berufungen.uni-koeln.de/?lang=en</a><br />
Email: mnf-berufungen@uni-koeln.de</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-19T12:37:15Z">Monday, December 19 2022, 12:37</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://blog.computationalcomplexity.org/2022/12/voter-suppression-harvard-style.html'>Voter Suppression, Harvard Style</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p><br></p><p>The following appeared on Harry Lewis's blog, here,&nbsp;hence it is written in his voice, though it is a co-authored. You'll see why later.&nbsp; I then have some comments on it.&nbsp;</p><p>------------------------------------------</p><p>Voter Suppression, Harvard-Style</p><p>(This piece is jointly authored by Harry Lewis (Harvard) and Bill Gasarch</p><p>(University of Maryland at College Park). Harry was Bill's PhD Advisor.)</p><p>There are elections in Hong Kong, but to get on the ballot you have to be nominated by a committee controlled by Beijing government.</p><p>Elections for the Harvard Board of Overseers—one of Harvard’s two governing bodies—are almost as well-controlled. A Harvard Alumni Association (HAA) nominating committee curates a slate of candidates, from which alumni make their selections.</p><p>But an alternative route to get on the Harvard ballot exists, at least in theory. So-called “petition” candidates have always been rare—but after several climate activists were elected in 2020, the rules were changed to make it even harder. Among other things, the number of petitions to get on the ballot was raised by a factor of fifteen, to more than three thousand.</p><p>This year, noted civil libertarian Harvey Silverglate, concerned about freedom of expression at Harvard, is trying to make it onto the ballot.</p><p>The authors are computer scientists. We are neither technologically naïve nor afraid of computers. Harry has long been concerned about issues of student freedom and Harvard governance, and suggested to Bill, Harry’s sometime PhD student, that he sign Silverglate’s petition. This is an account of Bill’s trip through the resulting electronic purgatory.</p><p>To add your name, you have to fill out a web form. To access the web form, you need a HarvardKey. To get a HarvardKey, you have to fill out another web form. So far, so good.</p><p>The HarvardKey web form wanted Bill’s 10-digit HAA ID, which he was told to find on the address sticker of his copy of a recent Harvard Magazine (sent to all alumni). Bill had one handy, so he looked and found … a 9-digit number. He tried entering that number—no luck. He noticed it began with three 0s, and tried adding a fourth—that did not work either.</p><p>The web form had a number to call. Someone answered, and said some information would be needed before dealing with digits. Name (fine). Year of degree (fine). MIDDLE name (well, fine, though no one but Bill’s mother ever used it, and only when indignant). Date of birth (well, OK, but now we’re getting into territory we don’t casually reveal any more). When he got his MASTER’s degree. Bill did not know—that’s just something Harvard gives en route to the PhD. Turned out he actually didn’t need to know, an estimate was good enough. The person on the phone gave him his HAA ID, which bore no relation to the number on his address sticker.</p><p>Let’s pause there. Some people never call tech support because they have never found it helpful to do so. Any such person with a 9-digit address sticker number could not participate in the petition process.</p><p>Bill entered his HAA ID and received an error message saying that … KEY-5003 was missing. Happily, Bill had kept the support person on the phone (this was not his first rodeo).</p><p>Missing KEY-5003 turned out to mean that Harvard did not have his email address. He supplied it and was told he would get an email confirmation later in the day.</p><p>He did get an email later in the day. It listed eleven steps to claim his HarvardKey. Step 6 was to wait for a confirming email (he thought this WAS the confirming email), but after step 5 the system told him he was not in the system and it could not continue.</p><p>Another call to a support line. No, Bill was told, he has to wait 24 hours to get his email address updated, and would not get a confirming email. Just try tomorrow. Like the email said. Except that it didn’t say that, nor had the person he spoke to on the previous call.</p><p>Bill waited 24 hours and tried again, and got a little further through the eleven steps—and then was told to wait ANOTHER 24 hours for the account to activate.</p><p>24 hours later he tried again, from home, and failed again. Then he went to his office and succeeded—no clue why.</p><p>Now finally he got to the petition, which required Bill’s graduation year—and Silverglate’s­­, which Bill found but shouldn’t have been needed since this petition was specific to Silverglate.</p><p>Three days and two phone calls to sign the petition. To be fair, the people Bill spoke to on the phone were kind and helpful. Probably they themselves were struggling with the systems.</p><p>And we knew already that HAA is technologically challenged. A few weeks ago, it abruptly announced that it could no longer handle email forwarding. After alumni blowback, it just as abruptly announced that it would NOT end its forwarding service—oddly, while cautioning that the service was unlikely ever to work very well.</p><p>When election officials want to suppress the vote somewhere, they under-resource the voting process, forcing voters to cross town and wait in long lines. What happened to Bill is so comical that it is hard to imagine that the specifics were intentional. On the other hand, under-resourcing the petitioning process, allowing it to be so defective, misinformed, and hard to use that many people won’t exercise their franchise—isn’t that a form of voter suppression?</p><p>Why not be true to Harvard’s motto, Veritas, and just post on the web, For the alumni to choose the Overseers is an anachronism. Today’s alumni voters can’t be trusted to do it wisely. Since we can’t get rid of this system, we are going to make it all but impossible to nominate by petition. Try if you wish, but if you do, abandon all hope, ye who enter here.</p><p>-------------------------</p><p>I originally emailed Harry what I had to do to sign the petition with my point being</p><p>`Yes, I am a luddite but that has NOTHING TO DO with why I am having a problem.''</p><p>(I had meant to do a blog on that topic. I may still.)</p><p>Harry saw this for what it was- Voter Suppression. He wrote the Op-Ed using my story and made me a co-author. (We have never published together so it amused me that our first pub together would be 37 years after he was my advisor- perhaps a record. Does having a joint blog post count?) What is above is basically that Op-Ed (I think the version Harry submitted omitted that my mom is the only one who uses my middle name.)&nbsp;</p><p>He submitted this as an Op-Ed to three places sequentially.&nbsp;</p><p>1) The New York Times turned us down (not a surprise) by not even acknowledging it had been submitted (really?).</p><p>2) The Boston Globe turned us down (a surprise).</p><p>3) The Harvard Crimson turned us down (a shock!).</p><p>Voter Suppression followed by censorship.</p><p>AND back to the original issue: Harvey does not have enough people on the petition yet. I urge you to READ Harry Lewis's post on Harvey&nbsp;here&nbsp;and IF you are a Harvard Alumni AND you agree that Harvey would be a good overseer THEN sign the petition. Or at least try.</p><br><p>By gasarch</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p><br /></p><p>The following appeared on Harry Lewis's blog, <a href="http://harry-lewis.blogspot.com/2022/12/voter-suppression-harvard-style.html">here</a>,&nbsp;hence it is written in his voice, though it is a co-authored. You'll see why later.&nbsp; I then have some comments on it.&nbsp;</p><p>------------------------------------------</p><p>Voter Suppression, Harvard-Style</p><p>(This piece is jointly authored by Harry Lewis (Harvard) and Bill Gasarch</p><p>(University of Maryland at College Park). Harry was Bill's PhD Advisor.)</p><p>There are elections in Hong Kong, but to get on the ballot you have to be nominated by a committee controlled by Beijing government.</p><p>Elections for the Harvard Board of Overseers—one of Harvard’s two governing bodies—are almost as well-controlled. A Harvard Alumni Association (HAA) nominating committee curates a slate of candidates, from which alumni make their selections.</p><p>But an alternative route to get on the Harvard ballot exists, at least in theory. So-called “petition” candidates have always been rare—but after several climate activists were elected in 2020, the rules were changed to make it even harder. Among other things, the number of petitions to get on the ballot was raised by a factor of fifteen, to more than three thousand.</p><p>This year, noted civil libertarian Harvey Silverglate, concerned about freedom of expression at Harvard, is trying to make it onto the ballot.</p><p>The authors are computer scientists. We are neither technologically naïve nor afraid of computers. Harry has long been concerned about issues of student freedom and Harvard governance, and suggested to Bill, Harry’s sometime PhD student, that he sign Silverglate’s petition. This is an account of Bill’s trip through the resulting electronic purgatory.</p><p>To add your name, you have to fill out a web form. To access the web form, you need a HarvardKey. To get a HarvardKey, you have to fill out another web form. So far, so good.</p><p>The HarvardKey web form wanted Bill’s 10-digit HAA ID, which he was told to find on the address sticker of his copy of a recent Harvard Magazine (sent to all alumni). Bill had one handy, so he looked and found … a 9-digit number. He tried entering that number—no luck. He noticed it began with three 0s, and tried adding a fourth—that did not work either.</p><p>The web form had a number to call. Someone answered, and said some information would be needed before dealing with digits. Name (fine). Year of degree (fine). MIDDLE name (well, fine, though no one but Bill’s mother ever used it, and only when indignant). Date of birth (well, OK, but now we’re getting into territory we don’t casually reveal any more). When he got his MASTER’s degree. Bill did not know—that’s just something Harvard gives en route to the PhD. Turned out he actually didn’t need to know, an estimate was good enough. The person on the phone gave him his HAA ID, which bore no relation to the number on his address sticker.</p><p>Let’s pause there. Some people never call tech support because they have never found it helpful to do so. Any such person with a 9-digit address sticker number could not participate in the petition process.</p><p>Bill entered his HAA ID and received an error message saying that … KEY-5003 was missing. Happily, Bill had kept the support person on the phone (this was not his first rodeo).</p><p>Missing KEY-5003 turned out to mean that Harvard did not have his email address. He supplied it and was told he would get an email confirmation later in the day.</p><p>He did get an email later in the day. It listed eleven steps to claim his HarvardKey. Step 6 was to wait for a confirming email (he thought this WAS the confirming email), but after step 5 the system told him he was not in the system and it could not continue.</p><p>Another call to a support line. No, Bill was told, he has to wait 24 hours to get his email address updated, and would not get a confirming email. Just try tomorrow. Like the email said. Except that it didn’t say that, nor had the person he spoke to on the previous call.</p><p>Bill waited 24 hours and tried again, and got a little further through the eleven steps—and then was told to wait ANOTHER 24 hours for the account to activate.</p><p>24 hours later he tried again, from home, and failed again. Then he went to his office and succeeded—no clue why.</p><p>Now finally he got to the petition, which required Bill’s graduation year—and Silverglate’s­­, which Bill found but shouldn’t have been needed since this petition was specific to Silverglate.</p><p>Three days and two phone calls to sign the petition. To be fair, the people Bill spoke to on the phone were kind and helpful. Probably they themselves were struggling with the systems.</p><p>And we knew already that HAA is technologically challenged. A few weeks ago, it abruptly announced that it could no longer handle email forwarding. After alumni blowback, it just as abruptly announced that it would NOT end its forwarding service—oddly, while cautioning that the service was unlikely ever to work very well.</p><p>When election officials want to suppress the vote somewhere, they under-resource the voting process, forcing voters to cross town and wait in long lines. What happened to Bill is so comical that it is hard to imagine that the specifics were intentional. On the other hand, under-resourcing the petitioning process, allowing it to be so defective, misinformed, and hard to use that many people won’t exercise their franchise—isn’t that a form of voter suppression?</p><p>Why not be true to Harvard’s motto, Veritas, and just post on the web, <i>For the alumni to choose the</i> <i>Overseers is an anachronism. Today’s alumni voters can’t be trusted to do it wisely. Since we can’t get</i> <i>rid of this system, we are going to make it all but impossible to nominate by petition. Try if you wish, but if you do, abandon all hope, ye who enter here.</i></p><p>-------------------------</p><p>I originally emailed Harry what I had to do to sign the petition with my point being</p><p>`Yes, I am a luddite but that has NOTHING TO DO with why I am having a problem.''</p><p>(I had meant to do a blog on that topic. I may still.)</p><p>Harry saw this for what it was- Voter Suppression. He wrote the Op-Ed using my story and made me a co-author. (We have never published together so it amused me that our first pub together would be 37 years after he was my advisor- perhaps a record. Does having a joint blog post count?) What is above is basically that Op-Ed (I think the version Harry submitted omitted that my mom is the only one who uses my middle name.)&nbsp;</p><p>He submitted this as an Op-Ed to three places sequentially.&nbsp;</p><p>1) The New York Times turned us down (not a surprise) by not even acknowledging it had been submitted (really?).</p><p>2) The Boston Globe turned us down (a surprise).</p><p>3) The Harvard Crimson turned us down (a shock!).</p><p>Voter Suppression followed by censorship.</p><p>AND back to the original issue: Harvey does not have enough people on the petition yet. I urge you to READ Harry Lewis's post on Harvey&nbsp;<a href="http://harry-lewis.blogspot.com/2022/11/harvard-alumni-sign-harvey-silverglates.html">here</a>&nbsp;and IF you are a Harvard Alumni AND you agree that Harvey would be a good overseer THEN sign the petition. Or at least try.</p><div><br /></div><p class="authors">By gasarch</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-19T05:12:00Z">Monday, December 19 2022, 05:12</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.08397'>Criticality of $AC^0$ formulae</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Prahladh Harsha, Tulasi mohan Molli, Ashutosh Shankar</p><p>Rossman [In \emph{Proc.\ $34$th Comput.\ Complexity Conf.}, 2019] introduced
the notion of \emph{criticality}. The criticality of a Boolean function $f
\colon \zo^n \to \zo$ is the minimum $\lambda \geq 1$ such that for all
positive integers $t$, \[ \Pr_{\brho \sim
\calR_p}\left[\DT_{\depth}(f|_{\brho}) \geq t\right] \leq (p\lambda)^t. \]
\hastad's celebrated switching lemma shows that the criticality of any $k$-DNF
is at most $O(k)$. Subsequent improvements to correlation bounds of
$\AC^0$-circuits against parity showed that the criticality of any
$\AC^0$-\emph{circuit} of size $S$ and depth $d+1$ is at most $O(\log S)^d$ and
any \emph{regular} $\AC^0$-\emph{formula} of size $S$ and depth $d+1$ is at
most $O(\frac1d \cdot \log S)^d$. We strengthen these results by showing that
the criticality of \emph{any} $\AC^0$-formula (not necessarily regular) of size
$S$ and depth $d+1$ is at most $O(\frac{\log S}{d})^d$, resolving a conjecture
due to Rossman.
</p>
<p>This result also implies Rossman's optimal lower bound on the size of any
depth-$d$ $\AC^0$-formula computing parity [Comput.\ Complexity,
27(2):209--223, 2018.]. Our result implies tight correlation bounds against
parity, tight Fourier concentration results and improved \#SAT algorithm for
$\AC^0$-formulae.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Harsha_P/0/1/0/all/0/1">Prahladh Harsha</a>, <a href="http://arxiv.org/find/cs/1/au:+Molli_T/0/1/0/all/0/1">Tulasi mohan Molli</a>, <a href="http://arxiv.org/find/cs/1/au:+Shankar_A/0/1/0/all/0/1">Ashutosh Shankar</a></p><p>Rossman [In \emph{Proc.\ $34$th Comput.\ Complexity Conf.}, 2019] introduced
the notion of \emph{criticality}. The criticality of a Boolean function $f
\colon \zo^n \to \zo$ is the minimum $\lambda \geq 1$ such that for all
positive integers $t$, \[ \Pr_{\brho \sim
\calR_p}\left[\DT_{\depth}(f|_{\brho}) \geq t\right] \leq (p\lambda)^t. \]
\hastad's celebrated switching lemma shows that the criticality of any $k$-DNF
is at most $O(k)$. Subsequent improvements to correlation bounds of
$\AC^0$-circuits against parity showed that the criticality of any
$\AC^0$-\emph{circuit} of size $S$ and depth $d+1$ is at most $O(\log S)^d$ and
any \emph{regular} $\AC^0$-\emph{formula} of size $S$ and depth $d+1$ is at
most $O(\frac1d \cdot \log S)^d$. We strengthen these results by showing that
the criticality of \emph{any} $\AC^0$-formula (not necessarily regular) of size
$S$ and depth $d+1$ is at most $O(\frac{\log S}{d})^d$, resolving a conjecture
due to Rossman.
</p>
<p>This result also implies Rossman's optimal lower bound on the size of any
depth-$d$ $\AC^0$-formula computing parity [Comput.\ Complexity,
27(2):209--223, 2018.]. Our result implies tight correlation bounds against
parity, tight Fourier concentration results and improved \#SAT algorithm for
$\AC^0$-formulae.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-19T01:30:00Z">Monday, December 19 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.08559'>Grothendieck inequalities characterize converses to the polynomial method</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jop Bri&#xeb;t, Francisco Escudero Guti&#xe9;rrez, Sander Gribling</p><p>A surprising 'converse to the polynomial method' of Aaronson et al. (CCC'16)
shows that any bounded quadratic polynomial can be computed exactly in
expectation by a 1-query algorithm up to a universal multiplicative factor
related to the famous Grothendieck constant. Here we show that such a result
does not generalize to quartic polynomials and 2-query algorithms, even when we
allow for additive approximations. We also show that the additive approximation
implied by their result is tight for bounded bilinear forms, which gives a new
characterization of the Grothendieck constant in terms of 1-query quantum
algorithms. Along the way we provide reformulations of the completely bounded
norm of a form, and its dual norm.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Briet_J/0/1/0/all/0/1">Jop Bri&#xeb;t</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Gutierrez_F/0/1/0/all/0/1">Francisco Escudero Guti&#xe9;rrez</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Gribling_S/0/1/0/all/0/1">Sander Gribling</a></p><p>A surprising 'converse to the polynomial method' of Aaronson et al. (CCC'16)
shows that any bounded quadratic polynomial can be computed exactly in
expectation by a 1-query algorithm up to a universal multiplicative factor
related to the famous Grothendieck constant. Here we show that such a result
does not generalize to quartic polynomials and 2-query algorithms, even when we
allow for additive approximations. We also show that the additive approximation
implied by their result is tight for bounded bilinear forms, which gives a new
characterization of the Grothendieck constant in terms of 1-query quantum
algorithms. Along the way we provide reformulations of the completely bounded
norm of a form, and its dual norm.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-19T01:30:00Z">Monday, December 19 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.08226'>Implementing Simulation of Simplicity for geometric degeneracies</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: W. Randolph Franklin (Rensselaer Polytechnic Institute), Salles Viana Gomes de Magalh&#xe3;es (Universidade Federal de Vi&#xe7;osa)</p><p>We describe how to implement Simulation of Simplicity (SoS). SoS removes
geometric degeneracies in point-in-polygon queries, polyhedron intersection,
map overlay, and other 2D and 3D geometric and spatial algorithms by
determining the effect of adding non-Archimedian infinitesimals of different
orders to the coordinates. Then it modifies the geometric predicates to emulate
that, and evaluates them in the usual arithmetic.
</p>
<p>A geometric degeneracy is a coincidence, such as a vertex of one polygon on
an edge of another polygon, that would have probability approaching zero if the
objects were distributed i.i.d. uniformly. However, in real data, they can
occur often. Especially in 3D, there are too many types of degeneracies to
reliably enumerate. But, if they are not handled, then predicates evaluate
wrong, and the output topology may be wrong.
</p>
<p>We describe the theory of SoS, and how several algorithms and programs were
successfully modified, including volume of the union of many cubes, point
location in a 3D mesh, and intersecting 3D meshes.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Franklin_W/0/1/0/all/0/1">W. Randolph Franklin</a> (Rensselaer Polytechnic Institute), <a href="http://arxiv.org/find/cs/1/au:+Magalhaes_S/0/1/0/all/0/1">Salles Viana Gomes de Magalh&#xe3;es</a> (Universidade Federal de Vi&#xe7;osa)</p><p>We describe how to implement Simulation of Simplicity (SoS). SoS removes
geometric degeneracies in point-in-polygon queries, polyhedron intersection,
map overlay, and other 2D and 3D geometric and spatial algorithms by
determining the effect of adding non-Archimedian infinitesimals of different
orders to the coordinates. Then it modifies the geometric predicates to emulate
that, and evaluates them in the usual arithmetic.
</p>
<p>A geometric degeneracy is a coincidence, such as a vertex of one polygon on
an edge of another polygon, that would have probability approaching zero if the
objects were distributed i.i.d. uniformly. However, in real data, they can
occur often. Especially in 3D, there are too many types of degeneracies to
reliably enumerate. But, if they are not handled, then predicates evaluate
wrong, and the output topology may be wrong.
</p>
<p>We describe the theory of SoS, and how several algorithms and programs were
successfully modified, including volume of the union of many cubes, point
location in a 3D mesh, and intersecting 3D meshes.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-19T01:30:00Z">Monday, December 19 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.08295'>Learning on Persistence Diagrams as Radon Measures</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Alex Elchesen, Iryna Hartsock, Jose A. Perea, Tatum Rask</p><p>Persistence diagrams are common descriptors of the topological structure of
data appearing in various classification and regression tasks. They can be
generalized to Radon measures supported on the birth-death plane and endowed
with an optimal transport distance. Examples of such measures are expectations
of probability distributions on the space of persistence diagrams. In this
paper, we develop methods for approximating continuous functions on the space
of Radon measures supported on the birth-death plane, as well as their
utilization in supervised learning tasks. Indeed, we show that any continuous
function defined on a compact subset of the space of such measures (e.g., a
classifier or regressor) can be approximated arbitrarily well by polynomial
combinations of features computed using a continuous compactly supported
function on the birth-death plane (a template). We provide insights into the
structure of relatively compact subsets of the space of Radon measures, and
test our approximation methodology on various data sets and supervised learning
tasks.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Elchesen_A/0/1/0/all/0/1">Alex Elchesen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hartsock_I/0/1/0/all/0/1">Iryna Hartsock</a>, <a href="http://arxiv.org/find/cs/1/au:+Perea_J/0/1/0/all/0/1">Jose A. Perea</a>, <a href="http://arxiv.org/find/cs/1/au:+Rask_T/0/1/0/all/0/1">Tatum Rask</a></p><p>Persistence diagrams are common descriptors of the topological structure of
data appearing in various classification and regression tasks. They can be
generalized to Radon measures supported on the birth-death plane and endowed
with an optimal transport distance. Examples of such measures are expectations
of probability distributions on the space of persistence diagrams. In this
paper, we develop methods for approximating continuous functions on the space
of Radon measures supported on the birth-death plane, as well as their
utilization in supervised learning tasks. Indeed, we show that any continuous
function defined on a compact subset of the space of such measures (e.g., a
classifier or regressor) can be approximated arbitrarily well by polynomial
combinations of features computed using a continuous compactly supported
function on the birth-death plane (a template). We provide insights into the
structure of relatively compact subsets of the space of Radon measures, and
test our approximation methodology on various data sets and supervised learning
tasks.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-19T01:30:00Z">Monday, December 19 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.08143'>Approximate counting using Taylor's theorem: a survey</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Viresh Patel, Guus Regts</p><p>In this article we consider certain well-known polynomials associated with
graphs including the independence polynomial and the chromatic polynomial.
These polynomials count certain objects in graphs: independent sets in the case
of the independence polynomial and proper colourings in the case of the
chromatic polynomial. They also have interpretations as partition functions in
statistical physics.
</p>
<p>The algorithmic problem of (approximately) computing these types of
polynomials has been studied for close to 50 years, especially using Markov
chain techniques. Around eight years ago, Barvinok devised a new algorithmic
approach based on Taylor's theorem for computing the permanent of certain
matrices, and the approach has been applied to various graph polynomials since
then. This article is intended as a gentle introduction to the approach as well
as a partial survey of associated techniques and results.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1">Viresh Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Regts_G/0/1/0/all/0/1">Guus Regts</a></p><p>In this article we consider certain well-known polynomials associated with
graphs including the independence polynomial and the chromatic polynomial.
These polynomials count certain objects in graphs: independent sets in the case
of the independence polynomial and proper colourings in the case of the
chromatic polynomial. They also have interpretations as partition functions in
statistical physics.
</p>
<p>The algorithmic problem of (approximately) computing these types of
polynomials has been studied for close to 50 years, especially using Markov
chain techniques. Around eight years ago, Barvinok devised a new algorithmic
approach based on Taylor's theorem for computing the permanent of certain
matrices, and the approach has been applied to various graph polynomials since
then. This article is intended as a gentle introduction to the approach as well
as a partial survey of associated techniques and results.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-19T01:30:00Z">Monday, December 19 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.08200'>Essentials of Parallel Graph Analytics</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Muhammad Osama, Serban D. Porumbescu, John D. Owens</p><p>We identify the graph data structure, frontiers, operators, an iterative loop
structure, and convergence conditions as essential components of graph
analytics systems based on the native-graph approach. Using these essential
components, we propose an abstraction that captures all the significant
programming models within graph analytics, such as bulk-synchronous,
asynchronous, shared-memory, message-passing, and push vs. pull traversals.
Finally, we demonstrate the power of our abstraction with an elegant modern C++
implementation of single-source shortest path and its required components.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Osama_M/0/1/0/all/0/1">Muhammad Osama</a>, <a href="http://arxiv.org/find/cs/1/au:+Porumbescu_S/0/1/0/all/0/1">Serban D. Porumbescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Owens_J/0/1/0/all/0/1">John D. Owens</a></p><p>We identify the graph data structure, frontiers, operators, an iterative loop
structure, and convergence conditions as essential components of graph
analytics systems based on the native-graph approach. Using these essential
components, we propose an abstraction that captures all the significant
programming models within graph analytics, such as bulk-synchronous,
asynchronous, shared-memory, message-passing, and push vs. pull traversals.
Finally, we demonstrate the power of our abstraction with an elegant modern C++
implementation of single-source shortest path and its required components.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-19T01:30:00Z">Monday, December 19 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.08599'>Computing Well-Covered Vector Spaces of Graphs using Modular Decomposition</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Martin Milani&#x10d;, Nevena Piva&#x10d;</p><p>A graph is well-covered if all its maximal independent sets have the same
cardinality. This well studied concept was introduced by Plummer in 1970 and
naturally generalizes to the weighted case. Given a graph $G$, a real-valued
vertex weight function $w$ is said to be a well-covered weighting of $G$ if all
its maximal independent sets are of the same weight. The set of all
well-covered weightings of a graph $G$ forms a vector space over the field of
real numbers, called the well-covered vector space of $G$. Since the problem of
recognizing well-covered graphs is $\mathsf{co}$-$\mathsf{NP}$-complete, the
problem of computing the well-covered vector space of a given graph is
$\mathsf{co}$-$\mathsf{NP}$-hard. Levit and Tankus showed in 2015 that the
problem admits a polynomial-time algorithm in the class of claw-free graph. In
this paper, we give two general reductions for the problem, one based on
anti-neighborhoods and one based on modular decomposition, combined with
Gaussian elimination. Building on these results, we develop a polynomial-time
algorithm for computing the well-covered vector space of a given fork-free
graph, generalizing the result of Levit and Tankus. Our approach implies that
well-covered fork-free graphs can be recognized in polynomial time and also
generalizes some known results on cographs.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Milanic_M/0/1/0/all/0/1">Martin Milani&#x10d;</a>, <a href="http://arxiv.org/find/math/1/au:+Pivac_N/0/1/0/all/0/1">Nevena Piva&#x10d;</a></p><p>A graph is well-covered if all its maximal independent sets have the same
cardinality. This well studied concept was introduced by Plummer in 1970 and
naturally generalizes to the weighted case. Given a graph $G$, a real-valued
vertex weight function $w$ is said to be a well-covered weighting of $G$ if all
its maximal independent sets are of the same weight. The set of all
well-covered weightings of a graph $G$ forms a vector space over the field of
real numbers, called the well-covered vector space of $G$. Since the problem of
recognizing well-covered graphs is $\mathsf{co}$-$\mathsf{NP}$-complete, the
problem of computing the well-covered vector space of a given graph is
$\mathsf{co}$-$\mathsf{NP}$-hard. Levit and Tankus showed in 2015 that the
problem admits a polynomial-time algorithm in the class of claw-free graph. In
this paper, we give two general reductions for the problem, one based on
anti-neighborhoods and one based on modular decomposition, combined with
Gaussian elimination. Building on these results, we develop a polynomial-time
algorithm for computing the well-covered vector space of a given fork-free
graph, generalizing the result of Levit and Tankus. Our approach implies that
well-covered fork-free graphs can be recognized in polynomial time and also
generalizes some known results on cographs.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-19T01:30:00Z">Monday, December 19 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Sunday, December 18
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://decentralizedthoughts.github.io/2022-12-18-what-is-responsiveness/'>What is Responsiveness?</a></h3>
        <p class='tr-article-feed'>from <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          In asynchronous protocols, latency to commit is a function of the actual maximum network delay $\delta$. In synchronous protocols, message delay is bounded by $\Delta$, and for $n/3 \leq f &lt; n/2$, the $\Delta$ bound is used to obtain both safety and liveness. In partial synchrony, message delay is bounded...
        
        </div>

        <div class='tr-article-summary'>
        
          
          In asynchronous protocols, latency to commit is a function of the actual maximum network delay $\delta$. In synchronous protocols, message delay is bounded by $\Delta$, and for $n/3 \leq f &lt; n/2$, the $\Delta$ bound is used to obtain both safety and liveness. In partial synchrony, message delay is bounded...
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-18T09:00:00Z">Sunday, December 18 2022, 09:00</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Friday, December 16
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/12/16/assistant-professors-at-aalto-university-apply-by-january-15-2023/'>Assistant Professors at Aalto University (apply by January 15, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The Department of Computer Science at Aalto University invites applications for tenure-track positions at the assistant professor level. We welcome applications in ALL AREAS of computer science. Website: www.aalto.fi/en/open-positions/assistant-professors-computer-science Email: laura.kuusisto-noponen@aalto.fi
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The Department of Computer Science at Aalto University invites applications for tenure-track positions at the assistant professor level. We welcome applications in ALL AREAS of computer science.</p>
<p>Website: <a href="https://www.aalto.fi/en/open-positions/assistant-professors-computer-science">https://www.aalto.fi/en/open-positions/assistant-professors-computer-science</a><br />
Email: laura.kuusisto-noponen@aalto.fi</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-16T11:50:24Z">Friday, December 16 2022, 11:50</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/182'>TR22-182 |  Criticality of AC0-Formulae | 

	Prahladh Harsha, 

	Tulasi mohan Molli, 

	A. Shankar</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Rossman [In Proc. 34th Comput. Complexity Conf., 2019] introduced the notion of criticality. The criticality of a Boolean function $f : \{0, 1\}^n\to \{0, 1\}$ is the minimum $\lambda \geq 1$ such that for all positive integers $t$,
\[Pr_{\rho\sim R_p} [\text{DT}_{\text{depth}}(f|_\rho) \geq  t] \leq (p\lambda)^t.\]
.
Håstad’s celebrated switching lemma shows that the criticality of any $k$-DNF is at most $O(k)$. Subsequent improvements to correlation bounds of AC0-circuits against parity showed that the criticality of any AC0-circuit of size $S$ and depth $d + 1$ is at most $O(\log S)^d$ and any regular AC0-formula of size $S$ and depth $d + 1$ is at most $O( \frac1d \cdot \log S)^d$. We strengthen these results by showing that the criticality of any AC0-formula (not necessarily regular) of size $S$ and depth $d + 1$ is at most $O(\frac1d\cdot\log S)^d$, resolving a conjecture due to Rossman.
This result also implies Rossman’s optimal lower bound on the size of any depth-d AC0-formula computing parity [Comput. Complexity, 27(2):209–223, 2018.]. Our result implies tight correlation bounds against parity, tight Fourier concentration results and improved #SAT algorithm for AC0-formulae.
        
        </div>

        <div class='tr-article-summary'>
        
          
          Rossman [In Proc. 34th Comput. Complexity Conf., 2019] introduced the notion of criticality. The criticality of a Boolean function $f : \{0, 1\}^n\to \{0, 1\}$ is the minimum $\lambda \geq 1$ such that for all positive integers $t$,
\[Pr_{\rho\sim R_p} [\text{DT}_{\text{depth}}(f|_\rho) \geq  t] \leq (p\lambda)^t.\]
.
Håstad’s celebrated switching lemma shows that the criticality of any $k$-DNF is at most $O(k)$. Subsequent improvements to correlation bounds of AC0-circuits against parity showed that the criticality of any AC0-circuit of size $S$ and depth $d + 1$ is at most $O(\log S)^d$ and any regular AC0-formula of size $S$ and depth $d + 1$ is at most $O( \frac1d \cdot \log S)^d$. We strengthen these results by showing that the criticality of any AC0-formula (not necessarily regular) of size $S$ and depth $d + 1$ is at most $O(\frac1d\cdot\log S)^d$, resolving a conjecture due to Rossman.
This result also implies Rossman’s optimal lower bound on the size of any depth-d AC0-formula computing parity [Comput. Complexity, 27(2):209–223, 2018.]. Our result implies tight correlation bounds against parity, tight Fourier concentration results and improved #SAT algorithm for AC0-formulae.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-16T07:31:23Z">Friday, December 16 2022, 07:31</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/181'>TR22-181 |  A New Berry-Esseen Theorem for Expander Walks | 

	Louis Golowich</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We prove that the sum of $t$ boolean-valued random variables sampled by a random walk on a regular expander converges in total variation distance to a discrete normal distribution at a rate of $O(\lambda/t^{1/2-o(1)})$, where $\lambda$ is the second largest eigenvalue of the random walk matrix in absolute value. To the best of our knowledge, among known Berry-Esseen bounds for Markov chains, our result is the first to show convergence in total variation distance, and is also the first to incorporate a linear dependence on expansion $\lambda$. In contrast, prior Markov chain Berry-Esseen bounds showed a convergence rate of $O(1/\sqrt{t})$ in weaker metrics such as Kolmogorov distance.

Our result also improves upon prior work in the pseudorandomness literature, which showed that the total variation distance is $O(\lambda)$ when the approximating distribution is taken to be a binomial distribution. We achieve the faster $O(\lambda/t^{1/2-o(1)})$ convergence rate by generalizing the binomial distribution to discrete normals of arbitrary variance. We specifically construct discrete normals using a random walk on an appropriate 2-state Markov chain. Our bound can therefore be viewed as a regularity lemma that reduces the study of arbitrary expanders to a small class of particularly simple expanders.
        
        </div>

        <div class='tr-article-summary'>
        
          
          We prove that the sum of $t$ boolean-valued random variables sampled by a random walk on a regular expander converges in total variation distance to a discrete normal distribution at a rate of $O(\lambda/t^{1/2-o(1)})$, where $\lambda$ is the second largest eigenvalue of the random walk matrix in absolute value. To the best of our knowledge, among known Berry-Esseen bounds for Markov chains, our result is the first to show convergence in total variation distance, and is also the first to incorporate a linear dependence on expansion $\lambda$. In contrast, prior Markov chain Berry-Esseen bounds showed a convergence rate of $O(1/\sqrt{t})$ in weaker metrics such as Kolmogorov distance.

Our result also improves upon prior work in the pseudorandomness literature, which showed that the total variation distance is $O(\lambda)$ when the approximating distribution is taken to be a binomial distribution. We achieve the faster $O(\lambda/t^{1/2-o(1)})$ convergence rate by generalizing the binomial distribution to discrete normals of arbitrary variance. We specifically construct discrete normals using a random walk on an appropriate 2-state Markov chain. Our bound can therefore be viewed as a regularity lemma that reduces the study of arbitrary expanders to a small class of particularly simple expanders.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-16T04:49:15Z">Friday, December 16 2022, 04:49</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/180'>TR22-180 |  A Lower Bound on the Constant in the Fourier Min-Entropy/Influence Conjecture | 

	Aniruddha  Biswas, 

	Palash Sarkar</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We describe a new construction of Boolean functions. A specific instance of our construction provides a 30-variable Boolean function having min-entropy/influence ratio to be 128/45 ? 2.8444 which is presently the highest known value of this ratio that is achieved by any Boolean function. Correspondingly, 128/45 is also presently the best known lower bound on the universal constant of the Fourier min-entropy/influence conjecture.
        
        </div>

        <div class='tr-article-summary'>
        
          
          We describe a new construction of Boolean functions. A specific instance of our construction provides a 30-variable Boolean function having min-entropy/influence ratio to be 128/45 ? 2.8444 which is presently the highest known value of this ratio that is achieved by any Boolean function. Correspondingly, 128/45 is also presently the best known lower bound on the universal constant of the Fourier min-entropy/influence conjecture.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-16T04:49:03Z">Friday, December 16 2022, 04:49</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/179'>TR22-179 |  Round-vs-Resilience Tradeoffs for Binary Feedback Channels | 

	Raghuvansh Saxena, 

	Gillat Kol, 

	Zhijun Zhang, 

	Klim Efremenko, 

	Mark Braverman</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          In a celebrated result from the $60$&#39;s, Berlekamp showed that feedback can be used to increase the maximum fraction of adversarial noise that can be tolerated by binary error correcting codes from $1/4$ to $1/3$. However, his result relies on the assumption that feedback is &quot;continuous&quot;, i.e., after every utilization of the channel, the sender gets the symbol received by the receiver. While this assumption is natural in some settings, in other settings it may be unreasonable or too costly to maintain.

In this work, we initiate the study of round-restricted feedback channels, where the number $r$ of feedback rounds is possibly much smaller than the number of utilizations of the channel. Error correcting codes for such channels are protocols where the sender can ask for feedback at most $r$ times, and, upon a feedback request, it obtains all the symbols received since its last feedback request. 

We design such error correcting protocols for both the adversarial binary erasure channel and for the adversarial binary corruption (bit flip) channel. For the erasure channel, we give an exact characterization of the round-vs-resilience tradeoff by designing a (constant rate) protocol with $r$ feedback rounds, for every $r$, and proving that the noise resilience it achieves is optimal. For the corruption channel, we give a protocol with one feedback round and prove that its optimality hinges on a &quot;clean&quot; combinatorial conjecture about the maximum cut in weighted graphs.
        
        </div>

        <div class='tr-article-summary'>
        
          
          In a celebrated result from the $60$&#39;s, Berlekamp showed that feedback can be used to increase the maximum fraction of adversarial noise that can be tolerated by binary error correcting codes from $1/4$ to $1/3$. However, his result relies on the assumption that feedback is &quot;continuous&quot;, i.e., after every utilization of the channel, the sender gets the symbol received by the receiver. While this assumption is natural in some settings, in other settings it may be unreasonable or too costly to maintain.

In this work, we initiate the study of round-restricted feedback channels, where the number $r$ of feedback rounds is possibly much smaller than the number of utilizations of the channel. Error correcting codes for such channels are protocols where the sender can ask for feedback at most $r$ times, and, upon a feedback request, it obtains all the symbols received since its last feedback request. 

We design such error correcting protocols for both the adversarial binary erasure channel and for the adversarial binary corruption (bit flip) channel. For the erasure channel, we give an exact characterization of the round-vs-resilience tradeoff by designing a (constant rate) protocol with $r$ feedback rounds, for every $r$, and proving that the noise resilience it achieves is optimal. For the corruption channel, we give a protocol with one feedback round and prove that its optimality hinges on a &quot;clean&quot; combinatorial conjecture about the maximum cut in weighted graphs.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-16T04:08:19Z">Friday, December 16 2022, 04:08</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.07713'>A Lower Bound on the Constant in the Fourier Min-Entropy/Influence Conjecture</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Aniruddha Biswas, Palash Sarkar</p><p>We describe a new construction of Boolean functions. A specific instance of
our construction provides a 30-variable Boolean function having
min-entropy/influence ratio to be $128/45 \approx 2.8444$ which is presently
the highest known value of this ratio that is achieved by any Boolean function.
Correspondingly, $128/45$ is also presently the best known lower bound on the
universal constant of the Fourier min-entropy/influence conjecture.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Biswas_A/0/1/0/all/0/1">Aniruddha Biswas</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarkar_P/0/1/0/all/0/1">Palash Sarkar</a></p><p>We describe a new construction of Boolean functions. A specific instance of
our construction provides a 30-variable Boolean function having
min-entropy/influence ratio to be $128/45 \approx 2.8444$ which is presently
the highest known value of this ratio that is achieved by any Boolean function.
Correspondingly, $128/45$ is also presently the best known lower bound on the
universal constant of the Fourier min-entropy/influence conjecture.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-16T01:30:00Z">Friday, December 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.08048'>A Graphical #SAT Algorithm for Formulae with Small Clause Density</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Tuomas Laakkonen, Konstantinos Meichanetzidis, John van de Wetering</p><p>We study the counting version of the Boolean satisfiability problem #SAT
using the ZH-calculus, a graphical language originally introduced to reason
about quantum circuits. Using this we find a natural extension of #SAT which we
call $\#SAT_\pm$, where variables are additionally labeled by phases, which is
GapP-complete. Using graphical reasoning, we find a reduction from #SAT to
$\#2SAT_\pm$ in the ZH-calculus. We observe that the DPLL algorithm for #2SAT
can be adapted to $\#2SAT_\pm$ directly and hence that Wahlstrom's
$O^*(1.2377^n)$ upper bound applies to $\#2SAT_\pm$ as well. Combining this
with our reduction from #SAT to $\#2SAT_\pm$ gives us novel upper bounds in
terms of clauses and variables that are better than $O^*(2^n)$ for small clause
densities of $\frac{m}{n} &lt; 2.25$. This is to our knowledge the first
non-trivial upper bound for #SAT that is independent of clause size. Our
algorithm improves on Dubois' upper bound for $\#kSAT$ whenever $\frac{m}{n} &lt;
1.85$ and $k \geq 4$, and the Williams' average-case analysis whenever
$\frac{m}{n} &lt; 1.21$ and $k \geq 6$. We also obtain an unconditional upper
bound of $O^*(1.88^m)$ for $\#4SAT$ in terms of clauses only, and find an
improved bound on $\#3SAT$ for $1.2577 &lt; \frac{m}{n} \leq \frac{7}{3}$. Our
results demonstrate that graphical reasoning can lead to new algorithmic
insights, even outside the domain of quantum computing that the calculus was
intended for.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Laakkonen_T/0/1/0/all/0/1">Tuomas Laakkonen</a>, <a href="http://arxiv.org/find/cs/1/au:+Meichanetzidis_K/0/1/0/all/0/1">Konstantinos Meichanetzidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Wetering_J/0/1/0/all/0/1">John van de Wetering</a></p><p>We study the counting version of the Boolean satisfiability problem #SAT
using the ZH-calculus, a graphical language originally introduced to reason
about quantum circuits. Using this we find a natural extension of #SAT which we
call $\#SAT_\pm$, where variables are additionally labeled by phases, which is
GapP-complete. Using graphical reasoning, we find a reduction from #SAT to
$\#2SAT_\pm$ in the ZH-calculus. We observe that the DPLL algorithm for #2SAT
can be adapted to $\#2SAT_\pm$ directly and hence that Wahlstrom's
$O^*(1.2377^n)$ upper bound applies to $\#2SAT_\pm$ as well. Combining this
with our reduction from #SAT to $\#2SAT_\pm$ gives us novel upper bounds in
terms of clauses and variables that are better than $O^*(2^n)$ for small clause
densities of $\frac{m}{n} &lt; 2.25$. This is to our knowledge the first
non-trivial upper bound for #SAT that is independent of clause size. Our
algorithm improves on Dubois' upper bound for $\#kSAT$ whenever $\frac{m}{n} &lt;
1.85$ and $k \geq 4$, and the Williams' average-case analysis whenever
$\frac{m}{n} &lt; 1.21$ and $k \geq 6$. We also obtain an unconditional upper
bound of $O^*(1.88^m)$ for $\#4SAT$ in terms of clauses only, and find an
improved bound on $\#3SAT$ for $1.2577 &lt; \frac{m}{n} \leq \frac{7}{3}$. Our
results demonstrate that graphical reasoning can lead to new algorithmic
insights, even outside the domain of quantum computing that the calculus was
intended for.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-16T01:30:00Z">Friday, December 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.07473'>MABSplit: Faster Forest Training Using Multi-Armed Bandits</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mo Tiwari, Ryan Kang, Je-Yong Lee, Sebastian Thrun, Chris Piech, Ilan Shomorony, Martin Jinye Zhang</p><p>Random forests are some of the most widely used machine learning models
today, especially in domains that necessitate interpretability. We present an
algorithm that accelerates the training of random forests and other popular
tree-based learning methods. At the core of our algorithm is a novel
node-splitting subroutine, dubbed MABSplit, used to efficiently find split
points when constructing decision trees. Our algorithm borrows techniques from
the multi-armed bandit literature to judiciously determine how to allocate
samples and computational power across candidate split points. We provide
theoretical guarantees that MABSplit improves the sample complexity of each
node split from linear to logarithmic in the number of data points. In some
settings, MABSplit leads to 100x faster training (an 99% reduction in training
time) without any decrease in generalization performance. We demonstrate
similar speedups when MABSplit is used across a variety of forest-based
variants, such as Extremely Random Forests and Random Patches. We also show our
algorithm can be used in both classification and regression tasks. Finally, we
show that MABSplit outperforms existing methods in generalization performance
and feature importance calculations under a fixed computational budget. All of
our experimental results are reproducible via a one-line script at
github.com/ThrunGroup/FastForest.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Tiwari_M/0/1/0/all/0/1">Mo Tiwari</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_R/0/1/0/all/0/1">Ryan Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Je-Yong Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Thrun_S/0/1/0/all/0/1">Sebastian Thrun</a>, <a href="http://arxiv.org/find/cs/1/au:+Piech_C/0/1/0/all/0/1">Chris Piech</a>, <a href="http://arxiv.org/find/cs/1/au:+Shomorony_I/0/1/0/all/0/1">Ilan Shomorony</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Martin Jinye Zhang</a></p><p>Random forests are some of the most widely used machine learning models
today, especially in domains that necessitate interpretability. We present an
algorithm that accelerates the training of random forests and other popular
tree-based learning methods. At the core of our algorithm is a novel
node-splitting subroutine, dubbed MABSplit, used to efficiently find split
points when constructing decision trees. Our algorithm borrows techniques from
the multi-armed bandit literature to judiciously determine how to allocate
samples and computational power across candidate split points. We provide
theoretical guarantees that MABSplit improves the sample complexity of each
node split from linear to logarithmic in the number of data points. In some
settings, MABSplit leads to 100x faster training (an 99% reduction in training
time) without any decrease in generalization performance. We demonstrate
similar speedups when MABSplit is used across a variety of forest-based
variants, such as Extremely Random Forests and Random Patches. We also show our
algorithm can be used in both classification and regression tasks. Finally, we
show that MABSplit outperforms existing methods in generalization performance
and feature importance calculations under a fixed computational budget. All of
our experimental results are reproducible via a one-line script at
https://github.com/ThrunGroup/FastForest.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-16T01:30:00Z">Friday, December 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.07490'>Performance Enhancement Strategies for Sparse Matrix-Vector Multiplication (SpMV) and Iterative Linear Solvers</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Thaha Mohammed, Rashid Mehmood</p><p>Iterative solutions of sparse linear systems and sparse eigenvalue problems
have a fundamental role in vital fields of scientific research and engineering.
The crucial computing kernel for such iterative solutions is the multiplication
of a sparse matrix by a dense vector. Efficient implementation of sparse
matrix-vector multiplication (SpMV) and linear solvers are therefore essential
and has been subjected to extensive research across a variety of computing
architectures and accelerators such as central processing units (CPUs),
graphical processing units (GPUs), many integrated cores (MICs), and field
programmable gate arrays (FPGAs). Unleashing the full potential of an
architecture/accelerator requires determining the factors that affect an
efficient implementation of SpMV. This article presents the first of its kind,
in-depth survey covering over two hundred state-of-the-art optimization schemes
for solving sparse iterative linear systems with a focus on computing SpMV. A
new taxonomy for iterative solutions and SpMV techniques common to all
architectures is proposed. This article includes reviews of SpMV techniques for
all architectures to consolidate a single taxonomy to encourage
cross-architectural and heterogeneous-architecture developments. However, the
primary focus is on GPUs. The major contributions as well as the primary,
secondary, and tertiary contributions of the SpMV techniques are first
highlighted utilizing the taxonomy and then qualitatively compared. A summary
of the current state of the research for each architecture is discussed
separately. Finally, several open problems and key challenges for future
research directions are outlined.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Mohammed_T/0/1/0/all/0/1">Thaha Mohammed</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehmood_R/0/1/0/all/0/1">Rashid Mehmood</a></p><p>Iterative solutions of sparse linear systems and sparse eigenvalue problems
have a fundamental role in vital fields of scientific research and engineering.
The crucial computing kernel for such iterative solutions is the multiplication
of a sparse matrix by a dense vector. Efficient implementation of sparse
matrix-vector multiplication (SpMV) and linear solvers are therefore essential
and has been subjected to extensive research across a variety of computing
architectures and accelerators such as central processing units (CPUs),
graphical processing units (GPUs), many integrated cores (MICs), and field
programmable gate arrays (FPGAs). Unleashing the full potential of an
architecture/accelerator requires determining the factors that affect an
efficient implementation of SpMV. This article presents the first of its kind,
in-depth survey covering over two hundred state-of-the-art optimization schemes
for solving sparse iterative linear systems with a focus on computing SpMV. A
new taxonomy for iterative solutions and SpMV techniques common to all
architectures is proposed. This article includes reviews of SpMV techniques for
all architectures to consolidate a single taxonomy to encourage
cross-architectural and heterogeneous-architecture developments. However, the
primary focus is on GPUs. The major contributions as well as the primary,
secondary, and tertiary contributions of the SpMV techniques are first
highlighted utilizing the taxonomy and then qualitatively compared. A summary
of the current state of the research for each architecture is discussed
separately. Finally, several open problems and key challenges for future
research directions are outlined.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-16T01:30:00Z">Friday, December 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.07533'>Correlating Theory and Practice in Finding Clubs and Plexes</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Aleksander Figiel, Tomohiro Koana, Andr&#xe9; Nichterlein, Niklas W&#xfc;nsche</p><p>Finding large "cliquish" subgraphs is a classic NP-hard graph problem. In
this work, we focus on finding maximum $s$-clubs and $s$-plexes, i.e., graphs
of diameter $s$ and graphs where each vertex is adjacent to all but $s$
vertices. Preprocessing based on Turing kernelization is a standard tool to
tackle these problems, especially on sparse graphs. We provide a new
parameterized analysis for the Turing kernelization and demonstrate their
usefulness in practice. Moreover, we provide evidence that the new theoretical
bounds indeed better explain the observed running times than the existing
theoretical running time bounds. To this end, we suggest a general method to
compare how well theoretical running time bounds fit to measured running times.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Figiel_A/0/1/0/all/0/1">Aleksander Figiel</a>, <a href="http://arxiv.org/find/cs/1/au:+Koana_T/0/1/0/all/0/1">Tomohiro Koana</a>, <a href="http://arxiv.org/find/cs/1/au:+Nichterlein_A/0/1/0/all/0/1">Andr&#xe9; Nichterlein</a>, <a href="http://arxiv.org/find/cs/1/au:+Wunsche_N/0/1/0/all/0/1">Niklas W&#xfc;nsche</a></p><p>Finding large "cliquish" subgraphs is a classic NP-hard graph problem. In
this work, we focus on finding maximum $s$-clubs and $s$-plexes, i.e., graphs
of diameter $s$ and graphs where each vertex is adjacent to all but $s$
vertices. Preprocessing based on Turing kernelization is a standard tool to
tackle these problems, especially on sparse graphs. We provide a new
parameterized analysis for the Turing kernelization and demonstrate their
usefulness in practice. Moreover, we provide evidence that the new theoretical
bounds indeed better explain the observed running times than the existing
theoretical running time bounds. To this end, we suggest a general method to
compare how well theoretical running time bounds fit to measured running times.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-16T01:30:00Z">Friday, December 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.07679'>Exact fixed-radius nearest neighbor search with an application to clustering</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xinye Chen, Stefan G&#xfc;ttel</p><p>Fixed-radius nearest-neighbor search is a common database operation that
retrieves all data points within a user-specified distance to a query point.
There are efficient approximate nearest neighbor search algorithms that provide
fast query responses but they often have a very compute-intensive indexing
phase and require parameter tuning. Therefore, exact brute force and tree-based
search methods are still widely used. Here we propose a new fixed-radius
nearest neighbor search method that significantly improves over brute force and
tree-based methods in terms of index and query time, returns exact results, and
requires no parameter tuning. The method exploits a sorting of the data points
by their first principal component, thereby facilitating a reduction in query
search space. Further speedup is gained from an efficient implementation using
high-level Basic Linear Algebra Subprograms (BLAS). We provide theoretical
analysis of our method and demonstrate its practical performance when used
stand-alone and when applied within a clustering algorithm.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinye Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Guttel_S/0/1/0/all/0/1">Stefan G&#xfc;ttel</a></p><p>Fixed-radius nearest-neighbor search is a common database operation that
retrieves all data points within a user-specified distance to a query point.
There are efficient approximate nearest neighbor search algorithms that provide
fast query responses but they often have a very compute-intensive indexing
phase and require parameter tuning. Therefore, exact brute force and tree-based
search methods are still widely used. Here we propose a new fixed-radius
nearest neighbor search method that significantly improves over brute force and
tree-based methods in terms of index and query time, returns exact results, and
requires no parameter tuning. The method exploits a sorting of the data points
by their first principal component, thereby facilitating a reduction in query
search space. Further speedup is gained from an efficient implementation using
high-level Basic Linear Algebra Subprograms (BLAS). We provide theoretical
analysis of our method and demonstrate its practical performance when used
stand-alone and when applied within a clustering algorithm.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-16T01:30:00Z">Friday, December 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.07682'>Min-max Submodular Ranking for Multiple Agents</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Qingyun Chen, Sungjin Im, Benjamin Moseley, Chenyang Xu, Ruilong Zhang</p><p>In the submodular ranking (SR) problem, the input consists of a set of
submodular functions defined on a ground set of elements. The goal is to order
elements for all the functions to have value above a certain threshold as soon
on average as possible, assuming we choose one element per time. The problem is
flexible enough to capture various applications in machine learning, including
decision trees.
</p>
<p>This paper considers the min-max version of SR where multiple instances share
the ground set. With the view of each instance being associated with an agent,
the min-max problem is to order the common elements to minimize the maximum
objective of all agents -- thus, finding a fair solution for all agents. We
give approximation algorithms for this problem and demonstrate their
effectiveness in the application of finding a decision tree for multiple
agents.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qingyun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1">Sungjin Im</a>, <a href="http://arxiv.org/find/cs/1/au:+Moseley_B/0/1/0/all/0/1">Benjamin Moseley</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chenyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruilong Zhang</a></p><p>In the submodular ranking (SR) problem, the input consists of a set of
submodular functions defined on a ground set of elements. The goal is to order
elements for all the functions to have value above a certain threshold as soon
on average as possible, assuming we choose one element per time. The problem is
flexible enough to capture various applications in machine learning, including
decision trees.
</p>
<p>This paper considers the min-max version of SR where multiple instances share
the ground set. With the view of each instance being associated with an agent,
the min-max problem is to order the common elements to minimize the maximum
objective of all agents -- thus, finding a fair solution for all agents. We
give approximation algorithms for this problem and demonstrate their
effectiveness in the application of finding a decision tree for multiple
agents.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-16T01:30:00Z">Friday, December 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.07870'>Parameterized Algorithms for String Matching to DAGs: Funnels and Beyond</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Manuel Caceres</p><p>The problem of String Matching to Labeled Graphs (SMLG) asks to find all the
paths in a labeled graph $G = (V, E)$ whose spellings match that of an input
string $S \in \Sigma^m$. SMLG can be solved in quadratic $O(m|E|)$ time [Amir
et al., JALG], which was proven to be optimal by a recent lower bound
conditioned on SETH [Equi et al., ICALP 2019]. The lower bound states that no
strongly subquadratic time algorithm exists, even if restricted to directed
acyclic graphs (DAGs).
</p>
<p>In this work we present the first parameterized algorithms for SMLG in DAGs.
Our parameters capture the topological structure of $G$. All our results are
derived from a generalization of the Knuth-Morris-Pratt algorithm [Park and
Kim, CPM 1995] optimized to work in time proportional to the number of
prefix-incomparable matches.
</p>
<p>To obtain the parameterization in the topological structure of $G$, we first
study a special class of DAGs called funnels [Millani et al., JCO] and
generalize them to $k$-funnels and the class $ST_k$. We present several novel
characterizations and algorithmic contributions on both funnels and their
generalizations.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Caceres_M/0/1/0/all/0/1">Manuel Caceres</a></p><p>The problem of String Matching to Labeled Graphs (SMLG) asks to find all the
paths in a labeled graph $G = (V, E)$ whose spellings match that of an input
string $S \in \Sigma^m$. SMLG can be solved in quadratic $O(m|E|)$ time [Amir
et al., JALG], which was proven to be optimal by a recent lower bound
conditioned on SETH [Equi et al., ICALP 2019]. The lower bound states that no
strongly subquadratic time algorithm exists, even if restricted to directed
acyclic graphs (DAGs).
</p>
<p>In this work we present the first parameterized algorithms for SMLG in DAGs.
Our parameters capture the topological structure of $G$. All our results are
derived from a generalization of the Knuth-Morris-Pratt algorithm [Park and
Kim, CPM 1995] optimized to work in time proportional to the number of
prefix-incomparable matches.
</p>
<p>To obtain the parameterization in the topological structure of $G$, we first
study a special class of DAGs called funnels [Millani et al., JCO] and
generalize them to $k$-funnels and the class $ST_k$. We present several novel
characterizations and algorithmic contributions on both funnels and their
generalizations.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-16T01:30:00Z">Friday, December 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.07907'>Automatic vehicle trajectory data reconstruction at scale</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yanbing Wang, Derek Gloudemans, Zi Nean Teoh, Lisa Liu, Gergely Zach&#xe1;r, William Barbour, Daniel Work</p><p>Vehicle trajectory data has received increasing research attention over the
past decades. With the technological sensing improvements such as
high-resolution video cameras, in-vehicle radars and lidars, abundant
individual and contextual traffic data is now available. However, though the
data quantity is massive, it is by itself of limited utility for traffic
research because of noise and systematic sensing errors, thus necessitates
proper processing to ensure data quality. We draw particular attention to
extracting high-resolution vehicle trajectory data from video cameras as
traffic monitoring cameras are becoming increasingly ubiquitous. We explore
methods for automatic trajectory data reconciliation, given "raw" vehicle
detection and tracking information from automatic video processing algorithms.
We propose a pipeline including a) an online data association algorithm to
match fragments that are associated to the same object (vehicle), which is
formulated as a min-cost network flow problem of a graph, and b) a trajectory
reconciliation method formulated as a quadratic program to enhance raw
detection data. The pipeline leverages vehicle dynamics and physical
constraints to associate tracked objects when they become fragmented, remove
measurement noise on trajectories and impute missing data due to
fragmentations. The accuracy is benchmarked on a sample of manually-labeled
data, which shows that the reconciled trajectories improve the accuracy on all
the tested input data for a wide range of measures. An online version of the
reconciliation pipeline is implemented and will be applied in a continuous
video processing system running on a camera network covering a 4-mile stretch
of Interstate-24 near Nashville, Tennessee.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanbing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gloudemans_D/0/1/0/all/0/1">Derek Gloudemans</a>, <a href="http://arxiv.org/find/cs/1/au:+Teoh_Z/0/1/0/all/0/1">Zi Nean Teoh</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lisa Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zachar_G/0/1/0/all/0/1">Gergely Zach&#xe1;r</a>, <a href="http://arxiv.org/find/cs/1/au:+Barbour_W/0/1/0/all/0/1">William Barbour</a>, <a href="http://arxiv.org/find/cs/1/au:+Work_D/0/1/0/all/0/1">Daniel Work</a></p><p>Vehicle trajectory data has received increasing research attention over the
past decades. With the technological sensing improvements such as
high-resolution video cameras, in-vehicle radars and lidars, abundant
individual and contextual traffic data is now available. However, though the
data quantity is massive, it is by itself of limited utility for traffic
research because of noise and systematic sensing errors, thus necessitates
proper processing to ensure data quality. We draw particular attention to
extracting high-resolution vehicle trajectory data from video cameras as
traffic monitoring cameras are becoming increasingly ubiquitous. We explore
methods for automatic trajectory data reconciliation, given "raw" vehicle
detection and tracking information from automatic video processing algorithms.
We propose a pipeline including a) an online data association algorithm to
match fragments that are associated to the same object (vehicle), which is
formulated as a min-cost network flow problem of a graph, and b) a trajectory
reconciliation method formulated as a quadratic program to enhance raw
detection data. The pipeline leverages vehicle dynamics and physical
constraints to associate tracked objects when they become fragmented, remove
measurement noise on trajectories and impute missing data due to
fragmentations. The accuracy is benchmarked on a sample of manually-labeled
data, which shows that the reconciled trajectories improve the accuracy on all
the tested input data for a wide range of measures. An online version of the
reconciliation pipeline is implemented and will be applied in a continuous
video processing system running on a camera network covering a 4-mile stretch
of Interstate-24 near Nashville, Tennessee.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-16T01:30:00Z">Friday, December 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.07997'>Beyond Square-root Error: New Algorithms for Differentially Private All Pairs Shortest Distances</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Chengyuan Deng, Jie Gao, Jalaj Upadhyay, Chen Wang</p><p>We consider the problem of releasing all pairs shortest path distances (APSD)
in a weighted undirected graph with differential privacy (DP) guarantee. We
consider the weight-level privacy introduced by Sealfon [PODS'16], where the
graph topology is public but the edge weight is considered sensitive and
protected from inference via the released all pairs shortest distances. The
privacy guarantee ensures that the probability of differentiating two sets of
edge weights on the same graph differing by an $\ell_1$ norm of $1$ is bounded.
The goal is to minimize the additive error introduced to the released APSD
while meeting the privacy guarantee. The best bounds known (Chen et al.
[SODA'23]; Fan et al. [Arxiv'22]) is an $\tilde{O}(n^{2/3})$ additive error for
$\varepsilon$-DP and an $\tilde{O}(n^{1/2})$ additive error for $(\varepsilon,
\delta)$-DP.
</p>
<p>In this paper, we present new algorithms with improved additive error bounds:
$\tilde{O}(n^{1/3})$ for $\varepsilon$-DP and $\tilde{O}(n^{1/4})$ for
$(\varepsilon, \delta)$-DP, narrowing the gap with the current lower bound of
$\tilde{\Omega}(n^{1/6})$ for $(\varepsilon, \delta)$-DP. The algorithms use
new ideas to carefully inject noises to a selective subset of shortest path
distances so as to control both `sensitivity' (the maximum number of times an
edge is involved) and the number of these perturbed values needed to produce
each of the APSD output. In addition, we also obtain, for $(\varepsilon,
\delta)$-DP shortest distances on lines, trees and cycles, a lower bound of
$\Omega(\log{n})$ for the additive error through a formulation by the matrix
mechanism.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1">Chengyuan Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jie Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Upadhyay_J/0/1/0/all/0/1">Jalaj Upadhyay</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chen Wang</a></p><p>We consider the problem of releasing all pairs shortest path distances (APSD)
in a weighted undirected graph with differential privacy (DP) guarantee. We
consider the weight-level privacy introduced by Sealfon [PODS'16], where the
graph topology is public but the edge weight is considered sensitive and
protected from inference via the released all pairs shortest distances. The
privacy guarantee ensures that the probability of differentiating two sets of
edge weights on the same graph differing by an $\ell_1$ norm of $1$ is bounded.
The goal is to minimize the additive error introduced to the released APSD
while meeting the privacy guarantee. The best bounds known (Chen et al.
[SODA'23]; Fan et al. [Arxiv'22]) is an $\tilde{O}(n^{2/3})$ additive error for
$\varepsilon$-DP and an $\tilde{O}(n^{1/2})$ additive error for $(\varepsilon,
\delta)$-DP.
</p>
<p>In this paper, we present new algorithms with improved additive error bounds:
$\tilde{O}(n^{1/3})$ for $\varepsilon$-DP and $\tilde{O}(n^{1/4})$ for
$(\varepsilon, \delta)$-DP, narrowing the gap with the current lower bound of
$\tilde{\Omega}(n^{1/6})$ for $(\varepsilon, \delta)$-DP. The algorithms use
new ideas to carefully inject noises to a selective subset of shortest path
distances so as to control both `sensitivity' (the maximum number of times an
edge is involved) and the number of these perturbed values needed to produce
each of the APSD output. In addition, we also obtain, for $(\varepsilon,
\delta)$-DP shortest distances on lines, trees and cycles, a lower bound of
$\Omega(\log{n})$ for the additive error through a formulation by the matrix
mechanism.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-16T01:30:00Z">Friday, December 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.08018'>Privately Estimating a Gaussian: Efficient, Robust and Optimal</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Daniel Alabi, Pravesh K. Kothari, Pranay Tankala, Prayaag Venkat, Fred Zhang</p><p>In this work, we give efficient algorithms for privately estimating a
Gaussian distribution in both pure and approximate differential privacy (DP)
models with optimal dependence on the dimension in the sample complexity. In
the pure DP setting, we give an efficient algorithm that estimates an unknown
$d$-dimensional Gaussian distribution up to an arbitrary tiny total variation
error using $\widetilde{O}(d^2 \log \kappa)$ samples while tolerating a
constant fraction of adversarial outliers. Here, $\kappa$ is the condition
number of the target covariance matrix. The sample bound matches best
non-private estimators in the dependence on the dimension (up to a
polylogarithmic factor). We prove a new lower bound on differentially private
covariance estimation to show that the dependence on the condition number
$\kappa$ in the above sample bound is also tight. Prior to our work, only
identifiability results (yielding inefficient super-polynomial time algorithms)
were known for the problem. In the approximate DP setting, we give an efficient
algorithm to estimate an unknown Gaussian distribution up to an arbitrarily
tiny total variation error using $\widetilde{O}(d^2)$ samples while tolerating
a constant fraction of adversarial outliers. Prior to our work, all efficient
approximate DP algorithms incurred a super-quadratic sample cost or were not
outlier-robust. For the special case of mean estimation, our algorithm achieves
the optimal sample complexity of $\widetilde O(d)$, improving on a $\widetilde
O(d^{1.5})$ bound from prior work. Our pure DP algorithm relies on a recursive
private preconditioning subroutine that utilizes the recent work on private
mean estimation [Hopkins et al., 2022]. Our approximate DP algorithms are based
on a substantial upgrade of the method of stabilizing convex relaxations
introduced in [Kothari et al., 2022].
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Alabi_D/0/1/0/all/0/1">Daniel Alabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kothari_P/0/1/0/all/0/1">Pravesh K. Kothari</a>, <a href="http://arxiv.org/find/cs/1/au:+Tankala_P/0/1/0/all/0/1">Pranay Tankala</a>, <a href="http://arxiv.org/find/cs/1/au:+Venkat_P/0/1/0/all/0/1">Prayaag Venkat</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fred Zhang</a></p><p>In this work, we give efficient algorithms for privately estimating a
Gaussian distribution in both pure and approximate differential privacy (DP)
models with optimal dependence on the dimension in the sample complexity. In
the pure DP setting, we give an efficient algorithm that estimates an unknown
$d$-dimensional Gaussian distribution up to an arbitrary tiny total variation
error using $\widetilde{O}(d^2 \log \kappa)$ samples while tolerating a
constant fraction of adversarial outliers. Here, $\kappa$ is the condition
number of the target covariance matrix. The sample bound matches best
non-private estimators in the dependence on the dimension (up to a
polylogarithmic factor). We prove a new lower bound on differentially private
covariance estimation to show that the dependence on the condition number
$\kappa$ in the above sample bound is also tight. Prior to our work, only
identifiability results (yielding inefficient super-polynomial time algorithms)
were known for the problem. In the approximate DP setting, we give an efficient
algorithm to estimate an unknown Gaussian distribution up to an arbitrarily
tiny total variation error using $\widetilde{O}(d^2)$ samples while tolerating
a constant fraction of adversarial outliers. Prior to our work, all efficient
approximate DP algorithms incurred a super-quadratic sample cost or were not
outlier-robust. For the special case of mean estimation, our algorithm achieves
the optimal sample complexity of $\widetilde O(d)$, improving on a $\widetilde
O(d^{1.5})$ bound from prior work. Our pure DP algorithm relies on a recursive
private preconditioning subroutine that utilizes the recent work on private
mean estimation [Hopkins et al., 2022]. Our approximate DP algorithms are based
on a substantial upgrade of the method of stabilizing convex relaxations
introduced in [Kothari et al., 2022].
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-16T01:30:00Z">Friday, December 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Thursday, December 15
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://rjlipton.wpcomstaging.com/2022/12/15/a-mutation-carol-2/'>A Mutation Carol 2</a></h3>
        <p class='tr-article-feed'>from <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Ghosts of creations past and citations not present Domenico Amalfitano, Ana Paiva, Alexis Inquel, Luis Pinto, Anna Rita Fasolino, and Ren&#233; Just are the authors of an article in this month&#8217;s Communications of the ACM. Their article is on the program testing method called mutation. Today we discuss how far back citations should go. When [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p><font color="#0044cc"><br />
<em>Ghosts of creations past and citations not present</em><br />
<font color="#000000"></p>
<p><a href="https://rjlipton.wpcomstaging.com/2022/12/15/a-mutation-carol-2/apipfj/" rel="attachment wp-att-20628"><img data-attachment-id="20628" data-permalink="https://rjlipton.wpcomstaging.com/2022/12/15/a-mutation-carol-2/apipfj/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?fit=3264%2C3264&amp;ssl=1" data-orig-size="3264,3264" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="APIPFJ" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?fit=300%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?fit=600%2C600&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ-150x150.jpg?resize=150%2C150&#038;ssl=1" alt="" width="150" height="150" class="alignright size-thumbnail wp-image-20628" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=1024%2C1024&amp;ssl=1 1024w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=1536%2C1536&amp;ssl=1 1536w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=2048%2C2048&amp;ssl=1 2048w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=1200%2C1200&amp;ssl=1 1200w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=800%2C800&amp;ssl=1 800w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=400%2C400&amp;ssl=1 400w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=200%2C200&amp;ssl=1 200w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?w=1800&amp;ssl=1 1800w" sizes="(max-width: 150px) 100vw, 150px" data-recalc-dims="1" /></a></p>
<p>
Domenico Amalfitano, Ana Paiva, Alexis Inquel, Luis Pinto, Anna Rita Fasolino, and Ren&eacute; Just are the authors of an article in this month&#8217;s Communications of the <a href="https://cacm.acm.org/magazines/2022/12">ACM</a>. Their <a href="https://cacm.acm.org/magazines/2022/12/266928-how-do-java-mutation-tools-differ/fulltext">article</a> is on the program testing method called <a href="https://en.wikipedia.org/wiki/Mutation_testing">mutation</a>. </p>
<p>
Today we discuss how far back citations should go.</p>
<p>
When I opened this month&#8217;s CACM hardcopy and saw the title &#8220;How do Java Mutation Tools Differ?,&#8221; I looked at the article&#8217;s references. Like most researchers, I am proud and enjoy seeing my work cited. Their article leads with the following as a major reference:</p>
<ul>
<li>
[33] Jeff Offutt. &#8220;A Mutation Carol: Past, Present, and Future.&#8221; <em>Information and Software Technology</em> <b>53</b>, 10 (2011), 1098&#8211;1107.
</ul>
<p>
This paper is cited twice, sandwiched around a mention of a 2019 survey. The second time is for a definition of <em>mutation analysis</em> as, &#8220;the use of well-defined rules defined on syntactic descriptions to make systematic changes to the syntax or to objects developed from the syntax.&#8221; There is only one citation dated before 2001, a 1992 paper by Offutt. </p>
<p>
What isn&#8217;t cited is anything from the more distant past, before the Internet, before <a href="https://www.indystar.com/story/entertainment/2021/12/09/seinfeld-festivus-episode-netflix-christmas-holiday-show-larry-david/8842569002/">Seinfeld</a> and <a href="https://en.wikipedia.org/wiki/Simpsons_Roasting_on_an_Open_Fire">The Simpsons</a>. In particular, not this:</p>
<ul>
<li>
Richard A. DeMillo, Richard J. Lipton, and Fred G. Sayward. &#8220;Hints on Test Data Selection: Help for the Practicing Programmer.&#8221; <em>IEEE Computer</em> <b>11</b>, 4 (1978), 34&#8211;41.
</ul>
<p>
<p><H2> Bringing Past to Present </H2></p>
<p><p>
I was shocked, then upset, and then amazed. This isn&#8217;t <em>plagiarizing</em>, but there is still a sense of using someone else&#8217;s ideas as currency without giving credit. Or maybe mutation testing is now coin-of-the-realm? Whatever, I felt <a href="https://www.rogerebert.com/reviews/scrooged-1988">scrooged</a>&#8212;or rather, &#8220;ghosted.&#8221; </p>
<p>
My disorientation was alleviated upon looking at the &#8220;Mutation Carol&#8221; paper after a prompt from Ken. It not only cites the 1978 paper but, with echoes of the Charles Dickens story, takes me all the way back to my school days:</p>
<blockquote><p><b> </b> <em> &#8220;Legend has it that the first ideas of mutation analysis were postulated in 1971 in a class term paper by Richard Lipton [2]. Depending on who we ask, his professor, Dave Parnas, either thought mutation was a bad idea or a reasonably clever idea that was not worthy of a PhD dissertation. The first research project was started in the late 1970s by DeMillo (Georgia Tech), Lipton (Princeton), and Sayward (Yale).&#8221; </em>
</p></blockquote>
<p><p>
The term paper that Offutt referred to is:</p>
<ul>
<li>
Richard J. Lipton. &#8220;Fault diagnosis of computer programs.&#8221; Technical Report, Student Report, Carnegie Mellon University, 1971.
</ul>
<p>
I am not suggesting people should cite that. But a second kind of source to cite in an applied survey is the <em>first implementation</em>. This source was far from tiny: Tim Budd&#8217;s PhD dissertation titled <a href="https://www.google.com/books/edition/_/V1b1vgEACAAJ?hl=en">Mutation Analysis</a> in 1980 from Yale University. Well, Offutt references Budd copiously in his next paragraphs, besides citing papers by him and others. And Offutt should know&#8212;he was a PhD student of DeMillo later in the 1980s. </p>
<p>
I guess&#8212;letting my heart soften a little here&#8212;the authors of the CACM paper figured that their major reference [33] sufficed for the record, all the more since its author was in the originators&#8217; circle. But readers may not look at paper 33. Holding hardcopy, one cannot. This leads to a wider question.</p>
<p>
<p><H2> Citation Proprieties </H2></p>
<p><p>
The question is, (when) <em>should one cite a paper that one hasn&#8217;t actually consulted?</em> Ken calls this &#8220;Transitive Citation.&#8221; Is it a vice? Here are some considerations:</p>
<ul>
<li>
The citation could be based on memory of having read the paper in the past. Even if you didn&#8217;t look at the paper while doing your current project, it may be material to your knowledge. </p>
<li>
Supposing one never read the stem paper&#8212;that a survey or monograph sufficed&#8212;the stem paper may still be more accessible or concisely informative for readers. </p>
<li>
The transitively cited papers may be used to set a context or tell a story. This is one reason many in computer science, especially for conference papers, use the &#8220;alpha&#8221; style of citation, like [DLS78] for the above paper. It takes less space that writing out the author names and spares readers who recognize the tag the interruption of going to the references. </p>
<li>
On the other hand, it may be that the contents of the stem paper have attained the status of <a href="https://libguides.sjsu.edu/plagiarism/what-does-not-need-to-be-cited">common knowledge</a> that does not need to be cited.
</ul>
<p>
What is common knowledge can be tricky because it depends on the scope of the audience. It is not just what they know but how readily they can find sources. For instance, the above link from the San Jos&eacute; State University Library lists the following as examples that need not be cited:</p>
<ul>
<li>
Abraham Lincoln was the 16th President of the United States. </p>
<li>
Sacramento is the capital of California. </p>
<li>
A genome is all the DNA in an organism, including its genes.
</ul>
<p>
The second item might not be known by a non-American (or even by an American) and the third presupposes memory of secondary school science. Yet the point is that they were established long ago and can be &#8220;found in many sources&#8221; as the link states. But in our case, the presence of many sources&#8212;when they have a unique and agreed lower bound&#8212;comes back to our original questions.</p>
<p>
<p><H2> Open Problems </H2></p>
<p><p>
What should we do about this paper? Did they violate basic citation rules? Or is it okay since our initial creation of the mutation method is well known to all those who work in the area&#8212;and the impressive panoply of tools covered in their article go well beyond origins? </p>
<p>
What do you think?  </p>
<p class="authors">By RJLipton+KWRegan</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-15T23:18:00Z">Thursday, December 15 2022, 23:18</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2022/12/15/linkage-end-fall.html'>Linkage for the end of the Fall term</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          This week’s hype (\(\mathbb{M}\), more, still more, even more, one more). And that’s just the Not Even Wrong posts; see them for a more thorough link and media roundup. Short summary: physicists ran a simulation of a quantum physics model that is unrelated to the conventional notion of wormholes from general relativity, sharing only a name with it. They used a quantum computer for the simulation even though it could as well have been run on a classical computer. And then they screamed from the rooftops that they had created the world’s first wormhole, apparently deliberately misleading everyone who didn’t read the fine print (including many major media outlets and research administrators) into thinking that they had brought into existence a physical wormhole.
        
        </div>

        <div class='tr-article-summary'>
        
          
          <ul>
  <li>
    <p><a href="https://www.math.columbia.edu/~woit/wordpress/?p=13181">This week’s hype</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@johncarlosbaez/109438052685084970">\(\mathbb{M}\)</a>,</span> <a href="https://www.math.columbia.edu/~woit/wordpress/?p=13209">more</a>, <a href="https://www.math.columbia.edu/~woit/wordpress/?p=13229">still more</a>, <a href="https://www.math.columbia.edu/~woit/wordpress/?p=13251">even more</a>, <a href="https://www.math.columbia.edu/~woit/wordpress/?p=13256">one more</a>). And that’s just the <em>Not Even Wrong</em> posts; see them for a more thorough link and media roundup. Short summary: physicists ran a simulation of a quantum physics model that is unrelated to the conventional notion of wormholes from general relativity, sharing only a name with it. They used a quantum computer for the simulation even though it could as well have been run on a classical computer. And then they screamed from the rooftops that they had created the world’s first wormhole, apparently deliberately misleading everyone who didn’t read the fine print (including many major media outlets and research administrators) into thinking that they had brought into existence a physical wormhole.</p>
  </li>
  <li>
    <p>When I viewed <a href="https://www.techradar.com/news/youre-not-wrong-websites-have-way-more-trackers-now">a recent blog post complaining about the increasing number of trackers embedded on social media websites</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109448435744739337">\(\mathbb{M}\)</a>),</span> including stats obtained using uBlock origin, I found that uBlock origin blocked 17 items from the post. On the social media websites I frequent (<a href="https://mathstodon.xyz/@11011110">mathstodon.xyz</a> and this blog), it blocks 0 items. Hmm.</p>
  </li>
  <li>
    <p>Certain phrases, once so commonplace that you could use them in analogies to explain more abstruse mathematical concepts and be instantly understood, have fallen by the wayside <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109451664672868335">\(\mathbb{M}\)</a>).</span> If you try to use them in the same way now, you will be met by blank stares instead of understanding. They are archaic and need to be retired from this sort of use. Today’s example: “telephone line”.</p>
  </li>
  <li>
    <p><a href="http://homepages.gac.edu/~jsiehler/NoThree/noThree.html">Playable puzzles based on the no-three-in-line problem</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@jsiehler/109439179003169135">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=jOTTZtVPrgo">Can the same net fold into two shapes</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@robinhouston/109449485664418535">\(\mathbb{M}\)</a>)?</span> Matt Parker explores what’s known about polyominoes that fold into more than one cuboid, with a nice shoutout to Demaine and O’Rourke’s <em>Geometric Folding Algorithms</em>. Still unknown: can you find one that folds into more than three cuboids?</p>
  </li>
  <li>
    <p>Another batch of three new mathematical Wikipedia Good Articles <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109468237232647886">\(\mathbb{M}\)</a>):</span></p>

    <ul>
      <li>
        <p>How many \(k\)-element subsets of \([1,n]\) can you find so that all pairs intersect? The answer is the <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Ko%E2%80%93Rado_theorem">Erdős–Ko–Rado theorem</a>.</p>
      </li>
      <li>
        <p>How many patterns of pairwise connections can the subscribers to a telephone system form? The answer is a <a href="https://en.wikipedia.org/wiki/Telephone_number_(mathematics)">telephone number</a>.</p>
      </li>
      <li>
        <p>Which graphs can you draw so that all vertices are one unit apart? The answer is a <a href="https://en.wikipedia.org/wiki/Unit_distance_graph">unit distance graph</a>.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@Danpiker/109467207392971380">Helicoidal decomposition of a tetrahedron into four identical curvy shapes</a>, closely related to the <a href="https://incoherency.co.uk/blog/stories/hanayama-cast-marble.html">Hanayama cast “Marble” puzzle</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109468194107944510">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@stecks/109477453878139223">Katie Steckles enjoyes the presence of a table of polar plots of real spherical harmonic amplitude from Wikipedia in an image search for “Twelve Days of Christmas”</a>.</p>
  </li>
  <li>
    <p>I recently learned, from <a href="https://retractionwatch.com/2022/12/05/a-paper-used-capital-ts-instead-of-error-bars-but-wait-theres-more/">an article about the entertainingly amateurish fake statistics in a bad paper they published</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109492742952418728">\(\mathbb{M}\)</a>),</span> that dubious journal publisher Hindawi was recently taken over by somewhat more reputable journal publisher Wiley, and that Wiley is rightly worried about the bad reputation Hindawi is casting on them.</p>
  </li>
  <li>
    <p>As a <a href="https://en.wikipedia.org/wiki/Wigner_crystal">Wigner crystal</a> is <a href="https://mathstodon.xyz/@lisyarus@mastodon.gamedev.place/109485736245660088">dynamically deformed</a>, its grid defects move around “in a kind of cosmic way”. Simulations by Nikita Lisitsa and Ricky Reusser.</p>
  </li>
  <li>
    <p><a href="https://techintersections.org/">Tech Intersections: Women of Color in Computing</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@ellenspertus@mastodon.lol/109484842652745472">\(\mathbb{M}\)</a>),</span> upcoming conference at Mills College in Oakland, California, January 28.</p>
  </li>
  <li>
    <p><a href="https://roganbrown.com/home.html">Rogan Brown – Paper Sculptures</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@Mndah@mastodon.art/109460158785990968">\(\mathbb{M}\)</a>).</span> Delicately cut traceries resembling microfauna.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@Danpiker/109512464477416714">What aspect ratios of rectangles can be used to tile a square?</a> <a href="https://mathstodon.xyz/@johncarlosbaez/109517010782719784">More</a>. Two long Mastodon threads. A couple of pointers into the middle of them: For recursive \(1\)-to-\((n-1)\) guillotine partitions, there’s <a href="https://mathstodon.xyz/@11011110/109514032375078374">a nice recursive construction for the polynomials</a> whose roots give you the possible aspect ratios. And <a href="https://mathstodon.xyz/@11011110/109519145856607390">one way to form subdivisions of a square into equally-oriented similar rectangles</a> is to start with a <a href="https://en.wikipedia.org/wiki/Squaring_the_square">squared square</a> or <a href="http://www.squaring.net/sq/sr/sr.html">squared rectangle</a> and then scale the axes.</p>
  </li>
  <li>
    <p>Even those so old-fashioned as to become monks and build stone Gothic cathedrals in the wilderness <a href="https://carmelitegothic.com/cnc-stone-carving/">use 3d modeling and CNC machining instead of painstaking hand carving</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109519963114960008">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=33940043">via</a>). As they write, “Medieval builders were always on the forefront of the technology of their day … To build Gothic today, it must become a reality, not a romantic idea locked up forever in one’s head.” See also <a href="https://www.arup.com/projects/sagrada-familia">a related article on digital modeling for the Sagrada Familia</a>.</p>
  </li>
</ul><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-15T17:45:00Z">Thursday, December 15 2022, 17:45</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://blog.computationalcomplexity.org/2022/12/fintech-is-dead-long-live-fintech.html'>FinTech is Dead, Long Live FinTech</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Bill didn't feel he had the expertise to share new insights on the FTX affair. Never stopped me.</p><p>FTX is nothing short of corporate malfeasance in a poorly regulated industry, and since Bill had posted, Sam Bankman-Fried was arrested and charged with fraud and conspiracy. The whole affair has rippled through the cryptocurrency ecosphere and will likely lead to a much higher regulatory framework than the industry was hoping for.</p><p>Cryptocurrencies have had an interesting 2022 starting with the Superbowl ads Bill was rallying against. FTX even sponsored baseball umpires, sports arenas&nbsp;and fortune cookies including this prescient one</p>♦<p>Those were in the glory times of FinTech, i.e. early 2022. Even before FTX had its problems, we had the Three Arrows Capital bankruptcy and the TerraUSD stablecoin collapse. Bitcoin lost over 60% of its value during the year.&nbsp;</p><p>Of all the problems we've seen with cryptocurrency, it's never because of the crypto. I'm still shocked how the person or people called Satoshi Nakamoto got it mostly right on the first try. They did fail to account for the energy needed to mine bitcoin once bitcoin became valuable, but on the other hand who could have predicted bitcoin would become so valuable. Certainly not me.</p><p>I still don't see much of a market for most cryptocurrencies beyond speculation and illegal activities, though we have much of both. Nevertheless it feels weird in 2022 that we still use physical bills and coins. I am hopeful for a CBDC (Central Bank Digital Currency), not a stablecoin tied to a dollar, but US currency itself just in digital form.</p><p>We now use digital tickets for most events and transport. Even driver's licenses&nbsp;and passports are moving digital--they only gets scanned anyway. There are a few remaining uses of paper--property titles, legal wills, social security cards, birth, death, marriage and divorce certificates. Perhaps we could use blockchain or even centralized databases to eliminate these as well.</p><p>Blockchain could also be helpful to track our digital goods, so we can use the music, movies, books, games, virtual clothes and accoutrements on different platforms without having to buy them twice. (HT Siva)</p><p>The most important use of FinTech will be for those who have the least, those who pay large fees to have a bank account, cash a paycheck or send money to family overseas. FinTech can be a great democratizing tool, if we use it that way and not just as another get rich scheme.</p><p>By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Bill <a href="https://blog.computationalcomplexity.org/2022/12/commercials-are-not-logical-ftx-edition.html">didn't feel</a> he had the expertise to share new insights on the FTX affair. Never stopped me.</p><p>FTX is nothing short of corporate malfeasance in a poorly regulated industry, and since Bill had posted, Sam Bankman-Fried was arrested and charged with fraud and conspiracy. The whole affair has rippled through the cryptocurrency ecosphere and will likely lead to a much higher regulatory framework than the industry was hoping for.</p><p>Cryptocurrencies have had an <a href="https://en.wikipedia.org/wiki/Cryptocurrency_bubble#2021%E2%80%932022_crash">interesting 2022</a> starting with the <a href="https://www.youtube.com/watch?v=gI0oRtEu7s0">Superbowl</a> <a href="https://youtu.be/hWMnbJJpeZc">ads</a> Bill was rallying against. FTX even sponsored <a href="https://news.sportslogos.net/2021/10/09/explaining-the-ftx-patch-worn-by-mlb-umpires/baseball/">baseball umpires</a>, <a href="https://frontofficesports.com/ftx-remains-attached-to-miami-heat-arena-month-after-bankruptcy/">sports arenas</a>&nbsp;and fortune cookies including this prescient one</p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaSJFlLuWRhcO_BHTa4HDGrDftf95SK4jBdpAbodo1hn2XhcSrJ01-HJuEpuut8tjd55UgMU_VGtKxTUv2XayM1VbEkkeWLyfZKQ1odpKo9mMXI0M4yuAHvEVNgroslbnrzZofIO14OmXLATmrwI9OnmGvV0NAqrc_S2ZAQpV2u8qAljo8sA/s767/PXL_20220621_174821537.jpg" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="219" data-original-width="767" height="91" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaSJFlLuWRhcO_BHTa4HDGrDftf95SK4jBdpAbodo1hn2XhcSrJ01-HJuEpuut8tjd55UgMU_VGtKxTUv2XayM1VbEkkeWLyfZKQ1odpKo9mMXI0M4yuAHvEVNgroslbnrzZofIO14OmXLATmrwI9OnmGvV0NAqrc_S2ZAQpV2u8qAljo8sA/s320/PXL_20220621_174821537.jpg" width="320" /></a></div><p>Those were in the glory times of FinTech, i.e. early 2022. Even before FTX had its problems, we had the Three Arrows Capital bankruptcy and the <a href="https://en.wikipedia.org/wiki/Terra_(blockchain)#Collapse">TerraUSD stablecoin collapse</a>. Bitcoin lost over 60% of its value during the year.&nbsp;</p><p>Of all the problems we've seen with cryptocurrency, it's never because of the crypto. I'm still shocked how the person or people called Satoshi Nakamoto got it mostly right on the <a href="https://bitcoin.org/bitcoin.pdf">first try</a>. They did fail to account for the energy needed to mine bitcoin once bitcoin became valuable, but on the other hand who could have predicted bitcoin would become so valuable. Certainly <a href="https://blog.computationalcomplexity.org/2011/11/making-money-computationally-hard-way.html">not me</a>.</p><p>I still don't see much of a market for most cryptocurrencies beyond speculation and illegal activities, though we have much of both. Nevertheless it feels weird in 2022 that we still use physical bills and coins. I am hopeful for a CBDC (Central Bank Digital Currency), not a stablecoin tied to a dollar, but US currency itself just in digital form.</p><p>We now use digital tickets for most events and transport. Even driver's licenses&nbsp;and passports are moving digital--they only gets scanned anyway. There are a few remaining uses of paper--property titles, legal wills, social security cards, birth, death, marriage and divorce certificates. Perhaps we could use blockchain or even centralized databases to eliminate these as well.</p><p>Blockchain could also be helpful to track our digital goods, so we can use the music, movies, books, games, virtual clothes and accoutrements on different platforms without having to buy them twice. (HT <a href="https://twitter.com/dsivakumar/status/1602484986845990914">Siva</a>)</p><p>The most important use of FinTech will be for those who have the least, those who pay large fees to have a bank account, cash a paycheck or send money to family overseas. FinTech can be a great democratizing tool, if we use it that way and not just as another get rich scheme.</p><p class="authors">By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-15T16:57:00Z">Thursday, December 15 2022, 16:57</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://francisbach.com/continuized-acceleration/'>Discrete, continuous and continuized accelerations</a></h3>
        <p class='tr-article-feed'>from <a href='https://francisbach.com'>Francis Bach</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          In optimization, acceleration is the art of modifying an algorithm in order to obtain faster convergence. Building accelerations and explaining their performance have been the subject of a countless number of publications, see [2] for a review. In this blog post, we give a vignette of these discussions on a minimal but challenging example, Nesterov...
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="has-text-align-justify justify-text">In optimization, acceleration is the art of modifying an algorithm in order to obtain faster convergence. Building accelerations and explaining their performance have been the subject of a countless number of publications, see [2] for a review. In this blog post, we give a vignette of these discussions on a minimal but challenging example, Nesterov acceleration. We introduce and illustrate different frameworks to understand and design acceleration techniques: the discrete framework based on iterations, the continuous-time framework, and the recent continuized framework that we proposed in [1].</p>



<p class="justify-text">First, let us describe the setting we use. Let \(f:\mathbb{R}^d \to \mathbb{R}\) be a convex and differentiable function, that we seek to minimize. We assume that \(f\) is \(L\)-smooth, i.e., $$\forall x,y \in \mathbb{R}^d, \qquad f(y) \leq f(x) + \langle \nabla f(x), y-x \rangle + \frac{L}{2} \Vert y-x \Vert^2 \, , $$ and that it is \(\mu\)-strongly convex for some \(\mu &gt; 0\), i.e., $$\forall x,y \in \mathbb{R}^d, \qquad f(y) \geq f(x) + \langle \nabla f(x), y-x \rangle + \frac{\mu}{2} \Vert y-x \Vert^2 \, .$$We had already made these assumptions in <a rel="noreferrer noopener" href="https://francisbach.com/computer-aided-analyses/" target="_blank">a previous blog post on computer-aided analyses in optimization</a>. In this setting, the function \(f\) has a unique minimizer that we denote \(x_*\). </p>



<figure class="wp-block-image size-full is-resized"><img src="https://francisbach.com/wp-content/uploads/2022/12/SmoothStronglyConvex.png" alt="" class="wp-image-8604" width="840" height="363" srcset="https://francisbach.com/wp-content/uploads/2022/12/SmoothStronglyConvex.png 1002w, https://francisbach.com/wp-content/uploads/2022/12/SmoothStronglyConvex-300x130.png 300w, https://francisbach.com/wp-content/uploads/2022/12/SmoothStronglyConvex-768x333.png 768w, https://francisbach.com/wp-content/uploads/2022/12/SmoothStronglyConvex-850x368.png 850w" sizes="(max-width: 840px) 100vw, 840px" /></figure>



<p class="has-text-align-justify">In this figure, the blue function \(f\) is \(L\)-smooth and \(\mu\)-strongly convex: it is possible to create global upper and lower quadratic bounds from every \(x \in \mathbb{R}^d\) with respective curvatures \(L\) and \(\mu\).</p>



<p class="justify-text">Of course, smoothness and strong convexity are not the only assumptions under which acceleration is studied (see [2] for a larger variety of accelerations). In particular, in [1], these is a discussion similar to the one of this blog post in the case where strong convexity is not assumed. </p>



<h2>Discrete-time acceleration</h2>



<p class="justify-text">The basic algorithm for minimizing \(f\) is gradient descent: we start from an initial guess \(x_0 \in \mathbb{R}^d\) and iterate $$x_{k+1} = x_k \, &#8211; \gamma \nabla f(x_k) \, ,$$where \(\gamma\) is a step size. </p>



<p class="justify-text">Let us show how this performs on an example in dimension \(d=2\). In the plot below, the thin curves represent the level sets of the function \(f\). With a slight abuse of notation, we denote \(x_1\) and \(x_2\) the coordinates of the vector \(x \in \mathbb{R}^2\).</p>


<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img loading="lazy" src="https://francisbach.com/wp-content/uploads/2022/12/gd-4.gif" alt="" class="wp-image-8662" width="600" height="450"/></figure></div>


<p class="justify-text">When \(\gamma \leq \frac{1}{L}\), we have the linear convergence rate $$f(x_k) \, &#8211; f(x_*) \leq \frac{L}{2} \Vert x_0 \, &#8211; x_* \Vert^2 (1-\gamma \mu)^k \, ,$$see [3, Theorem 2.1.15] for instance. This upper bound is minimized at \(\gamma = \frac{1}{L}\), leading to a \(O\big( \left( 1\, &#8211; \frac{\mu}{L} \right)^k \big)\) convergence rate. Although this convergence rate is linear, it can be slow when the condition number \(\frac{L}{\mu} \) is large. </p>



<p class="justify-text">To tackle this issue, Nesterov [3] proposed an alternative algorithm: $$\begin{aligned}<br>&amp;y_k = x_k + \frac{\sqrt{\gamma\mu}}{1 + \sqrt{\gamma\mu}}(z_k-x_k) \, , \\ &amp;x_{k+1} = y_k \, &#8211; \gamma \nabla f(y_k) \, , \\ &amp;z_{k+1} = z_k + \sqrt{\gamma\mu}(y_k-z_k)  \, &#8211;  \sqrt{\frac{\gamma}{\mu}}\nabla f(y_k ) \, . \end{aligned}$$</p>


<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img loading="lazy" src="https://francisbach.com/wp-content/uploads/2022/12/nest-4.gif" alt="" class="wp-image-8663" width="600" height="450"/></figure></div>


<p class="justify-text">Nesterov proved the following convergence bound for his algorithm: if \(\gamma \leq \frac{1}{L}\), $$f (x_k) \, &#8211; f(x_*) \leq \left(f(x_0) \, &#8211; f(x_*) + \frac{\mu}{2} \Vert z_0 \, &#8211; x_* \Vert^2 \right) \left(1\,  &#8211; \sqrt{\gamma\mu}\right)^k \, .$$ Again, this upper bound is minimized at \(\gamma = \frac{1}{L}\); this leads to a \(O\left( \left( 1 \, &#8211; \sqrt{\frac{\mu}{L}} \right)^k \right)\) convergence rate. The acceleration lies in this new square root dependence in the condition number, that enables significant speed-ups in practice. To show this, let us compare the sequence \(x_k\) of Nesterov acceleration with gradient descent.  </p>


<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img loading="lazy" src="https://francisbach.com/wp-content/uploads/2022/12/gd-nest-2.gif" alt="" class="wp-image-8664" width="600" height="450"/></figure></div>

<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img loading="lazy" src="https://francisbach.com/wp-content/uploads/2022/12/nest-dg-performances-1.png" alt="" class="wp-image-8665" width="600" height="450" srcset="https://francisbach.com/wp-content/uploads/2022/12/nest-dg-performances-1.png 1200w, https://francisbach.com/wp-content/uploads/2022/12/nest-dg-performances-1-300x225.png 300w, https://francisbach.com/wp-content/uploads/2022/12/nest-dg-performances-1-1024x768.png 1024w, https://francisbach.com/wp-content/uploads/2022/12/nest-dg-performances-1-768x576.png 768w, https://francisbach.com/wp-content/uploads/2022/12/nest-dg-performances-1-850x638.png 850w" sizes="(max-width: 600px) 100vw, 600px" /></figure></div>


<p class="justify-text">We observe that the asymptotic performance of Nesterov acceleration is much better. Of course, comparing the two algorithms in terms of the iteration number \(k\) is unfair to gradient descent, as the complexity per iteration of Nesterov acceleration is higher. However, as suggested by the theoretical convergence bound, the speed-up provided by Nesterov acceleration is largely worth the additional complexity per iteration on badly conditioned problems. </p>



<p class="justify-text">From a high-level perspective, Nesterov acceleration iterates over several variables, alternating between gradient steps (always with respect to the gradient at \(y_k\)) and mixing steps, where the running value of a variable is replaced by a linear combination of the other variables. However, the precise way gradient and mixing steps are coupled is rather mysterious, and the improved convergence bound relies heavily on the detailed structure of the iterations.</p>



<p class="justify-text">Only an inspection of the proof of the convergence bound [2,3] can provide a rigorous understanding of Nesterov acceleration and its performance. Many works contributed to interpret and motivate the iteration [11-15]. In this blog post, we try to give some high-level intuitions through the continuous and continuized points of view on Nesterov acceleration. </p>



<h2>Continuous-time acceleration</h2>



<p class="justify-text">Continuous-time approaches propose to gain insights on the above algorithms through their limit as the stepsize \(\gamma\) vanishes [4,10]. This was already the subject of a <a rel="noreferrer noopener" href="https://francisbach.com/gradient-flows/" target="_blank">previous blog post</a>. In this approach, one needs to rescale the iterate number \(k\) as \(\gamma \to 0\) in order to obtain a non-degenerate limit. </p>



<p class="justify-text">For instance, to study gradient descent, we define a rescaled time variable \(t = \gamma k \in \mathbb{R}_{\geq 0}\) and the reparametrized iterates \(X(t) = X(\gamma k) = x_k\). Then for small \(\gamma\), $$\frac{dX}{dt}(t) \approx \frac{X(t+\gamma)-X(t)}{\gamma} = \frac{x_{t/\gamma+1}-x_{t/\gamma}}{\gamma} = \, &#8211; \nabla f(x_{t/\gamma}) = \, &#8211; \nabla f(X(t)) \, .$$In the limit \(\gamma \to 0\), the approximation of the derivative becomes exact and this gives the gradient flow equation $$\frac{dX}{dt}(t) = \, &#8211; \nabla f(X(t)) \, .$$</p>


<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img loading="lazy" src="https://francisbach.com/wp-content/uploads/2022/12/gd-gf-2.gif" alt="" class="wp-image-8666" width="600" height="450"/></figure></div>


<p class="justify-text">Note that in the simulation above, the gradient descents are aligned with the gradient flow in terms of their common time parameter \(t = \gamma k\). However, as the stepsize \(\gamma\) is not the same in the two gradient descents, the number of iterations is also not the same. </p>



<p class="justify-text">In [4,5], a similar limit is taken for the iterates \(x_k\) of Nesterov acceleration. As the computations are a bit lengthy, let us simply state the result. Define a rescaled time variable \(t = \sqrt{\gamma} k \in \mathbb{R}_{\geq 0}\) and the reparametrized iterates \(X(t) = X(\sqrt{\gamma} k ) = x_k\). As \(\gamma \to 0\), \(X\) satisfies the ordinary differential equation (ODE) $$ \frac{d^2X}{dt^2}(t) + 2\sqrt{\mu} \frac{dX}{dt}(t) = \, &#8211; \nabla f(X(t)) \, .$$</p>


<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img loading="lazy" src="https://francisbach.com/wp-content/uploads/2022/12/nest-nf-2.gif" alt="" class="wp-image-8678" width="600" height="450"/></figure></div>


<p class="justify-text">At this point, it is tempting to try to overlap the last two plots in order to compare the gradient flow with the limiting ODE for Nesterov acceleration. However, this can not be done as the notion of time \(t\) is not the same in both cases. The gradient flow is obtained at the limit of gradient descent with \(t_1 = \gamma k\); meanwhile Nesterov acceleration has a continuous limit in the scaling \(t_2 = \sqrt{\gamma} k\). For a small stepsize \(\gamma\), \(t_2\) is an order of magnitude larger than \(t_1\); this confirms that Nesterov acceleration is indeed an acceleration. </p>



<p class="justify-text">The precise ODE obtained in the limit of Nesterov acceleration gives insights on the mechanism underlying acceleration. It is a second-order ODE, while the gradient flow is a first-order ODE. The gradient flow represents the movement of a particle rolling on the graph of \(f\), with no inertia. Meanwhile, the ODE for Nesterov acceleration also represents the movement of a particle rolling on the graph of \(f\), but with inertia and with a friction coefficient proportional to \(\sqrt{\mu}\). This comforts the high-level idea that acceleration is achieved by &#8221;giving inertia to the iterates&#8221;. </p>



<p class="justify-text">However, the continuous perspective on acceleration comes with important limitations. First, as illustrated in the plots, the continuous limits appear in a computationally inefficient limit of small stepsizes. However, the rates of convergence of the discrete algorithms depend on their ability to be stable while using large stepsizes; this aspect can not be apprehended in the continuous limit. </p>



<p class="justify-text">Relatedly, the continuous limits are not algorithms by themselves; they need to be discretized to be implemented. The discretization of the accelerated ODE could lead to Nesterov acceleration, but also to multiple other algorithms including Polyak&#8217;s heavy ball method (see [6] or [2, Section 2.3.3]), another algorithm that is not as stable on non-quadratic strongly convex functions. Said differently, the continuous limit is unable to discriminate two discrete algorithms with different performances. However, it should be noted that this limitation was adressed in [5] by considering other ODEs that approximate the discrete algorithms at a higher-order when \(\gamma \to 0\). </p>



<p class="justify-text">In the next section, we present the so-called &#8220;continuized&#8221; version of Nesterov acceleration, a joint work [1] with Mathieu Even, Francis Bach, Nicolas Flammarion, Hadrien Hendrikx, Pierre Gaillard, Laurent Massoulié, Adrien Taylor and myself. It is defined in continuous time, but it does not correspond to a limit where \(\gamma \to 0\). Furthermore, it does not need to be approximated to be implemented in discrete time. </p>



<h2>Continuized acceleration</h2>



<p class="justify-text">The continuized acceleration is composed of two variables \(x_t\), \(z_t\) indexed by a continuous time \(t \geq 0\), that are continuously mixing and that take gradient steps at random times. More precisely, let \(T_1, T_2, T_3, \dots \geq 0\) be random times such that \(T_1, T_2-T_1, T_3-T_2, \dots\) are independent identically distributed (i.i.d.), of distribution exponential with rate \(1\). The exponential distribution is a classical distribution which has a special <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Exponential_distribution" target="_blank">memoryless property</a>; in our case of rate \(1\), it is the distribution with density \(\exp(-t)\). Below, we show five realizations of the random times \(T_1, T_2, T_3, \dots\), between \(t=0\) and \(t = 10\):</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img loading="lazy" width="1024" height="205" src="https://francisbach.com/wp-content/uploads/2022/12/poisson-process0-1-1024x205.png" alt="" class="wp-image-8650" srcset="https://francisbach.com/wp-content/uploads/2022/12/poisson-process0-1-1024x205.png 1024w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process0-1-300x60.png 300w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process0-1-768x154.png 768w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process0-1-850x170.png 850w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process0-1.png 1500w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure></div>

<div class="wp-block-image">
<figure class="aligncenter size-large"><img loading="lazy" width="1024" height="205" src="https://francisbach.com/wp-content/uploads/2022/12/poisson-process1-1-1024x205.png" alt="" class="wp-image-8651" srcset="https://francisbach.com/wp-content/uploads/2022/12/poisson-process1-1-1024x205.png 1024w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process1-1-300x60.png 300w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process1-1-768x154.png 768w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process1-1-850x170.png 850w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process1-1.png 1500w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure></div>

<div class="wp-block-image">
<figure class="aligncenter size-large"><img loading="lazy" width="1024" height="205" src="https://francisbach.com/wp-content/uploads/2022/12/poisson-process2-1-1024x205.png" alt="" class="wp-image-8652" srcset="https://francisbach.com/wp-content/uploads/2022/12/poisson-process2-1-1024x205.png 1024w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process2-1-300x60.png 300w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process2-1-768x154.png 768w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process2-1-850x170.png 850w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process2-1.png 1500w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure></div>

<div class="wp-block-image">
<figure class="aligncenter size-large"><img loading="lazy" width="1024" height="205" src="https://francisbach.com/wp-content/uploads/2022/12/poisson-process3-1-1024x205.png" alt="" class="wp-image-8653" srcset="https://francisbach.com/wp-content/uploads/2022/12/poisson-process3-1-1024x205.png 1024w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process3-1-300x60.png 300w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process3-1-768x154.png 768w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process3-1-850x170.png 850w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process3-1.png 1500w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure></div>

<div class="wp-block-image">
<figure class="aligncenter size-large"><img loading="lazy" width="1024" height="205" src="https://francisbach.com/wp-content/uploads/2022/12/poisson-process4-1-1024x205.png" alt="" class="wp-image-8654" srcset="https://francisbach.com/wp-content/uploads/2022/12/poisson-process4-1-1024x205.png 1024w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process4-1-300x60.png 300w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process4-1-768x154.png 768w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process4-1-850x170.png 850w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process4-1.png 1500w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure></div>


<p class="justify-text">By convention, we choose that our stochastic processes \(t \mapsto x_t\), \(t \mapsto z_t\) are &#8220;<a href="https://en.wikipedia.org/wiki/C%C3%A0dl%C3%A0g" target="_blank" rel="noreferrer noopener">càdlàg</a>&#8221; almost surely, i.e., right continuous with well-defined left-limits \(x_{t-}\), \(z_{t-}\) at all points \(t\)<em>. </em>At random times \(T_1, T_2, \dots\), our sequences take gradient steps $$\begin{aligned} x_{T_k} &amp;= x_{T_k-} &#8211; \gamma \nabla f (x_{T_k-}) \, , \\z_{T_k} &amp;= z_{T_k-} &#8211; \sqrt{\frac{\gamma}{\mu}} \nabla f (x_{T_k-}) \, . \end{aligned}$$Because of the memoryless property of the exponential distribution, in a infinitesimal time interval \([t, t+dt]\), the variables take gradients steps with probability \(dt\), independently of the past.</p>



<p class="justify-text">Between these random times, the variables mix through a linear, translation-invariant, ODE $$\begin{aligned}&amp;dx_t = \sqrt{\gamma\mu} (z_t \, &#8211; x_t) dt \, , \\&amp;dz_t = \sqrt{\gamma\mu} (x_t \, &#8211; z_t) dt \, .\end{aligned}$$Following the notation of stochastic calculus, we can write the process more compactly. Denote \(dN(t) = \sum_{k\geq 1} \delta_{T_k}(dt)\) the Poisson point measure; it is the sum of Dirac masses at the random times \(T_1, T_2, \dots\). Then we write<br>$$\begin{aligned}<br>dx_t &amp;= \sqrt{\gamma\mu} (z_t \, &#8211; x_t) dt \, &#8211; \gamma \nabla f(x_t) dN(t) \, , \\<br>dz_t &amp;= \sqrt{\gamma\mu} (x_t \, &#8211; z_t) dt-  \sqrt{\frac{\gamma}{\mu}} \nabla f(x_t) dN(t) \, . <br>\end{aligned}$$This should be understood as detailed above: the first component of these equations is an evolution with respect to the Lebesgue measure, thus a continuous flow; the second component is an evolution with respect to a discrete measure; when the time \(t\) hits a Dirac mass of the measure \(dN(t)\), then we take discrete jumps. This gives a continuous-time random process; we show one realization below. </p>


<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img loading="lazy" src="https://francisbach.com/wp-content/uploads/2022/12/cont-3.gif" alt="" class="wp-image-8668" width="600" height="450"/></figure></div>


<p class="justify-text">In this simulation, we see similarities with the discrete acceleration of Nesterov. However, the process is now random and in continuous time. We comment this comparison in detail in the rest of the blog post. </p>



<p class="justify-text">In [1], we proved the following convergence bound for this process: if \(\gamma \leq \frac{1}{L}\), $$\mathbb{E} f(x_t) \, &#8211; f(x_*) \leq \left(f(x_0) \, &#8211; f(x_*) + \frac{\mu}{2} \Vert z_0 \,  &#8211; x_* \Vert^2 \right) \exp\left(  &#8211; \sqrt{\gamma \mu} t \right) \, .$$Note the similarity with the bound proved by Nesterov for the discrete acceleration. In fact, the proof through Lyapunov techniques is essentially the same. In the continuized acceleration, the main difference is that we have a statement in expectation over the sampling of the Poisson point measure.  </p>



<p class="justify-text">In summary, we observe from the bounds and the simulation that the discrete and continuized accelerations behave similarly. The continuized acceleration is more involved as it is a random process with ODE components and jumps. Why would one prefer the continuized acceleration? Can we discretize the continuized acceleration easily? </p>



<p class="justify-text">In the next section, we explain why the discretization of the continuized acceleration is not a problem. But before that, let us propose high-level answers to the first question. </p>



<p class="justify-text">First, from a Markov chain indexed by a discrete time index \(k\), one can associate the so-called <em>continuized</em> Markov chain, indexed by a continuous time \(t\), that makes transition with the same Markov kernel, but at random times, with independent exponential time intervals [8]. This terminology motivated the name of the continuized acceleration in [1]. The continuized Markov chain is appreciated for its continuous time parameter \(t\), for instance because it enables to use differential calculus. Still, the continuized Markov chain keeps many properties of the original Markov chain; similarly the continuized acceleration is arguably simpler to analyze, while performing similarly to Nesterov acceleration.</p>



<p class="justify-text">Second, processes that alternate randomly are generally simpler to analyze that those that alternate in cycles. For instance, stochastic gradient descent is easier to analyze when components are selected randomly rather than in an ordered way. Coordinate gradient descent is easier to analyze when coordinates are selected randomly rather than in an ordered way [9]. Similarly, the continuized acceleration is simpler to analyze because the gradient steps and the mixing steps alternate randomly, due to the randomness of \(T_k\).</p>



<p class="justify-text">Third, thanks to this random alternation, the continuized framework enables to build accelerations in asynchronous distributed optimization. In fact, this application was the original motivation to build the continuized acceleration in [1]. The interested reader can consult [1, Sections 6-7] to learn how accelerated distributed algorithms were built in asynchronous settings where acceleration was previously unknown. </p>



<p class="justify-text">After these high-level remarks, let us return to a concrete question that we left: how can we implement the continuized acceleration? </p>



<h2>The discrete implementation of the continuized acceleration</h2>



<p class="justify-text">The continuized acceleration can be implemented exactly as a discrete algorithm. Indeed, between the times of the jumps \(T_k\), the dynamics of the continuized acceleration are governed by an ODE that is integrable in closed form. It is thus wise to discretize at the times \(T_k\) of the jumps. More precisely, define $$\begin{aligned} &amp;\tilde{x}_k := x_{T_{k}} \, , &amp;&amp;\tilde{y}_k := x_{T_{k+1}-} \, , &amp;&amp;\tilde{z}_k := z_{T_{k}} \, .<br>\end{aligned}$$In [1], we proved that the three sequences \(\tilde{x}_k\), \(\tilde{y}_k\), \(\tilde{z}_k\), \(k \geq 0\), satisfy a recurrence relation: $$\begin{aligned}<br>&amp;\tilde{y}_k = \tilde{x}_k + \tau_k(\tilde{z}_k-\tilde{x}_k) \, , \\ &amp;\tilde{x}_{k+1} = \tilde{y}_k \, &#8211; \gamma \nabla f(\tilde{y}_k) \, , \\ &amp;\tilde{z}_{k+1} = \tilde{z}_k + \tau_k'(\tilde{y}_k-\tilde{z}_k) \, &#8211; \sqrt{\frac{\gamma}{\mu}} \nabla f(\tilde{y}_k ) \, ,<br>\end{aligned}$$ with $$\begin{aligned} &amp;{ \tau_k = \frac{1}{2}\left(1 &#8211; \exp\left(-2\sqrt{\gamma\mu}(T_{k+1}-T_k)\right)\right)} \, , &amp;&amp;\tau_k&#8217; = \tanh\left(\sqrt{\gamma\mu}(T_{k+1}-T_k)\right) \, .\end{aligned}$$Note that this recurrence relation has the same structure as Nesterov&#8217;s original acceleration: in fact, only the two coefficients \(\tau_k\) and \(\tau_k&#8217;\) have been randomized. In short, the continuized acceleration can be implemented as a randomized version of the discrete acceleration. </p>



<p class="justify-text">In this discrete implementation, the continuized acceleration performs similarly to Nesterov acceleration. </p>


<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img loading="lazy" src="https://francisbach.com/wp-content/uploads/2022/12/gd-nest-cont-3.gif" alt="" class="wp-image-8669" width="600" height="450"/></figure></div>

<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img loading="lazy" src="https://francisbach.com/wp-content/uploads/2022/12/cont-nest-gd-performances-3.png" alt="" class="wp-image-8670" width="600" height="450" srcset="https://francisbach.com/wp-content/uploads/2022/12/cont-nest-gd-performances-3.png 1200w, https://francisbach.com/wp-content/uploads/2022/12/cont-nest-gd-performances-3-300x225.png 300w, https://francisbach.com/wp-content/uploads/2022/12/cont-nest-gd-performances-3-1024x768.png 1024w, https://francisbach.com/wp-content/uploads/2022/12/cont-nest-gd-performances-3-768x576.png 768w, https://francisbach.com/wp-content/uploads/2022/12/cont-nest-gd-performances-3-850x638.png 850w" sizes="(max-width: 600px) 100vw, 600px" /></figure></div>


<h2>Conclusion</h2>



<p class="justify-text">For practical purposes, Nesterov&#8217;s discrete acceleration wins. The iteration is elementary to implement and it enjoys a great performance. (There are exceptions in distributed optimization for which Nesterov acceleration would not enable asynchrony but the continuized acceleration does [1]).</p>



<p class="justify-text">For conceptual purposes, the continuous perspective on acceleration gives a crude intuition on the mechanism at play. However, there are also important aspects lost in the limit of small stepsizes, related to the choice of the discretization. </p>



<p class="justify-text">The continuized acceleration has two faces, continuous and discrete. As a discrete iteration, it is a randomized version of Nesterov&#8217;s original iteration. As a continuous process, it is more sophisticated that a simple ODE, as it involves random jumps. However, it gives a continuous-time perspective on acceleration without any conceptual loss. </p>



<h2>References</h2>



<p class="justify-text">[1] Mathieu Even, Raphaël Berthier, Francis Bach, Nicolas Flammarion, Hadrien Hendrikx, Pierre Gaillard, Laurent Massoulié, and Adrien Taylor. <a rel="noreferrer noopener" href="https://proceedings.neurips.cc/paper/2021/hash/ec26fc2eb2b75aece19c70392dc744c2-Abstract.html" target="_blank">Continuized accelerations of deterministic and stochastic gradient descents, and of gossip algorithms</a>. <em>Advances in Neural Information Processing Systems</em> 34 (2021): 28054-28066.<br>[2] Alexandre d&#8217;Aspremont, Damien Scieur, and Adrien Taylor. <a rel="noreferrer noopener" href="https://www.nowpublishers.com/article/Details/OPT-036" target="_blank">Acceleration methods</a>. <em>Foundations and Trends® in Optimization</em> 5, no. 1-2 (2021): 1-245.<br>[3] Yurii Nesterov. <em><a rel="noreferrer noopener" href="https://link.springer.com/book/10.1007/978-1-4419-8853-9" target="_blank">Introductory lectures on convex optimization: A basic course</a></em>. Vol. 87. Springer Science &amp; Business Media, 2003.<br>[4] Weijie Su, Stephen Boyd, and Emmanuel Candes. <a rel="noreferrer noopener" href="https://jmlr.org/papers/v17/15-084.html" target="_blank">A differential equation for modeling Nesterov’s accelerated gradient method: theory and insights</a>. <em>Advances in Neural Information Processing Systems</em> 27 (2014).<br>[5] Bin Shi, Simon Du, Michael Jordan, and Weijie Su. <a rel="noreferrer noopener" href="https://link.springer.com/article/10.1007/s10107-021-01681-8" target="_blank">Understanding the acceleration phenomenon via high-resolution differential equations</a>. <em>Mathematical Programming</em> 195, no. 1 (2022): 79-148.<br>[6] Boris Polyak. <a rel="noreferrer noopener" href="https://www.sciencedirect.com/science/article/abs/pii/0041555364901375" target="_blank">Some methods of speeding up the convergence of iteration methods</a>. <em>USSR computational mathematics and mathematical physics</em>, <em>4</em>(5), pp.1-17 (1964).<br>[7] Lessard, Laurent, Benjamin Recht, and Andrew Packard. <a rel="noreferrer noopener" href="https://epubs.siam.org/doi/abs/10.1137/15M1009597?casa_token=eux_zMBz0-QAAAAA:s32oBPHvN0hyzdt9LQLJYhggXmncsC66SAFXiIhztJQ8Zdu7avLKCMhkfZaND2Jdd_WZcl-B5jKS" target="_blank">Analysis and design of optimization algorithms via integral quadratic constraints</a>. <em>SIAM Journal on Optimization</em> 26, no. 1 (2016): 57-95.<br>[8] David Aldous and James Fill. <a rel="noreferrer noopener" href="http://www.stat.berkeley.edu/$\sim$aldous/RWG/book.html" target="_blank">Reversible Markov chains and random walks on graphs</a>. Unpublished monograph (1995).<br>[9] Stephen Wright. <a rel="noreferrer noopener" href="https://link.springer.com/article/10.1007/s10107-015-0892-3" target="_blank">Coordinate descent algorithms</a>. <em>Mathematical Programming</em> 151, no. 1 (2015): 3-34.<br>[10] Hedy Attouch, Zaki Chbani, Juan Peypouquet, and Patrick Redont.<a rel="noreferrer noopener" href="https://link.springer.com/article/10.1007/s10107-016-0992-8" target="_blank"> Fast convergence of inertial dynamics and algorithms with asymptotic vanishing viscosity</a>. <em>Mathematical Programming</em> 168, no. 1 (2018): 123-175.<br>[11] Sébastien Bubeck, Yin Tat Lee, and Mohit Singh. <a rel="noreferrer noopener" href="https://arxiv.org/abs/1506.08187" target="_blank">A geometric alternative to Nesterov&#8217;s accelerated gradient descent</a>. <em>arXiv preprint arXiv:1506.08187</em> (2015).<br>[12] Nicolas Flammarion and Francis Bach. <a rel="noreferrer noopener" href="https://proceedings.mlr.press/v40/Flammarion15.html" target="_blank">From averaging to acceleration, there is only a step-size</a>. In <em>Conference on Learning Theory</em>, pp. 658-695. PMLR (2015).<br>[13] Yossi Arjevani, Shai Shalev-Shwartz, and Ohad Shamir. <a rel="noreferrer noopener" href="https://jmlr.org/papers/v17/15-106.html" target="_blank">On lower and upper bounds in smooth and strongly convex optimization</a>. <em>The Journal of Machine Learning Research</em> 17, no. 1 (2016): 4303-4353.<br>[14] Donghwan Kim and Jeffrey Fessler. <a rel="noreferrer noopener" href="https://link.springer.com/article/10.1007/s10107-015-0949-3" target="_blank">Optimized first-order methods for smooth convex minimization</a>. <em>Mathematical programming</em> 159, no. 1 (2016): 81-107.<br>[15] Zeyuan Allen-Zhu and Lorenzo Orecchia. <a rel="noreferrer noopener" href="https://drops.dagstuhl.de/opus/volltexte/2017/8185/pdf/LIPIcs-ITCS-2017-3.pdf" target="_blank">Linear coupling: An ultimate unification of gradient and mirror descent</a>. In <em>Proceedings of the 8th Innovations in Theoretical Computer Science ITCS&#8217;17</em> (2017).</p>
<p class="authors">By Raphael Berthier</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-15T15:25:08Z">Thursday, December 15 2022, 15:25</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://ptreview.sublinear.info/2022/12/a-pedagogical-reference-to-kick-off-the-new-year/'>A Pedagogical reference to kick off the New Year</a></h3>
        <p class='tr-article-feed'>from <a href='https://ptreview.sublinear.info'>Property Testing Review</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Dear Readers, Our own Clément Canonne has written a beautiful survey which is now available in FnT book format from now publishers. This appears to be a very promising read &#8212; especially for the Distribution Testers among you. Today&#8217;s post is a mere advertisement for this beautiful survey/book which is clearly the result of a [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Dear Readers,</p>



<p>Our own Clément Canonne has written a beautiful survey which is now available in <a href="https://www.nowpublishers.com/article/Details/CIT-114">FnT book format from now publishers</a>. This appears to be a very promising read &#8212; especially for the Distribution Testers among you. Today&#8217;s post is a mere advertisement for this beautiful survey/book which is clearly the result of a dedicated pursuit.</p>



<p>Let me now dig into this survey a teeny tiny bit. One among the many cool features of this survey is that it uses one central example (testing goodness-of-fit) to give a unified treatment to the diverse tools and techniques used in distribution testing. Another plus for me is the historical notes section that accompanies every chapter. In particular, I really liked jumping into the informative history section at the end of Chapter 2 which has an almost story like feel to it. If the above points do not catch your fancy, then please try opening the survey. You will be hardpressed to find a book that is typeset in such an aesthetically pleasing way with colored fonts to emphasize various parameters in several intricate proofs. Happy Reading!</p>
<p class="authors">By Akash</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-15T12:31:32Z">Thursday, December 15 2022, 12:31</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.07044'>3D Neuron Morphology Analysis</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jiaxiang Jiang, Michael Goebel, Cezar Borba, William Smith, B.S. Manjunath</p><p>We consider the problem of finding an accurate representation of neuron
shapes, extracting sub-cellular features, and classifying neurons based on
neuron shapes. In neuroscience research, the skeleton representation is often
used as a compact and abstract representation of neuron shapes. However,
existing methods are limited to getting and analyzing "curve" skeletons which
can only be applied for tubular shapes. This paper presents a 3D neuron
morphology analysis method for more general and complex neuron shapes. First,
we introduce the concept of skeleton mesh to represent general neuron shapes
and propose a novel method for computing mesh representations from 3D surface
point clouds. A skeleton graph is then obtained from skeleton mesh and is used
to extract sub-cellular features. Finally, an unsupervised learning method is
used to embed the skeleton graph for neuron classification. Extensive
experiment results are provided and demonstrate the robustness of our method to
analyze neuron morphology.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jiaxiang Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Goebel_M/0/1/0/all/0/1">Michael Goebel</a>, <a href="http://arxiv.org/find/cs/1/au:+Borba_C/0/1/0/all/0/1">Cezar Borba</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_W/0/1/0/all/0/1">William Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Manjunath_B/0/1/0/all/0/1">B.S. Manjunath</a></p><p>We consider the problem of finding an accurate representation of neuron
shapes, extracting sub-cellular features, and classifying neurons based on
neuron shapes. In neuroscience research, the skeleton representation is often
used as a compact and abstract representation of neuron shapes. However,
existing methods are limited to getting and analyzing "curve" skeletons which
can only be applied for tubular shapes. This paper presents a 3D neuron
morphology analysis method for more general and complex neuron shapes. First,
we introduce the concept of skeleton mesh to represent general neuron shapes
and propose a novel method for computing mesh representations from 3D surface
point clouds. A skeleton graph is then obtained from skeleton mesh and is used
to extract sub-cellular features. Finally, an unsupervised learning method is
used to embed the skeleton graph for neuron classification. Extensive
experiment results are provided and demonstrate the robustness of our method to
analyze neuron morphology.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-15T01:30:00Z">Thursday, December 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.07201'>Toroidal Coordinates: Decorrelating Circular Coordinates With Lattice Reduction</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Luis Scoccola, Hitesh Gakhar, Johnathan Bush, Nikolas Schonsheck, Tatum Rask, Ling Zhou, Jose A. Perea</p><p>The circular coordinates algorithm of de Silva, Morozov, and
Vejdemo-Johansson takes as input a dataset together with a cohomology class
representing a $1$-dimensional hole in the data; the output is a map from the
data into the circle that captures this hole, and that is of minimum energy in
a suitable sense. However, when applied to several cohomology classes, the
output circle-valued maps can be "geometrically correlated" even if the chosen
cohomology classes are linearly independent. It is shown in the original work
that less correlated maps can be obtained with suitable integer linear
combinations of the cohomology classes, with the linear combinations being
chosen by inspection. In this paper, we identify a formal notion of geometric
correlation between circle-valued maps which, in the Riemannian manifold case,
corresponds to the Dirichlet form, a bilinear form derived from the Dirichlet
energy. We describe a systematic procedure for constructing low energy
torus-valued maps on data, starting from a set of linearly independent
cohomology classes. We showcase our procedure with computational examples. Our
main algorithm is based on the Lenstra--Lenstra--Lov\'asz algorithm from
computational number theory.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Scoccola_L/0/1/0/all/0/1">Luis Scoccola</a>, <a href="http://arxiv.org/find/cs/1/au:+Gakhar_H/0/1/0/all/0/1">Hitesh Gakhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Bush_J/0/1/0/all/0/1">Johnathan Bush</a>, <a href="http://arxiv.org/find/cs/1/au:+Schonsheck_N/0/1/0/all/0/1">Nikolas Schonsheck</a>, <a href="http://arxiv.org/find/cs/1/au:+Rask_T/0/1/0/all/0/1">Tatum Rask</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Ling Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Perea_J/0/1/0/all/0/1">Jose A. Perea</a></p><p>The circular coordinates algorithm of de Silva, Morozov, and
Vejdemo-Johansson takes as input a dataset together with a cohomology class
representing a $1$-dimensional hole in the data; the output is a map from the
data into the circle that captures this hole, and that is of minimum energy in
a suitable sense. However, when applied to several cohomology classes, the
output circle-valued maps can be "geometrically correlated" even if the chosen
cohomology classes are linearly independent. It is shown in the original work
that less correlated maps can be obtained with suitable integer linear
combinations of the cohomology classes, with the linear combinations being
chosen by inspection. In this paper, we identify a formal notion of geometric
correlation between circle-valued maps which, in the Riemannian manifold case,
corresponds to the Dirichlet form, a bilinear form derived from the Dirichlet
energy. We describe a systematic procedure for constructing low energy
torus-valued maps on data, starting from a set of linearly independent
cohomology classes. We showcase our procedure with computational examples. Our
main algorithm is based on the Lenstra--Lenstra--Lov\'asz algorithm from
computational number theory.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-15T01:30:00Z">Thursday, December 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2212.07124'>Approximate Discrete Fr\'echet distance: simplified, extended and structured</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ivor van der Hoog, Eva Rotenberg, Sampsong Wong</p><p>The Fr\'{e}chet distance is one of the most studied distance measures between
curves $P$ and $Q$. The data structure variant of the problem is a longstanding
open problem: Efficiently preprocess $P$, so that for any $Q$ given at query
time, one can efficiently approximate their Fr\'{e}chet distance. There exist
conditional lower bounds that prohibit $(1 + \varepsilon)$-approximate
Fr\'{e}chet distance computations in subquadratic time, even when preprocessing
$P$ using any polynomial amount of time and space. As a consequence, the
problem has been studied under various restrictions: restricting $Q$ to be a
(horizontal) segment, or requiring $P$ and $Q$ to be so-called \emph{realistic}
input curves.
</p>
<p>We give a data structure for $(1+\varepsilon)$-approximate discrete
Fr\'{e}chet distance in any metric space $\mathcal{X}$ between a realistic
input curve $P$ and any query curve $Q$. After preprocessing the input curve
$P$ (of length $|P|=n$) in $O(n \log n)$ time, we may answer queries specifying
a query curve $Q$ and an $\varepsilon$, and output a value $d(P,Q)$ which is at
most a $(1+\varepsilon)$-factor away from the true Fr\'{e}chet distance between
$Q$ and $P$. Thus, we give the first data structure that adapts to
$\varepsilon$-values specified at query time, and the first data structure to
handle query curves with arbitrarily many vertices. Our query time is
asymptotically linear in $|Q|=m$, $\frac{1}{\varepsilon}$, $\log n$, and the
realism parameter $c$ or $\kappa$.
</p>
<p>The method presented in this paper simplifies and generalizes previous
contributions to the static problem variant. We obtain efficient queries (and
therefore static algorithms) for Fr\'{e}chet distance computation in
high-dimensional spaces and other metric spaces (e.g., when $\mathcal{X}$ is a
graph under the shortest path metric). Our method supports subcurve queries at
no additional cost.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Hoog_I/0/1/0/all/0/1">Ivor van der Hoog</a>, <a href="http://arxiv.org/find/cs/1/au:+Rotenberg_E/0/1/0/all/0/1">Eva Rotenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_S/0/1/0/all/0/1">Sampsong Wong</a></p><p>The Fr\'{e}chet distance is one of the most studied distance measures between
curves $P$ and $Q$. The data structure variant of the problem is a longstanding
open problem: Efficiently preprocess $P$, so that for any $Q$ given at query
time, one can efficiently approximate their Fr\'{e}chet distance. There exist
conditional lower bounds that prohibit $(1 + \varepsilon)$-approximate
Fr\'{e}chet distance computations in subquadratic time, even when preprocessing
$P$ using any polynomial amount of time and space. As a consequence, the
problem has been studied under various restrictions: restricting $Q$ to be a
(horizontal) segment, or requiring $P$ and $Q$ to be so-called \emph{realistic}
input curves.
</p>
<p>We give a data structure for $(1+\varepsilon)$-approximate discrete
Fr\'{e}chet distance in any metric space $\mathcal{X}$ between a realistic
input curve $P$ and any query curve $Q$. After preprocessing the input curve
$P$ (of length $|P|=n$) in $O(n \log n)$ time, we may answer queries specifying
a query curve $Q$ and an $\varepsilon$, and output a value $d(P,Q)$ which is at
most a $(1+\varepsilon)$-factor away from the true Fr\'{e}chet distance between
$Q$ and $P$. Thus, we give the first data structure that adapts to
$\varepsilon$-values specified at query time, and the first data structure to
handle query curves with arbitrarily many vertices. Our query time is
asymptotically linear in $|Q|=m$, $\frac{1}{\varepsilon}$, $\log n$, and the
realism parameter $c$ or $\kappa$.
</p>
<p>The method presented in this paper simplifies and generalizes previous
contributions to the static problem variant. We obtain efficient queries (and
therefore static algorithms) for Fr\'{e}chet distance computation in
high-dimensional spaces and other metric spaces (e.g., when $\mathcal{X}$ is a
graph under the shortest path metric). Our method supports subcurve queries at
no additional cost.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-12-15T01:30:00Z">Thursday, December 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
