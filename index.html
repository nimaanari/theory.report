<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.6 (2023-03-30) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2023-06-29T01:18:08Z">Thursday, June 29 2023, 01:18</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://minorfree.github.io/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://minorfree.github.io'>Hung Le</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>Sébastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Thursday, June 29
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15805'>G\"odel-Dummett linear temporal logic</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Juan Pablo Aguilera, Mart&#xed;n Di&#xe9;guez, David Fern&#xe1;ndez-Duque, Brett McLean</p><p>We investigate a version of linear temporal logic whose propositional
fragment is G\"odel-Dummett logic (which is well known both as a
superintuitionistic logic and a t-norm fuzzy logic). We define the logic using
two natural semantics: first a real-valued semantics, where statements have a
degree of truth in the real unit interval and second a `bi-relational'
semantics. We then show that these two semantics indeed define one and the same
logic: the statements that are valid for the real-valued semantics are the same
as those that are valid for the bi-relational semantics. This G\"odel temporal
logic does not have any form of the finite model property for these two
semantics: there are non-valid statements that can only be falsified on an
infinite model. However, by using the technical notion of a quasimodel, we show
that every falsifiable statement is falsifiable on a finite quasimodel,
yielding an algorithm for deciding if a statement is valid or not. Later, we
strengthen this decidability result by giving an algorithm that uses only a
polynomial amount of memory, proving that G\"odel temporal logic is
PSPACE-complete. We also provide a deductive calculus for G\"odel temporal
logic, and show this calculus to be sound and complete for the above-mentioned
semantics, so that all (and only) the valid statements can be proved with this
calculus.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Aguilera_J/0/1/0/all/0/1">Juan Pablo Aguilera</a>, <a href="http://arxiv.org/find/cs/1/au:+Dieguez_M/0/1/0/all/0/1">Mart&#xed;n Di&#xe9;guez</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandez_Duque_D/0/1/0/all/0/1">David Fern&#xe1;ndez-Duque</a>, <a href="http://arxiv.org/find/cs/1/au:+McLean_B/0/1/0/all/0/1">Brett McLean</a></p><p>We investigate a version of linear temporal logic whose propositional
fragment is G\"odel-Dummett logic (which is well known both as a
superintuitionistic logic and a t-norm fuzzy logic). We define the logic using
two natural semantics: first a real-valued semantics, where statements have a
degree of truth in the real unit interval and second a `bi-relational'
semantics. We then show that these two semantics indeed define one and the same
logic: the statements that are valid for the real-valued semantics are the same
as those that are valid for the bi-relational semantics. This G\"odel temporal
logic does not have any form of the finite model property for these two
semantics: there are non-valid statements that can only be falsified on an
infinite model. However, by using the technical notion of a quasimodel, we show
that every falsifiable statement is falsifiable on a finite quasimodel,
yielding an algorithm for deciding if a statement is valid or not. Later, we
strengthen this decidability result by giving an algorithm that uses only a
polynomial amount of memory, proving that G\"odel temporal logic is
PSPACE-complete. We also provide a deductive calculus for G\"odel temporal
logic, and show this calculus to be sound and complete for the above-mentioned
semantics, so that all (and only) the valid statements can be proved with this
calculus.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-29T00:30:00Z">Thursday, June 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15817'>Randomized vs. Deterministic Separation in Time-Space Tradeoffs of Multi-Output Functions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Huacheng Yu, Wei Zhan</p><p>We prove the first polynomial separation between randomized and deterministic
time-space tradeoffs of multi-output functions. In particular, we present a
total function that on the input of $n$ elements in $[n]$, outputs $O(n)$
elements, such that: (1) There exists a randomized oblivious algorithm with
space $O(\log n)$, time $O(n\log n)$ and one-way access to randomness, that
computes the function with probability $1-O(1/n)$; (2) Any deterministic
oblivious branching program with space $S$ and time $T$ that computes the
function must satisfy $T^2S\geq\Omega(n^{2.5}/\log n)$. This implies that
logspace randomized algorithms for multi-output functions cannot be black-box
derandomized without an $\widetilde{\Omega}(n^{1/4})$ overhead in time.
</p>
<p>Since previously all the polynomial time-space tradeoffs of multi-output
functions are proved via the Borodin-Cook method, which is a probabilistic
method that inherently gives the same lower bound for randomized and
deterministic branching programs, our lower bound proof is intrinsically
different from previous works. We also examine other natural candidates for
proving such separations, and show that any polynomial separation for these
problems would resolve the long-standing open problem of proving
$n^{1+\Omega(1)}$ time lower bound for decision problems with
$\mathrm{polylog}(n)$ space.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Huacheng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1">Wei Zhan</a></p><p>We prove the first polynomial separation between randomized and deterministic
time-space tradeoffs of multi-output functions. In particular, we present a
total function that on the input of $n$ elements in $[n]$, outputs $O(n)$
elements, such that: (1) There exists a randomized oblivious algorithm with
space $O(\log n)$, time $O(n\log n)$ and one-way access to randomness, that
computes the function with probability $1-O(1/n)$; (2) Any deterministic
oblivious branching program with space $S$ and time $T$ that computes the
function must satisfy $T^2S\geq\Omega(n^{2.5}/\log n)$. This implies that
logspace randomized algorithms for multi-output functions cannot be black-box
derandomized without an $\widetilde{\Omega}(n^{1/4})$ overhead in time.
</p>
<p>Since previously all the polynomial time-space tradeoffs of multi-output
functions are proved via the Borodin-Cook method, which is a probabilistic
method that inherently gives the same lower bound for randomized and
deterministic branching programs, our lower bound proof is intrinsically
different from previous works. We also examine other natural candidates for
proving such separations, and show that any polynomial separation for these
problems would resolve the long-standing open problem of proving
$n^{1+\Omega(1)}$ time lower bound for decision problems with
$\mathrm{polylog}(n)$ space.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-29T00:30:00Z">Thursday, June 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15967'>Fine-grained reductions around CFL-reachability</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Aleksandra Istomina, Semyon Grigorev, Ekaterina Shemetova</p><p>In this paper we study the fine-grained complexity of the CFL reachability
problem. We first present one of the existing algorithms for the problem and an
overview of conditional lower bounds based on widely believed hypotheses. We
then use the existing reduction techniques to obtain new conditional lower
bounds on CFL reachability and related problems. We also devise a faster
algorithm for the problem in case of bounded path lengths and a technique that
may be useful in finding new conditional lower bounds.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Istomina_A/0/1/0/all/0/1">Aleksandra Istomina</a>, <a href="http://arxiv.org/find/cs/1/au:+Grigorev_S/0/1/0/all/0/1">Semyon Grigorev</a>, <a href="http://arxiv.org/find/cs/1/au:+Shemetova_E/0/1/0/all/0/1">Ekaterina Shemetova</a></p><p>In this paper we study the fine-grained complexity of the CFL reachability
problem. We first present one of the existing algorithms for the problem and an
overview of conditional lower bounds based on widely believed hypotheses. We
then use the existing reduction techniques to obtain new conditional lower
bounds on CFL reachability and related problems. We also devise a faster
algorithm for the problem in case of bounded path lengths and a technique that
may be useful in finding new conditional lower bounds.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-29T00:30:00Z">Thursday, June 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.16287'>A Review on Optimality Investigation Strategies for the Balanced Assignment Problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Anurag Dutta, K. Lakshmanan, A. Ramamoorthy, Liton Chandra Voumik, John Harshith, John Pravin Motha</p><p>Mathematical Selection is a method in which we select a particular choice
from a set of such. It have always been an interesting field of study for
mathematicians. Accordingly, Combinatorial Optimization is a sub field of this
domain of Mathematical Selection, where we generally, deal with problems
subjecting to Operation Research, Artificial Intelligence and many more
promising domains. In a broader sense, an optimization problem entails
maximising or minimising a real function by systematically selecting input
values from within an allowed set and computing the function's value. A broad
region of applied mathematics is the generalisation of metaheuristic theory and
methods to other formulations. More broadly, optimization entails determining
the finest virtues of some fitness function, offered a fixed space, which may
include a variety of distinct types of decision variables and contexts. In this
work, we will be working on the famous Balanced Assignment Problem, and will
propose a comparative analysis on the Complexity Metrics of Computational Time
for different Notions of solving the Balanced Assignment Problem.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1">Anurag Dutta</a>, <a href="http://arxiv.org/find/cs/1/au:+Lakshmanan_K/0/1/0/all/0/1">K. Lakshmanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramamoorthy_A/0/1/0/all/0/1">A. Ramamoorthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Voumik_L/0/1/0/all/0/1">Liton Chandra Voumik</a>, <a href="http://arxiv.org/find/cs/1/au:+Harshith_J/0/1/0/all/0/1">John Harshith</a>, <a href="http://arxiv.org/find/cs/1/au:+Motha_J/0/1/0/all/0/1">John Pravin Motha</a></p><p>Mathematical Selection is a method in which we select a particular choice
from a set of such. It have always been an interesting field of study for
mathematicians. Accordingly, Combinatorial Optimization is a sub field of this
domain of Mathematical Selection, where we generally, deal with problems
subjecting to Operation Research, Artificial Intelligence and many more
promising domains. In a broader sense, an optimization problem entails
maximising or minimising a real function by systematically selecting input
values from within an allowed set and computing the function's value. A broad
region of applied mathematics is the generalisation of metaheuristic theory and
methods to other formulations. More broadly, optimization entails determining
the finest virtues of some fitness function, offered a fixed space, which may
include a variety of distinct types of decision variables and contexts. In this
work, we will be working on the famous Balanced Assignment Problem, and will
propose a comparative analysis on the Complexity Metrics of Computational Time
for different Notions of solving the Balanced Assignment Problem.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-29T00:30:00Z">Thursday, June 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15701'>Phase retrieval via non-rigid image registration</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Erik Malm</p><p>Phase retrieval is the numerical procedure of recovering a complex-valued
signal from knowledge about its amplitude and some additional information.
Here, an indirect registration procedure, based on the large deformation
diffeomorphic metric mapping (LDDMM) formalism, is investigated as a phase
retrieval method for coherent diffractive imaging. The method attempts to find
a deformation which transforms an initial, template image to match an unknown
target image by comparing the diffraction pattern to the data. The exterior
calculus framework is used to treat different types of deformations in a
unified and coordinate-free way. The algorithm performance with respect to
measurement noise, image topology, and particular action are explored through
numerical examples.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/eess/1/au:+Malm_E/0/1/0/all/0/1">Erik Malm</a></p><p>Phase retrieval is the numerical procedure of recovering a complex-valued
signal from knowledge about its amplitude and some additional information.
Here, an indirect registration procedure, based on the large deformation
diffeomorphic metric mapping (LDDMM) formalism, is investigated as a phase
retrieval method for coherent diffractive imaging. The method attempts to find
a deformation which transforms an initial, template image to match an unknown
target image by comparing the diffraction pattern to the data. The exterior
calculus framework is used to treat different types of deformations in a
unified and coordinate-free way. The algorithm performance with respect to
measurement noise, image topology, and particular action are explored through
numerical examples.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-29T00:30:00Z">Thursday, June 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.16239'>Equal area partitions of the sphere with diameter bounds, via optimal transport</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jun Kitagawa, Asuka Takatsu</p><p>We prove existence of equal area partitions of the unit sphere via optimal
transport methods, accompanied by diameter bounds written in terms of
Monge--Kantorovich distances. This can be used to obtain bounds on the
expectation of the maximum diameter of partition sets, when points are
uniformly sampled from the sphere. An application to the computation of sliced
Monge--Kantorovich distances is also presented.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Kitagawa_J/0/1/0/all/0/1">Jun Kitagawa</a>, <a href="http://arxiv.org/find/math/1/au:+Takatsu_A/0/1/0/all/0/1">Asuka Takatsu</a></p><p>We prove existence of equal area partitions of the unit sphere via optimal
transport methods, accompanied by diameter bounds written in terms of
Monge--Kantorovich distances. This can be used to obtain bounds on the
expectation of the maximum diameter of partition sets, when points are
uniformly sampled from the sphere. An application to the computation of sliced
Monge--Kantorovich distances is also presented.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-29T00:30:00Z">Thursday, June 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.16317'>On the complexity of isomorphism problems for tensors, groups, and polynomials IV: linear-length reductions and their applications</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Joshua A. Grochow, Youming Qiao</p><p>Many isomorphism problems for tensors, groups, algebras, and polynomials were
recently shown to be equivalent to one another under polynomial-time
reductions, prompting the introduction of the complexity class TI (Grochow &amp;
Qiao, ITCS '21; SIAM J. Comp., '23). Using the tensorial viewpoint, Grochow &amp;
Qiao (CCC '21) then gave moderately exponential-time search- and
counting-to-decision reductions for a class of $p$-groups. A significant issue
was that the reductions usually incurred a quadratic increase in the length of
the tensors involved. When the tensors represent $p$-groups, this corresponds
to an increase in the order of the group of the form $|G|^{\Theta(\log |G|)}$,
negating any asymptotic gains in the Cayley table model.
</p>
<p>In this paper, we present a new kind of tensor gadget that allows us to
replace those quadratic-length reductions with linear-length ones, yielding the
following consequences:
</p>
<p>1. Combined with the recent breakthrough $|G|^{O((\log |G|)^{5/6})}$-time
isomorphism-test for $p$-groups of class 2 and exponent $p$ (Sun, STOC '23),
our reductions extend this runtime to $p$-groups of class $c$ and exponent $p$
where $c&lt;p$.
</p>
<p>2. Our reductions show that Sun's algorithm solves several TI-complete
problems over $F_p$, such as isomorphism problems for cubic forms, algebras,
and tensors, in time $p^{O(n^{1.8} \log p)}$.
</p>
<p>3. Polynomial-time search- and counting-to-decision reduction for testing
isomorphism of $p$-groups of class $2$ and exponent $p$ in the Cayley table
model. This answers questions of Arvind and T\'oran (Bull. EATCS, 2005) for
this group class, thought to be one of the hardest cases of Group Isomorphism.
</p>
<p>4. If Graph Isomorphism is in P, then testing equivalence of cubic forms and
testing isomorphism of algebra over a finite field $F_q$ can both be solved in
time $q^{O(n)}$, improving from the brute-force upper bound $q^{O(n^2)}$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Grochow_J/0/1/0/all/0/1">Joshua A. Grochow</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Youming Qiao</a></p><p>Many isomorphism problems for tensors, groups, algebras, and polynomials were
recently shown to be equivalent to one another under polynomial-time
reductions, prompting the introduction of the complexity class TI (Grochow &amp;
Qiao, ITCS '21; SIAM J. Comp., '23). Using the tensorial viewpoint, Grochow &amp;
Qiao (CCC '21) then gave moderately exponential-time search- and
counting-to-decision reductions for a class of $p$-groups. A significant issue
was that the reductions usually incurred a quadratic increase in the length of
the tensors involved. When the tensors represent $p$-groups, this corresponds
to an increase in the order of the group of the form $|G|^{\Theta(\log |G|)}$,
negating any asymptotic gains in the Cayley table model.
</p>
<p>In this paper, we present a new kind of tensor gadget that allows us to
replace those quadratic-length reductions with linear-length ones, yielding the
following consequences:
</p>
<p>1. Combined with the recent breakthrough $|G|^{O((\log |G|)^{5/6})}$-time
isomorphism-test for $p$-groups of class 2 and exponent $p$ (Sun, STOC '23),
our reductions extend this runtime to $p$-groups of class $c$ and exponent $p$
where $c&lt;p$.
</p>
<p>2. Our reductions show that Sun's algorithm solves several TI-complete
problems over $F_p$, such as isomorphism problems for cubic forms, algebras,
and tensors, in time $p^{O(n^{1.8} \log p)}$.
</p>
<p>3. Polynomial-time search- and counting-to-decision reduction for testing
isomorphism of $p$-groups of class $2$ and exponent $p$ in the Cayley table
model. This answers questions of Arvind and T\'oran (Bull. EATCS, 2005) for
this group class, thought to be one of the hardest cases of Group Isomorphism.
</p>
<p>4. If Graph Isomorphism is in P, then testing equivalence of cubic forms and
testing isomorphism of algebra over a finite field $F_q$ can both be solved in
time $q^{O(n)}$, improving from the brute-force upper bound $q^{O(n^2)}$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-29T00:30:00Z">Thursday, June 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15744'>Ticketed Learning-Unlearning Schemes</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Ayush Sekhari, Chiyuan Zhang</p><p>We consider the learning--unlearning paradigm defined as follows. First given
a dataset, the goal is to learn a good predictor, such as one minimizing a
certain loss. Subsequently, given any subset of examples that wish to be
unlearnt, the goal is to learn, without the knowledge of the original training
dataset, a good predictor that is identical to the predictor that would have
been produced when learning from scratch on the surviving examples.
</p>
<p>We propose a new ticketed model for learning--unlearning wherein the learning
algorithm can send back additional information in the form of a small-sized
(encrypted) ``ticket'' to each participating training example, in addition to
retaining a small amount of ``central'' information for later. Subsequently,
the examples that wish to be unlearnt present their tickets to the unlearning
algorithm, which additionally uses the central information to return a new
predictor. We provide space-efficient ticketed learning--unlearning schemes for
a broad family of concept classes, including thresholds, parities,
intersection-closed classes, among others.
</p>
<p>En route, we introduce the count-to-zero problem, where during unlearning,
the goal is to simply know if there are any examples that survived. We give a
ticketed learning--unlearning scheme for this problem that relies on the
construction of Sperner families with certain properties, which might be of
independent interest.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ghazi_B/0/1/0/all/0/1">Badih Ghazi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamath_P/0/1/0/all/0/1">Pritish Kamath</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1">Ravi Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Manurangsi_P/0/1/0/all/0/1">Pasin Manurangsi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sekhari_A/0/1/0/all/0/1">Ayush Sekhari</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chiyuan Zhang</a></p><p>We consider the learning--unlearning paradigm defined as follows. First given
a dataset, the goal is to learn a good predictor, such as one minimizing a
certain loss. Subsequently, given any subset of examples that wish to be
unlearnt, the goal is to learn, without the knowledge of the original training
dataset, a good predictor that is identical to the predictor that would have
been produced when learning from scratch on the surviving examples.
</p>
<p>We propose a new ticketed model for learning--unlearning wherein the learning
algorithm can send back additional information in the form of a small-sized
(encrypted) ``ticket'' to each participating training example, in addition to
retaining a small amount of ``central'' information for later. Subsequently,
the examples that wish to be unlearnt present their tickets to the unlearning
algorithm, which additionally uses the central information to return a new
predictor. We provide space-efficient ticketed learning--unlearning schemes for
a broad family of concept classes, including thresholds, parities,
intersection-closed classes, among others.
</p>
<p>En route, we introduce the count-to-zero problem, where during unlearning,
the goal is to simply know if there are any examples that survived. We give a
ticketed learning--unlearning scheme for this problem that relies on the
construction of Sperner families with certain properties, which might be of
independent interest.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-29T00:30:00Z">Thursday, June 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15813'>Planar graphs are acyclically edge $(\Delta + 5)$-colorable</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Qiaojun Shu, Guohui Lin</p><p>An edge coloring of a graph $G$ is to color all the edges in the graph such
that adjacent edges receive different colors. It is acyclic if each cycle in
the graph receives at least three colors. Fiam{\v{c}}ik (1978) and Alon,
Sudakov and Zaks (2001) conjectured that every simple graph with maximum degree
$\Delta$ is acyclically edge $(\Delta + 2)$-colorable -- the well-known acyclic
edge coloring conjecture (AECC). Despite many major breakthroughs and minor
improvements, the conjecture remains open even for planar graphs. In this
paper, we prove that planar graphs are acyclically edge $(\Delta +
5)$-colorable. Our proof has two main steps: Using discharging methods, we
first show that every non-trivial planar graph must have one of the eight
groups of well characterized local structures; and then acyclically edge color
the graph using no more than $\Delta + 5$ colors by an induction on the number
of edges.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Shu_Q/0/1/0/all/0/1">Qiaojun Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guohui Lin</a></p><p>An edge coloring of a graph $G$ is to color all the edges in the graph such
that adjacent edges receive different colors. It is acyclic if each cycle in
the graph receives at least three colors. Fiam{\v{c}}ik (1978) and Alon,
Sudakov and Zaks (2001) conjectured that every simple graph with maximum degree
$\Delta$ is acyclically edge $(\Delta + 2)$-colorable -- the well-known acyclic
edge coloring conjecture (AECC). Despite many major breakthroughs and minor
improvements, the conjecture remains open even for planar graphs. In this
paper, we prove that planar graphs are acyclically edge $(\Delta +
5)$-colorable. Our proof has two main steps: Using discharging methods, we
first show that every non-trivial planar graph must have one of the eight
groups of well characterized local structures; and then acyclically edge color
the graph using no more than $\Delta + 5$ colors by an induction on the number
of edges.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-29T00:30:00Z">Thursday, June 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15863'>Increasing the Measured Effective Quantum Volume with Zero Noise Extrapolation</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Elijah Pelofske, Vincent Russo, Ryan LaRose, Andrea Mari, Dan Strano, Andreas B&#xe4;rtschi, Stephan Eidenbenz, William J. Zeng</p><p>Quantum Volume is a full-stack benchmark for near-term quantum computers. It
quantifies the largest size of a square circuit which can be executed on the
target device with reasonable fidelity. Error mitigation is a set of techniques
intended to remove the effects of noise present in the computation of noisy
quantum computers when computing an expectation value of interest. Effective
quantum volume is a proposed metric that applies error mitigation to the
quantum volume protocol in order to evaluate the effectiveness not only of the
target device but also of the error mitigation algorithm. Digital Zero-Noise
Extrapolation (ZNE) is an error mitigation technique that estimates the
noiseless expectation value using circuit folding to amplify errors by known
scale factors and extrapolating to the zero-noise limit. Here we demonstrate
that ZNE, with global and local unitary folding with fractional scale factors,
in conjunction with dynamical decoupling, can increase the effective quantum
volume over the vendor-measured quantum volume. Specifically, we measure the
effective quantum volume of four IBM Quantum superconducting processor units,
obtaining values that are larger than the vendor-measured quantum volume on
each device. This is the first such increase reported.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Pelofske_E/0/1/0/all/0/1">Elijah Pelofske</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Russo_V/0/1/0/all/0/1">Vincent Russo</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+LaRose_R/0/1/0/all/0/1">Ryan LaRose</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Mari_A/0/1/0/all/0/1">Andrea Mari</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Strano_D/0/1/0/all/0/1">Dan Strano</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Bartschi_A/0/1/0/all/0/1">Andreas B&#xe4;rtschi</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Eidenbenz_S/0/1/0/all/0/1">Stephan Eidenbenz</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Zeng_W/0/1/0/all/0/1">William J. Zeng</a></p><p>Quantum Volume is a full-stack benchmark for near-term quantum computers. It
quantifies the largest size of a square circuit which can be executed on the
target device with reasonable fidelity. Error mitigation is a set of techniques
intended to remove the effects of noise present in the computation of noisy
quantum computers when computing an expectation value of interest. Effective
quantum volume is a proposed metric that applies error mitigation to the
quantum volume protocol in order to evaluate the effectiveness not only of the
target device but also of the error mitigation algorithm. Digital Zero-Noise
Extrapolation (ZNE) is an error mitigation technique that estimates the
noiseless expectation value using circuit folding to amplify errors by known
scale factors and extrapolating to the zero-noise limit. Here we demonstrate
that ZNE, with global and local unitary folding with fractional scale factors,
in conjunction with dynamical decoupling, can increase the effective quantum
volume over the vendor-measured quantum volume. Specifically, we measure the
effective quantum volume of four IBM Quantum superconducting processor units,
obtaining values that are larger than the vendor-measured quantum volume on
each device. This is the first such increase reported.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-29T00:30:00Z">Thursday, June 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15944'>Pb-Hash: Partitioned b-bit Hashing</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ping Li, Weijie Zhao</p><p>Many hashing algorithms including minwise hashing (MinHash), one permutation
hashing (OPH), and consistent weighted sampling (CWS) generate integers of $B$
bits. With $k$ hashes for each data vector, the storage would be $B\times k$
bits; and when used for large-scale learning, the model size would be
$2^B\times k$, which can be expensive. A standard strategy is to use only the
lowest $b$ bits out of the $B$ bits and somewhat increase $k$, the number of
hashes. In this study, we propose to re-use the hashes by partitioning the $B$
bits into $m$ chunks, e.g., $b\times m =B$. Correspondingly, the model size
becomes $m\times 2^b \times k$, which can be substantially smaller than the
original $2^B\times k$.
</p>
<p>Our theoretical analysis reveals that by partitioning the hash values into
$m$ chunks, the accuracy would drop. In other words, using $m$ chunks of $B/m$
bits would not be as accurate as directly using $B$ bits. This is due to the
correlation from re-using the same hash. On the other hand, our analysis also
shows that the accuracy would not drop much for (e.g.,) $m=2\sim 4$. In some
regions, Pb-Hash still works well even for $m$ much larger than 4. We expect
Pb-Hash would be a good addition to the family of hashing methods/applications
and benefit industrial practitioners.
</p>
<p>We verify the effectiveness of Pb-Hash in machine learning tasks, for linear
SVM models as well as deep learning models. Since the hashed data are
essentially categorical (ID) features, we follow the standard practice of using
embedding tables for each hash. With Pb-Hash, we need to design an effective
strategy to combine $m$ embeddings. Our study provides an empirical evaluation
on four pooling schemes: concatenation, max pooling, mean pooling, and product
pooling. There is no definite answer which pooling would be always better and
we leave that for future study.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Ping Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Weijie Zhao</a></p><p>Many hashing algorithms including minwise hashing (MinHash), one permutation
hashing (OPH), and consistent weighted sampling (CWS) generate integers of $B$
bits. With $k$ hashes for each data vector, the storage would be $B\times k$
bits; and when used for large-scale learning, the model size would be
$2^B\times k$, which can be expensive. A standard strategy is to use only the
lowest $b$ bits out of the $B$ bits and somewhat increase $k$, the number of
hashes. In this study, we propose to re-use the hashes by partitioning the $B$
bits into $m$ chunks, e.g., $b\times m =B$. Correspondingly, the model size
becomes $m\times 2^b \times k$, which can be substantially smaller than the
original $2^B\times k$.
</p>
<p>Our theoretical analysis reveals that by partitioning the hash values into
$m$ chunks, the accuracy would drop. In other words, using $m$ chunks of $B/m$
bits would not be as accurate as directly using $B$ bits. This is due to the
correlation from re-using the same hash. On the other hand, our analysis also
shows that the accuracy would not drop much for (e.g.,) $m=2\sim 4$. In some
regions, Pb-Hash still works well even for $m$ much larger than 4. We expect
Pb-Hash would be a good addition to the family of hashing methods/applications
and benefit industrial practitioners.
</p>
<p>We verify the effectiveness of Pb-Hash in machine learning tasks, for linear
SVM models as well as deep learning models. Since the hashed data are
essentially categorical (ID) features, we follow the standard practice of using
embedding tables for each hash. With Pb-Hash, we need to design an effective
strategy to combine $m$ embeddings. Our study provides an empirical evaluation
on four pooling schemes: concatenation, max pooling, mean pooling, and product
pooling. There is no definite answer which pooling would be always better and
we leave that for future study.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-29T00:30:00Z">Thursday, June 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.16053'>Erasing-based lossless compression method for streaming floating-point time series</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ruiyuan Li, Zheng Li, Yi Wu, Chao Chen, Songtao Guo, Ming Zhang, Yu Zheng</p><p>There are a prohibitively large number of floating-point time series data
generated at an unprecedentedly high rate. An efficient, compact and lossless
compression for time series data is of great importance for a wide range of
scenarios. Most existing lossless floating-point compression methods are based
on the XOR operation, but they do not fully exploit the trailing zeros, which
usually results in an unsatisfactory compression ratio. This paper proposes an
Erasing-based Lossless Floating-point compression algorithm, i.e., Elf. The
main idea of Elf is to erase the last few bits (i.e., set them to zero) of
floating-point values, so the XORed values are supposed to contain many
trailing zeros. The challenges of the erasing-based method are three-fold.
First, how to quickly determine the erased bits? Second, how to losslessly
recover the original data from the erased ones? Third, how to compactly encode
the erased data? Through rigorous mathematical analysis, Elf can directly
determine the erased bits and restore the original values without losing any
precision. To further improve the compression ratio, we propose a novel
encoding strategy for the XORed values with many trailing zeros. Furthermore,
observing the values in a time series usually have similar significand counts,
we propose an upgraded version of Elf named Elf+ by optimizing the significand
count encoding strategy, which improves the compression ratio and reduces the
running time further. Both Elf and Elf+ work in a streaming fashion. They take
only O(N) (where N is the length of a time series) in time and O(1) in space,
and achieve a notable compression ratio with a theoretical guarantee. Extensive
experiments using 22 datasets show the powerful performance of Elf and Elf+
compared with 9 advanced competitors for both double-precision and
single-precision floating-point values.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruiyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Songtao Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Ming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yu Zheng</a></p><p>There are a prohibitively large number of floating-point time series data
generated at an unprecedentedly high rate. An efficient, compact and lossless
compression for time series data is of great importance for a wide range of
scenarios. Most existing lossless floating-point compression methods are based
on the XOR operation, but they do not fully exploit the trailing zeros, which
usually results in an unsatisfactory compression ratio. This paper proposes an
Erasing-based Lossless Floating-point compression algorithm, i.e., Elf. The
main idea of Elf is to erase the last few bits (i.e., set them to zero) of
floating-point values, so the XORed values are supposed to contain many
trailing zeros. The challenges of the erasing-based method are three-fold.
First, how to quickly determine the erased bits? Second, how to losslessly
recover the original data from the erased ones? Third, how to compactly encode
the erased data? Through rigorous mathematical analysis, Elf can directly
determine the erased bits and restore the original values without losing any
precision. To further improve the compression ratio, we propose a novel
encoding strategy for the XORed values with many trailing zeros. Furthermore,
observing the values in a time series usually have similar significand counts,
we propose an upgraded version of Elf named Elf+ by optimizing the significand
count encoding strategy, which improves the compression ratio and reduces the
running time further. Both Elf and Elf+ work in a streaming fashion. They take
only O(N) (where N is the length of a time series) in time and O(1) in space,
and achieve a notable compression ratio with a theoretical guarantee. Extensive
experiments using 22 datasets show the powerful performance of Elf and Elf+
compared with 9 advanced competitors for both double-precision and
single-precision floating-point values.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-29T00:30:00Z">Thursday, June 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.16065'>Approximate Cartesian Tree Matching: an Approach Using Swaps</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Bastien Auvray, Julien David, Richard Groult, Thierry Lecroq</p><p>Cartesian tree pattern matching consists of finding all the factors of a text
that have the same Cartesian tree than a given pattern. There already exist
theoretical and practical solutions for the exact case. In this paper, we
propose the first algorithm for solving approximate Cartesian tree pattern
matching. We consider Cartesian tree pattern matching with one swap: given a
pattern of length m and a text of length n we present two algorithms that find
all the factors of the text that have the same Cartesian tree of the pattern
after one transposition of two adjacent symbols. The first algorithm uses a
characterization of a linear representation of the Cartesian trees called
parent-distance after one swap and runs in time Theta(mn) using Theta(m) space.
The second algorithm generates all the parent-distance tables of sequences that
have the same Cartesian tree than the pattern after one swap. It runs in time
O((m^2 + n)log m) and has O(m^2) space complexity.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Auvray_B/0/1/0/all/0/1">Bastien Auvray</a>, <a href="http://arxiv.org/find/cs/1/au:+David_J/0/1/0/all/0/1">Julien David</a>, <a href="http://arxiv.org/find/cs/1/au:+Groult_R/0/1/0/all/0/1">Richard Groult</a>, <a href="http://arxiv.org/find/cs/1/au:+Lecroq_T/0/1/0/all/0/1">Thierry Lecroq</a></p><p>Cartesian tree pattern matching consists of finding all the factors of a text
that have the same Cartesian tree than a given pattern. There already exist
theoretical and practical solutions for the exact case. In this paper, we
propose the first algorithm for solving approximate Cartesian tree pattern
matching. We consider Cartesian tree pattern matching with one swap: given a
pattern of length m and a text of length n we present two algorithms that find
all the factors of the text that have the same Cartesian tree of the pattern
after one transposition of two adjacent symbols. The first algorithm uses a
characterization of a linear representation of the Cartesian trees called
parent-distance after one swap and runs in time Theta(mn) using Theta(m) space.
The second algorithm generates all the parent-distance tables of sequences that
have the same Cartesian tree than the pattern after one swap. It runs in time
O((m^2 + n)log m) and has O(m^2) space complexity.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-29T00:30:00Z">Thursday, June 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.16129'>Refining the Adaptivity Notion in the Huge Object Model</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Tomer Adar, Eldar Fischer</p><p>The Huge Object model for distribution testing, first defined by Goldreich
and Ron in 2022, combines the features of classical string testing and
distribution testing. In this model we are given access to independent samples
from an unknown distribution $P$ over the set of strings $\{0,1\}^n$, but are
only allowed to query a few bits from the samples.
</p>
<p>The distinction between adaptive and non-adaptive algorithms, which is
natural in the realm of string testing (but is not relevant for classical
distribution testing), plays a substantial role in the Huge Object model as
well. In this work we show that in fact, the full picture in the Huge Object
model is much richer than just that of the ``adaptive vs. non-adaptive''
dichotomy.
</p>
<p>We define and investigate several models of adaptivity that lie between the
fully-adaptive and the completely non-adaptive extremes. These models are
naturally grounded by viewing the querying process from each sample
independently, and considering the ``algorithmic flow'' between them. For
example, if we allow no information at all to cross over between samples (up to
the final decision), then we obtain the locally bounded adaptive model,
arguably the ``least adaptive'' one apart from being completely non-adaptive. A
slightly stronger model allows only a ``one-way'' information flow. Even
stronger (but still far from being fully adaptive) models follow by taking
inspiration from the setting of streaming algorithms.
</p>
<p>To show that we indeed have a hierarchy, we prove a chain of exponential
separations encompassing most of the models that we define.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Adar_T/0/1/0/all/0/1">Tomer Adar</a>, <a href="http://arxiv.org/find/cs/1/au:+Fischer_E/0/1/0/all/0/1">Eldar Fischer</a></p><p>The Huge Object model for distribution testing, first defined by Goldreich
and Ron in 2022, combines the features of classical string testing and
distribution testing. In this model we are given access to independent samples
from an unknown distribution $P$ over the set of strings $\{0,1\}^n$, but are
only allowed to query a few bits from the samples.
</p>
<p>The distinction between adaptive and non-adaptive algorithms, which is
natural in the realm of string testing (but is not relevant for classical
distribution testing), plays a substantial role in the Huge Object model as
well. In this work we show that in fact, the full picture in the Huge Object
model is much richer than just that of the ``adaptive vs. non-adaptive''
dichotomy.
</p>
<p>We define and investigate several models of adaptivity that lie between the
fully-adaptive and the completely non-adaptive extremes. These models are
naturally grounded by viewing the querying process from each sample
independently, and considering the ``algorithmic flow'' between them. For
example, if we allow no information at all to cross over between samples (up to
the final decision), then we obtain the locally bounded adaptive model,
arguably the ``least adaptive'' one apart from being completely non-adaptive. A
slightly stronger model allows only a ``one-way'' information flow. Even
stronger (but still far from being fully adaptive) models follow by taking
inspiration from the setting of streaming algorithms.
</p>
<p>To show that we indeed have a hierarchy, we prove a chain of exponential
separations encompassing most of the models that we define.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-29T00:30:00Z">Thursday, June 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.16134'>New Menger-like dualities in digraphs and applications to half-integral linkages</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Victor Campos, Jonas Costa, Raul Lopes, Ignasi Sau</p><p>We present new min-max relations in digraphs between the number of paths
satisfying certain conditions and the order of the corresponding cuts. We
define these objects in order to capture, in the context of solving the
half-integral linkage problem, the essential properties needed for reaching a
large bramble of congestion two (or any other constant) from the terminal set.
This strategy has been used ad-hoc in several articles, usually with lengthy
technical proofs, and our objective is to abstract it to make it applicable in
a simpler and unified way. We provide two proofs of the min-max relations, one
consisting in applying Menger's Theorem on appropriately defined auxiliary
digraphs, and an alternative simpler one using matroids, however with worse
polynomial running time.
</p>
<p>As an application, we manage to simplify and improve several results of
Edwards et al. [ESA 2017] and of Giannopoulou et al. [SODA 2022] about finding
half-integral linkages in digraphs. Concerning the former, besides being
simpler, our proof provides an almost optimal bound on the strong connectivity
of a digraph for it to be half-integrally feasible under the presence of a
large bramble of congestion two (or equivalently, if the directed tree-width is
large, which is the hard case). Concerning the latter, our proof uses brambles
as rerouting objects instead of cylindrical grids, hence yielding much better
bounds and being somehow independent of a particular topology.
</p>
<p>We hope that our min-max relations will find further applications as, in our
opinion, they are simple, robust, and versatile to be easily applicable to
different types of routing problems in digraphs.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Campos_V/0/1/0/all/0/1">Victor Campos</a>, <a href="http://arxiv.org/find/math/1/au:+Costa_J/0/1/0/all/0/1">Jonas Costa</a>, <a href="http://arxiv.org/find/math/1/au:+Lopes_R/0/1/0/all/0/1">Raul Lopes</a>, <a href="http://arxiv.org/find/math/1/au:+Sau_I/0/1/0/all/0/1">Ignasi Sau</a></p><p>We present new min-max relations in digraphs between the number of paths
satisfying certain conditions and the order of the corresponding cuts. We
define these objects in order to capture, in the context of solving the
half-integral linkage problem, the essential properties needed for reaching a
large bramble of congestion two (or any other constant) from the terminal set.
This strategy has been used ad-hoc in several articles, usually with lengthy
technical proofs, and our objective is to abstract it to make it applicable in
a simpler and unified way. We provide two proofs of the min-max relations, one
consisting in applying Menger's Theorem on appropriately defined auxiliary
digraphs, and an alternative simpler one using matroids, however with worse
polynomial running time.
</p>
<p>As an application, we manage to simplify and improve several results of
Edwards et al. [ESA 2017] and of Giannopoulou et al. [SODA 2022] about finding
half-integral linkages in digraphs. Concerning the former, besides being
simpler, our proof provides an almost optimal bound on the strong connectivity
of a digraph for it to be half-integrally feasible under the presence of a
large bramble of congestion two (or equivalently, if the directed tree-width is
large, which is the hard case). Concerning the latter, our proof uses brambles
as rerouting objects instead of cylindrical grids, hence yielding much better
bounds and being somehow independent of a particular topology.
</p>
<p>We hope that our min-max relations will find further applications as, in our
opinion, they are simple, robust, and versatile to be easily applicable to
different types of routing problems in digraphs.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-29T00:30:00Z">Thursday, June 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.16352'>Information-Computation Tradeoffs for Learning Margin Halfspaces with Random Classification Noise</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ilias Diakonikolas, Jelena Diakonikolas, Daniel M. Kane, Puqian Wang, Nikos Zarifis</p><p>We study the problem of PAC learning $\gamma$-margin halfspaces with Random
Classification Noise. We establish an information-computation tradeoff
suggesting an inherent gap between the sample complexity of the problem and the
sample complexity of computationally efficient algorithms. Concretely, the
sample complexity of the problem is $\widetilde{\Theta}(1/(\gamma^2
\epsilon))$. We start by giving a simple efficient algorithm with sample
complexity $\widetilde{O}(1/(\gamma^2 \epsilon^2))$. Our main result is a lower
bound for Statistical Query (SQ) algorithms and low-degree polynomial tests
suggesting that the quadratic dependence on $1/\epsilon$ in the sample
complexity is inherent for computationally efficient algorithms. Specifically,
our results imply a lower bound of $\widetilde{\Omega}(1/(\gamma^{1/2}
\epsilon^2))$ on the sample complexity of any efficient SQ learner or
low-degree test.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1">Ilias Diakonikolas</a>, <a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_J/0/1/0/all/0/1">Jelena Diakonikolas</a>, <a href="http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1">Daniel M. Kane</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Puqian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zarifis_N/0/1/0/all/0/1">Nikos Zarifis</a></p><p>We study the problem of PAC learning $\gamma$-margin halfspaces with Random
Classification Noise. We establish an information-computation tradeoff
suggesting an inherent gap between the sample complexity of the problem and the
sample complexity of computationally efficient algorithms. Concretely, the
sample complexity of the problem is $\widetilde{\Theta}(1/(\gamma^2
\epsilon))$. We start by giving a simple efficient algorithm with sample
complexity $\widetilde{O}(1/(\gamma^2 \epsilon^2))$. Our main result is a lower
bound for Statistical Query (SQ) algorithms and low-degree polynomial tests
suggesting that the quadratic dependence on $1/\epsilon$ in the sample
complexity is inherent for computationally efficient algorithms. Specifically,
our results imply a lower bound of $\widetilde{\Omega}(1/(\gamma^{1/2}
\epsilon^2))$ on the sample complexity of any efficient SQ learner or
low-degree test.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-29T00:30:00Z">Thursday, June 29 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Wednesday, June 28
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://windowsontheory.org/2023/06/28/metaphors-for-ai-and-why-i-dont-like-them/'>Metaphors for AI, and why I don’t like them</a></h3>
        <p class='tr-article-feed'>from <a href='https://windowsontheory.org'>Windows on Theory</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Photo from National Photo Company Collection; see also (Sobel, 2017) [Cross posted in lesswrong and windows on theory see here for my prior writings] “computer, n. /kəmˈpjuːtə/. One who computes; a calculator, reckoner; specifically a person employed to make calculations in an observatory, in surveying, etc”, Oxford English Dictionary “There is no reason why mental as well as bodily labor should &#8230; Continue reading Metaphors for AI, and why I don’t like&#160;them
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p><em>Photo from </em><a href="https://www.loc.gov/pictures/item/2016838906/"><em>National Photo Company Collection</em></a><em>; see also (</em><a href="https://www.amazon.com/Glass-Universe-Harvard-Observatory-Measure-ebook/dp/B01CZCW45O"><em><u>Sobel, 2017</u></em></a><em>)</em></p>



<p><em>[Cross posted in </em><a href="https://www.lesswrong.com/posts/pBHga8mFq88dK7548/metaphors-for-ai-and-why-i-don-t-like-them"><em><u>lesswrong</u></em></a><em> </em> and windows on theory<em> see here for my </em><a href="https://windowsontheory.org/category/philosophizing/"><em><u>prior writings</u></em></a><em>]</em><br><br>“computer, <em>n. /kəmˈpjuːtə/</em>. <em>One who computes; a calculator, reckoner; specifically a person employed to make calculations in an observatory, in surveying, etc</em>”, Oxford English Dictionary</p>



<p>“<em>There is no reason why mental as well as bodily labor should not be economized by the aid of machinery</em>”, Charles Babbage, 1852</p>



<p><em>“Computing is normally done by writing certain symbols on paper. We may suppose that this paper is divided into squares like a child’s arithmetic book.. The behavior of the [human] computer at any moment is determined by the symbols which he is observing, and of his’ state of mind’ at that moment… We may suppose that in a simple operation not more than one symbol is altered.“</em>, Alan Turing, 1936</p>



<p><em>Metaphors</em> can be a great way to understand new concepts. When the first digital computers were envisioned and constructed, people used different metaphors to describe them. One such metaphor, used as early as 1950, was an <a href="https://www.theguardian.com/century/1950-1959/Story/0,,105181,00.html"><u>“electronic brain”</u></a>. (Of course today we think of this metaphor as attempting to explain the brain using the computer rather than the other way around.) Another metaphor is the name “computer” itself. This word has meant since the 1700s a human being whose job is to perform calculations. Computers, who by the late 1800s and early 1900s were often female, were instrumental to many scientific and technological advances.<br><br>Thus the device we call “a computer” was literally named after the job it was destined to replace. Interestingly, as recounted in the book <a href="https://www.amazon.com/Hidden-Figures-American-Untold-Mathematicians-ebook/dp/B0166JFFD0/ref=sr_1_1?hvadid=241710192377&amp;hvdev=c&amp;hvlocint=9001996&amp;hvlocphy=1008366&amp;hvnetw=g&amp;hvqmt=b&amp;hvrand=4723742300797761391&amp;hvtargid=kwd-280240200699&amp;hydadcr=3203_10391923&amp;keywords=hidden+figures+the+book&amp;qid=1687559672&amp;sr=8-1"><u>“Hidden Figures”</u></a>, despite the first electronic computers constructed in the 1940s, up until the 1960s NASA, still needed human computers as well. Also, these days scientific computing is only a small aspect of what we use electronic computers for. Electronic computers did end up eliminating the jobs of human computers, but this ended up being a tiny fraction of their influence on the workforce.<br> </p>



<p>Recently, it seems that every week someone mints a fresh metaphor for AI, often in an article saying that&nbsp;<em>“AI is nothing new because it is just like X”</em>. I believe that&nbsp;<strong>modern deep-learning-based AI is a fundamentally new concept that is not the same as any prior one</strong>. (This is despite the fact that the idea of “stacking more layers” goes back to Rosenblatt in the late 1950s; as we’ve seen time and again in Science,&nbsp;<a href="https://www.science.org/doi/10.1126/science.177.4047.393"><u>“more is different”</u></a>.) So, I am deeply suspicious of such articles, especially when they are aimed at a general (non-scientific) audience that can be confused or misled by these metaphors. I also believe that&nbsp;<strong>we do not understand AI enough to make confident predictions or analogs</strong>, so we should take any metaphor with a grain of salt. Hence I am also suspicious of articles saying&nbsp;<em>“AI is going to certainly cause Y because it is just like X.”</em></p>



<p>To be clear, I am by no means “anti metaphor”. Metaphors can be extremely valuable “intuition pumps”. I often use them when teaching, and in research and blogging as well (e.g., my recent post on&nbsp;<a href="https://www.lesswrong.com/posts/wDL6wiqg3c6WFisHq/gpt-as-an-intelligence-forklift"><u>“intelligence forklift”</u></a>). Turing himself came up with the “Turing Machine” by modeling a human computer. However, it is always important to remember our metaphors’ limitations.&nbsp;<strong>No single metaphor can capture all of AI’s essential elements, and we should be careful of over-interpreting metaphors.</strong>&nbsp;<br>&nbsp;</p>



<p>For example, in my blog post on the&nbsp;<a href="https://windowsontheory.org/2022/06/20/the-uneasy-relationship-between-deep-learning-and-classical-statistics/"><u>uneasy relationship between deep learning and statistics</u></a>, I claimed that human learning is a better metaphor for deep learning than statistical learning. I still stand behind this claim. However, it does not mean that we can automatically port all intuitions from the human learning domain. One example is that the human learning metaphor suggests that&nbsp;<em>curriculum learning</em> should be extremely useful for deep learning systems. But I have yet to see a convincing demonstration of this. (Though&nbsp;<a href="https://arxiv.org/abs/2306.11644"><u>filtering to higher quality content</u></a> does seem to help.)</p>



<p>With disclaimers and caveats out of the way, let’s survey some of the metaphors that have been proposed for AI models.&nbsp;</p>



<figure class="wp-block-image is-resized"><img src="https://lh3.googleusercontent.com/WMJXHLmaCZi81466DjzoOCPwNBEge3EFiPsLeqRHdElf7NT1lww_L7yxg0W4LLo_YiU9hKZhfQ4gG2752UvU48PjWmoSaAPVrW7FbaTvEDreFPsdNmG776HN9j_zOyXztkvIs1b0SljU96o2WeCi8O8" alt="" width="582" height="303" /></figure>



<p>Figure: OpenAI’s text-davinci-003 is 77% sure that it is not simply a next-token predictor.</p>



<h3 class="wp-block-heading"><strong>Stochastic parrots&nbsp;<img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f99c.png" alt="🦜" class="wp-smiley" style="height: 1em; max-height: 1em;" />.&nbsp;</strong>&nbsp;</h3>



<p>In an&nbsp;<a href="https://dl.acm.org/doi/10.1145/3442188.3445922"><u>influential paper</u></a>, Bender, Gebru, McMillan-Major, and Mitchell (aka “Shmitchell”) proposed the “stochastic parrot” metaphor for large language models (LLMs). (The paper contains several different critiques of the reliance on size as the sole or main driver of performance, many of which do not rely on the validity of this metaphor.) In their words&nbsp;<em>“an LM is a system for haphazardly stitching together sequences of linguistic forms it has observed in its vast training data, according to probabilistic information about how they combine, but without any reference to meaning: a stochastic parrot.”</em>&nbsp; This metaphor is literally true, in the sense that, true to their name, language modes aim to&nbsp;<em>model</em> language in the sense of reproducing the probability distribution of their training data. However, most people that have interacted with ChatGPT can see that it does much more than merely parrot its training set.&nbsp;</p>



<p>While it is true that Large Language Models are trained to maximize “parroting”, i.e. minimize the cross-entropy loss, this loss function does not capture all of their capabilities. For example, according to the&nbsp;<a href="https://arxiv.org/abs/2005.14165"><u>GPT-3 paper</u></a>, (plugging numbers from Table D.1 into the formula in Figure 3.1),&nbsp; the loss for their 13B parameter model was 1.97 while the loss for the 175B model is 1.73. This means that the 175B model is just a mildly better “parrot” than the 13B model (roughly, guessing the next token with probability 17.7% instead of 14%). However, in many tasks, the 175B model is&nbsp;<em>qualitatively superior</em> to the 13B one, achieving near-perfect performance whereas the 13B one barely beats random guessing. Similarly, GPT4 is qualitatively stronger than GPT3 despite the (presumed) small difference in the validation loss. It is these emerging capabilities that are the essential property of LLMs, and the reason that the stochastic parrots metaphor is misleading.</p>



<p>Emerging capabilities are not unique to language. The starting point for the&nbsp;<a href="https://www.nature.com/articles/nature16961"><u>AlphaGo</u></a> algorithm was training to predict (aka “parrot”) human moves from the KGS Go Server. Then, using self-play, the algorithm ended up not merely mimicking human play but surpassing it. Ultimately, like the “stone soup” fable, it turned out that human data was not needed at all, and models could achieve superior play&nbsp;<a href="https://arxiv.org/abs/1712.01815"><u>starting from zero</u></a>.&nbsp;</p>



<figure class="wp-block-image is-resized"><img src="https://lh6.googleusercontent.com/G9GRhEOjubRWJBLClzuVAt39jl7Dq5cMKEAMGCh2Dkq2qUfSbk8NqL6aS0eWpgw7_al9spUzkv2xiF0p2q3mIUHUVrXmFD_zM67lrq-_qeo1-sM96lhpN75mVvruInK5ol1URXm1fkRMb0Kp96l65SI" alt="" width="568" height="370" /></figure>



<p>Figure 3.10 in the&nbsp;<a href="https://arxiv.org/abs/2005.14165"><u>GPT-3 paper</u></a>. We see a dramatic increase in capabilities from the 13B model to the 175B model (for example going from 9.2% accuracy to 94.2% in 3-digit subtraction).</p>



<h3 class="wp-block-heading"><strong>JPEG of the web</strong> <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f5bc.png" alt="🖼" class="wp-smiley" style="height: 1em; max-height: 1em;" /><em><strong> </strong> </em></h3>



<p>In a February 2023&nbsp;<a href="https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web"><em><u>New Yorker</u></em><u> article</u></a>, Ted Chiang wrote that&nbsp;<em>“ChatGPT is a blurry JPEG of the web.”&nbsp;</em>He claimed that ChatGPT is analogous to&nbsp;<em>“a compressed copy of all the text on the Web.”</em> Once again, there is a sense in which this claim is true. 175B parameters, whether stored in 8,16, or 32-bit format, clearly isn’t very much compared to the size of the Internet. But Chiang seems to think that this is the&nbsp;<em>essential</em> fact about ChatGPT and that it is basically some degraded version of Google that would only be useful if we wanted to store the web in limited space. In his words,&nbsp;<em>“we aren’t losing our access to the Internet. So just how much use is a blurry jpeg, when you still have the original?”</em> Yet, by the time Chiang wrote the article, ChatGPT already reached&nbsp;<a href="https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app"><u>100 million users</u></a>, so clearly plenty of people did find uses for this “blurry jpeg”. Indeed, as we discuss below, it seems that Chiang himself by now moved to a new metaphor.</p>



<h3 class="wp-block-heading"><strong>The new McKinsey.</strong><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f574.png" alt="🕴" class="wp-smiley" style="height: 1em; max-height: 1em;" /></h3>



<p>In a May 2023&nbsp;<a href="https://www.newyorker.com/science/annals-of-artificial-intelligence/will-ai-become-the-new-mckinsey"><em><u>New Yorker</u></em><u> article</u></a>, Chiang wrote<em> “I would like to propose another metaphor for the risks of artificial intelligence. I suggest that we think about A.I. as a management-consulting firm, along the lines of McKinsey &amp; Company.”&nbsp;</em>He thinks of&nbsp;<em>“A.I. as a broad set of technologies being marketed to companies to help them cut their costs.”&nbsp;</em>&nbsp;Once again, I believe Chiang misses the mark. First, the revenue of the major consulting companies put together is&nbsp;<a href="https://slidescience.co/mbb-firms/"><u>less than $30B per year</u></a>. If the overall impact of AI ended up being comparable, then it would certainly count as the scenario that Aaronson and I called&nbsp;<a href="https://scottaaronson.blog/?p=7266"><u>“AI Fizzle”</u></a>. Just like digital computers ended up being so much more than a cheaper replacement for human numerical analysts, the exciting applications of AI are not about cutting costs but about creating new possibilities.&nbsp;</p>



<p>Chiang makes other points in the article, including a discussion of whether AI will end up strengthening capital or labor. I honestly think it’s impossible to predict these in advance. In fact, it seems that we cannot even agree about the past. Chiang argues that since the 1980s, economic conditions have deteriorated for the average American and says that&nbsp;<em>“shopping online is fast and easy, and streaming movies at home is cool, but I think a lot of people would willingly trade those conveniences for the ability to own their own homes, send their kids to college without running up lifelong debt, and go to the hospital without falling into bankruptcy.”&nbsp; </em>Yet, home ownership has hovered between 63 to 67 percent in the last 50 years, while the share of the population that completed college and life expectancy (aside from COVID) steadily increased (see&nbsp;<a href="https://en.wikipedia.org/wiki/Home-ownership_in_the_United_States"><u>here</u></a>,&nbsp;<a href="https://en.wikipedia.org/wiki/Educational_attainment_in_the_United_States"><u>here</u></a>, and&nbsp;<a href="https://ourworldindata.org/grapher/life-expectancy"><u>here</u></a>). In any case, the U.S. economy is not the right lens to view AI. Whether or not the benefits of AI will accrue disproportionately to the top percentile has less to do with the technology, and more to do with economic policies, which vary by country.<br><br><img src="https://lh4.googleusercontent.com/Zmfk5VQbG77hHS2W5WzBmD5aoShTiDzUgVTtkb7X7hcHSYhuzKp14QGVxnPXxrhApsZKqEog5V_1plmJyo0TpUxTCFJQF2WRZ0IjT5FOxJHxwIvzV8Kim9TJgTYqtDosoJ_zasCsEKB7cgHqDKgs8qo" style="width: 400px"></p>



<figure class="wp-block-image is-resized"><img src="https://lh6.googleusercontent.com/R0eIt0Eb-yrYqulpf6SCTP-DdLmrRjBIPYgVVQqrKfXanI3cKuJJXS0CqzOgT0nnOdCAyu5cqKnfXUMfDXNc0w1_PF7el6LVxwud9sFUpRrHPRTdQn8rL8JkGuSrxJR4WG63wqRZvti3a6QI8XjPLbM" alt="" width="531" height="419" /></figure>



<figure class="wp-block-image is-resized"><img src="https://lh5.googleusercontent.com/zagieAqaK_psSzDgWFE72XVCNguIG18o6-Fzg7D5yiid3DIRtJd02Kba9xsKFMKo7pG3FhoSRliPv98vdzOKxX6LS14_xP7aMtUtianXvaAkE1vIPkPRa42RAmIcottA2axeB5B9Sq4mOrOIXvbuMIQ" alt="" width="556" height="393" /></figure>



<h3 class="wp-block-heading"><strong>Markets, Bureaucracies, Democracies&nbsp;<img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4b5.png" alt="💵" class="wp-smiley" style="height: 1em; max-height: 1em;" /> <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f5f3.png" alt="🗳" class="wp-smiley" style="height: 1em; max-height: 1em;" /></strong></h3>



<p>In a June 2023&nbsp;<a href="https://www.economist.com/by-invitation/2023/06/21/artificial-intelligence-is-a-familiar-looking-monster-say-henry-farrell-and-cosma-shalizi"><u>article in the Economist</u></a> (see also&nbsp;<a href="https://twitter.com/henryfarrell/status/1671547591262191618?s=20"><u>Twitter thread</u></a>), Henry Farrell and Cosma Shalizi compared AIs to entities such as&nbsp;<em>“markets, bureaucracies, and even democracy.”</em> To be fair, Farell and Shalizi do not say that AIs are the same as these entities (which obviously are not the same as one another), but rather that they share essential similarities. This is building on Shalizi’s fascinating short essay,&nbsp;<a href="http://bactra.org/weblog/699.html"><u>“The Singularity in our past light-cone”</u></a> (2010), which claimed that the singularity has already happened (or maybe we’ve already crossed the event horizon) during the industrial revolution. The idea is that we can think of markets, bureaucracies, and democracies as information-processing systems. Markets take as input massive amounts of data and produce the prices for goods, labor, and stocks. Democracy and bureaucracies take as input massive amounts of data (including citizens’ preferences, gathered from elections, protests, media, and more) and produce decisions. Large language models are yet another system that processes massive amounts of data.&nbsp;</p>



<p>I believe that this is an interesting perspective, but again it fails to capture many aspects of AI, and so I disagree with Farell when&nbsp;<a href="https://twitter.com/henryfarrell/status/1671547607217209346?s=20"><u>he says</u></a>&nbsp;<em>“The point is that [LLMs are] nothing new.”</em> Currently, people are using ChatGPT as a tutor, researcher, writing assistant, coding assistant, and many more. None of these uses is captured by the “mapping big data to society-wide decisions” framework of markets, bureaucracies, and democracy. Nevertheless, as Farell and Shalizi say, AI is likely to transform all of these entities, whether through automating arbitrages, gathering sentiments, or enabling new forms of regulations. Once again, I believe that whether or not this transformation will be for good or bad depends more on people’s decisions than on the technology itself. Furthermore, even after the transformation happened, we are likely not to have a consensus on whether it was good or bad.</p>



<h3 class="wp-block-heading"><strong>King Midas / Genie&nbsp;<img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f451.png" alt="👑" class="wp-smiley" style="height: 1em; max-height: 1em;" /><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f9de.png" alt="🧞" class="wp-smiley" style="height: 1em; max-height: 1em;" /></strong></h3>



<p>One metaphor used for potentially risky AI is&nbsp;<a href="https://futureoflife.org/ai/artificial-intelligence-king-midas-problem/"><u>“King Midas”</u></a> or a&nbsp; “Genie”. In this metaphor, AI is like one of the many entities in legends &#8211; Genie,&nbsp;<a href="https://en.wikipedia.org/wiki/The_Monkey%27s_Paw"><u>Monkey’s paw</u></a>, etc. &#8211; that follow their user’s literal wishes, but in the least helpful way possible. This is not an unreasonable assumption. In almost any setting (aside from games such as Chess and Go) we cannot specify precisely our objective, and end up optimizing some&nbsp;<em>proxy</em> for it. This notion of “over optimization” or “over fitting” is extremely general in Machine Learning, deep or not. In particular, learning algorithms minimize “train loss”, which is a proxy for the “validation loss”. As such, there have been many proposed solutions for over-optimizing, including early stopping and regularizing. Often protecting against over-optimization amounts to tuning a single hyperparameter.&nbsp; Given that this is a well-established problem, which has well-understood families of solutions, I find it hard to believe that it will be an unsurmountable obstacle for mitigating ML risk.&nbsp;<br>&nbsp;</p>



<figure class="wp-block-image is-resized"><img src="https://lh3.googleusercontent.com/MoRCsLLW0L8qeyP2TfGKPrNq5Jd6janyK9DqH4husP-9dRiz0L3rYZ-tQ-ZUh-7wamr6U623-uRNdNdKjLS-gIoINpty0cx-nGZpMAilIChXx1Wn0rrkwoe4sGNGEMRu3JylFSEoN4KMUr9FBxlaZVA" alt="" width="544" height="278" /></figure>



<p>Figure from&nbsp;<a href="https://sohl-dickstein.github.io/2022/11/06/strong-Goodhart.html"><u>Jascha Sohl-Dickstein’s blog</u></a>.</p>



<h3 class="wp-block-heading"><strong>Paperclip Maximizer&nbsp;<img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4ce.png" alt="📎" class="wp-smiley" style="height: 1em; max-height: 1em;" /></strong></h3>



<p>The<a href="https://www.lesswrong.com/tag/squiggle-maximizer-formerly-paperclip-maximizer"><u> “paperclip maximizer”</u></a> thought experiment is actually a metaphor I quite like. It does a good job of providing intuition for the scenario that it models. I just don’t believe that scenario is very likely. Unlike the “King Midas” metaphor, in the “paperclip maximizer” setting, the AI model is not “over optimizing” a human-provided objective but rather pursuing a goal that is completely unaligned with those of its human creators. Moreover, it pursues this goal with such fanaticism, that it cannot spare any resources, and so the whole galaxy is not big enough for humans and this goal to co-exist.&nbsp;</p>



<p>The paperclip maximizer is a fine metaphor, but I find it unlikely that highly intelligent entities will monomaniacally pursue any particular goal. Indeed, I think that highly-intelligent AIs will also realize (as do we) the dangers of “over optimizing” to any particular goal. Just as humans have learned the wisdom of diversifying our investments and the precautionary principle (even if to a more limited than some readers would like), I suspect AIs would know this too. Using all of humanity’s atoms to make paperclips or squiggles strikes me as analogous to burning all the fossil fuels in the earth for energy. AIs will be smarter than that. I also tend to agree with Sohl-Dickstein’s&nbsp;<a href="https://sohl-dickstein.github.io/2023/03/09/coherence.html"><u>“hot mess hypothesis”</u></a> that more intelligent entities are likely to be&nbsp;<em>less</em> coherent in the sense of having a variety of competing goals, none of which is being pursued monomaniacally.&nbsp;</p>



<figure class="wp-block-image is-resized"><img src="https://lh4.googleusercontent.com/Ntsfudiu47u0VeVqre-1dxyh-1LTNHst-x5xaKOf8erbcKzSx8AIQ_E7htzYVVFY3oFJlnQ6mHfbm6jcV8qVS9shqPa7UPtf4t3WW53Ro07iD50za2L4jKKsEc8BX0pAFQHudEfHyoqL0Bo5Q2Kd5Kw" alt="" width="579" height="477" /></figure>



<p>Figure from&nbsp;<a href="https://sohl-dickstein.github.io/2023/03/09/coherence.html"><u>Jascha Sohl-Dickstein’s blog</u></a>.</p>



<h3 class="wp-block-heading"><strong>Alien / New Species&nbsp;<img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f47d.png" alt="👽" class="wp-smiley" style="height: 1em; max-height: 1em;" /></strong></h3>



<figure class="wp-block-image is-resized"><img src="https://lh5.googleusercontent.com/5tcU02LK_EeMUuFVc_vB6_cIjSAnX7YhsbvVjvfrgdxOoQc-3nul8HnVwPrOWOCYuNVZt81Mb4iR3zio_rXX3G3RbaSS6aJDkNxeUhzSUi6G51U25cmw5M3pjiQu14Mzx3l-iEovJ9RqqCoMyCLDkCQ" alt="" width="567" height="315" /></figure>



<p>Figure: Cartoon from my&nbsp;<a href="https://www.lesswrong.com/posts/wDL6wiqg3c6WFisHq/gpt-as-an-intelligence-forklift"><u>“Intelligence forklift” post</u></a>, superimposing a 1000T parameter transformer on a figure from&nbsp;<a href="https://chomsky.info/20140826/"><u>Bolhuis, Tattersall, Chomsky and Berwick</u></a>.&nbsp;</p>



<p>A persistent metaphor for future AIs is that they will be essentially a new alien or animal species, perhaps essentially the next in line in the genus&nbsp;<em>Homo</em>. In particular, since our brain are 3 times bigger than those of Chimpanzees, if AI’s “brain size” (e.g., number of parameters) would be at least 3 times bigger than ours, we might expect the relationship between AIs and us to be roughly the same as the relation between us and Chimpanzees. This is an important viewpoint to consider, but I believe that it should be taken with significant grains of salt, as it does anthropomorphize AIs too much. As&nbsp;<a href="https://www.lesswrong.com/posts/wDL6wiqg3c6WFisHq/gpt-as-an-intelligence-forklift"><u>I wrote before</u></a>, the fact that intelligence and agency co-evolved in humans does not mean that they cannot be decoupled in AIs. My main issue with the “new species” metaphor is one of&nbsp;<em>quantifiers</em> (∀ vs. ∃). I have no doubt that eventually&nbsp;<em>there will exist</em> an AI that can be thought of as an individual creature. We should eventually be able to build a robot that has its own goals and can not only prove hard theorems but even&nbsp;<a href="https://behavior.stanford.edu/"><u>cook an Omelet in a messy kitchen</u></a>. However, I doubt that the species metaphor would be a good one for&nbsp;<em>every</em> strong AI or even for most of them.</p>



<p>There is another issue with the “new species” metaphor, which is that it suggests that as a new species with a “larger brain”, AIs would completely dominate humans. However,&nbsp;<strong>intelligence is not a single number</strong>. Intelligence is a multidimensional set of skills/capabilities. This is one area where intuitions from games such as Chess or Go lead us astray, While in Chess it is possible to assign an “ELO rating” which measures the skill (i.e. the&nbsp;<a href="https://arxiv.org/abs/2004.09468"><u>transitive dimension</u></a>), in real-life settings, people have a high-dimensional mixture of skills. Even in a field as “intellectually pure” as pure mathematics, we cannot find anything similar to a total ordering of mathematicians by skill. If you have worked in this field, you will see that top mathematicians have incomparable strengths and weaknesses, and the set of theorems that one can prove is not a subset of that which can be proven by another.</p>



<p>It is true that in the evolution of our genus, many of these skills improved together, and I believe it’s also true that humans are more intelligent than Chimpanzees across the board. But these correlations don’t necessarily indicate causation, and there is no reason for AIs to evolve the same way. Indeed, most people that have interacted with ChatGPT have seen it both perform highly impressive feats and make stupid mistakes. We also see it in&nbsp;<a href="https://huggingface.co/blog/evaluating-mmlu-leaderboard"><u>the sensitivity of benchmarks</u></a> to ways of measurement. (For example, unlike with humans, you actually need to worry about whether an LLM, when faced with answering a multiple choice question with options A,B,C,D, will output “Zygote” instead.)&nbsp;</p>



<p><img src="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/98ffa3182cd22a89aac0474026bfe8a9945ca54ad9f6617d.png" style="width: 450px"><br>Figure: Cartoon of various skills, with the bars corresponding to the compute required to reach mean human performance for each skill. If the gap between the easiest and hardest skills is X orders of magnitude, and we assume increased performance is primarily driven by scale and Moore&#8217;s law, then we might expect a period of roughly 7X years in which humans and AIs are incomparable.<br>&nbsp;</p>



<p>All of which is not to say that ultimately one cannot get AI systems that are very strong in all of these dimensions, but just that we should not expect it to happen quickly, and there is likely to be an extended period in which AI’s and humans have different (and hence incomparable) skill profiles.&nbsp; In particular, if we stay with our “intelligence through scale” paradigm, improving on any particular skill would be quite expensive. It can cost three or more orders of magnitude more compute to move from a passable performance to a great one. And at the moment we have no way of getting near-perfect accuracy (99.9% or more), which is crucial for many applications. While I don’t doubt it is physically possible, we might never get to the point where it’s energy efficient or cost-effective to manufacture AIs that dominate humans in all intellectual dimensions. (And of course, humans are great at using tools, including AIs, to amplify our own shortcomings; this is how we routinely perform intellectual and physical feats today that were unthinkable 500 years ago, not to mention 5000 years, using essentially the same brains we had then.)</p>



<figure class="wp-block-image is-resized"><img src="https://lh6.googleusercontent.com/AJB_m9NVa7xfQmOHZ0arJClEy6Vs9eYRqpsWX395CGreCG36EbBnwI9s-ezV5Krgthc_SzUUyVtFmRGq-a1psOdZWdarLakejAELU-FYkIaQQuHspQzxTPDMV4KI6_8xyI6X7cVR-fS3-fo-Xub8zTc" alt="" width="392" height="300" /></figure>



<p>Figure: Compute to benchmark extrapolations of&nbsp;<a href="https://www.alignmentforum.org/posts/75o8oja43LXGAqbAR/palm-2-and-gpt-4-in-extrapolating-gpt-n-performance"><u>Lukas Finnveden</u></a>. It typically takes around 3-4 orders of magnitude in compute cost from the point where models beat chance performance in a benchmark until the point they saturate it.</p>



<h3 class="wp-block-heading"><strong>Shoggoth</strong></h3>



<figure class="wp-block-image is-resized"><img src="https://lh6.googleusercontent.com/Cfw3h_OTJUk-bqh3xCuHhGz5YCi8G7rP31oMRpb85XgVv7Cn55h038uxP04Es1OozKu9Z-AqBsyv-YGcIIGLR2W3QBEuM26jZdOieuy54YiE0nMzhvAM4Hhz6GKUCluEVU9CmikzCeg2lO6wn7dTm28" alt="" width="633" height="187" /></figure>



<p>Left:&nbsp;<a href="https://www.deviantart.com/nottsuo"><u>Nottsuo</u></a>’s drawing of a Shoggoth.&nbsp; Right: Cartoon by&nbsp;<a href="https://twitter.com/TetraspaceWest"><u>@TetraspaceWest</u></a>.</p>



<p>A&nbsp;<a href="https://twitter.com/TetraspaceWest/status/1608966939929636864?s=20"><u>Shoggoth</u></a> is a perfect metaphor for AIs because no one knows what Shoggoths are. So, everyone can agree that “AI is a Shoggoth” means that AI is the thing that they already think AI is. (Apologies to the readers who propelled&nbsp;<a href="https://www.amazon.com/At-Mountains-Madness-H-Lovecraft/dp/1627555765/"><u>At the Mountains of Madness</u></a> to the 776,464th place on Amazon’s best-selling list.)&nbsp; In particular,&nbsp;<a href="https://www.economist.com/by-invitation/2023/06/21/artificial-intelligence-is-a-familiar-looking-monster-say-henry-farrell-and-cosma-shalizi"><u>Farrell and Shalizi</u></a> call markets and democracies “Shoggoths”. Wikipedia says that&nbsp;<em>“Cthulhu Mythos media ​​most commonly portray shoggoths as intelligent to some degree, but deal with problems using only their great size and strength”</em>: a description that almost seems lifted from the “Stochastic Parrots” paper.&nbsp;<a href="https://astralcodexten.substack.com/p/janus-simulators"><u>Scott Alexander</u></a> describes how the metaphors of Agent, Genie, or Oracle are not a good match for (non fine-tuned / RLHF’ed) language models like GPT-3. Rather they are best captured as what Janus called a&nbsp;<a href="https://www.lesswrong.com/s/N7nDePaNabJdnbXeE/p/vJFdjigzmcXMhNTsx"><u>Simulator</u></a> and Scott calls “an unmasked Shoggoth”. In both the market and simulator interpretations, the Shoggoth is a monster not because it’s evil but because it has no personality at all. (Though, despite writing in the&nbsp;<em>Economist</em>, Farrell and Shalizi seem to have a dim view of both markets and AIs.)&nbsp; In p<a href="https://astralcodexten.substack.com/i/97825611/iv-the-masked-shoggoth-between-keyboard-and-chair"><u>art IV of his essay</u></a>, Alexander speculates that perhaps we are all “masked Shoggoths”, and that becoming “enlightened” corresponds to removing the mask, stopping being an agent, and being at peace just predicting that universe.<br>&nbsp;</p>



<h3 class="wp-block-heading"><strong>Conclusions</strong></h3>



<p>AI systems of the type that have recently been created, and that will appear in the near future, are fundamentally new objects. Metaphors can be useful ways to build intuitions for them, but we should be wary of any article of the form “AI is like X”, especially one aimed at the general public. Probably the best we can say is that “This aspect of AI is like X” or “These types of AI systems, used in this way, are like X”. As mentioned, some of the people that coined these metaphors are well aware of these limitations, but metaphors have a way of being over-interpreted. I hope this essay can serve as a partial antidote to this tendency.<br>&nbsp;</p>



<p><strong>Acknowledgments:</strong> Thanks to Scott Aaronson, Michael Nielsen, and Preetum Nakkiran, for commenting on a previous version of this essay. Needless to say, they are not responsible for any views or errors in it.<br>&nbsp;</p>
<p class="authors">By Boaz Barak</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-28T22:46:34Z">Wednesday, June 28 2023, 22:46</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/092'>TR23-092 |  Extracting Mergers and Projections of Partitions | 

	Swastik Kopparty, 

	Vishvajeet N</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We study the problem of extracting randomness from somewhere random sources, and related combinatorial phenomena: partition analogues of Shearer&#39;s lemma on projections.

A somewhere-random source is a tuple $(X_1, \ldots, X_t)$ of (possibly correlated) $\{0,1\}^n$-valued random variables $X_i$ where for some unknown $i \in [t]$, $X_i$ is guaranteed to be uniformly distributed. 

An $extracting$ $merger$ is a seeded device that takes a somewhere-random source as input and outputs nearly uniform random bits. We study the seed-length needed for extracting mergers with constant $t$ and constant error.

Since a somewhere-random source has min-entropy at least $n$, a standard extractor can also serve as an extracting merger. Our goal is to understand whether the further structure of being somewhere random rather than just having high entropy enables smaller seed length, and towards this we show:

$\cdot$ Just like in the case of standard extractors, seedless extracting mergers with even just one output bit do not exist.

$\cdot$ Unlike the case of standard extractors, it $is$ possible to have extracting mergers that output a constant number of bits using only constant seed. Furthermore, a random choice of merger does not work for this purpose!

$\cdot$ Nevertheless, just like in the case of standard extractors, an extracting merger which gets most of the entropy out (namely, having $\Omega(n)$ output bits) must have $\Omega(\log n)$ seed. This is the main technical result of our work, and is proved by a second moment strengthening of the graph-theoretic approach of Radhakrishnan and Ta-Shma to extractors.

All this is in contrast to the status for condensing mergers (where the output is only required to have high min-entropy), whose seed length/output-length tradeoffs can all be fully explained by using standard condensers.

Inspired by such considerations, we also formulate a new and basic class of  problems in combinatorics: partition analogues of Shearer&#39;s lemma. We show basic results in this direction; in particular, we prove that in any partition of the $3$-dimensional cube $[0,1]^3$ into two parts, one of the parts has an axis parallel $2$-dimensional projection of area at least $3/4$.
        
        </div>

        <div class='tr-article-summary'>
        
          
          We study the problem of extracting randomness from somewhere random sources, and related combinatorial phenomena: partition analogues of Shearer&#39;s lemma on projections.

A somewhere-random source is a tuple $(X_1, \ldots, X_t)$ of (possibly correlated) $\{0,1\}^n$-valued random variables $X_i$ where for some unknown $i \in [t]$, $X_i$ is guaranteed to be uniformly distributed. 

An $extracting$ $merger$ is a seeded device that takes a somewhere-random source as input and outputs nearly uniform random bits. We study the seed-length needed for extracting mergers with constant $t$ and constant error.

Since a somewhere-random source has min-entropy at least $n$, a standard extractor can also serve as an extracting merger. Our goal is to understand whether the further structure of being somewhere random rather than just having high entropy enables smaller seed length, and towards this we show:

$\cdot$ Just like in the case of standard extractors, seedless extracting mergers with even just one output bit do not exist.

$\cdot$ Unlike the case of standard extractors, it $is$ possible to have extracting mergers that output a constant number of bits using only constant seed. Furthermore, a random choice of merger does not work for this purpose!

$\cdot$ Nevertheless, just like in the case of standard extractors, an extracting merger which gets most of the entropy out (namely, having $\Omega(n)$ output bits) must have $\Omega(\log n)$ seed. This is the main technical result of our work, and is proved by a second moment strengthening of the graph-theoretic approach of Radhakrishnan and Ta-Shma to extractors.

All this is in contrast to the status for condensing mergers (where the output is only required to have high min-entropy), whose seed length/output-length tradeoffs can all be fully explained by using standard condensers.

Inspired by such considerations, we also formulate a new and basic class of  problems in combinatorics: partition analogues of Shearer&#39;s lemma. We show basic results in this direction; in particular, we prove that in any partition of the $3$-dimensional cube $[0,1]^3$ into two parts, one of the parts has an axis parallel $2$-dimensional projection of area at least $3/4$.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-28T13:25:21Z">Wednesday, June 28 2023, 13:25</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15004'>One-step replica symmetry breaking in the language of tensor networks</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Nicola Pancotti, Johnnie Gray</p><p>We develop an exact mapping between the one-step replica symmetry breaking
cavity method and tensor networks. The two schemes come with complementary
mathematical and numerical toolboxes that could be leveraged to improve the
respective states of the art. As an example, we construct a tensor-network
representation of Survey Propagation, one of the best deterministic k-SAT
solvers. The resulting algorithm outperforms any existent tensor-network solver
by several orders of magnitude. We comment on the generality of these ideas,
and we show how to extend them to the context of quantum tensor networks.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Pancotti_N/0/1/0/all/0/1">Nicola Pancotti</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Gray_J/0/1/0/all/0/1">Johnnie Gray</a></p><p>We develop an exact mapping between the one-step replica symmetry breaking
cavity method and tensor networks. The two schemes come with complementary
mathematical and numerical toolboxes that could be leveraged to improve the
respective states of the art. As an example, we construct a tensor-network
representation of Survey Propagation, one of the best deterministic k-SAT
solvers. The resulting algorithm outperforms any existent tensor-network solver
by several orders of magnitude. We comment on the generality of these ideas,
and we show how to extend them to the context of quantum tensor networks.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-28T00:30:00Z">Wednesday, June 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15079'>From $O(\sqrt n)$ to $O(\log n)$ in Quadratic Programming</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Liang Wu</p><p>A "dark cloud" hangs over numerical optimization theory for decades, namely,
whether an optimization algorithm $O(\log(n))$ iteration complexity exists.
"Yes", this paper answers, with a new optimization algorithm and strict theory
proof. It starts with box-constrained quadratic programming (Box-QP), and many
practical optimization problems fall into Box-QP. Smooth quadratic programming
(QP) and nonsmooth Lasso can be reformulated as Box-QP via duality theory. It
is the first time to present an $O(\log(n))$ iteration complexity QP algorithm,
in particular, which behaves like a "direct" method: the required number of
iterations is deterministic with exact value
$\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$.
This significant breakthrough enables us to transition from the $O(\sqrt{n})$
to the $O(\log(n))$ optimization algorithm, whose amazing scalability is
particularly relevant in today's era of big data and artificial intelligence.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Wu_L/0/1/0/all/0/1">Liang Wu</a></p><p>A "dark cloud" hangs over numerical optimization theory for decades, namely,
whether an optimization algorithm $O(\log(n))$ iteration complexity exists.
"Yes", this paper answers, with a new optimization algorithm and strict theory
proof. It starts with box-constrained quadratic programming (Box-QP), and many
practical optimization problems fall into Box-QP. Smooth quadratic programming
(QP) and nonsmooth Lasso can be reformulated as Box-QP via duality theory. It
is the first time to present an $O(\log(n))$ iteration complexity QP algorithm,
in particular, which behaves like a "direct" method: the required number of
iterations is deterministic with exact value
$\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$.
This significant breakthrough enables us to transition from the $O(\sqrt{n})$
to the $O(\log(n))$ optimization algorithm, whose amazing scalability is
particularly relevant in today's era of big data and artificial intelligence.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-28T00:30:00Z">Wednesday, June 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15339'>A Note on the Complexity of One-Sided Crossing Minimization of Trees</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Alexander Dobler</p><p>In 2011, Harrigan and Healy published a polynomial-time algorithm for
one-sided crossing minimization for trees. We point out a counterexample to
that algorithm, and show that one-sided crossing minimization is NP-hard for
trees.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dobler_A/0/1/0/all/0/1">Alexander Dobler</a></p><p>In 2011, Harrigan and Healy published a polynomial-time algorithm for
one-sided crossing minimization for trees. We point out a counterexample to
that algorithm, and show that one-sided crossing minimization is NP-hard for
trees.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-28T00:30:00Z">Wednesday, June 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15656'>SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Fu-Ming Guo</p><p>This paper introduces SparseOptimizer, a novel deep learning optimizer that
exploits Moreau-Yosida regularization to naturally induce sparsity in large
language models such as BERT, ALBERT and GPT. Key to the design of
SparseOptimizer is an embedded shrinkage operator, which imparts sparsity
directly within the optimization process. This operator, backed by a sound
theoretical framework, includes an analytical solution, thereby reinforcing the
optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play
functionality eradicates the need for code modifications, making it a
universally adaptable tool for a wide array of large language models. Empirical
evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2
confirm that SparseBERT and SparseALBERT, when sparsified using
SparseOptimizer, achieve performance comparable to their dense counterparts,
BERT and ALBERT, while significantly reducing their parameter count. Further,
this work proposes an innovative optimizer-compiler co-design strategy,
demonstrating the potential of inference acceleration (\textbf{3.37x},
\textbf{6.30x}, and \textbf{7.15x} in comparison with Pytorch, TensorFlow, and
LLVM generic compile, respectively) in SparseBERT when paired with an
appropriately designed compiler. This study represents a significant step
forward in the evolution of efficient, scalable, and high-performing large
language models, setting a precedent for future exploration and optimization in
this domain. The SparseOptimizer code and SparseALBERT model will be made
available upon paper acceptance.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1">Fu-Ming Guo</a></p><p>This paper introduces SparseOptimizer, a novel deep learning optimizer that
exploits Moreau-Yosida regularization to naturally induce sparsity in large
language models such as BERT, ALBERT and GPT. Key to the design of
SparseOptimizer is an embedded shrinkage operator, which imparts sparsity
directly within the optimization process. This operator, backed by a sound
theoretical framework, includes an analytical solution, thereby reinforcing the
optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play
functionality eradicates the need for code modifications, making it a
universally adaptable tool for a wide array of large language models. Empirical
evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2
confirm that SparseBERT and SparseALBERT, when sparsified using
SparseOptimizer, achieve performance comparable to their dense counterparts,
BERT and ALBERT, while significantly reducing their parameter count. Further,
this work proposes an innovative optimizer-compiler co-design strategy,
demonstrating the potential of inference acceleration (\textbf{3.37x},
\textbf{6.30x}, and \textbf{7.15x} in comparison with Pytorch, TensorFlow, and
LLVM generic compile, respectively) in SparseBERT when paired with an
appropriately designed compiler. This study represents a significant step
forward in the evolution of efficient, scalable, and high-performing large
language models, setting a precedent for future exploration and optimization in
this domain. The SparseOptimizer code and SparseALBERT model will be made
available upon paper acceptance.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-28T00:30:00Z">Wednesday, June 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15097'>Approximating Median Points in a Convex Polygon</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Reyhaneh Mohammadi, Raghuveer Devulapalli, Mehdi Behroozi</p><p>We develop two simple and efficient approximation algorithms for the
continuous $k$-medians problems, where we seek to find the optimal location of
$k$ facilities among a continuum of client points in a convex polygon $C$ with
$n$ vertices in a way that the total (average) Euclidean distance between
clients and their nearest facility is minimized. Both algorithms run in
$\mathcal{O}(n + k + k \log n)$ time. Our algorithms produce solutions within a
factor of 2.002 of optimality. In addition, our simulation results applied to
the convex hulls of the State of Massachusetts and the Town of Brookline, MA
show that our algorithms generally perform within a range of 5\% to 22\% of
optimality in practice.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Mohammadi_R/0/1/0/all/0/1">Reyhaneh Mohammadi</a>, <a href="http://arxiv.org/find/math/1/au:+Devulapalli_R/0/1/0/all/0/1">Raghuveer Devulapalli</a>, <a href="http://arxiv.org/find/math/1/au:+Behroozi_M/0/1/0/all/0/1">Mehdi Behroozi</a></p><p>We develop two simple and efficient approximation algorithms for the
continuous $k$-medians problems, where we seek to find the optimal location of
$k$ facilities among a continuum of client points in a convex polygon $C$ with
$n$ vertices in a way that the total (average) Euclidean distance between
clients and their nearest facility is minimized. Both algorithms run in
$\mathcal{O}(n + k + k \log n)$ time. Our algorithms produce solutions within a
factor of 2.002 of optimality. In addition, our simulation results applied to
the convex hulls of the State of Massachusetts and the Town of Brookline, MA
show that our algorithms generally perform within a range of 5\% to 22\% of
optimality in practice.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-28T00:30:00Z">Wednesday, June 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15276'>Heuristic Approaches to Obtain Low-Discrepancy Point Sets via Subset Selection</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Fran&#xe7;ois Cl&#xe9;ment, Carola Doerr, Lu&#xed;s Paquete</p><p>Building upon the exact methods presented in our earlier work [J. Complexity,
2022], we introduce a heuristic approach for the star discrepancy subset
selection problem. The heuristic gradually improves the current-best subset by
replacing one of its elements at a time. While we prove that the heuristic does
not necessarily return an optimal solution, we obtain very promising results
for all tested dimensions. For example, for moderate point set sizes $30 \leq n
\leq 240$ in dimension 6, we obtain point sets with $L_{\infty}$ star
discrepancy up to 35% better than that of the first $n$ points of the Sobol'
sequence. Our heuristic works in all dimensions, the main limitation being the
precision of the discrepancy calculation algorithms.
</p>
<p>We also provide a comparison with a recent energy functional introduced by
Steinerberger [J. Complexity, 2019], showing that our heuristic performs better
on all tested instances.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Clement_F/0/1/0/all/0/1">Fran&#xe7;ois Cl&#xe9;ment</a>, <a href="http://arxiv.org/find/cs/1/au:+Doerr_C/0/1/0/all/0/1">Carola Doerr</a>, <a href="http://arxiv.org/find/cs/1/au:+Paquete_L/0/1/0/all/0/1">Lu&#xed;s Paquete</a></p><p>Building upon the exact methods presented in our earlier work [J. Complexity,
2022], we introduce a heuristic approach for the star discrepancy subset
selection problem. The heuristic gradually improves the current-best subset by
replacing one of its elements at a time. While we prove that the heuristic does
not necessarily return an optimal solution, we obtain very promising results
for all tested dimensions. For example, for moderate point set sizes $30 \leq n
\leq 240$ in dimension 6, we obtain point sets with $L_{\infty}$ star
discrepancy up to 35% better than that of the first $n$ points of the Sobol'
sequence. Our heuristic works in all dimensions, the main limitation being the
precision of the discrepancy calculation algorithms.
</p>
<p>We also provide a comparison with a recent energy functional introduced by
Steinerberger [J. Complexity, 2019], showing that our heuristic performs better
on all tested instances.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-28T00:30:00Z">Wednesday, June 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15338'>Insertion-Only Dynamic Connectivity in General Disk Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Haim Kaplan, Katharina Klost, Kristin Knorr, Wolfgang Mulzer, Liam Roditty</p><p>Let $S \subseteq \mathbb{R}^2$ be a set of $n$ \emph{sites} in the plane, so
that every site $s \in S$ has an \emph{associated radius} $r_s &gt; 0$. Let $D(S)$
be the \emph{disk intersection graph} defined by $S$, i.e., the graph with
vertex set $S$ and an edge between two distinct sites $s, t \in S$ if and only
if the disks with centers $s$, $t$ and radii $r_s$, $r_t$ intersect. Our goal
is to design data structures that maintain the connectivity structure of $D(S)$
as $S$ changes dynamically over time. We consider the incremental case, where
new sites can be inserted into $S$. While previous work focuses on data
structures whose running time depends on the ratio between the smallest and the
largest site in $S$, we present a data structure with $O(\alpha(n))$ amortized
query time and $O(\log^6 n)$ expected amortized insertion time.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kaplan_H/0/1/0/all/0/1">Haim Kaplan</a>, <a href="http://arxiv.org/find/cs/1/au:+Klost_K/0/1/0/all/0/1">Katharina Klost</a>, <a href="http://arxiv.org/find/cs/1/au:+Knorr_K/0/1/0/all/0/1">Kristin Knorr</a>, <a href="http://arxiv.org/find/cs/1/au:+Mulzer_W/0/1/0/all/0/1">Wolfgang Mulzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Roditty_L/0/1/0/all/0/1">Liam Roditty</a></p><p>Let $S \subseteq \mathbb{R}^2$ be a set of $n$ \emph{sites} in the plane, so
that every site $s \in S$ has an \emph{associated radius} $r_s &gt; 0$. Let $D(S)$
be the \emph{disk intersection graph} defined by $S$, i.e., the graph with
vertex set $S$ and an edge between two distinct sites $s, t \in S$ if and only
if the disks with centers $s$, $t$ and radii $r_s$, $r_t$ intersect. Our goal
is to design data structures that maintain the connectivity structure of $D(S)$
as $S$ changes dynamically over time. We consider the incremental case, where
new sites can be inserted into $S$. While previous work focuses on data
structures whose running time depends on the ratio between the smallest and the
largest site in $S$, we present a data structure with $O(\alpha(n))$ amortized
query time and $O(\log^6 n)$ expected amortized insertion time.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-28T00:30:00Z">Wednesday, June 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15621'>Approximate Nearest Neighbor Searching with Non-Euclidean and Weighted Distances</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ahmed Abdelkader, Sunil Arya, Guilherme D. da Fonseca, David M. Mount</p><p>We present a new approach to approximate nearest-neighbor queries in fixed
dimension under a variety of non-Euclidean distances. We are given a set $S$ of
$n$ points in $\mathbb{R}^d$, an approximation parameter $\varepsilon &gt; 0$, and
a distance function that satisfies certain smoothness and growth-rate
assumptions. The objective is to preprocess $S$ into a data structure so that
for any query point $q$ in $\mathbb{R}^d$, it is possible to efficiently report
any point of $S$ whose distance from $q$ is within a factor of $1+\varepsilon$
of the actual closest point.
</p>
<p>Prior to this work, the most efficient data structures for approximate
nearest-neighbor searching in spaces of constant dimensionality applied only to
the Euclidean metric. This paper overcomes this limitation through a method
called convexification. For admissible distance functions, the proposed data
structures answer queries in logarithmic time using $O(n \log (1 / \varepsilon)
/ \varepsilon^{d/2})$ space, nearly matching the best known bounds for the
Euclidean metric. These results apply to both convex scaling distance functions
(including the Mahalanobis distance and weighted Minkowski metrics) and Bregman
divergences (including the Kullback-Leibler divergence and the Itakura-Saito
distance).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Abdelkader_A/0/1/0/all/0/1">Ahmed Abdelkader</a>, <a href="http://arxiv.org/find/cs/1/au:+Arya_S/0/1/0/all/0/1">Sunil Arya</a>, <a href="http://arxiv.org/find/cs/1/au:+Fonseca_G/0/1/0/all/0/1">Guilherme D. da Fonseca</a>, <a href="http://arxiv.org/find/cs/1/au:+Mount_D/0/1/0/all/0/1">David M. Mount</a></p><p>We present a new approach to approximate nearest-neighbor queries in fixed
dimension under a variety of non-Euclidean distances. We are given a set $S$ of
$n$ points in $\mathbb{R}^d$, an approximation parameter $\varepsilon &gt; 0$, and
a distance function that satisfies certain smoothness and growth-rate
assumptions. The objective is to preprocess $S$ into a data structure so that
for any query point $q$ in $\mathbb{R}^d$, it is possible to efficiently report
any point of $S$ whose distance from $q$ is within a factor of $1+\varepsilon$
of the actual closest point.
</p>
<p>Prior to this work, the most efficient data structures for approximate
nearest-neighbor searching in spaces of constant dimensionality applied only to
the Euclidean metric. This paper overcomes this limitation through a method
called convexification. For admissible distance functions, the proposed data
structures answer queries in logarithmic time using $O(n \log (1 / \varepsilon)
/ \varepsilon^{d/2})$ space, nearly matching the best known bounds for the
Euclidean metric. These results apply to both convex scaling distance functions
(including the Mahalanobis distance and weighted Minkowski metrics) and Bregman
divergences (including the Kullback-Leibler divergence and the Itakura-Saito
distance).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-28T00:30:00Z">Wednesday, June 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15648'>Optimal Area-Sensitive Bounds for Polytope Approximation</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sunil Arya, Guilherme D. da Fonseca, David M. Mount</p><p>Approximating convex bodies is a fundamental question in geometry and has a
wide variety of applications. Given a convex body $K$ of diameter $\Delta$ in
$\mathbb{R}^d$ for fixed $d$, the objective is to minimize the number of
vertices (alternatively, the number of facets) of an approximating polytope for
a given Hausdorff error $\varepsilon$. The best known uniform bound, due to
Dudley (1974), shows that $O((\Delta/\varepsilon)^{(d-1)/2})$ facets suffice.
While this bound is optimal in the case of a Euclidean ball, it is far from
optimal for ``skinny'' convex bodies.
</p>
<p>A natural way to characterize a convex object's skinniness is in terms of its
relationship to the Euclidean ball. Given a convex body $K$, define its surface
diameter $\Delta_{d-1}$ to be the diameter of a Euclidean ball of the same
surface area as $K$. It follows from generalizations of the isoperimetric
inequality that $\Delta \geq \Delta_{d-1}$.
</p>
<p>We show that, under the assumption that the width of the body in any
direction is at least $\varepsilon$, it is possible to approximate a convex
body using $O((\Delta_{d-1}/\varepsilon)^{(d-1)/2})$ facets. This bound is
never worse than the previous bound and may be significantly better for skinny
bodies. The bound is tight, in the sense that for any value of $\Delta_{d-1}$,
there exist convex bodies that, up to constant factors, require this many
facets.
</p>
<p>The improvement arises from a novel approach to sampling points on the
boundary of a convex body. We employ a classical concept from convexity, called
Macbeath regions. We demonstrate that Macbeath regions in $K$ and $K$'s polar
behave much like polar pairs. We then apply known results on the Mahler volume
to bound their number.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Arya_S/0/1/0/all/0/1">Sunil Arya</a>, <a href="http://arxiv.org/find/cs/1/au:+Fonseca_G/0/1/0/all/0/1">Guilherme D. da Fonseca</a>, <a href="http://arxiv.org/find/cs/1/au:+Mount_D/0/1/0/all/0/1">David M. Mount</a></p><p>Approximating convex bodies is a fundamental question in geometry and has a
wide variety of applications. Given a convex body $K$ of diameter $\Delta$ in
$\mathbb{R}^d$ for fixed $d$, the objective is to minimize the number of
vertices (alternatively, the number of facets) of an approximating polytope for
a given Hausdorff error $\varepsilon$. The best known uniform bound, due to
Dudley (1974), shows that $O((\Delta/\varepsilon)^{(d-1)/2})$ facets suffice.
While this bound is optimal in the case of a Euclidean ball, it is far from
optimal for ``skinny'' convex bodies.
</p>
<p>A natural way to characterize a convex object's skinniness is in terms of its
relationship to the Euclidean ball. Given a convex body $K$, define its surface
diameter $\Delta_{d-1}$ to be the diameter of a Euclidean ball of the same
surface area as $K$. It follows from generalizations of the isoperimetric
inequality that $\Delta \geq \Delta_{d-1}$.
</p>
<p>We show that, under the assumption that the width of the body in any
direction is at least $\varepsilon$, it is possible to approximate a convex
body using $O((\Delta_{d-1}/\varepsilon)^{(d-1)/2})$ facets. This bound is
never worse than the previous bound and may be significantly better for skinny
bodies. The bound is tight, in the sense that for any value of $\Delta_{d-1}$,
there exist convex bodies that, up to constant factors, require this many
facets.
</p>
<p>The improvement arises from a novel approach to sampling points on the
boundary of a convex body. We employ a classical concept from convexity, called
Macbeath regions. We demonstrate that Macbeath regions in $K$ and $K$'s polar
behave much like polar pairs. We then apply known results on the Mahler volume
to bound their number.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-28T00:30:00Z">Wednesday, June 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15331'>Optimally Repurposing Existing Algorithms to Obtain Exponential-Time Approximations</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Bar&#x131;&#x15f; Can Esmer, Ariel Kulik, D&#xe1;niel Marx, Daniel Neuen, Roohani Sharma</p><p>The goal of this paper is to understand how exponential-time approximation
algorithms can be obtained from existing polynomial-time approximation
algorithms, existing parameterized exact algorithms, and existing parameterized
approximation algorithms. More formally, we consider a monotone subset
minimization problem over a universe of size $n$ (e.g., Vertex Cover or
Feedback Vertex Set). We have access to an algorithm that finds an
$\alpha$-approximate solution in time $c^k \cdot n^{O(1)}$ if a solution of
size $k$ exists (and more generally, an extension algorithm that can
approximate in a similar way if a set can be extended to a solution with $k$
further elements). Our goal is to obtain a $d^n \cdot n^{O(1)}$ time
$\beta$-approximation algorithm for the problem with $d$ as small as possible.
That is, for every fixed $\alpha,c,\beta \geq 1$, we would like to determine
the smallest possible $d$ that can be achieved in a model where our
problem-specific knowledge is limited to checking the feasibility of a solution
and invoking the $\alpha$-approximate extension algorithm. Our results
completely resolve this question:
</p>
<p>(1) For every fixed $\alpha,c,\beta \geq 1$, a simple algorithm
(``approximate monotone local search'') achieves the optimum value of $d$.
</p>
<p>(2) Given $\alpha,c,\beta \geq 1$, we can efficiently compute the optimum $d$
up to any precision $\varepsilon &gt; 0$.
</p>
<p>Earlier work presented algorithms (but no lower bounds) for the special case
$\alpha = \beta = 1$ [Fomin et al., J. ACM 2019] and for the special case
$\alpha = \beta &gt; 1$ [Esmer et al., ESA 2022]. Our work generalizes these
results and in particular confirms that the earlier algorithms are optimal in
these special cases.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Esmer_B/0/1/0/all/0/1">Bar&#x131;&#x15f; Can Esmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulik_A/0/1/0/all/0/1">Ariel Kulik</a>, <a href="http://arxiv.org/find/cs/1/au:+Marx_D/0/1/0/all/0/1">D&#xe1;niel Marx</a>, <a href="http://arxiv.org/find/cs/1/au:+Neuen_D/0/1/0/all/0/1">Daniel Neuen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1">Roohani Sharma</a></p><p>The goal of this paper is to understand how exponential-time approximation
algorithms can be obtained from existing polynomial-time approximation
algorithms, existing parameterized exact algorithms, and existing parameterized
approximation algorithms. More formally, we consider a monotone subset
minimization problem over a universe of size $n$ (e.g., Vertex Cover or
Feedback Vertex Set). We have access to an algorithm that finds an
$\alpha$-approximate solution in time $c^k \cdot n^{O(1)}$ if a solution of
size $k$ exists (and more generally, an extension algorithm that can
approximate in a similar way if a set can be extended to a solution with $k$
further elements). Our goal is to obtain a $d^n \cdot n^{O(1)}$ time
$\beta$-approximation algorithm for the problem with $d$ as small as possible.
That is, for every fixed $\alpha,c,\beta \geq 1$, we would like to determine
the smallest possible $d$ that can be achieved in a model where our
problem-specific knowledge is limited to checking the feasibility of a solution
and invoking the $\alpha$-approximate extension algorithm. Our results
completely resolve this question:
</p>
<p>(1) For every fixed $\alpha,c,\beta \geq 1$, a simple algorithm
(``approximate monotone local search'') achieves the optimum value of $d$.
</p>
<p>(2) Given $\alpha,c,\beta \geq 1$, we can efficiently compute the optimum $d$
up to any precision $\varepsilon &gt; 0$.
</p>
<p>Earlier work presented algorithms (but no lower bounds) for the special case
$\alpha = \beta = 1$ [Fomin et al., J. ACM 2019] and for the special case
$\alpha = \beta &gt; 1$ [Esmer et al., ESA 2022]. Our work generalizes these
results and in particular confirms that the earlier algorithms are optimal in
these special cases.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-28T00:30:00Z">Wednesday, June 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15040'>Robust and Space-Efficient Dual Adversary Quantum Query Algorithms</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Michael Czekanski, Shelby Kimmel, R. Teal Witter</p><p>The general adversary dual is a powerful tool in quantum computing because it
gives a query-optimal bounded-error quantum algorithm for deciding any Boolean
function. Unfortunately, the algorithm uses linear qubits in the worst case,
and only works if the constraints of the general adversary dual are exactly
satisfied. The challenge of improving the algorithm is that it is brittle to
arbitrarily small errors since it relies on a reflection over a span of
vectors. We overcome this challenge and build a robust dual adversary algorithm
that can handle approximately satisfied constraints. As one application of our
robust algorithm, we prove that for any Boolean function with polynomially many
1-valued inputs (or in fact a slightly weaker condition) there is a
query-optimal algorithm that uses logarithmic qubits. As another application,
we prove that numerically derived, approximate solutions to the general
adversary dual give a bounded-error quantum algorithm under certain conditions.
Further, we show that these conditions empirically hold with reasonable
iterations for Boolean functions with small domains. We also develop several
tools that may be of independent interest, including a robust approximate
spectral gap lemma, a method to compress a general adversary dual solution
using the Johnson-Lindenstrauss lemma, and open-source code to find solutions
to the general adversary dual.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Czekanski_M/0/1/0/all/0/1">Michael Czekanski</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Kimmel_S/0/1/0/all/0/1">Shelby Kimmel</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Witter_R/0/1/0/all/0/1">R. Teal Witter</a></p><p>The general adversary dual is a powerful tool in quantum computing because it
gives a query-optimal bounded-error quantum algorithm for deciding any Boolean
function. Unfortunately, the algorithm uses linear qubits in the worst case,
and only works if the constraints of the general adversary dual are exactly
satisfied. The challenge of improving the algorithm is that it is brittle to
arbitrarily small errors since it relies on a reflection over a span of
vectors. We overcome this challenge and build a robust dual adversary algorithm
that can handle approximately satisfied constraints. As one application of our
robust algorithm, we prove that for any Boolean function with polynomially many
1-valued inputs (or in fact a slightly weaker condition) there is a
query-optimal algorithm that uses logarithmic qubits. As another application,
we prove that numerically derived, approximate solutions to the general
adversary dual give a bounded-error quantum algorithm under certain conditions.
Further, we show that these conditions empirically hold with reasonable
iterations for Boolean functions with small domains. We also develop several
tools that may be of independent interest, including a robust approximate
spectral gap lemma, a method to compress a general adversary dual solution
using the Johnson-Lindenstrauss lemma, and open-source code to find solutions
to the general adversary dual.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-28T00:30:00Z">Wednesday, June 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15295'>Synthesis of Quantum Vector Databases Based on Grovers Algorithm</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Cesar Borisovich Pronin, Andrey Vladimirovich Ostroukh</p><p>This paper describes a method for using Grovers algorithm to create a quantum
vector database, the database stores embeddings based on Controlled-S gates,
which represent a binary numerical value. This value represents the embeddings
value. The process of creating meaningful embeddings is handled by a classical
computer and the search process is handled by the quantum computer. This search
approach might be beneficial for a large enough database, or it could be seen
as a very qubit-efficient (super dense) way for storing data on a quantum
computer, since the proposed circuit stores many embeddings inside one quantum
register simultaneously.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Pronin_C/0/1/0/all/0/1">Cesar Borisovich Pronin</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Ostroukh_A/0/1/0/all/0/1">Andrey Vladimirovich Ostroukh</a></p><p>This paper describes a method for using Grovers algorithm to create a quantum
vector database, the database stores embeddings based on Controlled-S gates,
which represent a binary numerical value. This value represents the embeddings
value. The process of creating meaningful embeddings is handled by a classical
computer and the search process is handled by the quantum computer. This search
approach might be beneficial for a large enough database, or it could be seen
as a very qubit-efficient (super dense) way for storing data on a quantum
computer, since the proposed circuit stores many embeddings inside one quantum
register simultaneously.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-28T00:30:00Z">Wednesday, June 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15395'>On the Deque and Rique Numbers of Complete and Complete Bipartite Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Michael A. Bekos, Michael Kaufmann, Maria Eleni Pavlidi, Xenia Rieger</p><p>Several types of linear layouts of graphs are obtained by leveraging known
data structures; the most notable representatives are the stack and the queue
layouts. In this content, given a data structure, one seeks to specify an order
of the vertices of the graph and a partition of its edges into pages, such that
the endpoints of the edges assigned to each page can be processed by the given
data structure in the underlying order. In this paper, we study deque and rique
layouts of graphs obtained by leveraging the double-ended queue and the
restricted-input double-ended queue (or deque and rique, for short),
respectively. Hence, they generalize both the stack and the queue layouts. We
focus on complete and complete bipartite graphs and present bounds on their
deque- and rique-numbers, that is, on the minimum number of pages needed by any
of these two types of linear layouts.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bekos_M/0/1/0/all/0/1">Michael A. Bekos</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaufmann_M/0/1/0/all/0/1">Michael Kaufmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlidi_M/0/1/0/all/0/1">Maria Eleni Pavlidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rieger_X/0/1/0/all/0/1">Xenia Rieger</a></p><p>Several types of linear layouts of graphs are obtained by leveraging known
data structures; the most notable representatives are the stack and the queue
layouts. In this content, given a data structure, one seeks to specify an order
of the vertices of the graph and a partition of its edges into pages, such that
the endpoints of the edges assigned to each page can be processed by the given
data structure in the underlying order. In this paper, we study deque and rique
layouts of graphs obtained by leveraging the double-ended queue and the
restricted-input double-ended queue (or deque and rique, for short),
respectively. Hence, they generalize both the stack and the queue layouts. We
focus on complete and complete bipartite graphs and present bounds on their
deque- and rique-numbers, that is, on the minimum number of pages needed by any
of these two types of linear layouts.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-28T00:30:00Z">Wednesday, June 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15597'>Scheduling with a Limited Testing Budget</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Christoph Damerius, Peter Kling, Minming Li, Chenyang Xu, Ruilong Zhang</p><p>Scheduling with testing falls under the umbrella of the research on
optimization with explorable uncertainty. In this model, each job has an upper
limit on its processing time that can be decreased to a lower limit (possibly
unknown) by some preliminary action (testing). Recently, D{\"{u}}rr et al.
\cite{DBLP:journals/algorithmica/DurrEMM20} has studied a setting where testing
a job takes a unit time, and the goal is to minimize total completion time or
makespan on a single machine. In this paper, we extend their problem to the
budget setting in which each test consumes a job-specific cost, and we require
that the total testing cost cannot exceed a given budget. We consider the
offline variant (the lower processing time is known) and the oblivious variant
(the lower processing time is unknown) and aim to minimize the total completion
time or makespan on a single machine.
</p>
<p>For the total completion time objective, we show NP-hardness and derive a
PTAS for the offline variant based on a novel LP rounding scheme. We give a
$(4+\epsilon)$-competitive algorithm for the oblivious variant based on a
framework inspired by the worst-case lower-bound instance. For the makespan
objective, we give an FPTAS for the offline variant and a
$(2+\epsilon)$-competitive algorithm for the oblivious variant. Our algorithms
for the oblivious variants under both objectives run in time
$O(poly(n/\epsilon))$. Lastly, we show that our results are essentially optimal
by providing matching lower bounds.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Damerius_C/0/1/0/all/0/1">Christoph Damerius</a>, <a href="http://arxiv.org/find/cs/1/au:+Kling_P/0/1/0/all/0/1">Peter Kling</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Minming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chenyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruilong Zhang</a></p><p>Scheduling with testing falls under the umbrella of the research on
optimization with explorable uncertainty. In this model, each job has an upper
limit on its processing time that can be decreased to a lower limit (possibly
unknown) by some preliminary action (testing). Recently, D{\"{u}}rr et al.
\cite{DBLP:journals/algorithmica/DurrEMM20} has studied a setting where testing
a job takes a unit time, and the goal is to minimize total completion time or
makespan on a single machine. In this paper, we extend their problem to the
budget setting in which each test consumes a job-specific cost, and we require
that the total testing cost cannot exceed a given budget. We consider the
offline variant (the lower processing time is known) and the oblivious variant
(the lower processing time is unknown) and aim to minimize the total completion
time or makespan on a single machine.
</p>
<p>For the total completion time objective, we show NP-hardness and derive a
PTAS for the offline variant based on a novel LP rounding scheme. We give a
$(4+\epsilon)$-competitive algorithm for the oblivious variant based on a
framework inspired by the worst-case lower-bound instance. For the makespan
objective, we give an FPTAS for the offline variant and a
$(2+\epsilon)$-competitive algorithm for the oblivious variant. Our algorithms
for the oblivious variants under both objectives run in time
$O(poly(n/\epsilon))$. Lastly, we show that our results are essentially optimal
by providing matching lower bounds.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-28T00:30:00Z">Wednesday, June 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.15632'>Asynchronous Algorithmic Alignment with Cocycles</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Andrew Dudzik, Tamara von Glehn, Razvan Pascanu, Petar Veli&#x10d;kovi&#x107;</p><p>State-of-the-art neural algorithmic reasoners make use of message passing in
graph neural networks (GNNs). But typical GNNs blur the distinction between the
definition and invocation of the message function, forcing a node to send
messages to its neighbours at every layer, synchronously. When applying GNNs to
learn to execute dynamic programming algorithms, however, on most steps only a
handful of the nodes would have meaningful updates to send. One, hence, runs
the risk of inefficiencies by sending too much irrelevant data across the graph
-- with many intermediate GNN steps having to learn identity functions. In this
work, we explicitly separate the concepts of node state update and message
function invocation. With this separation, we obtain a mathematical formulation
that allows us to reason about asynchronous computation in both algorithms and
neural networks.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dudzik_A/0/1/0/all/0/1">Andrew Dudzik</a>, <a href="http://arxiv.org/find/cs/1/au:+Glehn_T/0/1/0/all/0/1">Tamara von Glehn</a>, <a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1">Razvan Pascanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1">Petar Veli&#x10d;kovi&#x107;</a></p><p>State-of-the-art neural algorithmic reasoners make use of message passing in
graph neural networks (GNNs). But typical GNNs blur the distinction between the
definition and invocation of the message function, forcing a node to send
messages to its neighbours at every layer, synchronously. When applying GNNs to
learn to execute dynamic programming algorithms, however, on most steps only a
handful of the nodes would have meaningful updates to send. One, hence, runs
the risk of inefficiencies by sending too much irrelevant data across the graph
-- with many intermediate GNN steps having to learn identity functions. In this
work, we explicitly separate the concepts of node state update and message
function invocation. With this separation, we obtain a mathematical formulation
that allows us to reason about asynchronous computation in both algorithms and
neural networks.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-28T00:30:00Z">Wednesday, June 28 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://differentialprivacy.org/tpdp2023/'>Call for Papers - TPDP 2023 - Submission deadline July 7</a></h3>
        <p class='tr-article-feed'>from <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>The 9th Workshop on the Theory and Practice of Differential Privacy (TPDP 2023) will take place in Boston September 27-28, 2023.
This is the first year the workshop is a standalone event. However, the OpenDP community meeting is the following day (also in Boston). It is also moving from a one-day event to two days.</p>

<p>The workshop is intended to bring together the DP research community to discuss new developments over the past year. The workshop is non-archival, so does not preclude publishing the work elsewhere.</p>

<p>The submission deadline is July 7. Submissions should be 4 pages (plus references and appendices.)</p>

<p>Submission website: TBD</p><p>By </p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The <a href="https://tpdp.journalprivacyconfidentiality.org/2023/">9th Workshop on the Theory and Practice of Differential Privacy (TPDP 2023)</a> will take place in Boston September 27-28, 2023.
This is the first year the workshop is a standalone event. However, the <a href="https://opendp.org/event/opendp-community-meeting-2023">OpenDP community meeting</a> is the following day (also in Boston). It is also moving from a one-day event to two days.</p>

<p>The workshop is intended to bring together the DP research community to discuss new developments over the past year. The workshop is non-archival, so does not preclude publishing the work elsewhere.</p>

<p>The submission deadline is July 7. Submissions should be 4 pages (plus references and appendices.)</p>

<p>Submission website: TBD</p><p class="authors">By </p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-28T00:01:00Z">Wednesday, June 28 2023, 00:01</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Tuesday, June 27
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://differentialprivacy.org/open-problem-better-privacy-guarantees-for-larger-groups/'>Open problem - Better privacy guarantees for larger groups</a></h3>
        <p class='tr-article-feed'>from <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Consider a simple query counting the number of people in various mutually exclusive groups.
In the differential privacy literature, it is typical to assume that each of these groups should be subject to the same privacy loss: the noise added to each count has the same magnitude, and everyone gets the same privacy guarantees.
However, in settings where these groups have vastly different population sizes, larger populations may be willing to accept more error in exchange for stronger privacy protections.
In particular, in many use cases, relative error (the noisy count is within 5% of the true value) matters more than absolute error (the noisy count is at a distance of at most 100 of the true value).
This leads to a natural question: can we use this fact to develop a mechanism that improves the privacy guarantees of individuals in larger groups, subject to a constraint on relative error?</p>

Problem definition

<p>Our goal is to obtain a mechanism which minimizes the overall privacy loss for each group without exceeding a relative error threshold for each group.
To formalize this goal, we first define a notion of per-group privacy we call group-wise zero-concentrated differential privacy as follows.</p>

<p>Definition. Group-wise zero-concentrated differential privacy.
Assume possible datasets consist of records from domain \(U\), and \(U\) can be partitioned into \(k\) fixed, disjoint groups \(U_1\), …, \(U_k\). Let \(v : \mathcal{D} \rightarrow \mathbb{R}^k\) be a function associating a dataset to a vector of privacy budgets (one per group). We say a mechanism \(\mathcal{M}\) satisfies \(v\)-group-wise zero-concentrated differential privacy (zCDP) if for any two datasets \(D\), \(D’\) differing in the addition or removal of a record in \(U_i\), and for all \(\alpha&gt;1\), we have:
\[
D_\alpha\left(\mathcal{M}(D||\mathcal{M}(D’)\right) \le \alpha \cdot {v(D)}_i
\]
\[
D_\alpha\left(\mathcal{M}(D’)||\mathcal{M}(D)\right) \le \alpha \cdot {v(D)}_i
\]
where \(D_\alpha\) is the Rényi divergence of order \(\alpha\).</p>

<p>This definition is similar to tailored DP, defined in [LP15]: each individual gets a different privacy guarantee, depending on which group they belong to;
this guarantee also depends on how many people are in this group.
We use zCDP as our definition of privacy due to its compatibility with the Gaussian mechanism; the same idea could easily be applied to other definitions like with Rényi DP or pure DP.</p>

<p>From there we can give a more formal definition of the problem as follows. The goal is to minimize the privacy loss for each individual group, while keeping the error under a given threshold.
For larger groups that can accept more noise, this means adding more noise to achieve the smallest possible privacy loss.</p>

<p>Problem.
Let \(r \in (0,1]\) be an acceptable level of relative error, and \(k\) be the number of distinct, mutually-exclusive partitions of domain \(X\).
Given a dataset \(D\), let \(x(D)\) be a vector containing the count of records in each partition.
The objective is to find a mechanism \(\mathcal{M}\) which takes in \(r\), \(k\), and \(D\) and outputs \(\hat{x}(D)\) such that \(E\left[\left|{x(D)}_i-{\hat{x}(D)}_i\right|\right]&lt;r\cdot {x(D)}_i\) for all \(i\), and satisfies \(v\)-group-wise zCDP where \(v(D)_i\) is as small as possible for all \(i\).
<br>
To prevent pathological mechanisms that optimize for specific datasets, we add two constraints to the problem: the privacy guarantee \(v(D)_i\) should only depend on \(x(D)_i\), and should be nonincreasing with \(x(D)_i\).</p>

<p>Since the relative error thresholds are proportional to the population size, each population can tolerate a different amount of noise.
This means that to minimize the privacy loss for each group, the mechanism must add noise of different scales to each group.
Of course, directly using \(x(D)_i\) to determine the scale of the noise for group \(i\) leads to a privacy loss which is data dependent, similarly to e.g. PATE [PAEGT17], and as such should be treated as a protected value.</p>

An example mechanism

<p>An example mechanism that seems like it could address this problem is as follows.
First, perform the original counting query and add Gaussian noise to satisfy \(\rho\)-zCDP.
Then, add additional Gaussian noise to each count, with a variance that depends on the noisy count itself — adding more noise to larger groups.
This mechanism is outlined in Algorithm 1.</p>

<p>Algorithm 1.
Adding data-dependent noise as a post-processing step.
<br>
Require: A dataset \(D\) where each data point belongs to one of \(k\) groups, a privacy parameter \(\rho\), and a relative error rate \(r\).</p>
<ol>
  <li>Let \(\sigma^2 = 1/(2\rho)\)</li>
  <li>For \(i=1\) to \(k\) do</li>
  <li>\(\qquad\) Let \(x_i\) be the number of people in \(D\) in group \(i\)</li>
  <li>\(\qquad\) Sample \(X_i \sim \mathcal{N}(x_i, \sigma^2)\)</li>
  <li>\(\qquad\) Sample \(Y_i \sim \mathcal{N}_{k}(X_i, (rX_i)^2)\)</li>
  <li>end for</li>
  <li>return \(Y_1,\dots,Y_k\)</li>
</ol>

<p>Algorithm 1 achieves this goal of having approximately \(r\) error in each group: the total variance error of the mechanism is \(\sigma^2 + (rX)^2\), and \(X\) is a zCDP measure of \(f(D)\).
This mechanism satisfies at least \(\rho\)-zCDP: line 4 is an invocation of the Gaussian mechanism with privacy parameter \(\rho\), and line 5 is a post processing step and as such preserves the zCDP guarantee.
We would like to show that this algorithm also satisfies a stronger group-wise zCDP guarantee.</p>

<p>This makes intuitive sense: line 5 adds additional Gaussian noise without using the private data directly.
Since the noise scale in line 5 is proportional to the total count in line 4, we expect the privacy guarantee to be significantly stronger for large groups with more noise.
Further, we can verify experimentally that when the data magnitude is large compared to the noise, the output distribution for each group is close to a Gaussian distribution.</p>

<p>The below figure illustrates this finding.
We plot 1,000,000 sample outputs of Algorithm 1 (red) with parameters \(\sigma^2 = 100\) and \(r= 0.3\), and compare it to the best fit Gaussian distribution (black outline) with mean \(10,002.6\) and standard deviation of \(2995.1\).</p>

<p>♦</p>

<p>With parameters such as these, the output of the mechanism looks and behaves like a Gaussian distribution, which should be ideal to characterize the zCDP guarantee.
However, it is difficult to directly quantify this guarantee, due to the changing variance which is also a random variable.
Likewise, if the true count is close to zero or if the first instance of noise is large compared to the true count than the resulting distribution takes on a heavy skew and is no longer similar to a single Gaussian distribution.
Such distributions with randomized variances have not, to the best of our knowledge, been considered much in the literature, and we do not know whether the mechanism’s output distribution follows some well-studied distribution.</p>

<p>The randomized variance also makes it difficult to bound the Rényi divergence of the distribution and characterize the zCDP guarantees directly.
Current privacy amplification techniques are insufficient, as those techniques consider adding additional noise where the noise parameters are independent of the data itself.</p>

<p>Perhaps the most promising direction to understand more about such processes is the area of stochastic differential equations, where it is common to study noise with data-dependent variance.
The Bessel process [Øks03] is an example of such a process, where the noise is dependent on the current value.
This process captures the noise added as post-processing (Line 5), but not the initial noise-addition step (Line 4).
Furthermore, to the best of our knowledge, the Bessel process and other value-dependent stochastic differential equations do not have closed-form solutions.</p>

Goal

<p>We see two possible paths forward to address the original question. One path would be to obtain an analysis of Algorithm 1 which shows non-trivial improved privacy guarantees for larger groups.
We tried multiple approaches, but could not prove such a result.</p>

<p>An alternative path would be to develop a different algorithm, which achieves better privacy guarantees for larger groups while maintaining the error below the relative error threshold for all groups.</p><p>By </p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Consider a simple query counting the number of people in various mutually exclusive groups.
In the differential privacy literature, it is typical to assume that each of these groups should be subject to the same privacy loss: the noise added to each count has the same magnitude, and everyone gets the same privacy guarantees.
However, in settings where these groups have vastly different population sizes, larger populations may be willing to accept more error in exchange for stronger privacy protections.
In particular, in many use cases, <em>relative</em> error (the noisy count is within 5% of the true value) matters more than absolute error (the noisy count is at a distance of at most 100 of the true value).
This leads to a natural question: can we use this fact to develop a mechanism that improves the privacy guarantees of individuals in larger groups, subject to a constraint on relative error?</p>

<h3 id="problem-definition">Problem definition</h3>

<p>Our goal is to obtain a mechanism which minimizes the overall privacy loss for each group without exceeding a relative error threshold for each group.
To formalize this goal, we first define a notion of per-group privacy we call group-wise zero-concentrated differential privacy as follows.</p>

<p><strong>Definition.</strong> <em>Group-wise zero-concentrated differential privacy.</em>
Assume possible datasets consist of records from domain \(U\), and \(U\) can be partitioned into \(k\) fixed, disjoint groups \(U_1\), …, \(U_k\). Let \(v : \mathcal{D} \rightarrow \mathbb{R}^k\) be a function associating a dataset to a vector of privacy budgets (one per group). We say a mechanism \(\mathcal{M}\) satisfies \(v\)-group-wise zero-concentrated differential privacy (zCDP) if for any two datasets \(D\), \(D’\) differing in the addition or removal of a record in \(U_i\), and for all \(\alpha&gt;1\), we have:
\[
D_\alpha\left(\mathcal{M}(D||\mathcal{M}(D’)\right) \le \alpha \cdot {v(D)}_i
\]
\[
D_\alpha\left(\mathcal{M}(D’)||\mathcal{M}(D)\right) \le \alpha \cdot {v(D)}_i
\]
where \(D_\alpha\) is the Rényi divergence of order \(\alpha\).</p>

<p>This definition is similar to <em>tailored DP</em>, defined in [<a href="https://eprint.iacr.org/2014/982.pdf">LP15</a>]: each individual gets a different privacy guarantee, depending on which group they belong to;
this guarantee also depends on how many people are in this group.
We use zCDP as our definition of privacy due to its compatibility with the Gaussian mechanism; the same idea could easily be applied to other definitions like with Rényi DP or pure DP.</p>

<p>From there we can give a more formal definition of the problem as follows. The goal is to minimize the privacy loss for each individual group, while keeping the error under a given threshold.
For larger groups that can accept more noise, this means adding more noise to achieve the smallest possible privacy loss.</p>

<p><strong>Problem.</strong>
Let \(r \in (0,1]\) be an acceptable level of relative error, and \(k\) be the number of distinct, mutually-exclusive partitions of domain \(X\).
Given a dataset \(D\), let \(x(D)\) be a vector containing the count of records in each partition.
The objective is to find a mechanism \(\mathcal{M}\) which takes in \(r\), \(k\), and \(D\) and outputs \(\hat{x}(D)\) such that \(E\left[\left|{x(D)}_i-{\hat{x}(D)}_i\right|\right]&lt;r\cdot {x(D)}_i\) for all \(i\), and satisfies \(v\)-group-wise zCDP where \(v(D)_i\) is as small as possible for all \(i\).
<br />
To prevent pathological mechanisms that optimize for specific datasets, we add two constraints to the problem: the privacy guarantee \(v(D)_i\) should only depend on \(x(D)_i\), and should be nonincreasing with \(x(D)_i\).</p>

<p>Since the relative error thresholds are proportional to the population size, each population can tolerate a different amount of noise.
This means that to minimize the privacy loss for each group, the mechanism must add noise of different scales to each group.
Of course, directly using \(x(D)_i\) to determine the scale of the noise for group \(i\) leads to a privacy loss which is data dependent, similarly to e.g. PATE [<a href="https://openreview.net/forum?id=HkwoSDPg">PAEGT17</a>], and as such should be treated as a protected value.</p>

<h3 id="an-example-mechanism">An example mechanism</h3>

<p>An example mechanism that seems like it could address this problem is as follows.
First, perform the original counting query and add Gaussian noise to satisfy \(\rho\)-zCDP.
Then, add additional Gaussian noise to each count, with a variance that depends on the noisy count itself — adding more noise to larger groups.
This mechanism is outlined in Algorithm 1.</p>

<p><strong>Algorithm 1.</strong>
<em>Adding data-dependent noise as a post-processing step.</em>
<br />
Require: A dataset \(D\) where each data point belongs to one of \(k\) groups, a privacy parameter \(\rho\), and a relative error rate \(r\).</p>
<ol>
  <li>Let \(\sigma^2 = 1/(2\rho)\)</li>
  <li><strong>For</strong> \(i=1\) to \(k\) <strong>do</strong></li>
  <li>\(\qquad\) Let \(x_i\) be the number of people in \(D\) in group \(i\)</li>
  <li>\(\qquad\) Sample \(X_i \sim \mathcal{N}(x_i, \sigma^2)\)</li>
  <li>\(\qquad\) Sample \(Y_i \sim \mathcal{N}_{k}(X_i, (rX_i)^2)\)</li>
  <li><strong>end for</strong></li>
  <li><strong>return</strong> \(Y_1,\dots,Y_k\)</li>
</ol>

<p>Algorithm 1 achieves this goal of having approximately \(r\) error in each group: the total variance error of the mechanism is \(\sigma^2 + (rX)^2\), and \(X\) is a zCDP measure of \(f(D)\).
This mechanism satisfies at least \(\rho\)-zCDP: line 4 is an invocation of the Gaussian mechanism with privacy parameter \(\rho\), and line 5 is a post processing step and as such preserves the zCDP guarantee.
We would like to show that this algorithm also satisfies a stronger group-wise zCDP guarantee.</p>

<p>This makes intuitive sense: line 5 adds additional Gaussian noise without using the private data directly.
Since the noise scale in line 5 is proportional to the total count in line 4, we expect the privacy guarantee to be significantly stronger for large groups with more noise.
Further, we can verify experimentally that when the data magnitude is large compared to the noise, the output distribution for each group is close to a Gaussian distribution.</p>

<p>The below figure illustrates this finding.
We plot 1,000,000 sample outputs of Algorithm 1 (red) with parameters \(\sigma^2 = 100\) and \(r= 0.3\), and compare it to the best fit Gaussian distribution (black outline) with mean \(10,002.6\) and standard deviation of \(2995.1\).</p>

<p><img src="../images/two-stage-noise-gaussian.png" width="70%" alt="A comparison between sample outputs of Algorithm 1 and the best-fit Gaussian distribution, showing that both match very closely." style="margin:auto;display: block;" /></p>

<p>With parameters such as these, the output of the mechanism looks and behaves like a Gaussian distribution, which should be ideal to characterize the zCDP guarantee.
However, it is difficult to directly quantify this guarantee, due to the changing variance which is also a random variable.
Likewise, if the true count is close to zero or if the first instance of noise is large compared to the true count than the resulting distribution takes on a heavy skew and is no longer similar to a single Gaussian distribution.
Such distributions with randomized variances have not, to the best of our knowledge, been considered much in the literature, and we do not know whether the mechanism’s output distribution follows some well-studied distribution.</p>

<p>The randomized variance also makes it difficult to bound the Rényi divergence of the distribution and characterize the zCDP guarantees directly.
Current privacy amplification techniques are insufficient, as those techniques consider adding additional noise where the noise parameters are independent of the data itself.</p>

<p>Perhaps the most promising direction to understand more about such processes is the area of stochastic differential equations, where it is common to study noise with data-dependent variance.
The Bessel process [<a href="http://www.stat.ucla.edu/~ywu/research/documents/StochasticDifferentialEquations.pdf">Øks03</a>] is an example of such a process, where the noise is dependent on the current value.
This process captures the noise added as post-processing (Line 5), but not the initial noise-addition step (Line 4).
Furthermore, to the best of our knowledge, the Bessel process and other value-dependent stochastic differential equations do not have closed-form solutions.</p>

<h3 id="goal">Goal</h3>

<p>We see two possible paths forward to address the original question. One path would be to obtain an analysis of Algorithm 1 which shows non-trivial improved privacy guarantees for larger groups.
We tried multiple approaches, but could not prove such a result.</p>

<p>An alternative path would be to develop a different algorithm, which achieves better privacy guarantees for larger groups while maintaining the error below the relative error threshold for all groups.</p><p class="authors">By </p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-27T01:00:00Z">Tuesday, June 27 2023, 01:00</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13903'>On the local consequence of modal Product logic: standard completeness and decidability</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Amanda Vidal</p><p>Modal extensions of Product fuzzy logic can be considered both over Kripke
models whose accessibility relation is valued, and over Kripke models with
classical accessibility relation. We study the local consequence of the
previous two modal Product logics. We prove that these logics are standard
complete, in the sense that the logic defined over Kripke models evaluated over
all product algebras coincides with that defined over Kripke models evaluated
over the standard product algebra (with universe [0,1]). This holds both for
the logic over classical Kripke frames, and for that over frames with a valued
accessibility relation. Second, we prove that the previous logics are
decidable.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Vidal_A/0/1/0/all/0/1">Amanda Vidal</a></p><p>Modal extensions of Product fuzzy logic can be considered both over Kripke
models whose accessibility relation is valued, and over Kripke models with
classical accessibility relation. We study the local consequence of the
previous two modal Product logics. We prove that these logics are standard
complete, in the sense that the logic defined over Kripke models evaluated over
all product algebras coincides with that defined over Kripke models evaluated
over the standard product algebra (with universe [0,1]). This holds both for
the logic over classical Kripke frames, and for that over frames with a valued
accessibility relation. Second, we prove that the previous logics are
decidable.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-27T00:30:00Z">Tuesday, June 27 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.14087'>A Circuit Complexity Formulation of Algorithmic Information Theory</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Cole Wyeth, Carl Sturtivant</p><p>Inspired by Solomonoffs theory of inductive inference, we propose a prior
based on circuit complexity. There are several advantages to this approach.
First, it relies on a complexity measure that does not depend on the choice of
UTM. There is one universal definition for Boolean circuits involving an
universal operation such as nand with simple conversions to alternative
definitions such as and, or, and not. Second, there is no analogue of the
halting problem. The output value of a circuit can be calculated recursively by
computer in time proportional to the number of gates, while a short program may
run for a very long time. Our prior assumes that a Boolean function, or
equivalently, Boolean string of fixed length, is generated by some Bayesian
mixture of circuits. This model is appropriate for learning Boolean functions
from partial information, a problem often encountered within machine learning
as "binary classification." We argue that an inductive bias towards simple
explanations as measured by circuit complexity is appropriate for this problem.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Wyeth_C/0/1/0/all/0/1">Cole Wyeth</a>, <a href="http://arxiv.org/find/cs/1/au:+Sturtivant_C/0/1/0/all/0/1">Carl Sturtivant</a></p><p>Inspired by Solomonoffs theory of inductive inference, we propose a prior
based on circuit complexity. There are several advantages to this approach.
First, it relies on a complexity measure that does not depend on the choice of
UTM. There is one universal definition for Boolean circuits involving an
universal operation such as nand with simple conversions to alternative
definitions such as and, or, and not. Second, there is no analogue of the
halting problem. The output value of a circuit can be calculated recursively by
computer in time proportional to the number of gates, while a short program may
run for a very long time. Our prior assumes that a Boolean function, or
equivalently, Boolean string of fixed length, is generated by some Bayesian
mixture of circuits. This model is appropriate for learning Boolean functions
from partial information, a problem often encountered within machine learning
as "binary classification." We argue that an inductive bias towards simple
explanations as measured by circuit complexity is appropriate for this problem.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-27T00:30:00Z">Tuesday, June 27 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13802'>Using persistent homology to understand dimensionality reduction in resting-state fMRI</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ty Easley, Kevin Freese, Elizabeth Munch, Janine Bijsterbosch</p><p>Evaluating the success of a manifold learning method remains a challenging
problem, especially for methods adapted to a specific application domain. The
present work investigates shared geometric structure across different
dimensionality reduction (DR) algorithms within the scope of neuroimaging
applications. We examine reduced-dimension embeddings produced by a
representative assay of dimension reductions for brain data ("brain
representations") through the lens of persistent homology, making statistical
claims about topological differences using a recent topological boostrap
method. We cluster these methods based on their induced topologies, finding
feature type and number -- rather than reduction algorithm -- as the main
drivers of observed topological differences.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Easley_T/0/1/0/all/0/1">Ty Easley</a>, <a href="http://arxiv.org/find/cs/1/au:+Freese_K/0/1/0/all/0/1">Kevin Freese</a>, <a href="http://arxiv.org/find/cs/1/au:+Munch_E/0/1/0/all/0/1">Elizabeth Munch</a>, <a href="http://arxiv.org/find/cs/1/au:+Bijsterbosch_J/0/1/0/all/0/1">Janine Bijsterbosch</a></p><p>Evaluating the success of a manifold learning method remains a challenging
problem, especially for methods adapted to a specific application domain. The
present work investigates shared geometric structure across different
dimensionality reduction (DR) algorithms within the scope of neuroimaging
applications. We examine reduced-dimension embeddings produced by a
representative assay of dimension reductions for brain data ("brain
representations") through the lens of persistent homology, making statistical
claims about topological differences using a recent topological boostrap
method. We cluster these methods based on their induced topologies, finding
feature type and number -- rather than reduction algorithm -- as the main
drivers of observed topological differences.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-27T00:30:00Z">Tuesday, June 27 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13824'>Adaptive Privacy Composition for Accuracy-first Mechanisms</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ryan Rogers, Gennady Samorodnitsky, Zhiwei Steven Wu, Aaditya Ramdas</p><p>In many practical applications of differential privacy, practitioners seek to
provide the best privacy guarantees subject to a target level of accuracy. A
recent line of work by \cite{LigettNeRoWaWu17, WhitehouseWuRaRo22} has
developed such accuracy-first mechanisms by leveraging the idea of \emph{noise
reduction} that adds correlated noise to the sufficient statistic in a private
computation and produces a sequence of increasingly accurate answers. A major
advantage of noise reduction mechanisms is that the analysts only pay the
privacy cost of the least noisy or most accurate answer released. Despite this
appealing property in isolation, there has not been a systematic study on how
to use them in conjunction with other differentially private mechanisms. A
fundamental challenge is that the privacy guarantee for noise reduction
mechanisms is (necessarily) formulated as \emph{ex-post privacy} that bounds
the privacy loss as a function of the released outcome. Furthermore, there has
yet to be any study on how ex-post private mechanisms compose, which allows us
to track the accumulated privacy over several mechanisms. We develop privacy
filters \citep{RogersRoUlVa16, FeldmanZr21, WhitehouseRaRoWu22} that allow an
analyst to adaptively switch between differentially private and ex-post private
mechanisms subject to an overall privacy guarantee.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Rogers_R/0/1/0/all/0/1">Ryan Rogers</a>, <a href="http://arxiv.org/find/cs/1/au:+Samorodnitsky_G/0/1/0/all/0/1">Gennady Samorodnitsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiwei Steven Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramdas_A/0/1/0/all/0/1">Aaditya Ramdas</a></p><p>In many practical applications of differential privacy, practitioners seek to
provide the best privacy guarantees subject to a target level of accuracy. A
recent line of work by \cite{LigettNeRoWaWu17, WhitehouseWuRaRo22} has
developed such accuracy-first mechanisms by leveraging the idea of \emph{noise
reduction} that adds correlated noise to the sufficient statistic in a private
computation and produces a sequence of increasingly accurate answers. A major
advantage of noise reduction mechanisms is that the analysts only pay the
privacy cost of the least noisy or most accurate answer released. Despite this
appealing property in isolation, there has not been a systematic study on how
to use them in conjunction with other differentially private mechanisms. A
fundamental challenge is that the privacy guarantee for noise reduction
mechanisms is (necessarily) formulated as \emph{ex-post privacy} that bounds
the privacy loss as a function of the released outcome. Furthermore, there has
yet to be any study on how ex-post private mechanisms compose, which allows us
to track the accumulated privacy over several mechanisms. We develop privacy
filters \citep{RogersRoUlVa16, FeldmanZr21, WhitehouseRaRoWu22} that allow an
analyst to adaptively switch between differentially private and ex-post private
mechanisms subject to an overall privacy guarantee.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-27T00:30:00Z">Tuesday, June 27 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13937'>A Dynamic Data Structure for Representing Timed Transitive Closures on Disk</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Luiz F. Afra Brito, Marcelo Keese Albertini, Bruno A. N. Traven&#xe7;olo</p><p>Temporal graphs represent interactions between entities over time. These
interactions may be direct, a contact between two vertices at some time
instant, or indirect, through sequences of contacts called journeys. Deciding
whether an entity can reach another through a journey is useful for various
applications in complex networks. In this paper, we present a disk-based data
structure that maintains temporal reachability information under the addition
of new contacts in a non-chronological order. It represents the \emph{timed
transitive closure} (TTC) by a set of \emph{expanded} R-tuples of the form $(u,
v, t^-, t^+)$, which encodes the existence of journeys from vertex $u$ to
vertex $v$ with departure at time $t^-$ and arrival at time $t^+$. Let $n$ be
the number of vertices and $\tau$ be the number of timestamps in the lifetime
of the temporal graph. Our data structure explicitly maintains this information
in linear arrays using $O(n^2\tau)$ space so that sequential accesses on disk
are prioritized. Furthermore, it adds a new unsorted contact $(u, v, t)$
accessing $O\left(\frac{n^2\tau}{B}\right)$ sequential pages in the worst-case,
where $B$ is the of pages on disk; it answers whether there is of a journey
from a vertex $u$ to a vertex $v$ within a time interval $[t_1, t_2]$ accessing
a single page; it answers whether all vertices can reach each other in $[t_1,
t_2]$; and it reconstructs a valid journey that validates the reachability from
a vertex $u$ to a vertex $v$ within $[t_1, t_1]$ accessing
$O\left(\frac{n\tau}{B}\right)$ pages. Our experiments show that our novel data
structure are better that the best known approach for the majority of cases
using synthetic and real world datasets.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Brito_L/0/1/0/all/0/1">Luiz F. Afra Brito</a>, <a href="http://arxiv.org/find/cs/1/au:+Albertini_M/0/1/0/all/0/1">Marcelo Keese Albertini</a>, <a href="http://arxiv.org/find/cs/1/au:+Travencolo_B/0/1/0/all/0/1">Bruno A. N. Traven&#xe7;olo</a></p><p>Temporal graphs represent interactions between entities over time. These
interactions may be direct, a contact between two vertices at some time
instant, or indirect, through sequences of contacts called journeys. Deciding
whether an entity can reach another through a journey is useful for various
applications in complex networks. In this paper, we present a disk-based data
structure that maintains temporal reachability information under the addition
of new contacts in a non-chronological order. It represents the \emph{timed
transitive closure} (TTC) by a set of \emph{expanded} R-tuples of the form $(u,
v, t^-, t^+)$, which encodes the existence of journeys from vertex $u$ to
vertex $v$ with departure at time $t^-$ and arrival at time $t^+$. Let $n$ be
the number of vertices and $\tau$ be the number of timestamps in the lifetime
of the temporal graph. Our data structure explicitly maintains this information
in linear arrays using $O(n^2\tau)$ space so that sequential accesses on disk
are prioritized. Furthermore, it adds a new unsorted contact $(u, v, t)$
accessing $O\left(\frac{n^2\tau}{B}\right)$ sequential pages in the worst-case,
where $B$ is the of pages on disk; it answers whether there is of a journey
from a vertex $u$ to a vertex $v$ within a time interval $[t_1, t_2]$ accessing
a single page; it answers whether all vertices can reach each other in $[t_1,
t_2]$; and it reconstructs a valid journey that validates the reachability from
a vertex $u$ to a vertex $v$ within $[t_1, t_1]$ accessing
$O\left(\frac{n\tau}{B}\right)$ pages. Our experiments show that our novel data
structure are better that the best known approach for the majority of cases
using synthetic and real world datasets.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-27T00:30:00Z">Tuesday, June 27 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13958'>On Scalable Testing of Samplers</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yash Pote, Kuldeep S. Meel</p><p>In this paper we study the problem of testing of constrained samplers over
high-dimensional distributions with $(\varepsilon,\eta,\delta)$ guarantees.
Samplers are increasingly used in a wide range of safety-critical ML
applications, and hence the testing problem has gained importance. For
$n$-dimensional distributions, the existing state-of-the-art algorithm,
$\mathsf{Barbarik2}$, has a worst case query complexity of exponential in $n$
and hence is not ideal for use in practice. Our primary contribution is an
exponentially faster algorithm that has a query complexity linear in $n$ and
hence can easily scale to larger instances. We demonstrate our claim by
implementing our algorithm and then comparing it against $\mathsf{Barbarik2}$.
Our experiments on the samplers $\mathsf{wUnigen3}$ and $\mathsf{wSTS}$, find
that $\mathsf{Barbarik3}$ requires $10\times$ fewer samples for
$\mathsf{wUnigen3}$ and $450\times$ fewer samples for $\mathsf{wSTS}$ as
compared to $\mathsf{Barbarik2}$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Pote_Y/0/1/0/all/0/1">Yash Pote</a>, <a href="http://arxiv.org/find/cs/1/au:+Meel_K/0/1/0/all/0/1">Kuldeep S. Meel</a></p><p>In this paper we study the problem of testing of constrained samplers over
high-dimensional distributions with $(\varepsilon,\eta,\delta)$ guarantees.
Samplers are increasingly used in a wide range of safety-critical ML
applications, and hence the testing problem has gained importance. For
$n$-dimensional distributions, the existing state-of-the-art algorithm,
$\mathsf{Barbarik2}$, has a worst case query complexity of exponential in $n$
and hence is not ideal for use in practice. Our primary contribution is an
exponentially faster algorithm that has a query complexity linear in $n$ and
hence can easily scale to larger instances. We demonstrate our claim by
implementing our algorithm and then comparing it against $\mathsf{Barbarik2}$.
Our experiments on the samplers $\mathsf{wUnigen3}$ and $\mathsf{wSTS}$, find
that $\mathsf{Barbarik3}$ requires $10\times$ fewer samples for
$\mathsf{wUnigen3}$ and $450\times$ fewer samples for $\mathsf{wSTS}$ as
compared to $\mathsf{Barbarik2}$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-27T00:30:00Z">Tuesday, June 27 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13996'>Approximation Algorithm for Unrooted Prize-Collecting Forest with Multiple Components and Its Application on Prize-Collecting Sweep Coverage</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Wei Liang, Zhao Zhang</p><p>In this paper, we present a polynomial time 2-approximation algorithm for the
{\em unrooted prize-collecting forest with $K$ components} (URPCF$_K$) problem,
the goal of which is to find a forest with exactly $K$ connected components to
minimize the weight of the forest plus the penalty incurred by the vertices not
spanned by the forest. For its rooted version RPCF$_K$, a 2-approximation
algorithm is known. For the unrooted version, transforming it into a rooted
version by guessing roots runs in time exponentially depending on $K$, which is
unacceptable when $K$ is not a constant. We conquer this challenge by designing
a rootless growing plus rootless pruning algorithm. As an application, we make
use of this algorithm to solve the {\em prize-collecting min-sensor sweep
cover} problem, improving previous approximation ratio 8 to 5.
</p>
<p>Keywords: approximation algorithm, prize-collecting Steiner forest, sweep
cover.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1">Wei Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhao Zhang</a></p><p>In this paper, we present a polynomial time 2-approximation algorithm for the
{\em unrooted prize-collecting forest with $K$ components} (URPCF$_K$) problem,
the goal of which is to find a forest with exactly $K$ connected components to
minimize the weight of the forest plus the penalty incurred by the vertices not
spanned by the forest. For its rooted version RPCF$_K$, a 2-approximation
algorithm is known. For the unrooted version, transforming it into a rooted
version by guessing roots runs in time exponentially depending on $K$, which is
unacceptable when $K$ is not a constant. We conquer this challenge by designing
a rootless growing plus rootless pruning algorithm. As an application, we make
use of this algorithm to solve the {\em prize-collecting min-sensor sweep
cover} problem, improving previous approximation ratio 8 to 5.
</p>
<p>Keywords: approximation algorithm, prize-collecting Steiner forest, sweep
cover.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-27T00:30:00Z">Tuesday, June 27 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.14103'>On finding 2-cuts and 3-edge-connected components in parallel</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yung H. Tsin</p><p>Given a connected undirected multigraph G (a graph that may contain parallel
edges), the algorithm of [19] finds the 3-edge-connected components of $G$ in
linear time using an innovative graph contraction technique based on a
depth-first search. In [21], it was shown that the algorithm can be extended to
produce a Mader construction sequence for each 3-edge-connected component, a
cactus representation of the 2-cuts (cut-pairs) of each 2-edge-connected
component of $G$, and the 1-cuts (bridges) at the same time.
</p>
<p>In this paper, we further extend the algorithm of [19] to generate the 2-cuts
and the 3-edge-connected components of $G$ simultaneously in linear time by
performing only one depth-first search over the input graph. Previously known
algorithms solve the two problems separately in multiple phases.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Tsin_Y/0/1/0/all/0/1">Yung H. Tsin</a></p><p>Given a connected undirected multigraph G (a graph that may contain parallel
edges), the algorithm of [19] finds the 3-edge-connected components of $G$ in
linear time using an innovative graph contraction technique based on a
depth-first search. In [21], it was shown that the algorithm can be extended to
produce a Mader construction sequence for each 3-edge-connected component, a
cactus representation of the 2-cuts (cut-pairs) of each 2-edge-connected
component of $G$, and the 1-cuts (bridges) at the same time.
</p>
<p>In this paper, we further extend the algorithm of [19] to generate the 2-cuts
and the 3-edge-connected components of $G$ simultaneously in linear time by
performing only one depth-first search over the input graph. Previously known
algorithms solve the two problems separately in multiple phases.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-27T00:30:00Z">Tuesday, June 27 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Monday, June 26
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/'>AI Ends It All</a></h3>
        <p class='tr-article-feed'>from <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          I was getting a lift with a friend&#8212;Greg Skau&#8212;just the other day. No not in his boat, but in his car. Our conversation turned to the topic of: &#8220;is AI a threat to all of us?&#8221; Indeed. See this: The year is 2050. The location is London&#8212; but not as we know it. GodBot, a [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>
I was getting a lift with a friend&#8212;Greg Skau&#8212;just the other day. No not in his boat, but in his car. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/gs/" rel="attachment wp-att-21828"><img data-attachment-id="21828" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/gs/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?fit=219%2C320&amp;ssl=1" data-orig-size="219,320" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="gs" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?fit=205%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?fit=219%2C320&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?resize=219%2C320&#038;ssl=1" alt="" width="219" height="320" class="aligncenter size-full wp-image-21828" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?w=219&amp;ssl=1 219w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?resize=205%2C300&amp;ssl=1 205w" sizes="(max-width: 219px) 100vw, 219px" data-recalc-dims="1" /></a></p>
<p><P><br />
Our conversation turned to the topic of: &#8220;is AI a threat to all of us?&#8221; Indeed. See <a href="https://www.standard.co.uk/insider/ai-apocalypse-life-robots-take-over-elon-musk-chatgpt-b1078423.html">this</a>:</p>
<p><P></p>
<blockquote><p><b> </b> <em> The year is 2050. The location is London&#8212; but not as we know it. GodBot, a robot so intelligent it can out-smart any human, is in charge of the United Kingdom &#8212; the entire planet, in fact &#8212; and just announced its latest plan to reverse global temperature rises: an international zero-child, zero-reproduction policy, which will see all human females systematically destroyed and replaced with carbon-neutral sex robots. </em>
</p></blockquote>
<p><p>
That my friend Greg would raise questions about AI seemed pretty neat. It seemed natural yet quite cool that a friend&#8212;who was not an AI expert&#8212;would raise these issues. I am also not an AI expert&#8212;not even an advanced beginner. But it is on just about everyone&#8217;s top list of questions. We had a fun conversation, but failed to resolve the issue. Of course.</p>
<p>
<p><H2> AI Limits </H2></p>
<p><p>
<a href="https://openai.com">OpenAI</a> has just revealed that ChatGPT-4 has learned to lie, telling a human it was a blind person in order to get a task done. Somehow lies seem to set such AI systems apart from what we might have thought were the limits of AI.</p>
<p>
Another thought on AI is: Is physical law an <a href="https://getpocket.com/explore/item/is-physical-law-an-alien-intelligence?utm_source=pocket-newtab">Alien Intelligence?</a>:</p>
<blockquote><p><b> </b> <em> Arthur Clarke once pointed out that any sufficiently advanced technology is going to be indistinguishable from magic. If you dropped in on a bunch of Paleolithic farmers with your iPhone and a pair of sneakers, you&#8217;d undoubtedly seem pretty magical. But the contrast is only middling: The farmers would still recognize you as basically like them, and before long they&#8217;d be taking selfies. But what if life has moved so far on that it doesn&#8217;t just appear magical, but appears like physics? </em>
</p></blockquote>
<p><p>
This is related to Clarke&#8217;s <a href="https://en.wikipedia.org/wiki/Clarke&#37;27s_three_laws">three laws</a>:</p>
<ul>
<li>
When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong. </p>
<li>
The only way of discovering the limits of the possible is to venture a little way past them into the impossible. </p>
<li>
Any sufficiently advanced technology is indistinguishable from magic.
</ul>
<p>
Or take a look at his T-shirt (referring to <a href="https://web.mit.edu/m-i-t/science_fiction/jenkins/jenkins_4.html">this</a>): </p>
<p><P><br />
<a href="https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/ac/" rel="attachment wp-att-21829"><img data-attachment-id="21829" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/ac/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?fit=240%2C240&amp;ssl=1" data-orig-size="240,240" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ac" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?fit=240%2C240&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?fit=240%2C240&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?resize=240%2C240&#038;ssl=1" alt="" width="240" height="240" class="aligncenter size-full wp-image-21829" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?w=240&amp;ssl=1 240w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?resize=200%2C200&amp;ssl=1 200w" sizes="(max-width: 240px) 100vw, 240px" data-recalc-dims="1" /></a></p>
<p>
<p><H2> Open Problems </H2></p>
<p><p>
Alan Perlis&#8212;the first Turing award winner&#8212;is famous for his many <a href="http://www.cs.yale.edu/homes/perlis-alan/quotes.html">quotes</a>. One was: &#8220;A year spent in artificial intelligence is enough to make one believe in God.&#8221;</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/ai/" rel="attachment wp-att-21830"><img data-attachment-id="21830" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/ai/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ai.jpeg?fit=300%2C240&amp;ssl=1" data-orig-size="300,240" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ai" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ai.jpeg?fit=300%2C240&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ai.jpeg?fit=300%2C240&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ai.jpeg?resize=300%2C240&#038;ssl=1" alt="" width="300" height="240" class="aligncenter size-full wp-image-21830" data-recalc-dims="1" /></a></p>
<p>
I enjoyed working for Alan at my first job at Yale University, many years ago. See <a href="https://blog.computationalcomplexity.org/2021/06/i-went-to-debate-about-program-verif.html">Fortnow&#8217;s</a> blog for comments on our joint work with Perlis and Rich DeMillo&#8212;<a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/social.pdf">Social Processes and Proofs of Theorems and Programs</a>.</p>
<p>
Ken pipes in that the record seems to indicate that this late-1970s quote reflected frustration during the long &#8220;AI winter&#8221; when basic human capabilities stayed beyond reach. He also notes that the 2050 date for sex robots was <a href="https://en.wikipedia.org/wiki/Love_and_Sex_with_Robots">forecast</a> by the British chess master who was previously best known for winning a famous computer bet in 1978.</p>
<p>
<p class="authors">By rjlipton</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T17:56:41Z">Monday, June 26 2023, 17:56</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://decentralizedthoughts.github.io/2023-06-26-dls-meets-rollback/'>$3f+1$ is needed in Partial Synchrony even against a Rollback adversary</a></h3>
        <p class='tr-article-feed'>from <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We covered the classic DLS88 split brain impossibility result against a Byzantine adversary in a previous post: DLS88: (Theorem 4.4) It is impossible to solve Agreement under partial synchrony against a Byzantine adversary if $f \geq n/3$. In a follow up, we discussed how CJKR12 strengthen this result by observing...
        
        </div>

        <div class='tr-article-summary'>
        
          
          We covered the classic DLS88 split brain impossibility result against a Byzantine adversary in a previous post: DLS88: (Theorem 4.4) It is impossible to solve Agreement under partial synchrony against a Byzantine adversary if $f \geq n/3$. In a follow up, we discussed how CJKR12 strengthen this result by observing...
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T11:00:00Z">Monday, June 26 2023, 11:00</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13247'>Quantum Merlin-Arthur and proofs without relative phase</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Roozbeh Bassirian, Bill Fefferman, Kunal Marwaha</p><p>We study a variant of QMA where quantum proofs have no relative phase (i.e.
non-negative amplitudes, up to a global phase). If only completeness is
modified, this class is equal to QMA [arXiv:1410.2882]; but if both
completeness and soundness are modified, the class (named QMA+ by Jeronimo and
Wu) can be much more powerful. We show that QMA+ with some constant gap is
equal to NEXP, yet QMA+ with some *other* constant gap is equal to QMA. One
interpretation is that Merlin's ability to "deceive" originates from relative
phase at least as much as from entanglement, since QMA(2) $\subseteq$ NEXP.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Bassirian_R/0/1/0/all/0/1">Roozbeh Bassirian</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Fefferman_B/0/1/0/all/0/1">Bill Fefferman</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Marwaha_K/0/1/0/all/0/1">Kunal Marwaha</a></p><p>We study a variant of QMA where quantum proofs have no relative phase (i.e.
non-negative amplitudes, up to a global phase). If only completeness is
modified, this class is equal to QMA [<a href="/abs/1410.2882">arXiv:1410.2882</a>]; but if both
completeness and soundness are modified, the class (named QMA+ by Jeronimo and
Wu) can be much more powerful. We show that QMA+ with some constant gap is
equal to NEXP, yet QMA+ with some *other* constant gap is equal to QMA. One
interpretation is that Merlin's ability to "deceive" originates from relative
phase at least as much as from entanglement, since QMA(2) $\subseteq$ NEXP.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T00:30:00Z">Monday, June 26 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13319'>A SAT Solver and Computer Algebra Attack on the Minimum Kochen-Specker Problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Zhengyu Li, Curtis Bright, Vijay Ganesh</p><p>One of the foundational results in quantum mechanics is the Kochen-Specker
(KS) theorem, which states that any theory whose predictions agree with quantum
mechanics must be contextual, i.e., a quantum observation cannot be understood
as revealing a pre-existing value. The theorem hinges on the existence of a
mathematical object called a KS vector system. While many KS vector systems are
known to exist, the problem of finding the minimum KS vector system has
remained stubbornly open for over 55 years, despite significant attempts by
leading scientists and mathematicians. In this paper, we present a new method
based on a combination of a SAT solver and a computer algebra system (CAS) to
address this problem. Our approach improves the lower bound on the minimum
number of vectors in a KS system from 22 to 24, and is about 35,000 times more
efficient compared to the previous best computational methods. The increase in
efficiency derives from the fact we are able to exploit the powerful
combinatorial search-with-learning capabilities of a SAT solver together with
the isomorph-free exhaustive generation methods of a CAS. The quest for the
minimum KS vector system is motivated by myriad applications such as
simplifying experimental tests of contextuality, zero-error classical
communication, dimension witnessing, and the security of certain quantum
cryptographic protocols. To the best of our knowledge, this is the first
application of a novel SAT+CAS system to a problem in the realm of quantum
foundations.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Li_Z/0/1/0/all/0/1">Zhengyu Li</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Bright_C/0/1/0/all/0/1">Curtis Bright</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Ganesh_V/0/1/0/all/0/1">Vijay Ganesh</a></p><p>One of the foundational results in quantum mechanics is the Kochen-Specker
(KS) theorem, which states that any theory whose predictions agree with quantum
mechanics must be contextual, i.e., a quantum observation cannot be understood
as revealing a pre-existing value. The theorem hinges on the existence of a
mathematical object called a KS vector system. While many KS vector systems are
known to exist, the problem of finding the minimum KS vector system has
remained stubbornly open for over 55 years, despite significant attempts by
leading scientists and mathematicians. In this paper, we present a new method
based on a combination of a SAT solver and a computer algebra system (CAS) to
address this problem. Our approach improves the lower bound on the minimum
number of vectors in a KS system from 22 to 24, and is about 35,000 times more
efficient compared to the previous best computational methods. The increase in
efficiency derives from the fact we are able to exploit the powerful
combinatorial search-with-learning capabilities of a SAT solver together with
the isomorph-free exhaustive generation methods of a CAS. The quest for the
minimum KS vector system is motivated by myriad applications such as
simplifying experimental tests of contextuality, zero-error classical
communication, dimension witnessing, and the security of certain quantum
cryptographic protocols. To the best of our knowledge, this is the first
application of a novel SAT+CAS system to a problem in the realm of quantum
foundations.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T00:30:00Z">Monday, June 26 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13201'>Decomposition of Geometric Graphs into Star Forests</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: J&#xe1;nos Pach, Morteza Saghafian, Patrick Schnider</p><p>We solve a problem of Dujmovi\'c and Wood (2007) by showing that a complete
convex geometric graph on $n$ vertices cannot be decomposed into fewer than
$n-1$ star-forests, each consisting of noncrossing edges. This bound is clearly
tight. We also discuss similar questions for abstract graphs.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Pach_J/0/1/0/all/0/1">J&#xe1;nos Pach</a>, <a href="http://arxiv.org/find/math/1/au:+Saghafian_M/0/1/0/all/0/1">Morteza Saghafian</a>, <a href="http://arxiv.org/find/math/1/au:+Schnider_P/0/1/0/all/0/1">Patrick Schnider</a></p><p>We solve a problem of Dujmovi\'c and Wood (2007) by showing that a complete
convex geometric graph on $n$ vertices cannot be decomposed into fewer than
$n-1$ star-forests, each consisting of noncrossing edges. This bound is clearly
tight. We also discuss similar questions for abstract graphs.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T00:30:00Z">Monday, June 26 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13312'>Effective data reduction algorithm for topological data analysis</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Seonmi Choi, Jinseok Oh, Jeong Rye Park, Seung Yeop Yang, Hongdae Yun</p><p>One of the most interesting tools that have recently entered the data science
toolbox is topological data analysis (TDA). With the explosion of available
data sizes and dimensions, identifying and extracting the underlying structure
of a given dataset is a fundamental challenge in data science, and TDA provides
a methodology for analyzing the shape of a dataset using tools and prospects
from algebraic topology. However, the computational complexity makes it quickly
infeasible to process large datasets, especially those with high dimensions.
Here, we introduce a preprocessing strategy called the Characteristic Lattice
Algorithm (CLA), which allows users to reduce the size of a given dataset as
desired while maintaining geometric and topological features in order to make
the computation of TDA feasible or to shorten its computation time. In
addition, we derive a stability theorem and an upper bound of the barcode
errors for CLA based on the bottleneck distance.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1">Seonmi Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1">Jinseok Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jeong Rye Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Seung Yeop Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_H/0/1/0/all/0/1">Hongdae Yun</a></p><p>One of the most interesting tools that have recently entered the data science
toolbox is topological data analysis (TDA). With the explosion of available
data sizes and dimensions, identifying and extracting the underlying structure
of a given dataset is a fundamental challenge in data science, and TDA provides
a methodology for analyzing the shape of a dataset using tools and prospects
from algebraic topology. However, the computational complexity makes it quickly
infeasible to process large datasets, especially those with high dimensions.
Here, we introduce a preprocessing strategy called the Characteristic Lattice
Algorithm (CLA), which allows users to reduce the size of a given dataset as
desired while maintaining geometric and topological features in order to make
the computation of TDA feasible or to shorten its computation time. In
addition, we derive a stability theorem and an upper bound of the barcode
errors for CLA based on the bottleneck distance.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T00:30:00Z">Monday, June 26 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13119'>Adversarial Resilience in Sequential Prediction via Abstention</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Surbhi Goel, Steve Hanneke, Shay Moran, Abhishek Shetty</p><p>We study the problem of sequential prediction in the stochastic setting with
an adversary that is allowed to inject clean-label adversarial (or
out-of-distribution) examples. Algorithms designed to handle purely stochastic
data tend to fail in the presence of such adversarial examples, often leading
to erroneous predictions. This is undesirable in many high-stakes applications
such as medical recommendations, where abstaining from predictions on
adversarial examples is preferable to misclassification. On the other hand,
assuming fully adversarial data leads to very pessimistic bounds that are often
vacuous in practice.
</p>
<p>To capture this motivation, we propose a new model of sequential prediction
that sits between the purely stochastic and fully adversarial settings by
allowing the learner to abstain from making a prediction at no cost on
adversarial examples. Assuming access to the marginal distribution on the
non-adversarial examples, we design a learner whose error scales with the VC
dimension (mirroring the stochastic setting) of the hypothesis class, as
opposed to the Littlestone dimension which characterizes the fully adversarial
setting. Furthermore, we design a learner for VC dimension~1 classes, which
works even in the absence of access to the marginal distribution. Our key
technical contribution is a novel measure for quantifying uncertainty for
learning VC classes, which may be of independent interest.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1">Surbhi Goel</a>, <a href="http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1">Steve Hanneke</a>, <a href="http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1">Shay Moran</a>, <a href="http://arxiv.org/find/cs/1/au:+Shetty_A/0/1/0/all/0/1">Abhishek Shetty</a></p><p>We study the problem of sequential prediction in the stochastic setting with
an adversary that is allowed to inject clean-label adversarial (or
out-of-distribution) examples. Algorithms designed to handle purely stochastic
data tend to fail in the presence of such adversarial examples, often leading
to erroneous predictions. This is undesirable in many high-stakes applications
such as medical recommendations, where abstaining from predictions on
adversarial examples is preferable to misclassification. On the other hand,
assuming fully adversarial data leads to very pessimistic bounds that are often
vacuous in practice.
</p>
<p>To capture this motivation, we propose a new model of sequential prediction
that sits between the purely stochastic and fully adversarial settings by
allowing the learner to abstain from making a prediction at no cost on
adversarial examples. Assuming access to the marginal distribution on the
non-adversarial examples, we design a learner whose error scales with the VC
dimension (mirroring the stochastic setting) of the hypothesis class, as
opposed to the Littlestone dimension which characterizes the fully adversarial
setting. Furthermore, we design a learner for VC dimension~1 classes, which
works even in the absence of access to the marginal distribution. Our key
technical contribution is a novel measure for quantifying uncertainty for
learning VC classes, which may be of independent interest.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T00:30:00Z">Monday, June 26 2023, 00:30</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
