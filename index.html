<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.4 (2022-04-12) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2022-11-18T21:31:55Z">Friday, November 18 2022, 21:31</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>SÃ©bastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Friday, November 18
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/159'>TR22-159 |  Deep Neural Networks: The Missing Complexity Parameter | 

	Songhua He, 

	Periklis Papakonstantinou</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Deep neural networks are the dominant machine learning model. We show that this model is missing a crucial complexity parameter. Today, the standard neural network (NN) model is a circuit whose gates (neurons) are ReLU units. The complexity of a NN is quantified by the depth (number of layers) and the size (number of neurons = depth times width). This work shows that this alone is insufficient, resulting in NNs with unreasonable computing power. We show that the correct way to talk about the size complexity of a NN is beside the number of neurons to consider the precision (or magnitude) of the weights of the ReLU units. The main message of this work is that if the precision of the weights is not considered in the complexity of the NN then one can engineer weights to &quot;buy&quot; exponentially many neurons for free. In summary, we make three theoretical contributions, potentially affecting many theoretical works on NNs.

1. Every function $f:\{0,1\}^n\to\{0,1\}$ can be computed with $O(\sqrt{2^n})$ many neurons and constant fan-in per neuron; i.e.~exponential times less than Shannon&#39;s classic lower bound for usual combinatorial circuits. 

2. We give a new definition of circuit size that takes into account the precision/magnitude of the weights. Under this new definition of size we asymptotically match Shannon&#39;s bound for NNs.

3. We complement the above results showing that P-uniform NNs decide exactly P.
        
        </div>

        <div class='tr-article-summary'>
        
          
          Deep neural networks are the dominant machine learning model. We show that this model is missing a crucial complexity parameter. Today, the standard neural network (NN) model is a circuit whose gates (neurons) are ReLU units. The complexity of a NN is quantified by the depth (number of layers) and the size (number of neurons = depth times width). This work shows that this alone is insufficient, resulting in NNs with unreasonable computing power. We show that the correct way to talk about the size complexity of a NN is beside the number of neurons to consider the precision (or magnitude) of the weights of the ReLU units. The main message of this work is that if the precision of the weights is not considered in the complexity of the NN then one can engineer weights to &quot;buy&quot; exponentially many neurons for free. In summary, we make three theoretical contributions, potentially affecting many theoretical works on NNs.

1. Every function $f:\{0,1\}^n\to\{0,1\}$ can be computed with $O(\sqrt{2^n})$ many neurons and constant fan-in per neuron; i.e.~exponential times less than Shannon&#39;s classic lower bound for usual combinatorial circuits. 

2. We give a new definition of circuit size that takes into account the precision/magnitude of the weights. Under this new definition of size we asymptotically match Shannon&#39;s bound for NNs.

3. We complement the above results showing that P-uniform NNs decide exactly P.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T19:13:20Z">Friday, November 18 2022, 19:13</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/158'>TR22-158 |  Query Complexity of Inversion Minimization on Trees | 

	Ivan Hu, 

	Dieter van Melkebeek, 

	Andrew Morgan</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We consider the following computational problem: Given a rooted tree and a ranking of its leaves, what is the minimum number of inversions of the leaves that can be attained by ordering the tree? This variation of the well-known problem of counting inversions in arrays originated in mathematical psychology. It has the evaluation of the Mann-Whitney statistic for detecting differences between distributions as a special case. 

We study the complexity of the problem in the comparison-query model, the standard model for problems like sorting, selection, and heap construction. The complexity depends heavily on the shape of the tree: for trees of unit depth, the problem is trivial; for many other shapes, we establish lower bounds close to the strongest known in the model, namely the lower bound of $\log_2(n!)$ for sorting $n$ items. For trees with $n$ leaves we show, in increasing order of closeness to the sorting lower bound:
(a) $\log_2((\alpha(1-\alpha)n)!) - O(\log n)$ queries are needed whenever the tree has a subtree that contains a fraction $\alpha$ of the leaves. This implies a lower bound of $\log_2((\frac{k}{(k+1)^2}n)!) - O(\log n)$ for trees of degree $k$.
(b) $\log_2(n!) - O(\log n)$ queries are needed in case the tree is binary. 
(c) $\log_2(n!) - O(k \log k)$ queries are needed for certain classes of trees of degree $k$, including perfect trees with even $k$.

The lower bounds are obtained by developing two novel techniques for a generic problem $\Pi$ in the comparison-query model and applying them to inversion minimization on trees. Both techniques can be described in terms of the Cayley graph of the symmetric group with adjacent-rank transpositions as the generating set, or equivalently, in terms of the edge graph of the permutahedron, the polytope spanned by all permutations of the vector $(1,2,\dots,n)$. Consider the subgraph consisting of the edges between vertices with the same value under $\Pi$. We show that the size of any decision tree for $\Pi$ must be at least:
(i) the number of connected components of this subgraph, and
(ii) the factorial of the average degree of the complementary subgraph, divided by $n$.

Lower bounds on query complexity then follow by taking the base-2 logarithm. Technique (i) represents a discrete analog of a classical technique in algebraic complexity and allows us to establish (c) and a tight lower bound for counting cross inversions, as well as unify several of the known lower bounds in the comparison-query model. Technique (ii) represents an analog of sensitivity arguments in Boolean complexity and allows us to establish (a) and (b). 

Along the way to proving (b), we derive a tight upper bound on the maximum probability of the distribution of cross inversions, which is the distribution of the Mann-Whitney statistic in the case of the null hypothesis. Up to normalization the probabilities alternately appear in the literature as the coefficients of polynomials formed by the Gaussian binomial coefficients, also known as Gaussian polynomials.
        
        </div>

        <div class='tr-article-summary'>
        
          
          We consider the following computational problem: Given a rooted tree and a ranking of its leaves, what is the minimum number of inversions of the leaves that can be attained by ordering the tree? This variation of the well-known problem of counting inversions in arrays originated in mathematical psychology. It has the evaluation of the Mann-Whitney statistic for detecting differences between distributions as a special case. 

We study the complexity of the problem in the comparison-query model, the standard model for problems like sorting, selection, and heap construction. The complexity depends heavily on the shape of the tree: for trees of unit depth, the problem is trivial; for many other shapes, we establish lower bounds close to the strongest known in the model, namely the lower bound of $\log_2(n!)$ for sorting $n$ items. For trees with $n$ leaves we show, in increasing order of closeness to the sorting lower bound:
(a) $\log_2((\alpha(1-\alpha)n)!) - O(\log n)$ queries are needed whenever the tree has a subtree that contains a fraction $\alpha$ of the leaves. This implies a lower bound of $\log_2((\frac{k}{(k+1)^2}n)!) - O(\log n)$ for trees of degree $k$.
(b) $\log_2(n!) - O(\log n)$ queries are needed in case the tree is binary. 
(c) $\log_2(n!) - O(k \log k)$ queries are needed for certain classes of trees of degree $k$, including perfect trees with even $k$.

The lower bounds are obtained by developing two novel techniques for a generic problem $\Pi$ in the comparison-query model and applying them to inversion minimization on trees. Both techniques can be described in terms of the Cayley graph of the symmetric group with adjacent-rank transpositions as the generating set, or equivalently, in terms of the edge graph of the permutahedron, the polytope spanned by all permutations of the vector $(1,2,\dots,n)$. Consider the subgraph consisting of the edges between vertices with the same value under $\Pi$. We show that the size of any decision tree for $\Pi$ must be at least:
(i) the number of connected components of this subgraph, and
(ii) the factorial of the average degree of the complementary subgraph, divided by $n$.

Lower bounds on query complexity then follow by taking the base-2 logarithm. Technique (i) represents a discrete analog of a classical technique in algebraic complexity and allows us to establish (c) and a tight lower bound for counting cross inversions, as well as unify several of the known lower bounds in the comparison-query model. Technique (ii) represents an analog of sensitivity arguments in Boolean complexity and allows us to establish (a) and (b). 

Along the way to proving (b), we derive a tight upper bound on the maximum probability of the distribution of cross inversions, which is the distribution of the Mann-Whitney statistic in the case of the null hypothesis. Up to normalization the probabilities alternately appear in the literature as the coefficients of polynomials formed by the Gaussian binomial coefficients, also known as Gaussian polynomials.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T15:57:57Z">Friday, November 18 2022, 15:57</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=6818'>WINNERS of the Scott Aaronson Grant for Advanced Precollege STEM Education!</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          I&#8217;m thrilled to be able to interrupt your regular depressing programming for 100% happy news. Some readers will remember that, back in September, I announced that an unnamed charitable foundation had asked my advice on how best to donate $250,000 for advanced precollege STEM education. So, just like the previous time I got such a [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>I&#8217;m thrilled to be able to interrupt your regular depressing programming for 100% happy news.</p>



<p>Some readers will remember that, back in September, I <a href="https://scottaaronson.blog/?p=6678">announced</a> that an unnamed charitable foundation had asked my advice on how best to donate $250,000 for advanced precollege STEM education.  So, just like the <a href="https://scottaaronson.blog/?p=6232">previous time</a> I got such a request, from Jaan Tallinn&#8217;s <a href="https://survivalandflourishing.fund/">Survival and Flourishing Fund</a>, I decided to do a call for proposals on <em>Shtetl-Optimized</em> before passing along my recommendations.</p>



<p>I can now reveal that the generous foundation, this time around, was the <a href="https://www.packard.org/">Packard Foundation</a>.  Indeed, the idea and initial inquiries to me came directly from <a href="https://www.packard.org/about-the-foundation/our-people/bio/david-orr/">Dave Orr</a>: the chair of the foundation, grandson of Hewlett-Packard cofounder <a href="https://en.wikipedia.org/wiki/David_Packard">David Packard</a>, and (so I learned) longtime <em>Shtetl-Optimized</em> reader.</p>



<p>I can <em>also</em> now reveal the results.  I was honored to get more than a dozen excellent applications.  After carefully considering all of them, I passed along four finalists to the Packard Foundation, which preferred to award the entire allotment to a single program if possible.  After more discussion and research, the Foundation then actually decided on <em>two</em> winners:</p>



<ul>
<li>$225,000 for general support to <a href="https://promys.org/">PROMYS</a>: the long-running, world-renowned summer math camp for high-school students, which (among other things) is in the process of launching a new branch in India.  While I ended up at <a href="https://www.mathcamp.org/">Canada/USA Mathcamp</a> (which I supported in my <a href="https://scottaaronson.blog/?p=6256">first grant round</a>) rather than PROMYS, I knew all about and admired PROMYS even back when I was the right age to attend it.  I&#8217;m thrilled to be able to play a small role in its expansion.</li>
</ul>



<ul>
<li>$30,000 for general support to <a href="https://www.addiscoder.com/">AddisCoder</a>: the phenomenal program that introduces Ethiopian high-schoolers to programming and algorithms.  AddisCoder was founded by UC Berkeley theoretical computer science professor and longtime friend-of-the-blog <a href="https://people.eecs.berkeley.edu/~minilek/">Jelani Nelson</a>, and <em>also</em> received $30,000 in my <a href="https://scottaaronson.blog/?p=6256">first grant round</a>.  Jelani and his co-organizers will be pressing ahead with AddisCoder despite political conflict in Ethiopia including a recently-concluded <a href="https://en.wikipedia.org/wiki/Tigray_War">civil war</a>.  I&#8217;m humbled if I can make even the tiniest difference.</li>
</ul>



<p>Thanks so much to the Packard Foundation, and to Packard&#8217;s talented program officers, directors, and associates&#8212;especially Laura Sullivan, Jean Ries, and Prithi Trivedi&#8212;for their hard work to make this happen.  Thanks so much also to everyone who applied.  While I wish we could&#8217;ve funded everyone, I&#8217;ve learned a lot about programs to which I&#8217;d like to steer future support <strong>(other prospective benefactors: please email me!!)</strong>, <em>and</em> to which I&#8217;d like to steer kids: my own, once they&#8217;re old enough, and other kids of my acquaintance.</p>



<p>I feel good that, in the tiny, underfunded world of accelerated STEM education, the $255,000 that Packard is donating will already make a difference.  But of course, $255,000 is only a thousandth of $255 million, which is a thousandth of $255 billion.  Perhaps I could earn the latter sort of sums, to donate to STEM education or any other cause, by (for example) starting my own cryptocurrency exchange.  I hope my readers will forgive me for not having chosen that route, expected-utility-maximization arguments be damned.</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T08:01:17Z">Friday, November 18 2022, 08:01</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09229'>Improved Monotonicity Testers via Hypercube Embeddings</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mark Braverman, Subhash Khot, Guy Kindler, Dor Minzer</p><p>We show improved monotonicity testers for the Boolean hypercube under the
$p$-biased measure, as well as over the hypergrid $[m]^n$. Our results are:
</p>
<p>1. For any $p\in (0,1)$, for the $p$-biased hypercube we show a non-adaptive
tester that makes $\tilde{O}(\sqrt{n}/\varepsilon^2)$ queries, accepts monotone
functions with probability $1$ and rejects functions that are $\varepsilon$-far
from monotone with probability at least $2/3$.
</p>
<p>2. For all $m\in\mathbb{N}$, we show an
$\tilde{O}(\sqrt{n}m^3/\varepsilon^2)$ query monotonicity tester over $[m]^n$.
</p>
<p>We also establish corresponding directed isoperimetric inequalities in these
domains. Previously, the best known tester due to Black, Chakrabarty and
Seshadhri had $\Omega(n^{5/6})$ query complexity. Our results are optimal up to
poly-logarithmic factors and the dependency on $m$.
</p>
<p>Our proof uses a notion of monotone embeddings of measures into the Boolean
hypercube that can be used to reduce the problem of monotonicity testing over
an arbitrary product domains to the Boolean cube. The embedding maps a function
over a product domain of dimension $n$ into a function over a Boolean cube of a
larger dimension $n'$, while preserving its distance from being monotone; an
embedding is considered efficient if $n'$ is not much larger than $n$, and we
show how to construct efficient embeddings in the above mentioned settings.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Braverman_M/0/1/0/all/0/1">Mark Braverman</a>, <a href="http://arxiv.org/find/cs/1/au:+Khot_S/0/1/0/all/0/1">Subhash Khot</a>, <a href="http://arxiv.org/find/cs/1/au:+Kindler_G/0/1/0/all/0/1">Guy Kindler</a>, <a href="http://arxiv.org/find/cs/1/au:+Minzer_D/0/1/0/all/0/1">Dor Minzer</a></p><p>We show improved monotonicity testers for the Boolean hypercube under the
$p$-biased measure, as well as over the hypergrid $[m]^n$. Our results are:
</p>
<p>1. For any $p\in (0,1)$, for the $p$-biased hypercube we show a non-adaptive
tester that makes $\tilde{O}(\sqrt{n}/\varepsilon^2)$ queries, accepts monotone
functions with probability $1$ and rejects functions that are $\varepsilon$-far
from monotone with probability at least $2/3$.
</p>
<p>2. For all $m\in\mathbb{N}$, we show an
$\tilde{O}(\sqrt{n}m^3/\varepsilon^2)$ query monotonicity tester over $[m]^n$.
</p>
<p>We also establish corresponding directed isoperimetric inequalities in these
domains. Previously, the best known tester due to Black, Chakrabarty and
Seshadhri had $\Omega(n^{5/6})$ query complexity. Our results are optimal up to
poly-logarithmic factors and the dependency on $m$.
</p>
<p>Our proof uses a notion of monotone embeddings of measures into the Boolean
hypercube that can be used to reduce the problem of monotonicity testing over
an arbitrary product domains to the Boolean cube. The embedding maps a function
over a product domain of dimension $n$ into a function over a Boolean cube of a
larger dimension $n'$, while preserving its distance from being monotone; an
embedding is considered efficient if $n'$ is not much larger than $n$, and we
show how to construct efficient embeddings in the above mentioned settings.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09341'>Approaching the Soundness Barrier: A Near Optimal Analysis of the Cube versus Cube Test</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Dor Minzer, Kai Zheng</p><p>The Cube versus Cube test is a variant of the well-known Plane versus Plane
test of Raz and Safra, in which to each $3$-dimensional affine subspace $C$ of
$\mathbb{F}_q^n$, a polynomial of degree at most $d$, $T(C)$, is assigned in a
somewhat locally consistent manner: taking two cubes $C_1, C_2$ that intersect
in a plane uniformly at random, the probability that $T(C_1)$ and $T(C_2)$
agree on $C_1\cap C_2$ is at least some $\epsilon$. An element of interest is
the soundness threshold of this test, i.e. the smallest value of $\epsilon$,
such that this amount of local consistency implies a global structure; namely,
that there is a global degree $d$ function $g$ such that $g|_{C} \equiv T(C)$
for at least $\Omega(\epsilon)$ fraction of the cubes.
</p>
<p>We show that the cube versus cube low degree test has soundness ${\sf
poly}(d)/q$. This result achieves the optimal dependence on $q$ for soundness
in low degree testing and improves upon previous soundness results of ${\sf
poly}(d)/q^{1/2}$ due to Bhangale, Dinur and Navon.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Minzer_D/0/1/0/all/0/1">Dor Minzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1">Kai Zheng</a></p><p>The Cube versus Cube test is a variant of the well-known Plane versus Plane
test of Raz and Safra, in which to each $3$-dimensional affine subspace $C$ of
$\mathbb{F}_q^n$, a polynomial of degree at most $d$, $T(C)$, is assigned in a
somewhat locally consistent manner: taking two cubes $C_1, C_2$ that intersect
in a plane uniformly at random, the probability that $T(C_1)$ and $T(C_2)$
agree on $C_1\cap C_2$ is at least some $\epsilon$. An element of interest is
the soundness threshold of this test, i.e. the smallest value of $\epsilon$,
such that this amount of local consistency implies a global structure; namely,
that there is a global degree $d$ function $g$ such that $g|_{C} \equiv T(C)$
for at least $\Omega(\epsilon)$ fraction of the cubes.
</p>
<p>We show that the cube versus cube low degree test has soundness ${\sf
poly}(d)/q$. This result achieves the optimal dependence on $q$ for soundness
in low degree testing and improves upon previous soundness results of ${\sf
poly}(d)/q^{1/2}$ due to Bhangale, Dinur and Navon.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09482'>Unique-Neighbor-Like Expansion and Group-Independent Cosystolic Expansion</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Tali Kaufman, David Mass</p><p>In recent years, high dimensional expanders have been found to have a variety
of applications in theoretical computer science, such as efficient CSPs
approximations, improved sampling and list-decoding algorithms, and more.
Within that, an important high dimensional expansion notion is \emph{cosystolic
expansion}, which has found applications in the construction of efficiently
decodable quantum codes and in proving lower bounds for CSPs.
</p>
<p>Cosystolic expansion is considered with systems of equations over a group
where the variables and equations correspond to faces of the complex. Previous
works that studied cosystolic expansion were tailored to the specific group
$\mathbb{F}_2$. In particular, Kaufman, Kazhdan and Lubotzky (FOCS 2014), and
Evra and Kaufman (STOC 2016) in their breakthrough works, who solved a famous
open question of Gromov, have studied a notion which we term ``parity''
expansion for small sets. They showed that small sets of $k$-faces have
proportionally many $(k+1)$-faces that contain \emph{an odd number} of
$k$-faces from the set. Parity expansion for small sets could be used to imply
cosystolic expansion only over $\mathbb{F}_2$.
</p>
<p>In this work we introduce a stronger \emph{unique-neighbor-like} expansion
for small sets. We show that small sets of $k$-faces have proportionally many
$(k+1)$-faces that contain \emph{exactly one} $k$-face from the set. This
notion is fundamentally stronger than parity expansion and cannot be implied by
previous works.
</p>
<p>We then show, utilizing the new unique-neighbor-like expansion notion
introduced in this work, that cosystolic expansion can be made
\emph{group-independent}, i.e., unique-neighbor-like expansion for small sets
implies cosystolic expansion \emph{over any group}.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kaufman_T/0/1/0/all/0/1">Tali Kaufman</a>, <a href="http://arxiv.org/find/cs/1/au:+Mass_D/0/1/0/all/0/1">David Mass</a></p><p>In recent years, high dimensional expanders have been found to have a variety
of applications in theoretical computer science, such as efficient CSPs
approximations, improved sampling and list-decoding algorithms, and more.
Within that, an important high dimensional expansion notion is \emph{cosystolic
expansion}, which has found applications in the construction of efficiently
decodable quantum codes and in proving lower bounds for CSPs.
</p>
<p>Cosystolic expansion is considered with systems of equations over a group
where the variables and equations correspond to faces of the complex. Previous
works that studied cosystolic expansion were tailored to the specific group
$\mathbb{F}_2$. In particular, Kaufman, Kazhdan and Lubotzky (FOCS 2014), and
Evra and Kaufman (STOC 2016) in their breakthrough works, who solved a famous
open question of Gromov, have studied a notion which we term ``parity''
expansion for small sets. They showed that small sets of $k$-faces have
proportionally many $(k+1)$-faces that contain \emph{an odd number} of
$k$-faces from the set. Parity expansion for small sets could be used to imply
cosystolic expansion only over $\mathbb{F}_2$.
</p>
<p>In this work we introduce a stronger \emph{unique-neighbor-like} expansion
for small sets. We show that small sets of $k$-faces have proportionally many
$(k+1)$-faces that contain \emph{exactly one} $k$-face from the set. This
notion is fundamentally stronger than parity expansion and cannot be implied by
previous works.
</p>
<p>We then show, utilizing the new unique-neighbor-like expansion notion
introduced in this work, that cosystolic expansion can be made
\emph{group-independent}, i.e., unique-neighbor-like expansion for small sets
implies cosystolic expansion \emph{over any group}.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09485'>Double Balanced Sets in High Dimensional Expanders</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Tali Kaufman, David Mass</p><p>Recent works have shown that expansion of pseudorandom sets is of great
importance. However, all current works on pseudorandom sets are limited only to
product (or approximate product) spaces, where Fourier Analysis methods could
be applied. In this work we ask the natural question whether pseudorandom sets
are relevant in domains where Fourier Analysis methods cannot be applied, e.g.,
one-sided local spectral expanders.
</p>
<p>We take the first step in the path of answering this question. We put forward
a new definition for pseudorandom sets, which we call ``double balanced sets''.
We demonstrate the strength of our new definition by showing that small double
balanced sets in one-sided local spectral expanders have very strong expansion
properties, such as unique-neighbor-like expansion. We further show that
cohomologies in cosystolic expanders are double balanced, and use the newly
derived strong expansion properties of double balanced sets in order to obtain
an exponential improvement over the current state of the art lower bound on
their minimal distance.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kaufman_T/0/1/0/all/0/1">Tali Kaufman</a>, <a href="http://arxiv.org/find/cs/1/au:+Mass_D/0/1/0/all/0/1">David Mass</a></p><p>Recent works have shown that expansion of pseudorandom sets is of great
importance. However, all current works on pseudorandom sets are limited only to
product (or approximate product) spaces, where Fourier Analysis methods could
be applied. In this work we ask the natural question whether pseudorandom sets
are relevant in domains where Fourier Analysis methods cannot be applied, e.g.,
one-sided local spectral expanders.
</p>
<p>We take the first step in the path of answering this question. We put forward
a new definition for pseudorandom sets, which we call ``double balanced sets''.
We demonstrate the strength of our new definition by showing that small double
balanced sets in one-sided local spectral expanders have very strong expansion
properties, such as unique-neighbor-like expansion. We further show that
cohomologies in cosystolic expanders are double balanced, and use the newly
derived strong expansion properties of double balanced sets in order to obtain
an exponential improvement over the current state of the art lower bound on
their minimal distance.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09198'>Cooperative 2D Reconfiguration using Spatio-Temporal Planning and Load Transferring</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Javier Garcia, Michael Yannuzzi, Peter Kramer, Christian Rieck, S&#xe1;ndor P. Fekete, Aaron T. Becker</p><p>We present progress on the problem of reconfiguring a 2D arrangement of
building material by a cooperative set of robots. These robots are subjected to
the constraints of avoiding obstacles and maintaining connectivity of the
structure. We develop two reconfiguration methods, one based on spatio-temporal
planning, and one based on target swapping. Both methods achieve coordinated
motion of robots by avoiding deadlocks and maintaining all constraints. Both
methods also increase efficiency by reducing the amount of waiting times and
lowering combined travel costs. The resulting progress is validated by
simulations that also scale the number of robots.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1">Javier Garcia</a>, <a href="http://arxiv.org/find/cs/1/au:+Yannuzzi_M/0/1/0/all/0/1">Michael Yannuzzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kramer_P/0/1/0/all/0/1">Peter Kramer</a>, <a href="http://arxiv.org/find/cs/1/au:+Rieck_C/0/1/0/all/0/1">Christian Rieck</a>, <a href="http://arxiv.org/find/cs/1/au:+Fekete_S/0/1/0/all/0/1">S&#xe1;ndor P. Fekete</a>, <a href="http://arxiv.org/find/cs/1/au:+Becker_A/0/1/0/all/0/1">Aaron T. Becker</a></p><p>We present progress on the problem of reconfiguring a 2D arrangement of
building material by a cooperative set of robots. These robots are subjected to
the constraints of avoiding obstacles and maintaining connectivity of the
structure. We develop two reconfiguration methods, one based on spatio-temporal
planning, and one based on target swapping. Both methods achieve coordinated
motion of robots by avoiding deadlocks and maintaining all constraints. Both
methods also increase efficiency by reducing the amount of waiting times and
lowering combined travel costs. The resulting progress is validated by
simulations that also scale the number of robots.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09328'>Covering and packing with homothets of limited capacity</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Oriol Sol&#xe9; Pi</p><p>This work revolves around the two following questions: Given a convex body
$C\subset\mathbb{R}^d$, a positive integer $k$ and a finite set
$S\subset\mathbb{R}^d$ (or a finite $\mu$ Borel measure in $\mathbb{R}^d$), how
many homothets of $C$ are required to cover $S$ if no homothet is allowed to
cover more than $k$ points of $S$ (or have measure more than $k$)? how many
homothets of $C$ can be packed if each of them must cover at least $k$ points
of $S$ (or have measure at least $k$)? We prove that, so long as $S$ is not too
degenerate, the answer to both questions is $\Theta_d(\frac{|S|}{k})$, where
the hidden constant is independent of $d$, this is clearly best possible up to
a multiplicative constant. Analogous results hold in the case of measures. Then
we introduce a generalization of the standard covering and packing densities of
a convex body $C$ to Borel measure spaces in $\mathbb{R}^d$ and, using the
aforementioned bounds, we show that they are bounded from above and below,
respectively, by functions of $d$. As an intermediate result, we give a simple
proof the existence of weak $\epsilon$-nets of size $O(\frac{1}{\epsilon})$ for
the range space induced by all homothets of $C$. Following some recent work in
discrete geometry, we investigate the case $d=k=2$ in greater detail. We also
provide polynomial time algorithms for constructing a packing/covering
exhibiting the $\Theta_d(\frac{|S|}{k})$ bound mentioned above in the case that
$C$ is an Euclidean ball. Finally, it is shown that if $C$ is a square then it
is NP-hard to decide whether $S$ can be covered by $\frac{|S|}{4}$ squares
containing $4$ points each.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Pi_O/0/1/0/all/0/1">Oriol Sol&#xe9; Pi</a></p><p>This work revolves around the two following questions: Given a convex body
$C\subset\mathbb{R}^d$, a positive integer $k$ and a finite set
$S\subset\mathbb{R}^d$ (or a finite $\mu$ Borel measure in $\mathbb{R}^d$), how
many homothets of $C$ are required to cover $S$ if no homothet is allowed to
cover more than $k$ points of $S$ (or have measure more than $k$)? how many
homothets of $C$ can be packed if each of them must cover at least $k$ points
of $S$ (or have measure at least $k$)? We prove that, so long as $S$ is not too
degenerate, the answer to both questions is $\Theta_d(\frac{|S|}{k})$, where
the hidden constant is independent of $d$, this is clearly best possible up to
a multiplicative constant. Analogous results hold in the case of measures. Then
we introduce a generalization of the standard covering and packing densities of
a convex body $C$ to Borel measure spaces in $\mathbb{R}^d$ and, using the
aforementioned bounds, we show that they are bounded from above and below,
respectively, by functions of $d$. As an intermediate result, we give a simple
proof the existence of weak $\epsilon$-nets of size $O(\frac{1}{\epsilon})$ for
the range space induced by all homothets of $C$. Following some recent work in
discrete geometry, we investigate the case $d=k=2$ in greater detail. We also
provide polynomial time algorithms for constructing a packing/covering
exhibiting the $\Theta_d(\frac{|S|}{k})$ bound mentioned above in the case that
$C$ is an Euclidean ball. Finally, it is shown that if $C$ is a square then it
is NP-hard to decide whether $S$ can be covered by $\frac{|S|}{4}$ squares
containing $4$ points each.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09729'>Rounding via Low Dimensional Embeddings</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mark Braverman, Dor Minzer</p><p>A regular graph $G = (V,E)$ is an $(\varepsilon,\gamma)$ small-set expander
if for any set of vertices of fractional size at most $\varepsilon$, at least
$\gamma$ of the edges that are adjacent to it go outside. In this paper, we
give a unified approach to several known complexity-theoretic results on
small-set expanders. In particular, we show:
</p>
<p>1. Max-Cut: we show that if a regular graph $G = (V,E)$ is an
$(\varepsilon,\gamma)$ small-set expander that contains a cut of fractional
size at least $1-\delta$, then one can find in $G$ a cut of fractional size at
least $1-O\left(\frac{\delta}{\varepsilon\gamma^6}\right)$ in polynomial time.
</p>
<p>2. Improved spectral partitioning, Cheeger's inequality and the parallel
repetition theorem over small-set expanders. The general form of each one of
these results involves square-root loss that comes from certain rounding
procedure, and we show how this can be avoided over small set expanders.
</p>
<p>Our main idea is to project a high dimensional vector solution into a
low-dimensional space while roughly maintaining $\ell_2^2$ distances, and then
perform a pre-processing step using low-dimensional geometry and the properties
of $\ell_2^2$ distances over it. This pre-processing leverages the small-set
expansion property of the graph to transform a vector valued solution to a
different vector valued solution with additional structural properties, which
give rise to more efficient integral-solution rounding schemes.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Braverman_M/0/1/0/all/0/1">Mark Braverman</a>, <a href="http://arxiv.org/find/cs/1/au:+Minzer_D/0/1/0/all/0/1">Dor Minzer</a></p><p>A regular graph $G = (V,E)$ is an $(\varepsilon,\gamma)$ small-set expander
if for any set of vertices of fractional size at most $\varepsilon$, at least
$\gamma$ of the edges that are adjacent to it go outside. In this paper, we
give a unified approach to several known complexity-theoretic results on
small-set expanders. In particular, we show:
</p>
<p>1. Max-Cut: we show that if a regular graph $G = (V,E)$ is an
$(\varepsilon,\gamma)$ small-set expander that contains a cut of fractional
size at least $1-\delta$, then one can find in $G$ a cut of fractional size at
least $1-O\left(\frac{\delta}{\varepsilon\gamma^6}\right)$ in polynomial time.
</p>
<p>2. Improved spectral partitioning, Cheeger's inequality and the parallel
repetition theorem over small-set expanders. The general form of each one of
these results involves square-root loss that comes from certain rounding
procedure, and we show how this can be avoided over small set expanders.
</p>
<p>Our main idea is to project a high dimensional vector solution into a
low-dimensional space while roughly maintaining $\ell_2^2$ distances, and then
perform a pre-processing step using low-dimensional geometry and the properties
of $\ell_2^2$ distances over it. This pre-processing leverages the small-set
expansion property of the graph to transform a vector valued solution to a
different vector valued solution with additional structural properties, which
give rise to more efficient integral-solution rounding schemes.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09603'>(Re)packing Equal Disks into Rectangle</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Fedor V. Fomin, Petr A. Golovach, Tanmay Inamdar, Saket Saurabh, Meirav Zehavi</p><p>The problem of packing of equal disks (or circles) into a rectangle is a
fundamental geometric problem. (By a packing here we mean an arrangement of
disks in a rectangle without overlapping.) We consider the following
algorithmic generalization of the equal disk packing problem. In this problem,
for a given packing of equal disks into a rectangle, the question is whether by
changing positions of a small number of disks, we can allocate space for
packing more disks. More formally, in the repacking problem, for a given set of
$n$ equal disks packed into a rectangle and integers $k$ and $h$, we ask
whether it is possible by changing positions of at most $h$ disks to pack $n+k$
disks. Thus the problem of packing equal disks is the special case of our
problem with $n=h=0$.
</p>
<p>While the computational complexity of packing equal disks into a rectangle
remains open, we prove that the repacking problem is NP-hard already for $h=0$.
Our main algorithmic contribution is an algorithm that solves the repacking
problem in time $(h+k)^{O(h+k)}\cdot |I|^{O(1)}$, where $I$ is the input size.
That is, the problem is fixed-parameter tractable parameterized by $k$ and $h$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Fomin_F/0/1/0/all/0/1">Fedor V. Fomin</a>, <a href="http://arxiv.org/find/cs/1/au:+Golovach_P/0/1/0/all/0/1">Petr A. Golovach</a>, <a href="http://arxiv.org/find/cs/1/au:+Inamdar_T/0/1/0/all/0/1">Tanmay Inamdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Saurabh_S/0/1/0/all/0/1">Saket Saurabh</a>, <a href="http://arxiv.org/find/cs/1/au:+Zehavi_M/0/1/0/all/0/1">Meirav Zehavi</a></p><p>The problem of packing of equal disks (or circles) into a rectangle is a
fundamental geometric problem. (By a packing here we mean an arrangement of
disks in a rectangle without overlapping.) We consider the following
algorithmic generalization of the equal disk packing problem. In this problem,
for a given packing of equal disks into a rectangle, the question is whether by
changing positions of a small number of disks, we can allocate space for
packing more disks. More formally, in the repacking problem, for a given set of
$n$ equal disks packed into a rectangle and integers $k$ and $h$, we ask
whether it is possible by changing positions of at most $h$ disks to pack $n+k$
disks. Thus the problem of packing equal disks is the special case of our
problem with $n=h=0$.
</p>
<p>While the computational complexity of packing equal disks into a rectangle
remains open, we prove that the repacking problem is NP-hard already for $h=0$.
Our main algorithmic contribution is an algorithm that solves the repacking
problem in time $(h+k)^{O(h+k)}\cdot |I|^{O(1)}$, where $I$ is the input size.
That is, the problem is fixed-parameter tractable parameterized by $k$ and $h$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09133'>On the complexity of implementing Trotter steps</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Guang Hao Low, Yuan Su, Yu Tong, Minh C. Tran</p><p>Quantum dynamics can be simulated on a quantum computer by exponentiating
elementary terms from the Hamiltonian in a sequential manner. However, such an
implementation of Trotter steps has gate complexity depending on the total
Hamiltonian term number, comparing unfavorably to algorithms using more
advanced techniques. We develop methods to perform faster Trotter steps with
complexity sublinear in the number of terms. We achieve this for a class of
Hamiltonians whose interaction strength decays with distance according to power
law. Our methods include one based on a recursive block encoding and one based
on an average-cost simulation, overcoming the normalization-factor barrier of
these advanced quantum simulation techniques. We also realize faster Trotter
steps when certain blocks of Hamiltonian coefficients have low rank. Combining
with a tighter error analysis, we show that it suffices to use
$\left(\eta^{1/3}n^{1/3}+\frac{n^{2/3}}{\eta^{2/3}}\right)n^{1+o(1)}$ gates to
simulate uniform electron gas with $n$ spin orbitals and $\eta$ electrons in
second quantization in real space, asymptotically improving over the best
previous work. We obtain an analogous result when the external potential of
nuclei is introduced under the Born-Oppenheimer approximation. We prove a
circuit lower bound when the Hamiltonian coefficients take a continuum range of
values, showing that generic $n$-qubit $2$-local Hamiltonians with commuting
terms require at least $\Omega(n^2)$ gates to evolve with accuracy
$\epsilon=\Omega(1/poly(n))$ for time $t=\Omega(\epsilon)$. Our proof is based
on a gate-efficient reduction from the approximate synthesis of diagonal
unitaries within the Hamming weight-$2$ subspace, which may be of independent
interest. Our result thus suggests the use of Hamiltonian structural properties
as both necessary and sufficient to implement Trotter steps with lower gate
complexity.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Low_G/0/1/0/all/0/1">Guang Hao Low</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Su_Y/0/1/0/all/0/1">Yuan Su</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Tong_Y/0/1/0/all/0/1">Yu Tong</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Tran_M/0/1/0/all/0/1">Minh C. Tran</a></p><p>Quantum dynamics can be simulated on a quantum computer by exponentiating
elementary terms from the Hamiltonian in a sequential manner. However, such an
implementation of Trotter steps has gate complexity depending on the total
Hamiltonian term number, comparing unfavorably to algorithms using more
advanced techniques. We develop methods to perform faster Trotter steps with
complexity sublinear in the number of terms. We achieve this for a class of
Hamiltonians whose interaction strength decays with distance according to power
law. Our methods include one based on a recursive block encoding and one based
on an average-cost simulation, overcoming the normalization-factor barrier of
these advanced quantum simulation techniques. We also realize faster Trotter
steps when certain blocks of Hamiltonian coefficients have low rank. Combining
with a tighter error analysis, we show that it suffices to use
$\left(\eta^{1/3}n^{1/3}+\frac{n^{2/3}}{\eta^{2/3}}\right)n^{1+o(1)}$ gates to
simulate uniform electron gas with $n$ spin orbitals and $\eta$ electrons in
second quantization in real space, asymptotically improving over the best
previous work. We obtain an analogous result when the external potential of
nuclei is introduced under the Born-Oppenheimer approximation. We prove a
circuit lower bound when the Hamiltonian coefficients take a continuum range of
values, showing that generic $n$-qubit $2$-local Hamiltonians with commuting
terms require at least $\Omega(n^2)$ gates to evolve with accuracy
$\epsilon=\Omega(1/poly(n))$ for time $t=\Omega(\epsilon)$. Our proof is based
on a gate-efficient reduction from the approximate synthesis of diagonal
unitaries within the Hamming weight-$2$ subspace, which may be of independent
interest. Our result thus suggests the use of Hamiltonian structural properties
as both necessary and sufficient to implement Trotter steps with lower gate
complexity.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09251'>On the Power of Learning-Augmented BSTs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jingbang Chen, Li Chen</p><p>We present the first Learning-Augmented Binary Search Tree(BST) that attains
Static Optimality and Working-Set Bound given rough predictions. Following the
recent studies in algorithms with predictions and learned index structures,
Lin, Luo, and Woodruff (ICML 2022) introduced the concept of Learning-Augmented
BSTs, which aim to improve BSTs with learned advice. Unfortunately, their
construction gives only static optimality under strong assumptions on the
input.
</p>
<p>In this paper, we present a simple BST maintenance scheme that benefits from
learned advice. With proper predictions, the scheme achieves Static Optimality
and Working-Set Bound, respectively, which are important performance measures
for BSTs. Moreover, the scheme is robust to prediction errors and makes no
assumption on the input.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jingbang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Li Chen</a></p><p>We present the first Learning-Augmented Binary Search Tree(BST) that attains
Static Optimality and Working-Set Bound given rough predictions. Following the
recent studies in algorithms with predictions and learned index structures,
Lin, Luo, and Woodruff (ICML 2022) introduced the concept of Learning-Augmented
BSTs, which aim to improve BSTs with learned advice. Unfortunately, their
construction gives only static optimality under strong assumptions on the
input.
</p>
<p>In this paper, we present a simple BST maintenance scheme that benefits from
learned advice. With proper predictions, the scheme achieves Static Optimality
and Working-Set Bound, respectively, which are important performance measures
for BSTs. Moreover, the scheme is robust to prediction errors and makes no
assumption on the input.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09415'>Near-Optimal Distributed Computation of Small Vertex Cuts</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Merav Parter, Asaf Petruschka</p><p>We present near-optimal algorithms for detecting small vertex cuts in the
CONGEST model of distributed computing. Despite extensive research in this
area, our understanding of the vertex connectivity of a graph is still
incomplete, especially in the distributed setting. To this date, all
distributed algorithms for detecting cut vertices suffer from an inherent
dependency in the maximum degree of the graph, $\Delta$. Hence, in particular,
there is no truly sub-linear time algorithm for this problem, not even for
detecting a single cut vertex. We take a new algorithmic approach for vertex
connectivity which allows us to bypass the existing $\Delta$ barrier. As a
warm-up to our approach, we show a simple $\widetilde{O}(D)$-round randomized
algorithm for computing all cut vertices in a $D$-diameter $n$-vertex graph.
This improves upon the $O(D+\Delta/\log n)$-round algorithm of [Pritchard and
Thurimella, ICALP 2008]. Our key technical contribution is an
$\widetilde{O}(D)$-round randomized algorithm for computing all cut pairs in
the graph, improving upon the state-of-the-art $O(\Delta \cdot D)^4$-round
algorithm by [Parter, DISC '19]. Note that even for the considerably simpler
setting of edge cuts, currently $\widetilde{O}(D)$-round algorithms are known
only for detecting pairs of cut edges. Our approach is based on employing the
well-known linear graph sketching technique [Ahn, Guha and McGregor, SODA 2012]
along with the heavy-light tree decomposition of [Sleator and Tarjan, STOC
1981]. Combining this with a careful characterization of the survivable
subgraphs, allows us to determine the connectivity of $G \setminus \{x,y\}$ for
every pair $x,y \in V$, using $\widetilde{O}(D)$-rounds. We believe that the
tools provided in this paper are useful for omitting the $\Delta$-dependency
even for larger cut values.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Parter_M/0/1/0/all/0/1">Merav Parter</a>, <a href="http://arxiv.org/find/cs/1/au:+Petruschka_A/0/1/0/all/0/1">Asaf Petruschka</a></p><p>We present near-optimal algorithms for detecting small vertex cuts in the
CONGEST model of distributed computing. Despite extensive research in this
area, our understanding of the vertex connectivity of a graph is still
incomplete, especially in the distributed setting. To this date, all
distributed algorithms for detecting cut vertices suffer from an inherent
dependency in the maximum degree of the graph, $\Delta$. Hence, in particular,
there is no truly sub-linear time algorithm for this problem, not even for
detecting a single cut vertex. We take a new algorithmic approach for vertex
connectivity which allows us to bypass the existing $\Delta$ barrier. As a
warm-up to our approach, we show a simple $\widetilde{O}(D)$-round randomized
algorithm for computing all cut vertices in a $D$-diameter $n$-vertex graph.
This improves upon the $O(D+\Delta/\log n)$-round algorithm of [Pritchard and
Thurimella, ICALP 2008]. Our key technical contribution is an
$\widetilde{O}(D)$-round randomized algorithm for computing all cut pairs in
the graph, improving upon the state-of-the-art $O(\Delta \cdot D)^4$-round
algorithm by [Parter, DISC '19]. Note that even for the considerably simpler
setting of edge cuts, currently $\widetilde{O}(D)$-round algorithms are known
only for detecting pairs of cut edges. Our approach is based on employing the
well-known linear graph sketching technique [Ahn, Guha and McGregor, SODA 2012]
along with the heavy-light tree decomposition of [Sleator and Tarjan, STOC
1981]. Combining this with a careful characterization of the survivable
subgraphs, allows us to determine the connectivity of $G \setminus \{x,y\}$ for
every pair $x,y \in V$, using $\widetilde{O}(D)$-rounds. We believe that the
tools provided in this paper are useful for omitting the $\Delta$-dependency
even for larger cut values.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09606'>Incremental Approximate Maximum Flow in $m^{1/2+o(1)}$ update time</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Gramoz Goranci, Monika Henzinger</p><p>We show an $(1+\epsilon)$-approximation algorithm for maintaining maximum
$s$-$t$ flow under $m$ edge insertions in $m^{1/2+o(1)} \epsilon^{-1/2}$
amortized update time for directed, unweighted graphs. This constitutes the
first sublinear dynamic maximum flow algorithm in general sparse graphs with
arbitrarily good approximation guarantee.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Goranci_G/0/1/0/all/0/1">Gramoz Goranci</a>, <a href="http://arxiv.org/find/cs/1/au:+Henzinger_M/0/1/0/all/0/1">Monika Henzinger</a></p><p>We show an $(1+\epsilon)$-approximation algorithm for maintaining maximum
$s$-$t$ flow under $m$ edge insertions in $m^{1/2+o(1)} \epsilon^{-1/2}$
amortized update time for directed, unweighted graphs. This constitutes the
first sublinear dynamic maximum flow algorithm in general sparse graphs with
arbitrarily good approximation guarantee.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09618'>A (simple) classical algorithm for estimating Betti numbers</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Simon Apers, Sayantan Sen, D&#xe1;niel Szab&#xf3;</p><p>We describe a simple algorithm for estimating the $k$-th normalized Betti
number of a simplicial complex over $n$ elements using the path integral Monte
Carlo method. For a general simplicial complex, the running time of our
algorithm is $n^{O(\frac{1}{\gamma}\log\frac{1}{\varepsilon})}$ with $\gamma$
measuring the spectral gap of the combinatorial Laplacian and $\varepsilon \in
(0,1)$ the additive precision. In the case of a clique complex, the running
time of our algorithm improves to
$(n/\lambda_{\max})^{O(\frac{1}{\gamma}\log\frac{1}{\varepsilon})}$ with
$\lambda_{\max} \geq k$ the maximum eigenvalue of the combinatorial Laplacian.
Our algorithm provides a classical benchmark for a line of quantum algorithms
for estimating Betti numbers, and it matches their running time on clique
complexes when the spectral gap is constant and $k \in \Omega(n)$ or
$\lambda_{\max} \in \Omega(n)$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Apers_S/0/1/0/all/0/1">Simon Apers</a>, <a href="http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1">Sayantan Sen</a>, <a href="http://arxiv.org/find/cs/1/au:+Szabo_D/0/1/0/all/0/1">D&#xe1;niel Szab&#xf3;</a></p><p>We describe a simple algorithm for estimating the $k$-th normalized Betti
number of a simplicial complex over $n$ elements using the path integral Monte
Carlo method. For a general simplicial complex, the running time of our
algorithm is $n^{O(\frac{1}{\gamma}\log\frac{1}{\varepsilon})}$ with $\gamma$
measuring the spectral gap of the combinatorial Laplacian and $\varepsilon \in
(0,1)$ the additive precision. In the case of a clique complex, the running
time of our algorithm improves to
$(n/\lambda_{\max})^{O(\frac{1}{\gamma}\log\frac{1}{\varepsilon})}$ with
$\lambda_{\max} \geq k$ the maximum eigenvalue of the combinatorial Laplacian.
Our algorithm provides a classical benchmark for a line of quantum algorithms
for estimating Betti numbers, and it matches their running time on clique
complexes when the spectral gap is constant and $k \in \Omega(n)$ or
$\lambda_{\max} \in \Omega(n)$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09659'>Minimum Path Cover in Parameterized Linear Time</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Manuel Caceres, Massimo Cairo, Brendan Mumey, Romeo Rizzi, Alexandru I. Tomescu</p><p>A minimum path cover (MPC) of a directed acyclic graph (DAG) $G = (V,E)$ is a
minimum-size set of paths that together cover all the vertices of the DAG.
Computing an MPC is a basic polynomial problem, dating back to Dilworth's and
Fulkerson's results in the 1950s. Since the size $k$ of an MPC (also known as
the width) can be small in practical applications, research has also studied
algorithms whose running time is parameterized on $k$.
</p>
<p>We obtain a new MPC parameterized algorithm for DAGs running in time
$O(k^2|V| + |E|)$. Our algorithm is the first solving the problem in
parameterized linear time. Additionally, we obtain an edge sparsification
algorithm preserving the width of a DAG but reducing $|E|$ to less than $2|V|$.
This algorithm runs in time $O(k^2|V|)$ and requires an MPC of a DAG as input,
thus its total running time is the same as the running time of our MPC
algorithm.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Caceres_M/0/1/0/all/0/1">Manuel Caceres</a>, <a href="http://arxiv.org/find/cs/1/au:+Cairo_M/0/1/0/all/0/1">Massimo Cairo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mumey_B/0/1/0/all/0/1">Brendan Mumey</a>, <a href="http://arxiv.org/find/cs/1/au:+Rizzi_R/0/1/0/all/0/1">Romeo Rizzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomescu_A/0/1/0/all/0/1">Alexandru I. Tomescu</a></p><p>A minimum path cover (MPC) of a directed acyclic graph (DAG) $G = (V,E)$ is a
minimum-size set of paths that together cover all the vertices of the DAG.
Computing an MPC is a basic polynomial problem, dating back to Dilworth's and
Fulkerson's results in the 1950s. Since the size $k$ of an MPC (also known as
the width) can be small in practical applications, research has also studied
algorithms whose running time is parameterized on $k$.
</p>
<p>We obtain a new MPC parameterized algorithm for DAGs running in time
$O(k^2|V| + |E|)$. Our algorithm is the first solving the problem in
parameterized linear time. Additionally, we obtain an edge sparsification
algorithm preserving the width of a DAG but reducing $|E|$ to less than $2|V|$.
This algorithm runs in time $O(k^2|V|)$ and requires an MPC of a DAG as input,
thus its total running time is the same as the running time of our MPC
algorithm.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09665'>Features for the 0-1 knapsack problem based on inclusionwise maximal solutions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jorik Jooken, Pieter Leyman, Patrick De Causmaecker</p><p>Decades of research on the 0-1 knapsack problem led to very efficient
algorithms that are able to quickly solve large problem instances to
optimality. This prompted researchers to also investigate whether relatively
small problem instances exist that are hard for existing solvers and
investigate which features characterize their hardness. Previously the authors
proposed a new class of hard 0-1 knapsack problem instances and demonstrated
that the properties of so-called inclusionwise maximal solutions (IMSs) can be
important hardness indicators for this class. In the current paper, we
formulate several new computationally challenging problems related to the IMSs
of arbitrary 0-1 knapsack problem instances. Based on generalizations of
previous work and new structural results about IMSs, we formulate polynomial
and pseudopolynomial time algorithms for solving these problems. From this we
derive a set of 14 computationally expensive features, which we calculate for
two large datasets on a supercomputer in approximately 540 CPU-hours. We show
that the proposed features contain important information related to the
empirical hardness of a problem instance that was missing in earlier features
from the literature by training machine learning models that can accurately
predict the empirical hardness of a wide variety of 0-1 knapsack problem
instances. Using the instance space analysis methodology, we also show that
hard 0-1 knapsack problem instances are clustered together around a relatively
dense region of the instance space and several features behave differently in
the easy and hard parts of the instance space.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jooken_J/0/1/0/all/0/1">Jorik Jooken</a>, <a href="http://arxiv.org/find/cs/1/au:+Leyman_P/0/1/0/all/0/1">Pieter Leyman</a>, <a href="http://arxiv.org/find/cs/1/au:+Causmaecker_P/0/1/0/all/0/1">Patrick De Causmaecker</a></p><p>Decades of research on the 0-1 knapsack problem led to very efficient
algorithms that are able to quickly solve large problem instances to
optimality. This prompted researchers to also investigate whether relatively
small problem instances exist that are hard for existing solvers and
investigate which features characterize their hardness. Previously the authors
proposed a new class of hard 0-1 knapsack problem instances and demonstrated
that the properties of so-called inclusionwise maximal solutions (IMSs) can be
important hardness indicators for this class. In the current paper, we
formulate several new computationally challenging problems related to the IMSs
of arbitrary 0-1 knapsack problem instances. Based on generalizations of
previous work and new structural results about IMSs, we formulate polynomial
and pseudopolynomial time algorithms for solving these problems. From this we
derive a set of 14 computationally expensive features, which we calculate for
two large datasets on a supercomputer in approximately 540 CPU-hours. We show
that the proposed features contain important information related to the
empirical hardness of a problem instance that was missing in earlier features
from the literature by training machine learning models that can accurately
predict the empirical hardness of a wide variety of 0-1 knapsack problem
instances. Using the instance space analysis methodology, we also show that
hard 0-1 knapsack problem instances are clustered together around a relatively
dense region of the instance space and several features behave differently in
the easy and hard parts of the instance space.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09747'>Extensions of the $(p,q)$-Flexible-Graph-Connectivity model</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ishan Bansal, Joseph Cheriyan, Logan Grout, Sharat Ibrahimpur</p><p>We present approximation algorithms for network design problems in some
models related to the $(p,q)$-FGC model. Adjiashvili, Hommelsheim and
M\"uhlenthaler introduced the model of Flexible Graph Connectivity that we
denote by FGC. Boyd, Cheriyan, Haddadan and Ibrahimpur introduced a
generalization of FGC. Let $p\geq 1$ and $q\geq 0$ be integers. In an instance
of the $(p,q)$-Flexible Graph Connectivity problem, denoted $(p,q)$-FGC, we
have an undirected connected graph $G = (V,E)$, a partition of $E$ into a set
of safe edges and a set of unsafe edges, and nonnegative costs
$c\in\mathbb{R}_{\geq0}^E$ on the edges. A subset $F \subseteq E$ of edges is
feasible for the $(p,q)$-FGC problem if for any set of unsafe edges, $F'$, with
$|F'|\leq q$, the subgraph $(V, F \setminus F')$ is $p$-edge connected. The
algorithmic goal is to find a feasible edge-set $F$ that minimizes $c(F) =
\sum_{e \in F} c_e$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bansal_I/0/1/0/all/0/1">Ishan Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheriyan_J/0/1/0/all/0/1">Joseph Cheriyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Grout_L/0/1/0/all/0/1">Logan Grout</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahimpur_S/0/1/0/all/0/1">Sharat Ibrahimpur</a></p><p>We present approximation algorithms for network design problems in some
models related to the $(p,q)$-FGC model. Adjiashvili, Hommelsheim and
M\"uhlenthaler introduced the model of Flexible Graph Connectivity that we
denote by FGC. Boyd, Cheriyan, Haddadan and Ibrahimpur introduced a
generalization of FGC. Let $p\geq 1$ and $q\geq 0$ be integers. In an instance
of the $(p,q)$-Flexible Graph Connectivity problem, denoted $(p,q)$-FGC, we
have an undirected connected graph $G = (V,E)$, a partition of $E$ into a set
of safe edges and a set of unsafe edges, and nonnegative costs
$c\in\mathbb{R}_{\geq0}^E$ on the edges. A subset $F \subseteq E$ of edges is
feasible for the $(p,q)$-FGC problem if for any set of unsafe edges, $F'$, with
$|F'|\leq q$, the subgraph $(V, F \setminus F')$ is $p$-edge connected. The
algorithmic goal is to find a feasible edge-set $F$ that minimizes $c(F) =
\sum_{e \in F} c_e$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09776'>Cheeger Inequalities for Directed Graphs and Hypergraphs Using Reweighted Eigenvalues</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Lap Chi Lau, Kam Chuen Tung, Robert Wang</p><p>We derive Cheeger inequalities for directed graphs and hypergraphs using the
reweighted eigenvalue approach that was recently developed for vertex expansion
in undirected graphs [OZ22,KLT22,JPV22]. The goal is to develop a new spectral
theory for directed graphs and an alternative spectral theory for hypergraphs.
</p>
<p>The first main result is a Cheeger inequality relating the vertex expansion
$\vec{\psi}(G)$ of a directed graph $G$ to the vertex-capacitated maximum
reweighted second eigenvalue $\vec{\lambda}_2^{v*}$: \[ \vec{\lambda}_2^{v*}
\lesssim \vec{\psi}(G) \lesssim \sqrt{\vec{\lambda}_2^{v*} \cdot \log
(\Delta/\vec{\lambda}_2^{v*})}. \] This provides a combinatorial
characterization of the fastest mixing time of a directed graph by vertex
expansion, and builds a new connection between reweighted eigenvalued, vertex
expansion, and fastest mixing time for directed graphs.
</p>
<p>The second main result is a stronger Cheeger inequality relating the edge
conductance $\vec{\phi}(G)$ of a directed graph $G$ to the edge-capacitated
maximum reweighted second eigenvalue $\vec{\lambda}_2^{e*}$: \[
\vec{\lambda}_2^{e*} \lesssim \vec{\phi}(G) \lesssim \sqrt{\vec{\lambda}_2^{e*}
\cdot \log (1/\vec{\lambda}_2^{e*})}. \] This provides a certificate for a
directed graph to be an expander and a spectral algorithm to find a sparse cut
in a directed graph, playing a similar role as Cheeger's inequality in
certifying graph expansion and in the spectral partitioning algorithm for
undirected graphs.
</p>
<p>We also use this reweighted eigenvalue approach to derive the improved
Cheeger inequality for directed graphs, and furthermore to derive several
Cheeger inequalities for hypergraphs that match and improve the existing
results in [Lou15,CLTZ18]. These are supporting results that this provides a
unifying approach to lift the spectral theory for undirected graphs to more
general settings.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lau_L/0/1/0/all/0/1">Lap Chi Lau</a>, <a href="http://arxiv.org/find/cs/1/au:+Tung_K/0/1/0/all/0/1">Kam Chuen Tung</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Robert Wang</a></p><p>We derive Cheeger inequalities for directed graphs and hypergraphs using the
reweighted eigenvalue approach that was recently developed for vertex expansion
in undirected graphs [OZ22,KLT22,JPV22]. The goal is to develop a new spectral
theory for directed graphs and an alternative spectral theory for hypergraphs.
</p>
<p>The first main result is a Cheeger inequality relating the vertex expansion
$\vec{\psi}(G)$ of a directed graph $G$ to the vertex-capacitated maximum
reweighted second eigenvalue $\vec{\lambda}_2^{v*}$: \[ \vec{\lambda}_2^{v*}
\lesssim \vec{\psi}(G) \lesssim \sqrt{\vec{\lambda}_2^{v*} \cdot \log
(\Delta/\vec{\lambda}_2^{v*})}. \] This provides a combinatorial
characterization of the fastest mixing time of a directed graph by vertex
expansion, and builds a new connection between reweighted eigenvalued, vertex
expansion, and fastest mixing time for directed graphs.
</p>
<p>The second main result is a stronger Cheeger inequality relating the edge
conductance $\vec{\phi}(G)$ of a directed graph $G$ to the edge-capacitated
maximum reweighted second eigenvalue $\vec{\lambda}_2^{e*}$: \[
\vec{\lambda}_2^{e*} \lesssim \vec{\phi}(G) \lesssim \sqrt{\vec{\lambda}_2^{e*}
\cdot \log (1/\vec{\lambda}_2^{e*})}. \] This provides a certificate for a
directed graph to be an expander and a spectral algorithm to find a sparse cut
in a directed graph, playing a similar role as Cheeger's inequality in
certifying graph expansion and in the spectral partitioning algorithm for
undirected graphs.
</p>
<p>We also use this reweighted eigenvalue approach to derive the improved
Cheeger inequality for directed graphs, and furthermore to derive several
Cheeger inequalities for hypergraphs that match and improve the existing
results in [Lou15,CLTZ18]. These are supporting results that this provides a
unifying approach to lift the spectral theory for undirected graphs to more
general settings.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-18T01:30:00Z">Friday, November 18 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Thursday, November 17
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://gilkalai.wordpress.com/2022/11/17/amazing-justin-gilmer-gave-a-constant-lower-bound-for-the-union-closed-sets-conjecture/'>Amazing: Justin Gilmer gave a constant lower bound for the union-closed sets conjecture</a></h3>
        <p class='tr-article-feed'>from <a href='https://gilkalai.wordpress.com'>Gil Kalai</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Frankl&#8217;s conjecture (aka the union closed sets conjecture) asserts that if is a family of subsets of [n] (=: ) which is closed under union then there is an element such that Justin Gilmer just proved an amazing weaker form &#8230; Continue reading &#8594;
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Frankl&#8217;s conjecture (aka the union closed sets conjecture) asserts that if <img src="https://s0.wp.com/latex.php?latex=%5Ccal+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Ccal+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ccal+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;cal F" class="latex" /> is a family of subsets of [n] (=: <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C2%2C%5Cdots%2Cn+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5C%7B1%2C2%2C%5Cdots%2Cn+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5C%7B1%2C2%2C%5Cdots%2Cn+%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;{1,2,&#92;dots,n &#92;}" class="latex" />) which is closed under union then there is an element <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k" class="latex" /> such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%7C%5C%7BS+%5Cin+%7B%5Ccal+F%7D%3A+k+%5Cin+S%5C%7D%7C+%5Cge+%5Cfrac+%7B1%7D%7B2%7D%7C%7B%5Ccal+F%7D%7C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7C%5C%7BS+%5Cin+%7B%5Ccal+F%7D%3A+k+%5Cin+S%5C%7D%7C+%5Cge+%5Cfrac+%7B1%7D%7B2%7D%7C%7B%5Ccal+F%7D%7C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7C%5C%7BS+%5Cin+%7B%5Ccal+F%7D%3A+k+%5Cin+S%5C%7D%7C+%5Cge+%5Cfrac+%7B1%7D%7B2%7D%7C%7B%5Ccal+F%7D%7C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="|&#92;{S &#92;in {&#92;cal F}: k &#92;in S&#92;}| &#92;ge &#92;frac {1}{2}|{&#92;cal F}|." class="latex" /></p>
<p>Justin Gilmer just proved an amazing weaker form of the conjecture asserting that there always exists an element <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k" class="latex" /> such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%7C%5C%7BS+%5Cin+%7B%5Ccal+F%7D%3A+k+%5Cin+S%5C%7D%7C+%5Cge%C2%A0+0.01+%7C%7B%5Ccal+F%7D%7C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7C%5C%7BS+%5Cin+%7B%5Ccal+F%7D%3A+k+%5Cin+S%5C%7D%7C+%5Cge%C2%A0+0.01+%7C%7B%5Ccal+F%7D%7C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7C%5C%7BS+%5Cin+%7B%5Ccal+F%7D%3A+k+%5Cin+S%5C%7D%7C+%5Cge%C2%A0+0.01+%7C%7B%5Ccal+F%7D%7C.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="|&#92;{S &#92;in {&#92;cal F}: k &#92;in S&#92;}| &#92;geÂ  0.01 |{&#92;cal F}|." class="latex" /></p>
<p>This is am amazing progress! Congratulations, Justin.</p>
<p>The breakthrough paper, just posted on the arXiv is:</p>
<p><a href="https://arxiv.org/abs/2211.09055">A constant lower bound for the union-closed sets conjecture</a> by Justin Gilmer</p>
<p><strong>Abstract:</strong> We show that for any union-closed familyÂ  <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D+%5Csubseteq+2%5E%7B%5Bn%5D%7D%2C+%5Cmathcal%7BF%7D+%5Cneq+%5C%7B%5Cemptyset%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D+%5Csubseteq+2%5E%7B%5Bn%5D%7D%2C+%5Cmathcal%7BF%7D+%5Cneq+%5C%7B%5Cemptyset%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D+%5Csubseteq+2%5E%7B%5Bn%5D%7D%2C+%5Cmathcal%7BF%7D+%5Cneq+%5C%7B%5Cemptyset%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathcal{F} &#92;subseteq 2^{[n]}, &#92;mathcal{F} &#92;neq &#92;{&#92;emptyset&#92;}" class="latex" /> there exists an <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=i+%5Cin+%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=i+%5Cin+%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="i &#92;in [n]" class="latex" />Â  which is contained in a <span id="MathJax-Element-3-Frame" class="MathJax"><span id="MathJax-Span-29" class="math"><span id="MathJax-Span-30" class="mrow"><span id="MathJax-Span-31" class="mn">0.01</span></span></span></span> fraction of the sets in <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathcal+F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathcal F" class="latex" />.</p>
<p>This is the first known constant lower bound, and improves upon the <img src="https://s0.wp.com/latex.php?latex=%5COmega%28%5Clog_2%28%5Cmathcal%7BF%7D%7C%29%5E%7B-1%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5COmega%28%5Clog_2%28%5Cmathcal%7BF%7D%7C%29%5E%7B-1%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5COmega%28%5Clog_2%28%5Cmathcal%7BF%7D%7C%29%5E%7B-1%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;Omega(&#92;log_2(&#92;mathcal{F}|)^{-1})" class="latex" /> bounds of Knill and WÃ³jick.</p>
<p>Our result follows from an information theoretic strengthening of the conjecture. Specifically, we show that if <img src="https://s0.wp.com/latex.php?latex=A%2CB&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=A%2CB&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A%2CB&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="A,B" class="latex" /> are independent samples from a distribution over subsets of <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="[n]" class="latex" />Â  such that <img src="https://s0.wp.com/latex.php?latex=Pr%5Bi+%5Cin+A%5D+%3C+0.01&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=Pr%5Bi+%5Cin+A%5D+%3C+0.01&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=Pr%5Bi+%5Cin+A%5D+%3C+0.01&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="Pr[i &#92;in A] &lt; 0.01" class="latex" /> for all <span id="MathJax-Element-9-Frame" class="MathJax"><span id="MathJax-Span-83" class="math"><span id="MathJax-Span-84" class="mrow"><span id="MathJax-Span-85" class="mi"><img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="i" class="latex" /></span></span></span></span> and <img src="https://s0.wp.com/latex.php?latex=H%28A%29%3E0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=H%28A%29%3E0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=H%28A%29%3E0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="H(A)&gt;0" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=H%28A+%5Ccup+B%29%3E+H%28A%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=H%28A+%5Ccup+B%29%3E+H%28A%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=H%28A+%5Ccup+B%29%3E+H%28A%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="H(A &#92;cup B)&gt; H(A)" class="latex" />.</p>
<p>______</p>
<p>Mike Saks who first told me about the breakthrough wrote &#8220;the bound comes from a simple clever idea (using information theory) and 5 pages of gentle technical calculations.&#8221; (I thank Mike, Ryan <span class="gI"><span class="qu" role="gridcell"><span class="gD">Alweiss, and Nati Linial who wrote me about it.) </span></span></span></p>
<p>We mentioned Frankl&#8217;s conjecture several times including <a href="https://gilkalai.wordpress.com/2008/04/29/hello-world/">here</a>, <a href="https://gilkalai.wordpress.com/2017/12/26/ilam-karpas-frankls-conjecture-for-large-families/">here</a>, <a href="https://gilkalai.wordpress.com/2018/03/09/frankls-conjecture-for-large-families-ilan-karpas-proof/">here</a>, and <a href="https://gilkalai.wordpress.com/2021/01/29/possible-future-polymath-projects-2009-2021/">here</a>. <a href="https://gowers.wordpress.com/category/polymath11/">Polymath11</a> on Tim Gowers&#8217;s blog was devoted to the conjecture. Below the fold: What it will take to prove the conjecture in its full strength and another beautiful conjecture by Peter Frankl.</p>
<p><span id="more-23540"></span></p>
<h3>What is the limit of Gilmer&#8217;s method and what it will take to prove the Frankl conjecture</h3>
<p>Justin Gilmer&#8217;s mentions that proving a tight bout for Lemma 1 in the paper will push the 0.01 bound to <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B3-%5Csqrt+5%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cfrac%7B3-%5Csqrt+5%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B3-%5Csqrt+5%7D%7B2%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;frac{3-&#92;sqrt 5}{2}" class="latex" />=<span id="cwos" class="qv3Wpe" dir="ltr">0.381966&#8230; . He also presents an appealing information-theoretic strengthening of the conjecture which may consist of a path toward a proof. </span></p>
<h3>Another beautiful conjecture by Peter Frankl</h3>
<p>To face a possible risk that Frankl&#8217;s &#8220;union closed&#8221; conjecture will be solved here is another beautiful conjecture by Peter Frankl.</p>
<p>A family of sets is convex if whenever <img src="https://s0.wp.com/latex.php?latex=A+%5Csubset+B+%5Csubset+C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=A+%5Csubset+B+%5Csubset+C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A+%5Csubset+B+%5Csubset+C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="A &#92;subset B &#92;subset C" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=A%2CC+%5Cin+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=A%2CC+%5Cin+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A%2CC+%5Cin+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="A,C &#92;in {&#92;cal F}" class="latex" /> then also <img src="https://s0.wp.com/latex.php?latex=B+%5Cin+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=B+%5Cin+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=B+%5Cin+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="B &#92;in {&#92;cal F}" class="latex" />.</p>
<p>Conjecture (<strong>P. Frankl</strong>):Â  Let <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="{&#92;cal F}" class="latex" /> be a convex family of subsets of <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Bn%5D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="[n]" class="latex" />. Then there exists an antichain <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+G%7D+%5Csubset+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Ccal+G%7D+%5Csubset+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Ccal+G%7D+%5Csubset+%7B%5Ccal+F%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="{&#92;cal G} &#92;subset {&#92;cal F}" class="latex" /> such that</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%7C%7B%5Ccal+G%7D%7C%2F%7C%7B%5Ccal+F%7D%7C+%5Cge+%7B%7Bn%7D+%5Cchoose+%7B%5Bn%2F2%5D%7D%7D%2F2%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7C%7B%5Ccal+G%7D%7C%2F%7C%7B%5Ccal+F%7D%7C+%5Cge+%7B%7Bn%7D+%5Cchoose+%7B%5Bn%2F2%5D%7D%7D%2F2%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7C%7B%5Ccal+G%7D%7C%2F%7C%7B%5Ccal+F%7D%7C+%5Cge+%7B%7Bn%7D+%5Cchoose+%7B%5Bn%2F2%5D%7D%7D%2F2%5En.&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="|{&#92;cal G}|/|{&#92;cal F}| &#92;ge {{n} &#92;choose {[n/2]}}/2^n." class="latex" /></p>
<p class="authors">By Gil Kalai</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T20:47:56Z">Thursday, November 17 2022, 20:47</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://blog.computationalcomplexity.org/2022/11/fall-jobs-post-2022.html'>Fall Jobs Post 2022</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>In the fall I try to make my predictions on the faculty job market for the spring. The outlook this year is hazy as we have two forces pushing in opposite directions.&nbsp;</p><p>Most of the largest tech companies are having layoffs and hiring freezes amidst a recession, higher expenses and a drop in revenue from cloud and advertising. Meanwhile computing has never had a more exciting (or scary) year of advances, particularly in generative AI. I can't remember such a dichotomy in the past. In the downturn after the 2008 financial crisis computing wasn't particularly exciting as the cloud, smart phones and machine learning were then just nascent technologies.</p><p>We'll probably have more competition in the academic job market as many new PhDs may decide to look at academic positions because of limited opportunities in large tech companies. We might even see a reverse migration from industry to academia from those who now might see universities as a safe haven.</p><p>What about the students? Will they still come in droves driven by the excitement in computing or get scared off by the downturn in the tech industry. They shouldn't worry--the market should turn around by the time they graduate and even today there are plenty of tech jobs in smaller and midsize tech companies as well as companies that deal with data, which is pretty much every company.</p><p>But perception matters more than reality. If students do stay away that might reduce pressure to grow CS departments.</p><p>Onto my usual advice. Give yourself a good virtual face. Have a well-designed web page with access to all your job materials and papers. Maintain your Google Scholar page. Add yourself to the CRA's&nbsp;CV database. Find a way to stand out, perhaps a short video describing your research.&nbsp;</p><p>Best source for finding jobs are the ads from the&nbsp;CRA&nbsp;and the&nbsp;ACM. For theoretical computer science specific postdoc and faculty positions check out&nbsp;TCS Jobs&nbsp;and&nbsp;Theory Announcements. If you have jobs to announce, please post to the above and/or feel free to leave a comment on this post. Even if you don't see an ad for a specific school they may still be hiring, check out their website or email someone at the department. You'll never know if you don't ask.</p><p>By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>In the fall I try to make my predictions on the faculty job market for the spring. The outlook this year is hazy as we have two forces pushing in opposite directions.&nbsp;</p><p>Most of the largest tech companies are having layoffs and hiring freezes amidst a recession, higher expenses and a drop in revenue from cloud and advertising. Meanwhile computing has never had a more exciting (or scary) year of advances, particularly in generative AI. I can't remember such a dichotomy in the past. In the downturn after the 2008 financial crisis computing wasn't particularly exciting as the cloud, smart phones and machine learning were then just nascent technologies.</p><p>We'll probably have more competition in the academic job market as many new PhDs may decide to look at academic positions because of limited opportunities in large tech companies. We might even see a reverse migration from industry to academia from those who now might see universities as a safe haven.</p><p>What about the students? Will they still come in droves driven by the excitement in computing or get scared off by the downturn in the tech industry. They shouldn't worry--the market should turn around by the time they graduate and even today there are plenty of tech jobs in smaller and midsize tech companies as well as companies that deal with data, which is pretty much every company.</p><p>But perception matters more than reality. If students do stay away that might reduce pressure to grow CS departments.</p><p>Onto my usual advice. Give yourself a good virtual face. Have a well-designed web page with access to all your job materials and papers. Maintain your Google Scholar page. Add yourself to the CRA's&nbsp;<a href="https://cra.org/cv-database/">CV database</a>. Find a way to stand out, perhaps a short video describing your research.&nbsp;</p><p>Best source for finding jobs are the ads from the&nbsp;<a href="https://cra.org/ads/">CRA</a>&nbsp;and the&nbsp;<a href="https://jobs.acm.org/">ACM</a>. For theoretical computer science specific postdoc and faculty positions check out&nbsp;<a href="https://cstheory-jobs.org/">TCS Jobs</a>&nbsp;and&nbsp;<a href="http://dmatheorynet.blogspot.com/">Theory Announcements</a>. If you have jobs to announce, please post to the above and/or feel free to leave a comment on this post. Even if you don't see an ad for a specific school they may still be hiring, check out their website or email someone at the department. You'll never know if you don't ask.</p><p class="authors">By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T14:44:00Z">Thursday, November 17 2022, 14:44</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/17/senior-faculty-position-at-williams-college-apply-by-december-1-2022/'>Senior Faculty Position at Williams College (apply by December 1, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The Department of Computer Science at Williams College invites applications for a tenured faculty position at the associate or full professor level beginning July 1, 2023. We welcome candidates from all areas of computer science who can contribute to the vibrancy of our academic community through their research, teaching, and service. Website: apply.interfolio.com/111662 Email: cshiring@williams.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The Department of Computer Science at Williams College invites applications for a tenured faculty position at the associate or full professor level beginning July 1, 2023. We welcome candidates from all areas of computer science who can contribute to the vibrancy of our academic community through their research, teaching, and service.</p>
<p>Website: <a href="https://apply.interfolio.com/111662">https://apply.interfolio.com/111662</a><br />
Email: cshiring@williams.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T14:11:06Z">Thursday, November 17 2022, 14:11</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/17/postdocs-at-max-planck-institute-for-informatics-apply-by-december-31-2022/'>Postdocs at Max Planck Institute for Informatics (apply by December 31, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We are looking for applicants from all areas of algorithms and complexity, including related areas like mathematical optimization, distributed computing, and algorithms engineering. Postdoctoral fellowships are available at the algorithms and complexity department for two years through the Guest Program of our institute. Website: www.mpi-inf.mpg.de/d1postdoc Email: d1office@mpi-inf.mpg.de
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>We are looking for applicants from all areas of algorithms and complexity, including related areas like mathematical optimization, distributed computing, and algorithms engineering. Postdoctoral fellowships are available at the algorithms and complexity department for two years through the Guest Program of our institute.</p>
<p>Website: <a href="http://www.mpi-inf.mpg.de/d1postdoc">http://www.mpi-inf.mpg.de/d1postdoc</a><br />
Email: d1office@mpi-inf.mpg.de</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T13:29:17Z">Thursday, November 17 2022, 13:29</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/17/postdoc-position-in-algorithms-at-university-of-warwick-apply-by-december-6-2022/'>Postdoc position in algorithms at University of Warwick (apply by December 6, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          In connection with a research grant of Dr. Ramanujan Sridharan and Prof. Graham Cormode at University of Warwick, UK, we are seeking excellent candidates for a postdoctoral fellow position in the area of design and analysis of parameterized and approximation algorithms. The position is for 18 months and start date can be negotiated (preferably by [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>In connection with a research grant of Dr. Ramanujan Sridharan and Prof. Graham Cormode at University of Warwick, UK, we are seeking excellent candidates for a postdoctoral fellow position in the area of design and analysis of parameterized and approximation algorithms.<br />
The position is for 18 months and start date can be negotiated (preferably by March 2023).</p>
<p>Website: <a href="https://tinyurl.com/ksz8rjfa">https://tinyurl.com/ksz8rjfa</a><br />
Email: r.maadapuzhi-sridharan@warwick.ac.uk</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T08:54:03Z">Thursday, November 17 2022, 08:54</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=6813'>Sneerers</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          In the past few weeks, I&#8217;ve learned two ways to think about online sneerers that have been helping me tremendously, and that I wanted to share in case they&#8217;re helpful to others: First, they&#8217;re like a train in a movie that&#8217;s barreling directly towards the camera. If you haven&#8217;t yet internalized how the medium works, [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>In the past few weeks, I&#8217;ve learned two ways to think about online sneerers that have been helping me tremendously, and that I wanted to share in case they&#8217;re helpful to others:</p>



<p>First, they&#8217;re like a train in a movie that&#8217;s barreling directly towards the camera. If you haven&#8217;t yet internalized how the medium works, absolutely terrifying! Run from the theater! If you <em>have</em> internalized it, though, you can sit and watch without even flinching.</p>



<p>Second, the sneerers are like alligators&#8212;and about as likely to be moved by your appeals to reason and empathy. But if, like me, you&#8217;re lucky enough to have a loving family, friends, colleagues, and a nigh-uncancellable career, then it&#8217;s as though you&#8217;re standing on a bridge high above, looking down at the gators as they snap their jaws at you uselessly. There&#8217;s <em>really</em> no moral or intellectual obligation to go down to the swamp to wrestle them.  If they mean to attack you, let them at least come up to the bridge.</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T01:48:08Z">Thursday, November 17 2022, 01:48</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08524'>Complexity Results for Implication Bases of Convex Geometries</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Todd Bichoupan</p><p>A convex geometry is finite zero-closed closure system that satisfies the
anti-exchange property. Complexity results are given for two open problems
related to representations of convex geometries using implication bases. In
particular, the problem of optimizing an implication basis for a convex
geometry is shown to be NP-hard by establishing a reduction from the minimum
cardinality generator problem for general closure systems. Furthermore, even
the problem of deciding whether an implication basis defines a convex geometry
is shown to be co-NP-complete by a reduction from the Boolean tautology
problem.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bichoupan_T/0/1/0/all/0/1">Todd Bichoupan</a></p><p>A convex geometry is finite zero-closed closure system that satisfies the
anti-exchange property. Complexity results are given for two open problems
related to representations of convex geometries using implication bases. In
particular, the problem of optimizing an implication basis for a convex
geometry is shown to be NP-hard by establishing a reduction from the minimum
cardinality generator problem for general closure systems. Furthermore, even
the problem of deciding whether an implication basis defines a convex geometry
is shown to be co-NP-complete by a reduction from the Boolean tautology
problem.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T01:30:00Z">Thursday, November 17 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08563'>The wrong direction of Jensen's inequality is algorithmically right</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Or Zamir</p><p>Let $\mathcal{A}$ be an algorithm with expected running time $e^X$,
conditioned on the value of some random variable $X$. We construct an algorithm
$\mathcal{A'}$ with expected running time $O(e^{E[X]})$, that fully executes
$\mathcal{A}$. In particular, an algorithm whose running time is a random
variable $T$ can be converted to one with expected running time $O(e^{E[\ln
T]})$, which is never worse than $O(E[T])$. No information about the
distribution of $X$ is required for the construction of $\mathcal{A}'$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Zamir_O/0/1/0/all/0/1">Or Zamir</a></p><p>Let $\mathcal{A}$ be an algorithm with expected running time $e^X$,
conditioned on the value of some random variable $X$. We construct an algorithm
$\mathcal{A'}$ with expected running time $O(e^{E[X]})$, that fully executes
$\mathcal{A}$. In particular, an algorithm whose running time is a random
variable $T$ can be converted to one with expected running time $O(e^{E[\ln
T]})$, which is never worse than $O(E[T])$. No information about the
distribution of $X$ is required for the construction of $\mathcal{A}'$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T01:30:00Z">Thursday, November 17 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09106'>The Exact Bipartite Matching Polytope Has Exponential Extension Complexity</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xinrui Jia, Ola Svensson, Weiqiang Yuan</p><p>Given a graph with edges colored red or blue and an integer $k$, the exact
perfect matching problem asks if there exists a perfect matching with exactly
$k$ red edges. There exists a randomized polylogarithmic-time parallel
algorithm to solve this problem, dating back to the eighties, but no
deterministic polynomial-time algorithm is known, even for bipartite graphs. In
this paper we show that there is no sub-exponential sized linear program that
can describe the convex hull of exact matchings in bipartite graphs. In fact,
we prove something stronger, that there is no sub-exponential sized linear
program to describe the convex hull of perfect matchings with an odd number of
red edges.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1">Xinrui Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Svensson_O/0/1/0/all/0/1">Ola Svensson</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1">Weiqiang Yuan</a></p><p>Given a graph with edges colored red or blue and an integer $k$, the exact
perfect matching problem asks if there exists a perfect matching with exactly
$k$ red edges. There exists a randomized polylogarithmic-time parallel
algorithm to solve this problem, dating back to the eighties, but no
deterministic polynomial-time algorithm is known, even for bipartite graphs. In
this paper we show that there is no sub-exponential sized linear program that
can describe the convex hull of exact matchings in bipartite graphs. In fact,
we prove something stronger, that there is no sub-exponential sized linear
program to describe the convex hull of perfect matchings with an odd number of
red edges.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T01:30:00Z">Thursday, November 17 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09075'>Keeping it sparse: Computing Persistent Homology revised</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ulrich Bauer, Talha Bin Masood, Barbara Giunti, Guillaume Houry, Michael Kerber, Abhishek Rathod</p><p>In this work, we study several variants of matrix reduction via Gaussian
elimination that try to keep the reduced matrix sparse. The motivation comes
from the growing field of topological data analysis where matrix reduction is
the major subroutine to compute barcodes. We propose two novel variants of the
standard algorithm, called swap and retrospective reductions, which improve
upon state-of-the-art techniques on several examples in practice. We also
present novel output-sensitive bounds for the retrospective variant which
better explain the discrepancy between the cubic worst-case complexity bound
and the almost linear practical behavior of matrix reduction. Finally, we
provide several constructions on which one of the variants performs strictly
better than the others.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bauer_U/0/1/0/all/0/1">Ulrich Bauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Masood_T/0/1/0/all/0/1">Talha Bin Masood</a>, <a href="http://arxiv.org/find/cs/1/au:+Giunti_B/0/1/0/all/0/1">Barbara Giunti</a>, <a href="http://arxiv.org/find/cs/1/au:+Houry_G/0/1/0/all/0/1">Guillaume Houry</a>, <a href="http://arxiv.org/find/cs/1/au:+Kerber_M/0/1/0/all/0/1">Michael Kerber</a>, <a href="http://arxiv.org/find/cs/1/au:+Rathod_A/0/1/0/all/0/1">Abhishek Rathod</a></p><p>In this work, we study several variants of matrix reduction via Gaussian
elimination that try to keep the reduced matrix sparse. The motivation comes
from the growing field of topological data analysis where matrix reduction is
the major subroutine to compute barcodes. We propose two novel variants of the
standard algorithm, called swap and retrospective reductions, which improve
upon state-of-the-art techniques on several examples in practice. We also
present novel output-sensitive bounds for the retrospective variant which
better explain the discrepancy between the cubic worst-case complexity bound
and the almost linear practical behavior of matrix reduction. Finally, we
provide several constructions on which one of the variants performs strictly
better than the others.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T01:30:00Z">Thursday, November 17 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09101'>Comparative Learning: A Sample Complexity Theory for Two Hypothesis Classes</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Lunjia Hu, Charlotte Peale</p><p>In many learning theory problems, a central role is played by a hypothesis
class: we might assume that the data is labeled according to a hypothesis in
the class (usually referred to as the realizable setting), or we might evaluate
the learned model by comparing it with the best hypothesis in the class (the
agnostic setting).
</p>
<p>Taking a step beyond these classic setups that involve only a single
hypothesis class, we introduce comparative learning as a combination of the
realizable and agnostic settings in PAC learning: given two binary hypothesis
classes $S$ and $B$, we assume that the data is labeled according to a
hypothesis in the source class $S$ and require the learned model to achieve an
accuracy comparable to the best hypothesis in the benchmark class $B$. Even
when both $S$ and $B$ have infinite VC dimensions, comparative learning can
still have a small sample complexity. We show that the sample complexity of
comparative learning is characterized by the mutual VC dimension
$\mathsf{VC}(S,B)$ which we define to be the maximum size of a subset shattered
by both $S$ and $B$. We also show a similar result in the online setting, where
we give a regret characterization in terms of the mutual Littlestone dimension
$\mathsf{Ldim}(S,B)$. These results also hold for partial hypotheses.
</p>
<p>We additionally show that the insights necessary to characterize the sample
complexity of comparative learning can be applied to characterize the sample
complexity of realizable multiaccuracy and multicalibration using the mutual
fat-shattering dimension, an analogue of the mutual VC dimension for
real-valued hypotheses. This not only solves an open problem proposed by Hu,
Peale, Reingold (2022), but also leads to independently interesting results
extending classic ones about regression, boosting, and covering number to our
two-hypothesis-class setting.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Lunjia Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peale_C/0/1/0/all/0/1">Charlotte Peale</a></p><p>In many learning theory problems, a central role is played by a hypothesis
class: we might assume that the data is labeled according to a hypothesis in
the class (usually referred to as the realizable setting), or we might evaluate
the learned model by comparing it with the best hypothesis in the class (the
agnostic setting).
</p>
<p>Taking a step beyond these classic setups that involve only a single
hypothesis class, we introduce comparative learning as a combination of the
realizable and agnostic settings in PAC learning: given two binary hypothesis
classes $S$ and $B$, we assume that the data is labeled according to a
hypothesis in the source class $S$ and require the learned model to achieve an
accuracy comparable to the best hypothesis in the benchmark class $B$. Even
when both $S$ and $B$ have infinite VC dimensions, comparative learning can
still have a small sample complexity. We show that the sample complexity of
comparative learning is characterized by the mutual VC dimension
$\mathsf{VC}(S,B)$ which we define to be the maximum size of a subset shattered
by both $S$ and $B$. We also show a similar result in the online setting, where
we give a regret characterization in terms of the mutual Littlestone dimension
$\mathsf{Ldim}(S,B)$. These results also hold for partial hypotheses.
</p>
<p>We additionally show that the insights necessary to characterize the sample
complexity of comparative learning can be applied to characterize the sample
complexity of realizable multiaccuracy and multicalibration using the mutual
fat-shattering dimension, an analogue of the mutual VC dimension for
real-valued hypotheses. This not only solves an open problem proposed by Hu,
Peale, Reingold (2022), but also leads to independently interesting results
extending classic ones about regression, boosting, and covering number to our
two-hypothesis-class setting.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T01:30:00Z">Thursday, November 17 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08586'>Bandit Algorithms for Prophet Inequality and Pandora's Box</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Khashayar Gatmiry, Thomas Kesselheim, Sahil Singla, Yifan Wang</p><p>The Prophet Inequality and Pandora's Box problems are fundamental stochastic
problem with applications in Mechanism Design, Online Algorithms, Stochastic
Optimization, Optimal Stopping, and Operations Research. A usual assumption in
these works is that the probability distributions of the $n$ underlying random
variables are given as input to the algorithm. Since in practice these
distributions need to be learned, we initiate the study of such stochastic
problems in the Multi-Armed Bandits model.
</p>
<p>In the Multi-Armed Bandits model we interact with $n$ unknown distributions
over $T$ rounds: in round $t$ we play a policy $x^{(t)}$ and receive a partial
(bandit) feedback on the performance of $x^{(t)}$. The goal is to minimize the
regret, which is the difference over $T$ rounds in the total value of the
optimal algorithm that knows the distributions vs. the total value of our
algorithm that learns the distributions from the partial feedback. Our main
results give near-optimal $\tilde{O}(\mathsf{poly}(n)\sqrt{T})$ total regret
algorithms for both Prophet Inequality and Pandora's Box.
</p>
<p>Our proofs proceed by maintaining confidence intervals on the unknown indices
of the optimal policy. The exploration-exploitation tradeoff prevents us from
directly refining these confidence intervals, so the main technique is to
design a regret upper bound that is learnable while playing low-regret Bandit
policies.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gatmiry_K/0/1/0/all/0/1">Khashayar Gatmiry</a>, <a href="http://arxiv.org/find/cs/1/au:+Kesselheim_T/0/1/0/all/0/1">Thomas Kesselheim</a>, <a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1">Sahil Singla</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yifan Wang</a></p><p>The Prophet Inequality and Pandora's Box problems are fundamental stochastic
problem with applications in Mechanism Design, Online Algorithms, Stochastic
Optimization, Optimal Stopping, and Operations Research. A usual assumption in
these works is that the probability distributions of the $n$ underlying random
variables are given as input to the algorithm. Since in practice these
distributions need to be learned, we initiate the study of such stochastic
problems in the Multi-Armed Bandits model.
</p>
<p>In the Multi-Armed Bandits model we interact with $n$ unknown distributions
over $T$ rounds: in round $t$ we play a policy $x^{(t)}$ and receive a partial
(bandit) feedback on the performance of $x^{(t)}$. The goal is to minimize the
regret, which is the difference over $T$ rounds in the total value of the
optimal algorithm that knows the distributions vs. the total value of our
algorithm that learns the distributions from the partial feedback. Our main
results give near-optimal $\tilde{O}(\mathsf{poly}(n)\sqrt{T})$ total regret
algorithms for both Prophet Inequality and Pandora's Box.
</p>
<p>Our proofs proceed by maintaining confidence intervals on the unknown indices
of the optimal policy. The exploration-exploitation tradeoff prevents us from
directly refining these confidence intervals, so the main technique is to
design a regret upper bound that is learnable while playing low-regret Bandit
policies.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T01:30:00Z">Thursday, November 17 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08605'>A Dichotomy Theorem for Linear Time Homomorphism Orbit Counting in Bounded Degeneracy Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Daniel Paul-Pena, C. Seshadhri</p><p>Counting the number of homomorphisms of a pattern graph H in a large input
graph G is a fundamental problem in computer science. There are myriad
applications of this problem in databases, graph algorithms, and network
science. Often, we need more than just the total count. Especially in large
network analysis, we wish to compute, for each vertex v of G, the number of
H-homomorphisms that v participates in. This problem is referred to as
homomorphism orbit counting, as it relates to the orbits of vertices of H under
its automorphisms.
</p>
<p>Given the need for fast algorithms for this problem, we study when
near-linear time algorithms are possible. A natural restriction is to assume
that the input graph G has bounded degeneracy, a commonly observed property in
modern massive networks. Can we characterize the patterns H for which
homomorphism orbit counting can be done in linear time?
</p>
<p>We discover a dichotomy theorem that resolves this problem. For pattern H,
let l be the length of the longest induced path between any two vertices of the
same orbit (under the automorphisms of H). If l &lt;= 5, then H-homomorphism orbit
counting can be done in linear time for bounded degeneracy graphs. If l &gt; 5,
then (assuming fine-grained complexity conjectures) there is no near-linear
time algorithm for this problem. We build on existing work on dichotomy
theorems for counting the total H-homomorphism count. Somewhat surprisingly,
there exist (and we characterize) patterns H for which the total homomorphism
count can be computed in linear time, but the corresponding orbit counting
problem cannot be done in near-linear time.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Paul_Pena_D/0/1/0/all/0/1">Daniel Paul-Pena</a>, <a href="http://arxiv.org/find/cs/1/au:+Seshadhri_C/0/1/0/all/0/1">C. Seshadhri</a></p><p>Counting the number of homomorphisms of a pattern graph H in a large input
graph G is a fundamental problem in computer science. There are myriad
applications of this problem in databases, graph algorithms, and network
science. Often, we need more than just the total count. Especially in large
network analysis, we wish to compute, for each vertex v of G, the number of
H-homomorphisms that v participates in. This problem is referred to as
homomorphism orbit counting, as it relates to the orbits of vertices of H under
its automorphisms.
</p>
<p>Given the need for fast algorithms for this problem, we study when
near-linear time algorithms are possible. A natural restriction is to assume
that the input graph G has bounded degeneracy, a commonly observed property in
modern massive networks. Can we characterize the patterns H for which
homomorphism orbit counting can be done in linear time?
</p>
<p>We discover a dichotomy theorem that resolves this problem. For pattern H,
let l be the length of the longest induced path between any two vertices of the
same orbit (under the automorphisms of H). If l &lt;= 5, then H-homomorphism orbit
counting can be done in linear time for bounded degeneracy graphs. If l &gt; 5,
then (assuming fine-grained complexity conjectures) there is no near-linear
time algorithm for this problem. We build on existing work on dichotomy
theorems for counting the total H-homomorphism count. Somewhat surprisingly,
there exist (and we characterize) patterns H for which the total homomorphism
count can be computed in linear time, but the corresponding orbit counting
problem cannot be done in near-linear time.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T01:30:00Z">Thursday, November 17 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08711'>Beyond Worst-Case Budget-Feasible Mechanism Design</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Aviad Rubinstein, Junyao Zhao</p><p>Motivated by large-market applications such as crowdsourcing, we revisit the
problem of budget-feasible mechanism design under a "small-bidder assumption".
Anari, Goel, and Nikzad (2018) gave a mechanism that has optimal competitive
ratio $1-1/e$ on worst-case instances. However, we observe that on many
realistic instances, their mechanism is significantly outperformed by a simpler
open clock auction by Ensthaler and Giebe (2014), although the open clock
auction only achieves competitive ratio $1/2$ in the worst case. Is there a
mechanism that gets the best of both worlds, i.e., a mechanism that is
worst-case optimal and performs favorably on realistic instances?
</p>
<p>Our first main result is the design and the analysis of a natural mechanism
that gives an affirmative answer to our question above: (i) We prove that on
every instance, our mechanism performs at least as good as all uniform
mechanisms, including Anari, Goel, and Nikzad's and Ensthaler and Giebe's
mechanisms. (ii) Moreover, we empirically evaluate our mechanism on various
realistic instances and observe that it beats the worst-case $1-1/e$
competitive ratio by a large margin and compares favorably to both mechanisms
mentioned above.
</p>
<p>Our second main result is more interesting in theory: We show that in the
semi-adversarial model of budget-smoothed analysis, where the adversary designs
a single worst-case market for a distribution of budgets, our mechanism is
optimal among all (including non-uniform) mechanisms; furthermore our mechanism
guarantees a strictly better-than-$(1-1/e)$ expected competitive ratio for any
non-trivial budget distribution regardless of the market. We complement the
positive result with a characterization of the worst-case markets for any given
budget distribution and prove a fairly robust hardness result that holds
against any budget distribution and any mechanism.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Rubinstein_A/0/1/0/all/0/1">Aviad Rubinstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Junyao Zhao</a></p><p>Motivated by large-market applications such as crowdsourcing, we revisit the
problem of budget-feasible mechanism design under a "small-bidder assumption".
Anari, Goel, and Nikzad (2018) gave a mechanism that has optimal competitive
ratio $1-1/e$ on worst-case instances. However, we observe that on many
realistic instances, their mechanism is significantly outperformed by a simpler
open clock auction by Ensthaler and Giebe (2014), although the open clock
auction only achieves competitive ratio $1/2$ in the worst case. Is there a
mechanism that gets the best of both worlds, i.e., a mechanism that is
worst-case optimal and performs favorably on realistic instances?
</p>
<p>Our first main result is the design and the analysis of a natural mechanism
that gives an affirmative answer to our question above: (i) We prove that on
every instance, our mechanism performs at least as good as all uniform
mechanisms, including Anari, Goel, and Nikzad's and Ensthaler and Giebe's
mechanisms. (ii) Moreover, we empirically evaluate our mechanism on various
realistic instances and observe that it beats the worst-case $1-1/e$
competitive ratio by a large margin and compares favorably to both mechanisms
mentioned above.
</p>
<p>Our second main result is more interesting in theory: We show that in the
semi-adversarial model of budget-smoothed analysis, where the adversary designs
a single worst-case market for a distribution of budgets, our mechanism is
optimal among all (including non-uniform) mechanisms; furthermore our mechanism
guarantees a strictly better-than-$(1-1/e)$ expected competitive ratio for any
non-trivial budget distribution regardless of the market. We complement the
positive result with a characterization of the worst-case markets for any given
budget distribution and prove a fairly robust hardness result that holds
against any budget distribution and any mechanism.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T01:30:00Z">Thursday, November 17 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Wednesday, November 16
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/16/postdoc-at-university-of-oxford-apply-by-january-16-2023/'>postdoc at University of Oxford (apply by January 16, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          A postdoctoral position is available to work with James Worrell on a UKRI Frontier Research project (ERC replacement grant) on computational problems on dynamical systems. Website: www.cs.ox.ac.uk/news/2107-full.html Email: jbw@cs.ox.ac.uk
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>A postdoctoral position is available to work with James Worrell on a UKRI Frontier Research project (ERC replacement grant) on computational problems on dynamical systems.</p>
<p>Website: <a href="https://www.cs.ox.ac.uk/news/2107-full.html">https://www.cs.ox.ac.uk/news/2107-full.html</a><br />
Email: jbw@cs.ox.ac.uk</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T16:57:43Z">Wednesday, November 16 2022, 16:57</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/'>The Gerrymanders Have It</a></h3>
        <p class='tr-article-feed'>from <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The real winner of the 2022 midterms in the House David Wasserman is an elections analyst for the Cook Political Report. He is known for forecasting the results of elections after people have voted. His words &#8220;I&#8217;ve seen enough&#8221; to declare an outcome are taken as seriously as any network election call. This week, with [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>
<font color="#0044cc"><br />
<em>The real winner of the 2022 midterms in the House</em><br />
<font color="#000000"></p>
<p><a href="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/wassermancook/" rel="attachment wp-att-20492"><img data-attachment-id="20492" data-permalink="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/wassermancook/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/WassermanCook.jpg?fit=170%2C170&amp;ssl=1" data-orig-size="170,170" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="WassermanCook" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/WassermanCook.jpg?fit=170%2C170&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/WassermanCook.jpg?fit=170%2C170&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/WassermanCook.jpg?resize=140%2C140&#038;ssl=1" alt="" width="140" height="140" class="alignright wp-image-20492" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/WassermanCook.jpg?w=170&amp;ssl=1 170w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/WassermanCook.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 140px) 100vw, 140px" data-recalc-dims="1" /></a></p>
<p>
David Wasserman is an <a href="https://www.cookpolitical.com/about/staff/david-wasserman">elections analyst</a> for the <a href="https://en.wikipedia.org/wiki/The_Cook_Political_Report_with_Amy_Walter">Cook Political Report</a>. He is known for forecasting the results of elections <em>after</em> people have voted. His words &#8220;I&#8217;ve seen enough&#8221; to declare an outcome are taken as seriously as any network election call. </p>
<p>
This week, with nine US House races uncalled and control of the chamber still unknown, he is working overtime.</p>
<p>
So have been your humble blog staff, in various life-necessary ways besides this blog. For me (Ken) it is not just being referenced six times in a <a href="https://www.espn.com/espn/story/_/id/34840275/hans-niemann-files-100-million-lawsuit-magnus-carlsen">&#36;100M lawsuit</a>&#8212;much else has been going on. Right now I am preparing a full formal report to the International Chess Federation (FIDE) for their own investigation.</p>
<p>
The election has also diverted our time. Insofar as both of us have involvements in <em>predictive analytics</em>, it behooves us to examine how well election models have been faring and where they may have systematic failings. The Washington Post shows its <a href="https://www.washingtonpost.com/election-results/2022/house/?itid=sn_elections_4/">models</a> of several of the uncalled House races in California plus one in Oregon. The New York Times showed its &#8220;needle&#8221; on Election Night but stopped its <a href="https://www.nytimes.com/interactive/2022/11/08/us/elections/results-house.html">prognostications</a> once the long-count stage began. As we write, the Republicans are on the cusp of the House majority threshold of 218 and will likely exceed it by two or three seats, but they are not yet declared the winner. The winners we can declare, however, are the <em>gerrymanders</em></p>
<p>
<p><H2> Redistricting </H2></p>
<p><p>
Wasserman goes by the handle <a href="https://twitter.com/Redistrict">@Redistrict</a> on Twitter. Redistricting is a neutral name for the re-drawing of boundaries of a region in which an election occurs. This is not confined to the US, but has special status because the US Constitution requires updating the number of Representatives for each state after each decennial US Census, and districts can be redrawn to reflect population shifts within a state even if the state has not gained or lost a member. The political science of drawing these maps is Wasserman&#8217;s specialty. </p>
<p>
To illustrate how the choice of boundaries can affect election outcomes, say we have a &#8220;state&#8221; of just nine people to divide equally into three districts:</p>
<ul>
<li>
<a href="https://fivethirtyeight.com/features/the-case-for-a-democratic-surprise-on-election-night/">Nathaniel Bleu</a>, Carrie Cyan, Alberto Az&uacute;l, Sandy Sapphire. </p>
<li>
<a href="https://fivethirtyeight.com/features/the-case-for-a-republican-sweep-on-election-night/">Nathan Redd</a>, Corrie Crimson, Philip Roth, Sally Scarlet, Ruby Rover.
</ul>
<p>
The &#8220;red voters&#8221; have an overall 5-4 majority. But if the districts are drawn like so, then the state will elect more blue than red representatives:</p>
<ol>
<li>
Bleu, Cyan, Redd. </p>
<li>
Az&uacute;l, Sapphire, Crimson. </p>
<li>
Roth, Scarlet, Ruby.
</ol>
<p>
What happened is that the red votes in district 3 were overkill. This is shown at left in the picture below. Two other natural ways of drawing the boundaries, however, result in two majority red districts.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/gerrymandernogrid/" rel="attachment wp-att-20495"><img data-attachment-id="20495" data-permalink="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/gerrymandernogrid/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/GerrymanderNoGrid.jpg?fit=824%2C259&amp;ssl=1" data-orig-size="824,259" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;regan&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1668526288&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="GerrymanderNoGrid" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/GerrymanderNoGrid.jpg?fit=300%2C94&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/GerrymanderNoGrid.jpg?fit=600%2C189&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/GerrymanderNoGrid.jpg?resize=550%2C173&#038;ssl=1" alt="" width="550" height="173" class="aligncenter wp-image-20495" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/GerrymanderNoGrid.jpg?w=824&amp;ssl=1 824w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/GerrymanderNoGrid.jpg?resize=300%2C94&amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/GerrymanderNoGrid.jpg?resize=768%2C241&amp;ssl=1 768w" sizes="(max-width: 550px) 100vw, 550px" data-recalc-dims="1" /></a></p>
<p>
The third map at right favors Red more robustly in the following sense: If Ruby Rover is any of the bottom three red dots and flips to blue, Red will still win two seats. Whereas, in the second map, any of four flips costs Red a seat. </p>
<p>
The second map, however, gives Red a chance of a clean sweep if either of the two blue voters at left flips to red. Whereas, the third map gives no such chance. </p>
<p>
Redistricting becomes <em>gerrymandering</em> when one side has control to draw a map yielding outcomes out of proportion to the other side&#8217;s voters. The <a href="https://constitutioncenter.org/the-constitution/articles/article-i/clauses/750">Elections Clause</a> of the US Constitution empowers state legislatures to <em>prescribe the manner</em> of state elections, subject to regulation and revision <em>by the US Congress</em>. Some states&#8217; legislatures have vested non-partisan commissions with districting power, while others&#8217; legislatures assume this power to benefit the side currently controlling them.</p>
<p>
<p><H2> Mathematics of Redistricting </H2></p>
<p><p>
Dick wrote a 2019 <a href="https://rjlipton.wpcomstaging.com/2019/07/03/mathematics-of-gerrymandering/">post</a> on the mathematics of gerrymanders. It includes a richer graphic on how they work. Here we will take a view from 20,000 feet and begin with some airy generalities.</p>
<ul>
<li>
<em>Random assignment amplifies the majority</em>. One might think that a completely random assignment of voters to districts would be fairest. But doing so amplifies a distinct majority party into total command of the state&#8217;s races. If we multiplied our 9 people into 900 while keeping proportions, and then chose three groups of 300 at random, it is overwhelmingly likely that majority vote in each group would go red. </p>
<li>
<em>Gerrymanders can favor or disfavor the minority</em>. This is exemplified by both our graphic above and the richer one in Dick&#8217;s post. They are, however, <em>all</em> fairer to the minority than random assignment. </p>
<li>
<em>Proportional representation</em> is practiced in several foreign countries, notably in Europe. This is generally most <em>fair</em>, but runs counter to the notion of geographical community as especially enshrined in US traditions. </p>
<li>
In any map, the higher one side&#8217;s percentage of voters in any one district, the lower the <em>efficiency</em> of each of those voters. Broadly speaking, one side&#8217;s objective in any gerrymander is to minimize the efficiency of the other side&#8217;s voters. There are <a href="https://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=13749&#038;context=journal_articles">various</a> <a href="https://gerrymander.princeton.edu/">metrics</a> for quantifying this.
</ul>
<p>
The US has an organic tendency toward gerrymanders through its rural-suburban-urban spectrum. The rural and urban sides have become more partisan during our lifetimes. When a city has population near the share of one Representative, it is natural to make it into one district. If the blue voters are, say, 80&#37; in that district, then they are individually highly inefficient. Meanwhile, a higher number of red voters&#8212;those besides the 20&#37; inside the city district&#8212;are freed to be efficient elsewhere. </p>
<p>
The logic of clustering a blue city can, however, turn on a dime if the surrounding areas are red and populous enough. Then the city can be divided into pizza slices, each joined with enough red to overpower it. This recently <a href="https://www.cnn.com/interactive/2022/politics/us-redistricting/tennessee-redistricting-map/">happened</a> with Nashville in Tennessee:</p>
<p><a href="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/nashvillemap-2/" rel="attachment wp-att-20498"><img data-attachment-id="20498" data-permalink="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/nashvillemap-2/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NashvilleMap.png?fit=685%2C508&amp;ssl=1" data-orig-size="685,508" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="NashvilleMap" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NashvilleMap.png?fit=300%2C222&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NashvilleMap.png?fit=600%2C445&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NashvilleMap.png?resize=342%2C254&#038;ssl=1" alt="" width="342" height="254" class="aligncenter wp-image-20498" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NashvilleMap.png?w=685&amp;ssl=1 685w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NashvilleMap.png?resize=300%2C222&amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NashvilleMap.png?resize=200%2C150&amp;ssl=1 200w" sizes="(max-width: 342px) 100vw, 342px" data-recalc-dims="1" /></a></p>
<p>
This change strikes us as increasing the efficiency of the Nashville voters&#8212;and the surrounding rural voters too. Thus efficiency is not the only metrizable notion that is relevant to fairness. </p>
<p>
<p><H2> Difference Makers </H2></p>
<p><p>
A key episode in this year&#8217;s redistricting was the <a href="https://fivethirtyeight.com/features/new-york-just-cost-democrats-their-big-redistricting-advantage/">rejection</a> by the New York Court of Appeals of the district map drawn up by the Democratic-controlled state legislature. The <a href="https://projects.fivethirtyeight.com/redistricting-2022-maps/new-york/amended_democratic_proposal/">map at left</a> below, was replaced by the <a href="https://projects.fivethirtyeight.com/redistricting-2022-maps/new-york/">map at right</a>.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/nychange3/" rel="attachment wp-att-20500"><img data-attachment-id="20500" data-permalink="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/nychange3/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NYChange3.png?fit=1216%2C477&amp;ssl=1" data-orig-size="1216,477" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="NYChange3" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NYChange3.png?fit=300%2C118&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NYChange3.png?fit=600%2C236&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NYChange3.png?resize=460%2C180&#038;ssl=1" alt="" width="460" height="180" class="aligncenter wp-image-20500" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NYChange3.png?resize=1024%2C402&amp;ssl=1 1024w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NYChange3.png?resize=300%2C118&amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NYChange3.png?resize=768%2C301&amp;ssl=1 768w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NYChange3.png?resize=1200%2C471&amp;ssl=1 1200w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NYChange3.png?w=1216&amp;ssl=1 1216w" sizes="(max-width: 460px) 100vw, 460px" data-recalc-dims="1" /></a></p>
<p>
Among several of the first map&#8217;s sins was lack of geometric contiguity as codified in law in one district that hopped over the Long Island Sound. Three consequential changes were Syracuse losing its reach down to Ithaca while absorbing red areas northeast, Long Island&#8217;s red area being divided between two districts, and Staten Island being joined to red rather than blue areas of Brooklyn. <a href="https://en.wikipedia.org/wiki/The_City_(website)">The City</a> published an <a href="https://www.thecity.nyc/2022/11/9/23450433/max-rose-malliotakis-staten-island-brooklyn-redistricting-election">analysis</a> from last week&#8217;s voting records that the district with Staten Island would have gone blue with the original map. All close districts went Republican, and this alone may make the difference in tbe majority. </p>
<p><p>
Even while Illinois lost a seat from population shifts, their Democrats conjured a new blue seat snaking through Springfield. Again the maps are mashups of ones <a href="https://projects.fivethirtyeight.com/redistricting-2022-maps/illinois/">created</a> by FiveThirtyEight, not by Bart Simpson.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/illinoischange/" rel="attachment wp-att-20501"><img data-attachment-id="20501" data-permalink="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/illinoischange/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?fit=807%2C609&amp;ssl=1" data-orig-size="807,609" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;KWRegan&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1668559241&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="IllinoisChange" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?fit=300%2C226&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?fit=600%2C453&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?resize=410%2C309&#038;ssl=1" alt="" width="410" height="309" class="aligncenter wp-image-20501" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?w=807&amp;ssl=1 807w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?resize=300%2C226&amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?resize=768%2C580&amp;ssl=1 768w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?resize=800%2C600&amp;ssl=1 800w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?resize=400%2C300&amp;ssl=1 400w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?resize=200%2C150&amp;ssl=1 200w" sizes="(max-width: 410px) 100vw, 410px" data-recalc-dims="1" /></a></p>
<p><P><br />
Meanwhile, Florida not only gained a seat, but their Republicans <a href="https://projects.fivethirtyeight.com/redistricting-2022-maps/florida/">created</a> three more strong ones for themselves even before considering their increased Election Day margins on the whole. </p>
<p><P><br />
<a href="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/floridachange/" rel="attachment wp-att-20502"><img data-attachment-id="20502" data-permalink="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/floridachange/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/FloridaChange.jpg?fit=802%2C582&amp;ssl=1" data-orig-size="802,582" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;KWRegan&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1668559706&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="FloridaChange" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/FloridaChange.jpg?fit=300%2C218&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/FloridaChange.jpg?fit=600%2C435&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/FloridaChange.jpg?resize=410%2C309&#038;ssl=1" alt="" width="410" height="309" class="aligncenter wp-image-20502" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/FloridaChange.jpg?resize=400%2C300&amp;ssl=1 400w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/FloridaChange.jpg?resize=200%2C150&amp;ssl=1 200w" sizes="(max-width: 410px) 100vw, 410px" data-recalc-dims="1" /></a></p>
<p><H2> Variability </H2></p>
<p><p>
We have tried to be balanced in our choice of examples. Our main point is not whether the changes are signed blue or red, but rather their absolute value. The variance alone is likely to dwarf the margin of the final House majority. </p>
<p>
Thus, instead of trying to define districts according to some criterion of <em>fairness</em>, can we instead postulate that revisions adhere to metrics for minimizing <em>variability</em>? This requires maintaining the sequence of past maps and population distributions as a reference, rather than treating each new map <em>ab ovo</em>.</p>
<p>
To be sure, it is possible for maps to conserve variability while defying any notions of geometric regularity. Here are the 2000 and 2002 maps for one Chicago area district:</p>
<p><P><br />
<a href="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/obamachange2/" rel="attachment wp-att-20523"><img data-attachment-id="20523" data-permalink="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/obamachange2/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/ObamaChange2.jpg?fit=592%2C481&amp;ssl=1" data-orig-size="592,481" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;KWRegan&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1668562083&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="ObamaChange2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/ObamaChange2.jpg?fit=300%2C244&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/ObamaChange2.jpg?fit=592%2C481&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/ObamaChange2.jpg?resize=410%2C355&#038;ssl=1" alt="" width="410" height="355" class="aligncenter size-full wp-image-20523" data-recalc-dims="1" /></a></p>
<p><P><br />
As recounted <a href="https://redistricting.lls.edu/redistricting-101/why-should-we-care/">here</a>, only one element of variability mattered most to the incumbent about the right-hand map. That was to exclude the home marked by the blue pin at upper right. It was the residence of a potential challenger: Barack Obama.</p>
<p>
<p><H2> Open Questions </H2></p>
<p><p>
Have we shed any more light on mathematical criteria that might curtail the variability and arbitrariness of redistricting?</p>
<p>
Here is a second question, along lines of my saying above that how election models fare can matter to my chess work. FiveThirtyEight are catching heat for their modeling of Washington&#8217;s Third Congressional District, where Marie Gluesenkamp Perez upset the Republican Joe Kent. They had Perez at only a 2&#37; chance to win:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/kentperezrace538/" rel="attachment wp-att-20505"><img data-attachment-id="20505" data-permalink="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/kentperezrace538/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/KentPerezRace538.jpg?fit=668%2C363&amp;ssl=1" data-orig-size="668,363" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;KWRegan&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1668592000&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="KentPerezRace538" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/KentPerezRace538.jpg?fit=300%2C163&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/KentPerezRace538.jpg?fit=600%2C326&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/KentPerezRace538.jpg?resize=460%2C250&#038;ssl=1" alt="" width="460" height="250" class="aligncenter size-full wp-image-20505" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/KentPerezRace538.jpg?w=668&amp;ssl=1 668w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/KentPerezRace538.jpg?resize=300%2C163&amp;ssl=1 300w" sizes="(max-width: 460px) 100vw, 460px" data-recalc-dims="1" /></a></p>
<p>
Our question is, given that over a hundred races were under the 99&#37;-lock level, and allowing for covariance over all races, shouldn&#8217;t one expect to have one such case? If &#8220;a 2&#37; chance to win&#8221; really means what it says in your model, not just a hedge for modeling uncertainty, then it should have 2&#37; expectation, no?  This can be argued back-and-forth a few more rounds based on how FiveThirtyEight&#8217;s simulations work, but my point will remain&#8212;and it is important in both my top-level need to gauge unlikelihoods longer than 2&#37; <em>and</em> my model&#8217;s internal need for precision and accuracy on estimating low-probability moves, especially <em>blunders</em>.</p>
<p><P><br />
[fixed Obama pin, added to end question, some small word changes]</p>
<p class="authors">By KWRegan</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T15:42:52Z">Wednesday, November 16 2022, 15:42</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/157'>TR22-157 |  Border complexity via elementary symmetric polynomials | 

	Pranjal Dutta, 

	Fulvio Gesmundo, 

	Christian Ikenmeyer, 

	Gorav Jindal, 

	Vladimir Lysikov</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          In (ToCTâ20) Kumar surprisingly proved that every polynomial can be approximated as a sum of a constant and a product of linear polynomials. In this work, we prove the converse of Kumar&#39;s result which ramifies in a surprising new formulation of Waring rank and border Waring rank. From this conclusion, we branch out into two different directions, and implement the geometric complexity theory (GCT) approach in two different settings.

In the first direction, we study the orbit closure of the product-plus-power polynomial, determine its stabilizer, and determine the properties of its boundary points. We also connect its fundamental invariant to the Alon-Tarsi conjecture on Latin squares, and prove several exponential separations between related polynomials contained in the affine closure of product-plus-product polynomials. We fully implement the GCT approach and obtain several equations for the product-plus-power polynomial from its symmetries via representation theoretic multiplicity obstructions.

In the second direction, we demonstrate that the non-commutative variant of Kumar&#39;s result is intimately connected to the constructions of Ben-Or and Cleve (SICOMP&#39;92), and Bringmann, Ikenmeyer, Zuiddam (JACM&#39;18), which describe algebraic formulas in terms of iterated matrix multiplication. From this we obtain that a variant of the elementary symmetric polynomial is complete for V3F, a large subclass of VF, under homogeneous border projections. In the regime of quasipolynomial complexity, our polynomial has the same power as the determinant or as arbitrary circuits, i.e., VQP. This is the first completeness result under homogeneous projections for a subclass of VBP. Such results are required to set up the GCT approach in a way that avoids the no-go theorems of B\&quot;urgisser, Ikenmeyer, Panova (JAMS&#39;19).

Finally, using general geometric considerations, we significantly improve the relationship between the Waring rank and the border Waring rank of polynomials. In particular, if the border Waring rank of a homogeneous polynomial $f$ is $k$, then, the Waring rank of  $f$ can be at most $\exp(k) \cdot d$, while previously it was known to be $O(d^k)$.
        
        </div>

        <div class='tr-article-summary'>
        
          
          In (ToCTâ20) Kumar surprisingly proved that every polynomial can be approximated as a sum of a constant and a product of linear polynomials. In this work, we prove the converse of Kumar&#39;s result which ramifies in a surprising new formulation of Waring rank and border Waring rank. From this conclusion, we branch out into two different directions, and implement the geometric complexity theory (GCT) approach in two different settings.

In the first direction, we study the orbit closure of the product-plus-power polynomial, determine its stabilizer, and determine the properties of its boundary points. We also connect its fundamental invariant to the Alon-Tarsi conjecture on Latin squares, and prove several exponential separations between related polynomials contained in the affine closure of product-plus-product polynomials. We fully implement the GCT approach and obtain several equations for the product-plus-power polynomial from its symmetries via representation theoretic multiplicity obstructions.

In the second direction, we demonstrate that the non-commutative variant of Kumar&#39;s result is intimately connected to the constructions of Ben-Or and Cleve (SICOMP&#39;92), and Bringmann, Ikenmeyer, Zuiddam (JACM&#39;18), which describe algebraic formulas in terms of iterated matrix multiplication. From this we obtain that a variant of the elementary symmetric polynomial is complete for V3F, a large subclass of VF, under homogeneous border projections. In the regime of quasipolynomial complexity, our polynomial has the same power as the determinant or as arbitrary circuits, i.e., VQP. This is the first completeness result under homogeneous projections for a subclass of VBP. Such results are required to set up the GCT approach in a way that avoids the no-go theorems of B\&quot;urgisser, Ikenmeyer, Panova (JAMS&#39;19).

Finally, using general geometric considerations, we significantly improve the relationship between the Waring rank and the border Waring rank of polynomials. In particular, if the border Waring rank of a homogeneous polynomial $f$ is $k$, then, the Waring rank of  $f$ can be at most $\exp(k) \cdot d$, while previously it was known to be $O(d^k)$.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T15:03:39Z">Wednesday, November 16 2022, 15:03</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://gilkalai.wordpress.com/2022/11/16/barnabas-janzer-rotation-inside-convex-kakeya-sets/'>BarnabÃ¡s Janzer: Rotation inside convex Kakeya sets</a></h3>
        <p class='tr-article-feed'>from <a href='https://gilkalai.wordpress.com'>Gil Kalai</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          BarnabÃ¡s Janzer studied the following question: Suppose we have convex body in that contains a copy of a convex body in every orientation. Is it always possible to move any one copy of to another copy of , keeping inside &#8230; Continue reading &#8594;
        
        </div>

        <div class='tr-article-summary'>
        
          
          <blockquote><p>BarnabÃ¡s Janzer studied the following question:</p>
<p><span style="color:#993366;"><em>Suppose we have convex body <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="K" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathbb R^n" class="latex" /> that contains a copy of a convex body <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" /> in every orientation. Is it always possible to move any one copy of <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" /> to another copy of <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" />, keeping inside <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="K" class="latex" />?</em></span></p></blockquote>
<p>I will let you test your intuition about what the answer should be. First, some background.</p>
<p>A Kakeya set is a set that contains unit unterval in every direction. A famous open problem is the conjecture that every Kakeya set in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathbb R^d" class="latex" /> has Housdorff dimension <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="d" class="latex" />. in 2008, Zeev Dvir <a href="https://terrytao.wordpress.com/2008/03/24/dvirs-proof-of-the-finite-field-kakeya-conjecture/">found a simple remarkable proof</a> for a finite field analog of the conjecture. Finding possible connections between the finite field problem and the Euclidean problem is an exciting problem. Can we use the finite field result to prove the Euclidean result? Can we use or refine the finite field methods for the Euclidean problem? Here is a <a href="https://www.quantamagazine.org/new-number-systems-point-geometry-problem-toward-a-real-solution-20220726/">recent Quanta Magazine article</a> about exciting &#8220;intermediate results&#8221;.</p>
<p>The question about the connection between finite fields analogs and questions over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathbb Z" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb+R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathbb R" class="latex" /> can be asked about other problems. One example is the Roth problem (relevant posts <a href="https://gilkalai.wordpress.com/2020/07/08/to-cheer-you-up-in-difficult-times-7-bloom-and-sisask-just-broke-the-logarithm-barrier-for-roths-theorem/">I</a>,<a href="https://gilkalai.wordpress.com/2010/11/24/roths-theorem-sanders-reaches-the-logarithmic-barrier/">II</a>, <a href="https://gilkalai.wordpress.com/2009/03/25/an-open-discussion-and-polls-around-roths-theorem/">III</a>) vs. the cup set problems (relevant posts<a href="https://gilkalai.wordpress.com/2016/05/15/mind-boggling-following-the-work-of-croot-lev-and-pach-jordan-ellenberg-settled-the-cap-set-problem/"> I</a>,<a href="https://gilkalai.wordpress.com/2009/03/25/an-open-discussion-and-polls-around-roths-theorem/">II</a>,<a href="https://gilkalai.wordpress.com/2009/02/07/frankl-rodls-theorem-and-variations-on-the-cap-set-problem-a-recent-research-project-with-roy-meshulam-a/">III</a>).</p>
<p><span id="more-23515"></span></p>
<p>Going back to our problem (posed by H. T. Croft) about convex Kakeya sets:</p>
<blockquote><p>Suppose we have convex body <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="K" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathbb R^n" class="latex" /> that contains a copy of a convex body <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" /> in every orientation. Is it always possible to move any one copy of <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" /> to another copy of <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" />, keeping inside <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="K" class="latex" />?</p></blockquote>
<p>BarnabÃ¡s Janzer proved:</p>
<h3><span style="color:#000080;">The answer is yes in 2 dimensions:</span></h3>
<h3><span style="color:#000080;">The answer is also yes in <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="n" class="latex" /> dimensions if <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" /> is one-dimensional (i.e. an interval).</span></h3>
<h3><span style="color:#ff0000;"><strong>But, amazingly: the answer is no in general in 4 dimensions!</strong></span></h3>
<p>Here is the link to the paper:</p>
<h3><a href="https://arxiv.org/abs/2209.09728">Rotation inside convex Kakeya sets</a></h3>
<p><!--more--></p>
<p class="authors">By Gil Kalai</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T13:05:59Z">Wednesday, November 16 2022, 13:05</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://francisbach.com/sums-of-squares-for-dummies/'>Sums-of-squares for dummies: a view from the Fourier domain</a></h3>
        <p class='tr-article-feed'>from <a href='https://francisbach.com'>Francis Bach</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          In these last two years, I have been studying intensively sum-of-squares relaxations for optimization, learning a lot from many great research papers [1, 2], review papers [3], books [4, 5, 6, 7, 8], and even websites. Much of the literature focuses on polynomials as the de facto starting point. While this leads to deep connections...
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="justify-text">In these last two years, I have been studying intensively sum-of-squares relaxations for optimization, learning a lot from many great research papers [<a href="https://epubs.siam.org/doi/pdf/10.1137/S1052623400366802">1</a>, <a href="http://www.mit.edu/~parrilo/pubs/files/SDPrelaxations.pdf">2</a>], review papers [<a href="https://homepages.cwi.nl/~monique/files/laurent-ima.pdf">3</a>], books [4, 5, 6, 7, 8], and even <a href="https://www.sumofsquares.org/">websites</a>. </p>



<p class="justify-text">Much of the literature focuses on polynomials as the de facto starting point. While this leads to deep connections between many fields within mathematics, and many applications in various areas (optimal control, data science, etc.), the need for arguably non-natural hierarchies (at least for beginners) sometimes makes the exposition hard to follow at first, and notations a tiny bit cumbersome.</p>



<p class="justify-text">I think that one reason is the aim for generality of the approaches. If you want to minimize potentially arbitrary continuous functions on a set defined by arbitrary equality or inequality constraints, there is not much choice if you want a unique generic mathematical framework that can approximate all of these problems. But if you are willing to optimize an arbitrary continuous function only on a &#8220;simple&#8221; set, then things considerably simplify: most results have explicit reasonably simple proofs, and you can even prove the exponential convergence of the SOS hierarchy! The main goal of this post is to present such old and recent results.</p>



<p class="justify-text">One other aim is to highlight the classical dual view of optimization problems through the characterization of probability measures by their moments, which has many applications beyond optimization.</p>



<p class="justify-text">By simple sets, I mean the interval \([0,\! 1]\) as a toy example, but by taking tensor products, this extends to the hypercube \([0,\! 1]^n\), and with a bit more work, you can add the Boolean hypercube \(\{-1,1\}^n\) and Euclidean spheres (see below). The key property that we will need is the availability of a nice orthonormal basis of square integrable functions.</p>



<h2>Minimization of quadratic forms</h2>



<p class="justify-text">In order to cover all cases, I will consider a generic minimization problem, $$\min_{x \in \mathcal{X}} \ f(x),$$ where \(\mathcal{X}\) is a compact set, e.g., \([0,\! 1]^n\) or \(\{-1,+1\}^n\) (I will use \([0,\! 1]\) as a running example). I will also first assume that the function \(f: \mathcal{X} \to \mathbb{R}\) can be represented as a quadratic form in some complex vector-valued feature \(\varphi: \mathcal{X} \to \mathbb{C}^d\), that is, there exists a Hermitian matrix \(F \in \mathbb{C}^{d \times d}\) (i.e., such that \(F^\ast = F\), where \(F^\ast\) is the &#8220;<a href="https://en.wikipedia.org/wiki/Conjugate_transpose">conjugate-transpose</a>&#8221; of the matrix \(F\)), such that $$\forall x \in \mathcal{X}, \ f(x) = \varphi(x)^\ast F \varphi(x) = \sum_{j,k=1}^d \varphi_j(x) \varphi_k(x)^\ast F_{jk}.$$ I will also assume that features are normalized, that is, \(\| \varphi(x)\| = 1\) for all \(x \in \mathcal{X}\). Thus, the constant function equal to one is represented by the identity matrix.</p>



<p class="justify-text">For our running example, with \(\mathcal{X} = [0,\! 1]\), we can consider the Fourier basis $$\varphi_\omega(x) = \frac{1}{\sqrt{2r+1}} e^{2i\pi \omega x}, \ \ \omega \in \{-r,-r+1,\dots,r-1,r\}, $$ and then the assumption on \(f\) is that it is a real-valued linear combination of complex exponentials \(x \mapsto e^{2i\pi \omega x}\), for \(\omega \in \{-2r,-2r+1,\dots,2r-1,2r\}\), and thus a trigonometric polynomial of degree \(2r\) (that is a real bivariate polynomial of degree \(2r\) in \(\cos(2\pi x)\) and \(\sin(2\pi x)\)). </p>



<p class="justify-text">We see immediately in this example, that the representation of \(f\) through \(F\) is not unique. This is due to the fact that in the real vector space of Hermitian matrices of dimension \(d\), the rank-one matrices \( \varphi(x) \varphi(x)^\ast\) may only occupy a specific linear subspace, which we denote \(\mathcal{V}\). For the specific case of univariate trigonometric polynomials, we have $$(\varphi(x) \varphi(x)^\ast)_{\omega\omega&#8217;} =  \frac{1}{{2r+1}} e^{2i\pi (\omega\, -\, \omega&#8217;) x},$$ so that the set \(\mathcal{V}\) is exactly the set of Hermitian <a href="https://en.wikipedia.org/wiki/Toeplitz_matrix">Toeplitz matrices</a> (which are constant along all diagonals), and thus a strict subset of all Hermitian matrices.</p>



<p class="justify-text">We will often use the trace trick to represent a quadratic form as a linear form in \(\varphi(x) \varphi(x)^\ast\), that is, $$ \varphi(x)^\ast F \varphi(x)  = {\rm tr} \big[ F \varphi(x) \varphi(x)^\ast \big].$$  The set of matrices \(F\) that represent the function \(f\) exactly is an affine subspace orthogonal to \(\mathcal{V}\), that is, of the form \(F + \mathcal{V}^\perp\), where \(\mathcal{V}^\perp\) is the orthogonal of \(\mathcal{V}\) in the set of Hermitian matrices equipped with the dot-product \((M,N) \mapsto {\rm tr}(MN)\). For our running example, \(\mathcal{V}\) has dimension \(4r+1\), and hence its orthogonal has dimension \((2r+1)^2-4r-1 = 4r^2\).</p>



<h2>Two dual convex views</h2>



<p class="justify-text">The minimization of \(f\) has two dual convex optimization formulations, which are both natural, and which both provide insights into the problems.</p>



<p class="justify-text"><strong>Maximizing lower bounds. </strong>The first view is simply maximizing a lower bound on \(f\), that is, $$\min_{x \in \mathcal{X}} f(x) = \max_{c \in \mathbb{R}} \ c \ \mbox{ such that }\  \forall x \in \mathcal{X}, f(x)\, &#8211; c \geqslant 0.$$ The optimal \(c\) is then \(\min_{x \in \mathcal{X}} f(x)\). See an illustration below.</p>


<div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2022/09/f_above_c-1024x670.png" alt="" class="wp-image-8246" width="309" height="202" srcset="https://francisbach.com/wp-content/uploads/2022/09/f_above_c-1024x670.png 1024w, https://francisbach.com/wp-content/uploads/2022/09/f_above_c-300x196.png 300w, https://francisbach.com/wp-content/uploads/2022/09/f_above_c-768x503.png 768w, https://francisbach.com/wp-content/uploads/2022/09/f_above_c-1536x1006.png 1536w, https://francisbach.com/wp-content/uploads/2022/09/f_above_c-350x230.png 350w, https://francisbach.com/wp-content/uploads/2022/09/f_above_c-850x557.png 850w, https://francisbach.com/wp-content/uploads/2022/09/f_above_c.png 2007w" sizes="(max-width: 309px) 100vw, 309px" /></figure></div>


<p class="justify-text">As mentioned in an <a href="https://francisbach.com/finding-global-minima-with-kernel-approximations/">earlier post</a>, all optimization problems are thus convex! The hard computational task here is to represent efficiently non-negative functions to handle the (convex) constraint that \(f &#8211; c\) is non-negative.</p>



<p class="justify-text"><strong>Mininimizing expectations. </strong>The second view is looking for a probability measure \(\mu\) on \(\mathcal{X}\) with minimal expectation \(\int_\mathcal{X} \! f(x) d\mu(x)\). Denoting \(\mathcal{P}(\mathcal{X})\) the set of probability distributions on \(\mathcal{X}\), we have: $$\min_{x \in \mathcal{X}} f(x)  = \min_{ \mu \in \mathcal{P}(\mathcal{X})}  \int_\mathcal{X} f(x) d\mu(x),$$ and the minimizer is any probability measure supported on the minimizers of \(f\).</p>



<p class="justify-text">When \(f\) can be represented by a quadratic form, then the objective function becomes \(\displaystyle {\rm tr} \Big[ F\! \int_\mathcal{X} \varphi(x)\varphi(x)^\ast d\mu(x) \Big]\), which is linear in the moment matrix $$\Sigma = \int_\mathcal{X} \varphi(x)\varphi(x)^\ast d\mu(x).$$ The hard computational task is to represent efficiently the set of allowed moment matrices, which happens to be the closure of the convex hull \(\mathcal{K}\) of all \( \varphi(x) \varphi(x)^\ast\), \(x \in \mathcal{X}\) (remember that \(\mathcal{V}\) defined above is its linear span and thus contains \(\mathcal{K}\)). See an illustration below.</p>


<div class="wp-block-image justify-text">
<figure class="aligncenter size-large is-resized"><img loading="lazy" src="https://francisbach.com/wp-content/uploads/2022/09/KV-2-1024x279.png" alt="" class="wp-image-8257" width="588" height="160" srcset="https://francisbach.com/wp-content/uploads/2022/09/KV-2-1024x279.png 1024w, https://francisbach.com/wp-content/uploads/2022/09/KV-2-300x82.png 300w, https://francisbach.com/wp-content/uploads/2022/09/KV-2-768x209.png 768w, https://francisbach.com/wp-content/uploads/2022/09/KV-2-850x232.png 850w, https://francisbach.com/wp-content/uploads/2022/09/KV-2.png 1534w" sizes="(max-width: 588px) 100vw, 588px" /><figcaption>Convex hull \(\mathcal{K}\) and span \(\mathcal{V}\) of all \(\varphi(x) \varphi(x)^\ast\), \(x \in \mathcal{X}\), when \(\mathcal{X} = \{x_1,\dots,x_5\}\) has 5 elements.</figcaption></figure></div>


<p>A key computational task will be to find outer approximations of \(\mathcal{K}\) based in particular on the knowledge of \(\mathcal{V}\).</p>



<p class="justify-text"><strong>Equivalence by convex duality.</strong> The two views are equivalent as one can see \(\mu\) as the Lagrange multiplier for the constraint \(\forall x \in \mathcal{X}, f(x)\, &#8211; c \geqslant 0\), which has to be a finite positive measure [10]. The Lagrangian is $$\mathcal{L}(c,\mu) = c + \int_\mathcal{X} ( f(x) \, &#8211; c) d\mu(x),$$ and minimizing with respect to the primal variable \(c\) leads to the constraint \(\int_\mathcal{X} d\mu(x) = 1\), that is, \(\mu\) is a probability distribution. Note that the traditional complementary slackness condition imposes that the optimal measure \(\mu\) puts mass only on minimizers of \(f\).</p>



<h2>Convex relaxation: the sum-of-squares (SOS) view</h2>



<p class="justify-text">A natural idea to characterize non-negative functions represented as quadratic forms is to consider positive-semidefinite (PSD) Hermitian matrices, that is, Hermitian matrices with non-negative eigenvalues. Indeed, if \(g(x) = \varphi(x)^\ast G \varphi(x)\) with \(G \succcurlyeq 0\) (which is another notation for \(G\) PSD), then \(g(x) \geqslant 0\) for all \(x \in \mathcal{X}\). This leads to the relaxation: $$\tag{1} \max_{c \in \mathbb{R}, \ A \in \mathbb{C}^{d \times d}} \ c \  \mbox{ such that } \  \forall x \in \mathcal{X}, \ f(x) = c + \varphi(x)^\ast A \varphi(x), \ A \succcurlyeq 0,$$ which is always lower than the minimal value of \(f\) since we replace non-negativity of \(f-c\) by a stronger sufficient condition.  This is called an SOS relaxation because when \(G \succcurlyeq 0\), we can write \(g(x) = \varphi(x)^\ast G \varphi(x)\) as the sum of the squares of the functions \(x \mapsto \lambda_i^{1/2} u_i^\ast \varphi(x)\), where \(\lambda_i\) is the \(i\)-th (non-negative real) eigenvalue of \(G\), and \(u_i \in \mathbb{C}^d\) the corresponding eigenvector.</p>



<p class="justify-text">We can write the equality constraint compactly as (using that features have unit norm): $$ \forall x \in \mathcal{X}, \ \varphi(x)^\ast [ F &#8211; c I &#8211; A ] \varphi(x) = 0 \ \ \Leftrightarrow \ \ F &#8211; c I &#8211; A \in \mathcal{V}^\perp.$$ Using a variable \(-Y\) for an element of \(\mathcal{V}^\perp\) and eliminating \(A\), we get a constraint \(F   + Y \succcurlyeq cI \), leading to \(c\) being the lowest eigenvalue of \(F +Y\) at optimum. This leads to a particularly simple formula for the relaxation: $$\tag{2} \max_{Y \in \mathcal{V}^\perp} \  \lambda_{\min} ( F + Y ).$$</p>



<p class="justify-text">The relaxation is tight for all \(F\), if and only if all non-negative functions represented as quadratic forms are SOS. This is not the case in general, and the relaxation is thus not tight in general, except in a few cases like uni-dimensional trigonometric polynomials.</p>



<p class="justify-text">It is tempting to set \(Y = 0\), and consider \(\lambda_{\min}(F)\), which is directly a lower-bound on the minimal value of \(f\), because, since features have been assumed to have unit norm, $$\lambda_{\min}(F) \ =\  \min_{ \| z\|=1} z^\ast F z\  \leqslant\  \min_{x \in \mathcal{X}} \ \varphi(x)^\ast F \varphi(x).$$ This is the classical spectral relaxation, which is much simpler than the SOS relaxation and can already lead to interesting behaviors. Note that we can see problem \((2)\) as the spectral relaxation optimized over all matrices \(F + Y\) that define the same quadratic form in \(\varphi(x)\).</p>



<p class="justify-text">Before looking at the performance of such relaxations, let&#8217;s first look at the dual view.</p>



<h2>Convex relaxation: the moment view</h2>



<p class="justify-text">Given that the objective function is linear in \( \varphi(x) \varphi(x)^\ast\), a traditional approach is to find outer approximations of the convex set \(\mathcal{K}\). The least we can expect from an outer approximation is that the affine hull is preserved, that is, we want to make sure that a matrix \(\Sigma\) in our approximation satisfies \(\Sigma \in \mathcal{V}\) (that is, Toeplitz for the trigonometric polynomial case), and \({\rm tr }(\Sigma) = 1\) (coming from \({\rm tr}(\varphi(x)\varphi(x)^\ast) = \| \varphi(x)\|^2 = 1\)).</p>



<p class="justify-text">Another natural property of the matrices \( \varphi(x) \varphi(x)^\ast\) is that they are positive semi-definite (PSD). The moment relaxation will exactly be the combination of these two properties (affine hull + PSD), leading to the outer approximation $$\widehat{\mathcal{K}} = \big\{ \Sigma \in \mathcal{V}, \ {\rm tr}(\Sigma)=1, \ \Sigma \succcurlyeq 0 \big\},$$ and to the relaxation $$\tag{3} \min_{ \Sigma \in \mathbb{C}^{d \times d}} \ {\rm tr} [ F \Sigma ] \ \mbox{ such that } \ \Sigma \succcurlyeq 0, \ {\rm tr}( \Sigma) = 1, \ \Sigma \in \mathcal{V}.$$</p>



<p class="justify-text">Removing the constraint that \(\Sigma \in \mathcal{V}\), we obtain again the (weaker) spectral relaxation.</p>



<p class="justify-text">Note that the outer approximation of \(\mathcal{K}\) by \(\widehat{\mathcal{K}}\) has many applications beyond optimization (see, e.g., [4, 7]). We get a tight relaxation for all \(F\), if and only if \(\mathcal{K}=  \widehat{\mathcal{K}}\), which happens only in a few cases.</p>



<h2>Equivalence by convex duality</h2>



<p class="justify-text">By introducing the same Lagrange multiplier \(\mu\) which is now a <em>signed</em> measure (since we have an equality constraint), we obtain the Lagrangian $$\mathcal{L}(c,A, \mu) = c + \int_\mathcal{X} \big( f(x) \, &#8211; c \, &#8211; \,  \varphi(x)^\ast A \varphi(x)\big) d\mu(x),$$ and minimizing with respect to \(c\) leads to the constraint \(\int_\mathcal{X} d\mu(x) = 1\), while optimizing with respect to \(A\) leads to the constraint \(  \int_\mathcal{X} \varphi(x)\varphi(x)^\ast d\mu(x) \succcurlyeq 0\), and we exactly get the moment relaxation by setting \(\Sigma = \int_\mathcal{X} \varphi(x)\varphi(x)^\ast d\mu(x)\). Another way of seeing it is that the two semidefinite programs \((2)\) and \((3)\) are dual to each other.</p>



<p class="justify-text">The two views are thus equivalent, and tightness of one is equivalent to the tightness of the other. We will see below that for our running example of univariate trigonometric polynomials, the relaxation is tight, before looking at the multivariate case, where the relaxation is not tight, but can be made as tight as desired. We will then allow infinite-dimensional feature maps by allowing infinitely many frequencies.</p>



<p class="justify-text">Let&#8217;s first mention a few &#8220;practicalities&#8221;: solving either \((2)\) or \((3)\) with efficient algorithms, and obtaining a candidate minimizer from a moment matrix \(\Sigma\).</p>



<p class="justify-text"><strong>Convex optimization algorithms.</strong> Standard interior point methods for semi-definite programming problems can be used for either \((2)\) or \((3)\). This will become slow when the dimension \(d\) of the feature map \(\varphi(x)\) gets large, as each iteration will be of complexity \(O(d^3)\). Moreover, encoding the vector space \(\mathcal{V}\) can be rather painful when \(\mathcal{X}\) has many dimensions. For efficient descriptions of \(\mathcal{V}\) based on sampling, see the kernel section below. For efficient algorithms based on smoothing the minimal eigenvalue, see the bottom of the post.</p>



<p class="justify-text"><strong>Recovering a minimizer. </strong>This is often referred to as &#8220;extraction&#8221; in the SOS literature [<a href="https://homepages.laas.fr/henrion/papers/extract.pdf">29</a>], and a variety of techniques exist. Note also that randomized techniques which are classical in combinatorial optimization can also be considered [<a href="https://dl.acm.org/doi/pdf/10.1145/227683.227684">30</a>].</p>



<h2>Univariate trigonometric polynomials</h2>



<p class="justify-text">We first start with the SOS view, by showing that all non-negative trigonometric polynomials are sums-of-squares, and here a single square.</p>



<p class="justify-text"><strong>FejÃ©r-Riesz theorem</strong> [11, 12] (if you can read German). We consider a trigonometric polynomial of degree \(r\), that is, \(g(x) = \sum_{\omega = -r}^r c_\omega e^{2i\pi \omega x}\) with real non-negative values. This imposes that \(c_{-\omega} = c_\omega^\ast\). Then it is non-negative if and only if it can be written as the square modulus of a complex-valued trigonometric polynomial. The elementary proof based on roots of polynomials is shown at the end of the post.</p>



<p class="justify-text">We here get a single square, with an elementary proof. Note that this is a stronger result than being a sum of several squares. We can now look at the dual intepretation.</p>



<p class="justify-text"><strong>Positivity of Hermitian Toeplitz matrices.</strong> As mentioned earlier, our approximation set \(\widehat{\mathcal{K}}\) for \(\mathcal{K}\) is the set of PSD Toeplitz matrices of unit trace. We have a tight relaxation if we can show that for any such matrix \(\Sigma\), there exists a probability measure \(\mu\) on \([0,\! 1]\) such that $$ \Sigma_{\omega\omega&#8217;}= \int_{0}^1 e^{2 i \pi (\omega \, -\,  \omega&#8217;)x} d\mu(x)$$ for all \(\omega,\omega&#8217;\). This is a classical result in Toeplitz matrix theory, from [13] (if you read German), well summarized in [14].</p>



<p class="justify-text"><strong>Comparison with spectral relaxation.</strong> Assume we are given a trigonometric polynomial of degree \(2r\), that is, \(f(x) = \sum_{\omega = -2r}^{2r} c_\omega e^{2i\pi \omega x}\). As mentioned earlier, there are many ways of representing it as a quadratic form \(\varphi(x)^\ast F \varphi(x)\) with \(\varphi(x)_\omega = e^{2i\pi \omega x}/\sqrt{2r+1}\) for \(\omega \in \{-r,\dots,r\}\). This will not change the SOS relaxation but will make a difference for the spectral relaxation. One standard one is to choose \(F\) to be a Toeplitz matrix. Since \(F_{\omega\omega&#8217;}\) should depend only on \(\omega-\omega&#8217;\), by counting the number of elements in each diagonal, we must have $$\tag{4} F_{\omega \omega&#8217;} = c_{\omega\, -\, \omega&#8217;} \frac{2r+1}{2r+1-|\omega\, -\, \omega&#8217;|}= c_{\omega\, -\, \omega&#8217;} \frac{1}{1-|\omega\, -\, \omega&#8217;|/(2r+1)}.$$</p>



<p class="justify-text">The spectral relaxation amounts to compute the lowest eigenvalue of a Toeplitz matrix, which is a well studied problem [<a href="https://ee.stanford.edu/~gray/toeplitz.pdf">9</a>], in particular when the dimension of the matrix grows. In our context, this corresponds to considering a feature \(\varphi\) of size \(s\) larger than \(r\) (like we will do for multivariate polynomials below), representing the function \(f\) in this basis through a Toeplitz matrix \(F\) of size \((2s+1)\times(2s+1)\) with the same formula as \((4)\), and seeing how the spectral relaxation tends to the optimal value when \(s\) goes to infinity.</p>



<p class="justify-text">We first consider the Toeplitz matrices with values \(c_{\omega-\omega&#8217;}1_{|\omega-\omega&#8217;|\leqslant 2r}\) for \(|\omega|, |\omega&#8217;| \leqslant s\), which is, a band-diagonal Toeplitz matrix. it is shown in [31, Section 5.4] that the lowest eigenvalue of this Toeplitz matrix has the following asymptotic expansion \(f(x_\ast) +  \frac{f^{\prime\prime}(x_\ast)}{32 s^2}\) where \(x_\ast \in [0,\! 1]\) is the minimizer of \(f\) (assumed to be unique). We thus get an upper approximation of \(O(1/s^2)\). By taking into account the multiplicative factor \(\frac{1}{1-|\omega-\omega&#8217;|/(2s+1)}\) from Eq. \((4)\), we get an overall factor of \(O(1/s)\), which is what we observe in practice (see simple experiment below).</p>



<p>The spectral relaxation is convergent, but much slower than the SOS relaxation (which is tight for \(r\geqslant s\)).</p>


<div class="wp-block-image justify-text">
<figure class="aligncenter size-large is-resized"><img loading="lazy" src="https://francisbach.com/wp-content/uploads/2022/10/SOS_1d_blog-1024x401.png" alt="" class="wp-image-8339" width="633" height="247" srcset="https://francisbach.com/wp-content/uploads/2022/10/SOS_1d_blog-1024x401.png 1024w, https://francisbach.com/wp-content/uploads/2022/10/SOS_1d_blog-300x117.png 300w, https://francisbach.com/wp-content/uploads/2022/10/SOS_1d_blog-768x300.png 768w, https://francisbach.com/wp-content/uploads/2022/10/SOS_1d_blog-1536x601.png 1536w, https://francisbach.com/wp-content/uploads/2022/10/SOS_1d_blog-850x332.png 850w, https://francisbach.com/wp-content/uploads/2022/10/SOS_1d_blog.png 1846w" sizes="(max-width: 633px) 100vw, 633px" /><figcaption>Left: trigonometric polynomial \(f\) of degree 4. Right: performance of the spectral relaxation, which requires a large degree to be a close approximation (with observed rate \(1/s\)). Here the SOS relaxation for \(s = r\) is already tight.</figcaption></figure></div>


<p class="justify-text"><strong>Consequences for regular polynomials on \([-1,1]\).</strong> Non-negative polynomials \(P\) on \([-1,1]\) can also be characterized, by looking at the non-negativity of \(f(x) = P(\cos 2 \pi x)\) for \(x \in [0,\! 1]\), which is equivalent to the existence of a complex trigonometric polynomial whose square modulus is exactly \(f\). Playing around with <a href="https://en.wikipedia.org/wiki/Chebyshev_polynomials">Chebyshev polynomials of the two kinds</a>, one can show that \(P(y)\) then has to a sum of terms of the form \(Q(y)^2 + (1-y^2) R(y)^2\), where \(Q\) and \(R\) are polynomials, which is the classical SOS characterization of non-negative polynomials in \([-1,1]\) [<a href="https://www.ams.org/journals/tran/2000-352-10/S0002-9947-00-02595-2/S0002-9947-00-02595-2.pdf">15</a>]. See all details and the extension to higher dimensions in [<a href="https://arxiv.org/pdf/2211.04889.pdf">33</a>].</p>



<h2>Extension to multivariate trigonometric polynomials</h2>



<p class="justify-text">We now consider multivariate trigonometric polynomials in dimension \(n\), on \([0,\! 1]^n\), which are polynomials in \(\cos 2\pi x_1, \sin 2\pi x_1, \dots, \cos 2\pi x_n, \sin 2 \pi x_n\). We first start with a counter-example to the equivalence between non-negativity and being a sum of squares.</p>



<p class="justify-text"><strong>Negative result (counter-example). </strong>Non-negative polynomials may not be sums-of-squares, as shown by this counter example based on Motzkin&#8217;s example [22]: $$M(1-\cos 2 \pi x_1,1-\cos 2\pi x_2, 1 &#8211; \cos 2 \pi x_3)$$ $$\mbox{ with } M(y_1,y_2,y_3) = y_1^2 y_2  + y_1  y_2^2 + y_3^3 \, &#8211; 3 y_1  y_2  y_3.$$ It is non-negative as (up to a factor of \(1/3\)) the difference between the arithmetic and geometric mean of \(y_1^2 y_2\), \(y_1  y_2^2\), and \(y_3^3\), and not a sum of squares.  Note that to check that \(f\) is not a sum-of-squares of polynomials <em>of a given degree</em>, we can solve an SDP and look at the smallest \(c \geqslant 0\) such that \(f + c\) is a sum-of-squares, and check that the optimal \(c\) is strictly greater than zero (we here obtain \(c \approx 1.1 \times 10^{-4}\) for degree \(2\) polynomials). It is a bit more involved to show that there is no SOS polynomials of any degree (cancellation of terms are possible so the degree can be higher than the one of \(f\)), see [22].</p>



<p>So not all non-negative polynomials are sums-of-squares in dimension larger than 1. This is however not the end of the story, as we now describe.</p>



<p class="justify-text"><strong>Towards positive results. </strong>We consider a trigonometric polynomial, that is, a function \(f(x) = P(e^{2i\pi x_1},\dots,e^{2i\pi x_n})\) where \(P\) is a function of the form $$P(z) = \sum_{ \omega \in \mathbb{Z}^d} c_\omega z^\omega, $$ where only a finite number of \(c_\omega\)&#8217;s are not equal to zero (we use the notation \(z^\omega = \prod_{i=1}^n z_i^{\omega_i}\)). Like in the one-dimensional case, the function \(f\) is real-valued as soon as \(c_{-\omega} = c_\omega^\ast\) for all \(\omega \in \mathbb{Z}^n\). These are exactly multivariate (real-valued) trigonometric polynomials.</p>



<p class="justify-text">We now state a few results relating non-negativity and sums-of-squares, whose &#8220;elementary&#8221; proofs will either be sketched at the end of this post (below references) or available in [33].</p>



<p class="justify-text"><strong>Strictly positive polynomials are sums-of-squares.</strong> For any multivariate trigonometric polynomial \(f\) of the form above, if \(f\) is <em>strictly positive</em> on \([0,\! 1]^n\), then there exists a finite family of trigonometric polynomials \((g_i)_{i \in I}\)  such that $$ \forall x \in [0,\! 1]^n, \ f(x) = \sum_{i \in I} |g_i(x)|^2.$$ This result dates back to [28], and has a reasonably simple proof based on Bochner&#8217;s theorem (see at the bottom of the post the proof from [23]).</p>



<p class="justify-text">There are two key deviations from the univariate case: </p>



<ol class="justify-text"><li>The degrees of the squares \(|g_i|^2\) can be larger than the degree of \(f\) and are not bounded a priori, thus, we need to try sums of squares of degrees larger than the degree of \(f\), thus creating a <em>hierarchy</em> of optimization problems of higher dimensions, whose optimal value will converge to the minimum of \(f\).</li><li>Only <em>strictly positive</em> polynomials can always be decomposed, implying we may not have finite convergence of the hierarchy in general.</li></ol>



<p class="justify-text">It implies that SOS hierarchies will always converge to the true value, without giving any idea of the convergence rate. The next result provides such a result (but still quite pessimistic), without any assumption.</p>



<p class="justify-text"><strong>Adding an arbitrarily small constant to a non-negative trigonometric polynomial makes it an SOS of known degree.</strong> Given a multivariate trigonometric polynomial of degree less than \(2r\) with values in \([0,\! 1]\), \(f + c\) is a sum of squares of polynomials of degree less than \(s \geqslant r\) as soon as \(c \geqslant c^\ast(r,s)\) for some \(c^\ast(r,s)\). Moreover, when \(s \to +\infty\), we have recently shown in [33] that, asymptotically, \(c^\ast(r,s) \leqslant \frac{6 r^2 n}{s^2} \sum_{\omega \in \mathbb{Z}^n} | \hat{f}(\omega)|\). This immediately provides the convergence rate \(c^\ast(r,s)\) for the SOS relaxation of degree \(s\).</p>



<p class="justify-text">This results in an adaptation of<span style="font-size: revert; color: initial; font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen-Sans, Ubuntu, Cantarell, &quot;Helvetica Neue&quot;, sans-serif;"> [20] to \([0,\! 1]^n\) rather than the hypersphere, which makes the proof significantly faster (see [<a href="https://arxiv.org/pdf/2211.04889.pdf">33</a>] and the related results for regular polynomials on \([-1,1]^n\) in [<a href="https://link.springer.com/content/pdf/10.1007/s11590-022-01922-5.pdf">34</a>]). It provides a quantitative </span>guarantee (<span style="font-size: revert; color: initial; font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen-Sans, Ubuntu, Cantarell, &quot;Helvetica Neue&quot;, sans-serif;">even more explicit than for regular polynomials [26])</span>: the approximation guarantee scales as \(s^{-2}\), with a feature dimension \(d = (2s+1)^n \sim s^n\), and thus with a feature of dimension \(d\), we get an error proportional to \(d^{-2/n}\), thus with a scaling in the underlying dimension \(n\) subject to the curse of dimensionality.</p>



<p class="justify-text"><strong>Comparison with spectral relaxation.</strong> Using the same reasoning as in dimension \(n=1\), we get instead a convergence rate in \(O(1/s)\) for the spectral algorithm. So we get convergence, at rate which is only slightly worse. However, the behavior in \(O(1/s)\) is empirically observed in practice in all cases, while the rate in \(O(1/s^2)\) for the SOS relaxation is pessimistic, and the convergence rate is <em>much</em> better in practice. Can we obtain tighter convergence rates?</p>



<h2 class="justify-text">Exponential convergence</h2>



<p class="justify-text">In order to show exponential convergence when the degree \(s\) goes to infinity, we need to add an extra assumption regarding the behavior of the function \(f\) around its minimizer. Essentially, all global minimizers are isolated and the Hessian is invertible. This is often referred to as &#8220;strict-second-order minimizers&#8221;, and such assumptions have already been used to show finite convergence of the hierarchy, although with no bound on the required degree [<a href="https://link.springer.com/content/pdf/10.1007/s10107-013-0680-x.pdf">35</a>].</p>



<p class="justify-text">Leveraging our earlier work [21, 24], we recently showed in [<a href="https://arxiv.org/pdf/2211.04889.pdf">33</a>] with Alessandro Rudi that the convergence rate was exponential when the minimum is attained at isolated points with invertible Hessians. The proof is a bit involved, but rely on a crucial point. We deviate from previous work on polynomial hierarchies by a strong focus on <em>smoothness properties</em> of the optimization problems rather than its algebraic properties. More precisely, this allows us to use square roots and matrix square roots together with their differentiability properties (square roots typically lead to non-polynomial functions when taken on polynomials).</p>



<h2>Optimization beyond trigonometric polynomials</h2>



<p class="justify-text">When \(f\) is not a trigonometric polynomial, there are essentially two options: (1) approximate with a trigonometric polynomial and minimize the approximation with the SOS hierarchy, or (2) target directly the minimization of the original function through sampling. The second option is described in the next section (and was already the topic of an <a href="https://francisbach.com/finding-global-minima-with-kernel-approximations/">earlier blog post</a>). We now describe the first option.</p>



<p class="justify-text"><strong> Approximation by truncated Fourier series.</strong> Depending on how the function is accessed, through function values or through values of its Fourier series, the approximation of a regular function \(f\) through a degree \(r\) polynomial is a well-studied problem. For simplicity, I only consider truncating the Fourier series obtained by keeping frequencies \(\omega\) such that \(\| \omega\|_\infty \leqslant r\). The error which is made depends on the regularity of the original function, that is, if \(f\) has all of its \((\beta + n)\)-th order derivatives that are bounded, then the truncation error is of order \(1/r^{\beta}\). Thus, to reach precision \(\varepsilon\), we need \(d \sim r^n \sim \varepsilon^{-n/\beta}\) basis functions. See an illustration below in two dimensions, where aim to approximate the function below.</p>


<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img loading="lazy" src="https://francisbach.com/wp-content/uploads/2022/11/bike-1.gif" alt="" class="wp-image-8433" width="447" height="356"/></figure></div>


<p class="justify-text">The function is approximated by computing the Fourier series and keeping only frequencies \(\omega\) such that \(\| \omega \|_\infty \leqslant r\) for varying \(r\).</p>


<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img loading="lazy" src="https://francisbach.com/wp-content/uploads/2022/11/velo_continuous-1.gif" alt="" class="wp-image-8434" width="388" height="375"/></figure></div>


<p></p>



<h2>Extension to function value oracle</h2>



<p class="justify-text">Now that the traditional &#8220;dÃ©cor&#8221; of sum-of-squares optimization based on finite-dimensional expansions has been set, we can look at recent infinite-dimensional extensions based on positive definite kernels.</p>



<p class="justify-text"><strong>From knowing \(\mathcal{V}\) to exact sampling.</strong> In the formulation above,&nbsp;instead of using an explicit representation of \(f(x)\) as \(\varphi(x)^\ast F \varphi(x)\), and determine precisely \(\mathcal{V}\), an alternative is to use sufficiently many distinct points \(x_1,\dots,x_m \in \mathcal{X}\), and use the constraint: $$ \forall i \in \{1,\dots,m\}, \ f(x_i) -c = \varphi(x_i)^\ast A \varphi(x_i). $$ If the matrices \(\varphi(x_i) \varphi(x_i)^\ast\), \(i \in \{1,\dots,m\}\), span \(\mathcal{V}\), this is equivalent to the original formulation [<a href="https://epubs.siam.org/doi/pdf/10.1137/15M1052548">25</a>], and leads to formulations that are less cumbersome to code (multivariate Toeplitz matrices are an example). An extra advantage is to access \(f\) only through a function-value oracle (often called a zero-th order oracle). Note that we often need to take \(m\) larger than the dimension \(2d\) (the real dimension of \(\mathbb{C}^d\)), but always less than \(d^2\) (the real dimension of the set of Hermitian matrices).</p>



<p><strong>Undersampling and regularization.</strong> A key insight from [<a href="https://www.di.ens.fr/~fbach/gloptikernel.pdf">21</a>] is to consider situations where the number of samples \( m\) is not large enough, and add a regularizer \( -\lambda {\rm tr}(A)\), that is,&nbsp;$$ \sup_{c \in \mathbb{R}, \ A \succcurlyeq 0} c\,  &#8211; \lambda {\rm tr}(A) \&nbsp; \mbox{ such that } \ \forall i \in \{1,\dots,m\}, \ f(x_i) -c = \varphi(x_i)^\ast A \varphi(x_i).$$ The dual is then: $$  \inf_{\alpha \in \mathbb{R}^m} \sum_{i=1}^m \alpha_i f(x_i) \mbox{ such that } \sum_{i=1}^m \alpha_i = 1, \ \sum_{i=1}^m \alpha_i \varphi(x_i)\varphi(x_i)^\ast + \lambda I \succcurlyeq 0. $$</p>



<p class="justify-text">When \( m\) is large enough and \( \lambda\) is sufficiently small, this leads to a controlled approximation. This is particularly interesting when \(\varphi(x)\) is infinite-dimensional and such that&nbsp;the relaxation is tight, which we now look at.</p>



<p class="justify-text"><strong>Infinite-dimensional feature map.</strong> In the trigonometric polynomial example, we can consider $$\varphi(x)_\omega = \hat{q}(\omega)^{1/2} e^{2i \pi \omega^\top x},$$ with an extra weight function \(\hat{q}: \mathbb{Z}^n \to \mathbb{R}_+\), such that \(\sum_{\omega \in \mathbb{Z}^n} \hat{q}(\omega) = 1\), so that \(\| \varphi(x)\| = 1\) (we essentially get a probability distribution on \(\mathbb{Z}^n\)). Finite-dimensional embeddings correspond to \(\hat{q}\) having finite support, but we can now to apply the SOS technique in infinite dimensions, since \(\hat{q}\) may have infinitely many non-zero values.  </p>



<p class="justify-text"><strong>Tightness.</strong> A key benefit of going infinite-dimensional is that the SOS relaxation is always tight as soon as \(\hat{q}(\omega) &gt;0\) for all \(\omega \in \mathbb{Z}^d\). Indeed, the relaxation is the following optimization problem: $$\inf_{ \mu \ {\rm finite \ measure} } \int_{\mathcal{X}} f(x) d\mu(x) \ \mbox{ such that } \ \int_{\mathcal{X}} \varphi(x)\varphi(x)^\ast d\mu(x) \succcurlyeq 0, \ \int_\mathcal{X} d\mu(x) = 1.$$ Looking at all finite sub-matrices of \(\int_{\mathcal{X}} \varphi(x)\varphi(x)^\ast d\mu(x)\), we see that the function \(\omega \mapsto \int_{[0, 1]^n} e^{2i\pi \omega^\top x} d\mu(x)\) is positive definite, and thus by <a href="https://en.wikipedia.org/wiki/Bochner%27s_theorem">Bochner&#8217;s theorem</a>, \(\mu\) has to be a probability measure.</p>



<p class="justify-text">We can now combine with &#8220;sampling + regularization&#8221; technique mentioned above, and only query the function at a finite number of points. As shown in [21, 24], with a proper choice of \(\lambda\), then we get a convergence rate of \(O(1/m^{-\beta/n})\) when \(f\) is \((\beta + 2+ n/2)\)-times differentiable. When \(f\) is infinitely-differentiable (like for functions with finite support), we can get exponential convergence in \(m\), but with a constant in from of the exponential that can be exponential in dimension. See more details in an <a href="https://francisbach.com/finding-global-minima-with-kernel-approximations/">earlier blog post</a> and in [33].</p>



<p class="justify-text"><strong style="font-size: revert; color: initial; font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen-Sans, Ubuntu, Cantarell, &quot;Helvetica Neue&quot;, sans-serif;">Representer theorem. </strong><span style="font-size: revert; color: initial; font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen-Sans, Ubuntu, Cantarell, &quot;Helvetica Neue&quot;, sans-serif;">Another consequence is a &#8220;representer theorem&#8221;, that is, we can reduce the search to \(A = \sum_{i,j=1}^m C_{ij} \varphi(x_i)\varphi(x_j)^\ast\) with \( C \in  \mathbb{R}^{m \times m}\) positive-semidefinite. </span>Like for traditional kernel methods, the representer theorem is particularly interesting when \(d\) is infinite like above.</p>



<p class="justify-text">But this is already useful with finite-dimensional embeddings where the span \(\mathcal{V}\) is difficult to characterize. In this case, only the kernel has to be simple to compute; for example, for the usual finite support case, we have $$k(x,y) = \frac{1}{2r+1} \frac{ \sin (2r+1) \pi x }{\sin \pi x},$$ in one dimension (this extends to all dimensions). We can also get a closed form for \(\hat{q}(\omega) = \rho^{\|\omega\|_1}\).</p>



<h2>Extension to Boolean hypercube</h2>



<p class="justify-text">When \(\mathcal{X} = \{-1,1\}^n \), we can use feature vectors composed of Boolean Fourier components of increasing orders [<a href="https://arxiv.org/pdf/2105.10386.pdf">16</a>]. This corresponds to features of the form \(\varphi_A(x) = \prod_{i \in A} x_i \in \{-1,1\}\), where \(A\) is a subset of \(\{1,\dots,n\}\). They are all normalized.</p>



<p class="justify-text">If we consider a set \( \mathcal{A}\) of subsets of \( \{1,\dots,n\}\), then, the element indexed \((A,B)\) of \(\varphi(x) \varphi(x)^\ast\) only depends on the symmetric difference \(A \Delta B = ( A \backslash B) \cup ( B \backslash A)\). The relaxation is not tight in general, but if we see our moment matrix as a submatrix obtained from a sufficiently larger set of subsets, then we obtain a tight formulation (here we know that the hierarchy has to be finite [<a href="https://link.springer.com/content/pdf/10.1007/3-540-45535-3.pdf">17</a>, <a href="https://pubsonline.informs.org/doi/pdf/10.1287/moor.28.3.470.16391">18</a>, <a href="https://arxiv.org/pdf/2011.04027.pdf">19</a>]). </p>



<p class="justify-text">At the first layer (with \( \mathcal{A}\) composed of singletons), we get the traditional semi-definite programming relaxation of quadratic optimization problems on \(\{-1,1\}^n\) [<a href="https://dl.acm.org/doi/pdf/10.1145/227683.227684">30</a>]. See this nice <a href="https://www.sumofsquares.org/">website</a> for the many extensions. An interesting aspect is that tightness is not necessary in many applications.</p>



<h2>Extension to hyperspheres</h2>



<p class="justify-text">It is common to see the set \([0,\! 1]\) with periodic functions as the one-dimensional <a href="https://en.wikipedia.org/wiki/Torus">torus</a>, that is, the quotient set \(\mathbb{R} / \mathbb{Z}\). Another classical interpretation is to see it as the unit circle in \(\mathbb{R}^2\), with the bijection \(x \mapsto (\cos 2\pi x, \sin 2 \pi x)\). The real Fourier basis functions \(x \mapsto \cos 2 \pi \omega x\) and \(x \mapsto \sin 2 \pi \omega x\) are then polynomials \(P(y_1,y_2)\) in \(y_1 = \cos 2\pi x\) and \(y_2 = \sin 2\pi x\). Not all polynomials are recovered, as one can check that we need to have \(\Delta P = \frac{\partial^2 P}{\partial y_1^2} +  \frac{\partial^2 P}{\partial y_2^2}=0\), that is the Laplacian of \(P\) is zero, which is often referred to as being <a href="https://en.wikipedia.org/wiki/Harmonic_polynomial">harmonic</a>. This extends to hyperspheres in any dimension (see [<a href="https://arxiv.org/pdf/1908.05155.pdf">20</a>] for details), and also to Euclidean balls by adding a dimension.</p>



<h2>Conclusion</h2>



<p class="justify-text">In this blog post, I have tried to describe the basics of sums-of-squares optimization, hopefully from a simple perspective. The key was to reduce the scope to the minimization over simple sets with no complex constraints. This allowed us to present significantly improved convergence results from [33].</p>



<p class="justify-text">There is of course many more interesting ideas to cover, such as constrained optimisation, applications within signal processing [8], and extensions to the computation of log-partition functions [32], as well as important practical scalability issues to alleviate. Moreover, the SOS framework goes beyond optimization, either through other infinite-dimensional optimization problems or moment-based formulations (see many examples in [4, 7]). Many topics for future posts!</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Alessandro Rudi for proofreading this blog post and making good clarifying suggestions.</p>



<p>&nbsp;</p>



<h2>References</h2>



<p class="justify-text">[1] Jean-Bernard Lasserre. <a href="https://epubs.siam.org/doi/pdf/10.1137/S1052623400366802">Global optimization with polynomials and the problem of moments</a>. <em>SIAM Journal on Optimization</em>, 11(3):796â817, 2001.<br>[2] Pablo A. Parrilo. <a href="http://www.mit.edu/~parrilo/pubs/files/SDPrelaxations.pdf">Semidefinite programming relaxations for semialgebraic problems</a>. <em>Mathematical Programming</em>, 96(2):293â320, 2003.<br>[3] Monique Laurent. <a href="https://homepages.cwi.nl/~monique/files/laurent-ima.pdf">Sums of squares, moment matrices and optimization over polynomials</a>. In <em>Emerging applications of algebraic geometry</em>. Springer, 157-270, 2009.<br>[4] Jean-Bernard Lasserre.&nbsp;<em><a href="https://www.worldscientific.com/worldscibooks/10.1142/p665#t=aboutBook">Moments, positive polynomials and their applications</a></em>. World Scientific, 2009.<br>[5] Grigoriy Blekherman, Pablo A. Parrilo, Rekha R. Thomas, editors.&nbsp;<em><a href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611972290.fm">Semidefinite optimization and convex algebraic geometry</a></em>. Society for Industrial and Applied Mathematics, 2012.<br>[6] Jean-Bernard Lasserre.&nbsp;<em><a href="https://www.cambridge.org/core/books/an-introduction-to-polynomial-and-semialgebraic-optimization/5A7C6F7E54E28CE72BA7A7F84D0858ED">An introduction to polynomial and semi-algebraic optimization</a></em>. Cambridge University Press, 2015.<br>[7] Didier Henrion, Milan Korda, and Jean-Bernard Lasserre.&nbsp;<em><a href="https://www.worldscientific.com/worldscibooks/10.1142/q0252#t=aboutBook">The Moment-SOS Hierarchy: Lectures In Probability, Statistics, Computational Geometry, Control And Nonlinear PDEs</a></em>. World Scientific, 2020.<br>[8] Bogdan Dumitrescu.&nbsp;<em><a href="https://link.springer.com/book/10.1007/978-3-319-53688-0">Positive trigonometric polynomials and signal processing applications</a></em>. Springer, 2007.<br>[9] Robert M. Gray. <a href="https://ee.stanford.edu/~gray/toeplitz.pdf">Toeplitz and circulant matrices: A review</a>. <em>Foundations and Trends in Communications and Information Theory</em>&nbsp;2(3): 155-239, 2006.<br>[10] Johannes Jahn. <em><a href="https://link.springer.com/book/10.1007/978-3-540-49379-2">Introduction to the Theory of Nonlinear Optimization</a></em>. Springer, 2020<br>[11] Leopold FejÃ©r. <a href="https://www.degruyter.com/document/doi/10.1515/crll.1916.146.53/pdf">Ãber trigonometrische Polynome</a>. <em>Journal fÃ¼r die reine und angewandte Mathematik</em>. 146:55-82, 1916.<br>[12] Friedrich Riesz. <a href="https://www.degruyter.com/document/doi/10.1515/crll.1916.146.83/pdf">Ãber ein Problem des Herrn CarathÃ©odory</a>. <em>Journal fÃ¼r die reine und angewandte Mathematik</em>. 146:83-87, 1916.<br>[13] C. CarathÃ©odory and L. FejÃ©r. <a href="https://ia800708.us.archive.org/view_archive.php?archive=/28/items/crossref-pre-1923-scholarly-works/10.1007%252Fbf02983491.zip&amp;file=10.1007%252Fbf03014796.pdf">Ãber den Zusammenhang der Extreme von harmonischen Funktionen mit ihren Koeffizienten und Ã¼ber den Picard-Landauschen Satz</a>, Rendiconti del Circolo Matematico di Palermo, 32:218â239, 1911.<br>[14] MihÃ¡ly Bakonyi, Ekaterina V. Lopushanskaya. <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-0346-0180-1_4.pdf">Moment problems for real measures on the unit circle</a>.&nbsp;<em>Recent Advances in Operator Theory in Hilbert and Krein Spaces</em>. BirkhÃ¤user Basel, 49-60, 2009.<br>[15] Victoria Powers and Bruce Reznick. <em><a href="https://www.ams.org/journals/tran/2000-352-10/S0002-9947-00-02595-2/S0002-9947-00-02595-2.pdf">Polynomials that are positive on an interval</a></em>. Transactions of the American Mathematical Society, 352(10):4677â4692, 2000.<br>[16] Ryan OâDonnell. <em><a href="https://arxiv.org/pdf/2105.10386.pdf">Analysis of Boolean functions</a></em>. Cambridge University Press, 2014.<br>[17] Jean-Bernard Lasserre. <a href="https://link.springer.com/content/pdf/10.1007/3-540-45535-3.pdf">An explicit exact SDP relaxation for nonlinear 0â1 programs</a>. In <em>International Conference on Integer Programming and Combinatorial Optimization</em>, pages 293â303. Springer, 2001.<br>[18] Monique Laurent. <a href="https://pubsonline.informs.org/doi/pdf/10.1287/moor.28.3.470.16391">A comparison of the Sherali-Adams, LovÃ¡sz-Schrijver, and Lasserre relaxations for 0â1 programming</a>. <em>Mathematics of Operations Research</em>, 28(3):470â496, 2003.<br>[19] Lucas Slot and Monique Laurent. <a href="https://arxiv.org/pdf/2011.04027.pdf">Sum-of-squares hierarchies for binary polynomial optimization</a>. <em>Mathematical Programming</em>, pages 1â40, 2022.<br>[20] Kun Fang and Hamza Fawzi. <a href="https://arxiv.org/pdf/1908.05155.pdf">The sum-of-squares hierarchy on the sphere and applications in quantum information theory</a>. <em>Mathematical Programming</em>, 190(1):331â360, 2021.<br>[21] Alessandro Rudi, Ulysse Marteau-Ferey, Francis Bach. <a href="https://www.di.ens.fr/~fbach/gloptikernel.pdf">Finding Global Minima via Kernel </a><a href="https://arxiv.org/pdf/2012.11978">Approximations</a>. Technical report arXiv:2012.11978, 2020.<br>[22] Aron Naftalevich, and M. Schreiber. <a href="https://link.springer.com/content/pdf/10.1007/BFb0074598.pdf">Trigonometric polynomials and sums of squares</a>. In <em>Number Theory</em>. Springer, 225-238, 1985.<br>[23] Alexandre Megretski. <a href="http://www.mit.edu/~ameg/images/sos_cdc.ps">Positivity of trigonometric polynomials</a>.&nbsp;In <em>International Conference on Decision and Control</em>, 2003.<br>[24] Blake Woodworth, Francis Bach, Alessandro Rudi.&nbsp;<a href="https://proceedings.mlr.press/v178/woodworth22a/woodworth22a.pdf">Non-Convex Optimization with Certificates and Fast Rates Through Kernel Sums of Squares</a>. In <em>Proceedings of the Conference on Learning Theory</em>, 2022.<br>[25] Diego Cifuentes, Pablo A. Parrilo. <a href="https://epubs.siam.org/doi/pdf/10.1137/15M1052548">Sampling algebraic varieties for sum of squares programs</a>.&nbsp;<em>SIAM Journal on Optimization</em>,&nbsp;27(4): 2381-2404, 2017.<br>[26] Jean-Bernard Lasserre. <a href="https://epubs.siam.org/doi/pdf/10.1137/070693709">A sum of squares approximation of nonnegative polynomials</a>.&nbsp;<em>SIAM Review</em>,&nbsp;49(4):651-669, 2007.<br>[27] Yurii Nesterov. <a href="https://link.springer.com/content/pdf/10.1007/s10107-006-0001-8.pdf">Smoothing technique and its applications in semidefinite optimization</a>.&nbsp;<em>Mathematical Programming</em>&nbsp;110(2):245-259, 2007.<br>[28] Mihai Putinar. <a href="https://gallica.bnf.fr/ark:/12148/bpt6k58688425/f747.item">Sur la complexification du problÃ¨me des moments</a>. <em>Comptes Rendus de l&#8217;AcadÃ©mie des Sciences, </em>SÃ©rie 1, MathÃ©matique&nbsp;314(10):743-745, 1992.<br>[29] Didier Henrion and Jean-Bernard Lasserre. <a href="https://homepages.laas.fr/henrion/papers/extract.pdf">Detecting global optimality and extracting solutions in GloptiPoly</a>. In&nbsp;<em>Positive polynomials in control</em>, pages 293-310, 2005.<br>[30] Michel X. Goemans and David P. Williamson. <a href="https://dl.acm.org/doi/pdf/10.1145/227683.227684">Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming</a>. <em>Journal of the ACM</em>, 42(6):1115â1145, 1995.<br>[31] Ulf Grenander, and Gabor SzegÃ¶.&nbsp;<em>Toeplitz forms and their applications</em>. Univ of California Press, 1958.<br>[32] Francis Bach.&nbsp;<a href="https://arxiv.org/pdf/2206.13285">Sum-of-Squares Relaxations for Information Theory and Variational Inference</a>. Technical report, arXiv:2206.13285, 2022.<br>[33] Francis Bach and Alessandro Rudi. <a href="https://arxiv.org/pdf/2211.04889.pdf">Exponential convergence of sum-of-squares hierarchies for trigonometric polynomials</a>. Technical report arXiv:2211.04889, 2022.<br>[34] Monique Laurent and Lucas Slot. <a href="https://link.springer.com/content/pdf/10.1007/s11590-022-01922-5.pdf">An effective version of SchmÃ¼dgenâs Positivstellensatz for the hypercube</a>. <em>Optimization Letters</em>, 2022.<br>[35] Jiawang Nie. <a href="https://link.springer.com/content/pdf/10.1007/s10107-013-0680-x.pdf">Optimality conditions and finite convergence of Lasserreâs hierarchy</a>. <em>Mathematical Programming</em>, 146(1):97â121, 2014</p>



<p class="justify-text"><br></p>



<h2>Proof of FejÃ©r-Riesz theorem</h2>



<p class="justify-text">We consider a trigonometric polynomial of degree \(r\), that is, \(g(x) = \sum_{\omega = -r}^r c_\omega e^{2i\pi \omega x}\) with real non-negative values. This imposes that \(c_{-\omega} = c_\omega^\ast\). We assume that \(c_r \neq 0\). We consider the rational function \(R(z) = \sum_{\omega = -r}^r c_\omega z^\omega\), so that \(g(x) = R(e^{2i\pi x})\). We can write \(R\) as \(R(z) = z^{-r} Q(z)\) for a polynomial \(Q\) of degree \(2r\). We can now examine the roots of \(R\). The identity \(c_{-\omega} = c_\omega^\ast\) implies that if \(z\) is a root of \(Q\), so is \(1/z^\ast\). Thus all roots which do not have unit modulus come in pairs \((z_b,1/z_b^\ast)\), for \(| z_b| &lt; 1\), with \(b \in \{1,\dots,m\}\). We then have \(2r \, &#8211; 2m\) roots of unit modulus, denoted \(e^{2i\pi \rho_a}\), for \(a \in \{1,\dots,2r-2m\}\).</p>



<p class="justify-text">Thus, we can write the polynomial \(Q\) as: $$Q(z) = \kappa \prod_{a=1}^{2r-2m} ( z  \, &#8211; e^{2i\pi \rho_a})  \prod_{b = 1}^m ( z \, &#8211; z_b)( z \, &#8211; 1/z_b^\ast), $$ for a certain \(\kappa \in \mathbb{C}\), leading to $$R(z) = z^{-r} Q(z) = \kappa&#8217; z^{-r+m} \prod_{a=1}^{2r-2m} ( z \, &#8211; e^{2i\pi \rho_a}) \prod_{b =1}^m ( z \, &#8211; z_b)( z^{-1}  \, &#8211;  z_b^\ast),$$ for some \(\kappa&#8217;  \in \mathbb{C}\). For \(z = e^{2i\pi x}\), the second product is a positive number since \(z^{-1} = z^\ast\). Writing \( e^{2i\pi x}  \, &#8211; e^{2i\pi \rho_a} = 2i e^{i\pi(x+\rho_a)} \sin \pi (x-\rho_a)\), we can see that in order to avoid a change of sign of \(\sin \pi (x-\rho_a)\) around some \(x\), each \(\rho_a\) must appear with even multiplicity. Thus, for \(z = e^{2i\pi x}\), the term \(z^{-r+m} \prod_{a=1}^{2r-2m} ( z \, &#8211; e^{2i\pi \rho_a})\) is equal to \(\prod_{a=1}^{r-m} (-4) e^{2 i\pi \rho_a} \sin^2 \pi (x-\rho_a)\), and thus, \(\kappa^{\prime\prime} = \kappa&#8217; \prod_{a=1}^{r-m} (-4)  e^{2 i\pi \rho_a}\) has to be a non-negative real number. Thus, overall, $$g(x) = R(e^{2i\pi x}) = \Big| \sqrt{\kappa^{\prime\prime}}  \prod_{a=1}^r (  e^{2i\pi x}  &#8211; e^{2i\pi \rho_a}) \prod_{b =1}^m ( e^{2i\pi x}  &#8211; z_b) \Big|^2.$$ </p>



<p class="justify-text">We here get a single square, with an elementary proof. Note that this is a stronger result than being a sum of several squares. We can now look at the dual intepretation.</p>



<h2>First-order algorithms</h2>



<p class="justify-text">In order to solve the problem in Eq. \((1)\), we can use standard smoothing [27], that is, replace \(\lambda_{\min}(F+Y)\) by \(\varepsilon \log {\rm tr } \exp((F+Y) / \varepsilon)\), and apply accelerated projected gradient descent, which requires to orthogonally project on \(\mathcal{V}^\perp\), or equivalently on \(\mathcal{V}^\perp\).</p>



<h2>All strictly positive trigonometric polynomials are sums-of-squares</h2>



<p class="justify-text">In order to provide an &#8220;elementary&#8221; proof of this result that dates back to [28], we follow [23] and proceed by contradiction, and assume that there is a strictly positive trigonometric polynomial \(R\) which is not a sum-of-squares. Since the set of SOS polynomials is a convex cone, by the <a href="https://en.wikipedia.org/wiki/Hahn%E2%80%93Banach_theorem">Hahn-Banach theorem</a>, there must exist a non identically zero linear form \(L\) which is non-negative on all SOS polynomials, and such that \(L(R) \leqslant 0\). In order to conclude, we will need to show that linear forms on trigonometric polynomials that are non-negative on SOS polynomials, can be written as  $$\tag{4} L(P)  = \int_{[0, 1]^n} P(e^{ 2i\pi x_1} ,\dots, e^{2i\pi x_n}) d\mu(x)$$ for a non-negative measure \(\mu\) on \([0,\! 1]^n\) (which is not uniformly equal to zero). This then leads to a contradiction, since it implies $$0 \geqslant L(R) \geqslant \min_{x \in [0, 1]^n} R(e^{ 2i\pi x_1} ,\dots, e^{2i\pi x_n})  \times \mu([0,\! 1]^n).$$</p>



<p class="justify-text">We now need to show that all bounded linear forms on trigonometric polynomials that are non-negative on SOS polynomials can be represented as in Eq. \((4)\).</p>



<p class="justify-text"><strong>Using <a href="https://en.wikipedia.org/wiki/Bochner%27s_theorem">Bochner&#8217;s theorem</a>.</strong> We define a function \(g: \mathbb{Z}^n \to \mathbb{C}\) as \(g(\omega) = L( z^\omega)\). This function is <a href="https://en.wikipedia.org/wiki/Positive-definite_function">positive definite</a>, as, for any \(\omega_1,\dots,\omega_m \in \mathbb{Z}^n\) and any complex numbers \(\alpha_1,\dots,\alpha_m\), $$ \sum_{i,j=1}^m \alpha_i \alpha_j^\ast g(\omega_i &#8211; \omega_j) = L \Big( \Big| \sum_{i=1}^m \alpha_i z^{\omega_i} \Big|^2 \Big).$$ Thus by Bochner&#8217;s theorem, there exists a positive measure \(\mu\) on \([0,\! 1]^n\) such that $$g(\omega) = \int_{[0, 1]^n} e^{2i\pi \omega^\top x} d\mu(x),$$ which exactly leads to the desired result.</p>
<p class="authors">By Francis Bach</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T12:53:49Z">Wednesday, November 16 2022, 12:53</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07691'>Low-depth arithmetic circuit lower bounds via shifted partials</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Prashanth Amireddy, Ankit Garg, Neeraj Kayal, Chandan Saha, Bhargav Thankey</p><p>We prove super-polynomial lower bounds for low-depth arithmetic circuits
using the shifted partials measure [Gupta-Kamath-Kayal-Saptharishi, CCC 2013],
[Kayal, ECCC 2012] and the affine projections of partials measure
[Garg-Kayal-Saha, FOCS 2020], [Kayal-Nair-Saha, STACS 2016]. The recent
breakthrough work of Limaye, Srinivasan and Tavenas [FOCS 2021] proved these
lower bounds by proving lower bounds for low-depth set-multilinear circuits. An
interesting aspect of our proof is that it does not require conversion of a
circuit to a set-multilinear circuit, nor does it involve a random restriction.
We are able to upper bound the measures for homogeneous formulas directly,
without going via set-multilinearity. Our lower bounds hold for the iterated
matrix multiplication as well as the Nisan-Wigderson design polynomials. We
also define a subclass of homogeneous formulas which we call unique parse tree
(UPT) formulas, and prove superpolynomial lower bounds for these. This
generalizes the superpolynomial lower bounds for regular formulas in
[Kayal-Saha-Saptharishi, STOC 2014], [Fournier-Limaye-Malod-Srinivasan, STOC
2014].
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Amireddy_P/0/1/0/all/0/1">Prashanth Amireddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1">Ankit Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Kayal_N/0/1/0/all/0/1">Neeraj Kayal</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_C/0/1/0/all/0/1">Chandan Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Thankey_B/0/1/0/all/0/1">Bhargav Thankey</a></p><p>We prove super-polynomial lower bounds for low-depth arithmetic circuits
using the shifted partials measure [Gupta-Kamath-Kayal-Saptharishi, CCC 2013],
[Kayal, ECCC 2012] and the affine projections of partials measure
[Garg-Kayal-Saha, FOCS 2020], [Kayal-Nair-Saha, STACS 2016]. The recent
breakthrough work of Limaye, Srinivasan and Tavenas [FOCS 2021] proved these
lower bounds by proving lower bounds for low-depth set-multilinear circuits. An
interesting aspect of our proof is that it does not require conversion of a
circuit to a set-multilinear circuit, nor does it involve a random restriction.
We are able to upper bound the measures for homogeneous formulas directly,
without going via set-multilinearity. Our lower bounds hold for the iterated
matrix multiplication as well as the Nisan-Wigderson design polynomials. We
also define a subclass of homogeneous formulas which we call unique parse tree
(UPT) formulas, and prove superpolynomial lower bounds for these. This
generalizes the superpolynomial lower bounds for regular formulas in
[Kayal-Saha-Saptharishi, STOC 2014], [Fournier-Limaye-Malod-Srinivasan, STOC
2014].
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07900'>Parameterized Inapproximability of the Minimum Distance Problem over all Fields and the Shortest Vector Problem in all $\ell_p$ Norms</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Huck Bennett, Mahdi Cheraghchi, Venkatesan Guruswami, Jo&#xe3;o Ribeiro</p><p>We prove that the Minimum Distance Problem (MDP) on linear codes over any
fixed finite field and parameterized by the input distance bound is W[1]-hard
to approximate within any constant factor. We also prove analogous results for
the parameterized Shortest Vector Problem (SVP) on integer lattices.
Specifically, we prove that SVP in the $\ell_p$ norm is W[1]-hard to
approximate within any constant factor for any fixed $p &gt;1$ and W[1]-hard to
approximate within a factor approaching $2$ for $p=1$. (We show hardness under
randomized reductions in each case.)
</p>
<p>These results answer the main questions left open (and explicitly posed) by
Bhattacharyya, Bonnet, Egri, Ghoshal, Karthik C. S., Lin, Manurangsi, and Marx
(Journal of the ACM, 2021) on the complexity of parameterized MDP and SVP. For
MDP, they established similar hardness for binary linear codes and left the
case of general fields open. For SVP in $\ell_p$ norms with $p &gt; 1$, they
showed inapproximability within some constant factor (depending on $p$) and
left open showing such hardness for arbitrary constant factors. They also left
open showing W[1]-hardness even of exact SVP in the $\ell_1$ norm.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bennett_H/0/1/0/all/0/1">Huck Bennett</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheraghchi_M/0/1/0/all/0/1">Mahdi Cheraghchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Guruswami_V/0/1/0/all/0/1">Venkatesan Guruswami</a>, <a href="http://arxiv.org/find/cs/1/au:+Ribeiro_J/0/1/0/all/0/1">Jo&#xe3;o Ribeiro</a></p><p>We prove that the Minimum Distance Problem (MDP) on linear codes over any
fixed finite field and parameterized by the input distance bound is W[1]-hard
to approximate within any constant factor. We also prove analogous results for
the parameterized Shortest Vector Problem (SVP) on integer lattices.
Specifically, we prove that SVP in the $\ell_p$ norm is W[1]-hard to
approximate within any constant factor for any fixed $p &gt;1$ and W[1]-hard to
approximate within a factor approaching $2$ for $p=1$. (We show hardness under
randomized reductions in each case.)
</p>
<p>These results answer the main questions left open (and explicitly posed) by
Bhattacharyya, Bonnet, Egri, Ghoshal, Karthik C. S., Lin, Manurangsi, and Marx
(Journal of the ACM, 2021) on the complexity of parameterized MDP and SVP. For
MDP, they established similar hardness for binary linear codes and left the
case of general fields open. For SVP in $\ell_p$ norms with $p &gt; 1$, they
showed inapproximability within some constant factor (depending on $p$) and
left open showing such hardness for arbitrary constant factors. They also left
open showing W[1]-hardness even of exact SVP in the $\ell_1$ norm.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07923'>A Theory for Discrete-time Boolean Finite Dynamical Systems with Uncertainty</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mitsunori Ogihara, Kei Uchizawa</p><p>Dynamical Systems is a field that studies the collective behavior of objects
that update their states according to some rules. Discrete-time Boolean Finite
Dynamical System (DT-BFDS) is a subfield where the systems have some finite
number of objects whose states are Boolean values, and the state updates occur
in discrete time. In the subfield of DT-BFDS, researchers aim to (i) design
models for capturing real-world phenomena and using the models to make
predictions and (ii) develop simulation techniques for acquiring insights about
the systems' behavior. Useful for both aims is understanding the system
dynamics mathematically before executing the systems. Obtaining a mathematical
understanding of BFDS is quite challenging, even for simple systems, because
the state space of a system grows exponentially in the number of objects.
Researchers have used computational complexity to circumvent the challenge. The
complexity theoretic research in DT-BFDS has successfully produced complete
characterizations for many dynamical problems.
</p>
<p>The DT-BFDS studies have mainly dealt with deterministic models, where the
update at each time step is deterministic, so the system dynamics are
completely determinable from the initial setting. However, natural systems have
uncertainty. Models having uncertainty may lead to far-better understandings of
nature. Although a few attempts have explored DT-BFDS with uncertainty,
including stochastic initialization and tie-breaking, they have scratched only
a tiny surface of models with uncertainty. The introduction of uncertainty can
be through two schemes. One is the introduction of alternate update functions.
The other is the introduction of alternate update schedules. 37This paper
establishes a theory of models with uncertainty and proves some fundamental
results.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ogihara_M/0/1/0/all/0/1">Mitsunori Ogihara</a>, <a href="http://arxiv.org/find/cs/1/au:+Uchizawa_K/0/1/0/all/0/1">Kei Uchizawa</a></p><p>Dynamical Systems is a field that studies the collective behavior of objects
that update their states according to some rules. Discrete-time Boolean Finite
Dynamical System (DT-BFDS) is a subfield where the systems have some finite
number of objects whose states are Boolean values, and the state updates occur
in discrete time. In the subfield of DT-BFDS, researchers aim to (i) design
models for capturing real-world phenomena and using the models to make
predictions and (ii) develop simulation techniques for acquiring insights about
the systems' behavior. Useful for both aims is understanding the system
dynamics mathematically before executing the systems. Obtaining a mathematical
understanding of BFDS is quite challenging, even for simple systems, because
the state space of a system grows exponentially in the number of objects.
Researchers have used computational complexity to circumvent the challenge. The
complexity theoretic research in DT-BFDS has successfully produced complete
characterizations for many dynamical problems.
</p>
<p>The DT-BFDS studies have mainly dealt with deterministic models, where the
update at each time step is deterministic, so the system dynamics are
completely determinable from the initial setting. However, natural systems have
uncertainty. Models having uncertainty may lead to far-better understandings of
nature. Although a few attempts have explored DT-BFDS with uncertainty,
including stochastic initialization and tie-breaking, they have scratched only
a tiny surface of models with uncertainty. The introduction of uncertainty can
be through two schemes. One is the introduction of alternate update functions.
The other is the introduction of alternate update schedules. 37This paper
establishes a theory of models with uncertainty and proves some fundamental
results.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07798'>A Uniform Sampling Procedure for Abstract Triangulations of Surfaces</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Rajan Shankar, Jonathan Spreer</p><p>We present a procedure to sample uniformly from the set of combinatorial
isomorphism types of balanced triangulations of surfaces - also known as
graph-encoded surfaces. For a given number $n$, the sample is a weighted set of
graph-encoded surfaces with $2n$ triangles.
</p>
<p>The sampling procedure relies on connections between graph-encoded surfaces
and permutations, and basic properties of the symmetric group.
</p>
<p>We implement our method and present a number of experimental findings based
on the analysis of $138$ million runs of our sampling procedure, producing
graph-encoded surfaces with up to $280$ triangles.
</p>
<p>Namely, we determine that, for $n$ fixed, the empirical mean genus
$\bar{g}(n)$ of our sample is very close to $\bar{g}(n) = \frac{n-1}{2} -
(16.98n -110.61)^{1/4}$. Moreover, we present experimental evidence that the
associated genus distribution more and more concentrates on a vanishing portion
of all possible genera as $n$ tends to infinity. Finally, we observe from our
data that the mean number of non-trivial symmetries of a uniformly chosen graph
encoding of a surface decays to zero at a rate super-exponential in $n$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Shankar_R/0/1/0/all/0/1">Rajan Shankar</a>, <a href="http://arxiv.org/find/math/1/au:+Spreer_J/0/1/0/all/0/1">Jonathan Spreer</a></p><p>We present a procedure to sample uniformly from the set of combinatorial
isomorphism types of balanced triangulations of surfaces - also known as
graph-encoded surfaces. For a given number $n$, the sample is a weighted set of
graph-encoded surfaces with $2n$ triangles.
</p>
<p>The sampling procedure relies on connections between graph-encoded surfaces
and permutations, and basic properties of the symmetric group.
</p>
<p>We implement our method and present a number of experimental findings based
on the analysis of $138$ million runs of our sampling procedure, producing
graph-encoded surfaces with up to $280$ triangles.
</p>
<p>Namely, we determine that, for $n$ fixed, the empirical mean genus
$\bar{g}(n)$ of our sample is very close to $\bar{g}(n) = \frac{n-1}{2} -
(16.98n -110.61)^{1/4}$. Moreover, we present experimental evidence that the
associated genus distribution more and more concentrates on a vanishing portion
of all possible genera as $n$ tends to infinity. Finally, we observe from our
data that the mean number of non-trivial symmetries of a uniformly chosen graph
encoding of a surface decays to zero at a rate super-exponential in $n$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07978'>Shellability is hard even for balls</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Pavel Pat&#xe1;k, Martin Tancer</p><p>The main goal of this paper is to show that shellability is NP-hard for
triangulated d-balls (this also gives hardness for triangulated
d-manifolds/d-pseudomanifolds with boundary) as soon as d is at least 3. This
extends our earlier work with Goaoc, Pat\'akov\'a and Wagner on hardness of
shellability of 2-complexes and answers some questions implicitly raised by
Danaraj and Klee in 1978 and explicitly mentioned by Santamar\'ia-Galvis and
Woodroofe. Together with the main goal, we also prove that collapsibility is
NP-hard for 3-complexes embeddable in the 3-space, extending an earlier work of
the second author and answering an open question mentioned by Cohen, Fasy,
Miller, Nayyeri, Peng and Walkington; and that shellability is NP-hard for
2-complexes embeddable in the 3-space, answering another question of
Santamar\'ia-Galvis and Woodroofe (in a slightly stronger form than what is
given by the main result).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Patak_P/0/1/0/all/0/1">Pavel Pat&#xe1;k</a>, <a href="http://arxiv.org/find/cs/1/au:+Tancer_M/0/1/0/all/0/1">Martin Tancer</a></p><p>The main goal of this paper is to show that shellability is NP-hard for
triangulated d-balls (this also gives hardness for triangulated
d-manifolds/d-pseudomanifolds with boundary) as soon as d is at least 3. This
extends our earlier work with Goaoc, Pat\'akov\'a and Wagner on hardness of
shellability of 2-complexes and answers some questions implicitly raised by
Danaraj and Klee in 1978 and explicitly mentioned by Santamar\'ia-Galvis and
Woodroofe. Together with the main goal, we also prove that collapsibility is
NP-hard for 3-complexes embeddable in the 3-space, extending an earlier work of
the second author and answering an open question mentioned by Cohen, Fasy,
Miller, Nayyeri, Peng and Walkington; and that shellability is NP-hard for
2-complexes embeddable in the 3-space, answering another question of
Santamar\'ia-Galvis and Woodroofe (in a slightly stronger form than what is
given by the main result).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08091'>About the Reconstruction of Convex Lattice Sets from One or Two X-rays</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yan Gerard</p><p>We consider a class of problems of Discrete Tomography which has been deeply
investigated in the past: the reconstruction of convex lattice sets from their
horizontal and/or vertical X-rays, i.e. from the number of points in a sequence
of consecutive horizontal and vertical lines. The reconstruction of the
HV-convex polyominoes works usually in two steps, first the filling step
consisting in filling operations, second the convex aggregation of the
switching components. We prove three results about the convex aggregation step:
(1) The convex aggregation step used for the reconstruction of HV-convex
polyominoes does not always provide a solution. The example yielding to this
result is called \textit{the bad guy} and disproves a conjecture of the domain.
(2) The reconstruction of a digital convex lattice set from only one X-ray can
be performed in polynomial time. We prove it by encoding the convex aggregation
problem in a Directed Acyclic Graph. (3) With the same strategy, we prove that
the reconstruction of fat digital convex sets from their horizontal and
vertical X-rays can be solved in polynomial time. Fatness is a property of the
digital convex sets regarding the relative position of the left, right, top and
bottom points of the set. The complexity of the reconstruction of the lattice
sets which are not fat remains an open question.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gerard_Y/0/1/0/all/0/1">Yan Gerard</a></p><p>We consider a class of problems of Discrete Tomography which has been deeply
investigated in the past: the reconstruction of convex lattice sets from their
horizontal and/or vertical X-rays, i.e. from the number of points in a sequence
of consecutive horizontal and vertical lines. The reconstruction of the
HV-convex polyominoes works usually in two steps, first the filling step
consisting in filling operations, second the convex aggregation of the
switching components. We prove three results about the convex aggregation step:
(1) The convex aggregation step used for the reconstruction of HV-convex
polyominoes does not always provide a solution. The example yielding to this
result is called \textit{the bad guy} and disproves a conjecture of the domain.
(2) The reconstruction of a digital convex lattice set from only one X-ray can
be performed in polynomial time. We prove it by encoding the convex aggregation
problem in a Directed Acyclic Graph. (3) With the same strategy, we prove that
the reconstruction of fat digital convex sets from their horizontal and
vertical X-rays can be solved in polynomial time. Fatness is a property of the
digital convex sets regarding the relative position of the left, right, top and
bottom points of the set. The complexity of the reconstruction of the lattice
sets which are not fat remains an open question.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08184'>Improved Coresets for Euclidean $k$-Means</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Vincent Cohen-Addad, Kasper Green Larsen, David Saulpic, Chris Schwiegelshohn, Omar Ali Sheikh-Omar</p><p>Given a set of $n$ points in $d$ dimensions, the Euclidean $k$-means problem
(resp. the Euclidean $k$-median problem) consists of finding $k$ centers such
that the sum of squared distances (resp. sum of distances) from every point to
its closest center is minimized. The arguably most popular way of dealing with
this problem in the big data setting is to first compress the data by computing
a weighted subset known as a coreset and then run any algorithm on this subset.
The guarantee of the coreset is that for any candidate solution, the ratio
between coreset cost and the cost of the original instance is less than a
$(1\pm \varepsilon)$ factor. The current state of the art coreset size is
$\tilde O(\min(k^{2} \cdot \varepsilon^{-2},k\cdot \varepsilon^{-4}))$ for
Euclidean $k$-means and $\tilde O(\min(k^{2} \cdot \varepsilon^{-2},k\cdot
\varepsilon^{-3}))$ for Euclidean $k$-median. The best known lower bound for
both problems is $\Omega(k \varepsilon^{-2})$. In this paper, we improve the
upper bounds $\tilde O(\min(k^{3/2} \cdot \varepsilon^{-2},k\cdot
\varepsilon^{-4}))$ for $k$-means and $\tilde O(\min(k^{4/3} \cdot
\varepsilon^{-2},k\cdot \varepsilon^{-3}))$ for $k$-median. In particular, ours
is the first provable bound that breaks through the $k^2$ barrier while
retaining an optimal dependency on $\varepsilon$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cohen_Addad_V/0/1/0/all/0/1">Vincent Cohen-Addad</a>, <a href="http://arxiv.org/find/cs/1/au:+Larsen_K/0/1/0/all/0/1">Kasper Green Larsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Saulpic_D/0/1/0/all/0/1">David Saulpic</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwiegelshohn_C/0/1/0/all/0/1">Chris Schwiegelshohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheikh_Omar_O/0/1/0/all/0/1">Omar Ali Sheikh-Omar</a></p><p>Given a set of $n$ points in $d$ dimensions, the Euclidean $k$-means problem
(resp. the Euclidean $k$-median problem) consists of finding $k$ centers such
that the sum of squared distances (resp. sum of distances) from every point to
its closest center is minimized. The arguably most popular way of dealing with
this problem in the big data setting is to first compress the data by computing
a weighted subset known as a coreset and then run any algorithm on this subset.
The guarantee of the coreset is that for any candidate solution, the ratio
between coreset cost and the cost of the original instance is less than a
$(1\pm \varepsilon)$ factor. The current state of the art coreset size is
$\tilde O(\min(k^{2} \cdot \varepsilon^{-2},k\cdot \varepsilon^{-4}))$ for
Euclidean $k$-means and $\tilde O(\min(k^{2} \cdot \varepsilon^{-2},k\cdot
\varepsilon^{-3}))$ for Euclidean $k$-median. The best known lower bound for
both problems is $\Omega(k \varepsilon^{-2})$. In this paper, we improve the
upper bounds $\tilde O(\min(k^{3/2} \cdot \varepsilon^{-2},k\cdot
\varepsilon^{-4}))$ for $k$-means and $\tilde O(\min(k^{4/3} \cdot
\varepsilon^{-2},k\cdot \varepsilon^{-3}))$ for $k$-median. In particular, ours
is the first provable bound that breaks through the $k^2$ barrier while
retaining an optimal dependency on $\varepsilon$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08333'>Deformation Spaces and Static Animations</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Gabriel Dorfsman-Hopkins</p><p>We study applications of 3D printing to the broad goal of understanding how
mathematical objects vary continuously in families. To do so, we model the
varying parameter as the vertical axis of a 3D print, introducing the notion of
a static animation: a 3D printed object each of whose layers is a member of the
continuously deforming family. We survey examples and draw connections to
algebraic geometry, complex dynamics, chaos theory, and more. We also include a
detailed tutorial (with accompanying code and files) so that the reader can
create static animations of their own.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Dorfsman_Hopkins_G/0/1/0/all/0/1">Gabriel Dorfsman-Hopkins</a></p><p>We study applications of 3D printing to the broad goal of understanding how
mathematical objects vary continuously in families. To do so, we model the
varying parameter as the vertical axis of a 3D print, introducing the notion of
a static animation: a 3D printed object each of whose layers is a member of the
continuously deforming family. We survey examples and draw connections to
algebraic geometry, complex dynamics, chaos theory, and more. We also include a
detailed tutorial (with accompanying code and files) so that the reader can
create static animations of their own.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07644'>Bounds and Estimates on the Average Edit Distance</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Gianfranco Bilardi, Michele Schimd</p><p>The edit distance is a metric of dissimilarity between strings, widely
applied in computational biology, speech recognition, and machine learning. Let
$e_k(n)$ denote the average edit distance between random, independent strings
of $n$ characters from an alphabet of size $k$. For $k \geq 2$, it is an open
problem how to efficiently compute the exact value of $\alpha_{k}(n) =
e_k(n)/n$ as well as of $\alpha_{k} = \lim_{n \to \infty} \alpha_{k}(n)$, a
limit known to exist.
</p>
<p>This paper shows that $\alpha_k(n)-Q(n) \leq \alpha_k \leq \alpha_k(n)$, for
a specific $Q(n)=\Theta(\sqrt{\log n / n})$, a result which implies that
$\alpha_k$ is computable. The exact computation of $\alpha_k(n)$ is explored,
leading to an algorithm running in time $T=\mathcal{O}(n^2k\min(3^n,k^n))$, a
complexity that makes it of limited practical use.
</p>
<p>An analysis of statistical estimates is proposed, based on McDiarmid's
inequality, showing how $\alpha_k(n)$ can be evaluated with good accuracy, high
confidence level, and reasonable computation time, for values of $n$ say up to
a quarter million. Correspondingly, 99.9\% confidence intervals of width
approximately $10^{-2}$ are obtained for $\alpha_k$.
</p>
<p>Combinatorial arguments on edit scripts are exploited to analytically
characterize an efficiently computable lower bound $\beta_k^*$ to $\alpha_k$,
such that $ \lim_{k \to \infty} \beta_k^*=1$. In general, $\beta_k^* \leq
\alpha_k \leq 1-1/k$; for $k$ greater than a few dozens, computing $\beta_k^*$
is much faster than generating good statistical estimates with confidence
intervals of width $1-1/k-\beta_k^*$.
</p>
<p>The techniques developed in the paper yield improvements on most previously
published numerical values as well as results for alphabet sizes and string
lengths not reported before.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bilardi_G/0/1/0/all/0/1">Gianfranco Bilardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Schimd_M/0/1/0/all/0/1">Michele Schimd</a></p><p>The edit distance is a metric of dissimilarity between strings, widely
applied in computational biology, speech recognition, and machine learning. Let
$e_k(n)$ denote the average edit distance between random, independent strings
of $n$ characters from an alphabet of size $k$. For $k \geq 2$, it is an open
problem how to efficiently compute the exact value of $\alpha_{k}(n) =
e_k(n)/n$ as well as of $\alpha_{k} = \lim_{n \to \infty} \alpha_{k}(n)$, a
limit known to exist.
</p>
<p>This paper shows that $\alpha_k(n)-Q(n) \leq \alpha_k \leq \alpha_k(n)$, for
a specific $Q(n)=\Theta(\sqrt{\log n / n})$, a result which implies that
$\alpha_k$ is computable. The exact computation of $\alpha_k(n)$ is explored,
leading to an algorithm running in time $T=\mathcal{O}(n^2k\min(3^n,k^n))$, a
complexity that makes it of limited practical use.
</p>
<p>An analysis of statistical estimates is proposed, based on McDiarmid's
inequality, showing how $\alpha_k(n)$ can be evaluated with good accuracy, high
confidence level, and reasonable computation time, for values of $n$ say up to
a quarter million. Correspondingly, 99.9\% confidence intervals of width
approximately $10^{-2}$ are obtained for $\alpha_k$.
</p>
<p>Combinatorial arguments on edit scripts are exploited to analytically
characterize an efficiently computable lower bound $\beta_k^*$ to $\alpha_k$,
such that $ \lim_{k \to \infty} \beta_k^*=1$. In general, $\beta_k^* \leq
\alpha_k \leq 1-1/k$; for $k$ greater than a few dozens, computing $\beta_k^*$
is much faster than generating good statistical estimates with confidence
intervals of width $1-1/k-\beta_k^*$.
</p>
<p>The techniques developed in the paper yield improvements on most previously
published numerical values as well as results for alphabet sizes and string
lengths not reported before.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07794'>Augmented Thresholds for MONI</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: C&#xe9;sar Mart&#xed;nez-Guardiola, Nathaniel K. Brown, Fernando Silva-Coira, Dominik K&#xf6;ppl, Travis Gagie, Susana Ladra</p><p>MONI (Rossi et al., 2022) can store a pangenomic dataset T in small space and
later, given a pattern P, quickly find the maximal exact matches (MEMs) of P
with respect to T. In this paper we consider its one-pass version (Boucher et
al., 2021), whose query times are dominated in our experiments by longest
common extension (LCE) queries. We show how a small modification lets us avoid
most of these queries and thus significantly speeds up MONI in practice while
only slightly increasing its size.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Martinez_Guardiola_C/0/1/0/all/0/1">C&#xe9;sar Mart&#xed;nez-Guardiola</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1">Nathaniel K. Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_Coira_F/0/1/0/all/0/1">Fernando Silva-Coira</a>, <a href="http://arxiv.org/find/cs/1/au:+Koppl_D/0/1/0/all/0/1">Dominik K&#xf6;ppl</a>, <a href="http://arxiv.org/find/cs/1/au:+Gagie_T/0/1/0/all/0/1">Travis Gagie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ladra_S/0/1/0/all/0/1">Susana Ladra</a></p><p>MONI (Rossi et al., 2022) can store a pangenomic dataset T in small space and
later, given a pattern P, quickly find the maximal exact matches (MEMs) of P
with respect to T. In this paper we consider its one-pass version (Boucher et
al., 2021), whose query times are dominated in our experiments by longest
common extension (LCE) queries. We show how a small modification lets us avoid
most of these queries and thus significantly speeds up MONI in practice while
only slightly increasing its size.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07796'>Massively Parallel Algorithms for $b$-Matching</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mohsen Ghaffari, Christoph Grunau, Slobodan Mitrovi&#x107;</p><p>This paper presents an $O(\log\log \bar{d})$ round massively parallel
algorithm for $1+\epsilon$ approximation of maximum weighted $b$-matchings,
using near-linear memory per machine. Here $\bar{d}$ denotes the average degree
in the graph and $\epsilon$ is an arbitrarily small positive constant. Recall
that $b$-matching is the natural and well-studied generalization of the
matching problem where different vertices are allowed to have multiple (and
differing number of) incident edges in the matching. Concretely, each vertex
$v$ is given a positive integer budget $b_v$ and it can have up to $b_v$
incident edges in the matching. Previously, there were known algorithms with
round complexity $O(\log\log n)$, or $O(\log\log \Delta)$ where $\Delta$
denotes maximum degree, for $1+\epsilon$ approximation of weighted matching and
for maximal matching [Czumaj et al., STOC'18, Ghaffari et al. PODC'18; Assadi
et al. SODA'19; Behnezhad et al. FOCS'19; Gamlath et al. PODC'19], but these
algorithms do not extend to the more general $b$-matching problem.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1">Mohsen Ghaffari</a>, <a href="http://arxiv.org/find/cs/1/au:+Grunau_C/0/1/0/all/0/1">Christoph Grunau</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitrovic_S/0/1/0/all/0/1">Slobodan Mitrovi&#x107;</a></p><p>This paper presents an $O(\log\log \bar{d})$ round massively parallel
algorithm for $1+\epsilon$ approximation of maximum weighted $b$-matchings,
using near-linear memory per machine. Here $\bar{d}$ denotes the average degree
in the graph and $\epsilon$ is an arbitrarily small positive constant. Recall
that $b$-matching is the natural and well-studied generalization of the
matching problem where different vertices are allowed to have multiple (and
differing number of) incident edges in the matching. Concretely, each vertex
$v$ is given a positive integer budget $b_v$ and it can have up to $b_v$
incident edges in the matching. Previously, there were known algorithms with
round complexity $O(\log\log n)$, or $O(\log\log \Delta)$ where $\Delta$
denotes maximum degree, for $1+\epsilon$ approximation of weighted matching and
for maximal matching [Czumaj et al., STOC'18, Ghaffari et al. PODC'18; Assadi
et al. SODA'19; Behnezhad et al. FOCS'19; Gamlath et al. PODC'19], but these
algorithms do not extend to the more general $b$-matching problem.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
