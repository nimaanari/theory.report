<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.4 (2022-04-12) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2022-11-16T11:30:28Z">Wednesday, November 16 2022, 11:30</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>Sébastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Wednesday, November 16
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07691'>Low-depth arithmetic circuit lower bounds via shifted partials</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Prashanth Amireddy, Ankit Garg, Neeraj Kayal, Chandan Saha, Bhargav Thankey</p><p>We prove super-polynomial lower bounds for low-depth arithmetic circuits
using the shifted partials measure [Gupta-Kamath-Kayal-Saptharishi, CCC 2013],
[Kayal, ECCC 2012] and the affine projections of partials measure
[Garg-Kayal-Saha, FOCS 2020], [Kayal-Nair-Saha, STACS 2016]. The recent
breakthrough work of Limaye, Srinivasan and Tavenas [FOCS 2021] proved these
lower bounds by proving lower bounds for low-depth set-multilinear circuits. An
interesting aspect of our proof is that it does not require conversion of a
circuit to a set-multilinear circuit, nor does it involve a random restriction.
We are able to upper bound the measures for homogeneous formulas directly,
without going via set-multilinearity. Our lower bounds hold for the iterated
matrix multiplication as well as the Nisan-Wigderson design polynomials. We
also define a subclass of homogeneous formulas which we call unique parse tree
(UPT) formulas, and prove superpolynomial lower bounds for these. This
generalizes the superpolynomial lower bounds for regular formulas in
[Kayal-Saha-Saptharishi, STOC 2014], [Fournier-Limaye-Malod-Srinivasan, STOC
2014].
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Amireddy_P/0/1/0/all/0/1">Prashanth Amireddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1">Ankit Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Kayal_N/0/1/0/all/0/1">Neeraj Kayal</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_C/0/1/0/all/0/1">Chandan Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Thankey_B/0/1/0/all/0/1">Bhargav Thankey</a></p><p>We prove super-polynomial lower bounds for low-depth arithmetic circuits
using the shifted partials measure [Gupta-Kamath-Kayal-Saptharishi, CCC 2013],
[Kayal, ECCC 2012] and the affine projections of partials measure
[Garg-Kayal-Saha, FOCS 2020], [Kayal-Nair-Saha, STACS 2016]. The recent
breakthrough work of Limaye, Srinivasan and Tavenas [FOCS 2021] proved these
lower bounds by proving lower bounds for low-depth set-multilinear circuits. An
interesting aspect of our proof is that it does not require conversion of a
circuit to a set-multilinear circuit, nor does it involve a random restriction.
We are able to upper bound the measures for homogeneous formulas directly,
without going via set-multilinearity. Our lower bounds hold for the iterated
matrix multiplication as well as the Nisan-Wigderson design polynomials. We
also define a subclass of homogeneous formulas which we call unique parse tree
(UPT) formulas, and prove superpolynomial lower bounds for these. This
generalizes the superpolynomial lower bounds for regular formulas in
[Kayal-Saha-Saptharishi, STOC 2014], [Fournier-Limaye-Malod-Srinivasan, STOC
2014].
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07900'>Parameterized Inapproximability of the Minimum Distance Problem over all Fields and the Shortest Vector Problem in all $\ell_p$ Norms</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Huck Bennett, Mahdi Cheraghchi, Venkatesan Guruswami, Jo&#xe3;o Ribeiro</p><p>We prove that the Minimum Distance Problem (MDP) on linear codes over any
fixed finite field and parameterized by the input distance bound is W[1]-hard
to approximate within any constant factor. We also prove analogous results for
the parameterized Shortest Vector Problem (SVP) on integer lattices.
Specifically, we prove that SVP in the $\ell_p$ norm is W[1]-hard to
approximate within any constant factor for any fixed $p &gt;1$ and W[1]-hard to
approximate within a factor approaching $2$ for $p=1$. (We show hardness under
randomized reductions in each case.)
</p>
<p>These results answer the main questions left open (and explicitly posed) by
Bhattacharyya, Bonnet, Egri, Ghoshal, Karthik C. S., Lin, Manurangsi, and Marx
(Journal of the ACM, 2021) on the complexity of parameterized MDP and SVP. For
MDP, they established similar hardness for binary linear codes and left the
case of general fields open. For SVP in $\ell_p$ norms with $p &gt; 1$, they
showed inapproximability within some constant factor (depending on $p$) and
left open showing such hardness for arbitrary constant factors. They also left
open showing W[1]-hardness even of exact SVP in the $\ell_1$ norm.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bennett_H/0/1/0/all/0/1">Huck Bennett</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheraghchi_M/0/1/0/all/0/1">Mahdi Cheraghchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Guruswami_V/0/1/0/all/0/1">Venkatesan Guruswami</a>, <a href="http://arxiv.org/find/cs/1/au:+Ribeiro_J/0/1/0/all/0/1">Jo&#xe3;o Ribeiro</a></p><p>We prove that the Minimum Distance Problem (MDP) on linear codes over any
fixed finite field and parameterized by the input distance bound is W[1]-hard
to approximate within any constant factor. We also prove analogous results for
the parameterized Shortest Vector Problem (SVP) on integer lattices.
Specifically, we prove that SVP in the $\ell_p$ norm is W[1]-hard to
approximate within any constant factor for any fixed $p &gt;1$ and W[1]-hard to
approximate within a factor approaching $2$ for $p=1$. (We show hardness under
randomized reductions in each case.)
</p>
<p>These results answer the main questions left open (and explicitly posed) by
Bhattacharyya, Bonnet, Egri, Ghoshal, Karthik C. S., Lin, Manurangsi, and Marx
(Journal of the ACM, 2021) on the complexity of parameterized MDP and SVP. For
MDP, they established similar hardness for binary linear codes and left the
case of general fields open. For SVP in $\ell_p$ norms with $p &gt; 1$, they
showed inapproximability within some constant factor (depending on $p$) and
left open showing such hardness for arbitrary constant factors. They also left
open showing W[1]-hardness even of exact SVP in the $\ell_1$ norm.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07923'>A Theory for Discrete-time Boolean Finite Dynamical Systems with Uncertainty</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mitsunori Ogihara, Kei Uchizawa</p><p>Dynamical Systems is a field that studies the collective behavior of objects
that update their states according to some rules. Discrete-time Boolean Finite
Dynamical System (DT-BFDS) is a subfield where the systems have some finite
number of objects whose states are Boolean values, and the state updates occur
in discrete time. In the subfield of DT-BFDS, researchers aim to (i) design
models for capturing real-world phenomena and using the models to make
predictions and (ii) develop simulation techniques for acquiring insights about
the systems' behavior. Useful for both aims is understanding the system
dynamics mathematically before executing the systems. Obtaining a mathematical
understanding of BFDS is quite challenging, even for simple systems, because
the state space of a system grows exponentially in the number of objects.
Researchers have used computational complexity to circumvent the challenge. The
complexity theoretic research in DT-BFDS has successfully produced complete
characterizations for many dynamical problems.
</p>
<p>The DT-BFDS studies have mainly dealt with deterministic models, where the
update at each time step is deterministic, so the system dynamics are
completely determinable from the initial setting. However, natural systems have
uncertainty. Models having uncertainty may lead to far-better understandings of
nature. Although a few attempts have explored DT-BFDS with uncertainty,
including stochastic initialization and tie-breaking, they have scratched only
a tiny surface of models with uncertainty. The introduction of uncertainty can
be through two schemes. One is the introduction of alternate update functions.
The other is the introduction of alternate update schedules. 37This paper
establishes a theory of models with uncertainty and proves some fundamental
results.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ogihara_M/0/1/0/all/0/1">Mitsunori Ogihara</a>, <a href="http://arxiv.org/find/cs/1/au:+Uchizawa_K/0/1/0/all/0/1">Kei Uchizawa</a></p><p>Dynamical Systems is a field that studies the collective behavior of objects
that update their states according to some rules. Discrete-time Boolean Finite
Dynamical System (DT-BFDS) is a subfield where the systems have some finite
number of objects whose states are Boolean values, and the state updates occur
in discrete time. In the subfield of DT-BFDS, researchers aim to (i) design
models for capturing real-world phenomena and using the models to make
predictions and (ii) develop simulation techniques for acquiring insights about
the systems' behavior. Useful for both aims is understanding the system
dynamics mathematically before executing the systems. Obtaining a mathematical
understanding of BFDS is quite challenging, even for simple systems, because
the state space of a system grows exponentially in the number of objects.
Researchers have used computational complexity to circumvent the challenge. The
complexity theoretic research in DT-BFDS has successfully produced complete
characterizations for many dynamical problems.
</p>
<p>The DT-BFDS studies have mainly dealt with deterministic models, where the
update at each time step is deterministic, so the system dynamics are
completely determinable from the initial setting. However, natural systems have
uncertainty. Models having uncertainty may lead to far-better understandings of
nature. Although a few attempts have explored DT-BFDS with uncertainty,
including stochastic initialization and tie-breaking, they have scratched only
a tiny surface of models with uncertainty. The introduction of uncertainty can
be through two schemes. One is the introduction of alternate update functions.
The other is the introduction of alternate update schedules. 37This paper
establishes a theory of models with uncertainty and proves some fundamental
results.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07798'>A Uniform Sampling Procedure for Abstract Triangulations of Surfaces</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Rajan Shankar, Jonathan Spreer</p><p>We present a procedure to sample uniformly from the set of combinatorial
isomorphism types of balanced triangulations of surfaces - also known as
graph-encoded surfaces. For a given number $n$, the sample is a weighted set of
graph-encoded surfaces with $2n$ triangles.
</p>
<p>The sampling procedure relies on connections between graph-encoded surfaces
and permutations, and basic properties of the symmetric group.
</p>
<p>We implement our method and present a number of experimental findings based
on the analysis of $138$ million runs of our sampling procedure, producing
graph-encoded surfaces with up to $280$ triangles.
</p>
<p>Namely, we determine that, for $n$ fixed, the empirical mean genus
$\bar{g}(n)$ of our sample is very close to $\bar{g}(n) = \frac{n-1}{2} -
(16.98n -110.61)^{1/4}$. Moreover, we present experimental evidence that the
associated genus distribution more and more concentrates on a vanishing portion
of all possible genera as $n$ tends to infinity. Finally, we observe from our
data that the mean number of non-trivial symmetries of a uniformly chosen graph
encoding of a surface decays to zero at a rate super-exponential in $n$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Shankar_R/0/1/0/all/0/1">Rajan Shankar</a>, <a href="http://arxiv.org/find/math/1/au:+Spreer_J/0/1/0/all/0/1">Jonathan Spreer</a></p><p>We present a procedure to sample uniformly from the set of combinatorial
isomorphism types of balanced triangulations of surfaces - also known as
graph-encoded surfaces. For a given number $n$, the sample is a weighted set of
graph-encoded surfaces with $2n$ triangles.
</p>
<p>The sampling procedure relies on connections between graph-encoded surfaces
and permutations, and basic properties of the symmetric group.
</p>
<p>We implement our method and present a number of experimental findings based
on the analysis of $138$ million runs of our sampling procedure, producing
graph-encoded surfaces with up to $280$ triangles.
</p>
<p>Namely, we determine that, for $n$ fixed, the empirical mean genus
$\bar{g}(n)$ of our sample is very close to $\bar{g}(n) = \frac{n-1}{2} -
(16.98n -110.61)^{1/4}$. Moreover, we present experimental evidence that the
associated genus distribution more and more concentrates on a vanishing portion
of all possible genera as $n$ tends to infinity. Finally, we observe from our
data that the mean number of non-trivial symmetries of a uniformly chosen graph
encoding of a surface decays to zero at a rate super-exponential in $n$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07978'>Shellability is hard even for balls</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Pavel Pat&#xe1;k, Martin Tancer</p><p>The main goal of this paper is to show that shellability is NP-hard for
triangulated d-balls (this also gives hardness for triangulated
d-manifolds/d-pseudomanifolds with boundary) as soon as d is at least 3. This
extends our earlier work with Goaoc, Pat\'akov\'a and Wagner on hardness of
shellability of 2-complexes and answers some questions implicitly raised by
Danaraj and Klee in 1978 and explicitly mentioned by Santamar\'ia-Galvis and
Woodroofe. Together with the main goal, we also prove that collapsibility is
NP-hard for 3-complexes embeddable in the 3-space, extending an earlier work of
the second author and answering an open question mentioned by Cohen, Fasy,
Miller, Nayyeri, Peng and Walkington; and that shellability is NP-hard for
2-complexes embeddable in the 3-space, answering another question of
Santamar\'ia-Galvis and Woodroofe (in a slightly stronger form than what is
given by the main result).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Patak_P/0/1/0/all/0/1">Pavel Pat&#xe1;k</a>, <a href="http://arxiv.org/find/cs/1/au:+Tancer_M/0/1/0/all/0/1">Martin Tancer</a></p><p>The main goal of this paper is to show that shellability is NP-hard for
triangulated d-balls (this also gives hardness for triangulated
d-manifolds/d-pseudomanifolds with boundary) as soon as d is at least 3. This
extends our earlier work with Goaoc, Pat\'akov\'a and Wagner on hardness of
shellability of 2-complexes and answers some questions implicitly raised by
Danaraj and Klee in 1978 and explicitly mentioned by Santamar\'ia-Galvis and
Woodroofe. Together with the main goal, we also prove that collapsibility is
NP-hard for 3-complexes embeddable in the 3-space, extending an earlier work of
the second author and answering an open question mentioned by Cohen, Fasy,
Miller, Nayyeri, Peng and Walkington; and that shellability is NP-hard for
2-complexes embeddable in the 3-space, answering another question of
Santamar\'ia-Galvis and Woodroofe (in a slightly stronger form than what is
given by the main result).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08091'>About the Reconstruction of Convex Lattice Sets from One or Two X-rays</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yan Gerard</p><p>We consider a class of problems of Discrete Tomography which has been deeply
investigated in the past: the reconstruction of convex lattice sets from their
horizontal and/or vertical X-rays, i.e. from the number of points in a sequence
of consecutive horizontal and vertical lines. The reconstruction of the
HV-convex polyominoes works usually in two steps, first the filling step
consisting in filling operations, second the convex aggregation of the
switching components. We prove three results about the convex aggregation step:
(1) The convex aggregation step used for the reconstruction of HV-convex
polyominoes does not always provide a solution. The example yielding to this
result is called \textit{the bad guy} and disproves a conjecture of the domain.
(2) The reconstruction of a digital convex lattice set from only one X-ray can
be performed in polynomial time. We prove it by encoding the convex aggregation
problem in a Directed Acyclic Graph. (3) With the same strategy, we prove that
the reconstruction of fat digital convex sets from their horizontal and
vertical X-rays can be solved in polynomial time. Fatness is a property of the
digital convex sets regarding the relative position of the left, right, top and
bottom points of the set. The complexity of the reconstruction of the lattice
sets which are not fat remains an open question.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gerard_Y/0/1/0/all/0/1">Yan Gerard</a></p><p>We consider a class of problems of Discrete Tomography which has been deeply
investigated in the past: the reconstruction of convex lattice sets from their
horizontal and/or vertical X-rays, i.e. from the number of points in a sequence
of consecutive horizontal and vertical lines. The reconstruction of the
HV-convex polyominoes works usually in two steps, first the filling step
consisting in filling operations, second the convex aggregation of the
switching components. We prove three results about the convex aggregation step:
(1) The convex aggregation step used for the reconstruction of HV-convex
polyominoes does not always provide a solution. The example yielding to this
result is called \textit{the bad guy} and disproves a conjecture of the domain.
(2) The reconstruction of a digital convex lattice set from only one X-ray can
be performed in polynomial time. We prove it by encoding the convex aggregation
problem in a Directed Acyclic Graph. (3) With the same strategy, we prove that
the reconstruction of fat digital convex sets from their horizontal and
vertical X-rays can be solved in polynomial time. Fatness is a property of the
digital convex sets regarding the relative position of the left, right, top and
bottom points of the set. The complexity of the reconstruction of the lattice
sets which are not fat remains an open question.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08184'>Improved Coresets for Euclidean $k$-Means</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Vincent Cohen-Addad, Kasper Green Larsen, David Saulpic, Chris Schwiegelshohn, Omar Ali Sheikh-Omar</p><p>Given a set of $n$ points in $d$ dimensions, the Euclidean $k$-means problem
(resp. the Euclidean $k$-median problem) consists of finding $k$ centers such
that the sum of squared distances (resp. sum of distances) from every point to
its closest center is minimized. The arguably most popular way of dealing with
this problem in the big data setting is to first compress the data by computing
a weighted subset known as a coreset and then run any algorithm on this subset.
The guarantee of the coreset is that for any candidate solution, the ratio
between coreset cost and the cost of the original instance is less than a
$(1\pm \varepsilon)$ factor. The current state of the art coreset size is
$\tilde O(\min(k^{2} \cdot \varepsilon^{-2},k\cdot \varepsilon^{-4}))$ for
Euclidean $k$-means and $\tilde O(\min(k^{2} \cdot \varepsilon^{-2},k\cdot
\varepsilon^{-3}))$ for Euclidean $k$-median. The best known lower bound for
both problems is $\Omega(k \varepsilon^{-2})$. In this paper, we improve the
upper bounds $\tilde O(\min(k^{3/2} \cdot \varepsilon^{-2},k\cdot
\varepsilon^{-4}))$ for $k$-means and $\tilde O(\min(k^{4/3} \cdot
\varepsilon^{-2},k\cdot \varepsilon^{-3}))$ for $k$-median. In particular, ours
is the first provable bound that breaks through the $k^2$ barrier while
retaining an optimal dependency on $\varepsilon$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cohen_Addad_V/0/1/0/all/0/1">Vincent Cohen-Addad</a>, <a href="http://arxiv.org/find/cs/1/au:+Larsen_K/0/1/0/all/0/1">Kasper Green Larsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Saulpic_D/0/1/0/all/0/1">David Saulpic</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwiegelshohn_C/0/1/0/all/0/1">Chris Schwiegelshohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheikh_Omar_O/0/1/0/all/0/1">Omar Ali Sheikh-Omar</a></p><p>Given a set of $n$ points in $d$ dimensions, the Euclidean $k$-means problem
(resp. the Euclidean $k$-median problem) consists of finding $k$ centers such
that the sum of squared distances (resp. sum of distances) from every point to
its closest center is minimized. The arguably most popular way of dealing with
this problem in the big data setting is to first compress the data by computing
a weighted subset known as a coreset and then run any algorithm on this subset.
The guarantee of the coreset is that for any candidate solution, the ratio
between coreset cost and the cost of the original instance is less than a
$(1\pm \varepsilon)$ factor. The current state of the art coreset size is
$\tilde O(\min(k^{2} \cdot \varepsilon^{-2},k\cdot \varepsilon^{-4}))$ for
Euclidean $k$-means and $\tilde O(\min(k^{2} \cdot \varepsilon^{-2},k\cdot
\varepsilon^{-3}))$ for Euclidean $k$-median. The best known lower bound for
both problems is $\Omega(k \varepsilon^{-2})$. In this paper, we improve the
upper bounds $\tilde O(\min(k^{3/2} \cdot \varepsilon^{-2},k\cdot
\varepsilon^{-4}))$ for $k$-means and $\tilde O(\min(k^{4/3} \cdot
\varepsilon^{-2},k\cdot \varepsilon^{-3}))$ for $k$-median. In particular, ours
is the first provable bound that breaks through the $k^2$ barrier while
retaining an optimal dependency on $\varepsilon$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08333'>Deformation Spaces and Static Animations</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Gabriel Dorfsman-Hopkins</p><p>We study applications of 3D printing to the broad goal of understanding how
mathematical objects vary continuously in families. To do so, we model the
varying parameter as the vertical axis of a 3D print, introducing the notion of
a static animation: a 3D printed object each of whose layers is a member of the
continuously deforming family. We survey examples and draw connections to
algebraic geometry, complex dynamics, chaos theory, and more. We also include a
detailed tutorial (with accompanying code and files) so that the reader can
create static animations of their own.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Dorfsman_Hopkins_G/0/1/0/all/0/1">Gabriel Dorfsman-Hopkins</a></p><p>We study applications of 3D printing to the broad goal of understanding how
mathematical objects vary continuously in families. To do so, we model the
varying parameter as the vertical axis of a 3D print, introducing the notion of
a static animation: a 3D printed object each of whose layers is a member of the
continuously deforming family. We survey examples and draw connections to
algebraic geometry, complex dynamics, chaos theory, and more. We also include a
detailed tutorial (with accompanying code and files) so that the reader can
create static animations of their own.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07644'>Bounds and Estimates on the Average Edit Distance</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Gianfranco Bilardi, Michele Schimd</p><p>The edit distance is a metric of dissimilarity between strings, widely
applied in computational biology, speech recognition, and machine learning. Let
$e_k(n)$ denote the average edit distance between random, independent strings
of $n$ characters from an alphabet of size $k$. For $k \geq 2$, it is an open
problem how to efficiently compute the exact value of $\alpha_{k}(n) =
e_k(n)/n$ as well as of $\alpha_{k} = \lim_{n \to \infty} \alpha_{k}(n)$, a
limit known to exist.
</p>
<p>This paper shows that $\alpha_k(n)-Q(n) \leq \alpha_k \leq \alpha_k(n)$, for
a specific $Q(n)=\Theta(\sqrt{\log n / n})$, a result which implies that
$\alpha_k$ is computable. The exact computation of $\alpha_k(n)$ is explored,
leading to an algorithm running in time $T=\mathcal{O}(n^2k\min(3^n,k^n))$, a
complexity that makes it of limited practical use.
</p>
<p>An analysis of statistical estimates is proposed, based on McDiarmid's
inequality, showing how $\alpha_k(n)$ can be evaluated with good accuracy, high
confidence level, and reasonable computation time, for values of $n$ say up to
a quarter million. Correspondingly, 99.9\% confidence intervals of width
approximately $10^{-2}$ are obtained for $\alpha_k$.
</p>
<p>Combinatorial arguments on edit scripts are exploited to analytically
characterize an efficiently computable lower bound $\beta_k^*$ to $\alpha_k$,
such that $ \lim_{k \to \infty} \beta_k^*=1$. In general, $\beta_k^* \leq
\alpha_k \leq 1-1/k$; for $k$ greater than a few dozens, computing $\beta_k^*$
is much faster than generating good statistical estimates with confidence
intervals of width $1-1/k-\beta_k^*$.
</p>
<p>The techniques developed in the paper yield improvements on most previously
published numerical values as well as results for alphabet sizes and string
lengths not reported before.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bilardi_G/0/1/0/all/0/1">Gianfranco Bilardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Schimd_M/0/1/0/all/0/1">Michele Schimd</a></p><p>The edit distance is a metric of dissimilarity between strings, widely
applied in computational biology, speech recognition, and machine learning. Let
$e_k(n)$ denote the average edit distance between random, independent strings
of $n$ characters from an alphabet of size $k$. For $k \geq 2$, it is an open
problem how to efficiently compute the exact value of $\alpha_{k}(n) =
e_k(n)/n$ as well as of $\alpha_{k} = \lim_{n \to \infty} \alpha_{k}(n)$, a
limit known to exist.
</p>
<p>This paper shows that $\alpha_k(n)-Q(n) \leq \alpha_k \leq \alpha_k(n)$, for
a specific $Q(n)=\Theta(\sqrt{\log n / n})$, a result which implies that
$\alpha_k$ is computable. The exact computation of $\alpha_k(n)$ is explored,
leading to an algorithm running in time $T=\mathcal{O}(n^2k\min(3^n,k^n))$, a
complexity that makes it of limited practical use.
</p>
<p>An analysis of statistical estimates is proposed, based on McDiarmid's
inequality, showing how $\alpha_k(n)$ can be evaluated with good accuracy, high
confidence level, and reasonable computation time, for values of $n$ say up to
a quarter million. Correspondingly, 99.9\% confidence intervals of width
approximately $10^{-2}$ are obtained for $\alpha_k$.
</p>
<p>Combinatorial arguments on edit scripts are exploited to analytically
characterize an efficiently computable lower bound $\beta_k^*$ to $\alpha_k$,
such that $ \lim_{k \to \infty} \beta_k^*=1$. In general, $\beta_k^* \leq
\alpha_k \leq 1-1/k$; for $k$ greater than a few dozens, computing $\beta_k^*$
is much faster than generating good statistical estimates with confidence
intervals of width $1-1/k-\beta_k^*$.
</p>
<p>The techniques developed in the paper yield improvements on most previously
published numerical values as well as results for alphabet sizes and string
lengths not reported before.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07794'>Augmented Thresholds for MONI</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: C&#xe9;sar Mart&#xed;nez-Guardiola, Nathaniel K. Brown, Fernando Silva-Coira, Dominik K&#xf6;ppl, Travis Gagie, Susana Ladra</p><p>MONI (Rossi et al., 2022) can store a pangenomic dataset T in small space and
later, given a pattern P, quickly find the maximal exact matches (MEMs) of P
with respect to T. In this paper we consider its one-pass version (Boucher et
al., 2021), whose query times are dominated in our experiments by longest
common extension (LCE) queries. We show how a small modification lets us avoid
most of these queries and thus significantly speeds up MONI in practice while
only slightly increasing its size.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Martinez_Guardiola_C/0/1/0/all/0/1">C&#xe9;sar Mart&#xed;nez-Guardiola</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1">Nathaniel K. Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_Coira_F/0/1/0/all/0/1">Fernando Silva-Coira</a>, <a href="http://arxiv.org/find/cs/1/au:+Koppl_D/0/1/0/all/0/1">Dominik K&#xf6;ppl</a>, <a href="http://arxiv.org/find/cs/1/au:+Gagie_T/0/1/0/all/0/1">Travis Gagie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ladra_S/0/1/0/all/0/1">Susana Ladra</a></p><p>MONI (Rossi et al., 2022) can store a pangenomic dataset T in small space and
later, given a pattern P, quickly find the maximal exact matches (MEMs) of P
with respect to T. In this paper we consider its one-pass version (Boucher et
al., 2021), whose query times are dominated in our experiments by longest
common extension (LCE) queries. We show how a small modification lets us avoid
most of these queries and thus significantly speeds up MONI in practice while
only slightly increasing its size.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07796'>Massively Parallel Algorithms for $b$-Matching</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mohsen Ghaffari, Christoph Grunau, Slobodan Mitrovi&#x107;</p><p>This paper presents an $O(\log\log \bar{d})$ round massively parallel
algorithm for $1+\epsilon$ approximation of maximum weighted $b$-matchings,
using near-linear memory per machine. Here $\bar{d}$ denotes the average degree
in the graph and $\epsilon$ is an arbitrarily small positive constant. Recall
that $b$-matching is the natural and well-studied generalization of the
matching problem where different vertices are allowed to have multiple (and
differing number of) incident edges in the matching. Concretely, each vertex
$v$ is given a positive integer budget $b_v$ and it can have up to $b_v$
incident edges in the matching. Previously, there were known algorithms with
round complexity $O(\log\log n)$, or $O(\log\log \Delta)$ where $\Delta$
denotes maximum degree, for $1+\epsilon$ approximation of weighted matching and
for maximal matching [Czumaj et al., STOC'18, Ghaffari et al. PODC'18; Assadi
et al. SODA'19; Behnezhad et al. FOCS'19; Gamlath et al. PODC'19], but these
algorithms do not extend to the more general $b$-matching problem.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1">Mohsen Ghaffari</a>, <a href="http://arxiv.org/find/cs/1/au:+Grunau_C/0/1/0/all/0/1">Christoph Grunau</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitrovic_S/0/1/0/all/0/1">Slobodan Mitrovi&#x107;</a></p><p>This paper presents an $O(\log\log \bar{d})$ round massively parallel
algorithm for $1+\epsilon$ approximation of maximum weighted $b$-matchings,
using near-linear memory per machine. Here $\bar{d}$ denotes the average degree
in the graph and $\epsilon$ is an arbitrarily small positive constant. Recall
that $b$-matching is the natural and well-studied generalization of the
matching problem where different vertices are allowed to have multiple (and
differing number of) incident edges in the matching. Concretely, each vertex
$v$ is given a positive integer budget $b_v$ and it can have up to $b_v$
incident edges in the matching. Previously, there were known algorithms with
round complexity $O(\log\log n)$, or $O(\log\log \Delta)$ where $\Delta$
denotes maximum degree, for $1+\epsilon$ approximation of weighted matching and
for maximal matching [Czumaj et al., STOC'18, Ghaffari et al. PODC'18; Assadi
et al. SODA'19; Behnezhad et al. FOCS'19; Gamlath et al. PODC'19], but these
algorithms do not extend to the more general $b$-matching problem.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07829'>On Sparsification of Stochastic Packing Problems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Shaddin Dughmi, Yusuf Hakan Kalayci, Neel Patel</p><p>Motivated by recent progress on stochastic matching with few queries, we
embark on a systematic study of the sparsification of stochastic packing
problems (SPP) more generally. Specifically, we consider SPPs where elements
are independently active with a probability p, and ask whether one can
(non-adaptively) compute a sparse set of elements guaranteed to contain an
approximately optimal solution to the realized (active) subproblem. We seek
structural and algorithmic results of broad applicability to such problems. Our
focus is on computing sparse sets containing on the order of d feasible
solutions to the packing problem, where d is linear or at most poly. in 1/p.
Crucially, we require d to be independent of the any parameter related to the
``size'' of the packing problem. We refer to d as the degree of the sparsifier,
as is consistent with graph theoretic degree in the special case of matching.
First, we exhibit a generic sparsifier of degree 1/p based on contention
resolution. This sparsifier's approximation ratio matches the best contention
resolution scheme (CRS) for any packing problem for additive objectives, and
approximately matches the best monotone CRS for submodular objectives. Second,
we embark on outperforming this generic sparsifier for matroids, their
intersections and weighted matching. These improved sparsifiers feature
different algorithmic and analytic approaches, and have degree linear in 1/p.
In the case of a single matroid, our sparsifier tends to the optimal solution.
For weighted matching, we combine our contention-resolution-based sparsifier
with technical approaches of prior work to improve the state of the art ratio
from 0.501 to 0.536. Third, we examine packing problems with submodular
objectives. We show that even the simplest such problems do not admit
sparsifiers approaching optimality.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dughmi_S/0/1/0/all/0/1">Shaddin Dughmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalayci_Y/0/1/0/all/0/1">Yusuf Hakan Kalayci</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1">Neel Patel</a></p><p>Motivated by recent progress on stochastic matching with few queries, we
embark on a systematic study of the sparsification of stochastic packing
problems (SPP) more generally. Specifically, we consider SPPs where elements
are independently active with a probability p, and ask whether one can
(non-adaptively) compute a sparse set of elements guaranteed to contain an
approximately optimal solution to the realized (active) subproblem. We seek
structural and algorithmic results of broad applicability to such problems. Our
focus is on computing sparse sets containing on the order of d feasible
solutions to the packing problem, where d is linear or at most poly. in 1/p.
Crucially, we require d to be independent of the any parameter related to the
``size'' of the packing problem. We refer to d as the degree of the sparsifier,
as is consistent with graph theoretic degree in the special case of matching.
First, we exhibit a generic sparsifier of degree 1/p based on contention
resolution. This sparsifier's approximation ratio matches the best contention
resolution scheme (CRS) for any packing problem for additive objectives, and
approximately matches the best monotone CRS for submodular objectives. Second,
we embark on outperforming this generic sparsifier for matroids, their
intersections and weighted matching. These improved sparsifiers feature
different algorithmic and analytic approaches, and have degree linear in 1/p.
In the case of a single matroid, our sparsifier tends to the optimal solution.
For weighted matching, we combine our contention-resolution-based sparsifier
with technical approaches of prior work to improve the state of the art ratio
from 0.501 to 0.536. Third, we examine packing problems with submodular
objectives. We show that even the simplest such problems do not admit
sparsifiers approaching optimality.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08157'>Taming Large-Scale Genomic Analyses via Sparsified Genomics</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mohammed Alser, Julien Eudine, Onur Mutlu</p><p>Searching for similar genomic sequences is an essential and fundamental step
in biomedical research and an overwhelming majority of genomic analyses.
State-of-the-art computational methods performing such comparisons fail to cope
with the exponential growth of genomic sequencing data. We introduce the
concept of sparsified genomics where we systematically exclude a large number
of bases from genomic sequences and enable much faster and more
memory-efficient processing of the sparsified, shorter genomic sequences, while
providing similar or even higher accuracy compared to processing non-sparsified
sequences. Sparsified genomics provides significant benefits to many genomic
analyses and has broad applicability. We show that sparsifying genomic
sequences greatly accelerates the state-of-the-art read mapper (minimap2) by
1.54-8.8x using real Illumina, HiFi, and ONT reads, while providing a higher
number of mapped reads and more detected small and structural variations.
Sparsifying genomic sequences makes containment search through very large
genomes and very large databases 72.7-75.88x faster and 723.3x more
storage-efficient than searching through non-sparsified genomic sequences (with
CMash and KMC3). Sparsifying genomic sequences enables robust microbiome
discovery by providing 54.15-61.88x faster and 720x more storage-efficient
taxonomic profiling of metagenomic samples over the state-of-art tool
(Metalign). We design and open-source a framework called Genome-on-Diet as an
example tool for sparsified genomics, which can be freely downloaded from
github.com/CMU-SAFARI/Genome-on-Diet.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Alser_M/0/1/0/all/0/1">Mohammed Alser</a>, <a href="http://arxiv.org/find/cs/1/au:+Eudine_J/0/1/0/all/0/1">Julien Eudine</a>, <a href="http://arxiv.org/find/cs/1/au:+Mutlu_O/0/1/0/all/0/1">Onur Mutlu</a></p><p>Searching for similar genomic sequences is an essential and fundamental step
in biomedical research and an overwhelming majority of genomic analyses.
State-of-the-art computational methods performing such comparisons fail to cope
with the exponential growth of genomic sequencing data. We introduce the
concept of sparsified genomics where we systematically exclude a large number
of bases from genomic sequences and enable much faster and more
memory-efficient processing of the sparsified, shorter genomic sequences, while
providing similar or even higher accuracy compared to processing non-sparsified
sequences. Sparsified genomics provides significant benefits to many genomic
analyses and has broad applicability. We show that sparsifying genomic
sequences greatly accelerates the state-of-the-art read mapper (minimap2) by
1.54-8.8x using real Illumina, HiFi, and ONT reads, while providing a higher
number of mapped reads and more detected small and structural variations.
Sparsifying genomic sequences makes containment search through very large
genomes and very large databases 72.7-75.88x faster and 723.3x more
storage-efficient than searching through non-sparsified genomic sequences (with
CMash and KMC3). Sparsifying genomic sequences enables robust microbiome
discovery by providing 54.15-61.88x faster and 720x more storage-efficient
taxonomic profiling of metagenomic samples over the state-of-art tool
(Metalign). We design and open-source a framework called Genome-on-Diet as an
example tool for sparsified genomics, which can be freely downloaded from
https://github.com/CMU-SAFARI/Genome-on-Diet.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08283'>The RED-BLUE SEPARATION problem on graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Subhadeep Ranjan Dev, Sanjana Dey, Florent Foucaud, Ralf Klasing, Tuomo Lehtil&#xe4;</p><p>We introduce the Red-Blue Separation problem on graphs, where we are given a
graph $G=(V,E)$ whose vertices are colored either red or blue, and we want to
select a (small) subset $S \subseteq V$, called red-blue separating set, such
that for every red-blue pair of vertices, there is a vertex $s \in S$ whose
closed neighborhood contains exactly one of the two vertices of the pair. We
study the computational complexity of Red-Blue Separation, in which one asks
whether a given red-blue colored graph has a red-blue separating set of size at
most a given integer. We prove that the problem is NP-complete even for
restricted graph classes. We also show that it is always approximable in
polynomial time within a factor of $2\ln n$, where $n$ is the input graph's
order. In contrast, for triangle-free graphs and for graphs of bounded maximum
degree, we show that Red-Blue Separation is solvable in polynomial time when
the size of the smaller color class is bounded by a constant. However, on
general graphs, we show that the problem is $W[2]$-hard even when parameterized
by the solution size plus the size of the smaller color class. We also consider
the problem Max Red-Blue Separation where the coloring is not part of the
input. Here, given an input graph $G$, we want to determine the smallest
integer $k$ such that, for every possible red-blue coloring of $G$, there is a
red-blue separating set of size at most $k$. We derive tight bounds on the
cardinality of an optimal solution of Max Red-Blue Separation, showing that it
can range from logarithmic in the graph order, up to the order minus one. We
also give bounds with respect to related parameters. For trees however we prove
an upper bound of two-thirds the order. We then show that Max Red-Blue
Separation is NP-hard, even for graphs of bounded maximum degree, but can be
approximated in polynomial time within a factor of $O(\ln^2 n)$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1">Subhadeep Ranjan Dev</a>, <a href="http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1">Sanjana Dey</a>, <a href="http://arxiv.org/find/cs/1/au:+Foucaud_F/0/1/0/all/0/1">Florent Foucaud</a>, <a href="http://arxiv.org/find/cs/1/au:+Klasing_R/0/1/0/all/0/1">Ralf Klasing</a>, <a href="http://arxiv.org/find/cs/1/au:+Lehtila_T/0/1/0/all/0/1">Tuomo Lehtil&#xe4;</a></p><p>We introduce the Red-Blue Separation problem on graphs, where we are given a
graph $G=(V,E)$ whose vertices are colored either red or blue, and we want to
select a (small) subset $S \subseteq V$, called red-blue separating set, such
that for every red-blue pair of vertices, there is a vertex $s \in S$ whose
closed neighborhood contains exactly one of the two vertices of the pair. We
study the computational complexity of Red-Blue Separation, in which one asks
whether a given red-blue colored graph has a red-blue separating set of size at
most a given integer. We prove that the problem is NP-complete even for
restricted graph classes. We also show that it is always approximable in
polynomial time within a factor of $2\ln n$, where $n$ is the input graph's
order. In contrast, for triangle-free graphs and for graphs of bounded maximum
degree, we show that Red-Blue Separation is solvable in polynomial time when
the size of the smaller color class is bounded by a constant. However, on
general graphs, we show that the problem is $W[2]$-hard even when parameterized
by the solution size plus the size of the smaller color class. We also consider
the problem Max Red-Blue Separation where the coloring is not part of the
input. Here, given an input graph $G$, we want to determine the smallest
integer $k$ such that, for every possible red-blue coloring of $G$, there is a
red-blue separating set of size at most $k$. We derive tight bounds on the
cardinality of an optimal solution of Max Red-Blue Separation, showing that it
can range from logarithmic in the graph order, up to the order minus one. We
also give bounds with respect to related parameters. For trees however we prove
an upper bound of two-thirds the order. We then show that Max Red-Blue
Separation is NP-hard, even for graphs of bounded maximum degree, but can be
approximated in polynomial time within a factor of $O(\ln^2 n)$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08324'>Approximating Flexible Graph Connectivity via R\"acke Tree based Rounding</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Chandra Chekuri, Rhea Jain</p><p>Flexible graph connectivity is a new network design model introduced by
Adjiashvili. It has seen several recent algorithmic advances. Despite these,
the approximability even in the setting of a single-pair $(s,t)$ is poorly
understood. In our recent work, we raised the question of whether there is
poly-logarithmic approximation for the survivable network design version
(Flex-SNDP) when the connectivity requirements are fixed constants. In this
paper, we adapt a powerful framework for survivable network design recently
developed by Chen, Laekhanukit, Liao, and Zhang to give an affirmative answer
to the question. The framework of is based on R\"acke trees and group Steiner
tree rounding. The algorithm and analysis also establishes an upper bound on
the integrality gap of an LP relaxation for Flex-SNDP.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chekuri_C/0/1/0/all/0/1">Chandra Chekuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1">Rhea Jain</a></p><p>Flexible graph connectivity is a new network design model introduced by
Adjiashvili. It has seen several recent algorithmic advances. Despite these,
the approximability even in the setting of a single-pair $(s,t)$ is poorly
understood. In our recent work, we raised the question of whether there is
poly-logarithmic approximation for the survivable network design version
(Flex-SNDP) when the connectivity requirements are fixed constants. In this
paper, we adapt a powerful framework for survivable network design recently
developed by Chen, Laekhanukit, Liao, and Zhang to give an affirmative answer
to the question. The framework of is based on R\"acke trees and group Steiner
tree rounding. The algorithm and analysis also establishes an upper bound on
the integrality gap of an LP relaxation for Flex-SNDP.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08373'>SDPs and Robust Satisfiability of Promise CSP</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Joshua Brakensiek, Venkatesan Guruswami, Sai Sandeep</p><p>For a constraint satisfaction problem (CSP), a robust satisfaction algorithm
is one that outputs an assignment satisfying most of the constraints on
instances that are near-satisfiable. It is known that the CSPs that admit
efficient robust satisfaction algorithms are precisely those of bounded width,
i.e., CSPs whose satisfiability can be checked by a simple local consistency
algorithm (eg., 2-SAT or Horn-SAT in the Boolean case). While the exact
satisfiability of a bounded width CSP can be checked by combinatorial
algorithms, the robust algorithm is based on rounding a canonical Semidefinite
programming(SDP) relaxation.
</p>
<p>In this work, we initiate the study of robust satisfaction algorithms for
promise CSPs, which are a vast generalization of CSPs that have received much
attention recently. The motivation is to extend the theory beyond CSPs, as well
as to better understand the power of SDPs. We present robust SDP rounding
algorithms under some general conditions, namely the existence of majority or
alternating threshold polymorphisms. On the hardness front, we prove that the
lack of such polymorphisms makes the PCSP hard for all pairs of symmetric
Boolean predicates. Our method involves a novel method to argue SDP gaps via
the absence of certain colorings of the sphere, with connections to sphere
Ramsey theory.
</p>
<p>We conjecture that PCSPs with robust satisfaction algorithms are precisely
those for which the feasibility of the canonical SDP implies (exact)
satisfiability. We also give a precise algebraic condition, known as a minion
characterization, of which PCSPs have the latter property.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Brakensiek_J/0/1/0/all/0/1">Joshua Brakensiek</a>, <a href="http://arxiv.org/find/cs/1/au:+Guruswami_V/0/1/0/all/0/1">Venkatesan Guruswami</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandeep_S/0/1/0/all/0/1">Sai Sandeep</a></p><p>For a constraint satisfaction problem (CSP), a robust satisfaction algorithm
is one that outputs an assignment satisfying most of the constraints on
instances that are near-satisfiable. It is known that the CSPs that admit
efficient robust satisfaction algorithms are precisely those of bounded width,
i.e., CSPs whose satisfiability can be checked by a simple local consistency
algorithm (eg., 2-SAT or Horn-SAT in the Boolean case). While the exact
satisfiability of a bounded width CSP can be checked by combinatorial
algorithms, the robust algorithm is based on rounding a canonical Semidefinite
programming(SDP) relaxation.
</p>
<p>In this work, we initiate the study of robust satisfaction algorithms for
promise CSPs, which are a vast generalization of CSPs that have received much
attention recently. The motivation is to extend the theory beyond CSPs, as well
as to better understand the power of SDPs. We present robust SDP rounding
algorithms under some general conditions, namely the existence of majority or
alternating threshold polymorphisms. On the hardness front, we prove that the
lack of such polymorphisms makes the PCSP hard for all pairs of symmetric
Boolean predicates. Our method involves a novel method to argue SDP gaps via
the absence of certain colorings of the sphere, with connections to sphere
Ramsey theory.
</p>
<p>We conjecture that PCSPs with robust satisfaction algorithms are precisely
those for which the feasibility of the canonical SDP implies (exact)
satisfiability. We also give a precise algebraic condition, known as a minion
characterization, of which PCSPs have the latter property.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08381'>Optimizing Polymatroid Functions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sungjin Im, Benjamin Moseley, Hung Q. Ngo, Kirk Pruhs, Alireza Samadian</p><p>We consider a class of optimization problems that involve determining the
maximum value that a function in a particular class can attain subject to a
collection of difference constraints. We show that a particular linear
programming technique, based on duality and projections, can be used to
rederive some structural results that were previously established using more ad
hoc methods. We then show that this technique can be used to obtain a
polynomial-time algorithm for a certain type of simple difference constraints.
Finally we give lower bound results that show that certain possible extensions
of these results are probably not feasible.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1">Sungjin Im</a>, <a href="http://arxiv.org/find/cs/1/au:+Moseley_B/0/1/0/all/0/1">Benjamin Moseley</a>, <a href="http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1">Hung Q. Ngo</a>, <a href="http://arxiv.org/find/cs/1/au:+Pruhs_K/0/1/0/all/0/1">Kirk Pruhs</a>, <a href="http://arxiv.org/find/cs/1/au:+Samadian_A/0/1/0/all/0/1">Alireza Samadian</a></p><p>We consider a class of optimization problems that involve determining the
maximum value that a function in a particular class can attain subject to a
collection of difference constraints. We show that a particular linear
programming technique, based on duality and projections, can be used to
rederive some structural results that were previously established using more ad
hoc methods. We then show that this technique can be used to obtain a
polynomial-time algorithm for a certain type of simple difference constraints.
Finally we give lower bound results that show that certain possible extensions
of these results are probably not feasible.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Tuesday, November 15
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/156'>TR22-156 |  Parameterized Inapproximability of the Minimum Distance Problem over all Fields and the Shortest Vector Problem in all $\ell_p$ Norms | 

	Huck Bennett, 

	Mahdi Cheraghchi, 

	Venkatesan Guruswami, 

	Joao Ribeiro</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We prove that the Minimum Distance Problem (MDP) on linear codes over any fixed finite field and parameterized by the input distance bound is W[1]-hard to approximate within any constant factor. We also prove analogous results for the parameterized Shortest Vector Problem (SVP) on integer lattices. Specifically, we prove that SVP in the $\ell_p$ norm is W[1]-hard to approximate within any constant factor for any fixed $p &gt;1$ and W[1]-hard to approximate within a factor approaching $2$ for $p=1$.(We show hardness under randomized reductions in each case.)

These results answer the main questions left open (and explicitly posed) by Bhattacharyya, Bonnet, Egri, Ghoshal, Karthik C. S., Lin, Manurangsi, and Marx (Journal of the ACM, 2021) on the complexity of parameterized MDP and SVP. For MDP, they established similar hardness for binary linear codes and left the case of general fields open. For SVP in $\ell_p$ norms with $p &gt; 1$, they showed inapproximability within some constant factor (depending on $p$) and left open showing such hardness for arbitrary constant factors. They also left open showing W[1]-hardness even of exact SVP in the $\ell_1$ norm.
        
        </div>

        <div class='tr-article-summary'>
        
          
          We prove that the Minimum Distance Problem (MDP) on linear codes over any fixed finite field and parameterized by the input distance bound is W[1]-hard to approximate within any constant factor. We also prove analogous results for the parameterized Shortest Vector Problem (SVP) on integer lattices. Specifically, we prove that SVP in the $\ell_p$ norm is W[1]-hard to approximate within any constant factor for any fixed $p &gt;1$ and W[1]-hard to approximate within a factor approaching $2$ for $p=1$.(We show hardness under randomized reductions in each case.)

These results answer the main questions left open (and explicitly posed) by Bhattacharyya, Bonnet, Egri, Ghoshal, Karthik C. S., Lin, Manurangsi, and Marx (Journal of the ACM, 2021) on the complexity of parameterized MDP and SVP. For MDP, they established similar hardness for binary linear codes and left the case of general fields open. For SVP in $\ell_p$ norms with $p &gt; 1$, they showed inapproximability within some constant factor (depending on $p$) and left open showing such hardness for arbitrary constant factors. They also left open showing W[1]-hardness even of exact SVP in the $\ell_1$ norm.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T19:01:27Z">Tuesday, November 15 2022, 19:01</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/155'>TR22-155 |  Testing of Index-Invariant Properties in the Huge Object Model | 

	Sayantan Sen, 

	Sourav Chakraborty, 

	Eldar Fischer, 

	Arijit Ghosh, 

	Gopinath Mishra</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The study of distribution testing has become ubiquitous in the area of property testing, both for its theoretical appeal, as well as for its applications in other fields of Computer Science, and in various real-life statistical tasks.

The original distribution testing model relies on samples drawn independently from the distribution to be tested. However, when testing distributions over the $n$-dimensional Hamming cube $\left\{0,1\right\}^{n}$ for a large $n$, even reading a few samples is infeasible. To address this, Goldreich and Ron [ITCS 2022] have defined a model called the huge object model, in which the samples may only be queried in a few places.

In this work, we initiate a study of a general class of properties in the huge object model, those that are invariant under a permutation of the indices of the vectors in $\left\{0,1\right\}^{n}$, while still not being necessarily fully symmetric as per the definition used in traditional distribution testing.

We  prove that every index-invariant property satisfying a bounded VC-dimension restriction admits a property tester with a number of queries independent of $n$. To complement this result, we argue that satisfying only index-invariance or only a VC-dimension bound is insufficient to guarantee a tester whose query complexity is independent of $n$. Moreover, we prove that the dependency of sample and query complexities of our tester on the VC-dimension is essentially tight. As a second part of this work, we address the question of the number of queries required for non-adaptive testing. We show that it can be at most quadratic in the number of queries required for an adaptive tester in the case of index-invariant properties. This is in contrast with the tight (easily provable) exponential gap between adaptive and non-adaptive testers for general non-index-invariant properties. Finally, we provide an index-invariant property for which the quadratic gap between adaptive and non-adaptive query complexities for testing is almost tight.
        
        </div>

        <div class='tr-article-summary'>
        
          
          The study of distribution testing has become ubiquitous in the area of property testing, both for its theoretical appeal, as well as for its applications in other fields of Computer Science, and in various real-life statistical tasks.

The original distribution testing model relies on samples drawn independently from the distribution to be tested. However, when testing distributions over the $n$-dimensional Hamming cube $\left\{0,1\right\}^{n}$ for a large $n$, even reading a few samples is infeasible. To address this, Goldreich and Ron [ITCS 2022] have defined a model called the huge object model, in which the samples may only be queried in a few places.

In this work, we initiate a study of a general class of properties in the huge object model, those that are invariant under a permutation of the indices of the vectors in $\left\{0,1\right\}^{n}$, while still not being necessarily fully symmetric as per the definition used in traditional distribution testing.

We  prove that every index-invariant property satisfying a bounded VC-dimension restriction admits a property tester with a number of queries independent of $n$. To complement this result, we argue that satisfying only index-invariance or only a VC-dimension bound is insufficient to guarantee a tester whose query complexity is independent of $n$. Moreover, we prove that the dependency of sample and query complexities of our tester on the VC-dimension is essentially tight. As a second part of this work, we address the question of the number of queries required for non-adaptive testing. We show that it can be at most quadratic in the number of queries required for an adaptive tester in the case of index-invariant properties. This is in contrast with the tight (easily provable) exponential gap between adaptive and non-adaptive testers for general non-index-invariant properties. Finally, we provide an index-invariant property for which the quadratic gap between adaptive and non-adaptive query complexities for testing is almost tight.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T18:43:10Z">Tuesday, November 15 2022, 18:43</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2022/11/15/linkage.html'>Linkage</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The massive influx of new users and new content to Mastodon has caused a greater number of these to be boosts of someone else’s post rather than posts of my own.
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The massive influx of new users and new content to Mastodon has caused a greater number of these to be boosts of someone else’s post rather than posts of my own.</p>

<ul>
  <li>
    <p><a href="https://www.quantamagazine.org/in-math-and-life-svetlana-jitomirskaya-stares-down-complexity-20221101/"><em>Quanta</em> has a nice profile of my colleague at UC Irvine, Ukrainian-born mathematician Svetlana Jitomirskaya</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109269250786290297">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://westy31.home.xs4all.nl/Penrose/Carboard_Impossible_Penrose_triangle.html">A cardboard model of the “impossible” Penrose triangle</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@GerardWestendorp/109259133843305538">\(\mathbb{M}\)</a>),</span> with construction instructions, a printable cutout, and a demo video link, by Gerard Westendorp.</p>
  </li>
  <li>
    <p><a href="https://ventrella.com/organic-algorithm/">Organic algorithm series</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109282060578920502">\(\mathbb{M}\)</a>),</span> artworks by Jeffrey Ventrella.</p>
  </li>
  <li>
    <p>Fatih Kızılkaya, a student of David Kempe at USC, gave an excellent talk in our theory seminar on <a href="https://arxiv.org/abs/2206.07098">“Plurality Veto” voting</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109288019438622582">\(\mathbb{M}\)</a>).</span> The idea is to count first-place votes per candidate, then let each voter cancel a single vote for their least-favorite remaining candidate (one voter at a time or simultaneously and fractionally) until only one candidate has votes left. With voters and candidates in a metric space and preferences by distance, this 3-approximates the min average distance to voters, best possible.</p>
  </li>
  <li>
    <p><a href="https://scholarlykitchen.sspnet.org/2022/11/01/guest-post-wikipedias-citations-are-influencing-scholars-and-publishers/">Getting an academic publication cited on Wikipedia tends to lead to more citations elsewhere, and open-access publications tend to be more frequently cited on Wikipedia</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109292417799090772">\(\mathbb{M}\)</a>,</span> <a href="https://retractionwatch.com/2022/11/05/weekend-reads-double-edition-sciences-nasty-photoshopping-problem-dr-ozs-publication-ban-image-manipulation-detection-software/">via</a>).</p>
  </li>
  <li>
    <p><a href="https://gilkalai.wordpress.com/2022/10/19/james-davies-every-finite-colouring-of-the-plane-contains-a-monochromatic-pair-of-points-at-an-odd-distance-from-each-other/">Every finite colouring of the plane contains a monochromatic pair of points at an odd distance from each other</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109299861480951154">\(\mathbb{M}\)</a>),</span> based on <a href="https://arxiv.org/abs/2209.15598">a new arXiv preprint by James Davies</a>. The paper is pretty heavy going and the linked blog post doesn’t give much detail, so providing a more generally understandable version of this looks like a challenge.</p>
  </li>
  <li>
    <p><a href="http://fredrik-j.blogspot.com/2009/02/how-not-to-compute-harmonic-numbers.html">How (not) to compute harmonic numbers</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@peterluschny/109280460006661224">\(\mathbb{M}\)</a>).</span> Like for factorials, exact computation using divide-and-conquer works much better than the obvious method of sequentially adding each unit fraction. But factorials have an even faster algorithm; it’s unclear whether the same idea can be made to work for the harmonic numbers.</p>
  </li>
  <li>
    <p><a href="http://courses.csail.mit.edu/6.849/fall12/lectures/C01.html">Erik Demaine’s geometric folding algorithms course lectures</a>
 <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@j2kun/109305200398869548">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://xkcd.com/2694/">xkcd on the bridges of Königsburg</a> <span style="white-space:nowrap">(<a href="https://mastodon.xyz/@xkcd/109288968362775008">\(\mathbb{M}\)</a>).</span> This one is definitely going into my lecture notes for the next time it comes around.</p>

    <p style="text-align:center"><img src="https://imgs.xkcd.com/comics/konigsberg.png" alt="xkcd on the bridges of Königsburg" /></p>
  </li>
  <li>
    <p>News I didn’t know about the university I work for, UC Irvine: apparently <a href="https://www.latimes.com/sports/story/2022-11-01/esports-uc-irvine-overwatch-league-of-legends-valorant">we’re an “esports powerhouse”</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109322880998262132">\(\mathbb{M}\)</a>,</span> <a href="https://web.archive.org/web/20221107065218/https://www.latimes.com/sports/story/2022-11-01/esports-uc-irvine-overwatch-league-of-legends-valorant">archive</a>). Definitely preferable to funneling all the alumni donations into football stadium construction! (Unlike many big US universities we don’t have a football team and I like it that way.)</p>
  </li>
  <li>
    <p>Zhao Liang asks: <a href="https://mathstodon.xyz/@neozhaoliang/109324352766569049">which polyhedra have the property that if you make them out of mirrors and stand inside, you will see a tessellation of space?</a></p>
  </li>
  <li>
    <p><a href="https://discrete-notes.github.io/mandatory-attendance">About mandatory attendance by presenters at CS research conferences</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109331612952132380">\(\mathbb{M}\)</a>).</span> I <a href="/blog/2022/11/13/report-from-latin.html">just attended LATIN</a>, a hybrid in person/online conference that ended up heavily tilted to in-person participation. In-person clearly leads to significantly greater interaction rather than mere passive reception of talks. I am very attracted to the idea of reducing the carbon footprint of my travel but in my experience we have not found a successful online replacement for that aspect of conferencing.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@divbyzero/109314296701448645">Dave Richeson folds a regular octagon from a strip of paper</a>.</p>
  </li>
  <li>
    <p>Our graduate students and their union went on strike yesterday. The <a href="https://www.latimes.com/california/story/2022-11-14/photos-strike-by-48-000-university-of-california-academic-workers-causes-systemwide-disruptions"><em>LA Times</em> has a photo-essay</a>  <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109345285468088888">\(\mathbb{M}\)</a>,</span> <a href="https://web.archive.org/web/20221115011509/https://www.latimes.com/california/story/2022-11-14/photos-strike-by-48-000-university-of-california-academic-workers-causes-systemwide-disruptions">archive</a>). For the union’s demands and university’s counteroffers see <a href="https://www.statnews.com/2022/11/14/university-of-california-academic-workers-go-out-on-strike-to-demand-higher-wages/">this story</a>.
You can also find recent <a href="https://www.ucop.edu/academic-personnel-programs/_files/2022/oct-2021-scales/t22.pdf">grad student</a> and <a href="https://www.ucop.edu/academic-personnel-programs/_files/2022-23/oct-2022-salary-scales/t1.pdf">faculty</a> salary scales online, but note that notionally at least the student salaries are for halftime work.</p>
  </li>
</ul><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T17:54:00Z">Tuesday, November 15 2022, 17:54</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/15/post-doc-at-university-of-cambridge-apply-by-december-19-2022/'>Post-doc at University of Cambridge (apply by December 19, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          A post-doctoral research position is available to work with Prof. Anuj Dawar on a UKRI-funded (ERC advanced grant replacement) project exploring the limits of symmetric computation. Website: www.jobs.cam.ac.uk/job/37949/ Email: anuj.dawar@cl.cam.ac.uk
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>A post-doctoral research position is available to work with Prof. Anuj Dawar on a UKRI-funded (ERC advanced grant replacement) project exploring the limits of symmetric computation.</p>
<p>Website: <a href="https://www.jobs.cam.ac.uk/job/37949/">https://www.jobs.cam.ac.uk/job/37949/</a><br />
Email: anuj.dawar@cl.cam.ac.uk</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T17:21:22Z">Tuesday, November 15 2022, 17:21</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/15/professors-associate-professors-assistant-professors-at-the-chinese-university-of-hong-kong-apply-by-may-31-2023/'>Professor(s) / Associate Professor(s) / Assistant Professor(s) at The Chinese University of Hong Kong (apply by May 31, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Applications are invited for:- Department of Computer Science and Engineering Professor(s) / Associate Professor(s) / Assistant Professor(s) (Ref: 220002PK) (Closing date: May 31, 2023) The Department of Computer Science and Engineering (CSE) at the Chinese University of Hong Kong has multiple tenure-track faculty positions at all ranks to pursue new strategic research initiatives. Website: cuhk.taleo.net/careersection/cu_career_teach/jobdetail.ftl?job=220002PK&#38;tz=GMT%2B08%3A00&#38;tzname=Asia%2FHong_Kong [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Applications are invited for:-</p>
<p>Department of Computer Science and Engineering<br />
Professor(s) / Associate Professor(s) / Assistant Professor(s) (Ref: 220002PK) (Closing date: May 31, 2023)</p>
<p>The Department of Computer Science and Engineering (CSE) at the Chinese University of Hong Kong has multiple tenure-track faculty positions at all ranks to pursue new strategic research initiatives.</p>
<p>Website: <a href="https://cuhk.taleo.net/careersection/cu_career_teach/jobdetail.ftl?job=220002PK&amp;tz=GMT%2B08%3A00&amp;tzname=Asia%2FHong_Kong">https://cuhk.taleo.net/careersection/cu_career_teach/jobdetail.ftl?job=220002PK&amp;tz=GMT%2B08%3A00&amp;tzname=Asia%2FHong_Kong</a><br />
Email: dept@cse.cuhk.edu.hk</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T08:43:36Z">Tuesday, November 15 2022, 08:43</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://blog.computationalcomplexity.org/2022/11/who-first-thought-of-notion-of.html'>Who first thought of the notion of Polynomial Time?</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>(Updated version of&nbsp; Computational Intractability: A Guide to Algorithmic Lower Bound by Demaine-Gasarch-Hajiaghayi is&nbsp;here)</p><p><br></p><p>Any question like who first though of X is often hard to answer. I blogged about who first came up with the Fib numbers&nbsp;here. I've heard rumors that IBM had search engines way before Google but could not figure out how to make money off of it. There are other examples.&nbsp;</p><p>I had learned that Cobham defined P in the paper The intrinsic computational difficulty of functions, in 1965. It was The conference on Logic, Methodology, and Philosophy of Science. The paper is&nbsp;here.&nbsp; Jack Edmonds had the notion of P in the paper Paths, Trees, and Flowers&nbsp;here&nbsp;in 1965.</p><p>While it is true that Cobham defined P in that paper, and he might have been the first one to do so, was the notion somehow around earlier. I first thought the answer was no. Why?&nbsp; Because if you look at Joe Kruskal's paper on MST&nbsp; (see here) you don't see anything resembling time complexity. No O(E+Vlog V) or whatnot. So I thought that if the notion of&nbsp;&nbsp;this algorithm runs in such-and-such time was not in the air, then certainly any notion of P could not have been.&nbsp;</p><p>Hence I was surprised when I accidentally (more on that later) came across the following:&nbsp;</p><p>In 1910 (really, 1910)&nbsp; H.C.Pocklington analyzed two algorithms for solving quadratic congruences and noticed that&nbsp;</p><p>one took time proportional to a power of the log of the modulus, where as</p><p>the other took time proportional to the modulus itself or its square root.&nbsp;</p><p>THAT is the distinction between P and NOT-P.&nbsp;</p><p>The paper is titled The determination of the exponent to which a number belongs, the practical solution of certain congruences, and the law of quadratic reciprocity. It appeared in 1910, in the Proceedings of the Cambridge Philosophical&nbsp; Society, Volume 16, pages 1-5. (I could not find it online. If you know of a place online for it, leave a comment.)&nbsp;</p><p>ADDED LATER: Here is the article in pieces:&nbsp;Page 1,&nbsp;Pages2,3,&nbsp;Pages4,5.</p><p>How did I come across this? And why had I NOT come across this in my roughly 40 years working in complexity theory?&nbsp;</p><p>I came across it while reading a blog of Scotts, The Kolmogorov Option, see&nbsp;here&nbsp;where Pocklington is mentioned in passing. I am surprised how arbitrary the set of things ones knows can be. I have put the Pocklington story in the Demaine-Gasarch-Hajiaghayi book&nbsp;Computational Intractability: A Guide to Algorithmic Lower Bounds&nbsp;so that this knowledge gets to be better known.</p><p>ADDED LATER: That Cobham and Edmonds are known for discovering or inventing P is an example of&nbsp; the well known&nbsp;</p><p>Columbus Principle: Things are named after the LAST person to discover them (note that Columbus was the last person to discover America.)</p><p>Bonus Question: Most principles where the author is not on it, the author might be unknown. NOT in this case. I KNOW who coined the term `Columbus Principle' Do you? (It was not me.)&nbsp;</p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p>By gasarch</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>(Updated version of&nbsp; <i>Computational Intractability: A Guide to Algorithmic Lower Bound</i> by Demaine-Gasarch-Hajiaghayi is&nbsp;<a href="https://hardness.mit.edu/">here</a>)</p><p><br /></p><p>Any question like <i>who first though of X </i>is often hard to answer. I blogged about who first came up with the Fib numbers&nbsp;<a href="https://blog.computationalcomplexity.org/search?q=Fib">here</a>. I've heard rumors that IBM had search engines way before Google but could not figure out how to make money off of it. There are other examples.&nbsp;</p><p>I had learned that Cobham defined P in the paper <i>The intrinsic computational difficulty of functions</i>, in 1965. It was The conference on Logic, Methodology, and Philosophy of Science. The paper is&nbsp;<a href="https://www.cs.toronto.edu/~sacook/homepage/cobham_intrinsic.pdf">here</a>.&nbsp; Jack Edmonds had the notion of P in the paper <i>Paths, Trees, and Flowers&nbsp;<a href="https://math.nist.gov/~JBernal/p_t_f.pdf">here</a>&nbsp;</i>in 1965.</p><p>While it is true that Cobham defined P in that paper, and he might have been the first one to do so, was the notion somehow around earlier. I first thought the answer was no. Why?&nbsp; Because if you look at Joe Kruskal's paper on MST&nbsp; (see <a href="https://www.ams.org/journals/proc/1956-007-01/S0002-9939-1956-0078686-7/S0002-9939-1956-0078686-7.pdf">here</a>) you don't see anything resembling time complexity. No O(E+Vlog V) or whatnot. So I thought that if the notion of&nbsp;&nbsp;<i>this algorithm runs in such-and-such time </i>was not in the air, then certainly any notion of P could not have been.&nbsp;</p><p>Hence I was surprised when I accidentally (more on that later) came across the following:&nbsp;</p><p>In 1910 (really, 1910)&nbsp; H.C.Pocklington analyzed two algorithms for solving quadratic congruences and noticed that&nbsp;</p><p><i>one took time proportional to a power of the log of the modulus, where as</i></p><p><i>the other took time proportional to the modulus itself or its square root.&nbsp;</i></p><p>THAT is the distinction between P and NOT-P.&nbsp;</p><p>The paper is titled <i>The determination of the exponent to which a number belongs, the practical solution of</i> <i>certain congruences, and the law of quadratic reciprocity. </i>It appeared in 1910, in the Proceedings of the Cambridge Philosophical&nbsp; Society, Volume 16, pages 1-5. (I could not find it online. If you know of a place online for it, leave a comment.)&nbsp;</p><p>ADDED LATER: Here is the article in pieces:&nbsp;<a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/pockpage1.png">Page 1</a>,&nbsp;<a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/pockpage2,3.png">Pages2,3</a>,&nbsp;<a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/pockpage4,5.png">Pages4,5</a>.</p><p>How did I come across this? And why had I NOT come across this in my roughly 40 years working in complexity theory?&nbsp;</p><p>I came across it while reading a blog of Scotts, <i>The Kolmogorov Option, </i>see&nbsp;<a href="https://scottaaronson.blog/?p=3376">here</a>&nbsp;where Pocklington is mentioned in passing. I am surprised how arbitrary the set of things ones knows can be. I have put the Pocklington story in the Demaine-Gasarch-Hajiaghayi book&nbsp;<a href="https://hardness.mit.edu/">Computational Intractability: A Guide to Algorithmic Lower Bounds</a>&nbsp;so that this knowledge gets to be better known.</p><p>ADDED LATER: That Cobham and Edmonds are known for discovering or inventing P is an example of&nbsp; the well known&nbsp;</p><p><b>Columbus Principle</b>: Things are named after the LAST person to discover them (note that Columbus was the last person to discover America.)</p><p>Bonus Question: Most principles where the author is not on it, the author might be unknown. NOT in this case. I KNOW who coined the term `Columbus Principle' Do you? (It was not me.)&nbsp;</p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p class="authors">By gasarch</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T04:18:00Z">Tuesday, November 15 2022, 04:18</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/15/faculty-at-university-of-toronto-apply-by-january-9-2023/'>Faculty at University of Toronto (apply by January 9, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The Department of Computer Science at the University of Toronto invites applications for multiple positions. In addition to a number of positions open to all areas of computer science, both at the assistant and at the associate levels, we also have positions targeted to computer security and cryptography, as well as to machine learning. Website: [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The Department of Computer Science at the University of Toronto invites applications for multiple positions. In addition to a number of positions open to all areas of computer science, both at the assistant and at the associate levels, we also have positions targeted to computer security and cryptography, as well as to machine learning.</p>
<p>Website: <a href="https://web.cs.toronto.edu/employment-opportunities">https://web.cs.toronto.edu/employment-opportunities</a><br />
Email: recruit@cs.toronto.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T03:18:06Z">Tuesday, November 15 2022, 03:18</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06485'>On automorphism group of a possible short algorithm for multiplication of $3\times3$ matrices</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Vladimir Burichenko</p><p>Studying algorithms admitting nontrivial symmetries is a prospective way of
constructing new short algorithms of matrix multiplication. The main result of
the article is that if there exists an algorithm of multiplicative length
$l\leq22$ for multuplication of $3\times3$ matrices then its automorphism group
is isomorphic to a subgroup of $S_l\times S_3$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Burichenko_V/0/1/0/all/0/1">Vladimir Burichenko</a></p><p>Studying algorithms admitting nontrivial symmetries is a prospective way of
constructing new short algorithms of matrix multiplication. The main result of
the article is that if there exists an algorithm of multiplicative length
$l\leq22$ for multuplication of $3\times3$ matrices then its automorphism group
is isomorphic to a subgroup of $S_l\times S_3$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06472'>The Stackelberg Game: responses to regular strategies</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Thomas Byrne</p><p>Following the solution to the One-Round Voronoi Game in arXiv:2011.13275, we
naturally may want to consider similar games based upon the competitive
locating of points and subsequent dividing of territories. In order to appease
the tears of White (the first player) after they have potentially been tricked
into going first in a game of point-placement, an alternative game (or rather,
an extension of the Voronoi game) is the Stackelberg game where all is not lost
if Black (the second player) gains over half of the contested area. It turns
out that plenty of results can be transferred from One-Round Voronoi Game and
what remains to be explored for the Stackelberg game is how best White can
mitigate the damage of Black's placements. Since significant weaknesses in
certain arrangements were outlined in arXiv:2011.13275, we shall first consider
arrangements that still satisfy these results (namely, White plays a certain
grid arrangement) and then explore how Black can best exploit these positions.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Byrne_T/0/1/0/all/0/1">Thomas Byrne</a></p><p>Following the solution to the One-Round Voronoi Game in <a href="/abs/2011.13275">arXiv:2011.13275</a>, we
naturally may want to consider similar games based upon the competitive
locating of points and subsequent dividing of territories. In order to appease
the tears of White (the first player) after they have potentially been tricked
into going first in a game of point-placement, an alternative game (or rather,
an extension of the Voronoi game) is the Stackelberg game where all is not lost
if Black (the second player) gains over half of the contested area. It turns
out that plenty of results can be transferred from One-Round Voronoi Game and
what remains to be explored for the Stackelberg game is how best White can
mitigate the damage of Black's placements. Since significant weaknesses in
certain arrangements were outlined in <a href="/abs/2011.13275">arXiv:2011.13275</a>, we shall first consider
arrangements that still satisfy these results (namely, White plays a certain
grid arrangement) and then explore how Black can best exploit these positions.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06459'>Faster Walsh-Hadamard and Discrete Fourier Transforms From Matrix Non-Rigidity</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Josh Alman, Kevin Rao</p><p>We give algorithms with lower arithmetic operation counts for both the
Walsh-Hadamard Transform (WHT) and the Discrete Fourier Transform (DFT) on
inputs of power-of-2 size $N$.
</p>
<p>For the WHT, our new algorithm has an operation count of $\frac{23}{24}N \log
N + O(N)$. To our knowledge, this gives the first improvement on the $N \log N$
operation count of the simple, folklore Fast Walsh-Hadamard Transform
algorithm.
</p>
<p>For the DFT, our new FFT algorithm uses $\frac{15}{4}N \log N + O(N)$ real
arithmetic operations. Our leading constant $\frac{15}{4} = 3.75$ is the first
improvement in over 15 years; it improves on the prior best leading constant
$\frac{34}{9} = 3.777\ldots$ by Van Buskirk from 2004, which in turn improved
on the leading constant of $4$ from the split-radix algorithm of Yavne from
1968 and the leading constant of $5$ from the Cooley-Tukey algorithm from 1965.
</p>
<p>Our new WHT algorithm takes advantage of a recent line of work on the
non-rigidity of the WHT: we decompose the WHT matrix as the sum of a low-rank
matrix and a sparse matrix, and then analyze the structures of these matrices
to achieve a lower operation count. Our new DFT algorithm comes from a novel
reduction, showing that parts of the previous best FFT algorithms can be
replaced by calls to an algorithm for the WHT. Replacing the folklore WHT
algorithm with our new improved algorithm leads to our improved FFT.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Alman_J/0/1/0/all/0/1">Josh Alman</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1">Kevin Rao</a></p><p>We give algorithms with lower arithmetic operation counts for both the
Walsh-Hadamard Transform (WHT) and the Discrete Fourier Transform (DFT) on
inputs of power-of-2 size $N$.
</p>
<p>For the WHT, our new algorithm has an operation count of $\frac{23}{24}N \log
N + O(N)$. To our knowledge, this gives the first improvement on the $N \log N$
operation count of the simple, folklore Fast Walsh-Hadamard Transform
algorithm.
</p>
<p>For the DFT, our new FFT algorithm uses $\frac{15}{4}N \log N + O(N)$ real
arithmetic operations. Our leading constant $\frac{15}{4} = 3.75$ is the first
improvement in over 15 years; it improves on the prior best leading constant
$\frac{34}{9} = 3.777\ldots$ by Van Buskirk from 2004, which in turn improved
on the leading constant of $4$ from the split-radix algorithm of Yavne from
1968 and the leading constant of $5$ from the Cooley-Tukey algorithm from 1965.
</p>
<p>Our new WHT algorithm takes advantage of a recent line of work on the
non-rigidity of the WHT: we decompose the WHT matrix as the sum of a low-rank
matrix and a sparse matrix, and then analyze the structures of these matrices
to achieve a lower operation count. Our new DFT algorithm comes from a novel
reduction, showing that parts of the previous best FFT algorithms can be
replaced by calls to an algorithm for the WHT. Replacing the folklore WHT
algorithm with our new improved algorithm leads to our improved FFT.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06521'>On maximal 3-edge-connected subgraphs of undirected graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Loukas Georgiadis, Giuseppe F. Italiano, Evangelos Kosinas, Debasish Pattanayak</p><p>We show how to find and efficiently maintain maximal 3-edge-connected
subgraphs in undirected graphs. In particular, we provide the following
results. $(1)$ Two algorithms for the incremental maintenance of the maximal
$3$-edge-connected subgraphs. These algorithms allow for vertex and edge
insertions, interspersed with queries asking whether two vertices belong to the
same maximal $3$-edge-connected subgraph, and there is a trade-off between
their time- and space-complexity. Specifically, the first algorithm has
$O(m\alpha(m,n) + n^2\log^2 n)$ total running time and uses $O(n)$ space, where
$m$ is the number of edge insertions and queries, and $n$ is the total number
of vertices inserted starting from an empty graph. The second algorithm
performs the same operations in faster $O(m\alpha(m,n) + n^2\alpha(n,n))$ time
in total, using $O(n^2)$ space. $(2)$ A fully dynamic algorithm for maintaining
information about the maximal $k$-edge-connected subgraphs for fixed $k$. Our
update bounds are ${O}(n\sqrt{n}\,\log n)$ worst-case time for $k&gt;4$ and
${O}(n\sqrt{n}\,)$ worst-case time for $k\in\{3,4\}$. In both cases, we achieve
constant time for maximal $k$-edge-connected subgraph queries. $(3)$ A
deterministic algorithm for computing the maximal $k$-edge-connected subgraphs,
for any fixed $k&gt;2$, in $\widetilde{O}(m+n\sqrt{n}\,)$ time. This result
improves substantially on the previously best known deterministic bounds and
matches (modulo $\log$ factors) the expected time of the best randomized
algorithm for the same problem. $(4)$ A linear-time algorithm for computing a
spanning subgraph with $O(n\log n)$ edges that has the same maximal
$k$-edge-connected subgraphs as the original graph.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Georgiadis_L/0/1/0/all/0/1">Loukas Georgiadis</a>, <a href="http://arxiv.org/find/cs/1/au:+Italiano_G/0/1/0/all/0/1">Giuseppe F. Italiano</a>, <a href="http://arxiv.org/find/cs/1/au:+Kosinas_E/0/1/0/all/0/1">Evangelos Kosinas</a>, <a href="http://arxiv.org/find/cs/1/au:+Pattanayak_D/0/1/0/all/0/1">Debasish Pattanayak</a></p><p>We show how to find and efficiently maintain maximal 3-edge-connected
subgraphs in undirected graphs. In particular, we provide the following
results. $(1)$ Two algorithms for the incremental maintenance of the maximal
$3$-edge-connected subgraphs. These algorithms allow for vertex and edge
insertions, interspersed with queries asking whether two vertices belong to the
same maximal $3$-edge-connected subgraph, and there is a trade-off between
their time- and space-complexity. Specifically, the first algorithm has
$O(m\alpha(m,n) + n^2\log^2 n)$ total running time and uses $O(n)$ space, where
$m$ is the number of edge insertions and queries, and $n$ is the total number
of vertices inserted starting from an empty graph. The second algorithm
performs the same operations in faster $O(m\alpha(m,n) + n^2\alpha(n,n))$ time
in total, using $O(n^2)$ space. $(2)$ A fully dynamic algorithm for maintaining
information about the maximal $k$-edge-connected subgraphs for fixed $k$. Our
update bounds are ${O}(n\sqrt{n}\,\log n)$ worst-case time for $k&gt;4$ and
${O}(n\sqrt{n}\,)$ worst-case time for $k\in\{3,4\}$. In both cases, we achieve
constant time for maximal $k$-edge-connected subgraph queries. $(3)$ A
deterministic algorithm for computing the maximal $k$-edge-connected subgraphs,
for any fixed $k&gt;2$, in $\widetilde{O}(m+n\sqrt{n}\,)$ time. This result
improves substantially on the previously best known deterministic bounds and
matches (modulo $\log$ factors) the expected time of the best randomized
algorithm for the same problem. $(4)$ A linear-time algorithm for computing a
spanning subgraph with $O(n\log n)$ edges that has the same maximal
$k$-edge-connected subgraphs as the original graph.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06530'>Multi-Epoch Matrix Factorization Mechanisms for Private Machine Learning</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Christopher A. Choquette-Choo, H. Brendan McMahan, Keith Rush, Abhradeep Thakurta</p><p>We introduce new differentially private (DP) mechanisms for gradient-based
machine learning (ML) training involving multiple passes (epochs) of a dataset,
substantially improving the achievable privacy-utility-computation tradeoffs.
Our key contribution is an extension of the online matrix factorization DP
mechanism to multiple participations, substantially generalizing the approach
of DMRST2022. We first give conditions under which it is possible to reduce the
problem with per-iteration vector contributions to the simpler one of scalar
contributions. Using this, we formulate the construction of optimal (in total
squared error at each iterate) matrix mechanisms for SGD variants as a convex
program. We propose an efficient optimization algorithm via a closed form
solution to the dual function.
</p>
<p>While tractable, both solving the convex problem offline and computing the
necessary noise masks during training can become prohibitively expensive when
many training steps are necessary. To address this, we design a
Fourier-transform-based mechanism with significantly less computation and only
a minor utility decrease.
</p>
<p>Extensive empirical evaluation on two tasks: example-level DP for image
classification and user-level DP for language modeling, demonstrate substantial
improvements over the previous state-of-the-art. Though our primary application
is to ML, we note our main DP results are applicable to arbitrary linear
queries and hence may have much broader applicability.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Choquette_Choo_C/0/1/0/all/0/1">Christopher A. Choquette-Choo</a>, <a href="http://arxiv.org/find/cs/1/au:+McMahan_H/0/1/0/all/0/1">H. Brendan McMahan</a>, <a href="http://arxiv.org/find/cs/1/au:+Rush_K/0/1/0/all/0/1">Keith Rush</a>, <a href="http://arxiv.org/find/cs/1/au:+Thakurta_A/0/1/0/all/0/1">Abhradeep Thakurta</a></p><p>We introduce new differentially private (DP) mechanisms for gradient-based
machine learning (ML) training involving multiple passes (epochs) of a dataset,
substantially improving the achievable privacy-utility-computation tradeoffs.
Our key contribution is an extension of the online matrix factorization DP
mechanism to multiple participations, substantially generalizing the approach
of DMRST2022. We first give conditions under which it is possible to reduce the
problem with per-iteration vector contributions to the simpler one of scalar
contributions. Using this, we formulate the construction of optimal (in total
squared error at each iterate) matrix mechanisms for SGD variants as a convex
program. We propose an efficient optimization algorithm via a closed form
solution to the dual function.
</p>
<p>While tractable, both solving the convex problem offline and computing the
necessary noise masks during training can become prohibitively expensive when
many training steps are necessary. To address this, we design a
Fourier-transform-based mechanism with significantly less computation and only
a minor utility decrease.
</p>
<p>Extensive empirical evaluation on two tasks: example-level DP for image
classification and user-level DP for language modeling, demonstrate substantial
improvements over the previous state-of-the-art. Though our primary application
is to ML, we note our main DP results are applicable to arbitrary linear
queries and hence may have much broader applicability.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06549'>Hypercubes and Hamiltonian Cycles of Display Sets of Rooted Phylogenetic Networks</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Janosch D&#xf6;cker, Simone Linz, Charles Semple</p><p>In the context of reconstructing phylogenetic networks from a collection of
phylogenetic trees, several characterisations and subsequently algorithms have
been established to reconstruct a phylogenetic network that collectively embeds
all trees in the input in some minimum way. For many instances however, the
resulting network also embeds additional phylogenetic trees that are not part
of the input. However, little is known about these inferred trees. In this
paper, we explore the relationships among all phylogenetic trees that are
embedded in a given phylogenetic network. First, we investigate some
combinatorial properties of the collection P of all rooted binary phylogenetic
trees that are embedded in a rooted binary phylogenetic network N. To this end,
we associated a particular graph G, which we call rSPR graph, with the elements
in P and show that, if |P|=2^k, where k is the number of vertices with
in-degree two in N, then G has a Hamiltonian cycle. Second, by exploiting rSPR
graphs and properties of hypercubes, we turn to the well-studied class of
rooted binary level-1 networks and give necessary and sufficient conditions for
when a set of rooted binary phylogenetic trees can be embedded in a level-1
network without inferring any additional trees. Lastly, we show how these
conditions translate into a polynomial-time algorithm to reconstruct such a
network if it exists.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Docker_J/0/1/0/all/0/1">Janosch D&#xf6;cker</a>, <a href="http://arxiv.org/find/math/1/au:+Linz_S/0/1/0/all/0/1">Simone Linz</a>, <a href="http://arxiv.org/find/math/1/au:+Semple_C/0/1/0/all/0/1">Charles Semple</a></p><p>In the context of reconstructing phylogenetic networks from a collection of
phylogenetic trees, several characterisations and subsequently algorithms have
been established to reconstruct a phylogenetic network that collectively embeds
all trees in the input in some minimum way. For many instances however, the
resulting network also embeds additional phylogenetic trees that are not part
of the input. However, little is known about these inferred trees. In this
paper, we explore the relationships among all phylogenetic trees that are
embedded in a given phylogenetic network. First, we investigate some
combinatorial properties of the collection P of all rooted binary phylogenetic
trees that are embedded in a rooted binary phylogenetic network N. To this end,
we associated a particular graph G, which we call rSPR graph, with the elements
in P and show that, if |P|=2^k, where k is the number of vertices with
in-degree two in N, then G has a Hamiltonian cycle. Second, by exploiting rSPR
graphs and properties of hypercubes, we turn to the well-studied class of
rooted binary level-1 networks and give necessary and sufficient conditions for
when a set of rooted binary phylogenetic trees can be embedded in a level-1
network without inferring any additional trees. Lastly, we show how these
conditions translate into a polynomial-time algorithm to reconstruct such a
network if it exists.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06567'>Pareto-Optimal Learning-Augmented Algorithms for Online k-Search Problems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Russell Lee, Bo Sun, John C.S. Lui, Mohammad Hajiesmaili</p><p>This paper leverages machine learned predictions to design online algorithms
for the k-max and k-min search problems. Our algorithms can achieve
performances competitive with the offline algorithm in hindsight when the
predictions are accurate (i.e., consistency) and also provide worst-case
guarantees when the predictions are arbitrarily wrong (i.e., robustness).
Further, we show that our algorithms have attained the Pareto-optimal trade-off
between consistency and robustness, where no other algorithms for k-max or
k-min search can improve on the consistency for a given robustness. To
demonstrate the performance of our algorithms, we evaluate them in experiments
of buying and selling Bitcoin.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1">Russell Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1">Bo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Lui_J/0/1/0/all/0/1">John C.S. Lui</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajiesmaili_M/0/1/0/all/0/1">Mohammad Hajiesmaili</a></p><p>This paper leverages machine learned predictions to design online algorithms
for the k-max and k-min search problems. Our algorithms can achieve
performances competitive with the offline algorithm in hindsight when the
predictions are accurate (i.e., consistency) and also provide worst-case
guarantees when the predictions are arbitrarily wrong (i.e., robustness).
Further, we show that our algorithms have attained the Pareto-optimal trade-off
between consistency and robustness, where no other algorithms for k-max or
k-min search can improve on the consistency for a given robustness. To
demonstrate the performance of our algorithms, we evaluate them in experiments
of buying and selling Bitcoin.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06636'>Approximation Algorithms for Drone Delivery Scheduling Problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Saswata Jana, Partha Sarathi Mandal</p><p>The coordination among drones and ground vehicles for last-mile delivery has
gained significant interest in recent years. In this paper, we study
\textit{multiple drone delivery scheduling problem(MDSP) \cite{Betti_ICDCN22}
for last-mile delivery, where we have a set of drones with an identical battery
budget and a set of delivery locations, along with reward or profit for
delivery, cost and delivery time intervals. The objective of the MDSP is to
find a collection of conflict-free schedules for each drone such that the total
profit for delivery is maximum subject to the battery constraint of the drones.
Here we propose a fully polynomial time approximation scheme (FPTAS) for the
single drone delivery scheduling problem (SDSP) and a
$\frac{1}{4}$-approximation algorithm for MDSP with a constraint on the number
of drones.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jana_S/0/1/0/all/0/1">Saswata Jana</a>, <a href="http://arxiv.org/find/cs/1/au:+Mandal_P/0/1/0/all/0/1">Partha Sarathi Mandal</a></p><p>The coordination among drones and ground vehicles for last-mile delivery has
gained significant interest in recent years. In this paper, we study
\textit{multiple drone delivery scheduling problem(MDSP) \cite{Betti_ICDCN22}
for last-mile delivery, where we have a set of drones with an identical battery
budget and a set of delivery locations, along with reward or profit for
delivery, cost and delivery time intervals. The objective of the MDSP is to
find a collection of conflict-free schedules for each drone such that the total
profit for delivery is maximum subject to the battery constraint of the drones.
Here we propose a fully polynomial time approximation scheme (FPTAS) for the
single drone delivery scheduling problem (SDSP) and a
$\frac{1}{4}$-approximation algorithm for MDSP with a constraint on the number
of drones.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06732'>Distributed and secure linear algebra -- Master Thesis</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Hugo Mirault</p><p>Cryptography is the discipline that allows securing of the exchange of
information. In this internship, we will focus on a certain branch of this
discipline, secure computation in a network. The main goal of this internship,
illustrated in this report, is to adapt a roster of protocols intended to do
linear algebra. We want to adapt them to do algebra for matrices with
polynomial coefficients. We then wish to make a complete analysis of the
different complexities of these protocols.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Mirault_H/0/1/0/all/0/1">Hugo Mirault</a></p><p>Cryptography is the discipline that allows securing of the exchange of
information. In this internship, we will focus on a certain branch of this
discipline, secure computation in a network. The main goal of this internship,
illustrated in this report, is to adapt a roster of protocols intended to do
linear algebra. We want to adapt them to do algebra for matrices with
polynomial coefficients. We then wish to make a complete analysis of the
different complexities of these protocols.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06790'>Near-Linear Sample Complexity for $L_p$ Polynomial Regression</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Raphael A. Meyer, Cameron Musco, Christopher Musco, David P. Woodruff, Samson Zhou</p><p>We study $L_p$ polynomial regression. Given query access to a function
$f:[-1,1] \rightarrow \mathbb{R}$, the goal is to find a degree $d$ polynomial
$\hat{q}$ such that, for a given parameter $\varepsilon &gt; 0$, $$
\|\hat{q}-f\|_p\le (1+\varepsilon) \cdot \min_{q:\text{deg}(q)\le d}\|q-f\|_p.
$$ Here $\|\cdot\|_p$ is the $L_p$ norm, $\|g\|_p = (\int_{-1}^1 |g(t)|^p
dt)^{1/p}$. We show that querying $f$ at points randomly drawn from the
Chebyshev measure on $[-1,1]$ is a near-optimal strategy for polynomial
regression in all $L_p$ norms. In particular, to find $\hat q$, it suffices to
sample $O(d\, \frac{\text{polylog}\,d}{\text{poly}\,\varepsilon})$ points from
$[-1,1]$ with probabilities proportional to this measure. While the optimal
sample complexity for polynomial regression was well understood for $L_2$ and
$L_\infty$, our result is the first that achieves sample complexity linear in
$d$ and error $(1+\varepsilon)$ for other values of $p$ without any
assumptions.
</p>
<p>Our result requires two main technical contributions. The first concerns
$p\leq 2$, for which we provide explicit bounds on the $L_p$ Lewis weight
function of the infinite linear operator underlying polynomial regression.
Using tools from the orthogonal polynomial literature, we show that this
function is bounded by the Chebyshev density. Our second key contribution is to
take advantage of the structure of polynomials to reduce the $p&gt;2$ case to the
$p\leq 2$ case. By doing so, we obtain a better sample complexity than what is
possible for general $p$-norm linear regression problems, for which
$\Omega(d^{p/2})$ samples are required.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Meyer_R/0/1/0/all/0/1">Raphael A. Meyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1">Cameron Musco</a>, <a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1">Christopher Musco</a>, <a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1">David P. Woodruff</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Samson Zhou</a></p><p>We study $L_p$ polynomial regression. Given query access to a function
$f:[-1,1] \rightarrow \mathbb{R}$, the goal is to find a degree $d$ polynomial
$\hat{q}$ such that, for a given parameter $\varepsilon &gt; 0$, $$
\|\hat{q}-f\|_p\le (1+\varepsilon) \cdot \min_{q:\text{deg}(q)\le d}\|q-f\|_p.
$$ Here $\|\cdot\|_p$ is the $L_p$ norm, $\|g\|_p = (\int_{-1}^1 |g(t)|^p
dt)^{1/p}$. We show that querying $f$ at points randomly drawn from the
Chebyshev measure on $[-1,1]$ is a near-optimal strategy for polynomial
regression in all $L_p$ norms. In particular, to find $\hat q$, it suffices to
sample $O(d\, \frac{\text{polylog}\,d}{\text{poly}\,\varepsilon})$ points from
$[-1,1]$ with probabilities proportional to this measure. While the optimal
sample complexity for polynomial regression was well understood for $L_2$ and
$L_\infty$, our result is the first that achieves sample complexity linear in
$d$ and error $(1+\varepsilon)$ for other values of $p$ without any
assumptions.
</p>
<p>Our result requires two main technical contributions. The first concerns
$p\leq 2$, for which we provide explicit bounds on the $L_p$ Lewis weight
function of the infinite linear operator underlying polynomial regression.
Using tools from the orthogonal polynomial literature, we show that this
function is bounded by the Chebyshev density. Our second key contribution is to
take advantage of the structure of polynomials to reduce the $p&gt;2$ case to the
$p\leq 2$ case. By doing so, we obtain a better sample complexity than what is
possible for general $p$-norm linear regression problems, for which
$\Omega(d^{p/2})$ samples are required.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Monday, November 14
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/14/deputy-director-at-dimacs-rutgers-university-apply-by-january-6-2023/'>Deputy Director at DIMACS, Rutgers University (apply by January 6, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          DIMACS, the Center for Discrete Mathematics and Theoretical Computer Science, based at Rutgers University in New Brunswick, New Jersey, USA, seeks a Deputy Director of the Center who would also serve as an Associate or Full Professor in computer science, mathematics, statistics, or another Rutgers department. Fostering diversity and inclusion is part of DIMACS’s culture [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>DIMACS, the Center for Discrete Mathematics and Theoretical Computer Science, based at Rutgers University in New Brunswick, New Jersey, USA, seeks a Deputy Director of the Center who would also serve as an Associate or Full Professor in computer science, mathematics, statistics, or another Rutgers department. Fostering diversity and inclusion is part of DIMACS’s culture and mission.</p>
<p>Website: <a href="https://go.rutgers.edu/dimacsdeputy">https://go.rutgers.edu/dimacsdeputy</a><br />
Email: spassion@dimacs.rutgers.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-14T19:54:33Z">Monday, November 14 2022, 19:54</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06121'>A parameterized halting problem, $\Delta_0$ truth and the MRDP theorem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yijia Chen, Moritz M&#xfc;ller, Keita Yokoyama</p><p>We study the parameterized complexity of the problem to decide whether a
given natural number $n$ satisfies a given $\Delta_0$-formula $\varphi(x)$; the
parameter is the size of $\varphi$. This parameterization focusses attention on
instances where $n$ is large compared to the size of $\varphi$. We show
unconditionally that this problem does not belong to the parameterized analogue
of $\mathsf{AC}^0$. From this we derive that certain natural upper bounds on
the complexity of our parameterized problem imply certain separations of
classical complexity classes. This connection is obtained via an analysis of a
parameterized halting problem. Some of these upper bounds follow assuming that
$I\Delta_0$ proves the MRDP theorem in a certain weak sense.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yijia Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1">Moritz M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Yokoyama_K/0/1/0/all/0/1">Keita Yokoyama</a></p><p>We study the parameterized complexity of the problem to decide whether a
given natural number $n$ satisfies a given $\Delta_0$-formula $\varphi(x)$; the
parameter is the size of $\varphi$. This parameterization focusses attention on
instances where $n$ is large compared to the size of $\varphi$. We show
unconditionally that this problem does not belong to the parameterized analogue
of $\mathsf{AC}^0$. From this we derive that certain natural upper bounds on
the complexity of our parameterized problem imply certain separations of
classical complexity classes. This connection is obtained via an analysis of a
parameterized halting problem. Some of these upper bounds follow assuming that
$I\Delta_0$ proves the MRDP theorem in a certain weak sense.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-14T01:30:00Z">Monday, November 14 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.05891'>A Closer Cut: Computing Near-Optimal Lawn Mowing Tours</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: S&#xe1;ndor P. Fekete, Dominik Krupke, Michael Perk, Christian Rieck, Christian Scheffer</p><p>For a given polygonal region $P$, the Lawn Mowing Problem (LMP) asks for a
shortest tour $T$ that gets within Euclidean distance 1 of every point in $P$;
this is equivalent to computing a shortest tour for a unit-disk cutter $C$ that
covers all of $P$. As a geometric optimization problem of natural practical and
theoretical importance, the LMP generalizes and combines several notoriously
difficult problems, including minimum covering by disks, the Traveling Salesman
Problem with neighborhoods (TSPN), and the Art Gallery Problem (AGP).
</p>
<p>In this paper, we conduct the first study of the Lawn Mowing Problem with a
focus on practical computation of near-optimal solutions. We provide new
theoretical insights: Optimal solutions are polygonal paths with a bounded
number of vertices, allowing a restriction to straight-line solutions; on the
other hand, there can be relatively simple instances for which optimal
solutions require a large class of irrational coordinates. On the practical
side, we present a primal-dual approach with provable convergence properties
based on solving a special case of the TSPN restricted to witness sets. In each
iteration, this establishes both a valid solution and a valid lower bound, and
thereby a bound on the remaining optimality gap. As we demonstrate in an
extensive computational study, this allows us to achieve provably optimal and
near-optimal solutions for a large spectrum of benchmark instances with up to
2000 vertices.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Fekete_S/0/1/0/all/0/1">S&#xe1;ndor P. Fekete</a>, <a href="http://arxiv.org/find/cs/1/au:+Krupke_D/0/1/0/all/0/1">Dominik Krupke</a>, <a href="http://arxiv.org/find/cs/1/au:+Perk_M/0/1/0/all/0/1">Michael Perk</a>, <a href="http://arxiv.org/find/cs/1/au:+Rieck_C/0/1/0/all/0/1">Christian Rieck</a>, <a href="http://arxiv.org/find/cs/1/au:+Scheffer_C/0/1/0/all/0/1">Christian Scheffer</a></p><p>For a given polygonal region $P$, the Lawn Mowing Problem (LMP) asks for a
shortest tour $T$ that gets within Euclidean distance 1 of every point in $P$;
this is equivalent to computing a shortest tour for a unit-disk cutter $C$ that
covers all of $P$. As a geometric optimization problem of natural practical and
theoretical importance, the LMP generalizes and combines several notoriously
difficult problems, including minimum covering by disks, the Traveling Salesman
Problem with neighborhoods (TSPN), and the Art Gallery Problem (AGP).
</p>
<p>In this paper, we conduct the first study of the Lawn Mowing Problem with a
focus on practical computation of near-optimal solutions. We provide new
theoretical insights: Optimal solutions are polygonal paths with a bounded
number of vertices, allowing a restriction to straight-line solutions; on the
other hand, there can be relatively simple instances for which optimal
solutions require a large class of irrational coordinates. On the practical
side, we present a primal-dual approach with provable convergence properties
based on solving a special case of the TSPN restricted to witness sets. In each
iteration, this establishes both a valid solution and a valid lower bound, and
thereby a bound on the remaining optimality gap. As we demonstrate in an
extensive computational study, this allows us to achieve provably optimal and
near-optimal solutions for a large spectrum of benchmark instances with up to
2000 vertices.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-14T01:30:00Z">Monday, November 14 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06009'>What's the Situation with Intelligent Mesh Generation: A Survey and Perspectives</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Zezeng Li, Zebin Xu, Ying Li, Xianfeng Gu, Na Lei</p><p>Intelligent mesh generation (IMG) refers to a technique to generate mesh by
machine learning, which is a relatively new and promising research field.
Within its short life span, IMG has greatly expanded the generalizability and
practicality of mesh generation techniques and brought many breakthroughs and
potential possibilities for mesh generation. However, there is a lack of
surveys focusing on IMG methods covering recent works. In this paper, we are
committed to a systematic and comprehensive survey describing the contemporary
IMG landscape. Focusing on 110 preliminary IMG methods, we conducted an
in-depth analysis and evaluation from multiple perspectives, including the core
technique and application scope of the algorithm, agent learning goals, data
types, targeting challenges, advantages and limitations. With the aim of
literature collection and classification based on content extraction, we
propose three different taxonomies from three views of key technique, output
mesh unit element, and applicable input data types. Finally, we highlight some
promising future research directions and challenges in IMG. To maximize the
convenience of readers, a project page of IMG is provided at
\url{github.com/xzb030/IMG_Survey}.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zezeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zebin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Ying Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1">Xianfeng Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_N/0/1/0/all/0/1">Na Lei</a></p><p>Intelligent mesh generation (IMG) refers to a technique to generate mesh by
machine learning, which is a relatively new and promising research field.
Within its short life span, IMG has greatly expanded the generalizability and
practicality of mesh generation techniques and brought many breakthroughs and
potential possibilities for mesh generation. However, there is a lack of
surveys focusing on IMG methods covering recent works. In this paper, we are
committed to a systematic and comprehensive survey describing the contemporary
IMG landscape. Focusing on 110 preliminary IMG methods, we conducted an
in-depth analysis and evaluation from multiple perspectives, including the core
technique and application scope of the algorithm, agent learning goals, data
types, targeting challenges, advantages and limitations. With the aim of
literature collection and classification based on content extraction, we
propose three different taxonomies from three views of key technique, output
mesh unit element, and applicable input data types. Finally, we highlight some
promising future research directions and challenges in IMG. To maximize the
convenience of readers, a project page of IMG is provided at
\url{https://github.com/xzb030/IMG_Survey}.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-14T01:30:00Z">Monday, November 14 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.05906'>A New Conjecture on Hardness of Low-Degree 2-CSP's with Implications to Hardness of Densest $k$-Subgraph and Other Problems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Julia Chuzhoy, Mina Dalirrooyfard, Vadim Grinberg, Zihan Tan</p><p>We propose a new conjecture on hardness of low-degree $2$-CSP's, and show
that new hardness of approximation results for Densest $k$-Subgraph and several
other problems, including a graph partitioning problem, and a variation of the
Graph Crossing Number problem, follow from this conjecture. The conjecture can
be viewed as occupying a middle ground between the $d$-to-$1$ conjecture, and
hardness results for $2$-CSP's that can be obtained via standard techniques,
such as Parallel Repetition combined with standard $2$-prover protocols for the
3SAT problem. We hope that this work will motivate further exploration of
hardness of $2$-CSP's in the regimes arising from the conjecture. We believe
that a positive resolution of the conjecture will provide a good starting point
for further hardness of approximation proofs.
</p>
<p>Another contribution of our work is proving that the problems that we
consider are roughly equivalent from the approximation perspective. Some of
these problems arose in previous work, from which it appeared that they may be
related to each other. We formalize this relationship in this work.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chuzhoy_J/0/1/0/all/0/1">Julia Chuzhoy</a>, <a href="http://arxiv.org/find/cs/1/au:+Dalirrooyfard_M/0/1/0/all/0/1">Mina Dalirrooyfard</a>, <a href="http://arxiv.org/find/cs/1/au:+Grinberg_V/0/1/0/all/0/1">Vadim Grinberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1">Zihan Tan</a></p><p>We propose a new conjecture on hardness of low-degree $2$-CSP's, and show
that new hardness of approximation results for Densest $k$-Subgraph and several
other problems, including a graph partitioning problem, and a variation of the
Graph Crossing Number problem, follow from this conjecture. The conjecture can
be viewed as occupying a middle ground between the $d$-to-$1$ conjecture, and
hardness results for $2$-CSP's that can be obtained via standard techniques,
such as Parallel Repetition combined with standard $2$-prover protocols for the
3SAT problem. We hope that this work will motivate further exploration of
hardness of $2$-CSP's in the regimes arising from the conjecture. We believe
that a positive resolution of the conjecture will provide a good starting point
for further hardness of approximation proofs.
</p>
<p>Another contribution of our work is proving that the problems that we
consider are roughly equivalent from the approximation perspective. Some of
these problems arose in previous work, from which it appeared that they may be
related to each other. We formalize this relationship in this work.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-14T01:30:00Z">Monday, November 14 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.05908'>Accelerating Irregular Applications via Efficient Synchronization and Data Access Techniques</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Christina Giannoula</p><p>Irregular applications comprise an increasingly important workload domain for
many fields, including bioinformatics, chemistry, physics, social sciences and
machine learning. Therefore, achieving high performance and energy efficiency
in the execution of emerging irregular applications is of vital importance.
This dissertation studies the root causes of inefficiency of irregular
applications in modern computing systems, and fundamentally addresses such
inefficiencies, by proposing low-overhead synchronization techniques among
parallel threads in cooperation with well-crafted data access policies.
</p>
<p>We make four major contributions to accelerating irregular applications in
different contexts including CPU and Near-Data-Processing (NDP) (or
Processing-In-Memory (PIM)) systems. First, we design ColorTM, a novel parallel
graph coloring algorithm for CPU systems that trades off using synchronization
with lower data access costs. Second, we propose SmartPQ, an adaptive priority
queue that achieves high performance under all various contention scenarios in
Non-Uniform Memory Access CPU systems. Third, we introduce SynCron, the first
practical hardware synchronization mechanism tailored for NDP systems. Fourth,
we design SparseP, the first library for high-performance Sparse Matrix Vector
Multiplication on real PIM systems.
</p>
<p>We demonstrate that the execution of irregular applications in CPU and
NDP/PIM architectures can be significantly accelerated by co-designing
lightweight synchronization approaches along with well-crafted data access
policies. This dissertation bridges the gap between processor-centric CPU
systems and memory-centric PIM systems in the critically-important area of
irregular applications. We hope that this dissertation inspires future work in
co-designing software algorithms with cutting-edge computing platforms to
significantly accelerate emerging irregular applications.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Giannoula_C/0/1/0/all/0/1">Christina Giannoula</a></p><p>Irregular applications comprise an increasingly important workload domain for
many fields, including bioinformatics, chemistry, physics, social sciences and
machine learning. Therefore, achieving high performance and energy efficiency
in the execution of emerging irregular applications is of vital importance.
This dissertation studies the root causes of inefficiency of irregular
applications in modern computing systems, and fundamentally addresses such
inefficiencies, by proposing low-overhead synchronization techniques among
parallel threads in cooperation with well-crafted data access policies.
</p>
<p>We make four major contributions to accelerating irregular applications in
different contexts including CPU and Near-Data-Processing (NDP) (or
Processing-In-Memory (PIM)) systems. First, we design ColorTM, a novel parallel
graph coloring algorithm for CPU systems that trades off using synchronization
with lower data access costs. Second, we propose SmartPQ, an adaptive priority
queue that achieves high performance under all various contention scenarios in
Non-Uniform Memory Access CPU systems. Third, we introduce SynCron, the first
practical hardware synchronization mechanism tailored for NDP systems. Fourth,
we design SparseP, the first library for high-performance Sparse Matrix Vector
Multiplication on real PIM systems.
</p>
<p>We demonstrate that the execution of irregular applications in CPU and
NDP/PIM architectures can be significantly accelerated by co-designing
lightweight synchronization approaches along with well-crafted data access
policies. This dissertation bridges the gap between processor-centric CPU
systems and memory-centric PIM systems in the critically-important area of
irregular applications. We hope that this dissertation inspires future work in
co-designing software algorithms with cutting-edge computing platforms to
significantly accelerate emerging irregular applications.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-14T01:30:00Z">Monday, November 14 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06028'>Fair Curing and Network Design in SIS Epidemic Processes</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yuhao Yi, Liren Shan, Philip E. Par&#xe9;, Karl H. Johansson</p><p>This paper studies efficient algorithms for dynamic curing policies and the
corresponding network design problems to guarantee the fast extinction of
epidemic spread in a susceptible-infected-susceptible (SIS) model. We consider
a Markov process-based SIS epidemic model. We call a curing policy fair if
demographic groups are cured with comparable speeds. We propose a fair curing
policy based on the curing policy of Drakopoulos et al. Since these
optimization problems are NP-hard, finding optimal policies is intractable for
large graphs. We provide approximation guarantees on both curing and fair
curing policies. Moreover, when the total infection rate is large, the original
curing policy includes a waiting period in which no measures are taken to
mitigate the spread until the rate slows down. To avoid the waiting period, we
study network design problems to reduce the total infection rate by deleting
edges or reducing the weight of edges. Then the curing processes become
continuous since the total infection rate is restricted by network design. We
provide algorithms with provable guarantees for the considered network design
problems. In summary, the proposed fair curing and network design algorithms
together provide an effective, fair, and computationally efficient approach
that mitigates SIS epidemic spread in networks.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Yi_Y/0/1/0/all/0/1">Yuhao Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_L/0/1/0/all/0/1">Liren Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pare_P/0/1/0/all/0/1">Philip E. Par&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Johansson_K/0/1/0/all/0/1">Karl H. Johansson</a></p><p>This paper studies efficient algorithms for dynamic curing policies and the
corresponding network design problems to guarantee the fast extinction of
epidemic spread in a susceptible-infected-susceptible (SIS) model. We consider
a Markov process-based SIS epidemic model. We call a curing policy fair if
demographic groups are cured with comparable speeds. We propose a fair curing
policy based on the curing policy of Drakopoulos et al. Since these
optimization problems are NP-hard, finding optimal policies is intractable for
large graphs. We provide approximation guarantees on both curing and fair
curing policies. Moreover, when the total infection rate is large, the original
curing policy includes a waiting period in which no measures are taken to
mitigate the spread until the rate slows down. To avoid the waiting period, we
study network design problems to reduce the total infection rate by deleting
edges or reducing the weight of edges. Then the curing processes become
continuous since the total infection rate is restricted by network design. We
provide algorithms with provable guarantees for the considered network design
problems. In summary, the proposed fair curing and network design algorithms
together provide an effective, fair, and computationally efficient approach
that mitigates SIS epidemic spread in networks.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-14T01:30:00Z">Monday, November 14 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06033'>A Faster Small Treewidth SDP Solver</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yuzhou Gu, Zhao Song</p><p>Semidefinite programming is a fundamental tool in optimization and
theoretical computer science. It has been extensively used as a black-box for
solving many problems, such as embedding, complexity, learning, and
discrepancy.
</p>
<p>One natural setting of semidefinite programming is the small treewidth
setting. The best previous SDP solver under small treewidth setting is due to
Zhang-Lavaei '18, which takes $n^{1.5} \tau^{6.5}$ time. In this work, we show
how to solve a semidefinite programming with $n \times n$ variables, $m$
constraints and $\tau$ treewidth in $n \tau^{2\omega+0.5}$ time, where $\omega
&lt; 2.373$ denotes the exponent of matrix multiplication. We give the first SDP
solver that runs in time in linear in number of variables under this setting.
</p>
<p>In addition, we improve the running time that solves a linear programming
with tau treewidth from $n \tau^2$ (Dong-Lee-Ye '21) to $n
\tau^{(\omega+1)/2}$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Gu_Y/0/1/0/all/0/1">Yuzhou Gu</a>, <a href="http://arxiv.org/find/math/1/au:+Song_Z/0/1/0/all/0/1">Zhao Song</a></p><p>Semidefinite programming is a fundamental tool in optimization and
theoretical computer science. It has been extensively used as a black-box for
solving many problems, such as embedding, complexity, learning, and
discrepancy.
</p>
<p>One natural setting of semidefinite programming is the small treewidth
setting. The best previous SDP solver under small treewidth setting is due to
Zhang-Lavaei '18, which takes $n^{1.5} \tau^{6.5}$ time. In this work, we show
how to solve a semidefinite programming with $n \times n$ variables, $m$
constraints and $\tau$ treewidth in $n \tau^{2\omega+0.5}$ time, where $\omega
&lt; 2.373$ denotes the exponent of matrix multiplication. We give the first SDP
solver that runs in time in linear in number of variables under this setting.
</p>
<p>In addition, we improve the running time that solves a linear programming
with tau treewidth from $n \tau^2$ (Dong-Lee-Ye '21) to $n
\tau^{(\omega+1)/2}$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-14T01:30:00Z">Monday, November 14 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06044'>External-memory dictionaries with worst-case update cost</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Rathish Das, John Iacono, Yakov Nekrich</p><p>The $B^{\epsilon}$-tree [Brodal and Fagerberg 2003] is a simple I/O-efficient
external-memory-model data structure that supports updates orders of magnitude
faster than B-tree with a query performance comparable to the B-tree: for any
positive constant $\epsilon&lt;1$ insertions and deletions take
$O(\frac{1}{B^{1-\epsilon}}\log_{B}N)$ time (rather than $O(\log_BN)$ time for
the classic B-tree), queries take $O(\log_BN)$ time and range queries returning
$k$ items take $O(\log_BN+\frac{k}{B})$ time. Although the $B^{\epsilon}$-tree
has an optimal update/query tradeoff, the runtimes are amortized. Another
structure, the write-optimized skip list, introduced by Bender et al. [PODS
2017], has the same performance as the $B^{\epsilon}$-tree but with runtimes
that are randomized rather than amortized. In this paper, we present a variant
of the $B^{\epsilon}$-tree with deterministic worst-case running times that are
identical to the original's amortized running times.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1">Rathish Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Iacono_J/0/1/0/all/0/1">John Iacono</a>, <a href="http://arxiv.org/find/cs/1/au:+Nekrich_Y/0/1/0/all/0/1">Yakov Nekrich</a></p><p>The $B^{\epsilon}$-tree [Brodal and Fagerberg 2003] is a simple I/O-efficient
external-memory-model data structure that supports updates orders of magnitude
faster than B-tree with a query performance comparable to the B-tree: for any
positive constant $\epsilon&lt;1$ insertions and deletions take
$O(\frac{1}{B^{1-\epsilon}}\log_{B}N)$ time (rather than $O(\log_BN)$ time for
the classic B-tree), queries take $O(\log_BN)$ time and range queries returning
$k$ items take $O(\log_BN+\frac{k}{B})$ time. Although the $B^{\epsilon}$-tree
has an optimal update/query tradeoff, the runtimes are amortized. Another
structure, the write-optimized skip list, introduced by Bender et al. [PODS
2017], has the same performance as the $B^{\epsilon}$-tree but with runtimes
that are randomized rather than amortized. In this paper, we present a variant
of the $B^{\epsilon}$-tree with deterministic worst-case running times that are
identical to the original's amortized running times.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-14T01:30:00Z">Monday, November 14 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06065'>Extended Formulations via Decision Diagrams</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yuta Kurokawa, Ryotaro Mitsuboshi, Haruki Hamasaki, Kohei Hatano, Eiji Takimoto, Holakou Rahmanian</p><p>We propose a general algorithm of constructing an extended formulation for
any given set of linear constraints with integer coefficients. Our algorithm
consists of two phases: first construct a decision diagram $(V,E)$ that somehow
represents a given $m \times n$ constraint matrix, and then build an equivalent
set of $|E|$ linear constraints over $n+|V|$ variables. That is, the size of
the resultant extended formulation depends not explicitly on the number $m$ of
the original constraints, but on its decision diagram representation.
Therefore, we may significantly reduce the computation time for optimization
problems with integer constraint matrices by solving them under the extended
formulations, especially when we obtain concise decision diagram
representations for the matrices. We can apply our method to $1$-norm
regularized hard margin optimization over the binary instance space
$\{0,1\}^n$, which can be formulated as a linear programming problem with $m$
constraints with $\{-1,0,1\}$-valued coefficients over $n$ variables, where $m$
is the size of the given sample. Furthermore, introducing slack variables over
the edges of the decision diagram, we establish a variant formulation of soft
margin optimization. We demonstrate the effectiveness of our extended
formulations for integer programming and the $1$-norm regularized soft margin
optimization tasks over synthetic and real datasets.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kurokawa_Y/0/1/0/all/0/1">Yuta Kurokawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitsuboshi_R/0/1/0/all/0/1">Ryotaro Mitsuboshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamasaki_H/0/1/0/all/0/1">Haruki Hamasaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Hatano_K/0/1/0/all/0/1">Kohei Hatano</a>, <a href="http://arxiv.org/find/cs/1/au:+Takimoto_E/0/1/0/all/0/1">Eiji Takimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahmanian_H/0/1/0/all/0/1">Holakou Rahmanian</a></p><p>We propose a general algorithm of constructing an extended formulation for
any given set of linear constraints with integer coefficients. Our algorithm
consists of two phases: first construct a decision diagram $(V,E)$ that somehow
represents a given $m \times n$ constraint matrix, and then build an equivalent
set of $|E|$ linear constraints over $n+|V|$ variables. That is, the size of
the resultant extended formulation depends not explicitly on the number $m$ of
the original constraints, but on its decision diagram representation.
Therefore, we may significantly reduce the computation time for optimization
problems with integer constraint matrices by solving them under the extended
formulations, especially when we obtain concise decision diagram
representations for the matrices. We can apply our method to $1$-norm
regularized hard margin optimization over the binary instance space
$\{0,1\}^n$, which can be formulated as a linear programming problem with $m$
constraints with $\{-1,0,1\}$-valued coefficients over $n$ variables, where $m$
is the size of the given sample. Furthermore, introducing slack variables over
the edges of the decision diagram, we establish a variant formulation of soft
margin optimization. We demonstrate the effectiveness of our extended
formulations for integer programming and the $1$-norm regularized soft margin
optimization tasks over synthetic and real datasets.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-14T01:30:00Z">Monday, November 14 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06109'>A Dynamic MaxSAT-based Approach to Directed Feedback Vertex Sets</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Rafael Kiesel, Andr&#xe9; Schidler</p><p>We propose a new approach to the Directed Feedback Vertex Set Problem
(DFVSP), where the input is a directed graph and the solution is a minimum set
of vertices whose removal makes the graph acyclic.
</p>
<p>Our approach, implemented in the solver DAGer, is based on two novel
contributions: Firstly, we add a wide range of data reductions that are
partially inspired by reductions for the similar vertex cover problem. For
this, we give a theoretical basis for lifting reductions from vertex cover to
DFVSP but also incorporate novel ideas into strictly more general and new DFVSP
reductions.
</p>
<p>Secondly, we propose dynamically encoding DFVSP in propositional logic using
cycle propagation for improved performance. Cycle propagation builds on the
idea that already a limited number of the constraints in a propositional
encoding is usually sufficient for finding an optimal solution. Our algorithm,
therefore, starts with a small number of constraints and cycle propagation adds
additional constraints when necessary. We propose an efficient integration of
cycle propagation into the workflow of MaxSAT solvers, further improving the
performance of our algorithm.
</p>
<p>Our extensive experimental evaluation shows that DAGer significantly
outperforms the state-of-the-art solvers and that our data reductions alone
directly solve many of the instances.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kiesel_R/0/1/0/all/0/1">Rafael Kiesel</a>, <a href="http://arxiv.org/find/cs/1/au:+Schidler_A/0/1/0/all/0/1">Andr&#xe9; Schidler</a></p><p>We propose a new approach to the Directed Feedback Vertex Set Problem
(DFVSP), where the input is a directed graph and the solution is a minimum set
of vertices whose removal makes the graph acyclic.
</p>
<p>Our approach, implemented in the solver DAGer, is based on two novel
contributions: Firstly, we add a wide range of data reductions that are
partially inspired by reductions for the similar vertex cover problem. For
this, we give a theoretical basis for lifting reductions from vertex cover to
DFVSP but also incorporate novel ideas into strictly more general and new DFVSP
reductions.
</p>
<p>Secondly, we propose dynamically encoding DFVSP in propositional logic using
cycle propagation for improved performance. Cycle propagation builds on the
idea that already a limited number of the constraints in a propositional
encoding is usually sufficient for finding an optimal solution. Our algorithm,
therefore, starts with a small number of constraints and cycle propagation adds
additional constraints when necessary. We propose an efficient integration of
cycle propagation into the workflow of MaxSAT solvers, further improving the
performance of our algorithm.
</p>
<p>Our extensive experimental evaluation shows that DAGer significantly
outperforms the state-of-the-art solvers and that our data reductions alone
directly solve many of the instances.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-14T01:30:00Z">Monday, November 14 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06267'>Approximate Max-Flow Min-Multicut Theorem for Graphs of Bounded Treewidth</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Tobias Friedrich, Davis Issac, Nikhil Kumar, Nadym Mallek, Ziena Zeif</p><p>We prove an approximate max-multiflow min-multicut theorem for bounded
treewidth graphs. In particular, we show the following: Given a treewidth-$r$
graph, there exists a (fractional) multicommodity flow of value $f$, and a
multicut of capacity $c$ such that $ f \leq c \leq \mathcal{O}(\ln (r+1)) \cdot
f$. It is well known that the multiflow-multicut gap on an $r$-vertex (constant
degree) expander graph can be $\Omega(\ln r)$, and hence our result is tight up
to constant factors. Our proof is constructive, and we also obtain a polynomial
time $\mathcal{O}(\ln (r+1))$-approximation algorithm for the minimum multicut
problem on treewidth-$r$ graphs. Our algorithm proceeds by rounding the optimal
fractional solution to the natural linear programming relaxation of the
multicut problem. We introduce novel modifications to the well-known region
growing algorithm to facilitate the rounding while guaranteeing at most a
logarithmic factor loss in the treewidth.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Friedrich_T/0/1/0/all/0/1">Tobias Friedrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Issac_D/0/1/0/all/0/1">Davis Issac</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1">Nikhil Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mallek_N/0/1/0/all/0/1">Nadym Mallek</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeif_Z/0/1/0/all/0/1">Ziena Zeif</a></p><p>We prove an approximate max-multiflow min-multicut theorem for bounded
treewidth graphs. In particular, we show the following: Given a treewidth-$r$
graph, there exists a (fractional) multicommodity flow of value $f$, and a
multicut of capacity $c$ such that $ f \leq c \leq \mathcal{O}(\ln (r+1)) \cdot
f$. It is well known that the multiflow-multicut gap on an $r$-vertex (constant
degree) expander graph can be $\Omega(\ln r)$, and hence our result is tight up
to constant factors. Our proof is constructive, and we also obtain a polynomial
time $\mathcal{O}(\ln (r+1))$-approximation algorithm for the minimum multicut
problem on treewidth-$r$ graphs. Our algorithm proceeds by rounding the optimal
fractional solution to the natural linear programming relaxation of the
multicut problem. We introduce novel modifications to the well-known region
growing algorithm to facilitate the rounding while guaranteeing at most a
logarithmic factor loss in the treewidth.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-14T01:30:00Z">Monday, November 14 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06352'>Spectral Triadic Decompositions of Real-World Networks</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sabyasachi Basu, Suman Kalyan Bera, C. Seshadhri</p><p>A fundamental problem in mathematics and network analysis is to find
conditions under which a graph can be partitioned into smaller pieces. The most
important tool for this partitioning is the Fiedler vector or discrete Cheeger
inequality. These results relate the graph spectrum (eigenvalues of the
normalized adjacency matrix) to the ability to break a graph into two pieces,
with few edge deletions. An entire subfield of mathematics, called spectral
graph theory, has emerged from these results. Yet these results do not say
anything about the rich community structure exhibited by real-world networks,
which typically have a significant fraction of edges contained in numerous
densely clustered blocks. Inspired by the properties of real-world networks, we
discover a new spectral condition that relates eigenvalue powers to a network
decomposition into densely clustered blocks. We call this the \emph{spectral
triadic decomposition}. Our relationship exactly predicts the existence of
community structure, as commonly seen in real networked data. Our proof
provides an efficient algorithm to produce the spectral triadic decomposition.
We observe on numerous social, coauthorship, and citation network datasets that
these decompositions have significant correlation with semantically meaningful
communities.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1">Sabyasachi Basu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bera_S/0/1/0/all/0/1">Suman Kalyan Bera</a>, <a href="http://arxiv.org/find/cs/1/au:+Seshadhri_C/0/1/0/all/0/1">C. Seshadhri</a></p><p>A fundamental problem in mathematics and network analysis is to find
conditions under which a graph can be partitioned into smaller pieces. The most
important tool for this partitioning is the Fiedler vector or discrete Cheeger
inequality. These results relate the graph spectrum (eigenvalues of the
normalized adjacency matrix) to the ability to break a graph into two pieces,
with few edge deletions. An entire subfield of mathematics, called spectral
graph theory, has emerged from these results. Yet these results do not say
anything about the rich community structure exhibited by real-world networks,
which typically have a significant fraction of edges contained in numerous
densely clustered blocks. Inspired by the properties of real-world networks, we
discover a new spectral condition that relates eigenvalue powers to a network
decomposition into densely clustered blocks. We call this the \emph{spectral
triadic decomposition}. Our relationship exactly predicts the existence of
community structure, as commonly seen in real networked data. Our proof
provides an efficient algorithm to produce the spectral triadic decomposition.
We observe on numerous social, coauthorship, and citation network datasets that
these decompositions have significant correlation with semantically meaningful
communities.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-14T01:30:00Z">Monday, November 14 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06387'>\~Optimal Differentially Private Learning of Thresholds and Quasi-Concave Optimization</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Edith Cohen, Xin Lyu, Jelani Nelson, Tam&#xe1;s Sarl&#xf3;s, Uri Stemmer</p><p>The problem of learning threshold functions is a fundamental one in machine
learning. Classical learning theory implies sample complexity of $O(\xi^{-1}
\log(1/\beta))$ (for generalization error $\xi$ with confidence $1-\beta$). The
private version of the problem, however, is more challenging and in particular,
the sample complexity must depend on the size $|X|$ of the domain. Progress on
quantifying this dependence, via lower and upper bounds, was made in a line of
works over the past decade. In this paper, we finally close the gap for
approximate-DP and provide a nearly tight upper bound of $\tilde{O}(\log^*
|X|)$, which matches a lower bound by Alon et al (that applies even with
improper learning) and improves over a prior upper bound of $\tilde{O}((\log^*
|X|)^{1.5})$ by Kaplan et al. We also provide matching upper and lower bounds
of $\tilde{\Theta}(2^{\log^*|X|})$ for the additive error of private
quasi-concave optimization (a related and more general problem). Our
improvement is achieved via the novel Reorder-Slice-Compute paradigm for
private data analysis which we believe will have further applications.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cohen_E/0/1/0/all/0/1">Edith Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1">Xin Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nelson_J/0/1/0/all/0/1">Jelani Nelson</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarlos_T/0/1/0/all/0/1">Tam&#xe1;s Sarl&#xf3;s</a>, <a href="http://arxiv.org/find/cs/1/au:+Stemmer_U/0/1/0/all/0/1">Uri Stemmer</a></p><p>The problem of learning threshold functions is a fundamental one in machine
learning. Classical learning theory implies sample complexity of $O(\xi^{-1}
\log(1/\beta))$ (for generalization error $\xi$ with confidence $1-\beta$). The
private version of the problem, however, is more challenging and in particular,
the sample complexity must depend on the size $|X|$ of the domain. Progress on
quantifying this dependence, via lower and upper bounds, was made in a line of
works over the past decade. In this paper, we finally close the gap for
approximate-DP and provide a nearly tight upper bound of $\tilde{O}(\log^*
|X|)$, which matches a lower bound by Alon et al (that applies even with
improper learning) and improves over a prior upper bound of $\tilde{O}((\log^*
|X|)^{1.5})$ by Kaplan et al. We also provide matching upper and lower bounds
of $\tilde{\Theta}(2^{\log^*|X|})$ for the additive error of private
quasi-concave optimization (a related and more general problem). Our
improvement is achieved via the novel Reorder-Slice-Compute paradigm for
private data analysis which we believe will have further applications.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-14T01:30:00Z">Monday, November 14 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06418'>Re-Analyze Gauss: Bounds for Private Matrix Approximation via Dyson Brownian Motion</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Oren Mangoubi, Nisheeth K. Vishnoi</p><p>Given a symmetric matrix $M$ and a vector $\lambda$, we present new bounds on
the Frobenius-distance utility of the Gaussian mechanism for approximating $M$
by a matrix whose spectrum is $\lambda$, under
$(\varepsilon,\delta)$-differential privacy. Our bounds depend on both
$\lambda$ and the gaps in the eigenvalues of $M$, and hold whenever the top
$k+1$ eigenvalues of $M$ have sufficiently large gaps. When applied to the
problems of private rank-$k$ covariance matrix approximation and subspace
recovery, our bounds yield improvements over previous bounds. Our bounds are
obtained by viewing the addition of Gaussian noise as a continuous-time matrix
Brownian motion. This viewpoint allows us to track the evolution of eigenvalues
and eigenvectors of the matrix, which are governed by stochastic differential
equations discovered by Dyson. These equations allow us to bound the utility as
the square-root of a sum-of-squares of perturbations to the eigenvectors, as
opposed to a sum of perturbation bounds obtained via Davis-Kahan-type theorems.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Mangoubi_O/0/1/0/all/0/1">Oren Mangoubi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vishnoi_N/0/1/0/all/0/1">Nisheeth K. Vishnoi</a></p><p>Given a symmetric matrix $M$ and a vector $\lambda$, we present new bounds on
the Frobenius-distance utility of the Gaussian mechanism for approximating $M$
by a matrix whose spectrum is $\lambda$, under
$(\varepsilon,\delta)$-differential privacy. Our bounds depend on both
$\lambda$ and the gaps in the eigenvalues of $M$, and hold whenever the top
$k+1$ eigenvalues of $M$ have sufficiently large gaps. When applied to the
problems of private rank-$k$ covariance matrix approximation and subspace
recovery, our bounds yield improvements over previous bounds. Our bounds are
obtained by viewing the addition of Gaussian noise as a continuous-time matrix
Brownian motion. This viewpoint allows us to track the evolution of eigenvalues
and eigenvectors of the matrix, which are governed by stochastic differential
equations discovered by Dyson. These equations allow us to bound the utility as
the square-root of a sum-of-squares of perturbations to the eigenvectors, as
opposed to a sum of perturbation bounds obtained via Davis-Kahan-type theorems.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-14T01:30:00Z">Monday, November 14 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Sunday, November 13
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/154'>TR22-154 |  Spectral Expanding Expanders | 

	Gil Cohen, 

	Itay Cohen</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Dinitz, Schapira, and Valadarsky (Algorithmica 2017) introduced the intriguing notion of expanding expanders -- a family of expander graphs with the property that every two consecutive graphs in the family differ only on a small number of edges. Such a family allows one to add and remove vertices with only few edge updates, making them useful in dynamic settings such as for datacenter network topologies and for the design of distributed algorithms for self-healing expanders. Dinitz et al. constructed explicit expanding-expanders based on the Bilu-Linial construction of spectral expanders (Combinatorica 2006). The construction of expanding expanders, however, ends up being of edge expanders, thus, an open problem raised by Dinitz et al. is to construct spectral expanding expanders (SEE).

In this work, we resolve this question by constructing SEE with spectral expansion which, like the Bilu-Linial construction, is optimal up to a poly-logarithmic factor, and the number of edge updates is optimal up to a constant. We further give a simple proof for the existence of SEE that are close to Ramanujan up to a small additive term. As in the work of Dinitz et al., our construction is based on interpolating between a graph and its lift. However, to establish spectral expansion, we carefully weigh the interpolated graphs, dubbed partial lifts, in a way that enables us to conduct a delicate analysis of their spectrum. In particular, at a crucial point in the analysis, we consider the eigenvectors structure of the partial lifts.
        
        </div>

        <div class='tr-article-summary'>
        
          
          Dinitz, Schapira, and Valadarsky (Algorithmica 2017) introduced the intriguing notion of expanding expanders -- a family of expander graphs with the property that every two consecutive graphs in the family differ only on a small number of edges. Such a family allows one to add and remove vertices with only few edge updates, making them useful in dynamic settings such as for datacenter network topologies and for the design of distributed algorithms for self-healing expanders. Dinitz et al. constructed explicit expanding-expanders based on the Bilu-Linial construction of spectral expanders (Combinatorica 2006). The construction of expanding expanders, however, ends up being of edge expanders, thus, an open problem raised by Dinitz et al. is to construct spectral expanding expanders (SEE).

In this work, we resolve this question by constructing SEE with spectral expansion which, like the Bilu-Linial construction, is optimal up to a poly-logarithmic factor, and the number of edge updates is optimal up to a constant. We further give a simple proof for the existence of SEE that are close to Ramanujan up to a small additive term. As in the work of Dinitz et al., our construction is based on interpolating between a graph and its lift. However, to establish spectral expansion, we carefully weigh the interpolated graphs, dubbed partial lifts, in a way that enables us to conduct a delicate analysis of their spectrum. In particular, at a crucial point in the analysis, we consider the eigenvectors structure of the partial lifts.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-13T18:30:56Z">Sunday, November 13 2022, 18:30</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
