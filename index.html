<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>
  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.4 (2022-04-12) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <link rel='stylesheet' type='text/css' href='css/font-awesome.css'>
  <link rel='stylesheet' type='text/css' href='css/blank.css'>
</head>
<body>
  <div id='navwrap'>
    <div id='nav'>
      <p>
        Last Update
      </p>
      <p class='small'>
        
          <time class='timeago' datetime="2022-10-03T13:23:14Z">Monday, October 03 2022, 13:23</time>
        
      </p>

      <p>Feeds</p>
      <ul class='subscriptions small' >
      
        <li>
          <a href='http://arxiv.org/rss/cs.CC'><img src='i/feed.png'></a>
          <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
          
        </li>
      
        <li>
          <a href='http://arxiv.org/rss/cs.CG'><img src='i/feed.png'></a>
          <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
          
        </li>
      
        <li>
          <a href='http://arxiv.org/rss/cs.DS'><img src='i/feed.png'></a>
          <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
          
        </li>
      
        <li>
          <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='i/feed.png'></a>
          <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
          
        </li>
      
        <li>
          <a href='https://adamsheffer.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
          
        </li>
      
        <li>
          <a href='https://adamdsmith.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
          
        </li>
      
        <li>
          <a href='https://polylogblog.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
          
        </li>
      
        <li>
          <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='i/feed.png'></a>
          <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
          
        </li>
      
        <li>
          <a href='http://www.argmin.net/feed.xml'><img src='i/feed.png'></a>
          <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
          
        </li>
      
        <li>
          <a href='http://bit-player.org/feed/atom/'><img src='i/feed.png'></a>
          <a href='http://bit-player.org'>bit-player</a>
          
        </li>
      
        <li>
          <a href='https://cstheory-jobs.org/feed/'><img src='i/feed.png'></a>
          <a href='https://cstheory-jobs.org'>CCI: jobs</a>
          
        </li>
      
        <li>
          <a href='https://cstheory-events.org/feed/'><img src='i/feed.png'></a>
          <a href='https://cstheory-events.org'>CS Theory Events</a>
          
        </li>
      
        <li>
          <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='i/feed.png'></a>
          <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
          
        </li>
      
        <li>
          <a href='https://11011110.github.io/blog/feed.xml'><img src='i/feed.png'></a>
          <a href='https://11011110.github.io/blog/'>David Eppstein</a>
          
        </li>
      
        <li>
          <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='i/feed.png'></a>
          <a href='https://daveagp.wordpress.com'>David Pritchard</a>
          
        </li>
      
        <li>
          <a href='https://decentdescent.org/feed.xml'><img src='i/feed.png'></a>
          <a href='https://decentdescent.org/'>Decent Descent</a>
          
        </li>
      
        <li>
          <a href='https://decentralizedthoughts.github.io/feed'><img src='i/feed.png'></a>
          <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
          
        </li>
      
        <li>
          <a href='https://differentialprivacy.org/feed.xml'><img src='i/feed.png'></a>
          <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
          
        </li>
      
        <li>
          <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='i/feed.png'></a>
          <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
          
        </li>
      
        <li>
          <a href='https://emanueleviola.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
          
        </li>
      
        <li>
          <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='i/feed.png'></a>
          <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
          
        </li>
      
        <li>
          <a href='https://dstheory.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
          
        </li>
      
        <li>
          <a href='https://francisbach.com/feed/'><img src='i/feed.png'></a>
          <a href='https://francisbach.com'>Francis Bach</a>
          
        </li>
      
        <li>
          <a href='https://gilkalai.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
          
        </li>
      
        <li>
          <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='i/feed.png'></a>
          <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
          
        </li>
      
        <li>
          <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='i/feed.png'></a>
          <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
          
        </li>
      
        <li>
          <a href='https://gradientscience.org/feed.xml'><img src='i/feed.png'></a>
          <a href='https://gradientscience.org/'>Gradient Science</a>
          
        </li>
      
        <li>
          <a href='http://grigory.us/blog/feed.xml'><img src='i/feed.png'></a>
          <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
          
        </li>
      
        <li>
          <a href='https://tcsmath.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
          
        </li>
      
        <li>
          <a href='https://kamathematics.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
          
        </li>
      
        <li>
          <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='i/feed.png'></a>
          <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
          
        </li>
      
        <li>
          <a href='https://lucatrevisan.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
          
        </li>
      
        <li>
          <a href='https://mittheory.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
          
        </li>
      
        <li>
          <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='i/feed.png'></a>
          <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
          
        </li>
      
        <li>
          <a href='http://blog.mrtz.org/feed.xml'><img src='i/feed.png'></a>
          <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
          
        </li>
      
        <li>
          <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='i/feed.png'></a>
          <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
          
        </li>
      
        <li>
          <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
          
        </li>
      
        <li>
          <a href='http://www.solipsistslog.com/feed/'><img src='i/feed.png'></a>
          <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
          
        </li>
      
        <li>
          <a href='http://www.offconvex.org/feed.xml'><img src='i/feed.png'></a>
          <a href='http://offconvex.github.io/'>Off the Convex Path</a>
          
        </li>
      
        <li>
          <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='i/feed.png'></a>
          <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
          
        </li>
      
        <li>
          <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='i/feed.png'></a>
          <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
          
        </li>
      
        <li>
          <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='i/feed.png'></a>
          <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
          
        </li>
      
        <li>
          <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='i/feed.png'></a>
          <a href='https://blogs.princeton.edu/imabandit'>Sébastien Bubeck</a>
          
        </li>
      
        <li>
          <a href='https://scottaaronson.blog/?feed=atom'><img src='i/feed.png'></a>
          <a href='https://scottaaronson.blog'>Scott Aaronson</a>
          
        </li>
      
        <li>
          <a href='https://blog.simons.berkeley.edu/feed/'><img src='i/feed.png'></a>
          <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
          
        </li>
      
        <li>
          <a href='https://tcsplus.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
          
        </li>
      
        <li>
          <a href='https://toc4fairness.org/feed/'><img src='i/feed.png'></a>
          <a href='https://toc4fairness.org'>TOC for Fairness</a>
          
        </li>
      
        <li>
          <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='i/feed.png'></a>
          <a href='http://blog.geomblog.org/'>The Geomblog</a>
          
        </li>
      
        <li>
          <a href='https://www.let-all.com/blog/feed/'><img src='i/feed.png'></a>
          <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
          
        </li>
      
        <li>
          <a href='https://theorydish.blog/feed/'><img src='i/feed.png'></a>
          <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
          
        </li>
      
        <li>
          <a href='https://thmatters.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://thmatters.wordpress.com'>Theory Matters</a>
          
        </li>
      
        <li>
          <a href='https://mycqstate.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
          
        </li>
      
        <li>
          <a href='https://agtb.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
          
        </li>
      
        <li>
          <a href='https://windowsontheory.org/feed/'><img src='i/feed.png'></a>
          <a href='https://windowsontheory.org'>Windows on Theory</a>
          
        </li>
      
      </ul>

      <p class='small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
      <p class='small'>Subscribe to the <a href="atom.xml">Atom feed</a> or <a href="rss20.xml">RSS feed</a> to stay up to date.</p>
      <p class='small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
      <p class='small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
      <p class='small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
    </div>
  </div>

  <div id='opts'>
    <div style='width: 100%; text-align: right;'>
    <img src='i/view-headlines.png' id='show-headlines' title='Show Headlines Only' width='24' height='24'>
    <img src='i/view-snippets.png' id='show-snippets' title='Show Snippets' width='24' height='24'>
    <img src='i/view-standard.png' id='show-fulltext' title='Show Full Text' width='24' height='24'>
    </div>
  </div>

  <h1>
    Theory of Computing Report
  </h1>

  <div id="articles">
    
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Monday, October 03
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.15149'>Pure-Circuit: Strong Inapproximability for PPAD</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Argyrios Deligkas, John Fearnley, Alexandros Hollender, Themistoklis Melissourgos</p><p>The current state-of-the-art methods for showing inapproximability in PPAD
arise from the $\varepsilon$-Generalized-Circuit ($\varepsilon$-GCircuit)
problem. Rubinstein (2018) showed that there exists a small unknown constant
$\varepsilon$ for which $\varepsilon$-GCircuit is PPAD-hard, and subsequent
work has shown hardness results for other problems in PPAD by using
$\varepsilon$-GCircuit as an intermediate problem.
</p>
<p>We introduce Pure-Circuit, a new intermediate problem for PPAD, which can be
thought of as $\varepsilon$-GCircuit pushed to the limit as $\varepsilon
\rightarrow 1$, and we show that the problem is PPAD-complete. We then prove
that $\varepsilon$-GCircuit is PPAD-hard for all $\varepsilon &lt; 0.1$ by a
reduction from Pure-Circuit, and thus strengthen all prior work that has used
GCircuit as an intermediate problem from the existential-constant regime to the
large-constant regime.
</p>
<p>We show that stronger inapproximability results can be derived by reducing
directly from Pure-Circuit. In particular, we prove tight inapproximability
results for computing $\varepsilon$-well-supported Nash equilibria in
two-action polymatrix games, as well as for finding approximate equilibria in
threshold games.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Deligkas_A/0/1/0/all/0/1">Argyrios Deligkas</a>, <a href="http://arxiv.org/find/cs/1/au:+Fearnley_J/0/1/0/all/0/1">John Fearnley</a>, <a href="http://arxiv.org/find/cs/1/au:+Hollender_A/0/1/0/all/0/1">Alexandros Hollender</a>, <a href="http://arxiv.org/find/cs/1/au:+Melissourgos_T/0/1/0/all/0/1">Themistoklis Melissourgos</a></p><p>The current state-of-the-art methods for showing inapproximability in PPAD
arise from the $\varepsilon$-Generalized-Circuit ($\varepsilon$-GCircuit)
problem. Rubinstein (2018) showed that there exists a small unknown constant
$\varepsilon$ for which $\varepsilon$-GCircuit is PPAD-hard, and subsequent
work has shown hardness results for other problems in PPAD by using
$\varepsilon$-GCircuit as an intermediate problem.
</p>
<p>We introduce Pure-Circuit, a new intermediate problem for PPAD, which can be
thought of as $\varepsilon$-GCircuit pushed to the limit as $\varepsilon
\rightarrow 1$, and we show that the problem is PPAD-complete. We then prove
that $\varepsilon$-GCircuit is PPAD-hard for all $\varepsilon &lt; 0.1$ by a
reduction from Pure-Circuit, and thus strengthen all prior work that has used
GCircuit as an intermediate problem from the existential-constant regime to the
large-constant regime.
</p>
<p>We show that stronger inapproximability results can be derived by reducing
directly from Pure-Circuit. In particular, we prove tight inapproximability
results for computing $\varepsilon$-well-supported Nash equilibria in
two-action polymatrix games, as well as for finding approximate equilibria in
threshold games.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-03T00:30:00Z">Monday, October 03 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.15151'>Tight Inapproximability for Graphical Games</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Argyrios Deligkas, John Fearnley, Alexandros Hollender, Themistoklis Melissourgos</p><p>We provide a complete characterization for the computational complexity of
finding approximate equilibria in two-action graphical games. We consider the
two most well-studied approximation notions: $\varepsilon$-Nash equilibria
($\varepsilon$-NE) and $\varepsilon$-well-supported Nash equilibria
($\varepsilon$-WSNE), where $\varepsilon \in [0,1]$. We prove that computing an
$\varepsilon$-NE is PPAD-complete for any constant $\varepsilon &lt; 1/2$, while a
very simple algorithm (namely, letting all players mix uniformly between their
two actions) yields a $1/2$-NE. On the other hand, we show that computing an
$\varepsilon$-WSNE is PPAD-complete for any constant $\varepsilon &lt; 1$, while a
$1$-WSNE is trivial to achieve, because any strategy profile is a $1$-WSNE. All
of our lower bounds immediately also apply to graphical games with more than
two actions per player.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Deligkas_A/0/1/0/all/0/1">Argyrios Deligkas</a>, <a href="http://arxiv.org/find/cs/1/au:+Fearnley_J/0/1/0/all/0/1">John Fearnley</a>, <a href="http://arxiv.org/find/cs/1/au:+Hollender_A/0/1/0/all/0/1">Alexandros Hollender</a>, <a href="http://arxiv.org/find/cs/1/au:+Melissourgos_T/0/1/0/all/0/1">Themistoklis Melissourgos</a></p><p>We provide a complete characterization for the computational complexity of
finding approximate equilibria in two-action graphical games. We consider the
two most well-studied approximation notions: $\varepsilon$-Nash equilibria
($\varepsilon$-NE) and $\varepsilon$-well-supported Nash equilibria
($\varepsilon$-WSNE), where $\varepsilon \in [0,1]$. We prove that computing an
$\varepsilon$-NE is PPAD-complete for any constant $\varepsilon &lt; 1/2$, while a
very simple algorithm (namely, letting all players mix uniformly between their
two actions) yields a $1/2$-NE. On the other hand, we show that computing an
$\varepsilon$-WSNE is PPAD-complete for any constant $\varepsilon &lt; 1$, while a
$1$-WSNE is trivial to achieve, because any strategy profile is a $1$-WSNE. All
of our lower bounds immediately also apply to graphical games with more than
two actions per player.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-03T00:30:00Z">Monday, October 03 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.15275'>A Multivariate Complexity Analysis of Qualitative Reasoning Problems</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Leif Eriksson, Victor Lagerkvist</p><p>Qualitative reasoning is an important subfield of artificial intelligence
where one describes relationships with qualitative, rather than numerical,
relations. Many such reasoning tasks, e.g., Allen's interval algebra, can be
solved in $2^{O(n \cdot \log n)}$ time, but single-exponential running times
$2^{O(n)}$ are currently far out of reach. In this paper we consider
single-exponential algorithms via a multivariate analysis consisting of a
fine-grained parameter $n$ (e.g., the number of variables) and a coarse-grained
parameter $k$ expected to be relatively small. We introduce the classes FPE and
XE of problems solvable in $f(k) \cdot 2^{O(n)}$, respectively $f(k)^n$, time,
and prove several fundamental properties of these classes. We proceed by
studying temporal reasoning problems and (1) show that the Partially Ordered
Time problem of effective width $k$ is solvable in $16^{kn}$ time and is thus
included in XE, and (2) that the network consistency problem for Allen's
interval algebra with no interval overlapping with more than $k$ others is
solvable in $(2nk)^{2k} \cdot 2^{n}$ time and is included in FPE. Our
multivariate approach is in no way limited to these to specific problems and
may be a generally useful approach for obtaining single-exponential algorithms.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Eriksson_L/0/1/0/all/0/1">Leif Eriksson</a>, <a href="http://arxiv.org/find/cs/1/au:+Lagerkvist_V/0/1/0/all/0/1">Victor Lagerkvist</a></p><p>Qualitative reasoning is an important subfield of artificial intelligence
where one describes relationships with qualitative, rather than numerical,
relations. Many such reasoning tasks, e.g., Allen's interval algebra, can be
solved in $2^{O(n \cdot \log n)}$ time, but single-exponential running times
$2^{O(n)}$ are currently far out of reach. In this paper we consider
single-exponential algorithms via a multivariate analysis consisting of a
fine-grained parameter $n$ (e.g., the number of variables) and a coarse-grained
parameter $k$ expected to be relatively small. We introduce the classes FPE and
XE of problems solvable in $f(k) \cdot 2^{O(n)}$, respectively $f(k)^n$, time,
and prove several fundamental properties of these classes. We proceed by
studying temporal reasoning problems and (1) show that the Partially Ordered
Time problem of effective width $k$ is solvable in $16^{kn}$ time and is thus
included in XE, and (2) that the network consistency problem for Allen's
interval algebra with no interval overlapping with more than $k$ others is
solvable in $(2nk)^{2k} \cdot 2^{n}$ time and is included in FPE. Our
multivariate approach is in no way limited to these to specific problems and
may be a generally useful approach for obtaining single-exponential algorithms.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-03T00:30:00Z">Monday, October 03 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.15396'>Broadening the Complexity-theoretic Analysis of Manipulative Attacks in Group Identification</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Emil Junker</p><p>In the Group Identification problem, we are given a set of individuals and
are asked to identify a socially qualified subset among them. Each individual
in the set has an opinion about who should be considered socially qualified.
There are several different rules that can be used to determine the socially
qualified subset based on these mutual opinions. In a manipulative attack, an
outsider attempts to exploit the way the used rule works, with the goal of
changing the outcome of the selection process to their liking.
</p>
<p>In recent years, the complexity of group control and bribery based
manipulative attacks in Group Identification has been the subject of intense
research. However, the picture is far from complete, and there remain many open
questions related to what exactly makes certain problems hard, or certain rules
immune to some attacks.
</p>
<p>Supplementing previous results, we examine the complexity of group
microbribery on so-called protective problem instances; that is, instances
where all individuals from the constructive target set are already socially
qualified initially. In addition, we study a relaxed variant of group control
by deleting individuals for the consent rules, the consensus-start-respecting
rule, and the liberal-start-respecting rule. Based on existing literature, we
also formalize three new social rules of the iterative consensus type, and we
provide a comprehensive complexity-theoretic analysis of group control and
bribery problems for these rules.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Junker_E/0/1/0/all/0/1">Emil Junker</a></p><p>In the Group Identification problem, we are given a set of individuals and
are asked to identify a socially qualified subset among them. Each individual
in the set has an opinion about who should be considered socially qualified.
There are several different rules that can be used to determine the socially
qualified subset based on these mutual opinions. In a manipulative attack, an
outsider attempts to exploit the way the used rule works, with the goal of
changing the outcome of the selection process to their liking.
</p>
<p>In recent years, the complexity of group control and bribery based
manipulative attacks in Group Identification has been the subject of intense
research. However, the picture is far from complete, and there remain many open
questions related to what exactly makes certain problems hard, or certain rules
immune to some attacks.
</p>
<p>Supplementing previous results, we examine the complexity of group
microbribery on so-called protective problem instances; that is, instances
where all individuals from the constructive target set are already socially
qualified initially. In addition, we study a relaxed variant of group control
by deleting individuals for the consent rules, the consensus-start-respecting
rule, and the liberal-start-respecting rule. Based on existing literature, we
also formalize three new social rules of the iterative consensus type, and we
provide a comprehensive complexity-theoretic analysis of group control and
bribery problems for these rules.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-03T00:30:00Z">Monday, October 03 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.15410'>P vs NP</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Jian-Gang Tang</p><p>In this paper, we discuss the $\mathcal{NP}$ problem using the Henkin's
Theory and the Herbrand Theory in the first-order logic, and prove that
$\mathcal{P}$ is a proper subset of $\mathcal{NP}$.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jian-Gang Tang</a></p><p>In this paper, we discuss the $\mathcal{NP}$ problem using the Henkin's
Theory and the Herbrand Theory in the first-order logic, and prove that
$\mathcal{P}$ is a proper subset of $\mathcal{NP}$.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-03T00:30:00Z">Monday, October 03 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.15574'>An improved algorithm for Generalized \v{C}ech complex construction</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Jie Chu, Mikael Vejdemo-Johansson, Ping Ji</p><p>In this paper, we present an algorithm that computes the generalized \v{C}ech
complex for a finite set of disks where each may have a different radius in 2D
space. An extension of this algorithm is also proposed for a set of balls in 3D
space with different radius.
</p>
<p>To compute a $k$-simplex, we leverage the computation performed in the round
of $(k-1)$-simplices such that we can reduce the number of potential candidates
to verify to improve the efficiency. An efficient verification method is
proposed to confirm if a $k$-simplex can be constructed on the basis of the
$(k-1)$-simplices. We demonstrate the performance with a comparison to some
closely related algorithms.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chu_J/0/1/0/all/0/1">Jie Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vejdemo_Johansson_M/0/1/0/all/0/1">Mikael Vejdemo-Johansson</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1">Ping Ji</a></p><p>In this paper, we present an algorithm that computes the generalized \v{C}ech
complex for a finite set of disks where each may have a different radius in 2D
space. An extension of this algorithm is also proposed for a set of balls in 3D
space with different radius.
</p>
<p>To compute a $k$-simplex, we leverage the computation performed in the round
of $(k-1)$-simplices such that we can reduce the number of potential candidates
to verify to improve the efficiency. An efficient verification method is
proposed to confirm if a $k$-simplex can be constructed on the basis of the
$(k-1)$-simplices. We demonstrate the performance with a comparison to some
closely related algorithms.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-03T00:30:00Z">Monday, October 03 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.15219'>Optimal Query Complexities for Dynamic Trace Estimation</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: David P. Woodruff, Fred Zhang, Qiuyi Zhang</p><p>We consider the problem of minimizing the number of matrix-vector queries
needed for accurate trace estimation in the dynamic setting where our
underlying matrix is changing slowly, such as during an optimization process.
Specifically, for any $m$ matrices $A_1,...,A_m$ with consecutive differences
bounded in Schatten-$1$ norm by $\alpha$, we provide a novel binary tree
summation procedure that simultaneously estimates all $m$ traces up to
$\epsilon$ error with $\delta$ failure probability with an optimal query
complexity of $\widetilde{O}\left(m \alpha\sqrt{\log(1/\delta)}/\epsilon +
m\log(1/\delta)\right)$, improving the dependence on both $\alpha$ and $\delta$
from Dharangutte and Musco (NeurIPS, 2021). Our procedure works without
additional norm bounds on $A_i$ and can be generalized to a bound for the
$p$-th Schatten norm for $p \in [1,2]$, giving a complexity of
$\widetilde{O}\left(m \alpha\left(\sqrt{\log(1/\delta)}/\epsilon\right)^p +m
\log(1/\delta)\right)$.
</p>
<p>By using novel reductions to communication complexity and
information-theoretic analyses of Gaussian matrices, we provide matching lower
bounds for static and dynamic trace estimation in all relevant parameters,
including the failure probability. Our lower bounds (1) give the first tight
bounds for Hutchinson's estimator in the matrix-vector product model with
Frobenius norm error even in the static setting, and (2) are the first
unconditional lower bounds for dynamic trace estimation, resolving open
questions of prior work.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1">David P. Woodruff</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fred Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qiuyi Zhang</a></p><p>We consider the problem of minimizing the number of matrix-vector queries
needed for accurate trace estimation in the dynamic setting where our
underlying matrix is changing slowly, such as during an optimization process.
Specifically, for any $m$ matrices $A_1,...,A_m$ with consecutive differences
bounded in Schatten-$1$ norm by $\alpha$, we provide a novel binary tree
summation procedure that simultaneously estimates all $m$ traces up to
$\epsilon$ error with $\delta$ failure probability with an optimal query
complexity of $\widetilde{O}\left(m \alpha\sqrt{\log(1/\delta)}/\epsilon +
m\log(1/\delta)\right)$, improving the dependence on both $\alpha$ and $\delta$
from Dharangutte and Musco (NeurIPS, 2021). Our procedure works without
additional norm bounds on $A_i$ and can be generalized to a bound for the
$p$-th Schatten norm for $p \in [1,2]$, giving a complexity of
$\widetilde{O}\left(m \alpha\left(\sqrt{\log(1/\delta)}/\epsilon\right)^p +m
\log(1/\delta)\right)$.
</p>
<p>By using novel reductions to communication complexity and
information-theoretic analyses of Gaussian matrices, we provide matching lower
bounds for static and dynamic trace estimation in all relevant parameters,
including the failure probability. Our lower bounds (1) give the first tight
bounds for Hutchinson's estimator in the matrix-vector product model with
Frobenius norm error even in the static setting, and (2) are the first
unconditional lower bounds for dynamic trace estimation, resolving open
questions of prior work.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-03T00:30:00Z">Monday, October 03 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.15220'>Assortment Optimization Under the Multivariate MNL Model</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Xin Chen, Jiachun Li, Menglong Li, Tiancheng Zhao, Yuan Zhou</p><p>We study an assortment optimization problem under a multi-purchase choice
model in which customers choose a bundle of up to one product from each of two
product categories. Different bundles have different utilities and the bundle
price is the summation of the prices of products in it. For the uncapacitated
setting where any set of products can be offered, we prove that this problem is
strongly NP-hard. We show that an adjusted-revenue-ordered assortment provides
a 1/2-approximation. Furthermore, we develop an approximation framework based
on a linear programming relaxation of the problem and obtain a
0.74-approximation algorithm. This approximation ratio almost matches the
integrality gap of the linear program, which is proven to be at most 0.75. For
the capacitated setting, we prove that there does not exist a constant-factor
approximation algorithm assuming the Exponential Time Hypothesis. The same
hardness result holds for settings with general bundle prices or more than two
categories. Finally, we conduct numerical experiments on randomly generated
problem instances. The average approximation ratios of our algorithms are over
99%.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiachun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Menglong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tiancheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuan Zhou</a></p><p>We study an assortment optimization problem under a multi-purchase choice
model in which customers choose a bundle of up to one product from each of two
product categories. Different bundles have different utilities and the bundle
price is the summation of the prices of products in it. For the uncapacitated
setting where any set of products can be offered, we prove that this problem is
strongly NP-hard. We show that an adjusted-revenue-ordered assortment provides
a 1/2-approximation. Furthermore, we develop an approximation framework based
on a linear programming relaxation of the problem and obtain a
0.74-approximation algorithm. This approximation ratio almost matches the
integrality gap of the linear program, which is proven to be at most 0.75. For
the capacitated setting, we prove that there does not exist a constant-factor
approximation algorithm assuming the Exponential Time Hypothesis. The same
hardness result holds for settings with general bundle prices or more than two
categories. Finally, we conduct numerical experiments on randomly generated
problem instances. The average approximation ratios of our algorithms are over
99%.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-03T00:30:00Z">Monday, October 03 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.15300'>Deterministic Performance Guarantees for Bidirectional BFS on Real-World Networks</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Thomas Bl&#xe4;sius, Marcus Wilhelm</p><p>A common technique to speed up shortest path queries in graphs is to use a
bidirectional search, i.e., performing a forward search from the start and a
backward search from the destination until a common vertex on a shortest path
is found. In practice, this has a tremendous impact on the performance on some
real-world networks, while it only seems to save a constant factor on other
types of networks. Even though finding shortest paths is a ubiquitous problem,
there are only few studies attempting to understand the apparently asymptotic
speedups on some networks, using average case analysis on certain models for
real-world networks.
</p>
<p>In this paper we give a new perspective on this, by analyzing deterministic
properties that permit theoretical analysis and that can easily be checked on
any particular instance. We prove that these parameters imply sublinear running
time for the bidirectional breadth-first search in several regimes, some of
which are tight. Moreover, we perform experiments on a large set of real-world
networks showing that our parameters capture the concept of practical running
time well.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Blasius_T/0/1/0/all/0/1">Thomas Bl&#xe4;sius</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilhelm_M/0/1/0/all/0/1">Marcus Wilhelm</a></p><p>A common technique to speed up shortest path queries in graphs is to use a
bidirectional search, i.e., performing a forward search from the start and a
backward search from the destination until a common vertex on a shortest path
is found. In practice, this has a tremendous impact on the performance on some
real-world networks, while it only seems to save a constant factor on other
types of networks. Even though finding shortest paths is a ubiquitous problem,
there are only few studies attempting to understand the apparently asymptotic
speedups on some networks, using average case analysis on certain models for
real-world networks.
</p>
<p>In this paper we give a new perspective on this, by analyzing deterministic
properties that permit theoretical analysis and that can easily be checked on
any particular instance. We prove that these parameters imply sublinear running
time for the bidirectional breadth-first search in several regimes, some of
which are tight. Moreover, we perform experiments on a large set of real-world
networks showing that our parameters capture the concept of practical running
time well.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-03T00:30:00Z">Monday, October 03 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.15305'>Proportionally Fair Online Allocation of Public Goods with Predictions</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Siddhartha Banerjee, Vasilis Gkatzelis, Safwan Hossain, Billy Jin, Evi Micha, Nisarg Shah</p><p>We design online algorithms for the fair allocation of public goods to a set
of $N$ agents over a sequence of $T$ rounds and focus on improving their
performance using predictions. In the basic model, a public good arrives in
each round, the algorithm learns every agent's value for the good, and must
irrevocably decide the amount of investment in the good without exceeding a
total budget of $B$ across all rounds. The algorithm can utilize (potentially
inaccurate) predictions of each agent's total value for all the goods to
arrive. We measure the performance of the algorithm using a proportional
fairness objective, which informally demands that every group of agents be
rewarded in proportion to its size and the cohesiveness of its preferences.
</p>
<p>In the special case of binary agent preferences and a unit budget, we show
that $O(\log N)$ proportional fairness can be achieved without using any
predictions, and that this is optimal even if perfectly accurate predictions
were available. However, for general preferences and budget no algorithm can
achieve better than $\Theta(T/B)$ proportional fairness without predictions. We
show that algorithms with (reasonably accurate) predictions can do much better,
achieving $\Theta(\log (T/B))$ proportional fairness. We also extend this
result to a general model in which a batch of $L$ public goods arrive in each
round and achieve $O(\log (\min(N,L) \cdot T/B))$ proportional fairness. Our
exact bounds are parametrized as a function of the error in the predictions and
the performance degrades gracefully with increasing errors.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1">Siddhartha Banerjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Gkatzelis_V/0/1/0/all/0/1">Vasilis Gkatzelis</a>, <a href="http://arxiv.org/find/cs/1/au:+Hossain_S/0/1/0/all/0/1">Safwan Hossain</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1">Billy Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Micha_E/0/1/0/all/0/1">Evi Micha</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1">Nisarg Shah</a></p><p>We design online algorithms for the fair allocation of public goods to a set
of $N$ agents over a sequence of $T$ rounds and focus on improving their
performance using predictions. In the basic model, a public good arrives in
each round, the algorithm learns every agent's value for the good, and must
irrevocably decide the amount of investment in the good without exceeding a
total budget of $B$ across all rounds. The algorithm can utilize (potentially
inaccurate) predictions of each agent's total value for all the goods to
arrive. We measure the performance of the algorithm using a proportional
fairness objective, which informally demands that every group of agents be
rewarded in proportion to its size and the cohesiveness of its preferences.
</p>
<p>In the special case of binary agent preferences and a unit budget, we show
that $O(\log N)$ proportional fairness can be achieved without using any
predictions, and that this is optimal even if perfectly accurate predictions
were available. However, for general preferences and budget no algorithm can
achieve better than $\Theta(T/B)$ proportional fairness without predictions. We
show that algorithms with (reasonably accurate) predictions can do much better,
achieving $\Theta(\log (T/B))$ proportional fairness. We also extend this
result to a general model in which a batch of $L$ public goods arrive in each
round and achieve $O(\log (\min(N,L) \cdot T/B))$ proportional fairness. Our
exact bounds are parametrized as a function of the error in the predictions and
the performance degrades gracefully with increasing errors.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-03T00:30:00Z">Monday, October 03 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.15416'>Optimal Efficiency-Envy Trade-Off via Optimal Transport</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Steven Yin, Christian Kroer</p><p>We consider the problem of allocating a distribution of items to $n$
recipients where each recipient has to be allocated a fixed, prespecified
fraction of all items, while ensuring that each recipient does not experience
too much envy. We show that this problem can be formulated as a variant of the
semi-discrete optimal transport (OT) problem, whose solution structure in this
case has a concise representation and a simple geometric interpretation. Unlike
existing literature that treats envy-freeness as a hard constraint, our
formulation allows us to \emph{optimally} trade off efficiency and envy
continuously. Additionally, we study the statistical properties of the space of
our OT based allocation policies by showing a polynomial bound on the number of
samples needed to approximate the optimal solution from samples. Our approach
is suitable for large-scale fair allocation problems such as the blood donation
matching problem, and we show numerically that it performs well on a prior
realistic data simulator.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1">Steven Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kroer_C/0/1/0/all/0/1">Christian Kroer</a></p><p>We consider the problem of allocating a distribution of items to $n$
recipients where each recipient has to be allocated a fixed, prespecified
fraction of all items, while ensuring that each recipient does not experience
too much envy. We show that this problem can be formulated as a variant of the
semi-discrete optimal transport (OT) problem, whose solution structure in this
case has a concise representation and a simple geometric interpretation. Unlike
existing literature that treats envy-freeness as a hard constraint, our
formulation allows us to \emph{optimally} trade off efficiency and envy
continuously. Additionally, we study the statistical properties of the space of
our OT based allocation policies by showing a polynomial bound on the number of
samples needed to approximate the optimal solution from samples. Our approach
is suitable for large-scale fair allocation problems such as the blood donation
matching problem, and we show numerically that it performs well on a prior
realistic data simulator.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-03T00:30:00Z">Monday, October 03 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.15497'>Local dominance unveils clusters in networks</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Fan Shang, Bingsheng Chen, Paul Expert, Linyuan L&#xfc;, Ao Yang, H.Eugene Stanley, Renaud Lambiotte, Tim S.Evans, Ruiqi Li</p><p>Clusters or communities can provide a coarse-grained description of complex
systems at multiple scales, but their detection remains challenging in
practice. Community detection methods often define communities as dense
subgraphs, or subgraphs with few connections in-between, via concepts such as
the cut, conductance, or modularity. Here we consider another perspective built
on the notion of local dominance, where low-degree nodes are assigned to the
basin of influence of high-degree nodes, and design an efficient algorithm
based on local information. Local dominance gives rises to community centers,
and uncovers local hierarchies in the network. Community centers have a larger
degree than their neighbors and are sufficiently distant from other centers.
The strength of our framework is demonstrated on synthesized and empirical
networks with ground-truth community labels. The notion of local dominance and
the associated asymmetric relations between nodes are not restricted to
community detection, and can be utilised in clustering problems, as we
illustrate on networks derived from vector data.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/physics/1/au:+Shang_F/0/1/0/all/0/1">Fan Shang</a>, <a href="http://arxiv.org/find/physics/1/au:+Chen_B/0/1/0/all/0/1">Bingsheng Chen</a>, <a href="http://arxiv.org/find/physics/1/au:+Expert_P/0/1/0/all/0/1">Paul Expert</a>, <a href="http://arxiv.org/find/physics/1/au:+Lu_L/0/1/0/all/0/1">Linyuan L&#xfc;</a>, <a href="http://arxiv.org/find/physics/1/au:+Yang_A/0/1/0/all/0/1">Ao Yang</a>, <a href="http://arxiv.org/find/physics/1/au:+Stanley_H/0/1/0/all/0/1">H.Eugene Stanley</a>, <a href="http://arxiv.org/find/physics/1/au:+Lambiotte_R/0/1/0/all/0/1">Renaud Lambiotte</a>, <a href="http://arxiv.org/find/physics/1/au:+Evans_T/0/1/0/all/0/1">Tim S.Evans</a>, <a href="http://arxiv.org/find/physics/1/au:+Li_R/0/1/0/all/0/1">Ruiqi Li</a></p><p>Clusters or communities can provide a coarse-grained description of complex
systems at multiple scales, but their detection remains challenging in
practice. Community detection methods often define communities as dense
subgraphs, or subgraphs with few connections in-between, via concepts such as
the cut, conductance, or modularity. Here we consider another perspective built
on the notion of local dominance, where low-degree nodes are assigned to the
basin of influence of high-degree nodes, and design an efficient algorithm
based on local information. Local dominance gives rises to community centers,
and uncovers local hierarchies in the network. Community centers have a larger
degree than their neighbors and are sufficiently distant from other centers.
The strength of our framework is demonstrated on synthesized and empirical
networks with ground-truth community labels. The notion of local dominance and
the associated asymmetric relations between nodes are not restricted to
community detection, and can be utilised in clustering problems, as we
illustrate on networks derived from vector data.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-03T00:30:00Z">Monday, October 03 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Sunday, October 02
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://dstheory.wordpress.com/2022/10/02/wednesday-oct-5th-2022-david-woodruff-from-cmu/'>Wednesday Oct 5th 2022 — David Woodruff from CMU</a></h3>
          <p class='item-feed'>from <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          The first Foundations of Data Science virtual talk of the season, and first of a series on recent advances in adversarially robust streaming, will take place on Wednesday, October 5th at 1:00 PM Pacific Time (16:00 Eastern Time, 22:00 Central European Time, 20:00 UTC). David Woodruff from CMU will give us a survey about “AdversariallyContinue reading "Wednesday Oct 5th 2022 — David Woodruff from&#160;CMU"
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p>The first <a rel="noreferrer noopener" href="https://sites.google.com/view/dstheory/home" target="_blank">Foundations of Data Science</a> virtual talk of the season, and first of a series on recent advances in adversarially robust streaming, will take place on <strong>Wednesday, October 5th</strong> at <strong>1:00 PM Pacific Time</strong> (16:00 Eastern Time, 22:00 Central European Time, 20:00 UTC). <a href="http://www.cs.cmu.edu/~dwoodruf/">David Woodruff</a> from<strong> CMU</strong> will give us a survey about “Adversarially Robust Streaming Algorithms.<em>”</em></p>



<p><a href="https://sites.google.com/view/dstheory">Details of the talk (Zoom link) available here.</a></p>



<p><strong>Abstract</strong>:  A streaming algorithm is given a sequence of items and seeks to compute or approximate some function of this sequence using a small amount of memory. A body of work has been developed over the last two decades, resulting in optimal streaming algorithms for a number of problems. I will start by surveying some of these problems. I will then investigate the adversarial robustness of streaming algorithms. An algorithm is considered robust if its performance guarantees hold even if the stream is chosen adaptively by an adversary that observes the outputs of the algorithm along the stream and can react in an online manner. While deterministic streaming algorithms are inherently robust, many central problems do not admit sublinear-space deterministic algorithms; on the other hand, space-efficient randomized algorithms for these problems are generally not adversarially robust. This raises the question of whether there exist efficient adversarially robust (randomized) streaming algorithms for these problems. I will survey work showing for a number of streaming problems, one can obtain algorithms that are adversarially robust with a small overhead in their memory requirements.</p>



<p>&nbsp;The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p>
<p class="authors">By dstheory</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-10-02T10:01:30Z">Sunday, October 02 2022, 10:01</time>
        </div>
      </div>
    </article>
  
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Friday, September 30
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://scottaaronson.blog/?p=6736'>Shorties!</a></h3>
          <p class='item-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          (1) Since I didn&#8217;t blog about this before: huge congratulations to David Deutsch, Charles Bennett, Gilles Brassard, and my former MIT colleague Peter Shor, and separately to Dan Spielman, for their well-deserved Breakthrough Prizes! Their contributions are all so epochal, so universally known to all of us in quantum information and theoretical computer science, that [&#8230;]
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p>(1) Since I didn&#8217;t blog about this before: huge congratulations to David Deutsch, Charles Bennett, Gilles Brassard, and my former MIT colleague Peter Shor, and separately to Dan Spielman, for their well-deserved <a href="https://breakthroughprize.org/News/73">Breakthrough Prizes</a>! Their contributions are all so epochal, so universally known to all of us in quantum information and theoretical computer science, that there&#8217;s little I can write to gild the lily, except to say how much I&#8217;ve learned by interacting with all five of them personally. I did enjoy <a href="https://mobile.twitter.com/plinytheelder_t/status/1573080349512732672">this comment</a> on the Breakthrough Prizes by someone on Twitter: “As long as that loudmouth Scott Aaronson keeps getting ignored, I&#8217;ll be happy.”</p>



<p>(2) My former UT colleague Ila Fiete brought to my attention an <a href="https://ostp-letter.github.io/">important scientists&#8217; petition to the White House</a>.  The context is that the Biden administration has announced <a href="https://www.whitehouse.gov/ostp/news-updates/2022/08/25/ostp-issues-guidance-to-make-federally-funded-research-freely-available-without-delay/">new rules</a> requiring federally-funded research papers to be freely available to the public without delay.  This is <em>extremely</em> welcome&#8212;in fact, I&#8217;ve <a href="https://www.scottaaronson.com/writings/journal.html">advocated</a> such a step since I first became aware of the scourge of predatory journals around 2004.  But the looming danger is that publishers will just respond by leaning more heavily on the &#8220;author pays&#8221; model&#8212;i.e., hitting up authors or their institutions for thousands of dollars in page fees&#8212;and we&#8217;ll go from only the credentialed few being able to read papers that aren&#8217;t on preprint archives or the like, to only the credentialed few being able to publish them.  The petition urges the White House to build, or fund the research community to build, an infrastructure that will make scientific publishing truly open to everyone.  I&#8217;ve signed it, and I hope you&#8217;ll consider signing too.</p>



<p>(3) Bill Gasarch asked me to announce that he, my former MIT colleague Erik Demaine, and Mohammad Hajiaghayi have written a brand-new book entitled <em>Computational Intractability: A Guide to Algorithmic Lower Bounds</em>, and a <a href="https://hardness.mit.edu/">free draft is available online</a>.  It looks excellent, like a <a href="https://en.wikipedia.org/wiki/Computers_and_Intractability">Garey &amp; Johnson</a> for the 21st century.  Bill and his coauthors are looking for feedback.  I was happy to help them by advertising this&#8212;after all, it&#8217;s not as if Bill&#8217;s got his own complexity blog for such things!</p>



<p>(4) Back when Google was still a novelty—maybe 2000 or so—I had my best friend, the now-famous computer security researcher Alex Halderman, over for Rosh Hashanah dinner with my family. Alex and I were talking about how Google evaded the limitations of all the previous decades’ worth of information retrieval systems. One of my relatives, however, misheard “Google” as <a href="https://www.foodnetwork.com/recipes/dave-lieberman/noodle-kugel-recipe-1946564">“kugel”</a> (basically a dense block of noodles held together with egg), and so ended up passing the latter to Alex. “What is this?” Alex asked. Whereupon my uncle deadpanned, “it’s a noodle retrieval system.” Since then, every single Rosh Hashanah dinner, I think about querying the kugel to retrieve the noodles within, and how the desired search result is just the trivial “all of them.”</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T18:32:09Z">Friday, September 30 2022, 18:32</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://11011110.github.io/blog/2022/09/30/linkage.html'>Linkage</a></h3>
          <p class='item-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          The exciting new world of AI prompt injection (\(\mathbb{M}\), via). Promote your business by running a bot that uses other people’s social media post text to prompt a text-writing AI that generates customized responses to those posts. What could go wrong?
        
        </div>

        <div class='item-content item-summary'>
        
          
          <ul>
  <li>
    <p><a href="https://cohost.org/0xabad1dea/post/112175-the-exciting-new-wor">The exciting new world of AI prompt injection</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109010772219100161">\(\mathbb{M}\)</a>,</span> <a href="https://lobste.rs/s/v9skyo/exciting_new_world_ai_prompt_injection">via</a>). Promote your business by running a bot that uses other people’s social media post text to prompt a text-writing AI that generates customized responses to those posts. What could go wrong?</p>
  </li>
  <li>
    <p><a href="https://community.wolfram.com/groups/-/m/t/2617634">Ed Pegg constructs and visualizes Engel’s 38-sided space-filling polyhedron</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109018050225431095">\(\mathbb{M}\)</a>),</span> the most possible for a Voronoi cell of an isohedral Voronoi tessellation, surrounded by 38 copies of itself.</p>
  </li>
  <li>
    <p><a href="https://mastodon.social/@curved_ruler/109015404349191976">Cyclography</a>, an old form of data visualization in which 3d points are visualized as 2d circles, with the third coordinate used as their radius. Sort of an inverse to the lifting transformation in computational geometry, which turns 2d circles into 3d points in order to use point-based algorithms on them.</p>
  </li>
  <li>
    <p><a href="https://terrytao.wordpress.com/2022/09/19/a-counterexample-to-the-periodic-tiling-conjecture/">Terry Tao on his new preprint with Rachel Greenfeld</a>, “<a href="https://arxiv.org/abs/2209.08451">A counterexample to the periodic tiling conjecture</a>” <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109029559029937263">\(\mathbb{M}\)</a>).</span> The <a href="https://en.wikipedia.org/wiki/Gyrobifastigium">Schmitt-Conway-Danzer biprism</a> and <a href="https://en.wikipedia.org/wiki/Socolar%E2%80%93Taylor_tile">Socolar–Taylor tile</a> tile \(\mathbb{R}^3\) and \(\mathbb{R}^2\times{}\)finite only aperiodically. The <a href="https://en.wikipedia.org/wiki/Einstein_problem">einstein problem</a> asks if \(\mathbb{R}^2\) has an aperiodic tile. This work looks at analogous questions for tiling by translation of \(\mathbb{Z}^2\).</p>
  </li>
  <li>
    <p><a href="https://doi.org/10.1090/noti2539">Descriptive combinatorics and distributed algorithms</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109037227806099068">\(\mathbb{M}\)</a>).</span> Nice survey article by Anton Bernshteyn in <em>Notices of the AMS</em> about implications and in some cases equivalences between topological statements about whether certain infinite sets are Borel or measurable, and whether certain corresponding finite computational problems have distributed algorithms with sub-logarithmic round complexity.</p>
  </li>
  <li>
    <p><a href="https://www.libraryassociation.ie/irish-librarians-condemn-publisher-wileys-removal-of-hundreds-of-titles-from-ebook-collections/">Irish librarians protest</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109037822255336737">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=32926378">via</a>) as Wiley suddenly removes over 1300 ebooks from the existing subscription packages of academic libraries, in order to convert them to a fee-per-student individual-textbook subscription model. <a href="https://www.insidehighered.com/news/2022/09/28/publisher-blocks-access-ebooks-students-faculty-scramble">Now also affecting US universities</a>.</p>
  </li>
  <li>
    <p>Do you need another demonstration that the physics of liquids is strange and counterintuitive <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109054291006791921">\(\mathbb{M}\)</a>)?</span> I learned from <a href="https://www.youtube.com/watch?v=Vrl23FOgUck]">this “What’s eating Dan?” video</a> that, if you have the kind of peanut butter that needs mixing, but is too liquid (swimming in extra peanut oil), you can make it thicker by mixing in a little bit of water. The water droplets in the oil make an emulsion that is thicker than either the water or oil would be by themselves. I had occasion to try it recently and it worked! Science strikes again.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@ngons/109037311387589665">Two swirly rhombus tilings of 4-subdivided 20-gons</a>.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/the-new-math-of-wrinkling-patterns-20220922/">The new math of wrinkling</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109063241525281205">\(\mathbb{M}\)</a>).</span> <em>Quanta</em> on the research of Ian Tobasco on the way that crumpling thin surfaces (like paper) can sometimes lead to disordered folds and sometimes lead to regular patterns, like <a href="https://en.wikipedia.org/wiki/Yoshimura_buckling">Yoshimura buckling</a>, depending in part on local curvature. Based on two papers by Tobasco, “<a href="https://doi.org/10.1007/s00205-020-01566-8">Curvature-driven wrinkling of thin elastic shells</a>” (2021, <a href="https://arxiv.org/abs/1906.02153">arXiv:1906.02153</a>) and “<a href="https://doi.org/10.1038/s41567-022-01672-2">Exact solutions for the wrinkle patterns of confined elastic shells</a>” (2022, <a href="https://arxiv.org/abs/2004.02839">arXiv:2004.02839</a>).</p>
  </li>
  <li>
    <p><a href="https://www.wonkette.com/girls-who-code-books-banned"><em>Girls Who Code</em> appears on this year’s list of books banned by US schools</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109067124900840971">\(\mathbb{M}\)</a>).</span> Apparently this happened not directly because the kind of people who ban books want women to be ignorant, but rather because these books appeared on a diversity resource list and the kind of people who ban books oppose diversity (meaning anything that would challenge the white cis male evangelical-Christian point of view) in all forms. Fortunately local protests got the ban rescinded. More <a href="https://boingboing.net/2022/09/26/girls-who-code-book-series-banned-by-a-pennsylvania-school-district.html">on BoingBoing</a> and <a href="https://www.theguardian.com/us-news/2022/sep/26/pennsylvania-book-ban-girls-who-code">in <em>The Guardian</em></a>. The statement in <em>The Guardian</em> from a book-banning spokesperson “This book series has not been banned, and they remain available in our libraries” appears to actually mean that the ban blocked students from reading the books but failed to remove them permanently from the libraries, and that they became available because the ban was rescinded.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Mutilated_chessboard_problem">Mutilated chessboard problem</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109074266481611822">\(\mathbb{M}\)</a>):</span> remove opposite corners from a chessboard and try to cover the rest with dominos. It’s just planar bipartite perfect matching, easy for algorithms. There’s a cute trick for human problem solving that I won’t spoil. And yet, a logical formulation has been a test case for automated reasoning for nearly 60 years, and is provably hard for some systems (especially resolution). How can it be so easy and so hard? New Wikipedia Good Article.</p>
  </li>
  <li>
    <p><a href="https://jamesheathers.medium.com/publication-laundering-95c4888afd21">Publication laundering</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109080279085285756">\(\mathbb{M}\)</a>,</span> <a href="https://retractionwatch.com/2022/09/28/can-you-explain-what-these-1500-papers-are-doing-in-this-journal/">via</a>): James Heathers on how “proceedings journals” that accept whole special-issues without any internal oversight over relevance or quality ease the collaboration among academics desperate for publications, middlemen who sell authorship slots on mass-produced junk, and big publishers hungry for that publication-fee and subscription-fee cash as long as they can point the blame elsewhere.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@jsiehler/109082598903294416">Foxagon: A hexagon with exactly one line of reflective symmetry and one reflex angle</a>. Or maybe more specifically it’s what you get when you glue equilateral triangles onto two adjacent sides of a square. The more specific version tiles the plane; the tiling below hides a <a href="https://en.wikipedia.org/wiki/Snub_square_tiling">snub square tiling</a> but other tilings are possible.</p>

    <p style="text-align:center"><img src="/blog/assets/2022/foxagons.svg" alt="Tiling by foxagons" /></p>
  </li>
  <li>
    <p><a href="https://twitter.com/robinhouston/status/1556407331344244743">The strange Wiki-history of Sethahedra and Chestahedra</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109090046984135256">\(\mathbb{M}\)</a>,</span> <a href="https://aperiodical.com/2022/09/carnival-of-maths-208/">via</a>, <a href="https://alephjamesa.co.uk/posts.php?data=FoMSept22">via2</a>). The Chestahedron is a polyhedron whose faces are four equilateral triangles and three kites of the same area as the triangles. <a href="http://frankchester.com/project/chestahedron/">Frank Chester makes bronze sculptures of it</a>. The Sethahedron is a nonexistent variation with golden-ratio dimensions. If made of paper it will fold along kite diagonals to form ten faces. The promoter of the Sethahedron has been edit-warring to keep their erroneous version in Wikipedia.</p>
  </li>
</ul><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T18:28:00Z">Friday, September 30 2022, 18:28</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14530'>Low-Stabilizer-Complexity Quantum States Are Not Pseudorandom</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Sabee Grewal, Vishnu Iyer, William Kretschmer, Daniel Liang</p><p>We show that quantum states with "low stabilizer complexity" can be
efficiently distinguished from Haar-random. Specifically, given an $n$-qubit
pure state $|\psi\rangle$, we give an efficient algorithm that distinguishes
whether $|\psi\rangle$ is (i) Haar-random or (ii) a state with stabilizer
fidelity at least $\frac{1}{k}$ (i.e., has fidelity at least $\frac{1}{k}$ with
some stabilizer state), promised that one of these is the case. With black-box
access to $|\psi\rangle$, our algorithm uses $O\!\left( k^{12}
\log(1/\delta)\right)$ copies of $|\psi\rangle$ and $O\!\left(n k^{12}
\log(1/\delta)\right)$ time to succeed with probability at least $1-\delta$,
and, with access to a state preparation unitary for $|\psi\rangle$ (and its
inverse), $O\!\left( k^{3} \log(1/\delta)\right)$ queries and $O\!\left(n k^{3}
\log(1/\delta)\right)$ time suffice.
</p>
<p>As a corollary, we prove that $\omega(\log(n))$ $T$-gates are necessary for
any Clifford+$T$ circuit to prepare computationally pseudorandom quantum
states, a first-of-its-kind lower bound.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Grewal_S/0/1/0/all/0/1">Sabee Grewal</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Iyer_V/0/1/0/all/0/1">Vishnu Iyer</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Kretschmer_W/0/1/0/all/0/1">William Kretschmer</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Liang_D/0/1/0/all/0/1">Daniel Liang</a></p><p>We show that quantum states with "low stabilizer complexity" can be
efficiently distinguished from Haar-random. Specifically, given an $n$-qubit
pure state $|\psi\rangle$, we give an efficient algorithm that distinguishes
whether $|\psi\rangle$ is (i) Haar-random or (ii) a state with stabilizer
fidelity at least $\frac{1}{k}$ (i.e., has fidelity at least $\frac{1}{k}$ with
some stabilizer state), promised that one of these is the case. With black-box
access to $|\psi\rangle$, our algorithm uses $O\!\left( k^{12}
\log(1/\delta)\right)$ copies of $|\psi\rangle$ and $O\!\left(n k^{12}
\log(1/\delta)\right)$ time to succeed with probability at least $1-\delta$,
and, with access to a state preparation unitary for $|\psi\rangle$ (and its
inverse), $O\!\left( k^{3} \log(1/\delta)\right)$ queries and $O\!\left(n k^{3}
\log(1/\delta)\right)$ time suffice.
</p>
<p>As a corollary, we prove that $\omega(\log(n))$ $T$-gates are necessary for
any Clifford+$T$ circuit to prepare computationally pseudorandom quantum
states, a first-of-its-kind lower bound.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T00:30:00Z">Friday, September 30 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14914'>Quantum invariants for the graph isomorphism problem</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Hern&#xe1;n I. de la Cruz, Fernando L. Pelayo, Vicente Pascual, Jose J. Paulet, Fernando Cuartero, Luis Llana, Mauro Mezzini</p><p>Graph Isomorphism is such an important problem in computer science, that it
has been widely studied over the last decades. It is well known that it belongs
to NP class, but is not NP-complete. It is thought to be of comparable
difficulty to integer factorisation. The best known proved algorithm to solve
this problem in general, was proposed by L\'aszl\'o Babai and Eugene Luks in
1983.
</p>
<p>Recently, there has been some research in the topic by using quantum
computing, that also leads the present piece of research. In fact, we present a
quantum computing algorithm that defines an invariant over Graph Isomorphism
characterisation. This quantum algorithm is able to distinguish more
non-isomorphic graphs than most of the known invariants so far. The proof of
correctness and some hints illustrating the extent and reason of the
improvement are also included in this paper.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cruz_H/0/1/0/all/0/1">Hern&#xe1;n I. de la Cruz</a>, <a href="http://arxiv.org/find/cs/1/au:+Pelayo_F/0/1/0/all/0/1">Fernando L. Pelayo</a>, <a href="http://arxiv.org/find/cs/1/au:+Pascual_V/0/1/0/all/0/1">Vicente Pascual</a>, <a href="http://arxiv.org/find/cs/1/au:+Paulet_J/0/1/0/all/0/1">Jose J. Paulet</a>, <a href="http://arxiv.org/find/cs/1/au:+Cuartero_F/0/1/0/all/0/1">Fernando Cuartero</a>, <a href="http://arxiv.org/find/cs/1/au:+Llana_L/0/1/0/all/0/1">Luis Llana</a>, <a href="http://arxiv.org/find/cs/1/au:+Mezzini_M/0/1/0/all/0/1">Mauro Mezzini</a></p><p>Graph Isomorphism is such an important problem in computer science, that it
has been widely studied over the last decades. It is well known that it belongs
to NP class, but is not NP-complete. It is thought to be of comparable
difficulty to integer factorisation. The best known proved algorithm to solve
this problem in general, was proposed by L\'aszl\'o Babai and Eugene Luks in
1983.
</p>
<p>Recently, there has been some research in the topic by using quantum
computing, that also leads the present piece of research. In fact, we present a
quantum computing algorithm that defines an invariant over Graph Isomorphism
characterisation. This quantum algorithm is able to distinguish more
non-isomorphic graphs than most of the known invariants so far. The proof of
correctness and some hints illustrating the extent and reason of the
improvement are also included in this paper.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T00:30:00Z">Friday, September 30 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14778'>Batch Normalization Explained</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Randall Balestriero, Richard G. Baraniuk</p><p>A critically important, ubiquitous, and yet poorly understood ingredient in
modern deep networks (DNs) is batch normalization (BN), which centers and
normalizes the feature maps. To date, only limited progress has been made
understanding why BN boosts DN learning and inference performance; work has
focused exclusively on showing that BN smooths a DN's loss landscape. In this
paper, we study BN theoretically from the perspective of function
approximation; we exploit the fact that most of today's state-of-the-art DNs
are continuous piecewise affine (CPA) splines that fit a predictor to the
training data via affine mappings defined over a partition of the input space
(the so-called "linear regions"). {\em We demonstrate that BN is an
unsupervised learning technique that -- independent of the DN's weights or
gradient-based learning -- adapts the geometry of a DN's spline partition to
match the data.} BN provides a "smart initialization" that boosts the
performance of DN learning, because it adapts even a DN initialized with random
weights to align its spline partition with the data. We also show that the
variation of BN statistics between mini-batches introduces a dropout-like
random perturbation to the partition boundaries and hence the decision boundary
for classification problems. This per mini-batch perturbation reduces
overfitting and improves generalization by increasing the margin between the
training samples and the decision boundary.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1">Randall Balestriero</a>, <a href="http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1">Richard G. Baraniuk</a></p><p>A critically important, ubiquitous, and yet poorly understood ingredient in
modern deep networks (DNs) is batch normalization (BN), which centers and
normalizes the feature maps. To date, only limited progress has been made
understanding why BN boosts DN learning and inference performance; work has
focused exclusively on showing that BN smooths a DN's loss landscape. In this
paper, we study BN theoretically from the perspective of function
approximation; we exploit the fact that most of today's state-of-the-art DNs
are continuous piecewise affine (CPA) splines that fit a predictor to the
training data via affine mappings defined over a partition of the input space
(the so-called "linear regions"). {\em We demonstrate that BN is an
unsupervised learning technique that -- independent of the DN's weights or
gradient-based learning -- adapts the geometry of a DN's spline partition to
match the data.} BN provides a "smart initialization" that boosts the
performance of DN learning, because it adapts even a DN initialized with random
weights to align its spline partition with the data. We also show that the
variation of BN statistics between mini-batches introduces a dropout-like
random perturbation to the partition boundaries and hence the decision boundary
for classification problems. This per mini-batch perturbation reduces
overfitting and improves generalization by increasing the margin between the
training samples and the decision boundary.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T00:30:00Z">Friday, September 30 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14804'>Minimum Link Fencing</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Sujoy Bhore, Fabian Klute, Maarten L&#xf6;ffler, Martin N&#xf6;llenburg, Soeren Terziadis, Ana&#xef;s Villedieu</p><p>We study a variant of the geometric multicut problem, where we are given a
set $\mathcal{P}$ of colored and pairwise interior-disjoint polygons in the
plane. The objective is to compute a set of simple closed polygon boundaries
(fences) that separate the polygons in such a way that any two polygons that
are enclosed by the same fence have the same color, and the total number of
links of all fences is minimized. We call this the minimum link fencing (MLF)
problem and consider the natural case of bounded minimum link fencing (BMLF),
where $\mathcal{P}$ contains a polygon $Q$ that is unbounded in all directions
and can be seen as an outer polygon. We show that BMLF is NP-hard in general
and that it is XP-time solvable when each fence contains at most two polygons
and the number of segments per fence is the parameter. Finally, we present an
$O(n \log n)$-time algorithm for the case that the convex hull of $\mathcal{P}
\setminus \{Q\}$ does not intersect $Q$.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bhore_S/0/1/0/all/0/1">Sujoy Bhore</a>, <a href="http://arxiv.org/find/cs/1/au:+Klute_F/0/1/0/all/0/1">Fabian Klute</a>, <a href="http://arxiv.org/find/cs/1/au:+Loffler_M/0/1/0/all/0/1">Maarten L&#xf6;ffler</a>, <a href="http://arxiv.org/find/cs/1/au:+Nollenburg_M/0/1/0/all/0/1">Martin N&#xf6;llenburg</a>, <a href="http://arxiv.org/find/cs/1/au:+Terziadis_S/0/1/0/all/0/1">Soeren Terziadis</a>, <a href="http://arxiv.org/find/cs/1/au:+Villedieu_A/0/1/0/all/0/1">Ana&#xef;s Villedieu</a></p><p>We study a variant of the geometric multicut problem, where we are given a
set $\mathcal{P}$ of colored and pairwise interior-disjoint polygons in the
plane. The objective is to compute a set of simple closed polygon boundaries
(fences) that separate the polygons in such a way that any two polygons that
are enclosed by the same fence have the same color, and the total number of
links of all fences is minimized. We call this the minimum link fencing (MLF)
problem and consider the natural case of bounded minimum link fencing (BMLF),
where $\mathcal{P}$ contains a polygon $Q$ that is unbounded in all directions
and can be seen as an outer polygon. We show that BMLF is NP-hard in general
and that it is XP-time solvable when each fence contains at most two polygons
and the number of segments per fence is the parameter. Finally, we present an
$O(n \log n)$-time algorithm for the case that the convex hull of $\mathcal{P}
\setminus \{Q\}$ does not intersect $Q$.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T00:30:00Z">Friday, September 30 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14993'>Discrete Microlocal Morse Theory</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Adam Brown, Ondrej Draganov</p><p>We establish several results combining discrete Morse theory and microlocal
sheaf theory in the setting of finite posets and simplicial complexes. Our
primary tool is a computationally tractable description of the bounded derived
category of sheaves on a poset with the Alexandrov topology. We prove that each
bounded complex of sheaves on a finite poset admits a unique (up to isomorphism
of complexes) minimal injective resolution, and we provide algorithms for
computing minimal injective resolutions, as well as several useful functors
between derived categories of sheaves. For the constant sheaf on a simplicial
complex, we give asymptotically tight bounds on the complexity of computing the
minimal injective resolution with this algorithm. Our main result is a novel
definition of the discrete microsupport of a bounded complex of sheaves on a
finite poset. We detail several foundational properties of the discrete
microsupport, as well as a microlocal generalization of the discrete
homological Morse theorem and Morse inequalities.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Brown_A/0/1/0/all/0/1">Adam Brown</a>, <a href="http://arxiv.org/find/math/1/au:+Draganov_O/0/1/0/all/0/1">Ondrej Draganov</a></p><p>We establish several results combining discrete Morse theory and microlocal
sheaf theory in the setting of finite posets and simplicial complexes. Our
primary tool is a computationally tractable description of the bounded derived
category of sheaves on a poset with the Alexandrov topology. We prove that each
bounded complex of sheaves on a finite poset admits a unique (up to isomorphism
of complexes) minimal injective resolution, and we provide algorithms for
computing minimal injective resolutions, as well as several useful functors
between derived categories of sheaves. For the constant sheaf on a simplicial
complex, we give asymptotically tight bounds on the complexity of computing the
minimal injective resolution with this algorithm. Our main result is a novel
definition of the discrete microsupport of a bounded complex of sheaves on a
finite poset. We detail several foundational properties of the discrete
microsupport, as well as a microlocal generalization of the discrete
homological Morse theorem and Morse inequalities.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T00:30:00Z">Friday, September 30 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14358'>The minimal canonical form of a tensor network</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Arturo Acuaviva, Visu Makam, Harold Nieuwboer, David P&#xe9;rez-Garc&#xed;a, Friedrich Sittner, Michael Walter, Freek Witteveen</p><p>Tensor networks have a gauge degree of freedom on the virtual degrees of
freedom that are contracted. A canonical form is a choice of fixing this degree
of freedom. For matrix product states, choosing a canonical form is a powerful
tool, both for theoretical and numerical purposes. On the other hand, for
tensor networks in dimension two or greater there is only limited understanding
of the gauge symmetry. Here we introduce a new canonical form, the minimal
canonical form, which applies to projected entangled pair states (PEPS) in any
dimension, and prove a corresponding fundamental theorem. Already for matrix
product states this gives a new canonical form, while in higher dimensions it
is the first rigorous definition of a canonical form valid for any choice of
tensor. We show that two tensors have the same minimal canonical forms if and
only if they are gauge equivalent up to taking limits; moreover, this is the
case if and only if they give the same quantum state for any geometry. In
particular, this implies that the latter problem is decidable - in contrast to
the well-known undecidability for PEPS on grids. We also provide rigorous
algorithms for computing minimal canonical forms. To achieve this we draw on
geometric invariant theory and recent progress in theoretical computer science
in non-commutative group optimization.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Acuaviva_A/0/1/0/all/0/1">Arturo Acuaviva</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Makam_V/0/1/0/all/0/1">Visu Makam</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Nieuwboer_H/0/1/0/all/0/1">Harold Nieuwboer</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Perez_Garcia_D/0/1/0/all/0/1">David P&#xe9;rez-Garc&#xed;a</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Sittner_F/0/1/0/all/0/1">Friedrich Sittner</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Walter_M/0/1/0/all/0/1">Michael Walter</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Witteveen_F/0/1/0/all/0/1">Freek Witteveen</a></p><p>Tensor networks have a gauge degree of freedom on the virtual degrees of
freedom that are contracted. A canonical form is a choice of fixing this degree
of freedom. For matrix product states, choosing a canonical form is a powerful
tool, both for theoretical and numerical purposes. On the other hand, for
tensor networks in dimension two or greater there is only limited understanding
of the gauge symmetry. Here we introduce a new canonical form, the minimal
canonical form, which applies to projected entangled pair states (PEPS) in any
dimension, and prove a corresponding fundamental theorem. Already for matrix
product states this gives a new canonical form, while in higher dimensions it
is the first rigorous definition of a canonical form valid for any choice of
tensor. We show that two tensors have the same minimal canonical forms if and
only if they are gauge equivalent up to taking limits; moreover, this is the
case if and only if they give the same quantum state for any geometry. In
particular, this implies that the latter problem is decidable - in contrast to
the well-known undecidability for PEPS on grids. We also provide rigorous
algorithms for computing minimal canonical forms. To achieve this we draw on
geometric invariant theory and recent progress in theoretical computer science
in non-commutative group optimization.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T00:30:00Z">Friday, September 30 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14368'>Repeated Prophet Inequality with Near-optimal Bounds</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Krishnendu Chatterjee, Mona Mohammadi, Raimundo Saona</p><p>In modern sample-driven Prophet Inequality, an adversary chooses a sequence
of $n$ items with values $v_1, v_2, \ldots, v_n$ to be presented to a decision
maker (DM). The process follows in two phases. In the first phase (sampling
phase), some items, possibly selected at random, are revealed to the DM, but
she can never accept them. In the second phase, the DM is presented with the
other items in a random order and online fashion. For each item, she must make
an irrevocable decision to either accept the item and stop the process or
reject the item forever and proceed to the next item. The goal of the DM is to
maximize the expected value as compared to a Prophet (or offline algorithm)
that has access to all information. In this setting, the sampling phase has no
cost and is not part of the optimization process. However, in many scenarios,
the samples are obtained as part of the decision-making process.
</p>
<p>We model this aspect as a two-phase Prophet Inequality where an adversary
chooses a sequence of $2n$ items with values $v_1, v_2, \ldots, v_{2n}$ and the
items are randomly ordered. Finally, there are two phases of the Prophet
Inequality problem with the first $n$-items and the rest of the items,
respectively. We show that some basic algorithms achieve a ratio of at most
$0.450$. We present an algorithm that achieves a ratio of at least $0.495$.
Finally, we show that for every algorithm the ratio it can achieve is at most
$0.502$. Hence our algorithm is near-optimal.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Chatterjee_K/0/1/0/all/0/1">Krishnendu Chatterjee</a>, <a href="http://arxiv.org/find/math/1/au:+Mohammadi_M/0/1/0/all/0/1">Mona Mohammadi</a>, <a href="http://arxiv.org/find/math/1/au:+Saona_R/0/1/0/all/0/1">Raimundo Saona</a></p><p>In modern sample-driven Prophet Inequality, an adversary chooses a sequence
of $n$ items with values $v_1, v_2, \ldots, v_n$ to be presented to a decision
maker (DM). The process follows in two phases. In the first phase (sampling
phase), some items, possibly selected at random, are revealed to the DM, but
she can never accept them. In the second phase, the DM is presented with the
other items in a random order and online fashion. For each item, she must make
an irrevocable decision to either accept the item and stop the process or
reject the item forever and proceed to the next item. The goal of the DM is to
maximize the expected value as compared to a Prophet (or offline algorithm)
that has access to all information. In this setting, the sampling phase has no
cost and is not part of the optimization process. However, in many scenarios,
the samples are obtained as part of the decision-making process.
</p>
<p>We model this aspect as a two-phase Prophet Inequality where an adversary
chooses a sequence of $2n$ items with values $v_1, v_2, \ldots, v_{2n}$ and the
items are randomly ordered. Finally, there are two phases of the Prophet
Inequality problem with the first $n$-items and the rest of the items,
respectively. We show that some basic algorithms achieve a ratio of at most
$0.450$. We present an algorithm that achieves a ratio of at least $0.495$.
Finally, we show that for every algorithm the ratio it can achieve is at most
$0.502$. Hence our algorithm is near-optimal.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T00:30:00Z">Friday, September 30 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14401'>Shortest Beer Path Queries in Interval Graphs</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Rathish Das, Meng He, Eitan Kondratovsky, J. Ian Munro, Anurag Murty Naredla, Kaiyu Wu</p><p>Our interest is in paths between pairs of vertices that go through at least
one of a subset of the vertices known as beer vertices. Such a path is called a
beer path, and the beer distance between two vertices is the length of the
shortest beer path.
</p>
<p>We show that we can represent unweighted interval graphs using $2n \log n +
O(n) + O(|B|\log n)$ bits where $|B|$ is the number of beer vertices. This data
structure answers beer distance queries in $O(\log^\varepsilon n)$ time for any
constant $\varepsilon &gt; 0$ and shortest beer path queries in
$O(\log^\varepsilon n + d)$ time, where $d$ is the beer distance between the
two nodes. We also show that proper interval graphs may be represented using
$3n + o(n)$ bits to support beer distance queries in $O(f(n)\log n)$ time for
any $f(n) \in \omega(1)$ and shortest beer path queries in $O(d)$ time. All of
these results also have time-space trade-offs.
</p>
<p>Lastly we show that the information theoretic lower bound for beer proper
interval graphs is very close to the space of our structure, namely
$\log(4+2\sqrt{3})n - o(n)$ (or about $ 2.9 n$) bits.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1">Rathish Das</a>, <a href="http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1">Meng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Kondratovsky_E/0/1/0/all/0/1">Eitan Kondratovsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Munro_J/0/1/0/all/0/1">J. Ian Munro</a>, <a href="http://arxiv.org/find/cs/1/au:+Naredla_A/0/1/0/all/0/1">Anurag Murty Naredla</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kaiyu Wu</a></p><p>Our interest is in paths between pairs of vertices that go through at least
one of a subset of the vertices known as beer vertices. Such a path is called a
beer path, and the beer distance between two vertices is the length of the
shortest beer path.
</p>
<p>We show that we can represent unweighted interval graphs using $2n \log n +
O(n) + O(|B|\log n)$ bits where $|B|$ is the number of beer vertices. This data
structure answers beer distance queries in $O(\log^\varepsilon n)$ time for any
constant $\varepsilon &gt; 0$ and shortest beer path queries in
$O(\log^\varepsilon n + d)$ time, where $d$ is the beer distance between the
two nodes. We also show that proper interval graphs may be represented using
$3n + o(n)$ bits to support beer distance queries in $O(f(n)\log n)$ time for
any $f(n) \in \omega(1)$ and shortest beer path queries in $O(d)$ time. All of
these results also have time-space trade-offs.
</p>
<p>Lastly we show that the information theoretic lower bound for beer proper
interval graphs is very close to the space of our structure, namely
$\log(4+2\sqrt{3})n - o(n)$ (or about $ 2.9 n$) bits.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T00:30:00Z">Friday, September 30 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14429'>Efficient parameterized algorithms on graphs with heterogeneous structure: Combining tree-depth and modular-width</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Stefan Kratsch, Florian Nelles</p><p>Many computational problems admit fast algorithms on special inputs, however,
the required properties might be quite restrictive. E.g., many graph problems
can be solved much faster on interval or cographs, or on graphs of small
modular-width or small tree-width, than on general graphs. One challenge is to
attain the greatest generality of such results, i.e., being applicable to less
restrictive input classes, without losing much in terms of running time.
</p>
<p>Building on the use of algebraic expressions we present a clean and robust
way of combining such homogeneous structure into more complex heterogeneous
structure, and we show-case this for the combination of modular-width,
tree-depth, and a natural notion of modular tree-depth. We give a generic
framework for designing efficient parameterized algorithms on the created graph
classes, aimed at getting competitive running times that match the homogeneous
cases. To show the applicability we give efficient parameterized algorithms for
Negative Cycle Detection, Vertex-Weighted All-Pairs Shortest Paths, and
Triangle Counting.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kratsch_S/0/1/0/all/0/1">Stefan Kratsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Nelles_F/0/1/0/all/0/1">Florian Nelles</a></p><p>Many computational problems admit fast algorithms on special inputs, however,
the required properties might be quite restrictive. E.g., many graph problems
can be solved much faster on interval or cographs, or on graphs of small
modular-width or small tree-width, than on general graphs. One challenge is to
attain the greatest generality of such results, i.e., being applicable to less
restrictive input classes, without losing much in terms of running time.
</p>
<p>Building on the use of algebraic expressions we present a clean and robust
way of combining such homogeneous structure into more complex heterogeneous
structure, and we show-case this for the combination of modular-width,
tree-depth, and a natural notion of modular tree-depth. We give a generic
framework for designing efficient parameterized algorithms on the created graph
classes, aimed at getting competitive running times that match the homogeneous
cases. To show the applicability we give efficient parameterized algorithms for
Negative Cycle Detection, Vertex-Weighted All-Pairs Shortest Paths, and
Triangle Counting.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T00:30:00Z">Friday, September 30 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14501'>On Quantum Speedups for Nonconvex Optimization via Quantum Tunneling Walks</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Yizhou Liu, Weijie J. Su, Tongyang Li</p><p>Classical algorithms are often not effective for solving nonconvex
optimization problems where local minima are separated by high barriers. In
this paper, we explore possible quantum speedups for nonconvex optimization by
leveraging the global effect of quantum tunneling. Specifically, we introduce a
quantum algorithm termed the quantum tunneling walk (QTW) and apply it to
nonconvex problems where local minima are approximately global minima. We show
that QTW achieves quantum speedup over classical stochastic gradient descents
(SGD) when the barriers between different local minima are high but thin and
the minima are flat. Based on this observation, we construct a specific
double-well landscape, where classical algorithms cannot efficiently hit one
target well knowing the other well but QTW can when given proper initial states
near the known well. Finally, we corroborate our findings with numerical
experiments.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Liu_Y/0/1/0/all/0/1">Yizhou Liu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Su_W/0/1/0/all/0/1">Weijie J. Su</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Li_T/0/1/0/all/0/1">Tongyang Li</a></p><p>Classical algorithms are often not effective for solving nonconvex
optimization problems where local minima are separated by high barriers. In
this paper, we explore possible quantum speedups for nonconvex optimization by
leveraging the global effect of quantum tunneling. Specifically, we introduce a
quantum algorithm termed the quantum tunneling walk (QTW) and apply it to
nonconvex problems where local minima are approximately global minima. We show
that QTW achieves quantum speedup over classical stochastic gradient descents
(SGD) when the barriers between different local minima are high but thin and
the minima are flat. Based on this observation, we construct a specific
double-well landscape, where classical algorithms cannot efficiently hit one
target well knowing the other well but QTW can when given proper initial states
near the known well. Finally, we corroborate our findings with numerical
experiments.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T00:30:00Z">Friday, September 30 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14516'>Matroid Intersection under Restricted Oracles</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Krist&#xf3;f B&#xe9;rczi, Tam&#xe1;s Kir&#xe1;ly, Yutaro Yamaguchi, Yu Yokoi</p><p>Matroid intersection is one of the most powerful frameworks of matroid theory
that generalizes various problems in combinatorial optimization. Edmonds'
fundamental theorem provides a min-max characterization for the unweighted
setting, while Frank's weight-splitting theorem provides one for the weighted
case. Several efficient algorithms were developed for these problems, all
relying on the usage of one of the conventional oracles for both matroids.
</p>
<p>In the present paper, we consider the tractability of the matroid
intersection problem under restricted oracles. In particular, we focus on the
rank sum, common independence, and maximum rank oracles. We give a strongly
polynomial-time algorithm for weighted matroid intersection under the rank sum
oracle. In the common independence oracle model, we prove that the unweighted
matroid intersection problem is tractable when one of the matroids is a
partition matroid, and that even the weighted case is solvable when one of the
matroids is an elementary split matroid. Finally, we show that the common
independence and maximum rank oracles together are strong enough to realize the
steps of our algorithm under the rank sum oracle.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Berczi_K/0/1/0/all/0/1">Krist&#xf3;f B&#xe9;rczi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kiraly_T/0/1/0/all/0/1">Tam&#xe1;s Kir&#xe1;ly</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_Y/0/1/0/all/0/1">Yutaro Yamaguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yokoi_Y/0/1/0/all/0/1">Yu Yokoi</a></p><p>Matroid intersection is one of the most powerful frameworks of matroid theory
that generalizes various problems in combinatorial optimization. Edmonds'
fundamental theorem provides a min-max characterization for the unweighted
setting, while Frank's weight-splitting theorem provides one for the weighted
case. Several efficient algorithms were developed for these problems, all
relying on the usage of one of the conventional oracles for both matroids.
</p>
<p>In the present paper, we consider the tractability of the matroid
intersection problem under restricted oracles. In particular, we focus on the
rank sum, common independence, and maximum rank oracles. We give a strongly
polynomial-time algorithm for weighted matroid intersection under the rank sum
oracle. In the common independence oracle model, we prove that the unweighted
matroid intersection problem is tractable when one of the matroids is a
partition matroid, and that even the weighted case is solvable when one of the
matroids is an elementary split matroid. Finally, we show that the common
independence and maximum rank oracles together are strong enough to realize the
steps of our algorithm under the rank sum oracle.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T00:30:00Z">Friday, September 30 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14637'>Tensor-Based Sketching Method for the Low-Rank Approximation of Data Streams</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Cuiyu Liu, Chuanfu Xiao, Mingshuo Ding, Chao Yang</p><p>Low-rank approximation in data streams is a fundamental and significant task
in computing science, machine learning and statistics. Multiple streaming
algorithms have emerged over years and most of them are inspired by randomized
algorithms, more specifically, sketching methods. However, many algorithms are
not able to leverage information of data streams and consequently suffer from
low accuracy. Existing data-driven methods improve accuracy but the training
cost is expensive in practice. In this paper, from a subspace perspective, we
propose a tensor-based sketching method for low-rank approximation of data
streams. The proposed algorithm fully exploits the structure of data streams
and obtains quasi-optimal sketching matrices by performing tensor decomposition
on training data. A series of experiments are carried out and show that the
proposed tensor-based method can be more accurate and much faster than the
previous work.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Cuiyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chuanfu Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Mingshuo Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chao Yang</a></p><p>Low-rank approximation in data streams is a fundamental and significant task
in computing science, machine learning and statistics. Multiple streaming
algorithms have emerged over years and most of them are inspired by randomized
algorithms, more specifically, sketching methods. However, many algorithms are
not able to leverage information of data streams and consequently suffer from
low accuracy. Existing data-driven methods improve accuracy but the training
cost is expensive in practice. In this paper, from a subspace perspective, we
propose a tensor-based sketching method for low-rank approximation of data
streams. The proposed algorithm fully exploits the structure of data streams
and obtains quasi-optimal sketching matrices by performing tensor decomposition
on training data. A series of experiments are carried out and show that the
proposed tensor-based method can be more accurate and much faster than the
previous work.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T00:30:00Z">Friday, September 30 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14662'>A dichotomy for succinct representations of homomorphisms</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Christoph Berkholz, Harry Vinall-Smeeth</p><p>The task of computing homomorphisms between two finite relational structures
$\mathcal{A}$ and $\mathcal{B}$ is a well-studied question with numerous
applications. Since the set $\operatorname{Hom}(\mathcal{A},\mathcal{B})$ of
all homomorphisms may be very large having a method of representing it in a
succinct way, especially one which enables us to perform efficient enumeration
and counting, could be extremely useful.
</p>
<p>One simple yet powerful way of doing so is to decompose
$\operatorname{Hom}(\mathcal{A},\mathcal{B})$ using union and Cartesian
product. Such data structures, called d-representations, have been introduced
by Olteanu and Zavodny in the context of database theory. Their results also
imply that if the treewidth of the left-hand side structure $\mathcal{A}$ is
bounded, then a d-representation of polynomial size can be found in polynomial
time. We show that for structures of bounded arity this is optimal: if the
treewidth is unbounded then there are instances where the size of any
d-representation is superpolynomial. Along the way we develop tools for proving
lower bounds on the size of d-representations, in particular we define a notion
of reduction suitable for this context and prove an almost tight lower bound on
the size of d-representations of all $k$-cliques in a graph.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Berkholz_C/0/1/0/all/0/1">Christoph Berkholz</a>, <a href="http://arxiv.org/find/cs/1/au:+Vinall_Smeeth_H/0/1/0/all/0/1">Harry Vinall-Smeeth</a></p><p>The task of computing homomorphisms between two finite relational structures
$\mathcal{A}$ and $\mathcal{B}$ is a well-studied question with numerous
applications. Since the set $\operatorname{Hom}(\mathcal{A},\mathcal{B})$ of
all homomorphisms may be very large having a method of representing it in a
succinct way, especially one which enables us to perform efficient enumeration
and counting, could be extremely useful.
</p>
<p>One simple yet powerful way of doing so is to decompose
$\operatorname{Hom}(\mathcal{A},\mathcal{B})$ using union and Cartesian
product. Such data structures, called d-representations, have been introduced
by Olteanu and Zavodny in the context of database theory. Their results also
imply that if the treewidth of the left-hand side structure $\mathcal{A}$ is
bounded, then a d-representation of polynomial size can be found in polynomial
time. We show that for structures of bounded arity this is optimal: if the
treewidth is unbounded then there are instances where the size of any
d-representation is superpolynomial. Along the way we develop tools for proving
lower bounds on the size of d-representations, in particular we define a notion
of reduction suitable for this context and prove an almost tight lower bound on
the size of d-representations of all $k$-cliques in a graph.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T00:30:00Z">Friday, September 30 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14703'>Lattice Linear Algorithms</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Arya Tanmay Gupta, Sandeep S Kulkarni</p><p>This paper focuses on analyzing and differentiating between lattice linear
problems and algorithms. It introduces a new class of algorithms called
\textit{(fully) lattice linear algorithms}. A property of these algorithms is
that they induce a partial order among all states and form \textit{multiple
lattices}. An initial state locks in one of these lattices. We present a
lattice linear self-stabilizing algorithm for minimal dominating set.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Arya Tanmay Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_S/0/1/0/all/0/1">Sandeep S Kulkarni</a></p><p>This paper focuses on analyzing and differentiating between lattice linear
problems and algorithms. It introduces a new class of algorithms called
\textit{(fully) lattice linear algorithms}. A property of these algorithms is
that they induce a partial order among all states and form \textit{multiple
lattices}. An initial state locks in one of these lattices. We present a
lattice linear self-stabilizing algorithm for minimal dominating set.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T00:30:00Z">Friday, September 30 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14775'>On Constructing Spanners from Random Gaussian Projections</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Sepehr Assadi, Michael Kapralov, Huacheng Yu</p><p>Graph sketching is a powerful paradigm for analyzing graph structure via
linear measurements introduced by Ahn, Guha, and McGregor (SODA'12) that has
since found numerous applications in streaming, distributed computing, and
massively parallel algorithms, among others. Graph sketching has proven to be
quite successful for various problems such as connectivity, minimum spanning
trees, edge or vertex connectivity, and cut or spectral sparsifiers. Yet, the
problem of approximating shortest path metric of a graph, and specifically
computing a spanner, is notably missing from the list of successes. This has
turned the status of this fundamental problem into one of the most longstanding
open questions in this area.
</p>
<p>We present a partial explanation of this lack of success by proving a strong
lower bound for a large family of graph sketching algorithms that encompasses
prior work on spanners and many (but importantly not also all) related
cut-based problems mentioned above. Our lower bound matches the algorithmic
bounds of the recent result of Filtser, Kapralov, and Nouri (SODA'21), up to
lower order terms, for constructing spanners via the same graph sketching
family. This establishes near-optimality of these bounds, at least restricted
to this family of graph sketching techniques, and makes progress on a
conjecture posed in this latter work.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Assadi_S/0/1/0/all/0/1">Sepehr Assadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kapralov_M/0/1/0/all/0/1">Michael Kapralov</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Huacheng Yu</a></p><p>Graph sketching is a powerful paradigm for analyzing graph structure via
linear measurements introduced by Ahn, Guha, and McGregor (SODA'12) that has
since found numerous applications in streaming, distributed computing, and
massively parallel algorithms, among others. Graph sketching has proven to be
quite successful for various problems such as connectivity, minimum spanning
trees, edge or vertex connectivity, and cut or spectral sparsifiers. Yet, the
problem of approximating shortest path metric of a graph, and specifically
computing a spanner, is notably missing from the list of successes. This has
turned the status of this fundamental problem into one of the most longstanding
open questions in this area.
</p>
<p>We present a partial explanation of this lack of success by proving a strong
lower bound for a large family of graph sketching algorithms that encompasses
prior work on spanners and many (but importantly not also all) related
cut-based problems mentioned above. Our lower bound matches the algorithmic
bounds of the recent result of Filtser, Kapralov, and Nouri (SODA'21), up to
lower order terms, for constructing spanners via the same graph sketching
family. This establishes near-optimality of these bounds, at least restricted
to this family of graph sketching techniques, and makes progress on a
conjecture posed in this latter work.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T00:30:00Z">Friday, September 30 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14827'>On the Convergence of AdaGrad on $\R^{d}$: Beyond Convexity, Non-Asymptotic Rate and Acceleration</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Zijian Liu, Ta Duy Nguyen, Alina Ene, Huy L. Nguyen</p><p>Existing analysis of AdaGrad and other adaptive methods for smooth convex
optimization is typically for functions with bounded domain diameter. In
unconstrained problems, previous works guarantee an asymptotic convergence rate
without an explicit constant factor that holds true for the entire function
class. Furthermore, in the stochastic setting, only a modified version of
AdaGrad, different from the one commonly used in practice, in which the latest
gradient is not used to update the stepsize, has been analyzed. Our paper aims
at bridging these gaps and developing a deeper understanding of AdaGrad and its
variants in the standard setting of smooth convex functions as well as the more
general setting of quasar convex functions. First, we demonstrate new
techniques to explicitly bound the convergence rate of the vanilla AdaGrad for
unconstrained problems in both deterministic and stochastic settings. Second,
we propose a variant of AdaGrad for which we can show the convergence of the
last iterate, instead of the average iterate. Finally, we give new accelerated
adaptive algorithms and their convergence guarantee in the deterministic
setting with explicit dependency on the problem parameters, improving upon the
asymptotic rate shown in previous works.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zijian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Ta Duy Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ene_A/0/1/0/all/0/1">Alina Ene</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Huy L. Nguyen</a></p><p>Existing analysis of AdaGrad and other adaptive methods for smooth convex
optimization is typically for functions with bounded domain diameter. In
unconstrained problems, previous works guarantee an asymptotic convergence rate
without an explicit constant factor that holds true for the entire function
class. Furthermore, in the stochastic setting, only a modified version of
AdaGrad, different from the one commonly used in practice, in which the latest
gradient is not used to update the stepsize, has been analyzed. Our paper aims
at bridging these gaps and developing a deeper understanding of AdaGrad and its
variants in the standard setting of smooth convex functions as well as the more
general setting of quasar convex functions. First, we demonstrate new
techniques to explicitly bound the convergence rate of the vanilla AdaGrad for
unconstrained problems in both deterministic and stochastic settings. Second,
we propose a variant of AdaGrad for which we can show the convergence of the
last iterate, instead of the average iterate. Finally, we give new accelerated
adaptive algorithms and their convergence guarantee in the deterministic
setting with explicit dependency on the problem parameters, improving upon the
asymptotic rate shown in previous works.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T00:30:00Z">Friday, September 30 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14853'>META-STORM: Generalized Fully-Adaptive Variance Reduced SGD for Unbounded Functions</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Zijian Liu, Ta Duy Nguyen, Thien Hang Nguyen, Alina Ene, Huy L. Nguyen</p><p>We study the application of variance reduction (VR) techniques to general
non-convex stochastic optimization problems. In this setting, the recent work
STORM [Cutkosky-Orabona '19] overcomes the drawback of having to compute
gradients of "mega-batches" that earlier VR methods rely on. There, STORM
utilizes recursive momentum to achieve the VR effect and is then later made
fully adaptive in STORM+ [Levy et al., '21], where full-adaptivity removes the
requirement for obtaining certain problem-specific parameters such as the
smoothness of the objective and bounds on the variance and norm of the
stochastic gradients in order to set the step size. However, STORM+ crucially
relies on the assumption that the function values are bounded, excluding a
large class of useful functions. In this work, we propose META-STORM, a
generalized framework of STORM+ that removes this bounded function values
assumption while still attaining the optimal convergence rate for non-convex
optimization. META-STORM not only maintains full-adaptivity, removing the need
to obtain problem specific parameters, but also improves the convergence rate's
dependency on the problem parameters. Furthermore, META-STORM can utilize a
large range of parameter settings that subsumes previous methods allowing for
more flexibility in a wider range of settings. Finally, we demonstrate the
effectiveness of META-STORM through experiments across common deep learning
tasks. Our algorithm improves upon the previous work STORM+ and is competitive
with widely used algorithms after the addition of per-coordinate update and
exponential moving average heuristics.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zijian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Ta Duy Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thien Hang Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ene_A/0/1/0/all/0/1">Alina Ene</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Huy L. Nguyen</a></p><p>We study the application of variance reduction (VR) techniques to general
non-convex stochastic optimization problems. In this setting, the recent work
STORM [Cutkosky-Orabona '19] overcomes the drawback of having to compute
gradients of "mega-batches" that earlier VR methods rely on. There, STORM
utilizes recursive momentum to achieve the VR effect and is then later made
fully adaptive in STORM+ [Levy et al., '21], where full-adaptivity removes the
requirement for obtaining certain problem-specific parameters such as the
smoothness of the objective and bounds on the variance and norm of the
stochastic gradients in order to set the step size. However, STORM+ crucially
relies on the assumption that the function values are bounded, excluding a
large class of useful functions. In this work, we propose META-STORM, a
generalized framework of STORM+ that removes this bounded function values
assumption while still attaining the optimal convergence rate for non-convex
optimization. META-STORM not only maintains full-adaptivity, removing the need
to obtain problem specific parameters, but also improves the convergence rate's
dependency on the problem parameters. Furthermore, META-STORM can utilize a
large range of parameter settings that subsumes previous methods allowing for
more flexibility in a wider range of settings. Finally, we demonstrate the
effectiveness of META-STORM through experiments across common deep learning
tasks. Our algorithm improves upon the previous work STORM+ and is competitive
with widely used algorithms after the addition of per-coordinate update and
exponential moving average heuristics.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T00:30:00Z">Friday, September 30 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14878'>Enumerating Regular Languages in Constant Delay</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Antoine Amarilli, Mika&#xeb;l Monet</p><p>We study the task, for a given language $L$, of enumerating the (generally
infinite) sequence of its words, without repetitions, while bounding the delay
between two consecutive words. To allow for constant delay bounds, we assume a
model where we produce each word by editing the preceding word with a small
edit script, rather than writing out the word from scratch. In particular, this
witnesses that the language is orderable, i.e., we can write its words as an
infinite sequence such that the Levenshtein edit distance between any two
consecutive words is bounded by a constant. For instance, $(a+b)^*$ is
orderable (with a variant of the Gray code), but $a^* + b^*$ is not.
</p>
<p>We characterize which regular languages are enumerable in this sense, and
show that this can be decided in PTIME in an input deterministic finite
automaton (DFA) for the language. In fact, we show that, given a DFA $A$
recognizing a language $L$, we can compute in PTIME automata $A_1, \ldots, A_t$
such that $L$ is partitioned as $L(A_1) \sqcup \ldots \sqcup L(A_t)$ and every
$L(A_i)$ is orderable in this sense. Further, we show that this is optimal,
i.e., we cannot partition $L$ into less than $t$ orderable languages.
</p>
<p>In the case where $L$ is orderable, we show that the ordering can be computed
as a constant-delay algorithm: specifically, the algorithm runs in a suitable
pointer machine model, and produces a sequence of constant-length edit scripts
to visit the words of $L$ without repetitions, with constant delay between each
script. In fact, we show that we can achieve this while only allowing the edit
operations push and pop at the beginning and end of the word, which implies
that the word can in fact be maintained in a double-ended queue.
</p>
<p>We also show results on the complexity of a related problem, and study the
model where push-pop edits are only allowed at the end of the word.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Amarilli_A/0/1/0/all/0/1">Antoine Amarilli</a>, <a href="http://arxiv.org/find/cs/1/au:+Monet_M/0/1/0/all/0/1">Mika&#xeb;l Monet</a></p><p>We study the task, for a given language $L$, of enumerating the (generally
infinite) sequence of its words, without repetitions, while bounding the delay
between two consecutive words. To allow for constant delay bounds, we assume a
model where we produce each word by editing the preceding word with a small
edit script, rather than writing out the word from scratch. In particular, this
witnesses that the language is orderable, i.e., we can write its words as an
infinite sequence such that the Levenshtein edit distance between any two
consecutive words is bounded by a constant. For instance, $(a+b)^*$ is
orderable (with a variant of the Gray code), but $a^* + b^*$ is not.
</p>
<p>We characterize which regular languages are enumerable in this sense, and
show that this can be decided in PTIME in an input deterministic finite
automaton (DFA) for the language. In fact, we show that, given a DFA $A$
recognizing a language $L$, we can compute in PTIME automata $A_1, \ldots, A_t$
such that $L$ is partitioned as $L(A_1) \sqcup \ldots \sqcup L(A_t)$ and every
$L(A_i)$ is orderable in this sense. Further, we show that this is optimal,
i.e., we cannot partition $L$ into less than $t$ orderable languages.
</p>
<p>In the case where $L$ is orderable, we show that the ordering can be computed
as a constant-delay algorithm: specifically, the algorithm runs in a suitable
pointer machine model, and produces a sequence of constant-length edit scripts
to visit the words of $L$ without repetitions, with constant delay between each
script. In fact, we show that we can achieve this while only allowing the edit
operations push and pop at the beginning and end of the word, which implies
that the word can in fact be maintained in a double-ended queue.
</p>
<p>We also show results on the complexity of a related problem, and study the
model where push-pop edits are only allowed at the end of the word.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-30T00:30:00Z">Friday, September 30 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Thursday, September 29
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://blog.computationalcomplexity.org/2022/09/machine-learning-and-complexity.html'>Machine Learning and Complexity</a></h3>
          <p class='item-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>&nbsp;</p>♦Schloss Dagstuhl by Monet by Dall-E<br><p></p><p>At Dagstuhl earlier this month, I hung out for a little bit&nbsp;with the participants of the other seminar,&nbsp;Knowledge Graphs and their Role in the Knowledge Engineering of the 21st Century. Knowledge graphs are what you would expect them to be, nodes are objects like "Berlin" and "Germany" with directed edges with labels like "capital". Think of having knowledge graphs of hundreds of millions of nodes and how that could help answer queries about the world. These secondary workshops are shorter and focus on creating a new vision, in this case how to maximize the importance of knowledge graphs in an increasing ML-focused world.</p><p>Perhaps we need such a visioning seminar for complexity. While we often get lost in the mathematical questions and techniques in our field, computational complexity is designed to understand the difficulty of solving various problems. Machine learning and advances in optimization should be changing that conversation. If you imagine a world where P = NP (and I did exactly that in chapter 2 of my 2013 book) much of what you consider is starting to happen anyway.&nbsp;ML does fail to break cryptography but then again, isn't this the best of all possible worlds?&nbsp;</p><p>Look at what Scott Aaronson said back in 2006.</p><blockquote>If P=NP, then the world would be a profoundly different place than we usually assume it to be. There would be no special value in “creative leaps,” no fundamental gap between solving a problem and recognizing the solution once it’s found. Everyone who could appreciate a symphony would be Mozart; everyone who could follow a step-by-step argument would be Gauss; everyone who could recognize a good investment strategy would be Warren Buffett.&nbsp;</blockquote><p>If I can be a Monet, can Mozart be far behind? ML trading by some hedge funds are beating Warren Buffett but remember if everyone trades perfectly, no one beats the average. Gauss is going to be trickier but it's coming. There's a reason Scott is&nbsp;spending a year at OpenAI&nbsp;to understand "what, if anything, can computational complexity contribute to a principled understanding of how to get an AI to do what we want and not do what we don’t want".</p><p></p><p>By Lance Fortnow</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p>&nbsp;</p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjB-e4HIPenlJclDSenALB6otAw1Nga8deJ3zx3JoMuf41bWFlfy6u4fi-agHPIClWKvkOGcuQRyXbfXXI__l8YpgFjXBY17Y05mdXCMT6bZUbgoILvkQ7D2t4JRNvqFuG-LNaQMRC0HphwBVu3DoVnnHj1ojqbfCXov8yZaRNlj9hoGpqBAQ/s1024/DALL%C2%B7E%202022-09-17%2014.00.36%20-%20Schloss%20Dagstuhl%20in%20the%20style%20of%20monet.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1024" data-original-width="1024" height="400" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjB-e4HIPenlJclDSenALB6otAw1Nga8deJ3zx3JoMuf41bWFlfy6u4fi-agHPIClWKvkOGcuQRyXbfXXI__l8YpgFjXBY17Y05mdXCMT6bZUbgoILvkQ7D2t4JRNvqFuG-LNaQMRC0HphwBVu3DoVnnHj1ojqbfCXov8yZaRNlj9hoGpqBAQ/w400-h400/DALL%C2%B7E%202022-09-17%2014.00.36%20-%20Schloss%20Dagstuhl%20in%20the%20style%20of%20monet.png" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Schloss Dagstuhl by Monet by Dall-E</td></tr></tbody></table><br /><p></p><p>At Dagstuhl <a href="https://blog.computationalcomplexity.org/2022/09/thirty-years-of-dagstuhl.html">earlier this month</a>, I hung out for a little bit&nbsp;with the participants of the other seminar,&nbsp;<a href="https://www.dagstuhl.de/en/program/calendar/semhp/?semnr=22372">Knowledge Graphs and their Role in the Knowledge Engineering of the 21st Century</a>. Knowledge graphs are what you would expect them to be, nodes are objects like "Berlin" and "Germany" with directed edges with labels like "capital". Think of having knowledge graphs of hundreds of millions of nodes and how that could help answer queries about the world. These secondary workshops are shorter and focus on creating a new vision, in this case how to maximize the importance of knowledge graphs in an increasing ML-focused world.</p><p>Perhaps we need such a visioning seminar for complexity. While we often get lost in the mathematical questions and techniques in our field, computational complexity is designed to understand the difficulty of solving various problems. Machine learning and advances in optimization should be changing that conversation. If you imagine a world where P = NP (and I did exactly that in chapter 2 of <a href="https://goldenticket.fortnow.com/">my 2013 book</a>) much of what you consider is <a href="https://blog.computationalcomplexity.org/2020/12/optiland.html">starting to happen anyway</a>.&nbsp;ML does fail to break cryptography but then again, isn't this the best of all possible worlds?&nbsp;</p><p>Look at what Scott Aaronson <a href="https://scottaaronson.blog/?p=122#:~:text=If%20P%3DNP%2C%20then%20the,strategy%20would%20be%20Warren%20Buffett.">said back in 2006</a>.</p><blockquote>If P=NP, then the world would be a profoundly different place than we usually assume it to be. There would be no special value in “creative leaps,” no fundamental gap between solving a problem and recognizing the solution once it’s found. Everyone who could appreciate a symphony would be Mozart; everyone who could follow a step-by-step argument would be Gauss; everyone who could recognize a good investment strategy would be Warren Buffett.&nbsp;</blockquote><p>If I can be a Monet, can Mozart be far behind? ML trading by some hedge funds are beating Warren Buffett but remember if everyone trades perfectly, no one beats the average. Gauss is going to be trickier but <a href="https://www.quantamagazine.org/in-new-math-proofs-artificial-intelligence-plays-to-win-20220307/">it's coming</a>. There's a reason Scott is&nbsp;<a href="https://scottaaronson.blog/?p=6484">spending a year at OpenAI</a>&nbsp;to understand "what, if anything, can computational complexity contribute to a principled understanding of how to get an AI to do what we want and not do what we don’t want".</p><p></p><p class="authors">By Lance Fortnow</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-29T14:35:00Z">Thursday, September 29 2022, 14:35</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.13599'>A characterization of polynomial time computable functions from the integers to the reals using discrete ordinary differential equations</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Manon Blanc, Olivier Bournez</p><p>In a recent article, the class of functions from the integers to the integers
computable in polynomial time has been characterized using discrete ordinary
differential equations (ODE), also known as finite differences. Doing so, we
pointed out the fundamental role of linear (discrete) ODEs and classical ODE
tools such as changes of variables to capture computability and complexity
measures, or as a tool for programming. In this article, we extend the approach
to a characterization of functions from the integers to the reals computable in
polynomial time in the sense of computable analysis. In particular, we provide
a characterization of such functions in terms of the smallest class of
functions that contains some basic functions, and that is closed by
composition, linear length ODEs, and a natural effective limit schema.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Blanc_M/0/1/0/all/0/1">Manon Blanc</a>, <a href="http://arxiv.org/find/cs/1/au:+Bournez_O/0/1/0/all/0/1">Olivier Bournez</a></p><p>In a recent article, the class of functions from the integers to the integers
computable in polynomial time has been characterized using discrete ordinary
differential equations (ODE), also known as finite differences. Doing so, we
pointed out the fundamental role of linear (discrete) ODEs and classical ODE
tools such as changes of variables to capture computability and complexity
measures, or as a tool for programming. In this article, we extend the approach
to a characterization of functions from the integers to the reals computable in
polynomial time in the sense of computable analysis. In particular, we provide
a characterization of such functions in terms of the smallest class of
functions that contains some basic functions, and that is closed by
composition, linear length ODEs, and a natural effective limit schema.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-29T00:30:00Z">Thursday, September 29 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.13725'>On the Descriptive Complexity of Groups without Abelian Normal Subgroups</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Joshua A. Grochow, Michael Levet</p><p>In this paper, we explore the descriptive complexity theory of finite groups
by examining the power of the second Ehrenfeucht-Fra\"iss\'e bijective pebble
game in Hella's (Ann. Pure Appl. Log., 1989) heirarchy. This is a
Spoiler-Duplicator game in which Spoiler can place up to two pebbles each
round. While it trivially solves graph isomorphism, it may be nontrivial for
finite groups, and other ternary relational structures. We first provide a
novel generalization of Weisfeiler-Leman (WL) coloring, which we call 2-ary WL.
We then show that the 2-ary WL is equivalent to the second
Ehrenfeucht-Fra\"iss\'e bijective pebble game in Hella's heirarchy.
</p>
<p>Our main result is that, in the pebble game characterization, only $O(1)$
pebbles and $O(1)$ rounds are sufficient to identify all groups without Abelian
normal subgroups (a class of groups for which isomorphism testing is known to
be in $\mathsf{P}$; Babai, Codenotti, &amp; Qiao, ICALP 2012). In particular, we
show that within the first few rounds, Spoiler can force Duplicator to select
an isomorphism between two such groups at each subsequent round. By Hella's
results (\emph{ibid.}), this is equivalent to saying that these groups are
identified by formulas in first-order logic with generalized 2-ary quantifiers,
using only $O(1)$ variables and $O(1)$ quantifier depth.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Grochow_J/0/1/0/all/0/1">Joshua A. Grochow</a>, <a href="http://arxiv.org/find/cs/1/au:+Levet_M/0/1/0/all/0/1">Michael Levet</a></p><p>In this paper, we explore the descriptive complexity theory of finite groups
by examining the power of the second Ehrenfeucht-Fra\"iss\'e bijective pebble
game in Hella's (Ann. Pure Appl. Log., 1989) heirarchy. This is a
Spoiler-Duplicator game in which Spoiler can place up to two pebbles each
round. While it trivially solves graph isomorphism, it may be nontrivial for
finite groups, and other ternary relational structures. We first provide a
novel generalization of Weisfeiler-Leman (WL) coloring, which we call 2-ary WL.
We then show that the 2-ary WL is equivalent to the second
Ehrenfeucht-Fra\"iss\'e bijective pebble game in Hella's heirarchy.
</p>
<p>Our main result is that, in the pebble game characterization, only $O(1)$
pebbles and $O(1)$ rounds are sufficient to identify all groups without Abelian
normal subgroups (a class of groups for which isomorphism testing is known to
be in $\mathsf{P}$; Babai, Codenotti, &amp; Qiao, ICALP 2012). In particular, we
show that within the first few rounds, Spoiler can force Duplicator to select
an isomorphism between two such groups at each subsequent round. By Hella's
results (\emph{ibid.}), this is equivalent to saying that these groups are
identified by formulas in first-order logic with generalized 2-ary quantifiers,
using only $O(1)$ variables and $O(1)$ quantifier depth.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-29T00:30:00Z">Thursday, September 29 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14094'>Dynamic Embeddings of Dynamic Single-Source Upward Planar Graphs</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Ivor van der Hoog, Irene Parada, Eva Rotenberg</p><p>A directed graph $G$ is upward planar if it admits a planar embedding such
that each edge is $y$-monotone. Unlike planarity testing, upward planarity
testing is NP-hard except in restricted cases, such as when the graph has the
single-source property (i.e. each connected component only has one source).
</p>
<p>In this paper, we present a dynamic algorithm for maintaining a combinatorial
embedding $\mathcal{E}(G)$ of a single-source upward planar graph subject to
edge deletions, edge contractions, edge insertions upwards across a face, and
single-source-preserving vertex splits through specified corners. We
furthermore support changes to the embedding $\mathcal{E}(G)$ on the form of
subgraph flips that mirror or slide the placement of a subgraph that is
connected to the rest of the graph via at most two vertices.
</p>
<p>All update operations are supported as long as the graph remains upward
planar, and all queries are supported as long as the graph remains
single-source. Updates that violate upward planarity are identified as such and
rejected by our update algorithm. We dynamically maintain a linear-size data
structure on $G$ which supports incidence queries between a vertex and a face,
and upward-linkability of vertex pairs. If a pair of vertices are not
upwards-linkable, we facilitate one-flip-linkable queries that point to a
subgraph flip that makes them linkable, if any such flip exists.
</p>
<p>We support all updates and queries in $O(\log^2 n)$ time.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Hoog_I/0/1/0/all/0/1">Ivor van der Hoog</a>, <a href="http://arxiv.org/find/cs/1/au:+Parada_I/0/1/0/all/0/1">Irene Parada</a>, <a href="http://arxiv.org/find/cs/1/au:+Rotenberg_E/0/1/0/all/0/1">Eva Rotenberg</a></p><p>A directed graph $G$ is upward planar if it admits a planar embedding such
that each edge is $y$-monotone. Unlike planarity testing, upward planarity
testing is NP-hard except in restricted cases, such as when the graph has the
single-source property (i.e. each connected component only has one source).
</p>
<p>In this paper, we present a dynamic algorithm for maintaining a combinatorial
embedding $\mathcal{E}(G)$ of a single-source upward planar graph subject to
edge deletions, edge contractions, edge insertions upwards across a face, and
single-source-preserving vertex splits through specified corners. We
furthermore support changes to the embedding $\mathcal{E}(G)$ on the form of
subgraph flips that mirror or slide the placement of a subgraph that is
connected to the rest of the graph via at most two vertices.
</p>
<p>All update operations are supported as long as the graph remains upward
planar, and all queries are supported as long as the graph remains
single-source. Updates that violate upward planarity are identified as such and
rejected by our update algorithm. We dynamically maintain a linear-size data
structure on $G$ which supports incidence queries between a vertex and a face,
and upward-linkability of vertex pairs. If a pair of vertices are not
upwards-linkable, we facilitate one-flip-linkable queries that point to a
subgraph flip that makes them linkable, if any such flip exists.
</p>
<p>We support all updates and queries in $O(\log^2 n)$ time.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-29T00:30:00Z">Thursday, September 29 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.13712'>A Quantum Optimization Algorithm for Single Machine Total Weighted Tardiness Minimization</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Youhao Steve Wang, Julian Cheng</p><p>A single machine total weighted tardiness minimization (TWTM) problem in
operational planning is considered. The problem is formulated as an NP-hard
constrained combinatorial problem, which has no known deterministic polynomial
complexity solution using classical computing. Based on efficient Grover's
quantum search and Trugenberger's quantum optimization algorithms, a novel
efficient quantum optimization algorithm is proposed to solve the NP-hard
single machine TWTM problem, which makes the desired solution satisfying the
searching constraints and showing the minimal TWT value be measured with the
highest probability.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Youhao Steve Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Julian Cheng</a></p><p>A single machine total weighted tardiness minimization (TWTM) problem in
operational planning is considered. The problem is formulated as an NP-hard
constrained combinatorial problem, which has no known deterministic polynomial
complexity solution using classical computing. Based on efficient Grover's
quantum search and Trugenberger's quantum optimization algorithms, a novel
efficient quantum optimization algorithm is proposed to solve the NP-hard
single machine TWTM problem, which makes the desired solution satisfying the
searching constraints and showing the minimal TWT value be measured with the
highest probability.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-29T00:30:00Z">Thursday, September 29 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.13878'>Near-Optimal Adaptive Policies for Serving Stochastically Departing Customers</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Danny Segev</p><p>We consider a multi-stage stochastic optimization problem originally
introduced by Cygan et al. (2013), studying how a single server should
prioritize stochastically departing customers. In this setting, our objective
is to determine an adaptive service policy that maximizes the expected total
reward collected along a discrete planning horizon, in the presence of
customers who are independently departing between one stage and the next with
known stationary probabilities. In spite of its deceiving structural
simplicity, we are unaware of non-trivial results regarding the rigorous design
of optimal or truly near-optimal policies at present time.
</p>
<p>Our main contribution resides in proposing a quasi-polynomial-time
approximation scheme for adaptively serving impatient customers. Specifically,
letting $n$ be the number of underlying customers, our algorithm identifies in
$O( n^{ O_{ \epsilon }( \log^2 n ) } )$ time an adaptive service policy whose
expected reward is within factor $1 - \epsilon$ of the optimal adaptive reward.
Our method for deriving this approximation scheme synthesizes various
stochastic analyses in order to investigate how the adaptive optimum is
affected by alteration to several instance parameters, including the reward
values, the departure probabilities, and the collection of customers itself.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Segev_D/0/1/0/all/0/1">Danny Segev</a></p><p>We consider a multi-stage stochastic optimization problem originally
introduced by Cygan et al. (2013), studying how a single server should
prioritize stochastically departing customers. In this setting, our objective
is to determine an adaptive service policy that maximizes the expected total
reward collected along a discrete planning horizon, in the presence of
customers who are independently departing between one stage and the next with
known stationary probabilities. In spite of its deceiving structural
simplicity, we are unaware of non-trivial results regarding the rigorous design
of optimal or truly near-optimal policies at present time.
</p>
<p>Our main contribution resides in proposing a quasi-polynomial-time
approximation scheme for adaptively serving impatient customers. Specifically,
letting $n$ be the number of underlying customers, our algorithm identifies in
$O( n^{ O_{ \epsilon }( \log^2 n ) } )$ time an adaptive service policy whose
expected reward is within factor $1 - \epsilon$ of the optimal adaptive reward.
Our method for deriving this approximation scheme synthesizes various
stochastic analyses in order to investigate how the adaptive optimum is
affected by alteration to several instance parameters, including the reward
values, the departure probabilities, and the collection of customers itself.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-29T00:30:00Z">Thursday, September 29 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14079'>Worst-case Deterministic Fully-Dynamic Planar 2-vertex Connectivity</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Jacob Holm, Ivor van der Hoog, Eva Rotenberg</p><p>We study dynamic planar graphs with $n$ vertices, subject to edge deletion,
edge contraction, edge insertion across a face, and the splitting of a vertex
in specified corners. We dynamically maintain a combinatorial embedding of such
a planar graph, subject to connectivity and $2$-vertex-connectivity
(biconnectivity) queries between pairs of vertices. Whenever a query pair is
connected and not biconnected, we find the first and last cutvertex separating
them.
</p>
<p>Additionally, we allow local changes to the embedding by flipping the
embedding of a subgraph that is connected by at most two vertices to the rest
of the graph.
</p>
<p>We support all queries and updates in deterministic, worst-case, $O(\log^2
n)$ time, using an $O(n)$-sized data structure.
</p>
<p>Previously, the best bound for fully-dynamic planar biconnectivity (subject
to our set of operations) was an amortised $\tilde{O}(\log^3 n)$ for general
graphs, and algorithms with worst-case polylogarithmic update times were known
only in the partially dynamic (insertion-only or deletion-only) setting.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Holm_J/0/1/0/all/0/1">Jacob Holm</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoog_I/0/1/0/all/0/1">Ivor van der Hoog</a>, <a href="http://arxiv.org/find/cs/1/au:+Rotenberg_E/0/1/0/all/0/1">Eva Rotenberg</a></p><p>We study dynamic planar graphs with $n$ vertices, subject to edge deletion,
edge contraction, edge insertion across a face, and the splitting of a vertex
in specified corners. We dynamically maintain a combinatorial embedding of such
a planar graph, subject to connectivity and $2$-vertex-connectivity
(biconnectivity) queries between pairs of vertices. Whenever a query pair is
connected and not biconnected, we find the first and last cutvertex separating
them.
</p>
<p>Additionally, we allow local changes to the embedding by flipping the
embedding of a subgraph that is connected by at most two vertices to the rest
of the graph.
</p>
<p>We support all queries and updates in deterministic, worst-case, $O(\log^2
n)$ time, using an $O(n)$-sized data structure.
</p>
<p>Previously, the best bound for fully-dynamic planar biconnectivity (subject
to our set of operations) was an amortised $\tilde{O}(\log^3 n)$ for general
graphs, and algorithms with worst-case polylogarithmic update times were known
only in the partially dynamic (insertion-only or deletion-only) setting.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-29T00:30:00Z">Thursday, September 29 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14087'>Adaptive Out-Orientations with Applications</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Aleksander B. G. Christiansen, Jacob Holm, Ivor van der Hoog, Eva Rotenberg, Chris Schwiegelshohn</p><p>We give simple algorithms for maintaining edge-orientations of a
fully-dynamic graph, such that the out-degree of each vertex is bounded. On one
hand, we show how to orient the edges such that the out-degree of each vertex
is proportional to the arboricity $\alpha$ of the graph, in a worst-case update
time of $O(\log^2 n \log \alpha)$. On the other hand, motivated by applications
in dynamic maximal matching, we obtain a different trade-off, namely the
improved worst case update time of $O(\log n \log \alpha)$ for the problem of
maintaining an edge-orientation with at most $O(\alpha + \log n)$ out-edges per
vertex. Since our algorithms have update times with worst-case guarantees, the
number of changes to the solution (i.e. the recourse) is naturally limited.
</p>
<p>Our algorithms make choices based entirely on local information, which makes
them automatically adaptive to the current arboricity of the graph. In other
words, they are arboricity-oblivious, while they are arboricity-sensitive. This
both simplifies and improves upon previous work, by having fewer assumptions or
better asymptotic guarantees.
</p>
<p>As a consequence, one obtains an algorithm with improved efficiency for
maintaining a $(1+\varepsilon)$ approximation of the maximum subgraph density,
and an algorithm for dynamic maximal matching whose worst-case update time is
guaranteed to be upper bounded by $O(\alpha + \log n\log \alpha)$, where
$\alpha$ is the arboricity at the time of the update.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Christiansen_A/0/1/0/all/0/1">Aleksander B. G. Christiansen</a>, <a href="http://arxiv.org/find/cs/1/au:+Holm_J/0/1/0/all/0/1">Jacob Holm</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoog_I/0/1/0/all/0/1">Ivor van der Hoog</a>, <a href="http://arxiv.org/find/cs/1/au:+Rotenberg_E/0/1/0/all/0/1">Eva Rotenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwiegelshohn_C/0/1/0/all/0/1">Chris Schwiegelshohn</a></p><p>We give simple algorithms for maintaining edge-orientations of a
fully-dynamic graph, such that the out-degree of each vertex is bounded. On one
hand, we show how to orient the edges such that the out-degree of each vertex
is proportional to the arboricity $\alpha$ of the graph, in a worst-case update
time of $O(\log^2 n \log \alpha)$. On the other hand, motivated by applications
in dynamic maximal matching, we obtain a different trade-off, namely the
improved worst case update time of $O(\log n \log \alpha)$ for the problem of
maintaining an edge-orientation with at most $O(\alpha + \log n)$ out-edges per
vertex. Since our algorithms have update times with worst-case guarantees, the
number of changes to the solution (i.e. the recourse) is naturally limited.
</p>
<p>Our algorithms make choices based entirely on local information, which makes
them automatically adaptive to the current arboricity of the graph. In other
words, they are arboricity-oblivious, while they are arboricity-sensitive. This
both simplifies and improves upon previous work, by having fewer assumptions or
better asymptotic guarantees.
</p>
<p>As a consequence, one obtains an algorithm with improved efficiency for
maintaining a $(1+\varepsilon)$ approximation of the maximum subgraph density,
and an algorithm for dynamic maximal matching whose worst-case update time is
guaranteed to be upper bounded by $O(\alpha + \log n\log \alpha)$, where
$\alpha$ is the arboricity at the time of the update.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-29T00:30:00Z">Thursday, September 29 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14140'>Time and Energy Efficient Contention Resolution in Asynchronous Shared Channels</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Gianluca De Marco, Dariusz R. Kowalski, Grzegorz Stachowiak</p><p>A number of stations, independently activated over time, is able to
communicate by transmitting and listening to a shared channel in discrete time
slots, and a message is successfully delivered to all stations if and only if
its source station is the only transmitter at a time. Despite a vast amount of
work in the last decades, many fundamental questions remain open in the
realistic situation where stations do not start synchronously but are awaken in
arbitrary times. In this work we present a broad picture of results for the
fundamental problem of Contention resolution, in which each of the contending
stations needs to broadcast successfully its message.
</p>
<p>We show that adaptive algorithms or algorithms with the knowledge of the
contention size $k$ achieve a linear $O(k)$ message latency even if the channel
feedback is restricted to simple acknowledgements in case of successful
transmissions and in the absence of synchronization. This asymptotically
optimal performance cannot be extended to other settings: we prove that there
is no non-adaptive algorithm without the knowledge of contention size $k$
admitting latency $o(k\log k/(\log\log k)^2)$. This means, in particular, that
coding (even random) with acknowledgements is not very efficient on a shared
channel without synchronization or an estimate of the contention size. We also
present a non-adaptive algorithm with no knowledge of contention size that
almost matches the lower bound on latency.
</p>
<p>Finally, despite the absence of a collision detection mechanism, we show that
our algorithms are also efficient in terms of energy, understood as the total
number of transmissions performed by the stations during the execution.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Marco_G/0/1/0/all/0/1">Gianluca De Marco</a>, <a href="http://arxiv.org/find/cs/1/au:+Kowalski_D/0/1/0/all/0/1">Dariusz R. Kowalski</a>, <a href="http://arxiv.org/find/cs/1/au:+Stachowiak_G/0/1/0/all/0/1">Grzegorz Stachowiak</a></p><p>A number of stations, independently activated over time, is able to
communicate by transmitting and listening to a shared channel in discrete time
slots, and a message is successfully delivered to all stations if and only if
its source station is the only transmitter at a time. Despite a vast amount of
work in the last decades, many fundamental questions remain open in the
realistic situation where stations do not start synchronously but are awaken in
arbitrary times. In this work we present a broad picture of results for the
fundamental problem of Contention resolution, in which each of the contending
stations needs to broadcast successfully its message.
</p>
<p>We show that adaptive algorithms or algorithms with the knowledge of the
contention size $k$ achieve a linear $O(k)$ message latency even if the channel
feedback is restricted to simple acknowledgements in case of successful
transmissions and in the absence of synchronization. This asymptotically
optimal performance cannot be extended to other settings: we prove that there
is no non-adaptive algorithm without the knowledge of contention size $k$
admitting latency $o(k\log k/(\log\log k)^2)$. This means, in particular, that
coding (even random) with acknowledgements is not very efficient on a shared
channel without synchronization or an estimate of the contention size. We also
present a non-adaptive algorithm with no knowledge of contention size that
almost matches the lower bound on latency.
</p>
<p>Finally, despite the absence of a collision detection mechanism, we show that
our algorithms are also efficient in terms of energy, understood as the total
number of transmissions performed by the stations during the execution.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-29T00:30:00Z">Thursday, September 29 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14146'>Quantum Subroutine Composition</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Stacey Jeffery</p><p>An important tool in algorithm design is the ability to build algorithms from
other algorithms that run as subroutines. In the case of quantum algorithms, a
subroutine may be called on a superposition of different inputs, which
complicates things. For example, a classical algorithm that calls a subroutine
$Q$ times, where the average probability of querying the subroutine on input
$i$ is $p_i$, and the cost of the subroutine on input $i$ is $T_i$, incurs
expected cost $Q\sum_i p_i E[T_i]$ from all subroutine queries. While this
statement is obvious for classical algorithms, for quantum algorithms, it is
much less so, since naively, if we run a quantum subroutine on a superposition
of inputs, we need to wait for all branches of the superposition to terminate
before we can apply the next operation. We nonetheless show an analogous
quantum statement (*): If $q_i$ is the average query weight on $i$ over all
queries, the cost from all quantum subroutine queries is $Q\sum_i q_i E[T_i]$.
Here the query weight on $i$ for a particular query is the probability of
measuring $i$ in the input register if we were to measure right before the
query.
</p>
<p>We prove this result using the technique of multidimensional quantum walks,
recently introduced in arXiv:2208.13492. We present a more general version of
their quantum walk edge composition result, which yields variable-time quantum
walks, generalizing variable-time quantum search, by, for example, replacing
the update cost with $\sqrt{\sum_{u,v}\pi_u P_{u,v} E[T_{u,v}^2]}$, where
$T_{u,v}$ is the cost to move from vertex $u$ to vertex $v$. The same technique
that allows us to compose quantum subroutines in quantum walks can also be used
to compose in any quantum algorithm, which is how we prove (*).
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Jeffery_S/0/1/0/all/0/1">Stacey Jeffery</a></p><p>An important tool in algorithm design is the ability to build algorithms from
other algorithms that run as subroutines. In the case of quantum algorithms, a
subroutine may be called on a superposition of different inputs, which
complicates things. For example, a classical algorithm that calls a subroutine
$Q$ times, where the average probability of querying the subroutine on input
$i$ is $p_i$, and the cost of the subroutine on input $i$ is $T_i$, incurs
expected cost $Q\sum_i p_i E[T_i]$ from all subroutine queries. While this
statement is obvious for classical algorithms, for quantum algorithms, it is
much less so, since naively, if we run a quantum subroutine on a superposition
of inputs, we need to wait for all branches of the superposition to terminate
before we can apply the next operation. We nonetheless show an analogous
quantum statement (*): If $q_i$ is the average query weight on $i$ over all
queries, the cost from all quantum subroutine queries is $Q\sum_i q_i E[T_i]$.
Here the query weight on $i$ for a particular query is the probability of
measuring $i$ in the input register if we were to measure right before the
query.
</p>
<p>We prove this result using the technique of multidimensional quantum walks,
recently introduced in <a href="/abs/2208.13492">arXiv:2208.13492</a>. We present a more general version of
their quantum walk edge composition result, which yields variable-time quantum
walks, generalizing variable-time quantum search, by, for example, replacing
the update cost with $\sqrt{\sum_{u,v}\pi_u P_{u,v} E[T_{u,v}^2]}$, where
$T_{u,v}$ is the cost to move from vertex $u$ to vertex $v$. The same technique
that allows us to compose quantum subroutines in quantum walks can also be used
to compose in any quantum algorithm, which is how we prove (*).
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-29T00:30:00Z">Thursday, September 29 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.14197'>On Computing Exact Means of Time Series Using the Move-Split-Merge Metric</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Jana Holznigenkemper, Christian Komusiewicz, Bernhard Seeger</p><p>Computing an accurate mean of a set of time series is a critical task in
applications like nearest-neighbor classification and clustering of time
series. While there are many distance functions for time series, the most
popular distance function used for the computation of time series means is the
non-metric dynamic time warping (DTW) distance. A recent algorithm for the
exact computation of a DTW-Mean has a running time of
$\mathcal{O}(n^{2k+1}2^kk)$, where $k$ denotes the number of time series and
$n$ their maximum length. In this paper, we study the mean problem for the
move-split-merge (MSM) metric that not only offers high practical accuracy for
time series classification but also carries of the advantages of the metric
properties that enable further diverse applications. The main contribution of
this paper is an exact and efficient algorithm for the MSM-Mean problem of time
series. The running time of our algorithm is $\mathcal{O}(n^{k+3}2^k k^3 )$,
and thus better than the previous DTW-based algorithm. The results of an
experimental comparison confirm the running time superiority of our algorithm
in comparison to the DTW-Mean competitor. Moreover, we introduce a heuristic to
improve the running time significantly without sacrificing much accuracy.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Holznigenkemper_J/0/1/0/all/0/1">Jana Holznigenkemper</a>, <a href="http://arxiv.org/find/cs/1/au:+Komusiewicz_C/0/1/0/all/0/1">Christian Komusiewicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Seeger_B/0/1/0/all/0/1">Bernhard Seeger</a></p><p>Computing an accurate mean of a set of time series is a critical task in
applications like nearest-neighbor classification and clustering of time
series. While there are many distance functions for time series, the most
popular distance function used for the computation of time series means is the
non-metric dynamic time warping (DTW) distance. A recent algorithm for the
exact computation of a DTW-Mean has a running time of
$\mathcal{O}(n^{2k+1}2^kk)$, where $k$ denotes the number of time series and
$n$ their maximum length. In this paper, we study the mean problem for the
move-split-merge (MSM) metric that not only offers high practical accuracy for
time series classification but also carries of the advantages of the metric
properties that enable further diverse applications. The main contribution of
this paper is an exact and efficient algorithm for the MSM-Mean problem of time
series. The running time of our algorithm is $\mathcal{O}(n^{k+3}2^k k^3 )$,
and thus better than the previous DTW-based algorithm. The results of an
experimental comparison confirm the running time superiority of our algorithm
in comparison to the DTW-Mean competitor. Moreover, we introduce a heuristic to
improve the running time significantly without sacrificing much accuracy.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-29T00:30:00Z">Thursday, September 29 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Wednesday, September 28
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://cstheory-jobs.org/2022/09/28/phd-and-postdoc-at-irif-apply-by-november-1-2022/'>PhD and postdoc at IRIF (apply by November 1, 2022)</a></h3>
          <p class='item-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          The Algorithms &#38; Complexity group at IRIF (CNRS, Université Paris-Cité) in Paris offers multiple PhD and postdoc positions on the theory of quantum computing. The group has expertise in quantum algorithms and quantum complexity theory, with permanent members S. Apers, I. Kerenidis, S. Laplante and F. Magniez. Soft deadline: November 1st, 2022. Website: irif.fr/en/equipes/algocomp/ Email: [&#8230;]
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p>The Algorithms &amp; Complexity group at IRIF (CNRS, Université Paris-Cité) in Paris offers multiple PhD and postdoc positions on the theory of quantum computing. The group has expertise in quantum algorithms and quantum complexity theory, with permanent members S. Apers, I. Kerenidis, S. Laplante and F. Magniez.</p>
<p>Soft deadline: November 1st, 2022.</p>
<p>Website: irif.fr/en/equipes/algocomp/<br />
Email: apers@irif.fr</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-28T10:12:03Z">Wednesday, September 28 2022, 10:12</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.13148'>Strategyproofness-Exposing Mechanism Descriptions</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Yannai A. Gonczarowski, Ori Heffetz, Clayton Thomas</p><p>A menu description defines a mechanism to player $i$ in two steps. Step (1)
uses the reports of other players to describe $i$'s menu: the set of $i$'s
potential outcomes. Step (2) uses $i$'s report to select $i$'s favorite outcome
from her menu. Can menu descriptions better expose strategyproofness, without
sacrificing simplicity? We propose a new, simple menu description of Deferred
Acceptance. We prove that -- in contrast with other common matching mechanisms
-- this menu description must differ substantially from the corresponding
traditional description. We demonstrate, with a lab experiment on two simple
mechanisms, the promise and challenges of menu descriptions.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/econ/1/au:+Gonczarowski_Y/0/1/0/all/0/1">Yannai A. Gonczarowski</a>, <a href="http://arxiv.org/find/econ/1/au:+Heffetz_O/0/1/0/all/0/1">Ori Heffetz</a>, <a href="http://arxiv.org/find/econ/1/au:+Thomas_C/0/1/0/all/0/1">Clayton Thomas</a></p><p>A menu description defines a mechanism to player $i$ in two steps. Step (1)
uses the reports of other players to describe $i$'s menu: the set of $i$'s
potential outcomes. Step (2) uses $i$'s report to select $i$'s favorite outcome
from her menu. Can menu descriptions better expose strategyproofness, without
sacrificing simplicity? We propose a new, simple menu description of Deferred
Acceptance. We prove that -- in contrast with other common matching mechanisms
-- this menu description must differ substantially from the corresponding
traditional description. We demonstrate, with a lab experiment on two simple
mechanisms, the promise and challenges of menu descriptions.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-28T00:30:00Z">Wednesday, September 28 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.13404'>Polynomial time computable functions over the reals characterized using discrete ordinary differential equations</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Manon Blanc, Olivier Bournez</p><p>The class of functions from the integers to the integers computable in
polynomial time has been characterized recently using discrete ordinary
differential equations (ODE), also known as finite differences. In the
framework of ordinary differential equations, this is very natural to try to
extend the approach to classes of functions over the reals, and not only over
the integers. Recently, an extension of previous characterization was obtained
for functions from the integers to the reals, but the method used in the proof,
based on the existence of a continuous function from the integers to a suitable
discrete set of reals, cannot extend to functions from the reals to the reals,
as such a function cannot exist for clear topological reasons. In this article,
we prove that this is indeed possible to provide an elegant and simple
algebraic characterization of functions from the reals to the reals: we provide
a characterization of such functions as the smallest class of functions that
contains some basic functions, and that is closed by composition, linear length
ODEs, and a natural effective limit schema. This is obtained using an
alternative proof technique based on the construction of specific suitable
functions defined recursively, and a barycentric method. Furthermore, we also
extend previous characterizations in several directions: First, we prove that
there is no need of multiplication. We prove a normal form theorem, with a nice
side effect related to formal neural networks. Indeed, given some fixed error
and some polynomial time t(n), our settings produce effectively some neural
network that computes the function over its domain with the given precision,
for any t(n)-polynomial time computable function f .
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Blanc_M/0/1/0/all/0/1">Manon Blanc</a>, <a href="http://arxiv.org/find/cs/1/au:+Bournez_O/0/1/0/all/0/1">Olivier Bournez</a></p><p>The class of functions from the integers to the integers computable in
polynomial time has been characterized recently using discrete ordinary
differential equations (ODE), also known as finite differences. In the
framework of ordinary differential equations, this is very natural to try to
extend the approach to classes of functions over the reals, and not only over
the integers. Recently, an extension of previous characterization was obtained
for functions from the integers to the reals, but the method used in the proof,
based on the existence of a continuous function from the integers to a suitable
discrete set of reals, cannot extend to functions from the reals to the reals,
as such a function cannot exist for clear topological reasons. In this article,
we prove that this is indeed possible to provide an elegant and simple
algebraic characterization of functions from the reals to the reals: we provide
a characterization of such functions as the smallest class of functions that
contains some basic functions, and that is closed by composition, linear length
ODEs, and a natural effective limit schema. This is obtained using an
alternative proof technique based on the construction of specific suitable
functions defined recursively, and a barycentric method. Furthermore, we also
extend previous characterizations in several directions: First, we prove that
there is no need of multiplication. We prove a normal form theorem, with a nice
side effect related to formal neural networks. Indeed, given some fixed error
and some polynomial time t(n), our settings produce effectively some neural
network that computes the function over its domain with the given precision,
for any t(n)-polynomial time computable function f .
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-28T00:30:00Z">Wednesday, September 28 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.13024'>Improved and Generalized Algorithms for Burning a Planar Point Set</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Prashant Gokhale, J. Mark Keil, Debajyoti Mondal</p><p>Given a set $P$ of points in the plane, a point burning process is a discrete
time process to burn all the points of $P$ where fires must be initiated at the
given points. Specifically, the point burning process starts with a single
burnt point from $P$, and at each subsequent step, burns all the points in the
plane that are within one unit distance from the currently burnt points, as
well as one other unburnt point of $P$ (if exists). The point burning number of
$P$ is the smallest number of steps required to burn all the points of $P$. If
we allow the fire to be initiated anywhere, then the burning process is called
an anywhere burning process, and the corresponding burning number is called
anywhere burning number. Computing the point and anywhere burning number is
known to be NP-hard. In this paper we show that both these problems admit PTAS
in one dimension. We then show that in two dimensions, point burning and
anywhere burning are $(1.96296+\varepsilon)$ and $(1.92188+\varepsilon)$
approximable, respectively, for every $\varepsilon&gt;0$, which improves the
previously known $(2+\varepsilon)$ factor for these problems. We also observe
that a known result on set cover problem can be leveraged to obtain a
2-approximation for burning the maximum number of points in a given number of
steps. We show how the results generalize if we allow the points to have
different fire spreading rates. Finally, we prove that even if the burning
sources are given as input, finding a point burning sequence itself is NP-hard.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gokhale_P/0/1/0/all/0/1">Prashant Gokhale</a>, <a href="http://arxiv.org/find/cs/1/au:+Keil_J/0/1/0/all/0/1">J. Mark Keil</a>, <a href="http://arxiv.org/find/cs/1/au:+Mondal_D/0/1/0/all/0/1">Debajyoti Mondal</a></p><p>Given a set $P$ of points in the plane, a point burning process is a discrete
time process to burn all the points of $P$ where fires must be initiated at the
given points. Specifically, the point burning process starts with a single
burnt point from $P$, and at each subsequent step, burns all the points in the
plane that are within one unit distance from the currently burnt points, as
well as one other unburnt point of $P$ (if exists). The point burning number of
$P$ is the smallest number of steps required to burn all the points of $P$. If
we allow the fire to be initiated anywhere, then the burning process is called
an anywhere burning process, and the corresponding burning number is called
anywhere burning number. Computing the point and anywhere burning number is
known to be NP-hard. In this paper we show that both these problems admit PTAS
in one dimension. We then show that in two dimensions, point burning and
anywhere burning are $(1.96296+\varepsilon)$ and $(1.92188+\varepsilon)$
approximable, respectively, for every $\varepsilon&gt;0$, which improves the
previously known $(2+\varepsilon)$ factor for these problems. We also observe
that a known result on set cover problem can be leveraged to obtain a
2-approximation for burning the maximum number of points in a given number of
steps. We show how the results generalize if we allow the points to have
different fire spreading rates. Finally, we prove that even if the burning
sources are given as input, finding a point burning sequence itself is NP-hard.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-28T00:30:00Z">Wednesday, September 28 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.13311'>Optimal Placement of Base Stations in Border Surveillance using Limited Capacity Drones</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: S. Bereg, J.M. D&#xed;az-B&#xe1;&#xf1;ez, M. Haghpanah, P. Horn, M.A. Lopez, N. Mar&#xed;n, A. Ram&#xed;rez-Vigueras, F. Rodr&#xed;guez, O. Sol&#xe9;-Pi, A. Stevens, J. Urrutia</p><p>Imagine an island modeled as a simple polygon $\P$ with $n$ vertices whose
coastline we wish to monitor. We consider the problem of building the minimum
number of refueling stations along the boundary of $\P$ in such a way that a
drone can follow a polygonal route enclosing the island without running out of
fuel. A drone can fly a maximum distance $d$ between consecutive stations and
is restricted to move either along the boundary of $\P$ or its exterior (i.e.,
over water). We present an algorithm that, given $\mathcal P$, finds the
locations for a set of refueling stations whose cardinality is at most the
optimal plus one. The time complexity of this algorithm is $O(n^2 + \frac{L}{d}
n)$, where $L$ is the length of $\mathcal P$. We also present an algorithm that
returns an additive $\epsilon$-approximation for the problem of minimizing the
fuel capacity required for the drones when we are allowed to place $k$ base
stations around the boundary of the island; this algorithm also finds the
locations of these refueling stations. Finally, we propose a practical
discretization heuristic which, under certain conditions, can be used to
certify optimality of the results.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bereg_S/0/1/0/all/0/1">S. Bereg</a>, <a href="http://arxiv.org/find/cs/1/au:+Diaz_Banez_J/0/1/0/all/0/1">J.M. D&#xed;az-B&#xe1;&#xf1;ez</a>, <a href="http://arxiv.org/find/cs/1/au:+Haghpanah_M/0/1/0/all/0/1">M. Haghpanah</a>, <a href="http://arxiv.org/find/cs/1/au:+Horn_P/0/1/0/all/0/1">P. Horn</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_M/0/1/0/all/0/1">M.A. Lopez</a>, <a href="http://arxiv.org/find/cs/1/au:+Marin_N/0/1/0/all/0/1">N. Mar&#xed;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramirez_Vigueras_A/0/1/0/all/0/1">A. Ram&#xed;rez-Vigueras</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_F/0/1/0/all/0/1">F. Rodr&#xed;guez</a>, <a href="http://arxiv.org/find/cs/1/au:+Sole_Pi_O/0/1/0/all/0/1">O. Sol&#xe9;-Pi</a>, <a href="http://arxiv.org/find/cs/1/au:+Stevens_A/0/1/0/all/0/1">A. Stevens</a>, <a href="http://arxiv.org/find/cs/1/au:+Urrutia_J/0/1/0/all/0/1">J. Urrutia</a></p><p>Imagine an island modeled as a simple polygon $\P$ with $n$ vertices whose
coastline we wish to monitor. We consider the problem of building the minimum
number of refueling stations along the boundary of $\P$ in such a way that a
drone can follow a polygonal route enclosing the island without running out of
fuel. A drone can fly a maximum distance $d$ between consecutive stations and
is restricted to move either along the boundary of $\P$ or its exterior (i.e.,
over water). We present an algorithm that, given $\mathcal P$, finds the
locations for a set of refueling stations whose cardinality is at most the
optimal plus one. The time complexity of this algorithm is $O(n^2 + \frac{L}{d}
n)$, where $L$ is the length of $\mathcal P$. We also present an algorithm that
returns an additive $\epsilon$-approximation for the problem of minimizing the
fuel capacity required for the drones when we are allowed to place $k$ base
stations around the boundary of the island; this algorithm also finds the
locations of these refueling stations. Finally, we propose a practical
discretization heuristic which, under certain conditions, can be used to
certify optimality of the results.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-28T00:30:00Z">Wednesday, September 28 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.13538'>Mathematics and Flamenco: An Unexpected Partnership</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Jos&#xe9;-Miguel D&#xed;az-B&#xe1;&#xf1;ez</p><p>In this paper, we present a series of mathematical problems which throw
interesting lights on flamenco music. More specifically, these are problems in
discrete and computational mathematics suggested by an analytical (not
compositional) examination of flamenco ``cante'' (singing). As a consequence,
since the problems are taken from a culturally specific context, the examples
can make more effective mathematics education.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Diaz_Banez_J/0/1/0/all/0/1">Jos&#xe9;-Miguel D&#xed;az-B&#xe1;&#xf1;ez</a></p><p>In this paper, we present a series of mathematical problems which throw
interesting lights on flamenco music. More specifically, these are problems in
discrete and computational mathematics suggested by an analytical (not
compositional) examination of flamenco ``cante'' (singing). As a consequence,
since the problems are taken from a culturally specific context, the examples
can make more effective mathematics education.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-28T00:30:00Z">Wednesday, September 28 2022, 00:30</time>
        </div>
      </div>
    </article>
  
  </div>

  <script src='js/jquery-2.0.3.min.js'></script>
  <script src="js/jquery.timeago.js" type="text/javascript"></script>
  <script>
    jQuery(document).ready(function() {
      jQuery("time.timeago").timeago();
    });
  </script>
  <script src='js/blank.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
