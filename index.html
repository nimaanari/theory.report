<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.5 (2022-11-24) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2023-03-03T05:34:17Z">Friday, March 03 2023, 05:34</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://minorfree.github.io/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://minorfree.github.io'>Hung Le</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>SÃ©bastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Friday, March 03
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.01016'>On the Consistency of Circuit Lower Bounds for Non-Deterministic Time</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Albert Atserias, Sam Buss, Moritz M&#xfc;ller</p><p>We prove the first unconditional consistency result for superpolynomial
circuit lower bounds with a relatively strong theory of bounded arithmetic.
Namely, we show that the theory V$^0_2$ is consistent with the conjecture that
NEXP $\not\subseteq$ P/poly, i.e., some problem that is solvable in
non-deterministic exponential time does not have polynomial size circuits. We
suggest this is the best currently available evidence for the truth of the
conjecture. The same techniques establish the same results with NEXP replaced
by the class of problems that are decidable in non-deterministic barely
superpolynomial time such as NTIME$(n^{O(\log\log\log n)})$. Additionally, we
establish a magnification result on the hardness of proving circuit lower
bounds.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Atserias_A/0/1/0/all/0/1">Albert Atserias</a>, <a href="http://arxiv.org/find/cs/1/au:+Buss_S/0/1/0/all/0/1">Sam Buss</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1">Moritz M&#xfc;ller</a></p><p>We prove the first unconditional consistency result for superpolynomial
circuit lower bounds with a relatively strong theory of bounded arithmetic.
Namely, we show that the theory V$^0_2$ is consistent with the conjecture that
NEXP $\not\subseteq$ P/poly, i.e., some problem that is solvable in
non-deterministic exponential time does not have polynomial size circuits. We
suggest this is the best currently available evidence for the truth of the
conjecture. The same techniques establish the same results with NEXP replaced
by the class of problems that are decidable in non-deterministic barely
superpolynomial time such as NTIME$(n^{O(\log\log\log n)})$. Additionally, we
establish a magnification result on the hardness of proving circuit lower
bounds.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-03T01:30:00Z">Friday, March 03 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.01411'>Algorithmic Randomness and Probabilistic Laws</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jeffrey A. Barrett, Eddy Keming Chen</p><p>We consider two ways one might use algorithmic randomness to characterize a
probabilistic law. The first is a generative chance* law. Such laws involve a
nonstandard notion of chance. The second is a probabilistic* constraining law.
Such laws impose relative frequency and randomness constraints that every
physically possible world must satisfy. While each notion has virtues, we argue
that the latter has advantages over the former. It supports a unified governing
account of non-Humean laws and provides independently motivated solutions to
issues in the Humean best-system account. On both notions, we have a much
tighter connection between probabilistic laws and their corresponding sets of
possible worlds. Certain histories permitted by traditional probabilistic laws
are ruled out as physically impossible. As a result, such laws avoid one
variety of empirical underdetermination, but the approach reveals other
varieties of underdetermination that are typically overlooked.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/physics/1/au:+Barrett_J/0/1/0/all/0/1">Jeffrey A. Barrett</a>, <a href="http://arxiv.org/find/physics/1/au:+Chen_E/0/1/0/all/0/1">Eddy Keming Chen</a></p><p>We consider two ways one might use algorithmic randomness to characterize a
probabilistic law. The first is a generative chance* law. Such laws involve a
nonstandard notion of chance. The second is a probabilistic* constraining law.
Such laws impose relative frequency and randomness constraints that every
physically possible world must satisfy. While each notion has virtues, we argue
that the latter has advantages over the former. It supports a unified governing
account of non-Humean laws and provides independently motivated solutions to
issues in the Humean best-system account. On both notions, we have a much
tighter connection between probabilistic laws and their corresponding sets of
possible worlds. Certain histories permitted by traditional probabilistic laws
are ruled out as physically impossible. As a result, such laws avoid one
variety of empirical underdetermination, but the approach reveals other
varieties of underdetermination that are typically overlooked.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-03T01:30:00Z">Friday, March 03 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.01224'>Enumeration and Unimodular Equivalence of Empty Delta-Modular Simplices</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: D. Gribanov</p><p>Consider a class of simplices defined by systems $A x \leq b$ of linear
inequalities with $\Delta$-modular matrices. A matrix is called
$\Delta$-modular, if all its rank-order sub-determinants are bounded by
$\Delta$ in an absolute value. In our work we call a simplex $\Delta$-modular,
if it can be defined by a system $A x \leq b$ with a $\Delta$-modular matrix
$A$. And we call a simplex empty, if it contains no points with integer
coordinates. In literature, a simplex is called lattice-simplex, if all its
vertices have integer coordinates. And a lattice-simplex called empty, if it
contains no points with integer coordinates excluding its vertices.
</p>
<p>Recently, assuming that $\Delta$ is fixed, it was shown that the number of
$\Delta$-modular empty simplices modulo the unimodular equivalence relation is
bounded by a polynomial on dimension. We show that the analogous fact holds for
the class of $\Delta$-modular empty lattice-simplices. As the main result,
assuming again that the value of the parameter $\Delta$ is fixed, we show that
all unimodular equivalence classes of simplices of the both types can be
enumerated by a polynomial-time algorithm. As the secondary result, we show the
existence of a polynomial-time algorithm for the problem to check the
unimodular equivalence relation for a given pair of $\Delta$-modular, not
necessarily empty, simplices.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Gribanov_D/0/1/0/all/0/1">D. Gribanov</a></p><p>Consider a class of simplices defined by systems $A x \leq b$ of linear
inequalities with $\Delta$-modular matrices. A matrix is called
$\Delta$-modular, if all its rank-order sub-determinants are bounded by
$\Delta$ in an absolute value. In our work we call a simplex $\Delta$-modular,
if it can be defined by a system $A x \leq b$ with a $\Delta$-modular matrix
$A$. And we call a simplex empty, if it contains no points with integer
coordinates. In literature, a simplex is called lattice-simplex, if all its
vertices have integer coordinates. And a lattice-simplex called empty, if it
contains no points with integer coordinates excluding its vertices.
</p>
<p>Recently, assuming that $\Delta$ is fixed, it was shown that the number of
$\Delta$-modular empty simplices modulo the unimodular equivalence relation is
bounded by a polynomial on dimension. We show that the analogous fact holds for
the class of $\Delta$-modular empty lattice-simplices. As the main result,
assuming again that the value of the parameter $\Delta$ is fixed, we show that
all unimodular equivalence classes of simplices of the both types can be
enumerated by a polynomial-time algorithm. As the secondary result, we show the
existence of a polynomial-time algorithm for the problem to check the
unimodular equivalence relation for a given pair of $\Delta$-modular, not
necessarily empty, simplices.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-03T01:30:00Z">Friday, March 03 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.00878'>Interactive Exploration of the Temporal $\alpha$-Shape</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Felix Weitbrecht</p><p>An interesting subcomplex of the Delaunay triangulation are $\alpha$-shapes,
which give a more detailed representation of the shape of point sets than the
convex hull. We extend an algorithm which computes all Delaunay simplices over
all time windows to also compute the temporal $\alpha$-shape, which is a
description of all $\alpha$-shapes over all time windows and all values of
$\alpha$, in output-sensitive linear time. We present an interactive demo
application based on a fast query data structure. Experimental results show
that our algorithm is practical and can be used on real-world data sets.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Weitbrecht_F/0/1/0/all/0/1">Felix Weitbrecht</a></p><p>An interesting subcomplex of the Delaunay triangulation are $\alpha$-shapes,
which give a more detailed representation of the shape of point sets than the
convex hull. We extend an algorithm which computes all Delaunay simplices over
all time windows to also compute the temporal $\alpha$-shape, which is a
description of all $\alpha$-shapes over all time windows and all values of
$\alpha$, in output-sensitive linear time. We present an interactive demo
application based on a fast query data structure. Experimental results show
that our algorithm is practical and can be used on real-world data sets.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-03T01:30:00Z">Friday, March 03 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.01096'>Geometric Spanning Trees Minimizing the Wiener Index</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: A. Karim Abu-Affash, Paz Carmi, Ori Luwisch, Joseph S. B. Mitchell</p><p>The Wiener index of a network, introduced by the chemist Harry Wiener, is the
sum of distances between all pairs of nodes in the network. This index,
originally used in chemical graph representations of the non-hydrogen atoms of
a molecule, is considered to be a fundamental and useful network descriptor. We
study the problem of constructing geometric networks on point sets in Euclidean
space that minimize the Wiener index: given a set $P$ of $n$ points in
$\mathbb{R}^d$, the goal is to construct a network, spanning $P$ and satisfying
certain constraints, that minimizes the Wiener index among the allowable class
of spanning networks.
</p>
<p>In this work, we focus mainly on spanning networks that are trees and we
focus on problems in the plane ($d=2$). We show that any spanning tree that
minimizes the Wiener index has non-crossing edges in the plane. Then, we use
this fact to devise an $O(n^4)$-time algorithm that constructs a spanning tree
of minimum Wiener index for points in convex position. We also prove that the
problem of computing a spanning tree on $P$ whose Wiener index is at most $W$,
while having total (Euclidean) weight at most $B$, is NP-hard.
</p>
<p>Computing a tree that minimizes the Wiener index has been studied in the area
of communication networks, where it is known as the optimum communication
spanning tree problem.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Abu_Affash_A/0/1/0/all/0/1">A. Karim Abu-Affash</a>, <a href="http://arxiv.org/find/cs/1/au:+Carmi_P/0/1/0/all/0/1">Paz Carmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Luwisch_O/0/1/0/all/0/1">Ori Luwisch</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitchell_J/0/1/0/all/0/1">Joseph S. B. Mitchell</a></p><p>The Wiener index of a network, introduced by the chemist Harry Wiener, is the
sum of distances between all pairs of nodes in the network. This index,
originally used in chemical graph representations of the non-hydrogen atoms of
a molecule, is considered to be a fundamental and useful network descriptor. We
study the problem of constructing geometric networks on point sets in Euclidean
space that minimize the Wiener index: given a set $P$ of $n$ points in
$\mathbb{R}^d$, the goal is to construct a network, spanning $P$ and satisfying
certain constraints, that minimizes the Wiener index among the allowable class
of spanning networks.
</p>
<p>In this work, we focus mainly on spanning networks that are trees and we
focus on problems in the plane ($d=2$). We show that any spanning tree that
minimizes the Wiener index has non-crossing edges in the plane. Then, we use
this fact to devise an $O(n^4)$-time algorithm that constructs a spanning tree
of minimum Wiener index for points in convex position. We also prove that the
problem of computing a spanning tree on $P$ whose Wiener index is at most $W$,
while having total (Euclidean) weight at most $B$, is NP-hard.
</p>
<p>Computing a tree that minimizes the Wiener index has been studied in the area
of communication networks, where it is known as the optimum communication
spanning tree problem.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-03T01:30:00Z">Friday, March 03 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.01400'>Coresets for Clustering in Geometric Intersection Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sayan Bandyapadhyay, Fedor V. Fomin, Tanmay Inamdar</p><p>Designing coresets--small-space sketches of the data preserving cost of the
solutions within $(1\pm \epsilon)$-approximate factor--is an important research
direction in the study of center-based $k$-clustering problems, such as
$k$-means or $k$-median. Feldman and Langberg [STOC'11] have shown that for
$k$-clustering of $n$ points in general metrics, it is possible to obtain
coresets whose size depends logarithmically in $n$. Moreover, such a dependency
in $n$ is inevitable in general metrics. A significant amount of recent work in
the area is devoted to obtaining coresests whose sizes are independent of $n$
(i.e., ``small'' coresets) for special metrics, like $d$-dimensional Euclidean
spaces, doubling metrics, metrics of graphs of bounded treewidth, or those
excluding a fixed minor.
</p>
<p>In this paper, we provide the first constructions of small coresets for
$k$-clustering in the metrics induced by geometric intersection graphs, such as
Euclidean-weighted Unit Disk/Square Graphs. These constructions follow from a
general theorem that identifies two canonical properties of a graph metric
sufficient for obtaining small coresets. The proof of our theorem builds on the
recent work of Cohen-Addad, Saulpic, and Schwiegelshohn [STOC '21], which
ensures small-sized coresets conditioned on the existence of an interesting set
of centers, called ``centroid set''. The main technical contribution of our
work is the proof of the existence of such a small-sized centroid set for
graphs that satisfy the two canonical geometric properties. The new coreset
construction helps to design the first $(1+\epsilon)$-approximation for
center-based clustering problems in UDGs and USGs, that is fixed-parameter
tractable in $k$ and $\epsilon$ (FPT-AS).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bandyapadhyay_S/0/1/0/all/0/1">Sayan Bandyapadhyay</a>, <a href="http://arxiv.org/find/cs/1/au:+Fomin_F/0/1/0/all/0/1">Fedor V. Fomin</a>, <a href="http://arxiv.org/find/cs/1/au:+Inamdar_T/0/1/0/all/0/1">Tanmay Inamdar</a></p><p>Designing coresets--small-space sketches of the data preserving cost of the
solutions within $(1\pm \epsilon)$-approximate factor--is an important research
direction in the study of center-based $k$-clustering problems, such as
$k$-means or $k$-median. Feldman and Langberg [STOC'11] have shown that for
$k$-clustering of $n$ points in general metrics, it is possible to obtain
coresets whose size depends logarithmically in $n$. Moreover, such a dependency
in $n$ is inevitable in general metrics. A significant amount of recent work in
the area is devoted to obtaining coresests whose sizes are independent of $n$
(i.e., ``small'' coresets) for special metrics, like $d$-dimensional Euclidean
spaces, doubling metrics, metrics of graphs of bounded treewidth, or those
excluding a fixed minor.
</p>
<p>In this paper, we provide the first constructions of small coresets for
$k$-clustering in the metrics induced by geometric intersection graphs, such as
Euclidean-weighted Unit Disk/Square Graphs. These constructions follow from a
general theorem that identifies two canonical properties of a graph metric
sufficient for obtaining small coresets. The proof of our theorem builds on the
recent work of Cohen-Addad, Saulpic, and Schwiegelshohn [STOC '21], which
ensures small-sized coresets conditioned on the existence of an interesting set
of centers, called ``centroid set''. The main technical contribution of our
work is the proof of the existence of such a small-sized centroid set for
graphs that satisfy the two canonical geometric properties. The new coreset
construction helps to design the first $(1+\epsilon)$-approximation for
center-based clustering problems in UDGs and USGs, that is fixed-parameter
tractable in $k$ and $\epsilon$ (FPT-AS).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-03T01:30:00Z">Friday, March 03 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.00791'>Scarf's algorithm and stable marriages</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yuri Faenza, Chengyue He, Jay Sethuraman</p><p>Scarf's algorithm gives a pivoting procedure to find a special vertex -- a
dominating vertex -- in down-monotone polytopes. This paper studies the
behavior of Scarf's algorithm when employed to find stable matchings in
bipartite graphs. First, it proves that Scarf's algorithm can be implemented to
run in polynomial time, showing the first positive result on its runtime in
significant settings. Second, it shows an infinite family of instances where,
no matter the pivoting rule and runtime, Scarf's algorithm outputs a matching
from an exponentially small subset of all stable matchings, thus showing a
structural weakness of the approach.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Faenza_Y/0/1/0/all/0/1">Yuri Faenza</a>, <a href="http://arxiv.org/find/math/1/au:+He_C/0/1/0/all/0/1">Chengyue He</a>, <a href="http://arxiv.org/find/math/1/au:+Sethuraman_J/0/1/0/all/0/1">Jay Sethuraman</a></p><p>Scarf's algorithm gives a pivoting procedure to find a special vertex -- a
dominating vertex -- in down-monotone polytopes. This paper studies the
behavior of Scarf's algorithm when employed to find stable matchings in
bipartite graphs. First, it proves that Scarf's algorithm can be implemented to
run in polynomial time, showing the first positive result on its runtime in
significant settings. Second, it shows an infinite family of instances where,
no matter the pivoting rule and runtime, Scarf's algorithm outputs a matching
from an exponentially small subset of all stable matchings, thus showing a
structural weakness of the approach.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-03T01:30:00Z">Friday, March 03 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.00811'>Parallel and Distributed Exact Single-Source Shortest Paths with Negative Edge Weights</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Vikrant Ashvinkumar, Aaron Bernstein, Nairen Cao, Christoph Grunau, Bernhard Haeupler, Yonggang Jiang, Danupon Nanongkai, Hsin Hao Su</p><p>This paper presents parallel and distributed algorithms for single-source
shortest paths when edges can have negative weights (negative-weight SSSP). We
show a framework that reduces negative-weight SSSP in either setting to
$n^{o(1)}$ calls to any SSSP algorithm that works with a virtual source. More
specifically, for a graph with $m$ edges, $n$ vertices, undirected hop-diameter
$D$, and polynomially bounded integer edge weights, we show randomized
algorithms for negative-weight SSSP with (i) $W_{SSSP}(m,n)n^{o(1)}$ work and
$S_{SSSP}(m,n)n^{o(1)}$ span, given access to an SSSP algorithm with
$W_{SSSP}(m,n)$ work and $S_{SSSP}(m,n)$ span in the parallel model, (ii)
$T_{SSSP}(n,D)n^{o(1)}$, given access to an SSSP algorithm that takes
$T_{SSSP}(n,D)$ rounds in $\mathsf{CONGEST}$. This work builds off the recent
result of [Bernstein, Nanongkai, Wulff-Nilsen, FOCS'22], which gives a
near-linear time algorithm for negative-weight SSSP in the sequential setting.
</p>
<p>Using current state-of-the-art SSSP algorithms yields randomized algorithms
for negative-weight SSSP with (i) $m^{1+o(1)}$ work and $n^{1/2+o(1)}$ span in
the parallel model, (ii) $(n^{2/5}D^{2/5} + \sqrt{n} + D)n^{o(1)}$ rounds in
$\mathsf{CONGEST}$.
</p>
<p>Our main technical contribution is an efficient reduction for computing a
low-diameter decomposition (LDD) of directed graphs to computations of SSSP
with a virtual source. Efficiently computing an LDD has heretofore only been
known for undirected graphs in both the parallel and distributed models. The
LDD is a crucial step of the algorithm in [Bernstein, Nanongkai, Wulff-Nilsen,
FOCS'22], and we think that its applications to other problems in parallel and
distributed models are far from being exhausted.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ashvinkumar_V/0/1/0/all/0/1">Vikrant Ashvinkumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernstein_A/0/1/0/all/0/1">Aaron Bernstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1">Nairen Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Grunau_C/0/1/0/all/0/1">Christoph Grunau</a>, <a href="http://arxiv.org/find/cs/1/au:+Haeupler_B/0/1/0/all/0/1">Bernhard Haeupler</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yonggang Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nanongkai_D/0/1/0/all/0/1">Danupon Nanongkai</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hsin Hao Su</a></p><p>This paper presents parallel and distributed algorithms for single-source
shortest paths when edges can have negative weights (negative-weight SSSP). We
show a framework that reduces negative-weight SSSP in either setting to
$n^{o(1)}$ calls to any SSSP algorithm that works with a virtual source. More
specifically, for a graph with $m$ edges, $n$ vertices, undirected hop-diameter
$D$, and polynomially bounded integer edge weights, we show randomized
algorithms for negative-weight SSSP with (i) $W_{SSSP}(m,n)n^{o(1)}$ work and
$S_{SSSP}(m,n)n^{o(1)}$ span, given access to an SSSP algorithm with
$W_{SSSP}(m,n)$ work and $S_{SSSP}(m,n)$ span in the parallel model, (ii)
$T_{SSSP}(n,D)n^{o(1)}$, given access to an SSSP algorithm that takes
$T_{SSSP}(n,D)$ rounds in $\mathsf{CONGEST}$. This work builds off the recent
result of [Bernstein, Nanongkai, Wulff-Nilsen, FOCS'22], which gives a
near-linear time algorithm for negative-weight SSSP in the sequential setting.
</p>
<p>Using current state-of-the-art SSSP algorithms yields randomized algorithms
for negative-weight SSSP with (i) $m^{1+o(1)}$ work and $n^{1/2+o(1)}$ span in
the parallel model, (ii) $(n^{2/5}D^{2/5} + \sqrt{n} + D)n^{o(1)}$ rounds in
$\mathsf{CONGEST}$.
</p>
<p>Our main technical contribution is an efficient reduction for computing a
low-diameter decomposition (LDD) of directed graphs to computations of SSSP
with a virtual source. Efficiently computing an LDD has heretofore only been
known for undirected graphs in both the parallel and distributed models. The
LDD is a crucial step of the algorithm in [Bernstein, Nanongkai, Wulff-Nilsen,
FOCS'22], and we think that its applications to other problems in parallel and
distributed models are far from being exhausted.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-03T01:30:00Z">Friday, March 03 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.00837'>Predictive Flows for Faster Ford-Fulkerson</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sami Davies, Benjamin Moseley, Sergei Vassilvitskii, Yuyan Wang</p><p>Recent work has shown that leveraging learned predictions can improve the
running time of algorithms for bipartite matching and similar combinatorial
problems. In this work, we build on this idea to improve the performance of the
widely used Ford-Fulkerson algorithm for computing maximum flows by seeding
Ford-Fulkerson with predicted flows. Our proposed method offers strong
theoretical performance in terms of the quality of the prediction. We then
consider image segmentation, a common use-case of flows in computer vision, and
complement our theoretical analysis with strong empirical results.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Davies_S/0/1/0/all/0/1">Sami Davies</a>, <a href="http://arxiv.org/find/cs/1/au:+Moseley_B/0/1/0/all/0/1">Benjamin Moseley</a>, <a href="http://arxiv.org/find/cs/1/au:+Vassilvitskii_S/0/1/0/all/0/1">Sergei Vassilvitskii</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuyan Wang</a></p><p>Recent work has shown that leveraging learned predictions can improve the
running time of algorithms for bipartite matching and similar combinatorial
problems. In this work, we build on this idea to improve the performance of the
widely used Ford-Fulkerson algorithm for computing maximum flows by seeding
Ford-Fulkerson with predicted flows. Our proposed method offers strong
theoretical performance in terms of the quality of the prediction. We then
consider image segmentation, a common use-case of flows in computer vision, and
complement our theoretical analysis with strong empirical results.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-03T01:30:00Z">Friday, March 03 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.01078'>Pandora's Problem with Combinatorial Cost</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ben Berger, Tomer Ezra, Michal Feldman, Federico Fusco</p><p>Pandora's problem is a fundamental model in economics that studies optimal
search strategies under costly inspection. In this paper we initiate the study
of Pandora's problem with combinatorial costs, capturing many real-life
scenarios where search cost is non-additive. Weitzman's celebrated algorithm
[1979] establishes the remarkable result that, for additive costs, the optimal
search strategy is non-adaptive and computationally feasible.
</p>
<p>We inquire to which extent this structural and computational simplicity
extends beyond additive cost functions. Our main result is that the class of
submodular cost functions admits an optimal strategy that follows a fixed,
non-adaptive order, thus preserving the structural simplicity of additive cost
functions. In contrast, for the more general class of subadditive (or even XOS)
cost functions, the optimal strategy may already need to determine the search
order adaptively. On the computational side, obtaining any approximation to the
optimal utility requires super polynomially many queries to the cost function,
even for a strict subclass of submodular cost functions.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Berger_B/0/1/0/all/0/1">Ben Berger</a>, <a href="http://arxiv.org/find/cs/1/au:+Ezra_T/0/1/0/all/0/1">Tomer Ezra</a>, <a href="http://arxiv.org/find/cs/1/au:+Feldman_M/0/1/0/all/0/1">Michal Feldman</a>, <a href="http://arxiv.org/find/cs/1/au:+Fusco_F/0/1/0/all/0/1">Federico Fusco</a></p><p>Pandora's problem is a fundamental model in economics that studies optimal
search strategies under costly inspection. In this paper we initiate the study
of Pandora's problem with combinatorial costs, capturing many real-life
scenarios where search cost is non-additive. Weitzman's celebrated algorithm
[1979] establishes the remarkable result that, for additive costs, the optimal
search strategy is non-adaptive and computationally feasible.
</p>
<p>We inquire to which extent this structural and computational simplicity
extends beyond additive cost functions. Our main result is that the class of
submodular cost functions admits an optimal strategy that follows a fixed,
non-adaptive order, thus preserving the structural simplicity of additive cost
functions. In contrast, for the more general class of subadditive (or even XOS)
cost functions, the optimal strategy may already need to determine the search
order adaptively. On the computational side, obtaining any approximation to the
optimal utility requires super polynomially many queries to the cost function,
even for a strict subclass of submodular cost functions.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-03T01:30:00Z">Friday, March 03 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.01188'>Quantum Channel Certification with Incoherent Strategies</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Omar Fawzi, Nicolas Flammarion, Aur&#xe9;lien Garivier, Aadil Oufkir</p><p>In the problem of quantum channel certification, we have black box access to
a quantum process and would like to decide if this process matches some
predefined specification or is $\varepsilon$-far from this specification. The
objective is to achieve this task while minimizing the number of times the
black box is used.
</p>
<p>Here, we focus on optimal incoherent strategies for two relevant extreme
cases of channel certification. The first one is when the predefined
specification is a unitary channel, e.g., a gate in a quantum circuit.
</p>
<p>In this case, we show that testing whether the black box is described by a
fixed unitary operator in dimension $d$ or $\varepsilon$-far from it in the
trace norm requires $\Theta(d/\varepsilon^2)$ uses of the black box. The second
setting we consider is when the predefined specification is a completely
depolarizing channel with input dimension $d_{\text{in}}$ and output dimension
$d_{\text{out}}$.
</p>
<p>In this case, we prove that, in the non-adaptive setting,
$\tilde{\Theta}(d_{\text{in}}^2d_{\text{out}}^{1.5}/\varepsilon^2)$ uses of the
channel are necessary and sufficient to verify whether it is equal to the
depolarizing channel or $\varepsilon$-far from it in the diamond norm.
</p>
<p>Finally, we prove a lower bound of
$\Omega(d_{\text{in}}^2d_{\text{out}}/\varepsilon^2)$ for this problem in the
adaptive setting. Note that the special case $d_{\text{in}} = 1$ corresponds to
the well-studied quantum state certification problem.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Fawzi_O/0/1/0/all/0/1">Omar Fawzi</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Flammarion_N/0/1/0/all/0/1">Nicolas Flammarion</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Garivier_A/0/1/0/all/0/1">Aur&#xe9;lien Garivier</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Oufkir_A/0/1/0/all/0/1">Aadil Oufkir</a></p><p>In the problem of quantum channel certification, we have black box access to
a quantum process and would like to decide if this process matches some
predefined specification or is $\varepsilon$-far from this specification. The
objective is to achieve this task while minimizing the number of times the
black box is used.
</p>
<p>Here, we focus on optimal incoherent strategies for two relevant extreme
cases of channel certification. The first one is when the predefined
specification is a unitary channel, e.g., a gate in a quantum circuit.
</p>
<p>In this case, we show that testing whether the black box is described by a
fixed unitary operator in dimension $d$ or $\varepsilon$-far from it in the
trace norm requires $\Theta(d/\varepsilon^2)$ uses of the black box. The second
setting we consider is when the predefined specification is a completely
depolarizing channel with input dimension $d_{\text{in}}$ and output dimension
$d_{\text{out}}$.
</p>
<p>In this case, we prove that, in the non-adaptive setting,
$\tilde{\Theta}(d_{\text{in}}^2d_{\text{out}}^{1.5}/\varepsilon^2)$ uses of the
channel are necessary and sufficient to verify whether it is equal to the
depolarizing channel or $\varepsilon$-far from it in the diamond norm.
</p>
<p>Finally, we prove a lower bound of
$\Omega(d_{\text{in}}^2d_{\text{out}}/\varepsilon^2)$ for this problem in the
adaptive setting. Note that the special case $d_{\text{in}} = 1$ corresponds to
the well-studied quantum state certification problem.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-03T01:30:00Z">Friday, March 03 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.01256'>Choosing Public Datasets for Private Machine Learning via Gradient Subspace Distance</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xin Gu, Gautam Kamath, Zhiwei Steven Wu</p><p>Differentially private stochastic gradient descent privatizes model training
by injecting noise into each iteration, where the noise magnitude increases
with the number of model parameters. Recent works suggest that we can reduce
the noise by leveraging public data for private machine learning, by projecting
gradients onto a subspace prescribed by the public data. However, given a
choice of public datasets, it is not a priori clear which one may be most
appropriate for the private task. We give an algorithm for selecting a public
dataset by measuring a low-dimensional subspace distance between gradients of
the public and private examples. We provide theoretical analysis demonstrating
that the excess risk scales with this subspace distance. This distance is easy
to compute and robust to modifications in the setting. Empirical evaluation
shows that trained model accuracy is monotone in this distance.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/stat/1/au:+Gu_X/0/1/0/all/0/1">Xin Gu</a>, <a href="http://arxiv.org/find/stat/1/au:+Kamath_G/0/1/0/all/0/1">Gautam Kamath</a>, <a href="http://arxiv.org/find/stat/1/au:+Wu_Z/0/1/0/all/0/1">Zhiwei Steven Wu</a></p><p>Differentially private stochastic gradient descent privatizes model training
by injecting noise into each iteration, where the noise magnitude increases
with the number of model parameters. Recent works suggest that we can reduce
the noise by leveraging public data for private machine learning, by projecting
gradients onto a subspace prescribed by the public data. However, given a
choice of public datasets, it is not a priori clear which one may be most
appropriate for the private task. We give an algorithm for selecting a public
dataset by measuring a low-dimensional subspace distance between gradients of
the public and private examples. We provide theoretical analysis demonstrating
that the excess risk scales with this subspace distance. This distance is easy
to compute and robust to modifications in the setting. Empirical evaluation
shows that trained model accuracy is monotone in this distance.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-03T01:30:00Z">Friday, March 03 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.01290'>Solving Distance-constrained Labeling Problems for Small Diameter Graphs via TSP</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Tesshu Hanaka, Hirotaka Ono, Kosuke Sugiyama</p><p>In this paper, we give a simple polynomial-time reduction of {L(p)-Labeling}
on graphs with a small diameter to {Metric (Path) TSP}, which enables us to use
numerous results on {(Metric) TSP}. On the practical side, we can utilize
various high-performance heuristics for TSP, such as Concordo and LKH, to solve
our problem. On the theoretical side, we can see that the problem for any p
under this framework is 1.5-approximable, and it can be solved by the Held-Karp
algorithm in O(2^n n^2) time, where n is the number of vertices, and so on.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Hanaka_T/0/1/0/all/0/1">Tesshu Hanaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Ono_H/0/1/0/all/0/1">Hirotaka Ono</a>, <a href="http://arxiv.org/find/cs/1/au:+Sugiyama_K/0/1/0/all/0/1">Kosuke Sugiyama</a></p><p>In this paper, we give a simple polynomial-time reduction of {L(p)-Labeling}
on graphs with a small diameter to {Metric (Path) TSP}, which enables us to use
numerous results on {(Metric) TSP}. On the practical side, we can utilize
various high-performance heuristics for TSP, such as Concordo and LKH, to solve
our problem. On the theoretical side, we can see that the problem for any p
under this framework is 1.5-approximable, and it can be solved by the Held-Karp
algorithm in O(2^n n^2) time, where n is the number of vertices, and so on.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-03T01:30:00Z">Friday, March 03 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.01414'>Improved Algorithms for Monotone Moldable Job Scheduling using Compression and Convolution</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Kilian Grage, Klaus Jansen, Felix Ohnesorge</p><p>In the moldable job scheduling problem one has to assign a set of $n$ jobs to
$m$ machines, in order to minimize the time it takes to process all jobs. Each
job is moldable, so it can be assigned not only to one but any number of the
equal machines. We assume that the work of each job is monotone and that jobs
can be placed non-contiguously. In this work we present a $(\frac 3 2 +
\epsilon)$-approximation algorithm with a worst-case runtime of ${O(n
\log^2(\frac 1 \epsilon + \frac {\log (\epsilon m)} \epsilon) +
\frac{n}{\epsilon} \log(\frac 1 \epsilon) {\log (\epsilon m)})}$ when $m\le
16n$. This is an improvement over the best known algorithm of the same quality
by a factor of $\frac 1 \epsilon$ and several logarithmic dependencies. We
complement this result with an improved FPTAS with running time $O(n
\log^2(\frac 1 \epsilon + \frac {\log (\epsilon m)} \epsilon))$ for instances
with many machines $m&gt; 8\frac n \epsilon$. This yields a $\frac 3
2$-approximation with runtime $O(n \log^2(\log m))$ when $m&gt;16n$.
</p>
<p>We achieve these results through one new core observation: In an
approximation setting one does not need to consider all $m$ possible allotments
for each job. We will show that we can reduce the number of relevant allotments
for each job from $m$ to $O(\frac 1 \epsilon + \frac {\log (\epsilon
m)}{\epsilon})$. Using this observation immediately yields the improved FPTAS.
For the other result we use a reduction to the knapsack problem first
introduced by Mouni\'e, Rapine and Trystram. We use the reduced number of
machines to give a new elaborate rounding scheme and define a modified version
of this this knapsack instance. This in turn allows for the application of a
convolution based algorithm by Axiotis and Tzamos. We further back our
theoretical results through a practical implementation and compare our
algorithm to the previously known best result.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Grage_K/0/1/0/all/0/1">Kilian Grage</a>, <a href="http://arxiv.org/find/cs/1/au:+Jansen_K/0/1/0/all/0/1">Klaus Jansen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ohnesorge_F/0/1/0/all/0/1">Felix Ohnesorge</a></p><p>In the moldable job scheduling problem one has to assign a set of $n$ jobs to
$m$ machines, in order to minimize the time it takes to process all jobs. Each
job is moldable, so it can be assigned not only to one but any number of the
equal machines. We assume that the work of each job is monotone and that jobs
can be placed non-contiguously. In this work we present a $(\frac 3 2 +
\epsilon)$-approximation algorithm with a worst-case runtime of ${O(n
\log^2(\frac 1 \epsilon + \frac {\log (\epsilon m)} \epsilon) +
\frac{n}{\epsilon} \log(\frac 1 \epsilon) {\log (\epsilon m)})}$ when $m\le
16n$. This is an improvement over the best known algorithm of the same quality
by a factor of $\frac 1 \epsilon$ and several logarithmic dependencies. We
complement this result with an improved FPTAS with running time $O(n
\log^2(\frac 1 \epsilon + \frac {\log (\epsilon m)} \epsilon))$ for instances
with many machines $m&gt; 8\frac n \epsilon$. This yields a $\frac 3
2$-approximation with runtime $O(n \log^2(\log m))$ when $m&gt;16n$.
</p>
<p>We achieve these results through one new core observation: In an
approximation setting one does not need to consider all $m$ possible allotments
for each job. We will show that we can reduce the number of relevant allotments
for each job from $m$ to $O(\frac 1 \epsilon + \frac {\log (\epsilon
m)}{\epsilon})$. Using this observation immediately yields the improved FPTAS.
For the other result we use a reduction to the knapsack problem first
introduced by Mouni\'e, Rapine and Trystram. We use the reduced number of
machines to give a new elaborate rounding scheme and define a modified version
of this this knapsack instance. This in turn allows for the application of a
convolution based algorithm by Axiotis and Tzamos. We further back our
theoretical results through a practical implementation and compare our
algorithm to the previously known best result.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-03T01:30:00Z">Friday, March 03 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.01417'>Distributed Deep Multilevel Graph Partitioning</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Peter Sanders, Daniel Seemaier</p><p>We describe the engineering of the distributed-memory multilevel graph
partitioner dKaMinPar. It scales to (at least) 8192 cores while achieving
partitioning quality comparable to widely used sequential and shared-memory
graph partitioners. In comparison, previous distributed graph partitioners
scale only in more restricted scenarios and often induce a considerable quality
penalty compared to non-distributed partitioners. When partitioning into a
large number of blocks, they even produce infeasible solution that violate the
balancing constraint. dKaMinPar achieves its robustness by a scalable
distributed implementation of the deep-multilevel scheme for graph
partitioning. Crucially, this includes new algorithms for balancing during
refinement and coarsening.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Sanders_P/0/1/0/all/0/1">Peter Sanders</a>, <a href="http://arxiv.org/find/cs/1/au:+Seemaier_D/0/1/0/all/0/1">Daniel Seemaier</a></p><p>We describe the engineering of the distributed-memory multilevel graph
partitioner dKaMinPar. It scales to (at least) 8192 cores while achieving
partitioning quality comparable to widely used sequential and shared-memory
graph partitioners. In comparison, previous distributed graph partitioners
scale only in more restricted scenarios and often induce a considerable quality
penalty compared to non-distributed partitioners. When partitioning into a
large number of blocks, they even produce infeasible solution that violate the
balancing constraint. dKaMinPar achieves its robustness by a scalable
distributed implementation of the deep-multilevel scheme for graph
partitioning. Crucially, this includes new algorithms for balancing during
refinement and coarsening.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-03T01:30:00Z">Friday, March 03 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.01453'>Improved Space Bounds for Learning with Experts</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Anders Aamand, Justin Y. Chen, Huy L&#xea; Nguyen, Sandeep Silwal</p><p>We give improved tradeoffs between space and regret for the online learning
with expert advice problem over $T$ days with $n$ experts. Given a space budget
of $n^{\delta}$ for $\delta \in (0,1)$, we provide an algorithm achieving
regret $\tilde{O}(n^2 T^{1/(1+\delta)})$, improving upon the regret bound
$\tilde{O}(n^2 T^{2/(2+\delta)})$ in the recent work of [PZ23]. The improvement
is particularly salient in the regime $\delta \rightarrow 1$ where the regret
of our algorithm approaches $\tilde{O}_n(\sqrt{T})$, matching the $T$
dependence in the standard online setting without space restrictions.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Aamand_A/0/1/0/all/0/1">Anders Aamand</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Justin Y. Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Huy L&#xea; Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Silwal_S/0/1/0/all/0/1">Sandeep Silwal</a></p><p>We give improved tradeoffs between space and regret for the online learning
with expert advice problem over $T$ days with $n$ experts. Given a space budget
of $n^{\delta}$ for $\delta \in (0,1)$, we provide an algorithm achieving
regret $\tilde{O}(n^2 T^{1/(1+\delta)})$, improving upon the regret bound
$\tilde{O}(n^2 T^{2/(2+\delta)})$ in the recent work of [PZ23]. The improvement
is particularly salient in the regime $\delta \rightarrow 1$ where the regret
of our algorithm approaches $\tilde{O}_n(\sqrt{T})$, matching the $T$
dependence in the standard online setting without space restrictions.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-03T01:30:00Z">Friday, March 03 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.01478'>Faster exact and approximation algorithms for packing and covering matroids via push-relabel</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Kent Quanrud</p><p>Matroids are a fundamental object of study in combinatorial optimization.
Three closely related and important problems involving matroids are maximizing
the size of the union of $k$ independent sets (that is, $k$-fold matroid
union), computing $k$ disjoint bases (a.k.a. matroid base packing), and
covering the elements by $k$ bases (a.k.a. matroid base covering). These
problems generalize naturally to integral and real-valued capacities on the
elements. This work develops faster exact and/or approximation problems for
these and some other closely related problems such as optimal reinforcement and
matroid membership. We obtain improved running times both for general matroids
in the independence oracle model and for the graphic matroid. The main thrust
of our improvements comes from developing a faster and unifying push-relabel
algorithm for the integer-capacitated versions of these problems, building on
previous work by Frank and Mikl\'os [FM12]. We then build on this algorithm in
two directions. First we develop a faster augmenting path subroutine for
$k$-fold matroid union that, when appended to an approximation version of the
push-relabel algorithm, gives a faster exact algorithm for some parameters of
$k$. In particular we obtain a subquadratic-query running time in the
uncapacitated setting for the three basic problems listed above. We also obtain
faster approximation algorithms for these problems with real-valued capacities
by reducing to small integral capacities via randomized rounding. To this end,
we develop a new randomized rounding technique for base covering problems in
matroids that may also be of independent interest.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Quanrud_K/0/1/0/all/0/1">Kent Quanrud</a></p><p>Matroids are a fundamental object of study in combinatorial optimization.
Three closely related and important problems involving matroids are maximizing
the size of the union of $k$ independent sets (that is, $k$-fold matroid
union), computing $k$ disjoint bases (a.k.a. matroid base packing), and
covering the elements by $k$ bases (a.k.a. matroid base covering). These
problems generalize naturally to integral and real-valued capacities on the
elements. This work develops faster exact and/or approximation problems for
these and some other closely related problems such as optimal reinforcement and
matroid membership. We obtain improved running times both for general matroids
in the independence oracle model and for the graphic matroid. The main thrust
of our improvements comes from developing a faster and unifying push-relabel
algorithm for the integer-capacitated versions of these problems, building on
previous work by Frank and Mikl\'os [FM12]. We then build on this algorithm in
two directions. First we develop a faster augmenting path subroutine for
$k$-fold matroid union that, when appended to an approximation version of the
push-relabel algorithm, gives a faster exact algorithm for some parameters of
$k$. In particular we obtain a subquadratic-query running time in the
uncapacitated setting for the three basic problems listed above. We also obtain
faster approximation algorithms for these problems with real-valued capacities
by reducing to small integral capacities via randomized rounding. To this end,
we develop a new randomized rounding technique for base covering problems in
matroids that may also be of independent interest.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-03T01:30:00Z">Friday, March 03 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Thursday, March 02
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/020'>TR23-020 |  Certified Randomness from Quantum Supremacy | 

	Scott Aaronson, 

	Shih-Han Hung</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We propose an application for near-term quantum devices: namely, generating cryptographically certified random bits, to use (for example) in proof-of-stake cryptocurrencies. Our protocol repurposes the existing &quot;quantum supremacy&quot; experiments, based on random circuit sampling, that Google and USTC have successfully carried out starting in 2019. We show that, whenever the outputs of these experiments pass the now-standard Linear Cross-Entropy Benchmark (LXEB), under plausible hardness assumptions they necessarily contain $\Omega(n)$ min-entropy, where $n$ is the number of qubits. To achieve a net gain in randomness, we use a small random seed to produce pseudorandom challenge circuits. In response to the challenge circuits, the quantum computer generates output strings that, after verification, can then be fed into a randomness extractor to produce certified nearly-uniform bits---thereby &quot;bootstrapping&quot; from pseudorandomness to genuine randomness. We prove our protocol sound in two senses: (i) under a hardness assumption called Long List Quantum Supremacy Verification, which we justify in the random oracle model, and (ii) unconditionally in the random oracle model against an eavesdropper who could share arbitrary entanglement with the device. (Note that our protocol&#39;s output is unpredictable even to a computationally unbounded adversary who can see the random oracle.) Currently, the central drawback of our protocol is the exponential cost of verification, which in practice will limit its implementation to at most $n\sim 60$ qubits, a regime where attacks are expensive but not impossible. Modulo that drawback, our protocol appears to be the only practical application of quantum computing that both requires a QC and is physically realizable today.
        
        </div>

        <div class='tr-article-summary'>
        
          
          We propose an application for near-term quantum devices: namely, generating cryptographically certified random bits, to use (for example) in proof-of-stake cryptocurrencies. Our protocol repurposes the existing &quot;quantum supremacy&quot; experiments, based on random circuit sampling, that Google and USTC have successfully carried out starting in 2019. We show that, whenever the outputs of these experiments pass the now-standard Linear Cross-Entropy Benchmark (LXEB), under plausible hardness assumptions they necessarily contain $\Omega(n)$ min-entropy, where $n$ is the number of qubits. To achieve a net gain in randomness, we use a small random seed to produce pseudorandom challenge circuits. In response to the challenge circuits, the quantum computer generates output strings that, after verification, can then be fed into a randomness extractor to produce certified nearly-uniform bits---thereby &quot;bootstrapping&quot; from pseudorandomness to genuine randomness. We prove our protocol sound in two senses: (i) under a hardness assumption called Long List Quantum Supremacy Verification, which we justify in the random oracle model, and (ii) unconditionally in the random oracle model against an eavesdropper who could share arbitrary entanglement with the device. (Note that our protocol&#39;s output is unpredictable even to a computationally unbounded adversary who can see the random oracle.) Currently, the central drawback of our protocol is the exponential cost of verification, which in practice will limit its implementation to at most $n\sim 60$ qubits, a regime where attacks are expensive but not impossible. Modulo that drawback, our protocol appears to be the only practical application of quantum computing that both requires a QC and is physically realizable today.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-02T23:10:26Z">Thursday, March 02 2023, 23:10</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/019'>TR23-019 |  Theory of Unconditional Pseudorandom Generators | 

	Pooya Hatami, 

	William Hoza</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          This is a survey of unconditional *pseudorandom generators* (PRGs). A PRG uses a short, truly random seed to generate a long, &quot;pseudorandom&quot; sequence of bits. To be more specific, for each restricted model of computation (e.g., bounded-depth circuits or read-once branching programs), we would like to design a PRG that &quot;fools&quot; the model, meaning that every function computable in the model behaves approximately the same when we plug in pseudorandom bits from the PRG as it does when we plug in truly random bits. In this survey, we discuss four major paradigms for designing PRGs:

- We present several PRGs based on $k$-wise uniform generators, small-bias generators, and simple combinations thereof, including proofs of Viola&#39;s theorem on fooling low-degree polynomials (Comput. Complexity 2009) and Braverman&#39;s theorem on fooling $\mathbf{AC}^0$ circuits (J. ACM 2010).

- We present several PRGs based on &quot;recycling&quot; random bits to take advantage of communication bottlenecks, such as the Impagliazzo-Nisan-Wigderson generator (STOC 1994).

- We present connections between PRGs and computational hardness, including the Nisan-Wigderson framework for converting a hard Boolean function into a PRG (J. Comput. Syst. Sci. 1994).

- We present PRG frameworks based on random restrictions, including the &quot;polarizing random walks&quot; framework (Chattopadhyay, Hatami, Hosseini, and Lovett, Theory Comput. 2019).

We explain how to use these paradigms to construct PRGs that work *unconditionally*, with no unproven mathematical assumptions. The PRG constructions use ingredients such as finite field arithmetic, expander graphs, and randomness extractors. The analyses use techniques such as Fourier analysis, sandwiching approximators, and simplification-under-restrictions lemmas.
        
        </div>

        <div class='tr-article-summary'>
        
          
          This is a survey of unconditional *pseudorandom generators* (PRGs). A PRG uses a short, truly random seed to generate a long, &quot;pseudorandom&quot; sequence of bits. To be more specific, for each restricted model of computation (e.g., bounded-depth circuits or read-once branching programs), we would like to design a PRG that &quot;fools&quot; the model, meaning that every function computable in the model behaves approximately the same when we plug in pseudorandom bits from the PRG as it does when we plug in truly random bits. In this survey, we discuss four major paradigms for designing PRGs:

- We present several PRGs based on $k$-wise uniform generators, small-bias generators, and simple combinations thereof, including proofs of Viola&#39;s theorem on fooling low-degree polynomials (Comput. Complexity 2009) and Braverman&#39;s theorem on fooling $\mathbf{AC}^0$ circuits (J. ACM 2010).

- We present several PRGs based on &quot;recycling&quot; random bits to take advantage of communication bottlenecks, such as the Impagliazzo-Nisan-Wigderson generator (STOC 1994).

- We present connections between PRGs and computational hardness, including the Nisan-Wigderson framework for converting a hard Boolean function into a PRG (J. Comput. Syst. Sci. 1994).

- We present PRG frameworks based on random restrictions, including the &quot;polarizing random walks&quot; framework (Chattopadhyay, Hatami, Hosseini, and Lovett, Theory Comput. 2019).

We explain how to use these paradigms to construct PRGs that work *unconditionally*, with no unproven mathematical assumptions. The PRG constructions use ingredients such as finite field arithmetic, expander graphs, and randomness extractors. The analyses use techniques such as Fourier analysis, sandwiching approximators, and simplification-under-restrictions lemmas.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-02T23:09:23Z">Thursday, March 02 2023, 23:09</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://blog.computationalcomplexity.org/2023/03/goodbye-dilbert.html'>Goodbye Dilbert</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Scott Adams, creator of Dilbert, had a racist rant in a video he posted last week. As a result most newspapers that carried the comic strip are dropping Dilbert, including our local Chicago Tribune. I fully support these moves. Much as I believe in separating the art from the artist, it's different when the artist is living and profiting from their art.</p><p>So we need to say to Dilbert, making the end of an era. Dilbert started in 1989 as a strip that captured the absurdities of the work place in an anonymous tech company, predating movies like Office Space and shows like Better Off Ted and&nbsp;Silicon Valley. I used Dilbert strips (with permission) in my book, namely&nbsp;this strip to introduce Kolmogorov complexity and this strip to describe my research area. Just call me Dan.</p><p>Farewell to Dilbert, Dogbert, Wally, Alice, Asok, the pointy-haired boss and the rest. I won't miss Scott Adams, but I will miss his creations.</p><p>By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Scott Adams, creator of Dilbert, had a racist rant in a video he posted last week. As a result <a href="https://www.wsj.com/articles/newspapers-drop-dilbert-after-cartoonist-calls-black-americans-hate-group-21348ce1?st=mcxbs51a70by7i3&amp;reflink=desktopwebshare_permalink">most newspapers that carried the comic strip are dropping Dilbert</a>, including our local Chicago Tribune. I fully support these moves. Much as I believe in separating the art from the artist, it's different when the artist is living and profiting from their art.</p><p>So we need to say to Dilbert, making the end of an era. Dilbert started in 1989 as a strip that captured the absurdities of the work place in an anonymous tech company, predating movies like <a href="https://www.imdb.com/title/tt0151804/">Office Space</a> and shows like <a href="https://www.imdb.com/title/tt1235547/">Better Off Ted</a> and&nbsp;<a href="https://www.imdb.com/title/tt2575988/">Silicon Valley</a>. I used Dilbert strips (with permission) in my book, namely&nbsp;<a href="https://dilbert.com/strip/2001-10-25">this strip</a> to introduce Kolmogorov complexity and <a href="https://dilbert.com/strip/1997-12-22">this strip</a> to describe my research area. Just call me Dan.</p><p>Farewell to Dilbert, Dogbert, Wally, Alice, Asok, the pointy-haired boss and the rest. I won't miss Scott Adams, but I will miss his creations.</p><p class="authors">By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-02T13:19:00Z">Thursday, March 02 2023, 13:19</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/018'>TR23-018 |  Memory-Sample Lower Bounds for Learning with Classical-Quantum Hybrid Memory | 

	Qipeng Liu, 

	Ran Raz, 

	Wei Zhan</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          In a work by Raz (J. ACM and FOCS 16), it was proved that any algorithm for parity learning on $n$ bits requires either $\Omega(n^2)$ bits of classical memory or an exponential number (in~$n$) of random samples. A line of recent works continued that research direction and showed that for a large collection of classical learning tasks, either super-linear classical memory size or super-polynomially many samples are needed. All these works consider learning algorithms as classical branching programs, which perform classical computation within bounded memory.

However, these results do not capture all physical computational models, remarkably, quantum computers and the use of quantum memory. It leaves the possibility that a small piece of quantum memory could significantly reduce the need for classical memory or samples and thus completely change the nature of the classical learning task. Despite the recent research on the necessity of quantum memory for intrinsic quantum learning problems like shadow tomography and purity testing, the role of quantum memory in classical learning tasks remains obscure. 

In this work, we study classical learning tasks in the presence of quantum memory. We prove that any quantum algorithm with both, classical memory and quantum memory, for parity learning on $n$ bits, requires either $\Omega(n^2)$ bits of classical memory or $\Omega(n)$ bits of quantum  memory or an exponential number of samples. In other words, the memory-sample lower bound for parity learning remains qualitatively the same, even if the learning algorithm can use, in addition to the classical memory, a quantum memory of size $c n$ (for some constant $c&gt;0$).

Our result is more general and applies to many other classical learning tasks. Following previous works, we represent by the matrix $M: A \times X \to \{-1,1\}$ the following learning task. An unknown $x$ is sampled uniformly at random from a concept class $X$, and a learning algorithm tries to uncover $x$ by seeing streaming of random samples $(a_i, b_i = M(a_i, x))$ where for every $i$, $a_i\in A$ is chosen uniformly at random. Assume that $k,\ell,r$ are integers such that any submatrix of $M$ of at least $2^{-k}\cdot|A|$ rows and at least $2^{-\ell}\cdot|X|$ columns, has a bias of at most $2^{-r}$. We prove that any algorithm with classical and quantum hybrid memory for the learning problem corresponding to $M$ needs either (1) $\Omega(k \cdot \ell)$ bits of classical memory, or (2) $\Omega(r)$ qubits of quantum memory, or (3) $2^{\Omega(r)}$ random samples, to achieve a success probability at least $2^{-O(r)}$. 

Our results refute the possibility that a small amount of quantum memory significantly reduces the size of classical memory needed for efficient learning on these problems. Our results also imply improved security of several existing cryptographical protocols in the bounded-storage model (protocols that are based on parity learning on $n$ bits), proving that security holds even in the presence of a quantum adversary with at most $c n^2$ bits of classical memory and $c n$ bits of quantum memory (for some constant $c&gt;0$).
        
        </div>

        <div class='tr-article-summary'>
        
          
          In a work by Raz (J. ACM and FOCS 16), it was proved that any algorithm for parity learning on $n$ bits requires either $\Omega(n^2)$ bits of classical memory or an exponential number (in~$n$) of random samples. A line of recent works continued that research direction and showed that for a large collection of classical learning tasks, either super-linear classical memory size or super-polynomially many samples are needed. All these works consider learning algorithms as classical branching programs, which perform classical computation within bounded memory.

However, these results do not capture all physical computational models, remarkably, quantum computers and the use of quantum memory. It leaves the possibility that a small piece of quantum memory could significantly reduce the need for classical memory or samples and thus completely change the nature of the classical learning task. Despite the recent research on the necessity of quantum memory for intrinsic quantum learning problems like shadow tomography and purity testing, the role of quantum memory in classical learning tasks remains obscure. 

In this work, we study classical learning tasks in the presence of quantum memory. We prove that any quantum algorithm with both, classical memory and quantum memory, for parity learning on $n$ bits, requires either $\Omega(n^2)$ bits of classical memory or $\Omega(n)$ bits of quantum  memory or an exponential number of samples. In other words, the memory-sample lower bound for parity learning remains qualitatively the same, even if the learning algorithm can use, in addition to the classical memory, a quantum memory of size $c n$ (for some constant $c&gt;0$).

Our result is more general and applies to many other classical learning tasks. Following previous works, we represent by the matrix $M: A \times X \to \{-1,1\}$ the following learning task. An unknown $x$ is sampled uniformly at random from a concept class $X$, and a learning algorithm tries to uncover $x$ by seeing streaming of random samples $(a_i, b_i = M(a_i, x))$ where for every $i$, $a_i\in A$ is chosen uniformly at random. Assume that $k,\ell,r$ are integers such that any submatrix of $M$ of at least $2^{-k}\cdot|A|$ rows and at least $2^{-\ell}\cdot|X|$ columns, has a bias of at most $2^{-r}$. We prove that any algorithm with classical and quantum hybrid memory for the learning problem corresponding to $M$ needs either (1) $\Omega(k \cdot \ell)$ bits of classical memory, or (2) $\Omega(r)$ qubits of quantum memory, or (3) $2^{\Omega(r)}$ random samples, to achieve a success probability at least $2^{-O(r)}$. 

Our results refute the possibility that a small amount of quantum memory significantly reduces the size of classical memory needed for efficient learning on these problems. Our results also imply improved security of several existing cryptographical protocols in the bounded-storage model (protocols that are based on parity learning on $n$ bits), proving that security holds even in the presence of a quantum adversary with at most $c n^2$ bits of classical memory and $c n$ bits of quantum memory (for some constant $c&gt;0$).
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-02T04:17:11Z">Thursday, March 02 2023, 04:17</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.00209'>Memory-Sample Lower Bounds for Learning with Classical-Quantum Hybrid Memory</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Qipeng Liu, Ran Raz, Wei Zhan</p><p>In a work by Raz (J. ACM and FOCS 16), it was proved that any algorithm for
parity learning on $n$ bits requires either $\Omega(n^2)$ bits of classical
memory or an exponential number (in~$n$) of random samples. A line of recent
works continued that research direction and showed that for a large collection
of classical learning tasks, either super-linear classical memory size or
super-polynomially many samples are needed. However, these results do not
capture all physical computational models, remarkably, quantum computers and
the use of quantum memory. It leaves the possibility that a small piece of
quantum memory could significantly reduce the need for classical memory or
samples and thus completely change the nature of the classical learning task.
</p>
<p>In this work, we prove that any quantum algorithm with both, classical memory
and quantum memory, for parity learning on $n$ bits, requires either
$\Omega(n^2)$ bits of classical memory or $\Omega(n)$ bits of quantum memory or
an exponential number of samples. In other words, the memory-sample lower bound
for parity learning remains qualitatively the same, even if the learning
algorithm can use, in addition to the classical memory, a quantum memory of
size $c n$ (for some constant $c&gt;0$).
</p>
<p>Our results refute the possibility that a small amount of quantum memory
significantly reduces the size of classical memory needed for efficient
learning on these problems. Our results also imply improved security of several
existing cryptographical protocols in the bounded-storage model (protocols that
are based on parity learning on $n$ bits), proving that security holds even in
the presence of a quantum adversary with at most $c n^2$ bits of classical
memory and $c n$ bits of quantum memory (for some constant $c&gt;0$).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Liu_Q/0/1/0/all/0/1">Qipeng Liu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Raz_R/0/1/0/all/0/1">Ran Raz</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Zhan_W/0/1/0/all/0/1">Wei Zhan</a></p><p>In a work by Raz (J. ACM and FOCS 16), it was proved that any algorithm for
parity learning on $n$ bits requires either $\Omega(n^2)$ bits of classical
memory or an exponential number (in~$n$) of random samples. A line of recent
works continued that research direction and showed that for a large collection
of classical learning tasks, either super-linear classical memory size or
super-polynomially many samples are needed. However, these results do not
capture all physical computational models, remarkably, quantum computers and
the use of quantum memory. It leaves the possibility that a small piece of
quantum memory could significantly reduce the need for classical memory or
samples and thus completely change the nature of the classical learning task.
</p>
<p>In this work, we prove that any quantum algorithm with both, classical memory
and quantum memory, for parity learning on $n$ bits, requires either
$\Omega(n^2)$ bits of classical memory or $\Omega(n)$ bits of quantum memory or
an exponential number of samples. In other words, the memory-sample lower bound
for parity learning remains qualitatively the same, even if the learning
algorithm can use, in addition to the classical memory, a quantum memory of
size $c n$ (for some constant $c&gt;0$).
</p>
<p>Our results refute the possibility that a small amount of quantum memory
significantly reduces the size of classical memory needed for efficient
learning on these problems. Our results also imply improved security of several
existing cryptographical protocols in the bounded-storage model (protocols that
are based on parity learning on $n$ bits), proving that security holds even in
the presence of a quantum adversary with at most $c n^2$ bits of classical
memory and $c n$ bits of quantum memory (for some constant $c&gt;0$).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-02T01:30:00Z">Thursday, March 02 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.00109'>Linear Size Universal Point Sets for Classes of Planar Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Stefan Felsner, Hendrik Schrezenmaier, Felix Schr&#xf6;der, Raphael Steiner</p><p>A finite set $P$ of points in the plane is $n$-universal with respect to a
class $\mathcal{C}$ of planar graphs if every $n$-vertex graph in $\mathcal{C}$
admits a crossing-free straight-line drawing with vertices at points of $P$.
For the class of all planar graphs the best known upper bound on the size of a
universal point set is quadratic and the best known lower bound is linear in
$n$. Some classes of planar graphs are known to admit universal point sets of
near linear size, however, there are no truly linear bounds for interesting
classes beyond outerplanar graphs.
</p>
<p>In this paper, we show that there is a universal point set of size $2n-2$ for
the class of bipartite planar graphs with $n$ vertices. The same point set is
also universal for the class of $n$-vertex planar graphs of maximum degree $3$.
The point set used for the results is what we call an exploding double chain,
and we prove that this point set allows planar straight-line embeddings of many
more planar graphs, namely of all subgraphs of planar graphs admitting a
one-sided Hamiltonian cycle. The result for bipartite graphs also implies that
every $n$-vertex plane graph has a $1$-bend drawing all whose bends and
vertices are contained in a specific point set of size $4n-6$, this improves a
bound of $6n-10$ for the same problem by L\"offler and T\'oth.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Felsner_S/0/1/0/all/0/1">Stefan Felsner</a>, <a href="http://arxiv.org/find/cs/1/au:+Schrezenmaier_H/0/1/0/all/0/1">Hendrik Schrezenmaier</a>, <a href="http://arxiv.org/find/cs/1/au:+Schroder_F/0/1/0/all/0/1">Felix Schr&#xf6;der</a>, <a href="http://arxiv.org/find/cs/1/au:+Steiner_R/0/1/0/all/0/1">Raphael Steiner</a></p><p>A finite set $P$ of points in the plane is $n$-universal with respect to a
class $\mathcal{C}$ of planar graphs if every $n$-vertex graph in $\mathcal{C}$
admits a crossing-free straight-line drawing with vertices at points of $P$.
For the class of all planar graphs the best known upper bound on the size of a
universal point set is quadratic and the best known lower bound is linear in
$n$. Some classes of planar graphs are known to admit universal point sets of
near linear size, however, there are no truly linear bounds for interesting
classes beyond outerplanar graphs.
</p>
<p>In this paper, we show that there is a universal point set of size $2n-2$ for
the class of bipartite planar graphs with $n$ vertices. The same point set is
also universal for the class of $n$-vertex planar graphs of maximum degree $3$.
The point set used for the results is what we call an exploding double chain,
and we prove that this point set allows planar straight-line embeddings of many
more planar graphs, namely of all subgraphs of planar graphs admitting a
one-sided Hamiltonian cycle. The result for bipartite graphs also implies that
every $n$-vertex plane graph has a $1$-bend drawing all whose bends and
vertices are contained in a specific point set of size $4n-6$, this improves a
bound of $6n-10$ for the same problem by L\"offler and T\'oth.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-02T01:30:00Z">Thursday, March 02 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.00556'>A linear bound for the Colin de Verdi\'ere parameter $\mu$ for graphs embedded on surfaces</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Camille Lanuel, Francis Lazarus, Rudi Pendavingh</p><p>We provide a combinatorial and self-contained proof that for all graphs $G$
embedded on a surface $S$, the Colin de Verdi\`ere parameter $\mu(G)$ is upper
bounded by $7-2\chi(S)$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Lanuel_C/0/1/0/all/0/1">Camille Lanuel</a>, <a href="http://arxiv.org/find/math/1/au:+Lazarus_F/0/1/0/all/0/1">Francis Lazarus</a>, <a href="http://arxiv.org/find/math/1/au:+Pendavingh_R/0/1/0/all/0/1">Rudi Pendavingh</a></p><p>We provide a combinatorial and self-contained proof that for all graphs $G$
embedded on a surface $S$, the Colin de Verdi\`ere parameter $\mu(G)$ is upper
bounded by $7-2\chi(S)$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-02T01:30:00Z">Thursday, March 02 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.00252'>Is Planted Coloring Easier than Planted Clique?</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Pravesh K. Kothari, Santosh S. Vempala, Alexander S. Wein, Jeff Xu</p><p>We study the computational complexity of two related problems: recovering a
planted $q$-coloring in $G(n,1/2)$, and finding efficiently verifiable
witnesses of non-$q$-colorability (a.k.a. refutations) in $G(n,1/2)$. Our main
results show hardness for both these problems in a restricted-but-powerful
class of algorithms based on computing low-degree polynomials in the inputs.
</p>
<p>The problem of recovering a planted $q$-coloring is equivalent to recovering
$q$ disjoint planted cliques that cover all the vertices -- a potentially
easier variant of the well-studied planted clique problem. Our first result
shows that this variant is as hard as the original planted clique problem in
the low-degree polynomial model of computation: each clique needs to have size
$k \gg \sqrt{n}$ for efficient recovery to be possible. For the related variant
where the cliques cover a $(1-\epsilon)$-fraction of the vertices, we also show
hardness by reduction from planted clique.
</p>
<p>Our second result shows that refuting $q$-colorability of $G(n,1/2)$ is hard
in the low-degree polynomial model when $q \gg n^{2/3}$ but easy when $q
\lesssim n^{1/2}$, and we leave closing this gap for future work. Our proof is
more subtle than similar results for planted clique and involves constructing a
non-standard distribution over $q$-colorable graphs. We note that while related
to several prior works, this is the first work that explicitly formulates
refutation problems in the low-degree polynomial model.
</p>
<p>The proofs of our main results involve showing low-degree hardness of
hypothesis testing between an appropriately constructed pair of distributions.
For refutation, we show completeness of this approach: in the low-degree model,
the refutation task is precisely as hard as the hardest associated testing
problem, i.e., proving hardness of refutation amounts to finding a "hard"
distribution.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kothari_P/0/1/0/all/0/1">Pravesh K. Kothari</a>, <a href="http://arxiv.org/find/cs/1/au:+Vempala_S/0/1/0/all/0/1">Santosh S. Vempala</a>, <a href="http://arxiv.org/find/cs/1/au:+Wein_A/0/1/0/all/0/1">Alexander S. Wein</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jeff Xu</a></p><p>We study the computational complexity of two related problems: recovering a
planted $q$-coloring in $G(n,1/2)$, and finding efficiently verifiable
witnesses of non-$q$-colorability (a.k.a. refutations) in $G(n,1/2)$. Our main
results show hardness for both these problems in a restricted-but-powerful
class of algorithms based on computing low-degree polynomials in the inputs.
</p>
<p>The problem of recovering a planted $q$-coloring is equivalent to recovering
$q$ disjoint planted cliques that cover all the vertices -- a potentially
easier variant of the well-studied planted clique problem. Our first result
shows that this variant is as hard as the original planted clique problem in
the low-degree polynomial model of computation: each clique needs to have size
$k \gg \sqrt{n}$ for efficient recovery to be possible. For the related variant
where the cliques cover a $(1-\epsilon)$-fraction of the vertices, we also show
hardness by reduction from planted clique.
</p>
<p>Our second result shows that refuting $q$-colorability of $G(n,1/2)$ is hard
in the low-degree polynomial model when $q \gg n^{2/3}$ but easy when $q
\lesssim n^{1/2}$, and we leave closing this gap for future work. Our proof is
more subtle than similar results for planted clique and involves constructing a
non-standard distribution over $q$-colorable graphs. We note that while related
to several prior works, this is the first work that explicitly formulates
refutation problems in the low-degree polynomial model.
</p>
<p>The proofs of our main results involve showing low-degree hardness of
hypothesis testing between an appropriately constructed pair of distributions.
For refutation, we show completeness of this approach: in the low-degree model,
the refutation task is precisely as hard as the hardest associated testing
problem, i.e., proving hardness of refutation amounts to finding a "hard"
distribution.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-02T01:30:00Z">Thursday, March 02 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.00660'>Computing the Best Policy That Survives a Vote</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Andrei Constantinescu, Roger Wattenhofer</p><p>An assembly of $n$ voters needs to decide on $t$ independent binary issues.
Each voter has opinions about the issues, given by a $t$-bit vector. Anscombe's
paradox shows that a policy following the majority opinion in each issue may
not survive a vote by the very same set of $n$ voters, i.e., more voters may
feel unrepresented by such a majority-driven policy than represented. A natural
resolution is to come up with a policy that deviates a bit from the majority
policy but no longer gets more opposition than support from the electorate. We
show that a Hamming distance to the majority policy of at most $\lfloor (t - 1)
/ 2 \rfloor$ can always be guaranteed, by giving a new probabilistic argument
relying on structure-preserving symmetries of the space of potential policies.
Unless the electorate is evenly divided between the two options on all issues,
we in fact show that a policy strictly winning the vote exists within this
distance bound. Our approach also leads to a deterministic polynomial-time
algorithm for finding policies with the stated guarantees, answering an open
problem of previous work. For odd $t$, unless we are in the pathological case
described above, we also give a simpler and more efficient algorithm running in
expected polynomial time with the same guarantees. We further show that
checking whether distance strictly less than $\lfloor (t - 1) /2 \rfloor$ can
be achieved is NP-hard, and that checking for distance at most some input $k$
is FPT with respect to several natural parameters.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Constantinescu_A/0/1/0/all/0/1">Andrei Constantinescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1">Roger Wattenhofer</a></p><p>An assembly of $n$ voters needs to decide on $t$ independent binary issues.
Each voter has opinions about the issues, given by a $t$-bit vector. Anscombe's
paradox shows that a policy following the majority opinion in each issue may
not survive a vote by the very same set of $n$ voters, i.e., more voters may
feel unrepresented by such a majority-driven policy than represented. A natural
resolution is to come up with a policy that deviates a bit from the majority
policy but no longer gets more opposition than support from the electorate. We
show that a Hamming distance to the majority policy of at most $\lfloor (t - 1)
/ 2 \rfloor$ can always be guaranteed, by giving a new probabilistic argument
relying on structure-preserving symmetries of the space of potential policies.
Unless the electorate is evenly divided between the two options on all issues,
we in fact show that a policy strictly winning the vote exists within this
distance bound. Our approach also leads to a deterministic polynomial-time
algorithm for finding policies with the stated guarantees, answering an open
problem of previous work. For odd $t$, unless we are in the pathological case
described above, we also give a simpler and more efficient algorithm running in
expected polynomial time with the same guarantees. We further show that
checking whether distance strictly less than $\lfloor (t - 1) /2 \rfloor$ can
be achieved is NP-hard, and that checking for distance at most some input $k$
is FPT with respect to several natural parameters.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-02T01:30:00Z">Thursday, March 02 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.00147'>Non-crossing Hamiltonian Paths and Cycles in Output-Polynomial Time</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: David Eppstein</p><p>We show that, for planar point sets, the number of non-crossing Hamiltonian
paths is polynomially bounded in the number of non-crossing paths, and the
number of non-crossing Hamiltonian cycles (polygonalizations) is polynomially
bounded in the number of surrounding cycles. As a consequence, we can list the
non-crossing Hamiltonian paths or the polygonalizations, in time polynomial in
the output size, by filtering the output of simple backtracking algorithms for
non-crossing paths or surrounding cycles respectively. To prove these results
we relate the numbers of non-crossing structures to two easily-computed
parameters of the point set: the minimum number of points whose removal results
in a collinear set, and the number of points interior to the convex hull. These
relations also lead to polynomial-time approximation algorithms for the numbers
of structures of all four types, accurate to within a constant factor of the
logarithm of these numbers.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Eppstein_D/0/1/0/all/0/1">David Eppstein</a></p><p>We show that, for planar point sets, the number of non-crossing Hamiltonian
paths is polynomially bounded in the number of non-crossing paths, and the
number of non-crossing Hamiltonian cycles (polygonalizations) is polynomially
bounded in the number of surrounding cycles. As a consequence, we can list the
non-crossing Hamiltonian paths or the polygonalizations, in time polynomial in
the output size, by filtering the output of simple backtracking algorithms for
non-crossing paths or surrounding cycles respectively. To prove these results
we relate the numbers of non-crossing structures to two easily-computed
parameters of the point set: the minimum number of points whose removal results
in a collinear set, and the number of points interior to the convex hull. These
relations also lead to polynomial-time approximation algorithms for the numbers
of structures of all four types, accurate to within a constant factor of the
logarithm of these numbers.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-02T01:30:00Z">Thursday, March 02 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.00666'>Towards Space Efficient Two-Point Shortest Path Queries in a Polygonal Domain</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sarita de Berg, Tillman Miltzow, Frank Staals</p><p>We devise a data structure that can answer shortest path queries for two
query points in a polygonal domain $P$ on $n$ vertices. For any $\varepsilon &gt;
0$, the space complexity of the data structure is $O(n^{10+\varepsilon})$ and
queries can be answered in $O(\log n)$ time. This is the first improvement upon
a conference paper by Chiang and Mitchell from 1999. They present a data
structure with $O(n^{11})$ space complexity. Our main result can be extended to
include a space-time trade-off. Specifically, we devise data structures with
$O(n^{10+\varepsilon}/\hspace{1pt} \ell^{5 + O(\varepsilon)})$ space complexity
and $O(\ell \log n )$ query time for any integer $1 \leq \ell \leq n$.
</p>
<p>Furthermore, our main result can be improved if we restrict one (or both) of
the query points to lie on the boundary of $P$. When one of the query points is
restricted to lie on the boundary, and the other query point can still lie
anywhere in $P$, the space complexity becomes $O(n^{6+\varepsilon})$. When both
query points are on the boundary, the space is decreased further to
$O(n^{4+\varepsilon})$, thereby improving an earlier result of Bae and Okamoto.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Berg_S/0/1/0/all/0/1">Sarita de Berg</a>, <a href="http://arxiv.org/find/cs/1/au:+Miltzow_T/0/1/0/all/0/1">Tillman Miltzow</a>, <a href="http://arxiv.org/find/cs/1/au:+Staals_F/0/1/0/all/0/1">Frank Staals</a></p><p>We devise a data structure that can answer shortest path queries for two
query points in a polygonal domain $P$ on $n$ vertices. For any $\varepsilon &gt;
0$, the space complexity of the data structure is $O(n^{10+\varepsilon})$ and
queries can be answered in $O(\log n)$ time. This is the first improvement upon
a conference paper by Chiang and Mitchell from 1999. They present a data
structure with $O(n^{11})$ space complexity. Our main result can be extended to
include a space-time trade-off. Specifically, we devise data structures with
$O(n^{10+\varepsilon}/\hspace{1pt} \ell^{5 + O(\varepsilon)})$ space complexity
and $O(\ell \log n )$ query time for any integer $1 \leq \ell \leq n$.
</p>
<p>Furthermore, our main result can be improved if we restrict one (or both) of
the query points to lie on the boundary of $P$. When one of the query points is
restricted to lie on the boundary, and the other query point can still lie
anywhere in $P$, the space complexity becomes $O(n^{6+\varepsilon})$. When both
query points are on the boundary, the space is decreased further to
$O(n^{4+\varepsilon})$, thereby improving an earlier result of Bae and Okamoto.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-02T01:30:00Z">Thursday, March 02 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.00745'>Coordination of Multiple Robots along Given Paths with Bounded Junction Complexity</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mikkel Abrahamsen, Tzvika Geft, Dan Halperin, Barak Ugav</p><p>We study a fundamental NP-hard motion coordination problem for
multi-robot/multi-agent systems: We are given a graph $G$ and set of agents,
where each agent has a given directed path in $G$. Each agent is initially
located on the first vertex of its path. At each time step an agent can move to
the next vertex on its path, provided that the vertex is not occupied by
another agent. The goal is to find a sequence of such moves along the given
paths so that each reaches its target, or to report that no such sequence
exists. The problem models guidepath-based transport systems, which is a
pertinent abstraction for traffic in a variety of contemporary applications,
ranging from train networks or Automated Guided Vehicles (AGVs) in factories,
through computer game animations, to qubit transport in quantum computing. It
also arises as a sub-problem in the more general multi-robot motion-planning
problem.
</p>
<p>We provide a fine-grained tractability analysis of the problem by considering
new assumptions and identifying minimal values of key parameters for which the
problem remains NP-hard. Our analysis identifies a critical parameter called
vertex multiplicity (VM), defined as the maximum number of paths passing
through the same vertex. We show that a prevalent variant of the problem, which
is equivalent to Sequential Resource Allocation (concerning deadlock prevention
for concurrent processes), is NP-hard even when VM is 3. On the positive side,
for VM $\le$ 2 we give an efficient algorithm that iteratively resolves cycles
of blocking relations among agents. We also present a variant that is NP-hard
when the VM is 2 even when $G$ is a 2D grid and each path lies in a single grid
row or column. By studying highly distilled yet NP-hard variants, we deepen the
understanding of what makes the problem intractable and thereby guide the
search for efficient solutions under practical assumptions.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Abrahamsen_M/0/1/0/all/0/1">Mikkel Abrahamsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Geft_T/0/1/0/all/0/1">Tzvika Geft</a>, <a href="http://arxiv.org/find/cs/1/au:+Halperin_D/0/1/0/all/0/1">Dan Halperin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ugav_B/0/1/0/all/0/1">Barak Ugav</a></p><p>We study a fundamental NP-hard motion coordination problem for
multi-robot/multi-agent systems: We are given a graph $G$ and set of agents,
where each agent has a given directed path in $G$. Each agent is initially
located on the first vertex of its path. At each time step an agent can move to
the next vertex on its path, provided that the vertex is not occupied by
another agent. The goal is to find a sequence of such moves along the given
paths so that each reaches its target, or to report that no such sequence
exists. The problem models guidepath-based transport systems, which is a
pertinent abstraction for traffic in a variety of contemporary applications,
ranging from train networks or Automated Guided Vehicles (AGVs) in factories,
through computer game animations, to qubit transport in quantum computing. It
also arises as a sub-problem in the more general multi-robot motion-planning
problem.
</p>
<p>We provide a fine-grained tractability analysis of the problem by considering
new assumptions and identifying minimal values of key parameters for which the
problem remains NP-hard. Our analysis identifies a critical parameter called
vertex multiplicity (VM), defined as the maximum number of paths passing
through the same vertex. We show that a prevalent variant of the problem, which
is equivalent to Sequential Resource Allocation (concerning deadlock prevention
for concurrent processes), is NP-hard even when VM is 3. On the positive side,
for VM $\le$ 2 we give an efficient algorithm that iteratively resolves cycles
of blocking relations among agents. We also present a variant that is NP-hard
when the VM is 2 even when $G$ is a 2D grid and each path lies in a single grid
row or column. By studying highly distilled yet NP-hard variants, we deepen the
understanding of what makes the problem intractable and thereby guide the
search for efficient solutions under practical assumptions.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-02T01:30:00Z">Thursday, March 02 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.00217'>Improved Quantum Query Complexity on Easier Inputs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Noel T. Anderson, Jay-U Chung, Shelby Kimmel, Da-Yeon Koh, Xiaohan Ye</p><p>Quantum span program algorithms for function evaluation sometimes have
reduced query complexity when promised that the input has a certain structure.
We design a modified span program algorithm to show these improvements persist
even without a promise ahead of time, and we extend this approach to the more
general problem of state conversion. As an application, we prove exponential
and superpolynomial quantum advantages in average query complexity for several
search problems, generalizing Montanaro's Search with Advice [Montanaro, TQC
2010].
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Anderson_N/0/1/0/all/0/1">Noel T. Anderson</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Chung_J/0/1/0/all/0/1">Jay-U Chung</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Kimmel_S/0/1/0/all/0/1">Shelby Kimmel</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Koh_D/0/1/0/all/0/1">Da-Yeon Koh</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Ye_X/0/1/0/all/0/1">Xiaohan Ye</a></p><p>Quantum span program algorithms for function evaluation sometimes have
reduced query complexity when promised that the input has a certain structure.
We design a modified span program algorithm to show these improvements persist
even without a promise ahead of time, and we extend this approach to the more
general problem of state conversion. As an application, we prove exponential
and superpolynomial quantum advantages in average query complexity for several
search problems, generalizing Montanaro's Search with Advice [Montanaro, TQC
2010].
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-02T01:30:00Z">Thursday, March 02 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.00259'>Computing All Restricted Skyline Probabilities for Uncertain Data</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xiangyu Gao, Jianzhong Li, Dongjing Miao</p><p>Since data uncertainty is inherent in multi-criteria decision making, recent
years have witnessed a dramatically increasing amount of attention devoted to
conducting advanced analysis on uncertain data. In this paper, we revisit
restricted skyline query on uncertain datasets from both complexity and
algorithm perspective. Instead of conducting probabilistic restricted skyline
analysis under threshold or top-$k$ semantics, we focus on a more general
problem that aims to compute the restricted skyline probability of all objects.
We prove that the problem can not be solved in truly subquadratic-time unless
the Orthogonal Vectors conjecture fails, and propose two algorithms, one with
near-optimal time complexity and the other with better expected time
complexity. We also propose an algorithm with sublinear query time and
polynomial preprocessing time for the case where the preference region is
described by $d - 1$ ratio bound constraints. Our thorough experiments over
real and synthetic datasets demonstrate the effectiveness of the problem and
the efficiency of the proposed algorithms.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xiangyu Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianzhong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_D/0/1/0/all/0/1">Dongjing Miao</a></p><p>Since data uncertainty is inherent in multi-criteria decision making, recent
years have witnessed a dramatically increasing amount of attention devoted to
conducting advanced analysis on uncertain data. In this paper, we revisit
restricted skyline query on uncertain datasets from both complexity and
algorithm perspective. Instead of conducting probabilistic restricted skyline
analysis under threshold or top-$k$ semantics, we focus on a more general
problem that aims to compute the restricted skyline probability of all objects.
We prove that the problem can not be solved in truly subquadratic-time unless
the Orthogonal Vectors conjecture fails, and propose two algorithms, one with
near-optimal time complexity and the other with better expected time
complexity. We also propose an algorithm with sublinear query time and
polynomial preprocessing time for the case where the preference region is
described by $d - 1$ ratio bound constraints. Our thorough experiments over
real and synthetic datasets demonstrate the effectiveness of the problem and
the efficiency of the proposed algorithms.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-02T01:30:00Z">Thursday, March 02 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.00480'>Sampling with Barriers: Faster Mixing via Lewis Weights</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Khashayar Gatmiry, Jonathan Kelner, Santosh S. Vempala</p><p>We analyze Riemannian Hamiltonian Monte Carlo (RHMC) for sampling a polytope
defined by $m$ inequalities in $\R^n$ endowed with the metric defined by the
Hessian of a self-concordant convex barrier function. We use a hybrid of the
$p$-Lewis weight barrier and the standard logarithmic barrier and prove that
the mixing rate is bounded by $\tilde O(m^{1/3}n^{4/3})$, improving on the
previous best bound of $\tilde O(mn^{2/3})$, based on the log barrier. Our
analysis overcomes several technical challenges to establish this result, in
the process deriving smoothness bounds on Hamiltonian curves and extending
self-concordance notions to the infinity norm; both properties appear to be of
independent interest.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gatmiry_K/0/1/0/all/0/1">Khashayar Gatmiry</a>, <a href="http://arxiv.org/find/cs/1/au:+Kelner_J/0/1/0/all/0/1">Jonathan Kelner</a>, <a href="http://arxiv.org/find/cs/1/au:+Vempala_S/0/1/0/all/0/1">Santosh S. Vempala</a></p><p>We analyze Riemannian Hamiltonian Monte Carlo (RHMC) for sampling a polytope
defined by $m$ inequalities in $\R^n$ endowed with the metric defined by the
Hessian of a self-concordant convex barrier function. We use a hybrid of the
$p$-Lewis weight barrier and the standard logarithmic barrier and prove that
the mixing rate is bounded by $\tilde O(m^{1/3}n^{4/3})$, improving on the
previous best bound of $\tilde O(mn^{2/3})$, based on the log barrier. Our
analysis overcomes several technical challenges to establish this result, in
the process deriving smoothness bounds on Hamiltonian curves and extending
self-concordance notions to the infinity norm; both properties appear to be of
independent interest.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-02T01:30:00Z">Thursday, March 02 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.00569'>A linear time algorithm for linearizing quadratic and higher-order shortest path problems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Eranda &#xc7;ela, Bettina Klinz, Stefan Lendl, Gerhard J. Woeginger, Lasse Wulf</p><p>An instance of the NP-hard Quadratic Shortest Path Problem (QSPP) is called
linearizable iff it is equivalent to an instance of the classic Shortest Path
Problem (SPP) on the same input digraph. The linearization problem for the QSPP
(LinQSPP) decides whether a given QSPP instance is linearizable and determines
the corresponding SPP instance in the positive case. We provide a novel linear
time algorithm for the LinQSPP on acyclic digraphs which runs considerably
faster than the previously best algorithm. The algorithm is based on a new
insight revealing that the linearizability of the QSPP for acyclic digraphs can
be seen as a local property. Our approach extends to the more general
higher-order shortest path problem.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cela_E/0/1/0/all/0/1">Eranda &#xc7;ela</a>, <a href="http://arxiv.org/find/cs/1/au:+Klinz_B/0/1/0/all/0/1">Bettina Klinz</a>, <a href="http://arxiv.org/find/cs/1/au:+Lendl_S/0/1/0/all/0/1">Stefan Lendl</a>, <a href="http://arxiv.org/find/cs/1/au:+Woeginger_G/0/1/0/all/0/1">Gerhard J. Woeginger</a>, <a href="http://arxiv.org/find/cs/1/au:+Wulf_L/0/1/0/all/0/1">Lasse Wulf</a></p><p>An instance of the NP-hard Quadratic Shortest Path Problem (QSPP) is called
linearizable iff it is equivalent to an instance of the classic Shortest Path
Problem (SPP) on the same input digraph. The linearization problem for the QSPP
(LinQSPP) decides whether a given QSPP instance is linearizable and determines
the corresponding SPP instance in the positive case. We provide a novel linear
time algorithm for the LinQSPP on acyclic digraphs which runs considerably
faster than the previously best algorithm. The algorithm is based on a new
insight revealing that the linearizability of the QSPP for acyclic digraphs can
be seen as a local property. Our approach extends to the more general
higher-order shortest path problem.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-02T01:30:00Z">Thursday, March 02 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2303.00709'>Robust and Practical Solution of Laplacian Equations by Approximate Elimination</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yuan Gao, Rasmus Kyng, Daniel A. Spielman</p><p>We introduce a new algorithm and software for solving linear equations in
symmetric diagonally dominant matrices with non-positive off-diagonal entries
(SDDM matrices), including Laplacian matrices. We use pre-conditioned conjugate
gradient (PCG) to solve the system of linear equations. Our preconditioner is a
variant of the Approximate Cholesky factorization of Kyng and Sachdeva (FOCS
2016). Our factorization approach is simple: we eliminate matrix rows/columns
one at a time and update the remaining matrix using sampling to approximate the
outcome of complete Cholesky factorization. Unlike earlier approaches, our
sampling always maintains a connectivity in the remaining non-zero structure.
Our algorithm comes with a tuning parameter that upper bounds the number of
samples made per original entry. We implement our algorithm in Julia, providing
two versions, AC and AC2, that respectively use 1 and 2 samples per original
entry. We compare their single-threaded performance to that of current
state-of-the-art solvers Combinatorial Multigrid (CMG),
BoomerAMG-preconditioned Krylov solvers from HyPre and PETSc, Lean Algebraic
Multigrid (LAMG), and MATLAB's with Incomplete Cholesky Factorization (ICC).
Our evaluation uses a broad class of problems, including all large SDDM
matrices from the SuiteSparse collection and diverse programmatically generated
instances. Our experiments suggest that our algorithm attains a level of
robustness and reliability not seen before in SDDM solvers, while retaining
good performance across all instances. Our code and data are public, and we
provide a tutorial on how to replicate our tests. We hope that others will
adopt this suite of tests as a benchmark, which we refer to as SDDM2023. Our
solver code is available at: github.com/danspielman/Laplacians.jl/ Our
benchmarking data and tutorial are available at:
rjkyng.github.io/SDDM2023/
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Gao_Y/0/1/0/all/0/1">Yuan Gao</a>, <a href="http://arxiv.org/find/math/1/au:+Kyng_R/0/1/0/all/0/1">Rasmus Kyng</a>, <a href="http://arxiv.org/find/math/1/au:+Spielman_D/0/1/0/all/0/1">Daniel A. Spielman</a></p><p>We introduce a new algorithm and software for solving linear equations in
symmetric diagonally dominant matrices with non-positive off-diagonal entries
(SDDM matrices), including Laplacian matrices. We use pre-conditioned conjugate
gradient (PCG) to solve the system of linear equations. Our preconditioner is a
variant of the Approximate Cholesky factorization of Kyng and Sachdeva (FOCS
2016). Our factorization approach is simple: we eliminate matrix rows/columns
one at a time and update the remaining matrix using sampling to approximate the
outcome of complete Cholesky factorization. Unlike earlier approaches, our
sampling always maintains a connectivity in the remaining non-zero structure.
Our algorithm comes with a tuning parameter that upper bounds the number of
samples made per original entry. We implement our algorithm in Julia, providing
two versions, AC and AC2, that respectively use 1 and 2 samples per original
entry. We compare their single-threaded performance to that of current
state-of-the-art solvers Combinatorial Multigrid (CMG),
BoomerAMG-preconditioned Krylov solvers from HyPre and PETSc, Lean Algebraic
Multigrid (LAMG), and MATLAB's with Incomplete Cholesky Factorization (ICC).
Our evaluation uses a broad class of problems, including all large SDDM
matrices from the SuiteSparse collection and diverse programmatically generated
instances. Our experiments suggest that our algorithm attains a level of
robustness and reliability not seen before in SDDM solvers, while retaining
good performance across all instances. Our code and data are public, and we
provide a tutorial on how to replicate our tests. We hope that others will
adopt this suite of tests as a benchmark, which we refer to as SDDM2023. Our
solver code is available at: https://github.com/danspielman/Laplacians.jl/ Our
benchmarking data and tutorial are available at:
https://rjkyng.github.io/SDDM2023/
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-02T01:30:00Z">Thursday, March 02 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Wednesday, March 01
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://tcsplus.wordpress.com/2023/03/01/tcs-talk-wednesday-march-8-christos-tzamos-u-athens-uw-madison/'>TCS+ talk: Wednesday, March 8 â Christos Tzamos, U Athens/UW Madison</a></h3>
        <p class='tr-article-feed'>from <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The next TCS+ talk will take place this coming Wednesday, March 8th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). Christos Tzamos from University of Athens/UW Madison will speak about &#8220;A Strongly Polynomial Algorithm for Approximate Forster Transforms and its Application to Halfspace Learning&#8221; (abstract below). You can [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The next TCS+ talk will take place this coming Wednesday, March 8th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Christos Tzamos</strong> from University of Athens/UW Madison will speak about &#8220;<em>A Strongly Polynomial Algorithm for Approximate Forster Transforms and its Application to Halfspace Learning</em>&#8221; (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: The Forster transform is a method of regularizing a dataset by placing it in radial isotropic position while maintaining some of its essential properties. Forster transforms have played a key role in a diverse range of settings spanning computer science and functional analysis. Prior work had given weakly polynomial time algorithms for computing Forster transforms, when they exist. Our main result is the first strongly polynomial time algorithm to compute an approximate Forster transform of a given dataset or certify that no such transformation exists. By leveraging our strongly polynomial Forster algorithm, we obtain the first strongly polynomial time algorithm for distribution-free PAC learning of halfspaces. This learning result is surprising because proper PAC learning of halfspaces is equivalent to linear programming. Our learning approach extends to give a strongly polynomial halfspace learner in the presence of random classification noise and, more generally, Massart noise.</p></blockquote>
<p>&#8220;</p>
<p class="authors">By plustcs</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-01T21:43:08Z">Wednesday, March 01 2023, 21:43</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2023/03/01/non-crossing-hamiltonian.html'>Non-crossing Hamiltonian paths and cycles in output-polynomial time</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          My paper âNon-crossing Hamiltonian paths and cycles in output-polynomial timeâ, to appear at SoCG, is now online as a preprint at arXiv:2303.00147. This is the full version; the SoCG version will need to be cut down by omitting proofs to reach the 500-line proceedings limit. Itâs about polygonalization, the problem of finding all ways of connecting dots in the plane into a simple polygon (allowing connections that pass straight through a dot, but not allowing missing a dot altogether). The main results are that we can list all of these in time polynomial in the output size, and in polynomial time get an approximate count of them that is bounded above and below the true count by a polynomial of its value. Previously, the best we knew were that there were at most exponentially many polygonalizations and that we could list them in exponential time.
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>My paper âNon-crossing Hamiltonian paths and cycles in output-polynomial timeâ, to appear at SoCG, is now online as a preprint at <a href="https://arxiv.org/abs/2303.00147">arXiv:2303.00147</a>. This is the full version; the SoCG version will need to be cut down by omitting proofs to reach the 500-line proceedings limit. Itâs about <a href="https://en.wikipedia.org/wiki/Polygonalization">polygonalization</a>, the problem of finding all ways of connecting dots in the plane into a simple polygon (allowing connections that pass straight through a dot, but not allowing missing a dot altogether). The main results are that we can list all of these in time polynomial in the output size, and in polynomial time get an approximate count of them that is bounded above and below the true count by a polynomial of its value. Previously, the best we knew were that there were at most exponentially many polygonalizations and that we could list them in exponential time.</p>

<p>I think of this as being in the vein of recent conferences like the <a href="https://www.siam.org/conferences/cm/conference/sosa23">Symposium on Simplicity in Algorithms</a> or the new âsimplicity trackâ of the <a href="http://esa-symposium.org/">European Symposium on Algorithms</a>: simple algorithms whose analysis isnât. In fact, the algorithm in my paper isnât even new. Itâs the same one that was already used to achieve exponential time, in a paper âAlgorithmic enumeration of surrounding polygonsâ by Katsuhisa Yamanaka, David Avis, Takashi Horiyama, Yoshio Okamoto, Ryuhei Uehara, and Tanami Yamauchi, published in 2021 in <em>Discrete Applied Mathematics</em> (<a href="https://doi.org/10.1016/j.dam.2020.03.034">doi:10.1016/j.dam.2020.03.03</a>).</p>

<p>If we want to list all structures, from an exponentally large family of structures, in time polynomial per structure, then I think thereâs really only one idea and a lot of elaboration on that idea. The idea is: describe your structures as the vertices of a large state space, with some sort of local operation for moving from state to state; prove that this local operation suffices to connect all the states together; and then apply a graph exploration algorithm like depth-first search to find all of the states from some starting state. The trouble is, for polygonalizations, we donât know a good local operation. The obvious candidates, local moves that replace two or three edges of a polygon by a different set of edges, <a href="/blog/2020/01/29/unflippable-polygon.html">were proven not to work</a> in a 2002 paper by Carmen Fernando, Michael Houle, and Ferran Hurtado (<a href="https://doi.org/10.1016%2FS0304-3975%2801%2900409-1">doi:10.1016/S0304-3975(01)00409-1</a>). Instead, Yamanaka et al. propose to list all of the members of a larger family of structures, and then filter out the ones that are really polygonalizations. These more general structures are the âsurrounding polygonsâ of their paperâs title.</p>

<p>A surrounding polygon is just a simple polygon that uses some of the given dots as vertices and contains the rest. The example below is taken from the last section of my paper. There I show that point sets like the one in the illustration, with one concave chain of dots inside a triangle, have \((n-1)2^{n-4}\) polygonalizations but a polynomially-larger number of surrounding polygons proportional to \(n(1+\varphi)^n\). Here \(\varphi\) is the golden ratio; this is <a href="/blog/2020/01/12/counting-grid-polygonalizations.html">not the first occurrence of the golden ratio in counting polygonalizations</a>. A reviewer told me that these point sets are called âparty-hat setsâ or âice-cream cone setsâ but Iâm not sure I believe it; I couldnât find those names in a Google Scholar search.</p>

<p style="text-align:center"><img src="/blog/assets/2023/pseudotriangle.svg" alt="A set of points in the form of a triangle with a concave chain of points replacing one of its edges, and a surrounding polygon of the points. The points that are vertices of the polygon are colored blue, and the other points surrounded by the polygon are colored red." /></p>

<p>The simplest surrounding polygon of any input is just its <a href="https://en.wikipedia.org/wiki/Convex_hull">convex hull</a>. You can get from any surrounding polygon that is not the convex hull to a simpler one by â<a href="https://en.wikipedia.org/wiki/Two_ears_theorem">ear-cutting</a>â: find two consecutive edges of the polygon that form two sides of an empty triangle outside the polygon, and replace them by a single shortcut edge. The shortcutted vertex becomes surrounded, and the area of the polygon grows, so repeated ear-cutting can only stop at the convex hull, implying that all surrounding polygons are connected through the convex hull. If you choose carefully which ear to cut, you give all surrounding polygons the structure of a tree, and the algorithm of Yamanaka et al. amounts to depth-first search of this tree. You can then find the polygonalizations just by running this algorithm and outputting only the surrounding polygons that use all the dots, at some tree leaves.</p>

<p>The idea of my new paper is to analyze these structures in the style of my book, <a href="https://www.ics.uci.edu/~eppstein/forbidden/"><em>Forbidden Configurations in Discrete Geometry</em></a>, in terms of simple parameters of point sets that are monotonic (they donât go down when you add more points) and that depend only on the order-type of the point set and not its exact coordinates. The question I set out to answer is: which point sets have only a very small number of polygonalizations, and which have many? I quickly identified two ways in which a point set could only have a small number:</p>

<ul>
  <li>
    <p>Most of its points could belong to a single line. If a set of \(n\) points has \(n-k\) points on a line, and only a much smaller number \(k\) of points elsewhere, then most of the edges would have to connect paths of consecutive points along the line, and there arenât very many ways of doing that. This number \(k\) is one of the parameters studied in my book. Working out the details of this argument showed more specifically that the number of polygonalizations is \(n^{O(k)}\): there are only \(O(k)\) points of any polygonalization where something interesting happens, and only \(O(n)\) choices for what happens there.</p>
  </li>
  <li>
    <p>Most of its points could belong to the convex hull. If all points belong to the convex hull, then that is the only polygonalization. And if there are \(n-k\) points on the hull, and only a much smaller number \(k\) of points elsewhere, then the only points where something interesting happens are the \(O(k)\) points that are either not on the hull, or adjacent to a non-hull point. All the rest of their points have to be connected to their two hull neighbors. So again the number of polygonalizations is \(n^{O(k)}\). The parameter used here, the number of points interior to the hull, was not from my book, but maybe it should have been.</p>
  </li>
</ul>

<p>More strongly, upper bounds of the same form also apply to surrounding polygons. Allowing an interesting point to be skipped by the polygon doesnât increase its number of choices much. Consecutive blocks of uninteresting points along a long line of points must either all be skipped or all be part of a surrounding polygon, again not increasing the number of choices by much. And a surrounding polygon cannot skip any point of the convex hull, because then it would not be surrounded. The part of the analysis that I found more difficult was proving that these are the only cases. If you have points that are mostly not on a line and mostly not on a hull, then there are exponentially many polygonalizations. And if you have one of the two situations with few polygonalizations described above, then the number of polygonalizations is accurately described by the upper bounds above. For details of these lower bounds, see the paper. The number of surrounding polygons can only be at least as large as the number of polygonalizations, because every polygonalization is a surrounding polygon.</p>

<p>Once that analysis was done, the algorithms for listing polygonalizations and for approximately counting them came for free. The lower bound and the upper bound on the number of polygonalizations have the same form as each other, so they give an accurate approximation without any more effort. And the bounds on the number of polygonalizations and on the number of surrounding polygons have the same form as each other, so the analysis of the algorithm for surrounding polygons (that it takes input-polynomial time per polygon) also shows that it generates all polygonalizations in output-polynomial time.</p>

<p>The ânon-crossing Hamiltonian pathsâ of the new paperâs title are the same thing, but easier. The easier-to-generate structures are non-crossing paths, which you can form into a forest (rooted at the one-vertex paths) by a parent operation that removes the final edge of a path. And points in convex position still have many paths; the only point sets that have a small number of non-crossing Hamiltonian paths (or non-crossing paths) are the ones with most of the points on a single line.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/109951209389425592">Discuss on Mastodon</a>)</p><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-01T17:51:00Z">Wednesday, March 01 2023, 17:51</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://rjlipton.wpcomstaging.com/2023/03/01/soda-2023/'>SODA 2023</a></h3>
        <p class='tr-article-feed'>from <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Traces of strings, plus ways of tracing accepted papers Anindya De was at Northwestern University and is now at the University of Pennsylvania&#8212;see here. He was advised by two of the top advisors ever there were: Luca Trevisan and Umesh Vazirani. Traces I recently ran across a great paper by Anindya titled Approximate Trace Reconstruction [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>
<font color="#0044cc"><br />
<em>Traces of strings, plus ways of tracing accepted papers</em><br />
<font color="#000000"></p>
<p>
Anindya De was at Northwestern University and is now at the University of Pennsylvania&#8212;see <a href="https://www.seas.upenn.edu/~anindyad/">here</a>. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2023/03/01/soda-2023/ad-2/" rel="attachment wp-att-21192"><img data-attachment-id="21192" data-permalink="https://rjlipton.wpcomstaging.com/2023/03/01/soda-2023/ad-2/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/ad.jpg?fit=400%2C400&amp;ssl=1" data-orig-size="400,400" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;4&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Canon EOS 5D Mark III&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1551052800&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;200&quot;,&quot;iso&quot;:&quot;200&quot;,&quot;shutter_speed&quot;:&quot;0.00625&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="ad" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/ad.jpg?fit=300%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/ad.jpg?fit=400%2C400&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/ad.jpg?resize=250%2C250&#038;ssl=1" alt="" width="250" height="250" class="aligncenter wp-image-21192" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/ad.jpg?w=400&amp;ssl=1 400w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/ad.jpg?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/ad.jpg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/03/ad.jpg?resize=200%2C200&amp;ssl=1 200w" sizes="(max-width: 250px) 100vw, 250px" data-recalc-dims="1" /></a></p>
<p>
He was advised by two of the top advisors ever there were: Luca Trevisan and Umesh Vazirani. </p>
<p>
<p><H2> Traces </H2></p>
<p><p>
I recently ran across a great paper by Anindya titled <a href="https://arxiv.org/abs/2211.03292">Approximate Trace Reconstruction from a Single Trace</a>. It is co-authored with Xi Chen (Columbia University), Chin Ho Lee (Harvard University), and Rocco Servedio and Sandip Sinha (Columbia University). Notice that we did not put an <a href="https://jewishstandard.timesofisrael.com/horse-mule-horse-mule/">Oxford comma</a> between Servedio and Sinha as they are both from Columbia. The paper appeared at <a href="https://www.siam.org/conferences/cm/program/accepted-papers/soda23-accepted-papers">SODA 2023</a> this January. </p>
<p>
Here are pointers to the almost 200 papers that were in the conference. I put this together before discovering the site <a href="https://www.conference-publishing.com/">conference-publishing.com</a>, which as mentioned in my STOC 2023 <a href="https://rjlipton.wpcomstaging.com/2023/02/25/stoc-2023/">post</a> generates paper lists with links for a host of conferences. So I did all the following links myself. Do scroll past the list to the bottom to read a little more about traces which Ken and I put together.</p>
<ol>
<p><li>
<a href="https://arxiv.org/pdf/2204.09129.pdf">Small Shadows of Lattice Polytopes</a></p>
<p><li>
<a href="https://arxiv.org/abs/2202.05186">Fair allocation of a multiset of indivisible items</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.02277">Hierarchies of Minion Tests for PCSPs through Tensors</a></p>
<p><li>
<a href="https://arxiv.org/abs/2210.08293">Approximate Graph Colouring and Crystals</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.04455">The Price of Stability for First Price Auction</a></p>
<p><li>
<a href="https://arxiv.org/abs/2206.04549">Spencer&#8217;s theorem in nearly input-sparsity time</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.11195">Spatial Mixing and the random-cluster dynamics on lattices</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch157">Nonlinear codes exceeding the Gilbert-Varshamov and Tsfasman-Vladut-Zink bounds</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.09391">A Near-Linear Time Sampler for the Ising Model with External Field</a></p>
<p><li>
<a href="https://arxiv.org/abs/2209.02655">Concentration of polynomial random matrices via Efron-Stein inequalities</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.15945">Quantum Speed-ups for String Synchronizing Sets, Longest Common Substring, and kmismatch Matching</a></p>
<p><li>
<a href="https://arxiv.org/abs/2208.11275">Halving by a Thousand Cuts or Punctures</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611977554.ch50">On the number incidences when avoiding an induced biclique in geometric settings</a></p>
<p><li>
<a href="https://arxiv.org/abs/2206.00579">Subexponential mixing for partition chains on grid-like graphs</a></p>
<p><li>
<a href="https://arxiv.org/abs/2103.02972">Weisfeilera Leman and Graph Spectra</a></p>
<p><li>
<a href="https://arxiv.org/abs/2203.09334">Stronger 3SUM-Indexing Lower Bounds</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.10556">Tight Bounds for Monotone Minimal Perfect Hashing</a></p>
<p><li>
<a href="https://arxiv.org/abs/2208.02732">Almost Consistent Systems of Linear Equations</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.05898">Testing and Learning Quantum Juntas Nearly Optimally</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch155">Testing Convex Truncation</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch55">Player-optimal Stable Regret for Bandit Learning in Matching Markets</a></p>
<p><li>
<a href="https://arxiv.org/abs/2103.03769">Competitive Information Design for Pandoras Box</a></p>
<p><li>
<a href="https://arxiv.org/abs/2106.12725">Breaking the O(n)-Barrier in the Construction of Compressed Suffix Arrays and Suffix Trees</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.14108">Short Synchronizing Words for Random Automata</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.00450">Packing cycles in planar and bounded-genus graphs</a></p>
<p><li>
<a href="https://web.eecs.umich.edu/~pettie/papers/LLL.pdf">Improved Distributed Algorithms for the Lovasz Local Lemma and Edge Coloring</a></p>
<p><li>
<a href="https://arxiv.org/abs/2112.07050">Optimal Fully Dynamic <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{k}" class="latex" />-Center Clustering for Adaptive and Oblivious Adversaries</a></p>
<p><li>
<a href="https://arxiv.org/abs/2202.13335">A logic-based algorithmic meta-theorem for mim-width</a></p>
<p><li>
<a href="https://arxiv.org/abs/2111.12800">Tiny Pointers</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.07158">Streaming complexity of CSPs with randomly ordered constraints</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.04458">Computing Square Colorings on Bounded-Treewidth and Planar Graphs</a></p>
<p><li>
<a href="https://arxiv.org/abs/2203.00751">Near-Linear Time Approximations for Cut Problems via Fair Cuts</a></p>
<p><li>
<a href="https://deepai.org/publication/stronger-privacy-amplification-by-shuffling-for-renyi-and-approximate-differential-privacy">Stronger Privacy Amplification by Shuffling for R&eacute;nyi and Approximate Differential Privacy</a></p>
<p><li>
<a href="https://arxiv.org/abs/2208.13696">Minimizing Completion Times for Stochastic Jobs via Batched Free Times</a></p>
<p><li>
<a href="https://arxiv.org/abs/2106.02149">Optimal Pricing Schemes for an Impatient Buyer</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.07974">Online Prediction in Sub-linear Space</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.03268">Fast Discrepancy Minimization with Hereditary Guarantees</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.07363">Exact Flow Sparsification Requires Unbounded Size</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.07809">Curve Simplification and Clustering under Frechet Distance</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.08783">Almost Tight Bounds for Online Facility Location in the Random-Order Model</a></p>
<p><li>
<a href="https://arxiv.org/abs/2104.00406">The complete classification for quantified equality constraints</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.02170">Small subgraphs with large average degree</a></p>
<p><li>
<a href="https://arxiv.org/abs/2208.07544">Mean estimation when you have the source code; or, quantum Monte Carlo methods</a></p>
<p><li>
<a href="https://arxiv.org/abs/2212.03016">Online Min-Max Paging</a></p>
<p><li>
<a href="https://arxiv.org/abs/2209.08904">Gap-ETH-Tight Approximation Schemes for Red-Green-Blue Separation and BicoloredEuclidean Travelling Salesman Tours</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.02951">Map matching queries on realistic input graphs under the Frachet distance</a></p>
<p><li>
<a href="https://arxiv.org/abs/2208.07410">Private Query Release via the Johnson Lindenstrauss Transform</a></p>
<p><li>
<a href="https://arxiv.org/abs/2206.07571">Efficient decoding up to a constant fraction of the code length for asymptotically goodquantum codes</a> </p>
<p><li>
<a href="https://arxiv.org/abs/2202.01248">Passing the Limits of Pure Local Search for weighted k-Set Packing</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.11892">Improved Bounds for Sampling Solutions of Random CNF Formulas</a></p>
<p><li>
<a href="https://arxiv.org/abs/2111.03158">Pricing Query Complexity of Revenue Maximization</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.07422">Flow-augmentation III: complexity dichotomy for Boolean CSPs parameterized by thenumber of unsatisfied constraints</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.07425">Parameter tractability of Directed Multicut with three terminal pairs parameterizedby the size of the cutset: twin-width meets flow-augmentation</a> </p>
<p><li>
<a href="https://arxiv.org/abs/2012.06713">Approximate Trace Reconstruction from a Single Trace</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.07519">Dynamic Algorithms for Packing-Covering LPs via Multiplicative Weight Updates</a></p>
<p><li>
<a href="https://arxiv.org/abs/2204.03469">Sharp threshold sequence and universality for Ising perceptron models</a></p>
<p><li>
<a href="https://arxiv.org/abs/2204.02519">Maintaining Expander Decompositions via Sparse Cuts</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.09341">Near Optimal Analysis of the Cube versus Cube Test</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.09341">Approximating Knapsack and Partition via Dense Subset Sums</a></p>
<p><li>
<a href="https://arxiv.org/abs/2210.13755">Online and Bandit Algorithms for Norms with Gradient-Stable Approximations</a></p>
<p><li>
<a href="https://arxiv.org/abs/2204.04868">On complex roots of the independence polynomial</a></p>
<p><li>
<a href="https://deepai.org/publication/simplex-range-searching-revisited-how-to-shave-logs-in-multi-level-data-structures">Simplex Range Searching Revisited: How to Shave Logs in Multi-Level Data Structures</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.04112">Improved Pattern-Avoidance Bounds for Greedy BSTs via Matrix Decomposition</a></p>
<p><li>
<a href="https://arxiv.org/abs/2111.06527">Moser-Tardos Algorithm: Beyond Shearer&#8217;s Bound</a></p>
<p><li>
<a href="https://arxiv.org/abs/2212.14847">Deterministic counting Lovasz local lemma beyond linear programming</a></p>
<p><li>
<a href="https://arxiv.org/abs/2205.05564">Conflict-free hypergraph matchings</a></p>
<p><li>
<a href="https://arxiv.org/abs/2208.12721">A Subquadratic <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%5Cepsilon%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n^&#92;epsilon}" class="latex" />-approximation for the Continuous Frachet Distance</a></p>
<p><li>
<a href="https://arxiv.org/abs/2204.08044">Interdependent Public Projects</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.04994">A Nearly Time-Optimal Distributed Approximation of Minimum Cost <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{k}" class="latex" />-Edge-Connected Spanning Subgraph</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.07426">A tight quasi-polynomial bound for Global Label Min-Cut</a></p>
<p><li>
<a href="https://arxiv.org/abs/2205.07709">Polynomial formulations as a barrier for reduction-based hardness proofs</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch173">Faster Algorithm for Turn-based Stochastic Games with Bounded Treewidth</a></p>
<p><li>
<a href="https://arxiv.org/abs/2203.17144">Instability of backoff protocols with arbitrary arrival rates</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch158">On the orbit closure intersection problems for matrix tuples under conjugation and leftright actions</a></p>
<p><li>
<a href="https://arxiv.org/abs/2202.04377">Constant Approximating Parameterized k-SetCover is W[2]-hard</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.08268?context=stat.ML">Online Lewis Weight Sampling</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.11328">Toeplitz Low-Rank Approximation with Sublinear Query Complexity</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.06874">Kernelization for Graph Packing Problems via Rainbow Matching</a></p>
<p><li>
<a href="https://arxiv.org/abs/2202.01143">Improved Integrality Gap in Max-Min Allocation: or Topology at the North Pole</a></p>
<p><li>
<a href="https://arxiv.org/abs/2202.06292">Generalized Unrelated Machine Scheduling Problem</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.04278">Tight Complexity Bounds for Counting Generalized Dominating Sets in Bounded-Treewidth Graphs</a></p>
<p><li>
<a href="https://arxiv.org/abs/2301.07537">An Improved Approximation for Maximum Weighted k-Set Packing</a></p>
<p><li>
<a href="https://arxiv.org/abs/2212.09348">Excluding Single-Crossing Matching Minors in Bipartite Graphs</a></p>
<p><li>
<a href="https://arxiv.org/abs/2206.15335">Byzantine Agreement with Optimal Resilience via Statistical Fraud Detection</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.05345">Finding Triangles and Other Small Subgraphs in Geometric Intersection Graphs</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch188">Simple, deterministic, fast (but weak) approximations to edit distance and Dyck edit distance</a> </p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch132">From algorithms to connectivity and back: finding a giant component in random k-SAT</a></p>
<p><li>
<a href="https://arxiv.org/abs/2206.00594">Sparse graphs with bounded induced cycle packing number have logarithmic treewidth</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.08311">Shrunk subspaces via operator Sinkhorn iteration</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.10398">Improved Approximation Algorithms for Unrelated Machines</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.01723">Model-Checking for First-Order Logic with Disjoint Paths Predicates in Proper MinorClosed Graph Classes</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.10556">A Distanced Matching Game, Decremental APSP in Expanders, and Faster DeterministicAlgorithms for Graph Cut Problems</a> </p>
<p><li>
<a href="https://arxiv.org/abs/2005.06156">Super-resolution and Robust Sparse Continuous Fourier Transform in Any Constant Dimension: Nearly Linear Time and Sample Complexity</a></p>
<p><li>
<a href="https://chaoxuprime.com/files/papers/sub4part.pdf">A Polynomial Time Algorithm for Finding a Minimum 4-Partition of a Submodular Function</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.05423">Positivity of the symmetric group characters is PH-hard</a></p>
<p><li>
<a href="https://arxiv.org/abs/1905.08841">Parallel Exact Shortest Paths in Almost Linear Work and Square Root Depth</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/10.1137/1.9781611977554.ch40">On the Integrality Gap of MFN Relaxation for the Capacitated Facility Location Problem</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611977554.ch105">Weak Bisimulation Finiteness of Pushdown Systems With Deterministic Transitions-ExpTime-Complete</a></p>
<p><li>
<a href="https://arxiv.org/abs/2206.13057">Beating Greedy Matching in Sublinear Time</a></p>
<p><li>
<a href="https://arxiv.org/abs/2203.11863">Integrality Gaps for Random Integer Programs via Discrepancy</a></p>
<p><li>
<a href="https://arxiv.org/abs/2209.10265">Improved Approximation for Two-Edge-Connectivity</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch117">Zigzagging through acyclic orientations of chordal graphs and hypergraphs</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.04797">Shortest Cycles With Monotone Submodular Costs</a></p>
<p><li>
<a href="https://arxiv.org/abs/2107.07347">Traversing the FFT Computation Tree for Dimension-Independent Sparse FourierTransforms</a> </p>
<p><li>
<a href="https://arxiv.org/abs/2207.07449">Fixed-Parameter Tractability of Maximum Colored Path and Beyond</a></p>
<p><li>
<a href="https://arxiv.org/abs/2007.12257">A half-integral Erd&#337;s-P&oacute;sa theorem for directed odd cycles</a></p>
<p><li>
<a href="https://arxiv.org/abs/2210.13395">Improved Bi-point Rounding Algorithms and a Golden Barrier for <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{k}" class="latex" />-Median</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.05659">Approximate Distance Oracles for Planar Graphs with Subpolynomial Error Dependency</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.02717">A Framework for Approximation Schemes on Disk Graphs</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.13281">Cubic Goldreich-Levin</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.04507">Closing the Gap Between Directed Hopsets and Shortcut Sets</a></p>
<p><li>
<a href="https://arxiv.org/abs/2202.09215">&#8220;Who is Next in Line?&#8221; On the Significance of Knowing the Arrival Order in Bayesian Online Settings</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.05217">Smaller Low-Depth Circuits for Kronecker Powers</a></p>
<p><li>
<a href="https://arxiv.org/abs/2111.14759">Fast algorithms for solving the Hamilton Cycle problem with high probability</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.05053">On Minimizing Tardy Processing Time, Max-Min Skewed Convolution, and TrianglarStructured ILPs</a></p>
<p><li>
<a href="https://arxiv.org/abs/2302.02290">Maximal k-Edge-Connected Subgraphs in Weighted Graphs via Local Random Contraction</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.10850">A simple and sharper proof of the hypergraph Moore bound</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.08643">A Sublinear-Time Quantum Algorithm for Approximating Partition Functions</a></p>
<p><li>
<a href="https://arxiv.org/abs/2202.13484">On Problems Related to Unbounded SubsetSum: A Unified Combinatorial Approach</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.03893">Query Complexity of the Metric Steiner Tree Problem</a></p>
<p><li>
<a href="https://arxiv.org/abs/2210.12601">Sublinear-Time Algorithms for Max Cut, Max E2Lin<img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%28q%29%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{(q)}" class="latex" />, and Unique Label Cover on Expanders</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.06790">Near-Linear Sample Complexity for <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BL_p%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{L_p}" class="latex" /> Polynomial Regression</a></p>
<p><li>
<a href="https://arxiv.org/abs/2209.07520">On (Random-order) Online Contention Resolution Schemes for the Matching Polytope of (Bipartite) Graphs</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.09964">Optimal Algorithms for Linear Algebra in the Current Matrix Multiplication Time</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.07946">Algebraic Algorithms for Fractional Linear Matroid Parity via Non-commutative Rank</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.05006">Almost Tight Error Bounds on Differentially Private Continual Counting</a></p>
<p><li>
<a href="https://arxiv.org/abs/2208.09159">Secretary Problems: The Power of a Single Sample</a></p>
<p><li>
<a href="https://arxiv.org/abs/2112.06380">Robust Voting Rules from Algorithmic Robust Statistics</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch9">Faster and Unified Algorithms for Diameter Reducing Shortcuts and Minimum Chain Cover</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch85">Improved girth approximation in weighted undirected graphs</a></p>
<p><li>
<a href="https://arxiv.org/abs/2112.03791">Online Sorting and Translational Packing of Convex Polygons</a></p>
<p><li>
<a href="https://arxiv.org/abs/2302.05951">Fully Dynamic Exact Edge Connectivity in Sublinear Time</a></p>
<p><li>
<a href="https://arxiv.org/abs/2111.11072">Algorithmizing the Multiplicity Schwartz-Zippel Lemma</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.07949">A Nearly Tight Analysis of Greedy k-means++</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.07007?context=cs">A polynomial-time algorithm for 1/2-well-supported Nash equilibria in bimatrix games</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.10969">Bidder subset selection problem in auction design</a></p>
<p><li>
<a href="https://arxiv.org/abs/2204.09035">Massively Parallel Computation on Embedded Planar Graphs</a></p>
<p><li>
<a href="https://arxiv.org/abs/2301.09810">Balanced Allocations: The Power of Memory with Heterogeneous Bins</a></p>
<p><li>
<a href="https://arxiv.org/abs/2003.00545">Simple Mechanisms for Agents with Non-linear Utilities</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch59">The Power of Clairvoyance for Multi-Level Aggregation and Set Cover with Delay</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.05769">Steiner Connectivity Augmentation and Splitting-off in Poly-logarithmic Maximum Flows</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.05509">Discrepancy minimization via regularization</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch162">Equivalence Test for Read-Once Arithmetic Formulas</a></p>
<p><li>
<a href="https://arxiv.org/abs/2210.07534">Time-Space Tradeoffs for Element Distinctness and Set Intersection via Pseudorandomness</a></p>
<p><li>
<a href="https://arxiv.org/abs/2209.11651">Local Distributed Rounding: Generalized to MIS, Matching, Set Cover, and Beyond</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch143">Parameterized Approximation Scheme for Biclique-free Max k-Weight SAT and Max Coverage</a></p>
<p><li>
<a href="https://arxiv.org/abs/2209.11669">Improved Distributed Network Decomposition, Hitting Sets, and Spanners, via Derandomization</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.05150">Breaching the 2 LMP Approximation Barrier for Facility Location with Applications to kMedian</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.12441">Query Complexity of Inversion Minimization on Trees</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch4">Faster Deterministic Worst-Case Fully Dynamic All-Pairs Shortest Paths via Decremental Hop-Restricted Shortest Paths</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch189">Optimal Square Detection Over General Alphabets</a></p>
<p><li>
<a href="https://arxiv.org/abs/2203.16476">Differentially Private All-Pairs Shortest Path Distances: Improved Algorithms and Lower Bounds</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch96">Faster Computation of 3-Edge-Connected Components in Digraphs</a></p>
<p><li>
<a href="https://arxiv.org/abs/2201.10758">Sampling Equilibria: Fast No-Regret Learning in Structured Games</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.07327">Higher degree sum-of-squares relaxations robust against oblivious outliers</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.07606">Fast Distributed Brooks Theorem</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch119">Graph Classes With Few Minimal Separators. I. Finite Forbidden Subgraphs</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.03530">Optimal Deterministic Massively Parallel Connectivity on Forests</a></p>
<p><li>
<a href="https://arxiv.org/abs/2111.03151">Foundations of Transaction Fee Mechanism Design</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch119">Graph Classes With Few Minimal Separators. II. A Dichotomy</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.01945">Distributed Maximal Matching and Maximal Independent Set on Hypergraphs</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.08800">Quantum tomography using state-preparation unitaries</a></p>
<p><li>
<a href="https://arxiv.org/abs/2204.01911">Almost-Linear Planted Cliques Elude the Metropolis Process</a></p>
<p><li>
<a href="https://arxiv.org/abs/2111.05450">Timeliness Through Telephones</a></p>
<p><li>
<a href="https://www.ccs.neu.edu/home/viola/papers/resilient.pdf">Efficient resilient functions</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.07438">Dynamic Matching with Better-than-2 Approximation in Polylogarithmic Update Time</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.03877">The Need for Seed (in the abstract Tile Assembly Model)</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.08347">Private Convex Optimization in General Norms</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.07607">Dynamic Algorithms for Maximum Matching Size</a></p>
<p><li>
<a href="https://arxiv.org/abs/2111.01254">Unique Games hardness of Quantum Max-Cut, and a conjectured vector-valued Borell&#8217;s inequality</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch23">A Nearly-tight Analysis of Multipass Pairing Heaps</a></p>
<p><li>
<a href="https://arxiv.org/abs/2108.04458">A Tight Analysis of Slim Heaps and Smooth Heaps</a></p>
<p><li>
<a href="https://arxiv.org/abs/2106.04863">Lossless Online Rounding for Online Bipartite Matching (Despite its Impossibility)</a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.07983?context=cs">Approximation Algorithms for Steiner Tree Augmentation Problems</a></p>
<p><li>
<a href="https://arxiv.org/abs/2204.08404">Low Degree Testing over the Reals</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.05170">Streaming algorithms for the missing item finding problem</a></p>
<p><li>
<a href="https://cse.hkust.edu.hk/faculty/arya/pub/soda23.pdf">Economical Convex Coverings and Applications</a></p>
<p><li>
<a href="https://eccc.weizmann.ac.il/report/2022/146/">Interactive Coding with Small Memory</a></p>
<p><li>
<a href="https://arxiv.org/abs/2301.05682">Non-Stochastic CDF Estimation Using Threshold Queries</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch30">Elliptic Curve Fast Fourier Transform Part I : Low-degree extension in time O(n log n) over all finite fields</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch33">Single-Pass Streaming Algorithms for Correlation Clustering</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.01468">A New Approach to Estimating Effective Resistances and Counting Spanning Trees in Expander Graphs</a></p>
<p><li>
<a href="https://arxiv.org/abs/2210.06375">Superpolynomial Lower Bounds for Decision Tree Learning and Testing</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.07132">The <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_p%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;ell_p}" class="latex" />-Subspace Sketch Problem in Small Dimensions with Applications to Support Vector Machines</a></p>
<p><li>
<a href="https://arxiv.org/abs/2210.17515">Beating <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%281-1%2Fe%29%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{(1-1/e)}" class="latex" />-Approximation for Weighted Stochastic Matching</a></p>
<p><li>
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch35">Towards Multi-Pass Streaming Lower Bounds for Optimal Approximation of Max-Cut</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.03341">Parameterized Algorithm for the Planar Disjoint Paths Problem: Exponential in <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bk%5E2%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{k^2}" class="latex" />, and Linear in <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n}" class="latex" /></a></p>
<p><li>
<a href="https://arxiv.org/abs/2207.02581">Learning Hierarchical Cluster Structure of Graphs in Sublinear Time</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.09106">The Exact Bipartite Matching Polytope Has Exponential Extension Complexity</a></p>
<p><li>
<a href="https://arxiv.org/abs/2211.03161">4D Range Reporting in the Pointer Machine Model in Almost-Optimal Time</a></p>
</ol>
<p>
<p><H2> The Trace Result </H2></p>
<p><p>
The trace problem begins by sending a binary string <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x}" class="latex" /> of length <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n}" class="latex" /> through a <b>deletion channel</b> with parameter <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%5Cin+%5B0%2C1%5D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;delta &#92;in [0,1]}" class="latex" />. Each bit <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x_i}" class="latex" /> entering the channel survives with probability <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B1+-+%5Cdelta%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{1 - &#92;delta}" class="latex" /> to be part of the output string <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7By%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{y}" class="latex" />. That is, <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x_i}" class="latex" /> is deleted with probability <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;delta}" class="latex" />. The deletions are independent. For an unknown string <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x}" class="latex" />, the problem is:</p>
<blockquote><p><b> </b> <em> Given <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{k}" class="latex" /> strings <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7By_1%2C%5Cdots%2Cy_k%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{y_1,&#92;dots,y_k}" class="latex" /> produced by <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{k}" class="latex" /> runs of the channel on <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x}" class="latex" />, reconstruct <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x}" class="latex" /> if possible. Else, calculate a binary string <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%27%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x&#039;}" class="latex" /> of length <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n}" class="latex" /> that minimizes a distance metric <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bd%28x%2Cx%27%29%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{d(x,x&#039;)}" class="latex" />. The metric of choice is to maximize the length <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Cell%28x%2Cx%27%29%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;ell(x,x&#039;)}" class="latex" /> of the longest common subsequence (not necessarily contiguous) of <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x}" class="latex" /> and <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%27%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x&#039;}" class="latex" />, which corresponds to minimizing their edit distance. </em>
</p></blockquote>
<p><p>
As indicated by its title &#8220;Approximate Trace Reconstruction from a Single Trace,&#8221; the <a href="https://arxiv.org/abs/2211.03292">paper</a> tackles the extreme case <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bk%3D1%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{k=1}" class="latex" />. Of course one cannot reconstruct <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x}" class="latex" /> (unless no deletions occur so <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7By+%3D+x%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{y = x}" class="latex" />) so the game is to find <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%27%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x&#039;}" class="latex" /> that are most likely to have produced the lone observed <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7By%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{y}" class="latex" />. The scoring function takes the expectation of <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Cell%28x%27%2Cx%29%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;ell(x&#039;,x)}" class="latex" /> over both the generation of <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7By%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{y}" class="latex" /> from the true <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x}" class="latex" /> and the run of the algorithm guessing <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%27%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x&#039;}" class="latex" /> from <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7By%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{y}" class="latex" />. There are two main questions:</p>
<ul>
<li>
How well does the algorithm perform&#8212;relative to theoretically optimal choices given <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7By%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{y}" class="latex" />&#8212;when <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x}" class="latex" /> itself is generated uniformly at random? </p>
<li>
How well does the algorithm perform when <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x}" class="latex" /> is generated adversarially? Note that <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7By%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{y}" class="latex" /> is still probabilistic, and the performance of both the theoretical optimal algorithm and their algorithm are evaluated based on the distribution of <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7By%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{y}" class="latex" /> for the fixed (unseen) <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x}" class="latex" />.
</ul>
<p>
These questions are posed for small, medium, and large values of <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;delta}" class="latex" />. When the deletion probability is close to <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B1%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{1}" class="latex" />, the strings <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7By%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{y}" class="latex" /> are most often tiny. One would think they offer no help in coming close to <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x}" class="latex" />. However, they do help efficient algorithms come close to the optimal policy for a worst-case chosen <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x}" class="latex" />. The paradoxical results of their paper, in their own words (but reversing their order), are: </p>
<ol>
<li>
In the average-case setting, having access to a single trace is provably not very useful: no algorithm, computationally efficient or otherwise, can achieve significantly higher accuracy given one trace that is <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bo%28n%29%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{o(n)}" class="latex" /> bits long than it could with no traces. </p>
<li>
Having access to a single trace is already quite useful for worst-case trace reconstruction: an efficient algorithm can perform much more accurate reconstruction, given one trace that is even only a few bits long, than it could given no traces at all.
</ol>
<p>
The deep point is that when <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x}" class="latex" /> as well as <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7By%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{y}" class="latex" /> is random, seeing <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7By%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{y}" class="latex" /> gives little advantage to both the optimal strategy (which does not know <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x}" class="latex" />) and their algorithm. Whereas, when <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{x}" class="latex" /> is fixed, the knowledge of <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7By%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{y}" class="latex" /> is more valuable to the optimal strategy and separates it from the case of not seeing <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7By%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{y}" class="latex" /> at all. However, the profit given by even a short <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7By%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{y}" class="latex" /> is one that is apprehendable by a complexity-limited deterministic algorithm that sees only <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7By%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{y}" class="latex" />. That&#8217;s our attempt at an intuitive takeaway; as always we invite readers to consult the paper in detail.</p>
<p>
<p><H2> Open Problems </H2></p>
<p><p>
Comparing my list of pointer to the papers from SODA, which was a bit of trouble to create by hand, to the STOC&#8217;23 <a href="https://www.conference-publishing.com/list.php?Event=STOC23">output</a> from the conference-publishing site, leads to a curious question:</p>
<blockquote><p><b> </b> <em> Do we scan lists of papers more by looking for subject words in their titles or looking for authors we know? </em>
</p></blockquote>
<p><p>
Well, I have not found SODA&#8217;23 on that website, where authors too would be given; for me, copying the authors would more than double the manual work.</p>
<p>
<p class="authors">By RJLipton+KWRegan</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-01T06:02:29Z">Wednesday, March 01 2023, 06:02</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.14412'>An Algorithm and Complexity Results for Causal Unit Selection</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Haiying Huang, Adnan Darwiche</p><p>The unit selection problem aims to identify objects, called units, that are
most likely to exhibit a desired mode of behavior when subjected to stimuli
(e.g., customers who are about to churn but would change their mind if
encouraged). Unit selection with counterfactual objective functions was
introduced relatively recently with existing work focusing on bounding a
specific class of objective functions, called the benefit functions, based on
observational and interventional data -- assuming a fully specified model is
not available to evaluate these functions. We complement this line of work by
proposing the first exact algorithm for finding optimal units given a broad
class of causal objective functions and a fully specified structural causal
model (SCM). We show that unit selection under this class of objective
functions is $\text{NP}^\text{PP}$-complete but is $\text{NP}$-complete when
unit variables correspond to all exogenous variables in the SCM. We also
provide treewidth-based complexity bounds on our proposed algorithm while
relating it to a well-known algorithm for Maximum a Posteriori (MAP) inference.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Haiying Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Darwiche_A/0/1/0/all/0/1">Adnan Darwiche</a></p><p>The unit selection problem aims to identify objects, called units, that are
most likely to exhibit a desired mode of behavior when subjected to stimuli
(e.g., customers who are about to churn but would change their mind if
encouraged). Unit selection with counterfactual objective functions was
introduced relatively recently with existing work focusing on bounding a
specific class of objective functions, called the benefit functions, based on
observational and interventional data -- assuming a fully specified model is
not available to evaluate these functions. We complement this line of work by
proposing the first exact algorithm for finding optimal units given a broad
class of causal objective functions and a fully specified structural causal
model (SCM). We show that unit selection under this class of objective
functions is $\text{NP}^\text{PP}$-complete but is $\text{NP}$-complete when
unit variables correspond to all exogenous variables in the SCM. We also
provide treewidth-based complexity bounds on our proposed algorithm while
relating it to a well-known algorithm for Maximum a Posteriori (MAP) inference.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-01T01:30:00Z">Wednesday, March 01 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.14585'>On Degeneracy in the P-Matroid Oriented Matroid Complementarity Problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Michaela Borzechowski, Simon Weber</p><p>We investigate degeneracy in the P-Matroid Oriented Matroid Complementarity
Problem (P-OMCP) and its impact on the reduction of this problem to
sink-finding in Unique Sink Orientations (USOs). On one hand, this
understanding of degeneracies allows us to prove a linear lower bound for
sink-finding in P-matroid USOs. On the other hand, it allows us to prove a
promise preserving reduction from P-OMCP to USO sink-finding, where we can drop
the assumption that the given P-OMCP is non-degenerate. This places the promise
version of P-OMCP in the complexity class PromiseUEOPL.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Borzechowski_M/0/1/0/all/0/1">Michaela Borzechowski</a>, <a href="http://arxiv.org/find/math/1/au:+Weber_S/0/1/0/all/0/1">Simon Weber</a></p><p>We investigate degeneracy in the P-Matroid Oriented Matroid Complementarity
Problem (P-OMCP) and its impact on the reduction of this problem to
sink-finding in Unique Sink Orientations (USOs). On one hand, this
understanding of degeneracies allows us to prove a linear lower bound for
sink-finding in P-matroid USOs. On the other hand, it allows us to prove a
promise preserving reduction from P-OMCP to USO sink-finding, where we can drop
the assumption that the given P-OMCP is non-degenerate. This places the promise
version of P-OMCP in the complexity class PromiseUEOPL.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-01T01:30:00Z">Wednesday, March 01 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.14755'>Local Hamiltonians with no low-energy stabilizer states</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Nolan J. Coble, Matthew Coudron, Jon Nelson, Seyed Sajjad Nezhadi</p><p>The recently-defined No Low-energy Sampleable States (NLSS) conjecture of
Gharibian and Le Gall [GL22] posits the existence of a family of local
Hamiltonians where all states of low-enough constant energy do not have
succinct representations allowing perfect sampling access. States that can be
prepared using only Clifford gates (i.e. stabilizer states) are an example of
sampleable states, so the NLSS conjecture implies the existence of local
Hamiltonians whose low-energy space contains no stabilizer states. We describe
families that exhibit this requisite property via a simple alteration to local
Hamiltonians corresponding to CSS codes. Our method can also be applied to the
recent NLTS Hamiltonians of Anshu, Breuckmann, and Nirkhe [ABN22], resulting in
a family of local Hamiltonians whose low-energy space contains neither
stabilizer states nor trivial states. We hope that our techniques will
eventually be helpful for constructing Hamiltonians which simultaneously
satisfy NLSS and NLTS.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Coble_N/0/1/0/all/0/1">Nolan J. Coble</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Coudron_M/0/1/0/all/0/1">Matthew Coudron</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Nelson_J/0/1/0/all/0/1">Jon Nelson</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Nezhadi_S/0/1/0/all/0/1">Seyed Sajjad Nezhadi</a></p><p>The recently-defined No Low-energy Sampleable States (NLSS) conjecture of
Gharibian and Le Gall [GL22] posits the existence of a family of local
Hamiltonians where all states of low-enough constant energy do not have
succinct representations allowing perfect sampling access. States that can be
prepared using only Clifford gates (i.e. stabilizer states) are an example of
sampleable states, so the NLSS conjecture implies the existence of local
Hamiltonians whose low-energy space contains no stabilizer states. We describe
families that exhibit this requisite property via a simple alteration to local
Hamiltonians corresponding to CSS codes. Our method can also be applied to the
recent NLTS Hamiltonians of Anshu, Breuckmann, and Nirkhe [ABN22], resulting in
a family of local Hamiltonians whose low-energy space contains neither
stabilizer states nor trivial states. We hope that our techniques will
eventually be helpful for constructing Hamiltonians which simultaneously
satisfy NLSS and NLTS.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-01T01:30:00Z">Wednesday, March 01 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.14125'>A Note on the Faces of the Dual Koch Arrangement</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Bernd G&#xe4;rtner, Manuel Wettstein</p><p>We analyze the faces of the dual Koch arrangement, which is the arrangement
of $2^s + 1$ lines obtained by projective duality from the Koch chain $K_s$. In
particular, we show that this line arrangement does not contain any $k$-gons
for $k &gt; 5$, and that the number of pentagons is $3 \cdot 2^{s-1} - 3$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gartner_B/0/1/0/all/0/1">Bernd G&#xe4;rtner</a>, <a href="http://arxiv.org/find/cs/1/au:+Wettstein_M/0/1/0/all/0/1">Manuel Wettstein</a></p><p>We analyze the faces of the dual Koch arrangement, which is the arrangement
of $2^s + 1$ lines obtained by projective duality from the Koch chain $K_s$. In
particular, we show that this line arrangement does not contain any $k$-gons
for $k &gt; 5$, and that the number of pentagons is $3 \cdot 2^{s-1} - 3$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-01T01:30:00Z">Wednesday, March 01 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.14213'>Crossing Minimization in Time Interval Storylines</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Alexander Dobler, Martin N&#xf6;llenburg, Daniel Stojanovic, Ana&#xef;s Villedieu, Jules Wulms</p><p>Storyline visualizations are a popular way of visualizing characters and
their interactions over time: Characters are drawn as x-monotone curves and
interactions are visualized through close proximity of the corresponding
character curves in a vertical strip. Existing methods to generate storylines
assume a total ordering of the interactions, although real-world data often do
not contain such a total order. Instead, multiple interactions are often
grouped into coarser time intervals such as years. We exploit this grouping
property by introducing a new model called storylines with time intervals and
present two methods to minimize the number of crossings and horizontal space
usage. We then evaluate these algorithms on a small benchmark set to show their
effectiveness.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dobler_A/0/1/0/all/0/1">Alexander Dobler</a>, <a href="http://arxiv.org/find/cs/1/au:+Nollenburg_M/0/1/0/all/0/1">Martin N&#xf6;llenburg</a>, <a href="http://arxiv.org/find/cs/1/au:+Stojanovic_D/0/1/0/all/0/1">Daniel Stojanovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Villedieu_A/0/1/0/all/0/1">Ana&#xef;s Villedieu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wulms_J/0/1/0/all/0/1">Jules Wulms</a></p><p>Storyline visualizations are a popular way of visualizing characters and
their interactions over time: Characters are drawn as x-monotone curves and
interactions are visualized through close proximity of the corresponding
character curves in a vertical strip. Existing methods to generate storylines
assume a total ordering of the interactions, although real-world data often do
not contain such a total order. Instead, multiple interactions are often
grouped into coarser time intervals such as years. We exploit this grouping
property by introducing a new model called storylines with time intervals and
present two methods to minimize the number of crossings and horizontal space
usage. We then evaluate these algorithms on a small benchmark set to show their
effectiveness.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-01T01:30:00Z">Wednesday, March 01 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.14251'>LaplacianFusion: Detailed 3D Clothed-Human Body Reconstruction</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Hyomin Kim, Hyeonseo Nam, Jungeon Kim, Jaesik Park, Seungyong Lee</p><p>We propose LaplacianFusion, a novel approach that reconstructs detailed and
controllable 3D clothed-human body shapes from an input depth or 3D point cloud
sequence. The key idea of our approach is to use Laplacian coordinates,
well-known differential coordinates that have been used for mesh editing, for
representing the local structures contained in the input scans, instead of
implicit 3D functions or vertex displacements used previously. Our approach
reconstructs a controllable base mesh using SMPL, and learns a surface function
that predicts Laplacian coordinates representing surface details on the base
mesh. For a given pose, we first build and subdivide a base mesh, which is a
deformed SMPL template, and then estimate Laplacian coordinates for the mesh
vertices using the surface function. The final reconstruction for the pose is
obtained by integrating the estimated Laplacian coordinates as a whole.
Experimental results show that our approach based on Laplacian coordinates
successfully reconstructs more visually pleasing shape details than previous
methods. The approach also enables various surface detail manipulations, such
as detail transfer and enhancement.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyomin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Nam_H/0/1/0/all/0/1">Hyeonseo Nam</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jungeon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jaesik Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seungyong Lee</a></p><p>We propose LaplacianFusion, a novel approach that reconstructs detailed and
controllable 3D clothed-human body shapes from an input depth or 3D point cloud
sequence. The key idea of our approach is to use Laplacian coordinates,
well-known differential coordinates that have been used for mesh editing, for
representing the local structures contained in the input scans, instead of
implicit 3D functions or vertex displacements used previously. Our approach
reconstructs a controllable base mesh using SMPL, and learns a surface function
that predicts Laplacian coordinates representing surface details on the base
mesh. For a given pose, we first build and subdivide a base mesh, which is a
deformed SMPL template, and then estimate Laplacian coordinates for the mesh
vertices using the surface function. The final reconstruction for the pose is
obtained by integrating the estimated Laplacian coordinates as a whole.
Experimental results show that our approach based on Laplacian coordinates
successfully reconstructs more visually pleasing shape details than previous
methods. The approach also enables various surface detail manipulations, such
as detail transfer and enhancement.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-01T01:30:00Z">Wednesday, March 01 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.14721'>On the geometric thickness of 2-degenerate graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Rahul Jain, Marco Ricci, Jonathan Rollin, Andr&#xe9; Schulz</p><p>A graph is 2-degenerate if every subgraph contains a vertex of degree at most
2. We show that every 2-degenerate graph can be drawn with straight lines such
that the drawing decomposes into 4 plane forests. Therefore, the geometric
arboricity, and hence the geometric thickness, of 2-degenerate graphs is at
most 4. On the other hand, we show that there are 2-degenerate graphs that do
not admit any straight-line drawing with a decomposition of the edge set into 2
plane graphs. That is, there are 2-degenerate graphs with geometric thickness,
and hence geometric arboricity, at least 3. This answers two questions posed by
Eppstein [Separating thickness from geometric thickness. In Towards a Theory of
Geometric Graphs, vol. 342 of Contemp. Math., AMS, 2004].
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Jain_R/0/1/0/all/0/1">Rahul Jain</a>, <a href="http://arxiv.org/find/math/1/au:+Ricci_M/0/1/0/all/0/1">Marco Ricci</a>, <a href="http://arxiv.org/find/math/1/au:+Rollin_J/0/1/0/all/0/1">Jonathan Rollin</a>, <a href="http://arxiv.org/find/math/1/au:+Schulz_A/0/1/0/all/0/1">Andr&#xe9; Schulz</a></p><p>A graph is 2-degenerate if every subgraph contains a vertex of degree at most
2. We show that every 2-degenerate graph can be drawn with straight lines such
that the drawing decomposes into 4 plane forests. Therefore, the geometric
arboricity, and hence the geometric thickness, of 2-degenerate graphs is at
most 4. On the other hand, we show that there are 2-degenerate graphs that do
not admit any straight-line drawing with a decomposition of the edge set into 2
plane graphs. That is, there are 2-degenerate graphs with geometric thickness,
and hence geometric arboricity, at least 3. This answers two questions posed by
Eppstein [Separating thickness from geometric thickness. In Towards a Theory of
Geometric Graphs, vol. 342 of Contemp. Math., AMS, 2004].
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-01T01:30:00Z">Wednesday, March 01 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.14066'>Query-optimal estimation of unitary channels in diamond distance</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jeongwan Haah, Robin Kothari, Ryan O&#x27;Donnell, Ewin Tang</p><p>We consider process tomography for unitary quantum channels. Given access to
an unknown unitary channel acting on a $\textsf{d}$-dimensional qudit, we aim
to output a classical description of a unitary that is $\varepsilon$-close to
the unknown unitary in diamond norm. We design an algorithm achieving error
$\varepsilon$ using $O(\textsf{d}^2/\varepsilon)$ applications of the unknown
channel and only one qudit. This improves over prior results, which use
$O(\textsf{d}^3/\varepsilon^2)$ [via standard process tomography] or
$O(\textsf{d}^{2.5}/\varepsilon)$ [Yang, Renner, and Chiribella, PRL 2020]
applications. To show this result, we introduce a simple technique to
"bootstrap" an algorithm that can produce constant-error estimates to one that
can produce $\varepsilon$-error estimates with the Heisenberg scaling. Finally,
we prove a complementary lower bound showing that estimation requires
$\Omega(\textsf{d}^2/\varepsilon)$ applications, even with access to the
inverse or controlled versions of the unknown unitary. This shows that our
algorithm has both optimal query complexity and optimal space complexity.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Haah_J/0/1/0/all/0/1">Jeongwan Haah</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Kothari_R/0/1/0/all/0/1">Robin Kothari</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+ODonnell_R/0/1/0/all/0/1">Ryan O&#x27;Donnell</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Tang_E/0/1/0/all/0/1">Ewin Tang</a></p><p>We consider process tomography for unitary quantum channels. Given access to
an unknown unitary channel acting on a $\textsf{d}$-dimensional qudit, we aim
to output a classical description of a unitary that is $\varepsilon$-close to
the unknown unitary in diamond norm. We design an algorithm achieving error
$\varepsilon$ using $O(\textsf{d}^2/\varepsilon)$ applications of the unknown
channel and only one qudit. This improves over prior results, which use
$O(\textsf{d}^3/\varepsilon^2)$ [via standard process tomography] or
$O(\textsf{d}^{2.5}/\varepsilon)$ [Yang, Renner, and Chiribella, PRL 2020]
applications. To show this result, we introduce a simple technique to
"bootstrap" an algorithm that can produce constant-error estimates to one that
can produce $\varepsilon$-error estimates with the Heisenberg scaling. Finally,
we prove a complementary lower bound showing that estimation requires
$\Omega(\textsf{d}^2/\varepsilon)$ applications, even with access to the
inverse or controlled versions of the unknown unitary. This shows that our
algorithm has both optimal query complexity and optimal space complexity.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-01T01:30:00Z">Wednesday, March 01 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.14099'>On Differentially Private Online Predictions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Haim Kaplan, Yishay Mansour, Shay Moran, Kobbi Nissim, Uri Stemmer</p><p>In this work we introduce an interactive variant of joint differential
privacy towards handling online processes in which existing privacy definitions
seem too restrictive. We study basic properties of this definition and
demonstrate that it satisfies (suitable variants) of group privacy,
composition, and post processing. We then study the cost of interactive joint
privacy in the basic setting of online classification. We show that any
(possibly non-private) learning rule can be effectively transformed to a
private learning rule with only a polynomial overhead in the mistake bound.
This demonstrates a stark difference with more restrictive notions of privacy
such as the one studied by Golowich and Livni (2021), where only a double
exponential overhead on the mistake bound is known (via an information
theoretic upper bound).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kaplan_H/0/1/0/all/0/1">Haim Kaplan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1">Yishay Mansour</a>, <a href="http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1">Shay Moran</a>, <a href="http://arxiv.org/find/cs/1/au:+Nissim_K/0/1/0/all/0/1">Kobbi Nissim</a>, <a href="http://arxiv.org/find/cs/1/au:+Stemmer_U/0/1/0/all/0/1">Uri Stemmer</a></p><p>In this work we introduce an interactive variant of joint differential
privacy towards handling online processes in which existing privacy definitions
seem too restrictive. We study basic properties of this definition and
demonstrate that it satisfies (suitable variants) of group privacy,
composition, and post processing. We then study the cost of interactive joint
privacy in the basic setting of online classification. We show that any
(possibly non-private) learning rule can be effectively transformed to a
private learning rule with only a polynomial overhead in the mistake bound.
This demonstrates a stark difference with more restrictive notions of privacy
such as the one studied by Golowich and Livni (2021), where only a double
exponential overhead on the mistake bound is known (via an information
theoretic upper bound).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-01T01:30:00Z">Wednesday, March 01 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.14128'>Tight Algorithms for Connectivity Problems Parameterized by Modular-Treewidth</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Falko Hegerfeld, Stefan Kratsch</p><p>We study connectivity problems from a fine-grained parameterized perspective.
Cygan et al. (TALG 2022) obtained algorithms with single-exponential running
time $\alpha^{tw} n^{O(1)}$ for connectivity problems parameterized by
treewidth ($tw$) by introducing the cut-and-count-technique, which reduces
connectivity problems to locally checkable counting problems. In addition, the
bases $\alpha$ were proven to be optimal assuming the Strong Exponential-Time
Hypothesis (SETH).
</p>
<p>As only sparse graphs may admit small treewidth, these results do not apply
to graphs with dense structure. A well-known tool to capture dense structure is
the modular decomposition, which recursively partitions the graph into modules
whose members have the same neighborhood outside of the module. Contracting the
modules yields a quotient graph describing the adjacencies between modules.
Measuring the treewidth of the quotient graph yields the parameter
modular-treewidth, a natural intermediate step between treewidth and
clique-width.
</p>
<p>We obtain the first tight running times for connectivity problems
parameterized by modular-treewidth. For some problems the obtained bounds are
the same as relative to treewidth, showing that we can deal with a greater
generality in input structure at no cost in complexity. We obtain the following
randomized algorithms for graphs of modular-treewidth $k$, given an appropriate
decomposition: Steiner Tree can be solved in time $3^k n^{O(1)}$, Connected
Dominating Set can be solved in time $4^k n^{O(1)}$, Connected Vertex Cover can
be solved in time $5^k n^{O(1)}$, Feedback Vertex Set can be solved in time
$5^k n^{O(1)}$.
</p>
<p>The first two algorithms are tight due to known results and the last two
algorithms are complemented by new tight lower bounds under SETH.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Hegerfeld_F/0/1/0/all/0/1">Falko Hegerfeld</a>, <a href="http://arxiv.org/find/cs/1/au:+Kratsch_S/0/1/0/all/0/1">Stefan Kratsch</a></p><p>We study connectivity problems from a fine-grained parameterized perspective.
Cygan et al. (TALG 2022) obtained algorithms with single-exponential running
time $\alpha^{tw} n^{O(1)}$ for connectivity problems parameterized by
treewidth ($tw$) by introducing the cut-and-count-technique, which reduces
connectivity problems to locally checkable counting problems. In addition, the
bases $\alpha$ were proven to be optimal assuming the Strong Exponential-Time
Hypothesis (SETH).
</p>
<p>As only sparse graphs may admit small treewidth, these results do not apply
to graphs with dense structure. A well-known tool to capture dense structure is
the modular decomposition, which recursively partitions the graph into modules
whose members have the same neighborhood outside of the module. Contracting the
modules yields a quotient graph describing the adjacencies between modules.
Measuring the treewidth of the quotient graph yields the parameter
modular-treewidth, a natural intermediate step between treewidth and
clique-width.
</p>
<p>We obtain the first tight running times for connectivity problems
parameterized by modular-treewidth. For some problems the obtained bounds are
the same as relative to treewidth, showing that we can deal with a greater
generality in input structure at no cost in complexity. We obtain the following
randomized algorithms for graphs of modular-treewidth $k$, given an appropriate
decomposition: Steiner Tree can be solved in time $3^k n^{O(1)}$, Connected
Dominating Set can be solved in time $4^k n^{O(1)}$, Connected Vertex Cover can
be solved in time $5^k n^{O(1)}$, Feedback Vertex Set can be solved in time
$5^k n^{O(1)}$.
</p>
<p>The first two algorithms are tight due to known results and the last two
algorithms are complemented by new tight lower bounds under SETH.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-01T01:30:00Z">Wednesday, March 01 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.14168'>Signal Propagation in Double Edged Relays</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Adam Boucher</p><p>A discrete signal propagation model blending characteristics of linear wave
propagation and finite state automata is developed. We show this model obeys a
limited form of superposition and is capable of displaying a wide variety of
interesting behaviors. We show how the model's superposition properties permit
information to be encoded and retained by signals that pass through discrete
networks. We outline a SPIDER model replacement for Dijkstra's algorithm.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Boucher_A/0/1/0/all/0/1">Adam Boucher</a></p><p>A discrete signal propagation model blending characteristics of linear wave
propagation and finite state automata is developed. We show this model obeys a
limited form of superposition and is capable of displaying a wide variety of
interesting behaviors. We show how the model's superposition properties permit
information to be encoded and retained by signals that pass through discrete
networks. We outline a SPIDER model replacement for Dijkstra's algorithm.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-01T01:30:00Z">Wednesday, March 01 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.14324'>A CS guide to the quantum singular value transformation</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ewin Tang, Kevin Tian</p><p>We present a simplified exposition of some pieces of [Gily\'en, Su, Low, and
Wiebe, STOC'19, arXiv:1806.01838], who introduced a quantum singular value
transformation (QSVT) framework for applying polynomial functions to
block-encoded matrices. The QSVT framework has garnered substantial recent
interest from the quantum algorithms community, as it was demonstrated by
[GSLW19] to encapsulate many existing algorithms naturally phrased as an
application of a matrix function. First, we posit that the lifting of quantum
singular processing (QSP) to QSVT is better viewed not through Jordan's lemma
(as was suggested by [GSLW19]) but as an application of the cosine-sine
decomposition, which can be thought of as a more explicit and stronger version
of Jordan's lemma. Second, we demonstrate that the constructions of bounded
polynomial approximations given in [GSLW19], which use a variety of ad hoc
approaches drawing from Fourier analysis, Chebyshev series, and Taylor series,
can be unified under the framework of truncation of Chebyshev series, and
indeed, can in large part be matched via a bounded variant of a standard
meta-theorem from [Trefethen, 2013]. We hope this work finds use to the
community as a companion guide for understanding and applying the powerful
framework of [GSLW19].
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Tang_E/0/1/0/all/0/1">Ewin Tang</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Tian_K/0/1/0/all/0/1">Kevin Tian</a></p><p>We present a simplified exposition of some pieces of [Gily\'en, Su, Low, and
Wiebe, STOC'19, <a href="/abs/1806.01838">arXiv:1806.01838</a>], who introduced a quantum singular value
transformation (QSVT) framework for applying polynomial functions to
block-encoded matrices. The QSVT framework has garnered substantial recent
interest from the quantum algorithms community, as it was demonstrated by
[GSLW19] to encapsulate many existing algorithms naturally phrased as an
application of a matrix function. First, we posit that the lifting of quantum
singular processing (QSP) to QSVT is better viewed not through Jordan's lemma
(as was suggested by [GSLW19]) but as an application of the cosine-sine
decomposition, which can be thought of as a more explicit and stronger version
of Jordan's lemma. Second, we demonstrate that the constructions of bounded
polynomial approximations given in [GSLW19], which use a variety of ad hoc
approaches drawing from Fourier analysis, Chebyshev series, and Taylor series,
can be unified under the framework of truncation of Chebyshev series, and
indeed, can in large part be matched via a bounded variant of a standard
meta-theorem from [Trefethen, 2013]. We hope this work finds use to the
community as a companion guide for understanding and applying the powerful
framework of [GSLW19].
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-01T01:30:00Z">Wednesday, March 01 2023, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2302.14386'>Practical Algorithms for Orientations of Partially Directed Graphical Models</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Malte Luttermann, Marcel Wien&#xf6;bst, Maciej Li&#x15b;kiewicz</p><p>In observational studies, the true causal model is typically unknown and
needs to be estimated from available observational and limited experimental
data. In such cases, the learned causal model is commonly represented as a
partially directed acyclic graph (PDAG), which contains both directed and
undirected edges indicating uncertainty of causal relations between random
variables. The main focus of this paper is on the maximal orientation task,
which, for a given PDAG, aims to orient the undirected edges maximally such
that the resulting graph represents the same Markov equivalent DAGs as the
input PDAG. This task is a subroutine used frequently in causal discovery, e.
g., as the final step of the celebrated PC algorithm. Utilizing connections to
the problem of finding a consistent DAG extension of a PDAG, we derive faster
algorithms for computing the maximal orientation by proposing two novel
approaches for extending PDAGs, both constructed with an emphasis on simplicity
and practical effectiveness.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Luttermann_M/0/1/0/all/0/1">Malte Luttermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Wienobst_M/0/1/0/all/0/1">Marcel Wien&#xf6;bst</a>, <a href="http://arxiv.org/find/cs/1/au:+Liskiewicz_M/0/1/0/all/0/1">Maciej Li&#x15b;kiewicz</a></p><p>In observational studies, the true causal model is typically unknown and
needs to be estimated from available observational and limited experimental
data. In such cases, the learned causal model is commonly represented as a
partially directed acyclic graph (PDAG), which contains both directed and
undirected edges indicating uncertainty of causal relations between random
variables. The main focus of this paper is on the maximal orientation task,
which, for a given PDAG, aims to orient the undirected edges maximally such
that the resulting graph represents the same Markov equivalent DAGs as the
input PDAG. This task is a subroutine used frequently in causal discovery, e.
g., as the final step of the celebrated PC algorithm. Utilizing connections to
the problem of finding a consistent DAG extension of a PDAG, we derive faster
algorithms for computing the maximal orientation by proposing two novel
approaches for extending PDAGs, both constructed with an emphasis on simplicity
and practical effectiveness.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-03-01T01:30:00Z">Wednesday, March 01 2023, 01:30</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
