<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.4 (2022-04-12) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2022-11-23T17:31:00Z">Wednesday, November 23 2022, 17:31</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>SÃ©bastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Wednesday, November 23
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/23/faculty-any-rank-at-penn-state-apply-by-november-15-2022/'>FACULTY (ANY RANK) at PENN STATE (apply by November 15, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Applications are invited for tenure-track positions at all levels across all areas of theoretical computer science. Review of applications started on 11/15/2022 and will continue until the positions is filled. Website: academicjobsonline.org/ajo/jobs/23484 Email: ablanca@cse.psu.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Applications are invited for tenure-track positions at all levels across all areas of theoretical computer science. Review of applications started on 11/15/2022 and will continue until the positions is filled.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/23484">https://academicjobsonline.org/ajo/jobs/23484</a><br />
Email: ablanca@cse.psu.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T15:58:56Z">Wednesday, November 23 2022, 15:58</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/166'>TR22-166 |  Characterizing the Multi-Pass Streaming Complexity for Solving Boolean CSPs Exactly | 

	Raghuvansh Saxena, 

	Gillat Kol, 

	Dmitry Paramonov, 

	Huacheng Yu</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We study boolean constraint satisfaction problems (CSPs) $\mathrm{Max}\text{-}\mathrm{CSP}^f_n$ for all predicates $f: \{ 0, 1 \} ^k \to \{ 0, 1 \}$. In these problems, given an integer $v$ and a list of constraints over $n$ boolean variables, each obtained by applying $f$ to a sequence of literals, we wish to decide if there is an assignment to the variables that satisfies at least $v$ constraints. We consider these problems in the streaming model, where the algorithm makes a small number of passes over the list of constraints.

Our first and main result is the following complete characterization: For every predicate $f$, the streaming space complexity of the $\mathrm{Max}\text{-}\mathrm{CSP}^f_n$ problem is $\tilde{\Theta}(n^{\mathrm{deg}(f)})$, where $\mathrm{deg}(f)$ is the degree of $f$ when viewed as a multilinear polynomial. While the upper bound is obtained by a (very simple) one-pass streaming algorithm, our lower bound shows that a better space complexity is impossible even with constant-pass streaming algorithms. 

Building on our techniques, we are also able to get an optimal $\Omega(n^2)$ lower bound on the space complexity of constant-pass streaming algorithms for the well studied $\mathrm{Max}\text{-}\mathrm{CUT}$ problem, even though it is not technically a $\mathrm{Max}\text{-}\mathrm{CSP}^f_n$ problem as, e.g., negations of variables and repeated constraints are not allowed.
        
        </div>

        <div class='tr-article-summary'>
        
          
          We study boolean constraint satisfaction problems (CSPs) $\mathrm{Max}\text{-}\mathrm{CSP}^f_n$ for all predicates $f: \{ 0, 1 \} ^k \to \{ 0, 1 \}$. In these problems, given an integer $v$ and a list of constraints over $n$ boolean variables, each obtained by applying $f$ to a sequence of literals, we wish to decide if there is an assignment to the variables that satisfies at least $v$ constraints. We consider these problems in the streaming model, where the algorithm makes a small number of passes over the list of constraints.

Our first and main result is the following complete characterization: For every predicate $f$, the streaming space complexity of the $\mathrm{Max}\text{-}\mathrm{CSP}^f_n$ problem is $\tilde{\Theta}(n^{\mathrm{deg}(f)})$, where $\mathrm{deg}(f)$ is the degree of $f$ when viewed as a multilinear polynomial. While the upper bound is obtained by a (very simple) one-pass streaming algorithm, our lower bound shows that a better space complexity is impossible even with constant-pass streaming algorithms. 

Building on our techniques, we are also able to get an optimal $\Omega(n^2)$ lower bound on the space complexity of constant-pass streaming algorithms for the well studied $\mathrm{Max}\text{-}\mathrm{CUT}$ problem, even though it is not technically a $\mathrm{Max}\text{-}\mathrm{CSP}^f_n$ problem as, e.g., negations of variables and repeated constraints are not allowed.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T07:12:10Z">Wednesday, November 23 2022, 07:12</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=6830'>Dismantling Quantum Hype with Tim Nguyen</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Happy Thanksgiving to my American readers! While I enjoy a family holiday-week vacation in exotic Dallas&#8212;and yes, I will follow up on my old JFK post by visiting Dealey Plaza&#8212;please enjoy the following Thanksgiving victuals: I recently recorded a 3-hour (!) YouTube video with Timothy Nguyen, host of the Cartesian Cafe. Our episode is entitled [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Happy Thanksgiving to my American readers!  While I enjoy a family holiday-week vacation in exotic Dallas&#8212;and yes, I <em>will</em> follow up on my old <a href="https://scottaaronson.blog/?p=1596">JFK post</a> by visiting Dealey Plaza&#8212;please enjoy the following Thanksgiving victuals:</p>



<p>I recently recorded a 3-hour (!) YouTube video with <a href="https://timothynguyen.org/">Timothy Nguyen</a>, host of the <a href="https://timothynguyen.org/videos/#cartesian-cafe">Cartesian Cafe</a>.  Our episode is entitled <a href="https://www.youtube.com/watch?v=qs0D9sdbKPU">Quantum Computing: Dismantling the Hype</a>.  In it, I teach a sort of <em>extremely</em> compressed version of my <a href="https://www.scottaaronson.com/qclec.pdf">undergraduate Intro to Quantum Information Science course</a>, unburdening myself about whatever Tim prompts me to explain: the basic rules of quantum information, <a href="https://en.wikipedia.org/wiki/Quantum_circuit">quantum circuits</a>, the <a href="https://arxiv.org/abs/1712.06349">quantum black-box model</a>, the <a href="https://en.wikipedia.org/wiki/Deutsch%E2%80%93Jozsa_algorithm">Deutsch-Jozsa algorithm</a>, <a href="https://en.wikipedia.org/wiki/BQP">BQP</a> and its relationship to classical complexity classes, and sampling-based quantum supremacy experiments.  This is a lot more technical than an average podcast, a lot <em>less</em> technical than an actual course, and hopefully just right for some nonempty subset of readers.</p>



<p>Outside of his podcasting career, some of you might recognize Nguyen as the coauthor, with Theo Polya, of a <a href="https://timothynguyen.files.wordpress.com/2021/02/geometric_unity.pdf">rebuttal of &#8220;Geometric Unity.&#8221;</a>  This latter is the proposal by the financier, podcaster, and leading &#8220;Intellectual Dark Web&#8221; figure <a href="https://en.wikipedia.org/wiki/Eric_Weinstein">Eric Weinstein</a> for a unified theory of particle physics.  Now, I slightly know Weinstein, and have even found him fascinating, eloquent, and correct about various issues.  So, in an <a href="https://www.youtube.com/watch?v=wd-0COLM8oc">addendum to the main video</a>, Nguyen chats with me about his experience critiquing Weinstein&#8217;s theory, and also about something where my knowledge is far greater: namely, my <a href="https://arxiv.org/abs/quant-ph/0206089">2002 rebuttal</a> of some of the central claims in Stephen Wolfram&#8217;s <em><a href="https://en.wikipedia.org/wiki/A_New_Kind_of_Science">A New Kind of Science</a></em>, and whether there are any updates to that story twenty years later.</p>



<p>Enjoy!</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T04:12:34Z">Wednesday, November 23 2022, 04:12</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.11839'>Celeste is PSPACE-hard</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Lily Chung, Erik D. Demaine</p><p>We investigate the complexity of the platform video game Celeste. We prove
that navigating Celeste is PSPACE-hard in five different ways, corresponding to
different subsets of the game mechanics. In particular, we prove the game
PSPACE-hard even without player input.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chung_L/0/1/0/all/0/1">Lily Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Demaine_E/0/1/0/all/0/1">Erik D. Demaine</a></p><p>We investigate the complexity of the platform video game Celeste. We prove
that navigating Celeste is PSPACE-hard in five different ways, corresponding to
different subsets of the game mechanics. In particular, we prove the game
PSPACE-hard even without player input.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.12447'>Quantum algorithms and the power of forgetting</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Andrew M. Childs, Matthew Coudron, Amin Shiraz Gilani</p><p>The so-called welded tree problem provides an example of a black-box problem
that can be solved exponentially faster by a quantum walk than by any classical
algorithm. Given the name of a special ENTRANCE vertex, a quantum walk can find
another distinguished EXIT vertex using polynomially many queries, though
without finding any particular path from ENTRANCE to EXIT. It has been an open
problem for twenty years whether there is an efficient quantum algorithm for
finding such a path, or if the path-finding problem is hard even for quantum
computers. We show that a natural class of efficient quantum algorithms
provably cannot find a path from ENTRANCE to EXIT. Specifically, we consider
algorithms that, within each branch of their superposition, always store a set
of vertex labels that form a connected subgraph including the ENTRANCE, and
that only provide these vertex labels as inputs to the oracle. While this does
not rule out the possibility of a quantum algorithm that efficiently finds a
path, it is unclear how an algorithm could benefit by deviating from this
behavior. Our no-go result suggests that, for some problems, quantum algorithms
must necessarily forget the path they take to reach a solution in order to
outperform classical computation.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Childs_A/0/1/0/all/0/1">Andrew M. Childs</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Coudron_M/0/1/0/all/0/1">Matthew Coudron</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Gilani_A/0/1/0/all/0/1">Amin Shiraz Gilani</a></p><p>The so-called welded tree problem provides an example of a black-box problem
that can be solved exponentially faster by a quantum walk than by any classical
algorithm. Given the name of a special ENTRANCE vertex, a quantum walk can find
another distinguished EXIT vertex using polynomially many queries, though
without finding any particular path from ENTRANCE to EXIT. It has been an open
problem for twenty years whether there is an efficient quantum algorithm for
finding such a path, or if the path-finding problem is hard even for quantum
computers. We show that a natural class of efficient quantum algorithms
provably cannot find a path from ENTRANCE to EXIT. Specifically, we consider
algorithms that, within each branch of their superposition, always store a set
of vertex labels that form a connected subgraph including the ENTRANCE, and
that only provide these vertex labels as inputs to the oracle. While this does
not rule out the possibility of a quantum algorithm that efficiently finds a
path, it is unclear how an algorithm could benefit by deviating from this
behavior. Our no-go result suggests that, for some problems, quantum algorithms
must necessarily forget the path they take to reach a solution in order to
outperform classical computation.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.12456'>Exponential separations using guarded extension variables</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Emre Yolcu, Marijn J.H. Heule</p><p>We study the complexity of proof systems augmenting resolution with inference
rules that allow, given a formula $\Gamma$ in conjunctive normal form, deriving
clauses that are not necessarily logically implied by $\Gamma$ but whose
addition to $\Gamma$ preserves satisfiability. When the derived clauses are
allowed to introduce variables not occurring in $\Gamma$, the systems we
consider become equivalent to extended resolution. We are concerned with the
versions of these systems without new variables. They are called BC${}^-$,
RAT${}^-$, SBC${}^-$, and GER${}^-$, denoting respectively blocked clauses,
resolution asymmetric tautologies, set-blocked clauses, and generalized
extended resolution. Each of these systems formalizes some restricted version
of the ability to make assumptions that hold "without loss of generality,"
which is commonly used informally to simplify or shorten proofs.
</p>
<p>Except for SBC${}^-$, these systems are known to be exponentially weaker than
extended resolution. They are, however, all equivalent to it under a relaxed
notion of simulation that allows the translation of the formula along with the
proof when moving between proof systems. By taking advantage of this fact, we
construct formulas that separate RAT${}^-$ from GER${}^-$ and vice versa. With
the same strategy, we also separate SBC${}^-$ from RAT${}^-$. Additionally, we
give polynomial-size SBC${}^-$ proofs of the pigeonhole principle, which
separates SBC${}^-$ from GER${}^-$ by a previously known lower bound. These
results also separate the three systems from BC${}^-$ since they all simulate
it. We thus give an almost complete picture of their relative strengths.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Yolcu_E/0/1/0/all/0/1">Emre Yolcu</a>, <a href="http://arxiv.org/find/cs/1/au:+Heule_M/0/1/0/all/0/1">Marijn J.H. Heule</a></p><p>We study the complexity of proof systems augmenting resolution with inference
rules that allow, given a formula $\Gamma$ in conjunctive normal form, deriving
clauses that are not necessarily logically implied by $\Gamma$ but whose
addition to $\Gamma$ preserves satisfiability. When the derived clauses are
allowed to introduce variables not occurring in $\Gamma$, the systems we
consider become equivalent to extended resolution. We are concerned with the
versions of these systems without new variables. They are called BC${}^-$,
RAT${}^-$, SBC${}^-$, and GER${}^-$, denoting respectively blocked clauses,
resolution asymmetric tautologies, set-blocked clauses, and generalized
extended resolution. Each of these systems formalizes some restricted version
of the ability to make assumptions that hold "without loss of generality,"
which is commonly used informally to simplify or shorten proofs.
</p>
<p>Except for SBC${}^-$, these systems are known to be exponentially weaker than
extended resolution. They are, however, all equivalent to it under a relaxed
notion of simulation that allows the translation of the formula along with the
proof when moving between proof systems. By taking advantage of this fact, we
construct formulas that separate RAT${}^-$ from GER${}^-$ and vice versa. With
the same strategy, we also separate SBC${}^-$ from RAT${}^-$. Additionally, we
give polynomial-size SBC${}^-$ proofs of the pigeonhole principle, which
separates SBC${}^-$ from GER${}^-$ by a previously known lower bound. These
results also separate the three systems from BC${}^-$ since they all simulate
it. We thus give an almost complete picture of their relative strengths.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.11987'>The Tight Spanning Ratio of the Rectangle Delaunay Triangulation</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Andr&#xe8; van Renssen, Yuan Sha, Yucheng Sun, Sampson Wong</p><p>Spanner construction is a well-studied problem and Delaunay triangulations
are among the most popular spanners. Tight bounds are known if the Delaunay
triangulation is constructed using an equilateral triangle, a square, or a
regular hexagon. However, all other shapes have remained elusive. In this paper
we extend the restricted class of spanners for which tight bounds are known. We
prove that Delaunay triangulations constructed using rectangles with aspect
ratio $\A$ have spanning ratio at most $\sqrt{2} \sqrt{1+\A^2 + \A \sqrt{\A^2 +
1}}$, which matches the known lower bound.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Renssen_A/0/1/0/all/0/1">Andr&#xe8; van Renssen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sha_Y/0/1/0/all/0/1">Yuan Sha</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yucheng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_S/0/1/0/all/0/1">Sampson Wong</a></p><p>Spanner construction is a well-studied problem and Delaunay triangulations
are among the most popular spanners. Tight bounds are known if the Delaunay
triangulation is constructed using an equilateral triangle, a square, or a
regular hexagon. However, all other shapes have remained elusive. In this paper
we extend the restricted class of spanners for which tight bounds are known. We
prove that Delaunay triangulations constructed using rectangles with aspect
ratio $\A$ have spanning ratio at most $\sqrt{2} \sqrt{1+\A^2 + \A \sqrt{\A^2 +
1}}$, which matches the known lower bound.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.12203'>Edge Multiway Cut and Node Multiway Cut are NP-complete on subcubic graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Matthew Johnson, Barnaby Martin, Siani Smith, Sukanya Pandey, Daniel Paulusma, Erik Jan van Leeuwen</p><p>We show that Edge Multiway Cut (also called Multiterminal Cut) and Node
Multiway Cut are NP-complete on graphs of maximum degree $3$ (also known as
subcubic graphs). This improves on a previous degree bound of $11$. Our
NP-completeness result holds even for subcubic graphs that are planar.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1">Matthew Johnson</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_B/0/1/0/all/0/1">Barnaby Martin</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1">Siani Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1">Sukanya Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Paulusma_D/0/1/0/all/0/1">Daniel Paulusma</a>, <a href="http://arxiv.org/find/cs/1/au:+Leeuwen_E/0/1/0/all/0/1">Erik Jan van Leeuwen</a></p><p>We show that Edge Multiway Cut (also called Multiterminal Cut) and Node
Multiway Cut are NP-complete on graphs of maximum degree $3$ (also known as
subcubic graphs). This improves on a previous degree bound of $11$. Our
NP-completeness result holds even for subcubic graphs that are planar.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.11846'>Labeled Nearest Neighbor Search and Metric Spanners via Locality Sensitive Orderings</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Arnold Filtser</p><p>Chan, Har-Peled, and Jones [SICOMP 2020] developed locality-sensitive
orderings (LSO) for Euclidean space. A $(\tau,\rho)$-LSO is a collection
$\Sigma$ of orderings such that for every $x,y\in\mathbb{R}^d$ there is an
ordering $\sigma\in\Sigma$, where all the points between $x$ and $y$ w.r.t.
$\sigma$ are in the $\rho$-neighborhood of either $x$ or $y$. In essence, LSO
allow one to reduce problems to the $1$-dimensional line. Later, Filtser and Le
[STOC 2022] developed LSO's for doubling metrics, general metric spaces, and
minor free graphs.
</p>
<p>For Euclidean and doubling spaces, the number of orderings in the LSO is
exponential in the dimension, which made them mainly useful for the low
dimensional regime. In this paper, we develop new LSO's for Euclidean,
$\ell_p$, and doubling spaces that allow us to trade larger stretch for a much
smaller number of orderings. We then use our new LSO's (as well as the previous
ones) to construct path reporting low hop spanners, fault tolerant spanners,
reliable spanners, and light spanners for different metric spaces.
</p>
<p>While many nearest neighbor search (NNS) data structures were constructed for
metric spaces with implicit distance representations (where the distance
between two metric points can be computed using their names, e.g. Euclidean
space), for other spaces almost nothing is known. In this paper we initiate the
study of the labeled NNS problem, where one is allowed to artificially assign
labels (short names) to metric points. We use LSO's to construct efficient
labeled NNS data structures in this model.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Filtser_A/0/1/0/all/0/1">Arnold Filtser</a></p><p>Chan, Har-Peled, and Jones [SICOMP 2020] developed locality-sensitive
orderings (LSO) for Euclidean space. A $(\tau,\rho)$-LSO is a collection
$\Sigma$ of orderings such that for every $x,y\in\mathbb{R}^d$ there is an
ordering $\sigma\in\Sigma$, where all the points between $x$ and $y$ w.r.t.
$\sigma$ are in the $\rho$-neighborhood of either $x$ or $y$. In essence, LSO
allow one to reduce problems to the $1$-dimensional line. Later, Filtser and Le
[STOC 2022] developed LSO's for doubling metrics, general metric spaces, and
minor free graphs.
</p>
<p>For Euclidean and doubling spaces, the number of orderings in the LSO is
exponential in the dimension, which made them mainly useful for the low
dimensional regime. In this paper, we develop new LSO's for Euclidean,
$\ell_p$, and doubling spaces that allow us to trade larger stretch for a much
smaller number of orderings. We then use our new LSO's (as well as the previous
ones) to construct path reporting low hop spanners, fault tolerant spanners,
reliable spanners, and light spanners for different metric spaces.
</p>
<p>While many nearest neighbor search (NNS) data structures were constructed for
metric spaces with implicit distance representations (where the distance
between two metric points can be computed using their names, e.g. Euclidean
space), for other spaces almost nothing is known. In this paper we initiate the
study of the labeled NNS problem, where one is allowed to artificially assign
labels (short names) to metric points. We use LSO's to construct efficient
labeled NNS data structures in this model.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.11923'>Towards Optimal Coreset Construction for $(k,z)$-Clustering: Breaking the Quadratic Dependency on $k$</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Lingxiao Huang, Jian Li, Xuan Wu</p><p>Constructing small-sized coresets for various clustering problems has
attracted significant attention recently. We provide efficient coreset
construction algorithms for $(k, z)$-Clustering with improved coreset sizes in
several metric spaces. In particular, we provide an
$\tilde{O}_z(k^{(2z+2)/(z+2)}\varepsilon^{-2})$-sized coreset for $(k,
z)$-Clustering for all $z\geq 1$ in Euclidean space, improving upon the best
known $\tilde{O}_z(k^2\varepsilon^{-2})$ size upper bound [Cohen-Addad, Larsen,
Saulpic, Schwiegelshohn. STOC'22], breaking the quadratic dependency on $k$ for
the first time (when $k\leq \varepsilon^{-1}$). For example, our coreset size
for Euclidean $k$-Median is $\tilde{O}(k^{4/3} \varepsilon^{-2})$, improving
the best known result $\tilde{O}(\min\left\{k^2\varepsilon^{-2},
k\varepsilon^{-3}\right\})$ by a factor $k^{2/3}$ when $k\leq
\varepsilon^{-1}$; for Euclidean $k$-Means, our coreset size is
$\tilde{O}(k^{3/2} \varepsilon^{-2})$, improving the best known result
$\tilde{O}(\min\left\{k^2\varepsilon^{-2}, k\varepsilon^{-4}\right\})$ by a
factor $k^{1/2}$ when $k\leq \varepsilon^{-2}$. We also obtain optimal or
improved coreset sizes for general metric space, metric space with bounded
doubling dimension, and shortest path metric when the underlying graph has
bounded treewidth, for all $z\geq 1$. Our algorithm largely follows the
framework developed by Cohen-Addad et al. with some minor but useful changes.
Our technical contribution mainly lies in the analysis. An important
improvement in our analysis is a new notion of $\alpha$-covering of distance
vectors with a novel error metric, which allows us to provide a tighter
variance bound. Another useful technical ingredient is terminal embedding with
additive errors, for bounding the covering number in the Euclidean case.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lingxiao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xuan Wu</a></p><p>Constructing small-sized coresets for various clustering problems has
attracted significant attention recently. We provide efficient coreset
construction algorithms for $(k, z)$-Clustering with improved coreset sizes in
several metric spaces. In particular, we provide an
$\tilde{O}_z(k^{(2z+2)/(z+2)}\varepsilon^{-2})$-sized coreset for $(k,
z)$-Clustering for all $z\geq 1$ in Euclidean space, improving upon the best
known $\tilde{O}_z(k^2\varepsilon^{-2})$ size upper bound [Cohen-Addad, Larsen,
Saulpic, Schwiegelshohn. STOC'22], breaking the quadratic dependency on $k$ for
the first time (when $k\leq \varepsilon^{-1}$). For example, our coreset size
for Euclidean $k$-Median is $\tilde{O}(k^{4/3} \varepsilon^{-2})$, improving
the best known result $\tilde{O}(\min\left\{k^2\varepsilon^{-2},
k\varepsilon^{-3}\right\})$ by a factor $k^{2/3}$ when $k\leq
\varepsilon^{-1}$; for Euclidean $k$-Means, our coreset size is
$\tilde{O}(k^{3/2} \varepsilon^{-2})$, improving the best known result
$\tilde{O}(\min\left\{k^2\varepsilon^{-2}, k\varepsilon^{-4}\right\})$ by a
factor $k^{1/2}$ when $k\leq \varepsilon^{-2}$. We also obtain optimal or
improved coreset sizes for general metric space, metric space with bounded
doubling dimension, and shortest path metric when the underlying graph has
bounded treewidth, for all $z\geq 1$. Our algorithm largely follows the
framework developed by Cohen-Addad et al. with some minor but useful changes.
Our technical contribution mainly lies in the analysis. An important
improvement in our analysis is a new notion of $\alpha$-covering of distance
vectors with a novel error metric, which allows us to provide a tighter
variance bound. Another useful technical ingredient is terminal embedding with
additive errors, for bounding the covering number in the Euclidean case.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.11856'>String Covering: A Survey</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Neerja Mhaskar, W. F. Smyth</p><p>The study of strings is an important combinatorial field that precedes the
digital computer. Strings can be very long, trillions of letters, so it is
important to find compact representations. Here we first survey various forms
of one potential compaction methodology, the cover of a given string x,
initially proposed in a simple form in 1990, but increasingly of interest as
more sophisticated variants have been discovered. We then consider covering by
a seed; that is, a cover of a superstring of x. We conclude with many proposals
for research directions that could make significant contributions to string
processing in future.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Mhaskar_N/0/1/0/all/0/1">Neerja Mhaskar</a>, <a href="http://arxiv.org/find/cs/1/au:+Smyth_W/0/1/0/all/0/1">W. F. Smyth</a></p><p>The study of strings is an important combinatorial field that precedes the
digital computer. Strings can be very long, trillions of letters, so it is
important to find compact representations. Here we first survey various forms
of one potential compaction methodology, the cover of a given string x,
initially proposed in a simple form in 1990, but increasingly of interest as
more sophisticated variants have been discovered. We then consider covering by
a seed; that is, a cover of a superstring of x. We conclude with many proposals
for research directions that could make significant contributions to string
processing in future.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.11860'>Upper and Lower Bounds on the Smoothed Complexity of the Simplex Method</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sophie Huiberts, Yin Tat Lee, Xinzhi Zhang</p><p>The simplex method for linear programming is known to be highly efficient in
practice, and understanding its performance from a theoretical perspective is
an active research topic. The framework of smoothed analysis, first introduced
by Spielman and Teng (JACM '04) for this purpose, defines the smoothed
complexity of solving a linear program with $d$ variables and $n$ constraints
as the expected running time when Gaussian noise of variance $\sigma^2$ is
added to the LP data. We prove that the smoothed complexity of the simplex
method is $O(\sigma^{-3/2} d^{13/4}\log^{7/4} n)$, improving the dependence on
$1/\sigma$ compared to the previous bound of $O(\sigma^{-2} d^2\sqrt{\log n})$.
We accomplish this through a new analysis of the \emph{shadow bound}, key to
earlier analyses as well. Illustrating the power of our new method, we use our
method to prove a nearly tight upper bound on the smoothed complexity of
two-dimensional polygons.
</p>
<p>We also establish the first non-trivial lower bound on the smoothed
complexity of the simplex method, proving that the \emph{shadow vertex simplex
method} requires at least $\Omega \Big(\min \big(\sigma^{-1/2}
d^{-1/2}\log^{-1/4} d,2^d \big) \Big)$ pivot steps with high probability. A key
part of our analysis is a new variation on the extended formulation for the
regular $2^k$-gon. We end with a numerical experiment that suggests this
analysis could be further improved.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Huiberts_S/0/1/0/all/0/1">Sophie Huiberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yin Tat Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinzhi Zhang</a></p><p>The simplex method for linear programming is known to be highly efficient in
practice, and understanding its performance from a theoretical perspective is
an active research topic. The framework of smoothed analysis, first introduced
by Spielman and Teng (JACM '04) for this purpose, defines the smoothed
complexity of solving a linear program with $d$ variables and $n$ constraints
as the expected running time when Gaussian noise of variance $\sigma^2$ is
added to the LP data. We prove that the smoothed complexity of the simplex
method is $O(\sigma^{-3/2} d^{13/4}\log^{7/4} n)$, improving the dependence on
$1/\sigma$ compared to the previous bound of $O(\sigma^{-2} d^2\sqrt{\log n})$.
We accomplish this through a new analysis of the \emph{shadow bound}, key to
earlier analyses as well. Illustrating the power of our new method, we use our
method to prove a nearly tight upper bound on the smoothed complexity of
two-dimensional polygons.
</p>
<p>We also establish the first non-trivial lower bound on the smoothed
complexity of the simplex method, proving that the \emph{shadow vertex simplex
method} requires at least $\Omega \Big(\min \big(\sigma^{-1/2}
d^{-1/2}\log^{-1/4} d,2^d \big) \Big)$ pivot steps with high probability. A key
part of our analysis is a new variation on the extended formulation for the
regular $2^k$-gon. We end with a numerical experiment that suggests this
analysis could be further improved.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.11912'>Quasi-stable Coloring for Graph Compression: Approximating Max-Flow, Linear Programs, and Centrality</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Moe Kayali, Dan Suciu</p><p>We propose quasi-stable coloring, an approximate version of stable coloring.
Stable coloring, also called color refinement, is a well-studied technique in
graph theory for classifying vertices, which can be used to build compact,
lossless representations of graphs. However, its usefulness is limited due to
its reliance on strict symmetries. Real data compresses very poorly using color
refinement. We propose the first, to our knowledge, approximate color
refinement scheme, which we call quasi-stable coloring. By using approximation,
we alleviate the need for strict symmetry, and allow for a tradeoff between the
degree of compression and the accuracy of the representation. We study three
applications: Linear Programming, Max-Flow, and Betweenness Centrality, and
provide theoretical evidence in each case that a quasi-stable coloring can lead
to good approximations on the reduced graph. Next, we consider how to compute a
maximal quasi-stable coloring: we prove that, in general, this problem is
NP-hard, and propose a simple, yet effective algorithm based on heuristics.
Finally, we evaluate experimentally the quasi-stable coloring technique on
several real graphs and applications, comparing with prior approximation
techniques.
</p>
<p>A reference implementation and the experiment code are available at
github.com/mkyl/QuasiStableColors.jl
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kayali_M/0/1/0/all/0/1">Moe Kayali</a>, <a href="http://arxiv.org/find/cs/1/au:+Suciu_D/0/1/0/all/0/1">Dan Suciu</a></p><p>We propose quasi-stable coloring, an approximate version of stable coloring.
Stable coloring, also called color refinement, is a well-studied technique in
graph theory for classifying vertices, which can be used to build compact,
lossless representations of graphs. However, its usefulness is limited due to
its reliance on strict symmetries. Real data compresses very poorly using color
refinement. We propose the first, to our knowledge, approximate color
refinement scheme, which we call quasi-stable coloring. By using approximation,
we alleviate the need for strict symmetry, and allow for a tradeoff between the
degree of compression and the accuracy of the representation. We study three
applications: Linear Programming, Max-Flow, and Betweenness Centrality, and
provide theoretical evidence in each case that a quasi-stable coloring can lead
to good approximations on the reduced graph. Next, we consider how to compute a
maximal quasi-stable coloring: we prove that, in general, this problem is
NP-hard, and propose a simple, yet effective algorithm based on heuristics.
Finally, we evaluate experimentally the quasi-stable coloring technique on
several real graphs and applications, comparing with prior approximation
techniques.
</p>
<p>A reference implementation and the experiment code are available at
https://github.com/mkyl/QuasiStableColors.jl
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.11961'>Online facility location with timed-requests and congestion</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Arghya Chakraborty, Rahul Vaze</p><p>The classic online facility location problem deals with finding the optimal
set of facilities in an online fashion when demand requests arrive one at a
time and facilities need to be opened to service these requests. In this work,
we study two variants of the online facility location problem; (1) timed
requests and (2) congestion. Both of these variants are motivated by the
applications to real life and the previously known results on online facility
location cannot be directly adapted to analyse them.
</p>
<p>Timed requests : In this variant, each demand request is a pair $(x,t)$ where
the $x$ is the standard location of the demand while $t$ is the corresponding
weight of the request. The cost of servicing request $(x,t)$ at facility $F$ is
$t\cdot d(x,F')$ where $F'$ is the set of facilities available at the time of
request $(x,t)$. For this variant, we present an online algorithm attaining a
competitive ratio of $\mathcal{O}(\log n)$ in the secretarial model for the
timed requests and show that it is optimal.
</p>
<p>Congestion : The congestion variant considers the case when there is an
additional congestion cost that grows with the number of requests served by
each request. For this variant, when the congestion cost is a monomial, we show
that there exists an algorithm attaining a constant competitive ratio. This
constant is a function of the exponent of the monomial and the facility opening
cost but independent of the number of requests.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1">Arghya Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Vaze_R/0/1/0/all/0/1">Rahul Vaze</a></p><p>The classic online facility location problem deals with finding the optimal
set of facilities in an online fashion when demand requests arrive one at a
time and facilities need to be opened to service these requests. In this work,
we study two variants of the online facility location problem; (1) timed
requests and (2) congestion. Both of these variants are motivated by the
applications to real life and the previously known results on online facility
location cannot be directly adapted to analyse them.
</p>
<p>Timed requests : In this variant, each demand request is a pair $(x,t)$ where
the $x$ is the standard location of the demand while $t$ is the corresponding
weight of the request. The cost of servicing request $(x,t)$ at facility $F$ is
$t\cdot d(x,F')$ where $F'$ is the set of facilities available at the time of
request $(x,t)$. For this variant, we present an online algorithm attaining a
competitive ratio of $\mathcal{O}(\log n)$ in the secretarial model for the
timed requests and show that it is optimal.
</p>
<p>Congestion : The congestion variant considers the case when there is an
additional congestion cost that grows with the number of requests served by
each request. For this variant, when the congestion cost is a monomial, we show
that there exists an algorithm attaining a constant competitive ratio. This
constant is a function of the exponent of the monomial and the facility opening
cost but independent of the number of requests.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.11967'>Support Size Estimation: The Power of Conditioning</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Diptarka Chakraborty, Gunjan Kumar, Kuldeep S. Meel</p><p>We consider the problem of estimating the support size of a distribution $D$.
Our investigations are pursued through the lens of distribution testing and
seek to understand the power of conditional sampling (denoted as COND), wherein
one is allowed to query the given distribution conditioned on an arbitrary
subset $S$. The primary contribution of this work is to introduce a new
approach to lower bounds for the COND model that relies on using powerful tools
from information theory and communication complexity.
</p>
<p>Our approach allows us to obtain surprisingly strong lower bounds for the
COND model and its extensions.
</p>
<p>1) We bridge the longstanding gap between the upper ($O(\log \log n +
\frac{1}{\epsilon^2})$) and the lower bound $\Omega(\sqrt{\log \log n})$ for
COND model by providing a nearly matching lower bound. Surprisingly, we show
that even if we get to know the actual probabilities along with COND samples,
still $\Omega(\log \log n + \frac{1}{\epsilon^2 \log (1/\epsilon)})$ queries
are necessary.
</p>
<p>2) We obtain the first non-trivial lower bound for COND equipped with an
additional oracle that reveals the conditional probabilities of the samples (to
the best of our knowledge, this subsumes all of the models previously studied):
in particular, we demonstrate that $\Omega(\log \log \log n +
\frac{1}{\epsilon^2 \log (1/\epsilon)})$ queries are necessary.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_D/0/1/0/all/0/1">Diptarka Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1">Gunjan Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Meel_K/0/1/0/all/0/1">Kuldeep S. Meel</a></p><p>We consider the problem of estimating the support size of a distribution $D$.
Our investigations are pursued through the lens of distribution testing and
seek to understand the power of conditional sampling (denoted as COND), wherein
one is allowed to query the given distribution conditioned on an arbitrary
subset $S$. The primary contribution of this work is to introduce a new
approach to lower bounds for the COND model that relies on using powerful tools
from information theory and communication complexity.
</p>
<p>Our approach allows us to obtain surprisingly strong lower bounds for the
COND model and its extensions.
</p>
<p>1) We bridge the longstanding gap between the upper ($O(\log \log n +
\frac{1}{\epsilon^2})$) and the lower bound $\Omega(\sqrt{\log \log n})$ for
COND model by providing a nearly matching lower bound. Surprisingly, we show
that even if we get to know the actual probabilities along with COND samples,
still $\Omega(\log \log n + \frac{1}{\epsilon^2 \log (1/\epsilon)})$ queries
are necessary.
</p>
<p>2) We obtain the first non-trivial lower bound for COND equipped with an
additional oracle that reveals the conditional probabilities of the samples (to
the best of our knowledge, this subsumes all of the models previously studied):
in particular, we demonstrate that $\Omega(\log \log \log n +
\frac{1}{\epsilon^2 \log (1/\epsilon)})$ queries are necessary.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.12063'>Generalized Private Selection and Testing with High Confidence</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Edith Cohen, Xin Lyu, Jelani Nelson, Tam&#xe1;s Sarl&#xf3;s, Uri Stemmer</p><p>Composition theorems are general and powerful tools that facilitate privacy
accounting across multiple data accesses from per-access privacy bounds.
However they often result in weaker bounds compared with end-to-end analysis.
Two popular tools that mitigate that are the exponential mechanism (or report
noisy max) and the sparse vector technique, generalized in a recent private
selection framework by Liu and Talwar (STOC 2019). In this work, we propose a
flexible framework of private selection and testing that generalizes the one
proposed by Liu and Talwar, supporting a wide range of applications. We apply
our framework to solve several fundamental tasks, including query releasing,
top-$k$ selection, and stable selection, with improved confidence-accuracy
tradeoffs. Additionally, for online settings, we apply our private testing to
design a mechanism for adaptive query releasing, which improves the sample
complexity dependence on the confidence parameter for the celebrated private
multiplicative weights algorithm of Hardt and Rothblum (FOCS 2010).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cohen_E/0/1/0/all/0/1">Edith Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1">Xin Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nelson_J/0/1/0/all/0/1">Jelani Nelson</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarlos_T/0/1/0/all/0/1">Tam&#xe1;s Sarl&#xf3;s</a>, <a href="http://arxiv.org/find/cs/1/au:+Stemmer_U/0/1/0/all/0/1">Uri Stemmer</a></p><p>Composition theorems are general and powerful tools that facilitate privacy
accounting across multiple data accesses from per-access privacy bounds.
However they often result in weaker bounds compared with end-to-end analysis.
Two popular tools that mitigate that are the exponential mechanism (or report
noisy max) and the sparse vector technique, generalized in a recent private
selection framework by Liu and Talwar (STOC 2019). In this work, we propose a
flexible framework of private selection and testing that generalizes the one
proposed by Liu and Talwar, supporting a wide range of applications. We apply
our framework to solve several fundamental tasks, including query releasing,
top-$k$ selection, and stable selection, with improved confidence-accuracy
tradeoffs. Additionally, for online settings, we apply our private testing to
design a mechanism for adaptive query releasing, which improves the sample
complexity dependence on the confidence parameter for the celebrated private
multiplicative weights algorithm of Hardt and Rothblum (FOCS 2010).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.12101'>Efficient Sampling Algorithms for Approximate Motif Counting in Temporal Graph Streams</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jingjing Wang, Yanhao Wang, Wenjun Jiang, Yuchen Li, Kian-Lee Tan</p><p>A great variety of complex systems, from user interactions in communication
networks to transactions in financial markets, can be modeled as temporal
graphs consisting of a set of vertices and a series of timestamped and directed
edges. Temporal motifs are generalized from subgraph patterns in static graphs
which consider edge orderings and durations in addition to topologies. Counting
the number of occurrences of temporal motifs is a fundamental problem for
temporal network analysis. However, existing methods either cannot support
temporal motifs or suffer from performance issues. Moreover, they cannot work
in the streaming model where edges are observed incrementally over time. In
this paper, we focus on approximate temporal motif counting via random
sampling. We first propose two sampling algorithms for temporal motif counting
in the offline setting. The first is an edge sampling (ES) algorithm for
estimating the number of instances of any temporal motif. The second is an
improved edge-wedge sampling (EWS) algorithm that hybridizes edge sampling with
wedge sampling for counting temporal motifs with $3$ vertices and $3$ edges.
Furthermore, we propose two algorithms to count temporal motifs incrementally
in temporal graph streams by extending the ES and EWS algorithms referred to as
SES and SEWS. We provide comprehensive analyses of the theoretical bounds and
complexities of our proposed algorithms. Finally, we perform extensive
experimental evaluations of our proposed algorithms on several real-world
temporal graphs. The results show that ES and EWS have higher efficiency,
better accuracy, and greater scalability than state-of-the-art sampling methods
for temporal motif counting in the offline setting. Moreover, SES and SEWS
achieve up to three orders of magnitude speedups over ES and EWS while having
comparable estimation errors for temporal motif counting in the streaming
setting.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingjing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanhao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1">Wenjun Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuchen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1">Kian-Lee Tan</a></p><p>A great variety of complex systems, from user interactions in communication
networks to transactions in financial markets, can be modeled as temporal
graphs consisting of a set of vertices and a series of timestamped and directed
edges. Temporal motifs are generalized from subgraph patterns in static graphs
which consider edge orderings and durations in addition to topologies. Counting
the number of occurrences of temporal motifs is a fundamental problem for
temporal network analysis. However, existing methods either cannot support
temporal motifs or suffer from performance issues. Moreover, they cannot work
in the streaming model where edges are observed incrementally over time. In
this paper, we focus on approximate temporal motif counting via random
sampling. We first propose two sampling algorithms for temporal motif counting
in the offline setting. The first is an edge sampling (ES) algorithm for
estimating the number of instances of any temporal motif. The second is an
improved edge-wedge sampling (EWS) algorithm that hybridizes edge sampling with
wedge sampling for counting temporal motifs with $3$ vertices and $3$ edges.
Furthermore, we propose two algorithms to count temporal motifs incrementally
in temporal graph streams by extending the ES and EWS algorithms referred to as
SES and SEWS. We provide comprehensive analyses of the theoretical bounds and
complexities of our proposed algorithms. Finally, we perform extensive
experimental evaluations of our proposed algorithms on several real-world
temporal graphs. The results show that ES and EWS have higher efficiency,
better accuracy, and greater scalability than state-of-the-art sampling methods
for temporal motif counting in the offline setting. Moreover, SES and SEWS
achieve up to three orders of magnitude speedups over ES and EWS while having
comparable estimation errors for temporal motif counting in the streaming
setting.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.12136'>Minimum-Cost Temporal Walks under Waiting-Time Constraints in Linear Time</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Filippo Brunelli (UPCit&#xe9;, IRIF (UMR\_8243)), Laurent Viennot (UPCit&#xe9;, IRIF (UMR\_8243))</p><p>In a temporal graph, each edge is available at specific points in time. Such
an availability point is often represented by a ''temporal edge'' that can be
traversed from its tail only at a specific departure time, for arriving in its
head after a specific travel time. In such a graph, the connectivity from one
node to another is naturally captured by the existence of a temporal path where
temporal edges can be traversed one after the other. When imposing constraints
on how much time it is possible to wait at a node in-between two temporal
edges, it then becomes interesting to consider temporal walks where it is
allowed to visit several times the same node, possibly at different times. We
study the complexity of computing minimum-cost temporal walks from a single
source under waiting-time constraints in a temporal graph, and ask under which
conditions this problem can be solved in linear time. Our main result is a
linear time algorithm when temporal edges are provided in input by
non-decreasing departure time and also by non-decreasing arrival time. We use
an algebraic framework for manipulating abstract costs, enabling the
optimization of a large variety of criteria or even combinations of these. It
allows to improve previous results for several criteria such as number of edges
or overall waiting time. This result is somehow optimal: a logarithmic factor
in the time complexity appears to be necessary if the input contains only one
ordering of the temporal edges (either by arrival times or departure times).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Brunelli_F/0/1/0/all/0/1">Filippo Brunelli</a> (UPCit&#xe9;, IRIF (UMR\_8243)), <a href="http://arxiv.org/find/cs/1/au:+Viennot_L/0/1/0/all/0/1">Laurent Viennot</a> (UPCit&#xe9;, IRIF (UMR\_8243))</p><p>In a temporal graph, each edge is available at specific points in time. Such
an availability point is often represented by a ''temporal edge'' that can be
traversed from its tail only at a specific departure time, for arriving in its
head after a specific travel time. In such a graph, the connectivity from one
node to another is naturally captured by the existence of a temporal path where
temporal edges can be traversed one after the other. When imposing constraints
on how much time it is possible to wait at a node in-between two temporal
edges, it then becomes interesting to consider temporal walks where it is
allowed to visit several times the same node, possibly at different times. We
study the complexity of computing minimum-cost temporal walks from a single
source under waiting-time constraints in a temporal graph, and ask under which
conditions this problem can be solved in linear time. Our main result is a
linear time algorithm when temporal edges are provided in input by
non-decreasing departure time and also by non-decreasing arrival time. We use
an algebraic framework for manipulating abstract costs, enabling the
optimization of a large variety of criteria or even combinations of these. It
allows to improve previous results for several criteria such as number of edges
or overall waiting time. This result is somehow optimal: a logarithmic factor
in the time complexity appears to be necessary if the input contains only one
ordering of the temporal edges (either by arrival times or departure times).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.12225'>Reversible Programming: A Case Study of Two String-Matching Algorithms</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Robert Gl&#xfc;ck (Copenhagen University), Tetsuo Yokoyama (Nanzan University)</p><p>String matching is a fundamental problem in algorithm. This study examines
the development and construction of two reversible string-matching algorithms:
a naive string-matching algorithm and the Rabin-Karp algorithm. The algorithms
are used to introduce reversible computing concepts, beginning from basic
reversible programming techniques to more advanced considerations about the
injectivization of the polynomial hash-update function employed by the
Rabin-Karp algorithm. The results are two clean input-preserving reversible
algorithms that require no additional space and have the same asymptotic time
complexity as their classic irreversible originals. This study aims to
contribute to the body of reversible algorithms and to the discipline of
reversible programming.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gluck_R/0/1/0/all/0/1">Robert Gl&#xfc;ck</a> (Copenhagen University), <a href="http://arxiv.org/find/cs/1/au:+Yokoyama_T/0/1/0/all/0/1">Tetsuo Yokoyama</a> (Nanzan University)</p><p>String matching is a fundamental problem in algorithm. This study examines
the development and construction of two reversible string-matching algorithms:
a naive string-matching algorithm and the Rabin-Karp algorithm. The algorithms
are used to introduce reversible computing concepts, beginning from basic
reversible programming techniques to more advanced considerations about the
injectivization of the polynomial hash-update function employed by the
Rabin-Karp algorithm. The results are two clean input-preserving reversible
algorithms that require no additional space and have the same asymptotic time
complexity as their classic irreversible originals. This study aims to
contribute to the body of reversible algorithms and to the discipline of
reversible programming.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.12226'>On Structural Parameterizations of Star Coloring</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sriram Bhyravarapu, I. Vinod Reddy</p><p>A Star Coloring of a graph G is a proper vertex coloring such that every path
on four vertices uses at least three distinct colors. The minimum number of
colors required for such a star coloring of G is called star chromatic number,
denoted by \chi_s(G). Given a graph G and a positive integer k, the STAR
COLORING PROBLEM asks whether $G$ has a star coloring using at most k colors.
This problem is NP-complete even on restricted graph classes such as bipartite
graphs.
</p>
<p>In this paper, we initiate a study of STAR COLORING from the parameterized
complexity perspective. We show that STAR COLORING is fixed-parameter tractable
when parameterized by (a) neighborhood diversity, (b) twin-cover, and (c) the
combined parameters clique-width and the number of colors.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bhyravarapu_S/0/1/0/all/0/1">Sriram Bhyravarapu</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_I/0/1/0/all/0/1">I. Vinod Reddy</a></p><p>A Star Coloring of a graph G is a proper vertex coloring such that every path
on four vertices uses at least three distinct colors. The minimum number of
colors required for such a star coloring of G is called star chromatic number,
denoted by \chi_s(G). Given a graph G and a positive integer k, the STAR
COLORING PROBLEM asks whether $G$ has a star coloring using at most k colors.
This problem is NP-complete even on restricted graph classes such as bipartite
graphs.
</p>
<p>In this paper, we initiate a study of STAR COLORING from the parameterized
complexity perspective. We show that STAR COLORING is fixed-parameter tractable
when parameterized by (a) neighborhood diversity, (b) twin-cover, and (c) the
combined parameters clique-width and the number of colors.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.12389'>The Burer-Monteiro SDP method can fail even above the Barvinok-Pataki bound</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Liam O&#x27;Carroll, Vaidehi Srinivas, Aravindan Vijayaraghavan</p><p>The most widely used technique for solving large-scale semidefinite programs
(SDPs) in practice is the non-convex Burer-Monteiro method, which explicitly
maintains a low-rank SDP solution for memory efficiency. There has been much
recent interest in obtaining a better theoretical understanding of the
Burer-Monteiro method. When the maximum allowed rank $p$ of the SDP solution is
above the Barvinok-Pataki bound (where a globally optimal solution of rank at
most $p$ is guaranteed to exist), a recent line of work established convergence
to a global optimum for generic or smoothed instances of the problem. However,
it was open whether there even exists an instance in this regime where the
Burer-Monteiro method fails. We prove that the Burer-Monteiro method can fail
for the Max-Cut SDP on $n$ vertices when the rank is above the Barvinok-Pataki
bound ($p \ge \sqrt{2n}$). We provide a family of instances that have spurious
local minima even when the rank $p = n/2$. Combined with existing guarantees,
this settles the question of the existence of spurious local minima for the
Max-Cut formulation in all ranges of the rank and justifies the use of beyond
worst-case paradigms like smoothed analysis to obtain guarantees for the
Burer-Monteiro method.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+OCarroll_L/0/1/0/all/0/1">Liam O&#x27;Carroll</a>, <a href="http://arxiv.org/find/math/1/au:+Srinivas_V/0/1/0/all/0/1">Vaidehi Srinivas</a>, <a href="http://arxiv.org/find/math/1/au:+Vijayaraghavan_A/0/1/0/all/0/1">Aravindan Vijayaraghavan</a></p><p>The most widely used technique for solving large-scale semidefinite programs
(SDPs) in practice is the non-convex Burer-Monteiro method, which explicitly
maintains a low-rank SDP solution for memory efficiency. There has been much
recent interest in obtaining a better theoretical understanding of the
Burer-Monteiro method. When the maximum allowed rank $p$ of the SDP solution is
above the Barvinok-Pataki bound (where a globally optimal solution of rank at
most $p$ is guaranteed to exist), a recent line of work established convergence
to a global optimum for generic or smoothed instances of the problem. However,
it was open whether there even exists an instance in this regime where the
Burer-Monteiro method fails. We prove that the Burer-Monteiro method can fail
for the Max-Cut SDP on $n$ vertices when the rank is above the Barvinok-Pataki
bound ($p \ge \sqrt{2n}$). We provide a family of instances that have spurious
local minima even when the rank $p = n/2$. Combined with existing guarantees,
this settles the question of the existence of spurious local minima for the
Max-Cut formulation in all ranges of the rank and justifies the use of beyond
worst-case paradigms like smoothed analysis to obtain guarantees for the
Burer-Monteiro method.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.12431'>Choose your witnesses wisely</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Dylan Hyatt-Denesik, Afrouz Jabal Ameli, Laura Sanit&#xe0;</p><p>This paper addresses a graph optimization problem, called the Witness Tree
problem, which seeks a spanning tree of a graph minimizing a certain non-linear
objective function. This problem is of interest because it plays a crucial role
in the analysis of the best approximation algorithms for two fundamental
network design problems: Steiner Tree and Node-Tree Augmentation. We will show
how a wiser choice of witness trees leads to an improved approximation for
Node-Tree Augmentation, and for Steiner Tree in special classes of graphs.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Hyatt_Denesik_D/0/1/0/all/0/1">Dylan Hyatt-Denesik</a>, <a href="http://arxiv.org/find/cs/1/au:+Ameli_A/0/1/0/all/0/1">Afrouz Jabal Ameli</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanita_L/0/1/0/all/0/1">Laura Sanit&#xe0;</a></p><p>This paper addresses a graph optimization problem, called the Witness Tree
problem, which seeks a spanning tree of a graph minimizing a certain non-linear
objective function. This problem is of interest because it plays a crucial role
in the analysis of the best approximation algorithms for two fundamental
network design problems: Steiner Tree and Node-Tree Augmentation. We will show
how a wiser choice of witness trees leads to an improved approximation for
Node-Tree Augmentation, and for Steiner Tree in special classes of graphs.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.12441'>Query Complexity of Inversion Minimization on Trees</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ivan Hu, Dieter van Melkebeek, Andrew Morgan</p><p>We consider the following computational problem: Given a rooted tree and a
ranking of its leaves, what is the minimum number of inversions of the leaves
that can be attained by ordering the tree? This variation of the problem of
counting inversions in arrays originated in mathematical psychology, with the
evaluation of the Mann--Whitney statistic for detecting differences between
distributions as a special case.
</p>
<p>We study the complexity of the problem in the comparison-query model, used
for problems like sorting and selection. For many types of trees with $n$
leaves, we establish lower bounds close to the strongest known in the model,
namely the lower bound of $\log_2(n!)$ for sorting $n$ items. We show:
</p>
<p>(a) $\log_2((\alpha(1-\alpha)n)!) - O(\log n)$ queries are needed whenever
the tree has a subtree that contains a fraction $\alpha$ of the leaves. This
implies a lower bound of $\log_2((\frac{k}{(k+1)^2}n)!) - O(\log n)$ for trees
of degree $k$.
</p>
<p>(b) $\log_2(n!) - O(\log n)$ queries are needed in case the tree is binary.
</p>
<p>(c) $\log_2(n!) - O(k \log k)$ queries are needed for certain classes of
trees of degree $k$, including perfect trees with even $k$.
</p>
<p>The lower bounds are obtained by developing two novel techniques for a
generic problem $\Pi$ in the comparison-query model and applying them to
inversion minimization on trees. Both techniques can be described in terms of
the Cayley graph of the symmetric group with adjacent-rank transpositions as
the generating set. Consider the subgraph consisting of the edges between
vertices with the same value under $\Pi$. We show that the size of any decision
tree for $\Pi$ must be at least:
</p>
<p>(i) the number of connected components of this subgraph, and
</p>
<p>(ii) the factorial of the average degree of the complementary subgraph,
divided by $n$.
</p>
<p>Lower bounds on query complexity then follow by taking the base-2 logarithm.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Hu_I/0/1/0/all/0/1">Ivan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Melkebeek_D/0/1/0/all/0/1">Dieter van Melkebeek</a>, <a href="http://arxiv.org/find/cs/1/au:+Morgan_A/0/1/0/all/0/1">Andrew Morgan</a></p><p>We consider the following computational problem: Given a rooted tree and a
ranking of its leaves, what is the minimum number of inversions of the leaves
that can be attained by ordering the tree? This variation of the problem of
counting inversions in arrays originated in mathematical psychology, with the
evaluation of the Mann--Whitney statistic for detecting differences between
distributions as a special case.
</p>
<p>We study the complexity of the problem in the comparison-query model, used
for problems like sorting and selection. For many types of trees with $n$
leaves, we establish lower bounds close to the strongest known in the model,
namely the lower bound of $\log_2(n!)$ for sorting $n$ items. We show:
</p>
<p>(a) $\log_2((\alpha(1-\alpha)n)!) - O(\log n)$ queries are needed whenever
the tree has a subtree that contains a fraction $\alpha$ of the leaves. This
implies a lower bound of $\log_2((\frac{k}{(k+1)^2}n)!) - O(\log n)$ for trees
of degree $k$.
</p>
<p>(b) $\log_2(n!) - O(\log n)$ queries are needed in case the tree is binary.
</p>
<p>(c) $\log_2(n!) - O(k \log k)$ queries are needed for certain classes of
trees of degree $k$, including perfect trees with even $k$.
</p>
<p>The lower bounds are obtained by developing two novel techniques for a
generic problem $\Pi$ in the comparison-query model and applying them to
inversion minimization on trees. Both techniques can be described in terms of
the Cayley graph of the symmetric group with adjacent-rank transpositions as
the generating set. Consider the subgraph consisting of the edges between
vertices with the same value under $\Pi$. We show that the size of any decision
tree for $\Pi$ must be at least:
</p>
<p>(i) the number of connected components of this subgraph, and
</p>
<p>(ii) the factorial of the average degree of the complementary subgraph,
divided by $n$.
</p>
<p>Lower bounds on query complexity then follow by taking the base-2 logarithm.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.12496'>An Algorithmic Bridge Between Hamming and Levenshtein Distances</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Elazar Goldenberg, Tomasz Kociumaka, Robert Krauthgamer, Barna Saha</p><p>The edit distance between strings classically assigns unit cost to every
character insertion, deletion, and substitution, whereas the Hamming distance
only allows substitutions. In many real-life scenarios, insertions and
deletions (abbreviated indels) appear frequently but significantly less so than
substitutions. To model this, we consider substitutions being cheaper than
indels, with cost $1/a$ for a parameter $a\ge 1$. This basic variant, denoted
$ED_a$, bridges classical edit distance ($a=1$) with Hamming distance
($a\to\infty$), leading to interesting algorithmic challenges: Does the time
complexity of computing $ED_a$ interpolate between that of Hamming distance
(linear time) and edit distance (quadratic time)? What about approximating
$ED_a$?
</p>
<p>We first present a simple deterministic exact algorithm for $ED_a$ and
further prove that it is near-optimal assuming the Orthogonal Vectors
Conjecture. Our main result is a randomized algorithm computing a
$(1+\epsilon)$-approximation of $ED_a(X,Y)$, given strings $X,Y$ of total
length $n$ and a bound $k\ge ED_a(X,Y)$. For simplicity, let us focus on $k\ge
1$ and a constant $\epsilon &gt; 0$; then, our algorithm takes $\tilde{O}(n/a +
ak^3)$ time. Unless $a=\tilde{O}(1)$ and for small enough $k$, this running
time is sublinear in $n$. We also consider a very natural version that asks to
find a $(k_I, k_S)$-alignment -- an alignment with at most $k_I$ indels and
$k_S$ substitutions. In this setting, we give an exact algorithm and, more
importantly, an $\tilde{O}(nk_I/k_S + k_S\cdot k_I^3)$-time
$(1,1+\epsilon)$-bicriteria approximation algorithm. The latter solution is
based on the techniques we develop for $ED_a$ for $a=\Theta(k_S / k_I)$. These
bounds are in stark contrast to unit-cost edit distance, where state-of-the-art
algorithms are far from achieving $(1+\epsilon)$-approximation in sublinear
time, even for a favorable choice of $k$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Goldenberg_E/0/1/0/all/0/1">Elazar Goldenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Kociumaka_T/0/1/0/all/0/1">Tomasz Kociumaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Krauthgamer_R/0/1/0/all/0/1">Robert Krauthgamer</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_B/0/1/0/all/0/1">Barna Saha</a></p><p>The edit distance between strings classically assigns unit cost to every
character insertion, deletion, and substitution, whereas the Hamming distance
only allows substitutions. In many real-life scenarios, insertions and
deletions (abbreviated indels) appear frequently but significantly less so than
substitutions. To model this, we consider substitutions being cheaper than
indels, with cost $1/a$ for a parameter $a\ge 1$. This basic variant, denoted
$ED_a$, bridges classical edit distance ($a=1$) with Hamming distance
($a\to\infty$), leading to interesting algorithmic challenges: Does the time
complexity of computing $ED_a$ interpolate between that of Hamming distance
(linear time) and edit distance (quadratic time)? What about approximating
$ED_a$?
</p>
<p>We first present a simple deterministic exact algorithm for $ED_a$ and
further prove that it is near-optimal assuming the Orthogonal Vectors
Conjecture. Our main result is a randomized algorithm computing a
$(1+\epsilon)$-approximation of $ED_a(X,Y)$, given strings $X,Y$ of total
length $n$ and a bound $k\ge ED_a(X,Y)$. For simplicity, let us focus on $k\ge
1$ and a constant $\epsilon &gt; 0$; then, our algorithm takes $\tilde{O}(n/a +
ak^3)$ time. Unless $a=\tilde{O}(1)$ and for small enough $k$, this running
time is sublinear in $n$. We also consider a very natural version that asks to
find a $(k_I, k_S)$-alignment -- an alignment with at most $k_I$ indels and
$k_S$ substitutions. In this setting, we give an exact algorithm and, more
importantly, an $\tilde{O}(nk_I/k_S + k_S\cdot k_I^3)$-time
$(1,1+\epsilon)$-bicriteria approximation algorithm. The latter solution is
based on the techniques we develop for $ED_a$ for $a=\Theta(k_S / k_I)$. These
bounds are in stark contrast to unit-cost edit distance, where state-of-the-art
algorithms are far from achieving $(1+\epsilon)$-approximation in sublinear
time, even for a favorable choice of $k$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-23T01:30:00Z">Wednesday, November 23 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Tuesday, November 22
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/165'>TR22-165 |  Separation of the factorization norm and randomized communication complexity | 

	TsunMing Cheung, 

	Hamed Hatami, 

	Kaave Hosseini, 

	Morgan Shirley</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          In an influential paper, Linial and Shraibman (STOC &#39;07) introduced  the factorization norm as a powerful tool for proving lower bounds against randomized and quantum communication complexities. They showed that the logarithm of the approximate $\gamma_2$-factorization norm is a lower bound for these parameters and asked whether a stronger lower bound that replaces approximate $\gamma_2$ norm with the $\gamma_2$ norm holds.
    
    We answer the question of Linial and Shraibman in the negative by exhibiting a $2^n\times2^n $ Boolean matrix with $\gamma_2$ norm $2^{\Omega(n)}$ and randomized communication complexity $O(\log n)$. 

 As a corollary, we recover the recent result of Chattopadhyay, Lovett, and Vinyals (CCC &#39;19) that deterministic protocols with access to an Equality oracle are exponentially weaker than (one-sided error) randomized protocols.  In fact,  as a stronger consequence, our result implies an exponential separation between the power of  unambiguous nondeterministic protocols with access to Equality oracle and (one-sided error) randomized protocols, which answers a question of  Pitassi, Shirley, and Shraibman (ITSC &#39;23). 

Our result also  implies a conjecture of Sherif (Ph.D. thesis) that the $\gamma_2$ norm of the Integer Inner Product function (IIP) in dimension 3 or higher is exponential in its input size.
        
        </div>

        <div class='tr-article-summary'>
        
          
          In an influential paper, Linial and Shraibman (STOC &#39;07) introduced  the factorization norm as a powerful tool for proving lower bounds against randomized and quantum communication complexities. They showed that the logarithm of the approximate $\gamma_2$-factorization norm is a lower bound for these parameters and asked whether a stronger lower bound that replaces approximate $\gamma_2$ norm with the $\gamma_2$ norm holds.
    
    We answer the question of Linial and Shraibman in the negative by exhibiting a $2^n\times2^n $ Boolean matrix with $\gamma_2$ norm $2^{\Omega(n)}$ and randomized communication complexity $O(\log n)$. 

 As a corollary, we recover the recent result of Chattopadhyay, Lovett, and Vinyals (CCC &#39;19) that deterministic protocols with access to an Equality oracle are exponentially weaker than (one-sided error) randomized protocols.  In fact,  as a stronger consequence, our result implies an exponential separation between the power of  unambiguous nondeterministic protocols with access to Equality oracle and (one-sided error) randomized protocols, which answers a question of  Pitassi, Shirley, and Shraibman (ITSC &#39;23). 

Our result also  implies a conjecture of Sherif (Ph.D. thesis) that the $\gamma_2$ norm of the Integer Inner Product function (IIP) in dimension 3 or higher is exponential in its input size.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-22T23:24:49Z">Tuesday, November 22 2022, 23:24</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://windowsontheory.org/2022/11/22/ai-will-change-the-world-but-wont-take-it-over-by-playing-3-dimensional-chess/'>AI will change the world, but wonât take it over by playing â3-dimensional chessâ.</a></h3>
        <p class='tr-article-feed'>from <a href='https://windowsontheory.org'>Windows on Theory</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          By Boaz Barak and&#160;Ben Edelman [Cross-posted on Lesswrong ; See also Boazâs posts onÂ longtermism andÂ AGI via scaling , as well as other &#8220;philosophizing&#8221; posts. This post also puts us in Aaronson&#8217;s &#8220;Reform AI Alignment&#8221; religion] [Disclaimer:&#160;Predictions are very hard, especially about the future. In fact, this is one of the points of this essay. Hence, &#8230; Continue reading AI will change the world, but wonât take it over by playing â3-dimensional chessâ.
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>By <a href="https://www.boazbarak.org/">Boaz Barak</a> and&nbsp;<a href="https://www.benjaminedelman.com/">Ben Edelman</a></p>



<p><em>[Cross-posted on <a href="https://www.lesswrong.com/posts/zB3ukZJqt3pQDw9jz/ai-will-change-the-world-but-won-t-take-it-over-by-playing-3">Lesswrong</a> ;  See also Boazâs posts onÂ </em><a href="https://windowsontheory.org/2022/05/23/why-i-am-not-a-longtermist/"><em><u>longtermism</u></em></a><em> andÂ </em><a href="https://windowsontheory.org/2022/06/27/injecting-some-numbers-into-the-agi-debate/"><em><u>AGI via scaling</u></em></a> , as well as other &#8220;<a href="https://windowsontheory.org/category/philosophizing/">philosophizing</a>&#8221; posts. This post also puts us in Aaronson&#8217;s <a href="https://scottaaronson.blog/?p=6821">&#8220;Reform AI Alignment&#8221; </a>religion<em>] </em></p>



<p><em>[Disclaimer:&nbsp;Predictions are very hard, especially about the future. In fact, this is one of the points of this essay. Hence, while for concreteness, we phrase our claims as if we are confident about them, these are not mathematically proven facts. However we do believe that the claims below are more likely to be true than false, and, even more confidently, believe some of the ideas herein are underrated in current discussions around risks from future AI systems.]</em></p>



<p>In the past, the word âcomputerâ was used to denote a person that performs calculations. Such people were highly skilled and were crucial to scientific enterprises. As described in the book â<a href="https://en.wikipedia.org/wiki/Hidden_Figures_(book)"><u>Hidden Figures</u></a>â, until the 1960s, NASA still used human computers for the space mission. However, these days a $10 calculator can instantly perform calculations beyond the capabilities of every human on earth.</p>



<p>On a high level, the situation in Chess and other games is similar. Humans used to be the reigning champions in Chess and Go, but have now been surpassed by computers. Yet, while the success of computers in performing calculations has not engendered fears of them âtaking over the world,â the growing powers of AI systems have more people increasingly worried about their long-term implications. Some reasons why the success of AI systems such as&nbsp;<a href="https://www.deepmind.com/blog/alphazero-shedding-new-light-on-chess-shogi-and-go"><u>AlphaZero</u></a> in Go and Chess is more concerning than the success of calculation programs include</p>



<ol>
<li>Unlike when working with numerical computation programs, it seems that in Chess and Go humans are entirely âunnecessary.â There is no need to have a âhuman in the loopâ. Computer systems are so powerful that no meaningful competition is possible between even the best human players and software running on commodity laptops.<sup><a href="#footnotes">[1]</a></sup><br>&nbsp;</li>



<li>Unlike the numerical algorithms used for calculations, we do not understand the inner workings of AI chess systems, especially ones trained without any hand-designed knowledge. These systems are to a large extent âblack boxes,â which even their creators do not fully understand and hence cannot fully predict or control.<br>&nbsp;</li>



<li>Moreover, AlphaZero was trained using a paradigm known as&nbsp;<a href="https://en.wikipedia.org/wiki/Reinforcement_learning"><u>reinforcement learning</u></a> or RL (see also this&nbsp;<a href="https://rltheorybook.github.io/"><u>book</u></a>). At a high level, RL can be described as training an agent to learn a&nbsp;<em>strategy</em> (i.e., a rule to decide on a move or action based on the history of all prior ones) in order to maximize a long-term reward (e.g., âwin the gameâ). The result is a system that is capable of executing actions that may seem wrong in the short term (e.g., sacrificing a queen) but will help achieve the long-term goal.&nbsp;</li>
</ol>



<p>While RL so far has had very limited success outside specific realms such as games or low-complexity settings, the success of (non-RL) deep learning systems such as&nbsp;<a href="https://en.wikipedia.org/wiki/GPT-3"><u>GPT-3</u></a> or<a href="https://openai.com/dall-e-2/"><u> Dall-E</u></a> in open-ended text or image generation has raised fears of future AI systems that could both act in the real world, interacting with humans, physical, and digital systems, and do so in the pursuit of long term goals that may not be âalignedâ with the interests of humanity. The fear is that such systems could become so powerful that they could end up destroying much or all of humanity. We refer to the above scenario as the&nbsp;<strong>loss of control</strong> scenario. It is distinct from other potential risks of Artificial Intelligence, including the risks of AI being used by humans to develop more lethal weapons, better ways for repressive regimes to surveil their population or more effective ways of spreading misinformation.</p>



<p>In this essay,&nbsp;<strong>we claim that the âloss of controlâ scenario rests on a few key assumptions that are not justified by our current understanding of artificial intelligence research</strong>. (This doesnât mean the assumptions are necessarily wrongâjust that we donât believe the preponderance of the evidence supports them.)&nbsp; To be clear, we are not âAI skepticsâ by any means. We fully believe that over the next few decades, AI will continue to make breakthrough advances, and AI systems will surpass current human performance in many creative and technical fields, including, but not limited to, software engineering, hacking, marketing, visual design, (at least some components of) scientific discovery, and more. We are also not âtechno-optimists.â The world already faces risks, and even existential ones, from the actions of humans. People who have had control over nuclear weapons over the course of history include Joseph Stalin, Kim Jong-un, Vladimir Putin, and many others whose moral judgment is suspect, to say the least. Nuclear weapons are not the only way humans can and have caused suffering on a mass scale; whether it is biological, chemical, or even so-called âconventionalâ weapons, climate change, exploitation of resources and people, or others, humans have a long history of pain and destruction. Like any new technology, AI will be (and in fact already has been) used by humans for warfare, manipulations, and other illicit goals. These risks are real and should be studied, but are not the focus of this essay.</p>



<h2><strong>Our argument: an executive summary.</strong></h2>



<p>The loss of control scenario is typically described as a âbattleâ between AIs and humans, in which AIs would eventually win due to their superior abilities. However, unlike in Chess games, humans can and will use all the tools at their disposal, including many tools (e.g., code-completion engines, optimizers for protein folding, etc..) that are currently classified as âArtificial Intelligenceâ. So to understand the balance of power, we need to distinguish between systems or agents that have only&nbsp;<strong>short-term goals</strong>, versus systems that&nbsp;<strong>plan their own long-term strategies</strong>.&nbsp;</p>



<p>The distinction above applies not just to artificial systems but also to human occupations as well. As an example, software developers, architects, engineers, or artists have&nbsp;<em>short-term</em> goals, in the sense that they provide some particular&nbsp;<em>product</em> (piece of software, design for a bridge, artwork, scientific paper) that can stand and be evaluated on its own merits. In contrast, leaders of companies and countries set&nbsp;<em>long-term goals</em> in the sense that they need to come up with a strategy that will yield benefits in the long run and cannot be assessed with confidence until it is implemented.<sup><a href="#footnotes">[2</a><a href="#fnh8xhqamyb87">]</a></sup>&nbsp;</p>



<p>We already have at least partial âshort-term AIâ, even if not at the level of replacing e.g., human software engineers. The existence of successful âlong-term AIâ that can come up with strategies which are enacted over a scale of, say, years is still an open question, but for the sake of this essay we accept that assumption.</p>



<p>We believe that when evaluating the loss-of-control scenario, the relevant competition is not between humans and AI systems, but rather between humans aided with short-term AI systems and long-term AI systems (themselves possibly aided with short-term components). One thought experiment we have in mind is a competition between two firms: one with a human CEO, but with AI engineers and advisors, and the other a fully AI firm.</p>



<p>While it might seem âobviousâ that eventually AI would be far superior to humans in all endeavors, including being a CEO, we argue that this is not so obviously the case. We agree that future AIs could possess superior information processing and cognitive skills &#8211; a.k.a. âintelligenceâ &#8211; compared to humans. But the evidence so far suggests the&nbsp;<strong>advantages of these skills would be much more significant in some fields than in others</strong>. We believe that this is uncontroversial &#8211; for example, itâs not far-fetched to claim that AI would make much better chess players than kindergarten teachers. Specifically, there are&nbsp;<strong>âdiminishing returnsâ</strong> for superior information-processing capabilities in the context of setting<strong> longer-term goals or strategies</strong>. The long time horizon and the relevance of interactions among high numbers of agents (who are themselves often difficult to predict) make real-life large-scale systems&nbsp;<strong>âchaoticâ</strong> in the sense that even with superior analytic abilities, they are still unpredictable (see Figure 1).</p>



<p>As a consequence, we believe the<strong> main fields where AI systems will yield advantages will be in short-term domains</strong>. An AI engineer will be much more useful than an AI CEO (see also Table 2). We do not claim that it would be impossible to build an AI system that can conceive and execute long-term plans; only that this would not be where AI would have a âcompetitive advantageâ. Short-term goals that can be evaluated and graded also mesh much better with the current paradigm of training AI systems on vast amounts of data.</p>



<p>We believe it&nbsp;<strong>will be possible to construct very useful AIs with only short-term goals,</strong> and in fact that the vast majority of AIâs power will come from such short-term systems. Even if a long-term AI system is built, it will likely&nbsp;<strong>not have a significant advantage over humans assisted with short-term AIs</strong>. There can be many risks even from short-term AI systems, but such machines cannot by design have any long-term goals, including the goal of taking over the world and killing all humans.<sup><a href="#footnotes">[3</a><a href="#fnk2m16hjnqf">]</a></sup></p>



<p><strong>Perspective.&nbsp;</strong>Our analysis also has a lesson for AI safety research. Traditionally, approaches to mitigate the behavior of bad actors include</p>



<ul>
<li><strong>Prevention:&nbsp;</strong>We prevent break-ins by putting locks on our doors, we prevent hacks by securing our systems, etcâ¦&nbsp;</li>



<li><strong>Deterrence:&nbsp;</strong>Another way we prevent bad actions is by ensuring that the negative consequences for these actions will outweigh benefits. This is one basis for the penal system, as well as the âmutually assured destructionâ paradigm that has kept Russia and US from a nuclear war.</li>



<li><strong>Alignment:</strong> We try to educate children and adults and socialize them to our values, so they are not motivated to pursue the actions we consider as bad.</li>
</ul>



<p>Much of AI safety research (wrt to the âloss of controlâ scenario) has been focused on the third approach, with the expectation that these systems may be so powerful that prevention and deterrence will be impossible. However, it is unclear to us that this will be the case. For example, it may well be that humans, aided by short-term AI systems, could vastly expand the scope of formally verified secure systems, and so prevent hacking attacks against sensitive resources. A huge advantage of research on prevention is that it is highly relevant not just to protect against hypothetical future bad AI actors, but also against current malicious humans. Such research might greatly&nbsp;<em>benefit&nbsp;</em>from advances in AI code-completion engines and other tools, hence belying the notion that there is a âzero-sum gameâ between âAI safetyâ and âAI capabilitiesâ research.&nbsp;</p>



<p>Furthermore, one advantage of studying AI systems, as opposed to other organisms, is that we can try to extract useful modules and representations for them. (Indeed, this is already done in âtransfer learning.â) Hence, it may be possible to extract useful and beneficial âshort-term AIâ even from long-term systems. Such restricted systems would still give most of the utility, but with less risk. Once again, increasing the capabilities of short-term AI systems will empower humans that are assisted by such systems.<br>&nbsp;</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img src="https://lh5.googleusercontent.com/CrDF9kngAXpxxa1YLkXY1hdD7K2YMgMgvc8U8joLPJDX-PZyXIGMqR21cd7ebrszWUCK_VJEN2DcpHcb9NLbi6Y3Fi_HKlRzvirnv5wv_K2DW9md2s2WVpghmkoppI2Qros6HDeGRcwh7ZZHO9dWCAult2H-m1xjeiTgwy0O1GXtFShyNuk4EsopMR0rFQ" alt="" width="566" height="383" /></figure></div>


<p><strong>Figure 1:</strong> Cartoon of the feasibility of predicting future events and the level of ability (i.e., cognitive skill / compute / data) required to do so (approximately) optimally. As the horizon grows, events have more inherent uncertainty and also require more skills/data to predict. However, many realistic systems are&nbsp;<em>chaotic</em> and become unpredictable at some finite horizon.<sup><a href="#footnotes">[4</a><a href="#fnmrfsb0zx5hf">]</a></sup>&nbsp; At that point, even sophisticated agents cannot predict better than baseline heuristics, which require only a bounded level of skill.<br>&nbsp;</p>



<figure class="wp-block-table"><table><tbody><tr><td><em>Profession</em></td><td><em>Cognitive Score (standard deviations)</em></td><td><em>Annual Earnings&nbsp;</em></td></tr><tr><td><strong>Mayors</strong></td><td><strong>6.2 ( â +0.6Ï )</strong></td><td><strong>679K SEK</strong></td></tr><tr><td><strong>Parliamentarians</strong></td><td><strong>6.4 ( â +0.7Ï )</strong></td><td><strong>802K SEK</strong></td></tr><tr><td><strong>CEOs (10-24 employees)</strong></td><td><strong>5.8 ( â +0.4Ï )</strong></td><td><strong>675K SEK</strong></td></tr><tr><td><strong>CEOs (25-249 employees)</strong></td><td><strong>6.2 ( â +0.6Ï )</strong></td><td><strong>1,046K SEK</strong></td></tr><tr><td><strong>CEOs (â¥ 250 employees)</strong></td><td><strong>6.7 ( â +0.85Ï )</strong></td><td><strong>1,926K SEK</strong></td></tr><tr><td>Medical Doctors</td><td>7.4 ( â +1.2Ï )</td><td>640K SEK</td></tr><tr><td>Lawyers and Judges</td><td>6.8 ( â +0.9Ï )</td><td>568K SEK</td></tr><tr><td>Economists</td><td>7 ( â +1Ï )</td><td>530K SEK</td></tr><tr><td>Political Scientists</td><td>6.8 ( â +0.9Ï )</td><td>513 SEK</td></tr></tbody></table></figure>



<p><strong>Table 2:</strong> Cognitive scores for Swedish men in various âeliteâ occupations, based on Swedish army entrance examinations, taken from&nbsp;<a href="https://academic.oup.com/qje/article-abstract/132/4/1877/3859758?redirectedFrom=fulltext"><u>Dal BÃ³ et al</u></a> (Table II). Emphases ours: bold text corresponds to jobs that (in our view) require longer horizon decision-making across time or number of people. Note that despite being apparently less cognitively demanding, the âboldâ professions are higher paying.</p>



<h2><strong>A digression: what is intelligence</strong></h2>



<p>Merriam-Webster&nbsp;<a href="https://www.merriam-webster.com/dictionary/intelligence"><u>defines</u></a> intelligence as âthe skilled use of reasonâ, âthe ability to learn or understand or to deal with new or trying situationsâ, or âto apply knowledge to manipulate one&#8217;s environment or to think abstractly.â Intelligence is similar to&nbsp;<em>computation</em>, in the sense that its main components are the ability to take in observations (aka âinputsâ) and use reasoning (aka âalgorithmsâ) to decide on actions (aka âoutputsâ). In fact, in the currently dominant paradigm of AI, performance is primarily determined by the amount of computation performed during learning, and AI systems consist of enormous homogeneous circuits executing a series of simple operations on (a large quantity of) inputs and learned knowledge.&nbsp;<a href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies"><u>Bostrom</u></a> (Chapter 3) defines three forms of âsuperintelligenceâ: âspeed superintelligenceâ, âcollective superintelligenceâ and âquality superintelligenceâ. In the language of computing, speed super-intelligence corresponds to clock speed of processors, while collective super-intelligence corresponds to massive parallelism. âQuality superintelligenceâ is not well defined, but is presumably some type of emergent phenomenon from passing some thresholds of speed and parallelism.</p>



<p><br>A fundamental phenomenon in computing is&nbsp;<em>universality:</em> there are many&nbsp;<em>restricted</em> computational models (finite state automata, context-free grammars, simply-typed lambda calculus), but once a computational model passes a certain threshold or&nbsp;<em>phase transition</em>, it becomes universal (a.k.a. âTuring completeâ), and all universal models are equivalent to one another in computational power. For example, in a cellular automata, even though each cell is very restricted (can only store a constant amount of memory and process a finite rule based only on the state of its immediate neighbors), given enough cells we can simulate any arbitrarily complex machine.<sup><a href="#footnotes">[5]</a></sup>&nbsp; Once a system passes the universality transition, it is not bottlenecked any more by the complexity of an individual unit, but rather by the resources in the system as a whole.</p>



<p>In the animal kingdom, we seem to have undergone a similar phase transition, whereby humans are qualitatively more intelligent than any other animal or creature. It also seems to be the case that with the invention of language, the printing press, and the Internet, we (like cellular automata) are able to combine large numbers of humans to achieve feats of collective intelligence that are beyond any one individual. In particular, the fruits of the scientific revolution of the 1500-1600s increased the scale of GDP by 10,000-fold (to the extent such comparisons are meaningful) and the distance we can measure in space a trillion-fold, all with the same brains used by our hunter-gatherer ancestors (or&nbsp;<a href="https://www.frontiersin.org/articles/10.3389/fevo.2022.963568/full"><u>maybe</u></a> somewhat&nbsp;<a href="https://www.frontiersin.org/articles/10.3389/fevo.2021.742639/full"><u>smaller</u></a> ones).&nbsp;</p>



<p><a href="https://www.pnas.org/doi/full/10.1073/pnas.1100290108"><u>Arguably</u></a>, the fact humans are far better than chimpanzees at culturally transmitting knowledge is more significant than the gap in intelligence between individuals of the two species. Ever since the development of language, the intelligence of an individual human has&nbsp;<em>not</em> been a bottleneck for the achievements of humanity. The brilliance of individuals like Newton may have been crucial for speeding up the Scientific Revolution, but there have been brilliant individuals for millennia. The crucial difference between Newton and Archimedes is not that Newton was smarter, but rather that he lived at a later time and thus was able to stand on the shoulders of more giants. As another example, a collection of humans, aided by Internet-connected computers, can do much better at pretty much any intelligence feat (including but not limited to IQ exams) than any single human.&nbsp;<br>&nbsp;</p>



<figure class="wp-block-image"><img src="https://lh3.googleusercontent.com/ENKzULhufaFT8CI2c_l-YKNl5XldYKJA8CjBtw9TrV6NFao1pEhyk9pmyDHSsA1iY0aZG-zJxv1wVb7zaF6zaZnyptxKs2CXe5zaS-40ZR_m_Sf59X2HorWz1xZdGFd-aSfhdgpjPPRfYpj5OWBbZMqkpCZEj9sdeGIcRXN1tjjxucVZPUcgbKyHiGLmkA" alt="" /></figure>



<p><strong>Figure 3:</strong> Measures of human progress both in terms of GDP and the scale of objects we can measure. Taken from&nbsp;<a href="https://windowsontheory.org/2022/05/03/philosophy-of-science-and-the-blockchain-a-book-review/"><u>this blog post</u></a>, with the first figure from&nbsp;<a href="https://ourworldindata.org/grapher/world-gdp-over-the-last-two-millennia"><u>Our World in Data</u></a>, and data for second figure from Terence Taoâs&nbsp;<a href="https://terrytao.wordpress.com/2010/10/10/the-cosmic-distance-ladder-ver-4-1/"><u>cosmic ladder presentation</u></a>.</p>



<p><br>The âloss of controlâ scenario posits a&nbsp;<em>second phase transition</em>, whereby once AI systems become more powerful, they would not merely enable humans to achieve more objectives quicker but would themselves become as qualitatively superior to humans as humans are to other animals. We are suggesting an alternative future scenario, in which while AI would provide powerful new capabilities to human society that can (and unfortunately likely will) be used for ill as well as good, the AI systems themselves would not be the inevitable leaders of this society.</p>



<p>Indeed, our societies and firms do not currently select our leaders to be the top individuals in intellectual capacity. The evidence is very limited that ânatural talent for leadershipâ (to the extent it exists) is as measurable and transferable as talent for chess, math, or athletics. There are many examples of leaders who have been extremely successful in one setting but failed in another which seems rather similar.<sup><a href="#footnotes">[6]</a></sup>&nbsp;</p>



<p>Whether or not an AI system should be considered an âindividualâ is a matter for debate, but regardless, it is not at all clear that such individuals would be the leaders of the society, rather than being employed in domains such as software development and scientific discovery, where their superior information-processing capabilities would provide the most competitive advantage. Bostrom (Table 8 in Chapter 6) lists several potential âcognitive superpowersâ that an AI system might develop. One category is&nbsp;<em>âhackingâ</em>,&nbsp;<em>âtechnology researchâ</em>, and&nbsp;<em>âeconomic productivityâ</em>. These are skills that correspond to jobs that are not in the domain of CEOs or leaders, but rather engineers, middle managers, scientists, etc. AI systems may well be able to assist or even replace such individuals, but this does not mean such systems will be the leaders of companies or countries.</p>



<p>Another task Bostrom considers is&nbsp;<em>âintelligence amplificationâ</em> which is the ability to improve AI systems. Again, it is quite possible that AI systems would help in improving other or the same AI systems, but this on its own does not imply that they would become infinitely powerful. Specifically, if indeed stronger AI would arrive through âscalingâ of massive computational resources, then there would be&nbsp;<a href="https://windowsontheory.org/2022/06/27/injecting-some-numbers-into-the-agi-debate/"><u>some hard limits</u></a> on the ability to improve AIâs power solely through software updates. It is not at all clear that in terms of energy efficiency, AI systems would be much better (if at all) than humans. If the gains from scaling are far more important than gains from improved algorithms/architectures, then intelligence amplification might be primarily a function of resource acquisition rather than algorithmic research.</p>



<p>A third task listed is&nbsp;<em>âsocial manipulation.â</em> Here we must admit we are skeptical. Anyone who has ever tried to convince a dog to part with a bone or a child with a toy could attest to the diminishing returns that an intelligence advantage has in such a situation.&nbsp;</p>



<p>Finally, Boston lists the cognitive superpower of&nbsp;<em>âstrategizingâ</em>, which is the ability to make long-term plans to achieve distant goals. This is the point we focus on in this essay. In short, our belief is that the chaotic nature of the real world implies diminishing returns to âthree-dimensional chessâ strategies that are beyond the comprehension of mere humans. Hence we do not believe that this would be a domain where AI systems have a strong competitive advantage.</p>



<h2><br><strong>A thought experiment: âThe AI CEO vs. the AI advisorâ</strong></h2>



<p>Before we delve into the technical(-ish) analysis, let us consider a thought experiment. At its heart, our argument is that the power of AI systems, present and future, will not come from the ability to make long-term strategic plans (âthree-dimensional chessâ) but rather from the ability to produce pieces of work that can be evaluated on their own terms. In short, we believe that even if a long-term malicious AI system is constructed, it will not have an insurmountable advantage over humans that are assisted with short-term AIs. To examine this, let us imagine two possible scenarios for how future AI could assist humans in making strategic decisions, such as running a company:</p>



<ul>
<li>In the&nbsp;<strong>âAI Advisorâ</strong> model, leaders could use AI to come up with simulations of the impact of decisions and possibly make some suggestions. However, humans would ultimately make the decision and evaluate their results. Key for this is that an AI would be able not just to produce a recommendation for a decision but explain how this decision would lead to improvement in some interpretable metric (e.g., revenue, market share, etc..). For example, a decision might be âletâs sell this product at a loss so we can increase our market share.â<br>&nbsp;</li>



<li>In the&nbsp;<strong>âAI CEOâ</strong> model, AIs could use their superior powers to choose an optimal long-term&nbsp;<em>strategy</em> as opposed to an individual decision. The strategy would not be âgreedyâ, in the sense of a sequence of steps each making progress on measurable goals, and it would not have any compact analysis of why it is good. Also, the only way to accrue the benefits of the strategy would be to continue pursuing it in the long term. Hence users would have to trust the AI and follow its recommendations blindly. For example, think of the case in Chess where an AI figures out that the best move is to sacrifice the queen because for any one of the possible opponentâs moves, there is a countermove, and so on and so forth. The only explanation for why this strategy is a good one may consist of an exponentially big game tree up to a certain depth.</li>
</ul>



<p>Our sense is that there is strong evidence that AI would be incredibly useful for making low-level decisions (i.e., optimizing objectives under constraints) once the high-level strategy was set. Indeed, by far the most exciting advances for deep learning have&nbsp;<em>not</em> been through reinforcement learning, but rather through techniques such as supervised and unsupervised learning. (With the major exception being games like Chess and Go, though even there, given the&nbsp;<a href="https://lichess.org/broadcast/tcec-season-21-superfinal--stockfish-vs-leela-chess-zero/rounds-1-100/Mtixisaw"><u>success</u></a> of non-RL engines such as<a href="https://towardsdatascience.com/dissecting-stockfish-part-2-in-depth-look-at-a-chess-engine-2643cdc35c9a"><u> Stockfish&nbsp;</u></a>versions 12 and later, it is not clear RL is needed.) There is less evidence that âAI advisorsâ would be useful for setting high-level strategies, but it is certainly plausible. In particular, the power of prompt-based generative models suggests that AI could be useful for generating realistic simulations that can help better convey the impact of various decisions and events. So, while âAI engineersâ might be more useful than âAI advisorsâ, the latter might well have their role as well.&nbsp;</p>



<p>In contrast, we believe that there is little to no evidence for the benefits of âthree-dimensional chessâ strategies of the type required for the âAI CEOâ scenario. The real world (unlike the game of chess or even poker), involves a significant amount of unpredictability and chaos, which makes highly elaborate strategies depending on complex branching trees of moves and counter-moves far less useful. We also find it unlikely that savvy corporate boards would place blind trust in an AI CEO given that (as mentioned above) evaluation of even human CEOs tends to be controversial.&nbsp;</p>



<p>There is an alternative viewpoint, which is that an AI CEO would basically be equivalent to a human CEO but with superhuman âintuitionâ or âgut feelingâ that they cannot explain but somehow leads to decisions that yield enormous benefits in the long term. While this viewpoint cannot be ruled out, there is no evidence in current deep learning successes to support it. Moreover, often great CEOâs âgut feelingsâ are less about particular decisions, but more about the relative importance of particular metrics (e.g., prioritizing market share or user experience over short-term profits).&nbsp;</p>



<p>In any case, even if one does not agree with our judgment of the relative likelihoods of the above scenarios, we hope that this essay will help sharpen the questions that need to be studied, as well as what lessons can we draw about them from the progress so far of AI systems.</p>



<h1><strong>Technical Analysis</strong></h1>



<h2><br><strong>1. Key hypotheses behind the âLoss of Controlâ Scenario</strong></h2>



<p>For the sake of the discussion below, letâs assume that at some future time there exists an artificial intelligence system that in a unified way achieves performance far superior to that achieved by all humans today across many fields. This is a necessary assumption for the âloss of controlâ scenario and an assumption we accept in this essay. For the sake of simplicity, below we refer to such AI systems as âpowerfulâ.</p>



<p>We will also assume that powerful AI will be constructed following the general paradigm that has been so successful in the last decade of machine learning. Specifically, the system will be obtained by going through a large amount of data and computational steps to find some instantiation (a.k.a. âparametersâ or âweightsâ) of it that optimizes some chosen objective. Depending on the choice of the objective, this paradigm includes supervised learning (âclassify this imageâ), unsupervised learning (âpredict the next tokenâ), reinforcement learning (âwin the gameâ), and more.</p>



<p><br>For the loss of control scenario to occur, the following two hypotheses must be true:<br>&nbsp;</p>



<blockquote class="wp-block-quote">
<p><strong>Loss-of-Control Hypothesis 1:</strong> There will exist a powerful AI that has long-term goals.</p>
</blockquote>



<p>For an AI to have misaligned long-term goals, it needs to have some long-term goals in the first place. There is a question of how to define the âgoalsâ of an AI system or even a human for that matter. In this essay, we say that an agent has a goal X if, looking retrospectively at the history of the agentâs actions, the most parsimonious explanation for its actions was that it was attempting to achieve X, subject to other constraints or objectives. For example, while chess experts often find it hard to understand why an engine such as AlphaZero makes a specific move, by the end of the game, they often understand the reasoning retrospectively and the sub-goals it was pursuing.</p>



<p>In our parlance, a goal is âlong-termâ if it has a similar horizon to goals such as&nbsp;<em>âtake over the world and kill all the humansâ</em> ârequiring planning over large scales of time, complexity, and number of agents involved.<sup><a href="#footnotes">[7]</a></sup>&nbsp;&nbsp;</p>



<p>In contrast, we consider goals such as âwin a chess gameâ, âcome up with a plan for a bridge that minimizes cost and can carry X trafficâ, or âwrite a piece of software that meets the requirements Yâ, as short-term goals.&nbsp; As another example, âcome up with a mix of stocks to invest today that will maximize return next weekâ is a short-term goal, while âcome up with a strategy for our company that will maximize our market cap over the next decadeâ or âcome up with a strategy for our country that will maximize our GDP for the next generationâ would be long-term goals. The distinction between âshort-term goals AIâ and âlong-term goals AIâ is somewhat similar to the distinction between âTool AIâ and âAgent AIâ (see&nbsp;<a href="https://www.gwern.net/Tool-AI"><u>here</u></a>). However, what we call âshort-term AIâ encompasses much more than âTool AIâ, and absolutely includes systems that can take actions such as driving cars, executing trading actions, and so on and so forth.</p>



<p>We claim that for the âloss of controlâ scenario to materialize, we need not only Hypothesis 1 but also the following stronger hypothesis:</p>



<blockquote class="wp-block-quote">
<p><strong>Loss-of-Control Hypothesis 2:&nbsp;</strong>In several key domains,&nbsp;<em>only</em> AIs with long-term goals will be powerful.</p>
</blockquote>



<p><br>By this, we mean that AIs with long-term goals would completely dominate other AIs, in that they would be much more useful for any user (or for furthering their own goals). In particular,&nbsp; a country, company or organization that restricts itself to only using AIs with short term goals would be at a severe competitive disadvantage compared to one that uses AIs with long-term goals.</p>



<p>Why is Hypothesis 2 necessary for the âloss of controlâ scenario? The reason is that this scenario requires the âmisaligned long-term powerful AIâ to be not merely more powerful than humanity as it exists today, but more powerful than humanity in the future. Future humans will have at their disposal the assistance of short-term AIs.</p>



<h2><strong>2. Understanding the validity of the hypotheses</strong></h2>



<p>We now make the following claims, which we believe cast significant doubt on Hypothesis 2.</p>



<p><br><strong>Claim 1: There are diminishing returns to information-processing skills with longer horizons.</strong></p>



<p>Consider the task of predicting the consequences of a particular action in the future. In any sufficiently complex real-life scenario, the further away we attempt to predict, the more there is inherent uncertainty. For example, we can use advanced methods to predict the weather over a short time frame, but the further away the prediction, the more the system âregresses to the meanâ, and&nbsp;<a href="https://www.globalagtechinitiative.com/digital-farming/data-management/weather-forecasting-how-does-it-work-and-how-reliable-is-it/"><u>the less advantage</u></a> that highly complex models have over simpler ones (see Figure 4). As in meteorology, this story seems to play out similarly in&nbsp;<a href="https://www.federalreserve.gov/econres/feds/the-accuracy-of-forecasts-prepared-for-the-federal-open-market-committee.htm"><u>macroeconomic forecasting</u></a>.&nbsp; In general, we expect prediction success to behave like Figure 1 belowâthe error increases with the horizon until it plateaus to a baseline level of some simple heuristic(s). Hence while initially highly sophisticated models can beat simpler ones by a wide margin, this advantage eventually diminishes with the time horizon.</p>



<p>Tetlockâs&nbsp;<a href="https://www.lesswrong.com/posts/dvYeSKDRd68GcrWoe/ten-commandments-for-aspiring-superforecasters"><u>first commandment</u></a> to potential superforecasters is to triage: âDonât waste time either on âclocklikeâ questions (where simple rules of thumb can get you close to the right answer) or on impenetrable âcloud-likeâ questions (where even fancy statistical models canât beat the dart-throwing chimp). Concentrate on questions in the Goldilocks zone of difficulty, where effort pays off the most.â&nbsp; Another way to say it is that outside of the Goldilocks zone, more effort or cognitive power does not give much returns.&nbsp;</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2022/11/image.png"><img data-attachment-id="8474" data-permalink="https://windowsontheory.org/2022/11/22/ai-will-change-the-world-but-wont-take-it-over-by-playing-3-dimensional-chess/image-9/" data-orig-file="https://windowsontheory.files.wordpress.com/2022/11/image.png" data-orig-size="1224,440" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://windowsontheory.files.wordpress.com/2022/11/image.png?w=300" data-large-file="https://windowsontheory.files.wordpress.com/2022/11/image.png?w=656" src="https://windowsontheory.files.wordpress.com/2022/11/image.png?w=1024" alt="" class="wp-image-8474" srcset="https://windowsontheory.files.wordpress.com/2022/11/image.png?w=1024 1024w, https://windowsontheory.files.wordpress.com/2022/11/image.png?w=150 150w, https://windowsontheory.files.wordpress.com/2022/11/image.png?w=300 300w, https://windowsontheory.files.wordpress.com/2022/11/image.png?w=768 768w, https://windowsontheory.files.wordpress.com/2022/11/image.png 1224w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></figure>



<p><br><strong>Figure 4:&nbsp;</strong>&nbsp;Left: Historical weather prediction accuracy data taken from a&nbsp;<a href="https://qr.ae/pvVKIJ"><u>Quora answer of Mikko Strahlendorff</u></a>. With technological advances, accuracy has improved significantly, but prediction accuracy sharply decays with time. Right: Figure on relative applicability of different methods from&nbsp;<a href="https://www.globalagtechinitiative.com/digital-farming/data-management/weather-forecasting-how-does-it-work-and-how-reliable-is-it/"><u>Brent Shaw</u></a>. Computationally intensive numerical prediction applies in a âgoldilocks zoneâ of days to weeks.</p>



<p>In a variety of human endeavors, it seems that the cognitive skills needed to make decisions display a similar phenomenon. Occupations involving making decisions on the mid-range horizon, such as engineering, law, and medicine, require higher cognitive skills than those requiring long-term decisions such as CEOs or Politicians (see Table 3).</p>



<p>One argument people make is that intelligence is not just about IQ or â<a href="https://www.lesswrong.com/posts/aiQabnugDhcrFtr9n/the-power-of-intelligence"><u>booksmarts</u></a>â. We do not dispute this. However, we do believe that the key potential advantage of AI systems over their human counterparts would be the ability to quickly process large amounts of information, which in humans is approximated by scores such as IQ. If that skill were key to successful leadership of companies or countries, then we would expect CEOs and leaders to come from the top 0.1% (â +3Ï)&nbsp; of the distribution of such scores. The data does not bear this out.<sup><a href="#footnotes">[8]</a></sup>&nbsp;</p>



<p><strong>Claim 2:</strong>&nbsp;<strong>It may be possible to extract powerful short-term modules from long-term systems.</strong></p>



<p>For Hypothesis 2 to be true, it should not be possible to take a powerful AI system with long-term goals, and extract from it modules that would be just as powerful in the key domains, but would have short-term goals. However, a nascent body of work identifies and extracts useful representations and sub-modules in deep neural networks. See, for example, this recent investigation of&nbsp;<a href="https://arxiv.org/abs/2111.09259"><u>AlphaZero</u></a>. We remark that some components of AlphaZero also inspired advances to the Stockfish Chess Engine (which is not trained using RL and involves a lot of hand-coded features), and whose latest version&nbsp;<a href="https://ccrl.chessdom.com/ccrl/4040/"><u>does in fact beat</u></a> RL trained methods a-la AlphaZero.</p>



<p>A related issue is that a consistent theme of theoretical computer science is that verification is easier than solving or proving. Hence even a complex system could explain its reasoning to a simple verifier, even if that reasoning required a significant effort to discover. There are similar examples in human affairs: e.g., even though the discovery of quantum mechanics took thousands of years and multiple scientific revolutions, we can still teach it to undergraduates today whose brains are no better than those of the ancient Greeks.&nbsp;</p>



<h2><br><strong>2.1 The impact of the deep learning paradigm on Hypothesis 2</strong></h2>



<p><br>The following claims have to do with the way we believe advanced AI systems will be constructed. We believe it is fair to assume that the paradigm of using massive data and computation to create such systems, by optimizing with respect to a certain objective, will continue to be used. Indeed, it is the success of this paradigm that has caused the rise in concerns about AI in the first place.&nbsp; In particular, we want to make a clear distinction between the&nbsp;<em>training objective</em>, which the system is designed to optimize, versus the goals that the system appears to follow during its&nbsp;<em>deployment</em>.</p>



<p><strong>Claim 3: There may be fundamental âscaling lawsâ governing the amount of performance AI systems can achieve as a function of the data and computational resources.</strong></p>



<p>One of the original worries in the AI risk literature is the â<a href="https://edoras.sdsu.edu/~vinge/misc/singularity.html"><u>singularity</u></a>â scenario, by which an AI system continuously improves its own performance without limit. However, this assumes that a system can improve itself by rewriting its code, without requiring additional hardware resources.&nbsp; If there are hard limits to what can be achieved with a certain level of resources, then such self-improvements will also hit diminishing returns. There has been significant evidence for&nbsp;<a href="https://arxiv.org/abs/1909.12673"><u>the</u></a> â<a href="https://arxiv.org/abs/2001.08361"><u>scaling</u></a>&nbsp;<a href="https://arxiv.org/abs/2203.15556"><u>laws</u></a>â hypothesis in recent years.</p>



<p><br><br><img src="https://lh4.googleusercontent.com/b_3ZFahE15wnxQCDcRBEnnbGytqGJayIVfztpee7Ff3X8DkyqQT6VCpLABMnBUMy0dRaNpbQdpiWrwYGz_C-NSxovBP9E48nRixKyFT0PgnqhYdaxEUAeXMVxCtsT9ib4RSvhd7fvYgS3ZUJjg5HuQVpfwke49K8Ytys3mVX7hdCUbDugfkL42FnEZYi5Q" style="width:450px;"><br><strong>Figure 5:</strong> Scaling laws as computed by&nbsp;<a href="https://arxiv.org/abs/2203.15556"><u>Hoffman et al</u></a> (âChinchillaâ), see Figure A4 there. While the scaling laws are shaped differently from those of&nbsp;<a href="https://arxiv.org/abs/2001.08361"><u>Kaplan et al</u></a>, the qualitative point we make remains the same.</p>



<p><strong>Claim 4: When training with reinforcement learning, the gradient signal may decrease exponentially with the length of the horizon.</strong></p>



<p>Consider training a system that chooses a sequence of actions, and only gets a reward after H steps (where H is known as the âhorizonâ). If at any step there is some probability of an action leading to a âdead endâ then the chances of getting a meaningful signal decrease&nbsp;<em>exponentially</em> with H. This is a fundamental obstacle to reinforcement learning and its applicability in open-ended situations with a very large space of actions, and a non-trivial cost for any interaction. In particular, one reason AlphaZero was successful was that in games such as chess, the space of legal moves is very constrained, and in the artificial context of a game it is possible to âresetâ to a particular position: that is, one can try out different actions and see what their consequences are, and then go back to the same position. This is not possible when interacting in the real world.</p>



<p><br>&nbsp;As a corollary of Claim 4, we claim the following:</p>



<p><strong>Claim 5: There will be powerful AI systems that are trained with short-term objective functions.</strong></p>



<p>By this, we mean models that are trained on a reward/loss function that only depends on a relatively short span of actions/outputs. A canonical example of this is next-token prediction. That is, even if the eventual&nbsp;<em>deployment</em> of the model will involve it making actions and decisions over a long time horizon, its&nbsp;<em>training</em> will involve optimizing short-term rewards.</p>



<p>&nbsp;One might think that the model&#8217;s training does not matter as much, since once it is deployed in the real world, much of what it will learn will be âon the jobâ. However, this is not at all clear. Suppose the average worker reads/hears about 10 pages per day, which is roughly 5K tokens, leading to roughly 2M tokens per year. In contrast, future AIs will likely be trained on a trillion tokens or so, corresponding to the amount a worker will see in 5 million years! This means that while âfine-tuningâ or âin contextâ learning can and will occur, many of the fundamental capabilities of the systems will be fixed at the time of training (as appears to be the case for pre-trained language models that are fine-tuned with human feedback).</p>



<p><strong>Claim 6: For a long-term goal to necessarily emerge from a system trained with a short-term objective, it must be correlated or causally related to that objective.</strong></p>



<p>If we assume that powerful AIs will be trained with short-term objectives, then Hypothesis 2 requires that (in several key domains)&nbsp;<em>every</em> such system will develop long-term goals. In fact, for the loss-of-control scenario to hold, every such system should develop more-or-less the same sort of goal (e.g., âtake over the worldâ).</p>



<p>While it is certainly possible for systems that evolve from simple rules to develop complex behavior (e.g.,&nbsp;<a href="https://en.wikipedia.org/wiki/Cellular_automaton"><u>cellular automata</u></a>), for a long-term goal to&nbsp;<em>consistently emerge</em> from mere short-term training, there should be some causal relation (or at least persistent correlation)&nbsp; between the long-term goal and the short-term training objective. This is because an AI system can be modeled as a maximizer of the objective on which it was trained. Thus for such a system to&nbsp;<em>always</em> pursue a particular long-term goal, that goal should be correlated with maximizing the training objective.</p>



<p>We illustrate this with an example. Consider an AI software developer which is trained to receive a specification of a software task (say, given by some unit tests) and then come up with a module implementing it, obtaining a reward if the module passes the tests. Now suppose that in actual deployment, the system is also writing the tests that would be used to check its future outputs. We might worry that the system would develop a âlong-termâ goal to maximize total reward by writing one faulty test, taking the âhitâ on it, and receiving a low reward, but then getting high rewards on future tasks. However, that worry would be unfounded, since the AI software developer system is trained to maximize the reward for each task separately, as opposed to maximizing the sum of rewards over time over adaptively chosen inputs of its own making.</p>



<p>Indeed, this situation can already happen today. Next-token prediction models such as GPT-3 are trained on the reward of the perplexity over a single token, but when they are deployed, we typically generate a long sequence of tokens. Now consider a model that simply outputs an endless repetition of the word âblahâ. The first few repetitions would get very low rewards, since they are completely unexpected, but once n is large enough (e.g. 10 or so), if youâve already seen n âblahâs then the probability that the n+1 st word is also âblahâ is very high.&nbsp; So if the model were to be maximizing total reward, it may well be worth âtaking the hitâ by outputting a few blahs. The key point is that GPT-3 does&nbsp;<em>not</em> do that. Since it is trained on predicting the next token for human-generated (as opposed to the text generated by itself), it will optimize for this short-term objective rather than the long-term one.</p>



<p>We believe the example above generalizes to many other cases. An AI system trained in the current paradigm is, by default, a maximizer of the objective it was trained on, rather than an autonomous agent that pursues goals of its own design. The shorter the horizon and more well-defined the objective is, the less likely that optimizing it will lead to systems that appear to take elaborate plans to pursue far-reaching (good or bad) long-term goals.&nbsp;</p>



<h1><br><strong>Summary</strong></h1>



<p>Given the above, we believe that while AI will continue to yield breakthroughs in many areas of human endeavor, we will not see a unitary nigh-omnipotent AI system that acts autonomously to pursue long-term goals. Concretely, even if a successful long-term AI system could be constructed, we believe that this is not a domain where AI will have a significant âcompetitive advantageâ over humans.</p>



<p>Rather, based on what we know, it is likely that AI systems will have a âsweet spotâ of a not-too-long horizon in which they can provide significant benefits. For strategic and long-term decisions that are far beyond this sweet spot, the superior information processing skills of AIs will give diminishing returns. (Although AIs will likely supply valuable input and analysis to the decision makers.).&nbsp; An AI engineer may well dominate a human engineer (or at least one that is not aided by AI tools), but an AI CEOâs advantage will be much more muted, if any, over its human counterpart. Like our world, such a world will still involve much conflict and competition, with all sides aided by advanced technology, but without one system that dominates all others.</p>



<p>If our analysis holds, then it also suggests different approaches to mitigating AI risk than have been considered in the âAI safetyâ community. Currently, the prevailing wisdom in that community is that AI systems with long-term goals are a given, and hence the approach to mitigate their risk is to âalignâ these goals with human values. However, perhaps more evidence should be placed on building just-as-powerful AI systems that are restricted to short time horizons. Such systems could also be used to monitor and control other AIs, whether autonomous or directed by humans. This includes monitoring and hardening systems against hacking, detecting misinformation, and more. Regardless, we believe that more research needs to be done on understanding the internal representations of deep learning systems, and what features and strategies emerge from the training process (so we are happy that the AI safety community is putting increasing resources into âinterpretabilityâ research). There is&nbsp;<a href="https://arxiv.org/abs/2106.07682"><u>some evidence</u></a> that the same internal representations emerge regardless of the choices made in training.</p>



<p>There are also some technical research directions that would affect whether our argument is correct. For instance, we are interested in seeing work on the impacts of noise and unpredictability on the performance of reinforcement learning algorithms; in particular, on the&nbsp;<em>relative</em> performance of models of varying complexity (i.e.&nbsp;<a href="https://arxiv.org/abs/2104.03113"><u>scaling</u></a>&nbsp;<a href="https://arxiv.org/abs/2210.00849"><u>laws</u></a> for RL).</p>



<p><strong>Acknowledgments: </strong>Thanks to Yafah Edelman for comments on an earlier version of this essay.</p>



<h2 id="footnotes"><strong>Footnotes</strong></h2>



<ol>
<li>During the 90s-2000s, human-engine teams were able to consistently beat engines in&nbsp;<a href="https://en.wikipedia.org/wiki/Advanced_chess"><u>âadvanced chessâ</u></a> tournaments, but no major advanced chess tournament seems to have taken place since the release of AlphaZero and the resulting jump in engine strength, presumably because the human half of each team would be superfluous.</li>



<li>The success of a bridge does hinge on its long-term stability, but stability can be tested before the bridge is built, and coming up with measures for load-bearing and other desiderata is standard practice in the engineering profession. An AI trained using such a short-term evaluation suite as its reward function may still â<a href="https://arxiv.org/abs/2210.10760"><u>overoptimize</u></a>â against the metric, a la Goodhartâs Law, but this can likely be addressed with regularization techniques.</li>



<li>It may be the case that, for subtle reasons, if we try to train an AI with only short-term goalsâe.g. by training in a series of short episodesâwe could accidentally end up with an AI that has long-term goals. See Claim 6 below. But avoiding this pitfall seems like an easier problem than âaligningâ the goals of an AI that is explicitly meant to care about the long-term.</li>



<li>We donât mean that they satisfy&nbsp;<a href="https://en.wikipedia.org/wiki/Chaos_theory#Chaotic_dynamics"><u>all the formal requirements</u></a> to be defined as a chaotic system; though sensitivity to initial conditions is crucial.</li>



<li>For a nice illustration, see Sam Trajtenbergâs construction of&nbsp;<a href="https://twitter.com/boazbaraktcs/status/1586798286463270914?s=20&amp;t=bEhHVwKP7N6yyp_s2K2HRw"><u>Minecraft in Minecraft</u></a>, or this construction of&nbsp;<a href="https://www.youtube.com/watch?v=xP5-iIeKXE8"><u>Life in Life</u></a>.</li>



<li>Steve Jobs at Apple vs NeXT is one such example; success and failure can themselves be difficult to distinguish even with the benefit of hindsight, as in the case of&nbsp;<a href="https://www.newyorker.com/magazine/2022/11/07/was-jack-welch-the-greatest-ceo-of-his-day-or-the-worst"><u>Jack Welch</u></a>.</li>



<li>For example, such planning might require setting up many companies to earn large amounts of funds, conducting successful political campaigns in several countries, constructing laboratories without being detected, etc. Some such âtake-over scenariosâ are listed by Bostrom, as well as&nbsp;<a href="https://forum.effectivealtruism.org/posts/zzFbZyGP6iz8jLe9n/agi-ruin-a-list-of-lethalities"><u>Yudkowski</u></a> and&nbsp;<a href="https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html"><u>Urban</u></a>.</li>



<li>It is hypothetically possible that companies would be better off en masse if they hired smarter CEOs than they currently do, but given the high compensation CEOs receive this doesnât seem like a particularly plausible equilibrium.</li>
</ol>
<p class="authors">By Boaz Barak</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-22T14:05:27Z">Tuesday, November 22 2022, 14:05</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10669'>Littlewood-Richardson coefficients and Kostka number</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sagar Shrivastava</p><p>Littlewood-Richardson (LR) coefficients and Kostka Numbers appear in
representation theory and combinatorics related to GLn . It is known that
Kostka numbers can be represented as special Littlewood-Rischardson
coefficient. In this paper, we show how one can represent LR coefficient in
terms of Kostka numbers, and use the formulation to give a polynomial time
algorithm for the same, hence showing that they belong to the same class of
decision problems.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Shrivastava_S/0/1/0/all/0/1">Sagar Shrivastava</a></p><p>Littlewood-Richardson (LR) coefficients and Kostka Numbers appear in
representation theory and combinatorics related to GLn . It is known that
Kostka numbers can be represented as special Littlewood-Rischardson
coefficient. In this paper, we show how one can represent LR coefficient in
terms of Kostka numbers, and use the formulation to give a polynomial time
algorithm for the same, hence showing that they belong to the same class of
decision problems.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-22T01:30:00Z">Tuesday, November 22 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10471'>Prophet-Inequalities over Time</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Andreas Abels, Elias Pitschmann, Daniel Schmand</p><p>In this paper, we introduce an over-time variant of the well-known
prophet-inequality with i.i.d. random variables. Instead of stopping with one
realized value at some point in the process, we decide for each step how long
we select the value. Then we cannot select another value until this period is
over. The goal is to maximize the expectation of the sum of selected values. We
describe the structure of the optimal stopping rule and give upper and lower
bounds on the prophet-inequality. - Which, in online algorithms terminology,
corresponds to bounds on the competitive ratio of an online algorithm.
</p>
<p>We give a surprisingly simple algorithm with a single threshold that results
in a prophet-inequality of $\approx 0.396$ for all input lengths $n$.
Additionally, as our main result, we present a more advanced algorithm
resulting in a prophet-inequality of $\approx 0.598$ when the number of steps
tends to infinity. We complement our results by an upper bound that shows that
the best possible prophet-inequality is at most $1/\varphi \approx 0.618$,
where $\varphi$ denotes the golden ratio. As part of the proof, we give an
advanced bound on the weighted mediant.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Abels_A/0/1/0/all/0/1">Andreas Abels</a>, <a href="http://arxiv.org/find/cs/1/au:+Pitschmann_E/0/1/0/all/0/1">Elias Pitschmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmand_D/0/1/0/all/0/1">Daniel Schmand</a></p><p>In this paper, we introduce an over-time variant of the well-known
prophet-inequality with i.i.d. random variables. Instead of stopping with one
realized value at some point in the process, we decide for each step how long
we select the value. Then we cannot select another value until this period is
over. The goal is to maximize the expectation of the sum of selected values. We
describe the structure of the optimal stopping rule and give upper and lower
bounds on the prophet-inequality. - Which, in online algorithms terminology,
corresponds to bounds on the competitive ratio of an online algorithm.
</p>
<p>We give a surprisingly simple algorithm with a single threshold that results
in a prophet-inequality of $\approx 0.396$ for all input lengths $n$.
Additionally, as our main result, we present a more advanced algorithm
resulting in a prophet-inequality of $\approx 0.598$ when the number of steps
tends to infinity. We complement our results by an upper bound that shows that
the best possible prophet-inequality is at most $1/\varphi \approx 0.618$,
where $\varphi$ denotes the golden ratio. As part of the proof, we give an
advanced bound on the weighted mediant.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-22T01:30:00Z">Tuesday, November 22 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10507'>Efficient Determinant Maximization for All Matroids</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Adam Brown, Aditi Laddha, Madhusudhan Pittu, Mohit Singh</p><p>Determinant maximization provides an elegant generalization of problems in
many areas, including convex geometry, statistics, machine learning, fair
allocation of goods, and network design. In an instance of the determinant
maximization problem, we are given a collection of vectors $v_1,\ldots, v_n \in
\mathbb{R}^d$, and the goal is to pick a subset $S\subseteq [n]$ of given
vectors to maximize the determinant of the matrix $\sum_{i \in S} v_iv_i^\top$,
where the picked set of vectors $S$ must satisfy some combinatorial constraint
such as cardinality constraint ($|S| \leq k$) or matroid constraint ($S$ is a
basis of a matroid defined on $[n]$).
</p>
<p>In this work, we give a combinatorial algorithm for the determinant
maximization problem under a matroid constraint that achieves
$O(d^{O(d)})$-approximation for any matroid of rank $r\geq d$. This complements
the recent result of~\cite{BrownLPST22} that achieves a similar bound for
matroids of rank $r\leq d$, relying on a geometric interpretation of the
determinant. Our result matches the best-known estimation
algorithms~\cite{madan2020maximizing} for the problem, which could estimate the
objective value but could not give an approximate solution with a similar
guarantee. Our work follows the framework developed by~\cite{BrownLPST22} of
using matroid intersection based algorithms for determinant maximization. To
overcome the lack of a simple geometric interpretation of the objective when $r
\geq d$, our approach combines ideas from combinatorial optimization with
algebraic properties of the determinant. We also critically use the properties
of a convex programming relaxation of the problem introduced
by~\cite{madan2020maximizing}.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1">Adam Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Laddha_A/0/1/0/all/0/1">Aditi Laddha</a>, <a href="http://arxiv.org/find/cs/1/au:+Pittu_M/0/1/0/all/0/1">Madhusudhan Pittu</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1">Mohit Singh</a></p><p>Determinant maximization provides an elegant generalization of problems in
many areas, including convex geometry, statistics, machine learning, fair
allocation of goods, and network design. In an instance of the determinant
maximization problem, we are given a collection of vectors $v_1,\ldots, v_n \in
\mathbb{R}^d$, and the goal is to pick a subset $S\subseteq [n]$ of given
vectors to maximize the determinant of the matrix $\sum_{i \in S} v_iv_i^\top$,
where the picked set of vectors $S$ must satisfy some combinatorial constraint
such as cardinality constraint ($|S| \leq k$) or matroid constraint ($S$ is a
basis of a matroid defined on $[n]$).
</p>
<p>In this work, we give a combinatorial algorithm for the determinant
maximization problem under a matroid constraint that achieves
$O(d^{O(d)})$-approximation for any matroid of rank $r\geq d$. This complements
the recent result of~\cite{BrownLPST22} that achieves a similar bound for
matroids of rank $r\leq d$, relying on a geometric interpretation of the
determinant. Our result matches the best-known estimation
algorithms~\cite{madan2020maximizing} for the problem, which could estimate the
objective value but could not give an approximate solution with a similar
guarantee. Our work follows the framework developed by~\cite{BrownLPST22} of
using matroid intersection based algorithms for determinant maximization. To
overcome the lack of a simple geometric interpretation of the objective when $r
\geq d$, our approach combines ideas from combinatorial optimization with
algebraic properties of the determinant. We also critically use the properties
of a convex programming relaxation of the problem introduced
by~\cite{madan2020maximizing}.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-22T01:30:00Z">Tuesday, November 22 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10516'>PIM-tree: A Skew-resistant Index for Processing-in-Memory</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Hongbo Kang, Yiwei Zhao, Guy E. Blelloch, Laxman Dhulipala, Yan Gu, Charles McGuffey, Phillip B. Gibbons</p><p>The performance of today's in-memory indexes is bottlenecked by the memory
latency/bandwidth wall. Processing-in-memory (PIM) is an emerging approach that
potentially mitigates this bottleneck, by enabling low-latency memory access
whose aggregate memory bandwidth scales with the number of PIM nodes. There is
an inherent tension, however, between minimizing inter-node communication and
achieving load balance in PIM systems, in the presence of workload skew. This
paper presents PIM-tree, an ordered index for PIM systems that achieves both
low communication and high load balance, regardless of the degree of skew in
the data and the queries. Our skew-resistant index is based on a novel division
of labor between the multi-core host CPU and the PIM nodes, which leverages the
strengths of each. We introduce push-pull search, which dynamically decides
whether to push queries to a PIM-tree node (CPU -&gt; PIM-node) or pull the node's
keys back to the CPU (PIM-node -&gt; CPU) based on workload skew. Combined with
other PIM-friendly optimizations (shadow subtrees and chunked skip lists), our
PIM-tree provides high-throughput, (guaranteed) low communication, and
(guaranteed) high load balance, for batches of point queries, updates, and
range scans.
</p>
<p>We implement the PIM-tree structure, in addition to prior proposed PIM
indexes, on the latest PIM system from UPMEM, with 32 CPU cores and 2048 PIM
nodes. On workloads with 500 million keys and batches of one million queries,
the throughput using PIM-trees is up to 69.7x and 59.1x higher than the two
best prior methods. As far as we know these are the first implementations of an
ordered index on a real PIM system.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1">Hongbo Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yiwei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Blelloch_G/0/1/0/all/0/1">Guy E. Blelloch</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhulipala_L/0/1/0/all/0/1">Laxman Dhulipala</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yan Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+McGuffey_C/0/1/0/all/0/1">Charles McGuffey</a>, <a href="http://arxiv.org/find/cs/1/au:+Gibbons_P/0/1/0/all/0/1">Phillip B. Gibbons</a></p><p>The performance of today's in-memory indexes is bottlenecked by the memory
latency/bandwidth wall. Processing-in-memory (PIM) is an emerging approach that
potentially mitigates this bottleneck, by enabling low-latency memory access
whose aggregate memory bandwidth scales with the number of PIM nodes. There is
an inherent tension, however, between minimizing inter-node communication and
achieving load balance in PIM systems, in the presence of workload skew. This
paper presents PIM-tree, an ordered index for PIM systems that achieves both
low communication and high load balance, regardless of the degree of skew in
the data and the queries. Our skew-resistant index is based on a novel division
of labor between the multi-core host CPU and the PIM nodes, which leverages the
strengths of each. We introduce push-pull search, which dynamically decides
whether to push queries to a PIM-tree node (CPU -&gt; PIM-node) or pull the node's
keys back to the CPU (PIM-node -&gt; CPU) based on workload skew. Combined with
other PIM-friendly optimizations (shadow subtrees and chunked skip lists), our
PIM-tree provides high-throughput, (guaranteed) low communication, and
(guaranteed) high load balance, for batches of point queries, updates, and
range scans.
</p>
<p>We implement the PIM-tree structure, in addition to prior proposed PIM
indexes, on the latest PIM system from UPMEM, with 32 CPU cores and 2048 PIM
nodes. On workloads with 500 million keys and batches of one million queries,
the throughput using PIM-trees is up to 69.7x and 59.1x higher than the two
best prior methods. As far as we know these are the first implementations of an
ordered index on a real PIM system.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-22T01:30:00Z">Tuesday, November 22 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10556'>A Distanced Matching Game, Decremental APSP in Expanders, and Faster Deterministic Algorithms for Graph Cut Problems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Julia Chuzhoy</p><p>Expander graphs play a central role in graph theory and algorithms. With a
number of powerful algorithmic tools developed around them, such as the
Cut-Matching game, expander pruning, expander decomposition, and algorithms for
decremental All-Pairs Shortest Paths (APSP) in expanders, to name just a few,
the use of expanders in the design of graph algorithms has become ubiquitous.
Specific applications of interest to us are fast deterministic algorithms for
cut problems in static graphs, and algorithms for dynamic distance-based graph
problems, such as APSP.
</p>
<p>Unfortunately, the use of expanders in these settings incurs a number of
drawbacks. For example, the best currently known algorithm for decremental APSP
in constant-degree expanders can only achieve a $(\log
n)^{O(1/\epsilon^2)}$-approximation with $n^{1+O(\epsilon)}$ total update time
for any $\epsilon$. All currently known algorithms for the Cut Player in the
Cut-Matching game are either randomized, or provide rather weak guarantees.
This, in turn, leads to somewhat weak algorithmic guarantees for several
central cut problems: for example, the best current almost linear time
deterministic algorithm for Sparsest Cut can only achieve approximation factor
$(\log n)^{\omega(1)}$. Lastly, when relying on expanders in distance-based
problems, such as dynamic APSP, via current methods, it seems inevitable that
one has to settle for approximation factors that are at least $\Omega(\log n)$.
</p>
<p>In this paper we propose the use of well-connected graphs, and introduce a
new algorithmic toolkit for such graphs that, in a sense, mirrors the above
mentioned algorithmic tools for expanders. One of these new tools is the
Distanced Matching game, an analogue of the Cut-Matching game for
well-connected graphs. We demonstrate the power of these new tools by obtaining
better results for several of the problems mentioned above.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chuzhoy_J/0/1/0/all/0/1">Julia Chuzhoy</a></p><p>Expander graphs play a central role in graph theory and algorithms. With a
number of powerful algorithmic tools developed around them, such as the
Cut-Matching game, expander pruning, expander decomposition, and algorithms for
decremental All-Pairs Shortest Paths (APSP) in expanders, to name just a few,
the use of expanders in the design of graph algorithms has become ubiquitous.
Specific applications of interest to us are fast deterministic algorithms for
cut problems in static graphs, and algorithms for dynamic distance-based graph
problems, such as APSP.
</p>
<p>Unfortunately, the use of expanders in these settings incurs a number of
drawbacks. For example, the best currently known algorithm for decremental APSP
in constant-degree expanders can only achieve a $(\log
n)^{O(1/\epsilon^2)}$-approximation with $n^{1+O(\epsilon)}$ total update time
for any $\epsilon$. All currently known algorithms for the Cut Player in the
Cut-Matching game are either randomized, or provide rather weak guarantees.
This, in turn, leads to somewhat weak algorithmic guarantees for several
central cut problems: for example, the best current almost linear time
deterministic algorithm for Sparsest Cut can only achieve approximation factor
$(\log n)^{\omega(1)}$. Lastly, when relying on expanders in distance-based
problems, such as dynamic APSP, via current methods, it seems inevitable that
one has to settle for approximation factors that are at least $\Omega(\log n)$.
</p>
<p>In this paper we propose the use of well-connected graphs, and introduce a
new algorithmic toolkit for such graphs that, in a sense, mirrors the above
mentioned algorithmic tools for expanders. One of these new tools is the
Distanced Matching game, an analogue of the Cut-Matching game for
well-connected graphs. We demonstrate the power of these new tools by obtaining
better results for several of the problems mentioned above.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-22T01:30:00Z">Tuesday, November 22 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10887'>Differential Privacy from Locally Adjustable Graph Algorithms: $k$-Core Decomposition, Low Out-Degree Ordering, and Densest Subgraphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Laxman Dhulipala, Quanquan C. Liu, Sofya Raskhodnikova, Jessica Shi, Julian Shun, Shangdi Yu</p><p>Differentially private algorithms allow large-scale data analytics while
preserving user privacy. Designing such algorithms for graph data is gaining
importance with the growth of large networks that model various (sensitive)
relationships between individuals. While there exists a rich history of
important literature in this space, to the best of our knowledge, no results
formalize a relationship between certain parallel and distributed graph
algorithms and differentially private graph analysis. In this paper, we define
\emph{locally adjustable} graph algorithms and show that algorithms of this
type can be transformed into differentially private algorithms.
</p>
<p>Our formalization is motivated by a set of results that we present in the
central and local models of differential privacy for a number of problems,
including $k$-core decomposition, low out-degree ordering, and densest
subgraphs. First, we design an $\varepsilon$-edge differentially private (DP)
algorithm that returns a subset of nodes that induce a subgraph of density at
least $\frac{D^*}{1+\eta} - O\left(\text{poly}(\log n)/\varepsilon\right),$
where $D^*$ is the density of the densest subgraph in the input graph (for any
constant $\eta &gt; 0$). This algorithm achieves a two-fold improvement on the
multiplicative approximation factor of the previously best-known private
densest subgraph algorithms while maintaining a near-linear runtime.
</p>
<p>Then, we present an $\varepsilon$-locally edge differentially private (LEDP)
algorithm for $k$-core decompositions. Our LEDP algorithm provides approximates
the core numbers (for any constant $\eta &gt; 0$) with $(2+\eta)$ multiplicative
and $O\left(\text{poly}\left(\log n\right)/\varepsilon\right)$ additive error.
This is the first differentially private algorithm that outputs private
$k$-core decomposition statistics.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dhulipala_L/0/1/0/all/0/1">Laxman Dhulipala</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Quanquan C. Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Raskhodnikova_S/0/1/0/all/0/1">Sofya Raskhodnikova</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jessica Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shun_J/0/1/0/all/0/1">Julian Shun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Shangdi Yu</a></p><p>Differentially private algorithms allow large-scale data analytics while
preserving user privacy. Designing such algorithms for graph data is gaining
importance with the growth of large networks that model various (sensitive)
relationships between individuals. While there exists a rich history of
important literature in this space, to the best of our knowledge, no results
formalize a relationship between certain parallel and distributed graph
algorithms and differentially private graph analysis. In this paper, we define
\emph{locally adjustable} graph algorithms and show that algorithms of this
type can be transformed into differentially private algorithms.
</p>
<p>Our formalization is motivated by a set of results that we present in the
central and local models of differential privacy for a number of problems,
including $k$-core decomposition, low out-degree ordering, and densest
subgraphs. First, we design an $\varepsilon$-edge differentially private (DP)
algorithm that returns a subset of nodes that induce a subgraph of density at
least $\frac{D^*}{1+\eta} - O\left(\text{poly}(\log n)/\varepsilon\right),$
where $D^*$ is the density of the densest subgraph in the input graph (for any
constant $\eta &gt; 0$). This algorithm achieves a two-fold improvement on the
multiplicative approximation factor of the previously best-known private
densest subgraph algorithms while maintaining a near-linear runtime.
</p>
<p>Then, we present an $\varepsilon$-locally edge differentially private (LEDP)
algorithm for $k$-core decompositions. Our LEDP algorithm provides approximates
the core numbers (for any constant $\eta &gt; 0$) with $(2+\eta)$ multiplicative
and $O\left(\text{poly}\left(\log n\right)/\varepsilon\right)$ additive error.
This is the first differentially private algorithm that outputs private
$k$-core decomposition statistics.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-22T01:30:00Z">Tuesday, November 22 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Monday, November 21
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2022/11/21/straight-line-through.html'>A straight line through every face</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          While updating my online publications list for something else I noticed that I had neglected to discuss one of my papers from earlier this fall: âGeodesic paths passing through all faces on a polyhedronâ (with Demaine, Demaine, Ito, Katayama, Maruyama, and Uno), in the booklet of abstracts from JCDCG3 2022, the Japanese Conference on Discrete and Computational Geometry, Graphs, and Games.
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>While updating my online publications list for something else I noticed that I had neglected to discuss one of my papers from earlier this fall: âGeodesic paths passing through all faces on a polyhedronâ (with Demaine, Demaine, Ito, Katayama, Maruyama, and Uno), in the <a href="https://www.rs.tus.ac.jp/jcdcggg/JCDCG3-2022Proceedings(r2).pdf">booklet of abstracts from JCDCG<sup>3</sup> 2022</a>, the Japanese Conference on Discrete and Computational Geometry, Graphs, and Games.</p>

<p>The paper is kind of telegraphic, but the question it considers is easily stated. On the surface of a polyhedron, the analogue of a straight line is a geodesic, the shortest curve between two points. Which polyhedra have geodesics that cross through all of their faces? Maybe the 2d version is easier to explain: any two points on a convex polygon split the polygon into two arcs, and a geodesic is the shorter of the two. Which polygons have at least one geodesic that includes a segment from each edge?</p>

<p style="text-align:center"><img src="/blog/assets/2022/2d-univ-geodesics.svg" alt="Geodesics through all edges of a kite and a trapezoid" style="width:100%;max-width:600px" /></p>

<p>The endpoints of such a geodesic \(A\) must be in different edges, because if they were in the same edge then the complementary arc \(\bar A\) would be a straight line segment, shorter than any other arc. Those two edges must be adjacent, because otherwise \(\bar A\) would include an edge missed by \(A\). And these two adjacent edges must be longer than the sum of all the other edges, so that \(A\) (a superset of the other edges) can be the shorter than \(\bar A\) (a subset of the two adjacent edges). That turns out to be an exact characterization: a convex polygon has two points whose geodesic passes through all edges, if and only if it has two adjacent edges that together have more than half the perimeter. For these polygons, the arc \(A\) can be chosen to have its endpoints near the outer vertices of the two long adjacent edges. So this is possible for all triangles, for any quadrilateral that is not a parallelogram, and for many other polygons of arbitrarily many sides. But it does not work for any centrally symmetric polygon, because each two adjacent sides are at least matched in length by the two opposite sides.</p>

<p>When we first discussed this problem (five years ago at a Barbados workshop), we started with the idea that no polyhedron with two parallel faces can have a geodesic through all faces. In particular, this would imply that the only regular polyhedron with a geodesic through all faces is a regular tetrahedron. But itâs not true! Instead, if \(P\) is any polygon with a geodesic through all edges, then long-enough right prisms over \(P\) have geodesics through all faces.</p>

<p>Geodesics on the surface of a convex polyhedron may be easier to understand by unfolding the polyhedron into a <a href="https://en.wikipedia.org/wiki/Net_(polyhedron)">net</a>, a flat system of polygons in the plane, drawing the line segment between the endpoints of the geodesics in the net, and then folding it back up. The complication is that the line segment needs to stay inside the net, and there may be many different nets with different line segments.</p>

<p>Suppose \(P\) is a polygon with a geodesic \(A\) through all edges, like the yellow kite above. The prism over \(P\) has two copies of \(P\), connected by rectangles. It can be unfolded by unrolling the rectangles into one long rectangular strip, and connecting the two copies of \(P\) to the top and bottom of the strip, as shown below. (The lightly shaded copy of \(P\) is an alternative placement on the top of the strip; you should only keep one of the two top copies.) To make a curve through all faces on a prism over \(P\),
attach each copy of \(P\) along one of its two adjacent long edges, and arrange the rectangular strip with these two attached copies at opposite ends. Then, connect a point on the top copy of \(P\), near the start of \(A\) on that copy, to another point on the bottom copy of \(P\), near the end of \(A\) on that copy. The resulting curve is shown below as the red segment on its net.</p>

<p style="text-align:center"><img src="/blog/assets/2022/prism-univ-geodesic.svg" alt="A geodesic through all faces of an unfolded prism over a kite (red) and a different curve that is not a geodesic (blue)" style="width:100%;max-width:600px" /></p>

<p>The red curve is definitely shorter than the curve that you would get by applying the same construction to \(\bar A\), which would be drawn in its unfolding as a segment with the same height but greater width. But what we have to worry more about is the blue curve in the figure, which cuts through one of the rectangular sides of the prism before cutting straight across one of the copies of \(P\). Could such a curve be shorter? It isnât in the figure (I measured), but what about more generally?</p>

<p>When the height of the prism is small, the blue curve can be shorter. But in the limit as the height of the prism gets much larger than the size of \(P\), it cannot. The length of curves like the blue one, in the limit, approaches the height of the prism plus the height of the endpoint above the central rectangular region. Instead, in the limit, the length of curves like the red one approaches the height of the prism: the horizontal component of the curve contributes negligibly to its length. So for tall enough prisms the red curve is shorter than any curve like the blue one, and we have a universal geodesic. (You might think that attaching the top and bottom face in the middle of the rectangular strip would produce a shorter geodesic, and for the illustrated prism maybe it does, but as long as the endpoints are much closer to the edge of \(P\) than to the corner of \(P\), the same argument also applies to these alternative curves.)</p>

<p>Wataru Maruyama, a student of Hiro Ito and a coauthor of the paper, succeeded in proving that the other regular polyhedra indeed do not have universal geodesics. We could also prove that every tetrahedron or right prism over a triangle does have one. On the other hand, much more remains unknown. In particular, we do not have an answer to the following question: is there a centrally symmetric polyhedron with a universal geodesic?</p>

<p>(<a href="https://mathstodon.xyz/@11011110/109386054076782254">Discuss on Mastodon</a>)</p><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T22:08:00Z">Monday, November 21 2022, 22:08</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/21/faculty-position-at-cs-department-boston-university-apply-by-december-2-2022/'>Faculty position at CS  department,   Boston University  (apply by December 2, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Two tenure-track assistant professorships beginning July 1, 2023. Strong applicants in all areas of computer science are encouraged to apply, particularly in theory of computation, algorithms, and systems. Applicants working on foundational, methodological, or use-inspired AI research are encouraged to apply to the AI cluster hire initiative. Website: www.bu.edu/cs/2022/10/04/bu-cs-invites-applications-for-new-faculty-members-2022/ Email: canetti@bu.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Two tenure-track assistant professorships beginning July 1, 2023. Strong applicants in all areas of computer science are encouraged to apply, particularly in theory of computation, algorithms, and systems. Applicants working on foundational, methodological, or use-inspired AI research are encouraged to apply to the AI cluster hire initiative.</p>
<p>Website: <a href="https://www.bu.edu/cs/2022/10/04/bu-cs-invites-applications-for-new-faculty-members-2022/">https://www.bu.edu/cs/2022/10/04/bu-cs-invites-applications-for-new-faculty-members-2022/</a><br />
Email: canetti@bu.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T21:26:41Z">Monday, November 21 2022, 21:26</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/21/faculty-at-rutgers-university-new-brunswick-apply-by-january-3-2023/'>Faculty at Rutgers University (New Brunswick) (apply by January 3, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The Computer Science Department at Rutgers University, New Brunswick NJ, invites applications for multiple tenure-track/tenured. We invite applications from candidates specializing in any area of CS, and welcome applicants with interdisciplinary approaches. We are especially interested in Algorithms, Machine Learning and Data Science, High-performance Computing and Scalable Systems. Website: jobs.rutgers.edu/postings/183703 Email: hiring-committee@cs.rutgers.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The Computer Science Department at Rutgers University, New Brunswick NJ, invites applications for multiple tenure-track/tenured. We invite applications from candidates specializing in any area of CS, and welcome applicants with interdisciplinary approaches. We are especially interested in Algorithms, Machine Learning and Data Science, High-performance Computing and Scalable Systems.</p>
<p>Website: <a href="https://jobs.rutgers.edu/postings/183703">https://jobs.rutgers.edu/postings/183703</a><br />
Email: hiring-committee@cs.rutgers.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T19:14:14Z">Monday, November 21 2022, 19:14</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://blog.computationalcomplexity.org/2022/11/a-celebration-of-juris.html'>A Celebration of Juris</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p></p>â¦<br>On November 4th I travelled to my undergraduate alma mater Cornell for a Celebration of the Life and Career of Juris Hartmanis&nbsp;who passed away in July. The workshop attracted many Cornell faculty and students, many of Hartmanis' former colleague and students, grad and undergrad, as well as his family. For the most part, the talks did not focus on technical content but rather memories of the great man.&nbsp;<p></p><p>I talked about&nbsp;how Hartmanis founded the field of Computational Complexity and brought me into it. Herbert Lin told the story behind Computing the Future, a 1992 agenda for the future of computer science led by Hartmanis and the challenge to the report by John McCarthy, one of the founders of AI. Should the agenda of computer science be solely in the hands of academic computer scientists, or should it take into account its role in the larger scientific and world-wide community? We still face these questions today.</p><p>Ryan Williams gave a powerful talk&nbsp;about how Hartmanis personally intervened to ensure Ryan had a future in complexity. We are all better off for that.</p><p>After the workshop, Ryan and I walked around the campus and Collegetown reminiscing on how things have changed in the two decades since Ryan was an undergrad and the four decades (!) since I was. Most of the bars and restaurants have disappeared. The Arts quad is mostly the same, while the engineering building have been mostly rebuilt. There's a new computer science building with another on the way.&nbsp;</p><p>I stayed in town to catch the Cornell football game the next day, as I once was on that field playing tuba for the marching band. They tore down the west stands to put up a parking lot and the east stands were sparsely filled watching Penn dominate the game.</p><p>Good bye Juris. You created a discipline, started one of the first CS departments, and plotted the future of both computational complexity and computer science as a whole. A master and commander indeed.</p><p>By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p></p><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxWze7urg3GUKNZRP8vI4maCWwNHi1JQkVcFCRfLas3dZQbmvYZz4jLurkuCXLAiLOFmKXjg7QAFHh5iNwt2vCR4ONgOSIHSBRpw1cC0rQjCvm9bKSSREiDMmPRLK8N3xtg9la8HCM7yr1iOTwP9v4FWv1eYhwGAQoF6JrbXlEZvw8LR-kug/s4080/PXL_20221104_140728676.jpg" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="3072" data-original-width="4080" height="241" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxWze7urg3GUKNZRP8vI4maCWwNHi1JQkVcFCRfLas3dZQbmvYZz4jLurkuCXLAiLOFmKXjg7QAFHh5iNwt2vCR4ONgOSIHSBRpw1cC0rQjCvm9bKSSREiDMmPRLK8N3xtg9la8HCM7yr1iOTwP9v4FWv1eYhwGAQoF6JrbXlEZvw8LR-kug/s320/PXL_20221104_140728676.jpg" width="320" /></a></div><br />On November 4th I travelled to my undergraduate alma mater Cornell for a <a href="https://cis.cornell.edu/bowers-cis-community-celebrates-life-juris-hartmanis">Celebration of the Life and Career of Juris Hartmanis</a>&nbsp;who <a href="https://blog.computationalcomplexity.org/2022/08/the-godfather-of-complexity.html">passed away</a> in July. The workshop attracted many Cornell faculty and students, many of Hartmanis' former colleague and students, grad and undergrad, as well as his family. For the most part, the talks did not focus on technical content but rather memories of the great man.&nbsp;<p></p><p>I <a href="https://www.youtube.com/watch?v=ACxU-90O-ag&amp;t=2768s">talked about</a>&nbsp;how Hartmanis founded the field of Computational Complexity and brought me into it. Herbert Lin <a href="https://www.youtube.com/watch?v=QKW_GalI31o&amp;t=2900s">told the story</a> behind <a href="https://www.google.com/books/edition/Computing_the_Future/tYBQAAAAMAAJ">Computing the Future</a>, a 1992 agenda for the future of computer science led by Hartmanis and the challenge to the report by John McCarthy, one of the founders of AI. Should the agenda of computer science be solely in the hands of academic computer scientists, or should it take into account its role in the larger scientific and world-wide community? We still face these questions today.</p><p>Ryan Williams gave <a href="https://www.youtube.com/watch?v=ACxU-90O-ag&amp;t=10350s">a powerful talk</a>&nbsp;about how Hartmanis personally intervened to ensure Ryan had a future in complexity. We are all better off for that.</p><p>After the workshop, Ryan and I walked around the campus and Collegetown reminiscing on how things have changed in the two decades since Ryan was an undergrad and the four decades (!) since I was. Most of the bars and restaurants have disappeared. The Arts quad is mostly the same, while the engineering building have been mostly rebuilt. There's a <a href="https://www.engineering.cornell.edu/magazine/features/gates-hall-new-home-cis">new computer science building</a> with <a href="https://ithacavoice.com/2022/05/cornell-plans-new-computer-science-building-on-hoy-field/">another on the way</a>.&nbsp;</p><p>I stayed in town to catch the Cornell football game the next day, as I once was on that field playing tuba for the marching band. They tore down the west stands to put up a parking lot and the east stands were sparsely filled watching Penn dominate the game.</p><p>Good bye Juris. You created a discipline, started one of the first CS departments, and plotted the future of both computational complexity and computer science as a whole. A master and commander indeed.</p><p class="authors">By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T14:52:00Z">Monday, November 21 2022, 14:52</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10144'>Computational Short Cuts in Infinite Domain Constraint Satisfaction</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Peter Jonsson, Victor Lagerkvist, Sebastian Ordyniak</p><p>A backdoor in a finite-domain CSP instance is a set of variables where each
possible instantiation moves the instance into a polynomial-time solvable
class. Backdoors have found many applications in artificial intelligence and
elsewhere, and the algorithmic problem of finding such backdoors has
consequently been intensively studied. Sioutis and Janhunen (Proc. 42nd German
Conference on AI (KI-2019)) have proposed a generalised backdoor concept
suitable for infinite-domain CSP instances over binary constraints. We
generalise their concept into a large class of CSPs that allow for higher-arity
constraints. We show that this kind of infinite-domain backdoors have many of
the positive computational properties that finite-domain backdoors have: the
associated computational problems are fixed-parameter tractable whenever the
underlying constraint language is finite. On the other hand, we show that
infinite languages make the problems considerably harder: the general backdoor
detection problem is W[2]-hard and fixed-parameter tractability is ruled out
under standard complexity-theoretic assumptions. We demonstrate that backdoors
may have suboptimal behaviour on binary constraints -- this is detrimental from
an AI perspective where binary constraints are predominant in, for instance,
spatiotemporal applications. In response to this, we introduce sidedoors as an
alternative to backdoors. The fundamental computational problems for sidedoors
remain fixed-parameter tractable for finite constraint language (possibly also
containing non-binary relations). Moreover, the sidedoor approach has appealing
computational properties that sometimes leads to faster algorithms than the
backdoor approach.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jonsson_P/0/1/0/all/0/1">Peter Jonsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Lagerkvist_V/0/1/0/all/0/1">Victor Lagerkvist</a>, <a href="http://arxiv.org/find/cs/1/au:+Ordyniak_S/0/1/0/all/0/1">Sebastian Ordyniak</a></p><p>A backdoor in a finite-domain CSP instance is a set of variables where each
possible instantiation moves the instance into a polynomial-time solvable
class. Backdoors have found many applications in artificial intelligence and
elsewhere, and the algorithmic problem of finding such backdoors has
consequently been intensively studied. Sioutis and Janhunen (Proc. 42nd German
Conference on AI (KI-2019)) have proposed a generalised backdoor concept
suitable for infinite-domain CSP instances over binary constraints. We
generalise their concept into a large class of CSPs that allow for higher-arity
constraints. We show that this kind of infinite-domain backdoors have many of
the positive computational properties that finite-domain backdoors have: the
associated computational problems are fixed-parameter tractable whenever the
underlying constraint language is finite. On the other hand, we show that
infinite languages make the problems considerably harder: the general backdoor
detection problem is W[2]-hard and fixed-parameter tractability is ruled out
under standard complexity-theoretic assumptions. We demonstrate that backdoors
may have suboptimal behaviour on binary constraints -- this is detrimental from
an AI perspective where binary constraints are predominant in, for instance,
spatiotemporal applications. In response to this, we introduce sidedoors as an
alternative to backdoors. The fundamental computational problems for sidedoors
remain fixed-parameter tractable for finite constraint language (possibly also
containing non-binary relations). Moreover, the sidedoor approach has appealing
computational properties that sometimes leads to faster algorithms than the
backdoor approach.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T01:30:00Z">Monday, November 21 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09904'>Crossing and intersecting families of geometric graphs on point sets</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jos&#xe9; Luis &#xc1;lvarez-Rebollar (1), Jorge Cravioto-Lagos (2), Nestaly Mar&#xed;n (2), Oriol Sol&#xe9;-Pi (3), Jorge Urrutia (4) ((1) Posgrado en Ciencias Matem&#xe1;ticas, UNAM and Departamento de Ciencias B&#xe1;sicas, Instituto Tecnol&#xf3;gico de Zit&#xe1;cuaro, (2) Posgrado en Ciencia e Ingenier&#xed;a de la Computaci&#xf3;n, UNAM, (3) Facultad de Ciencias, UNAM, (4) Instituto de Matem&#xe1;ticas, UNAM)</p><p>Let $S$ be a set of $n$ points in the plane in general position. Two line
segments connecting pairs of points of $S$ cross if they have an interior point
in common. Two vertex disjoint geometric graphs with vertices in $S$ cross if
there are two edges, one from each graph, which cross. A set of vertex disjoint
geometric graphs with vertices in $S$ is called mutually crossing if any two of
them cross.
</p>
<p>We show that there exists a constant $c$ such that from any family of $n$
mutually crossing triangles, one can always obtain a family of at least $n^c$
mutually crossing $2$-paths (each of which is the result of deleting an edge
from one of the triangles) and then provide an example that implies that $c$
cannot be taken to be larger than $2/3$. For every $n$ we determine the maximum
number of crossings that a Hamiltonian cycle on a set of $n$ points might have.
Next, we construct a point set whose longest perfect matching contains no
crossings. We also consider edges consisting of a horizontal and a vertical
line segment joining pairs of points of $S$, which we call elbows, and prove
that in any point set $S$ there exists a family of $\lfloor n/4 \rfloor$ vertex
disjoint mutually crossing elbows. Additionally, we show a point set that
admits no more than $n/3$ mutually crossing elbows.
</p>
<p>Finally we study intersecting families of graphs, which are not necessarily
vertex disjoint. A set of edge disjoint graphs with vertices in $S$ is called
an intersecting family if for any two graphs in the set we can choose an edge
in each of them such that they cross. We prove a conjecture by Lara and
Rubio-Montiel, namely, that any set $S$ of $n$ points in general position
admits a family of intersecting triangles with a quadratic number of elements.
</p>
<p>Some other results are obtained throughout this work.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Alvarez_Rebollar_J/0/1/0/all/0/1">Jos&#xe9; Luis &#xc1;lvarez-Rebollar</a> (1), <a href="http://arxiv.org/find/math/1/au:+Cravioto_Lagos_J/0/1/0/all/0/1">Jorge Cravioto-Lagos</a> (2), <a href="http://arxiv.org/find/math/1/au:+Marin_N/0/1/0/all/0/1">Nestaly Mar&#xed;n</a> (2), <a href="http://arxiv.org/find/math/1/au:+Sole_Pi_O/0/1/0/all/0/1">Oriol Sol&#xe9;-Pi</a> (3), <a href="http://arxiv.org/find/math/1/au:+Urrutia_J/0/1/0/all/0/1">Jorge Urrutia</a> (4) ((1) Posgrado en Ciencias Matem&#xe1;ticas, UNAM and Departamento de Ciencias B&#xe1;sicas, Instituto Tecnol&#xf3;gico de Zit&#xe1;cuaro, (2) Posgrado en Ciencia e Ingenier&#xed;a de la Computaci&#xf3;n, UNAM, (3) Facultad de Ciencias, UNAM, (4) Instituto de Matem&#xe1;ticas, UNAM)</p><p>Let $S$ be a set of $n$ points in the plane in general position. Two line
segments connecting pairs of points of $S$ cross if they have an interior point
in common. Two vertex disjoint geometric graphs with vertices in $S$ cross if
there are two edges, one from each graph, which cross. A set of vertex disjoint
geometric graphs with vertices in $S$ is called mutually crossing if any two of
them cross.
</p>
<p>We show that there exists a constant $c$ such that from any family of $n$
mutually crossing triangles, one can always obtain a family of at least $n^c$
mutually crossing $2$-paths (each of which is the result of deleting an edge
from one of the triangles) and then provide an example that implies that $c$
cannot be taken to be larger than $2/3$. For every $n$ we determine the maximum
number of crossings that a Hamiltonian cycle on a set of $n$ points might have.
Next, we construct a point set whose longest perfect matching contains no
crossings. We also consider edges consisting of a horizontal and a vertical
line segment joining pairs of points of $S$, which we call elbows, and prove
that in any point set $S$ there exists a family of $\lfloor n/4 \rfloor$ vertex
disjoint mutually crossing elbows. Additionally, we show a point set that
admits no more than $n/3$ mutually crossing elbows.
</p>
<p>Finally we study intersecting families of graphs, which are not necessarily
vertex disjoint. A set of edge disjoint graphs with vertices in $S$ is called
an intersecting family if for any two graphs in the set we can choose an edge
in each of them such that they cross. We prove a conjecture by Lara and
Rubio-Montiel, namely, that any set $S$ of $n$ points in general position
admits a family of intersecting triangles with a quadratic number of elements.
</p>
<p>Some other results are obtained throughout this work.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T01:30:00Z">Monday, November 21 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09964'>Optimal Algorithms for Linear Algebra in the Current Matrix Multiplication Time</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yeshwanth Cherapanamjeri, Sandeep Silwal, David P. Woodruff, Samson Zhou</p><p>We study fundamental problems in linear algebra, such as finding a maximal
linearly independent subset of rows or columns (a basis), solving linear
regression, or computing a subspace embedding. For these problems, we consider
input matrices $\mathbf{A}\in\mathbb{R}^{n\times d}$ with $n &gt; d$. The input
can be read in $\text{nnz}(\mathbf{A})$ time, which denotes the number of
nonzero entries of $\mathbf{A}$. In this paper, we show that beyond the time
required to read the input matrix, these fundamental linear algebra problems
can be solved in $d^{\omega}$ time, i.e., where $\omega \approx 2.37$ is the
current matrix-multiplication exponent.
</p>
<p>To do so, we introduce a constant-factor subspace embedding with the optimal
$m=\mathcal{O}(d)$ number of rows, and which can be applied in time
$\mathcal{O}\left(\frac{\text{nnz}(\mathbf{A})}{\alpha}\right) + d^{2 +
\alpha}\text{poly}(\log d)$ for any trade-off parameter $\alpha&gt;0$, tightening
a recent result by Chepurko et. al. [SODA 2022] that achieves an
$\exp(\text{poly}(\log\log n))$ distortion with $m=d\cdot\text{poly}(\log\log
d)$ rows in
$\mathcal{O}\left(\frac{\text{nnz}(\mathbf{A})}{\alpha}+d^{2+\alpha+o(1)}\right)$
time. Our subspace embedding uses a recently shown property of stacked
Subsampled Randomized Hadamard Transforms (SRHT), which actually increase the
input dimension, to "spread" the mass of an input vector among a large number
of coordinates, followed by random sampling. To control the effects of random
sampling, we use fast semidefinite programming to reweight the rows. We then
use our constant-factor subspace embedding to give the first optimal runtime
algorithms for finding a maximal linearly independent subset of columns,
regression, and leverage score sampling. To do so, we also introduce a novel
subroutine that iteratively grows a set of independent rows, which may be of
independent interest.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cherapanamjeri_Y/0/1/0/all/0/1">Yeshwanth Cherapanamjeri</a>, <a href="http://arxiv.org/find/cs/1/au:+Silwal_S/0/1/0/all/0/1">Sandeep Silwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1">David P. Woodruff</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Samson Zhou</a></p><p>We study fundamental problems in linear algebra, such as finding a maximal
linearly independent subset of rows or columns (a basis), solving linear
regression, or computing a subspace embedding. For these problems, we consider
input matrices $\mathbf{A}\in\mathbb{R}^{n\times d}$ with $n &gt; d$. The input
can be read in $\text{nnz}(\mathbf{A})$ time, which denotes the number of
nonzero entries of $\mathbf{A}$. In this paper, we show that beyond the time
required to read the input matrix, these fundamental linear algebra problems
can be solved in $d^{\omega}$ time, i.e., where $\omega \approx 2.37$ is the
current matrix-multiplication exponent.
</p>
<p>To do so, we introduce a constant-factor subspace embedding with the optimal
$m=\mathcal{O}(d)$ number of rows, and which can be applied in time
$\mathcal{O}\left(\frac{\text{nnz}(\mathbf{A})}{\alpha}\right) + d^{2 +
\alpha}\text{poly}(\log d)$ for any trade-off parameter $\alpha&gt;0$, tightening
a recent result by Chepurko et. al. [SODA 2022] that achieves an
$\exp(\text{poly}(\log\log n))$ distortion with $m=d\cdot\text{poly}(\log\log
d)$ rows in
$\mathcal{O}\left(\frac{\text{nnz}(\mathbf{A})}{\alpha}+d^{2+\alpha+o(1)}\right)$
time. Our subspace embedding uses a recently shown property of stacked
Subsampled Randomized Hadamard Transforms (SRHT), which actually increase the
input dimension, to "spread" the mass of an input vector among a large number
of coordinates, followed by random sampling. To control the effects of random
sampling, we use fast semidefinite programming to reweight the rows. We then
use our constant-factor subspace embedding to give the first optimal runtime
algorithms for finding a maximal linearly independent subset of columns,
regression, and leverage score sampling. To do so, we also introduce a novel
subroutine that iteratively grows a set of independent rows, which may be of
independent interest.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T01:30:00Z">Monday, November 21 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10022'>Listing 4-Cycles</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Amir Abboud, Seri Khoury, Oree Leibowitz, Ron Safier</p><p>In this note we present an algorithm that lists all $4$-cycles in a graph in
time $\tilde{O}(\min(n^2,m^{4/3})+t)$ where $t$ is their number. Notably, this
separates $4$-cycle listing from triangle-listing, since the latter has a
$(\min(n^3,m^{3/2})+t)^{1-o(1)}$ lower bound under the $3$-SUM Conjecture.
</p>
<p>Our upper bound is conditionally tight because (1) $O(n^2,m^{4/3})$ is the
best known bound for detecting if the graph has any $4$-cycle, and (2) it
matches a recent $(\min(n^3,m^{3/2})+t)^{1-o(1)}$ $3$-SUM lower bound for
enumeration algorithms.
</p>
<p>The latter lower bound was proved very recently by Abboud, Bringmann, and
Fischer [arXiv, 2022] and independently by Jin and Xu [arXiv, 2022].
</p>
<p>In an independent work, Jin and Xu [arXiv, 2022] also present an algorithm
with the same time bound.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Abboud_A/0/1/0/all/0/1">Amir Abboud</a>, <a href="http://arxiv.org/find/cs/1/au:+Khoury_S/0/1/0/all/0/1">Seri Khoury</a>, <a href="http://arxiv.org/find/cs/1/au:+Leibowitz_O/0/1/0/all/0/1">Oree Leibowitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Safier_R/0/1/0/all/0/1">Ron Safier</a></p><p>In this note we present an algorithm that lists all $4$-cycles in a graph in
time $\tilde{O}(\min(n^2,m^{4/3})+t)$ where $t$ is their number. Notably, this
separates $4$-cycle listing from triangle-listing, since the latter has a
$(\min(n^3,m^{3/2})+t)^{1-o(1)}$ lower bound under the $3$-SUM Conjecture.
</p>
<p>Our upper bound is conditionally tight because (1) $O(n^2,m^{4/3})$ is the
best known bound for detecting if the graph has any $4$-cycle, and (2) it
matches a recent $(\min(n^3,m^{3/2})+t)^{1-o(1)}$ $3$-SUM lower bound for
enumeration algorithms.
</p>
<p>The latter lower bound was proved very recently by Abboud, Bringmann, and
Fischer [arXiv, 2022] and independently by Jin and Xu [arXiv, 2022].
</p>
<p>In an independent work, Jin and Xu [arXiv, 2022] also present an algorithm
with the same time bound.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T01:30:00Z">Monday, November 21 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10041'>The communication cost of security and privacy in federated frequency estimation</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Wei-Ning Chen, Ayfer &#xd6;zg&#xfc;r, Graham Cormode, Akash Bharadwaj</p><p>We consider the federated frequency estimation problem, where each user holds
a private item $X_i$ from a size-$d$ domain and a server aims to estimate the
empirical frequency (i.e., histogram) of $n$ items with $n \ll d$. Without any
security and privacy considerations, each user can communicate its item to the
server by using $\log d$ bits. A naive application of secure aggregation
protocols would, however, require $d\log n$ bits per user. Can we reduce the
communication needed for secure aggregation, and does security come with a
fundamental cost in communication?
</p>
<p>In this paper, we develop an information-theoretic model for secure
aggregation that allows us to characterize the fundamental cost of security and
privacy in terms of communication. We show that with security (and without
privacy) $\Omega\left( n \log d \right)$ bits per user are necessary and
sufficient to allow the server to compute the frequency distribution. This is
significantly smaller than the $d\log n$ bits per user needed by the naive
scheme, but significantly higher than the $\log d$ bits per user needed without
security. To achieve differential privacy, we construct a linear scheme based
on a noisy sketch which locally perturbs the data and does not require a
trusted server (a.k.a. distributed differential privacy). We analyze this
scheme under $\ell_2$ and $\ell_\infty$ loss. By using our
information-theoretic framework, we show that the scheme achieves the optimal
accuracy-privacy trade-off with optimal communication cost, while matching the
performance in the centralized case where data is stored in the central server.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wei-Ning Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozgur_A/0/1/0/all/0/1">Ayfer &#xd6;zg&#xfc;r</a>, <a href="http://arxiv.org/find/cs/1/au:+Cormode_G/0/1/0/all/0/1">Graham Cormode</a>, <a href="http://arxiv.org/find/cs/1/au:+Bharadwaj_A/0/1/0/all/0/1">Akash Bharadwaj</a></p><p>We consider the federated frequency estimation problem, where each user holds
a private item $X_i$ from a size-$d$ domain and a server aims to estimate the
empirical frequency (i.e., histogram) of $n$ items with $n \ll d$. Without any
security and privacy considerations, each user can communicate its item to the
server by using $\log d$ bits. A naive application of secure aggregation
protocols would, however, require $d\log n$ bits per user. Can we reduce the
communication needed for secure aggregation, and does security come with a
fundamental cost in communication?
</p>
<p>In this paper, we develop an information-theoretic model for secure
aggregation that allows us to characterize the fundamental cost of security and
privacy in terms of communication. We show that with security (and without
privacy) $\Omega\left( n \log d \right)$ bits per user are necessary and
sufficient to allow the server to compute the frequency distribution. This is
significantly smaller than the $d\log n$ bits per user needed by the naive
scheme, but significantly higher than the $\log d$ bits per user needed without
security. To achieve differential privacy, we construct a linear scheme based
on a noisy sketch which locally perturbs the data and does not require a
trusted server (a.k.a. distributed differential privacy). We analyze this
scheme under $\ell_2$ and $\ell_\infty$ loss. By using our
information-theoretic framework, we show that the scheme achieves the optimal
accuracy-privacy trade-off with optimal communication cost, while matching the
performance in the centralized case where data is stored in the central server.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T01:30:00Z">Monday, November 21 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10137'>Identifying Correlation in Stream of Samples</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Zhenhao Gu, Hao Zhang</p><p>Identifying independence between two random variables or correlated given
their samples has been a fundamental problem in Statistics. However, how to do
so in a space-efficient way if the number of states is large is not quite
well-studied.
</p>
<p>We propose a new, simple counter matrix algorithm, which utilize hash
functions and a compressed counter matrix to give an unbiased estimate of the
$\ell_2$ independence metric. With $\mathcal{O}(\epsilon^{-4}\log\delta^{-1})$
(very loose bound) space, we can guarantee $1\pm\epsilon$ multiplicative error
with probability at least $1-\delta$. We also provide a comparison of our
algorithm with the state-of-the-art sketching of sketches algorithm and show
that our algorithm is effective, and actually faster and at least 2 times more
space-efficient.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1">Zhenhao Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hao Zhang</a></p><p>Identifying independence between two random variables or correlated given
their samples has been a fundamental problem in Statistics. However, how to do
so in a space-efficient way if the number of states is large is not quite
well-studied.
</p>
<p>We propose a new, simple counter matrix algorithm, which utilize hash
functions and a compressed counter matrix to give an unbiased estimate of the
$\ell_2$ independence metric. With $\mathcal{O}(\epsilon^{-4}\log\delta^{-1})$
(very loose bound) space, we can guarantee $1\pm\epsilon$ multiplicative error
with probability at least $1-\delta$. We also provide a comparison of our
algorithm with the state-of-the-art sketching of sketches algorithm and show
that our algorithm is effective, and actually faster and at least 2 times more
space-efficient.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T01:30:00Z">Monday, November 21 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.10398'>Improved Approximations for Unrelated Machine Scheduling</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sungjin Im, Shi Li</p><p>We revisit two well-studied scheduling problems in the unrelated machines
setting where each job can have a different processing time on each machine.
For minimizing total weighted completion time we give a 1.45-approximation,
which improves upon the previous 1.488-approximation [Im and Shadloo SODA
2020]. The key technical ingredient in this improvement lies in a new rounding
scheme that gives strong negative correlation with less restrictions. For
minimizing $L_k$-norms of machine loads, inspired by [Kalaitzis et al. SODA
2017], we give better approximation algorithms. In particular we give a $\sqrt
{4/3}$-approximation for the $L_2$-norm which improves upon the former $\sqrt
2$-approximations due to [Azar-Epstein STOC 2005] and [Kumar et al. JACM 2009].
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1">Sungjin Im</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shi Li</a></p><p>We revisit two well-studied scheduling problems in the unrelated machines
setting where each job can have a different processing time on each machine.
For minimizing total weighted completion time we give a 1.45-approximation,
which improves upon the previous 1.488-approximation [Im and Shadloo SODA
2020]. The key technical ingredient in this improvement lies in a new rounding
scheme that gives strong negative correlation with less restrictions. For
minimizing $L_k$-norms of machine loads, inspired by [Kalaitzis et al. SODA
2017], we give better approximation algorithms. In particular we give a $\sqrt
{4/3}$-approximation for the $L_2$-norm which improves upon the former $\sqrt
2$-approximations due to [Azar-Epstein STOC 2005] and [Kumar et al. JACM 2009].
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-21T01:30:00Z">Monday, November 21 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Sunday, November 20
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=6821'>Reform AI Alignment</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Update (Nov. 22): Theoretical computer scientist and longtime friend-of-the-blog Boaz Barak writes to tell me that, coincidentally, he and Ben Edelman just released a big essay advocating a version of &#8220;Reform AI Alignment&#8221; on Boaz&#8217;s Windows on Theory blog, as well as on LessWrong. (I warned Boaz that, having taken the momentous step of posting [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p><strong><mark style="background-color:rgba(0, 0, 0, 0)" class="has-inline-color has-vivid-red-color">Update (Nov. 22):</mark></strong> Theoretical computer scientist and longtime friend-of-the-blog <a href="https://www.boazbarak.org/">Boaz Barak</a> writes to tell me that, coincidentally, he and Ben Edelman just released a big essay advocating a version of &#8220;Reform AI Alignment&#8221; <a href="https://windowsontheory.org/2022/11/22/ai-will-change-the-world-but-wont-take-it-over-by-playing-3-dimensional-chess/">on Boaz&#8217;s Windows on Theory blog</a>, <a href="https://www.lesswrong.com/posts/zB3ukZJqt3pQDw9jz/ai-will-change-the-world-but-won-t-take-it-over-by-playing-3">as well as on LessWrong</a>.  (I warned Boaz that, having taken the momentous step of posting to LessWrong, in 6 months he should expect to find himself living in a rationalist group house in Oakland&#8230;)  Needless to say, I don&#8217;t necessarily endorse their every word or vice versa, but there&#8217;s a striking amount of convergence.  They also have a much more detailed discussion of (e.g.) which kinds of optimization processes they consider relatively safe.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Nearly halfway into my year at OpenAI, still reeling from the FTX collapse, I feel like it&#8217;s finally time to start blogging my AI safety thoughts&#8212;starting with a little appetizer course today, more substantial fare to come.</p>



<p>Many people claim that&nbsp;AI&nbsp;alignment is little more a modern eschatological religion&#8212;with prophets, an end-times prophecy, sacred scriptures, and even a god (albeit, one who doesn&#8217;t exist quite yet).  The obvious response to that claim is that, while there&#8217;s some truth to it, &#8220;religions&#8221; based around technology are a little different from the old kind, because technological progress <em>actually happens</em> regardless of whether you believe in it.</p>



<p>I mean, the Internet is sort of like the old concept of the collective unconscious, except that it actually exists and you&#8217;re using it right now.  Airplanes and spacecraft are kind of like the ancient dream of Icarus&#8212;except, again, for the actually existing part.  Today GPT-3 and DALL-E2 and LaMDA and AlphaTensor exist, as they didn&#8217;t two years ago, and one has to try to project forward to what their vastly-larger successors will be doing a decade from now.  Though some of my colleagues are still in denial about it, I regard the fact that such systems will have transformative effects on civilization, comparable to or greater than those of the Internet itself, as &#8220;already baked in&#8221;&#8212;as just the mainstream position, not even a question anymore.  That doesn&#8217;t mean that future AIs are going to convert the earth into paperclips, or give us eternal life in a simulated utopia.  But their story <em>will</em> be a central part of the story of this century.</p>



<p>Which brings me to a second response.  If&nbsp;AI&nbsp;alignment is a religion, itâs now large and established enough to have a thriving &#8220;Reform&#8221; branch, in addition to the original &#8220;Orthodox&#8221; branch epitomized by Eliezer Yudkowsky and <a href="https://intelligence.org/">MIRI</a>.&nbsp; As far as I can tell, this Reform branch now counts among its members a large fraction of the&nbsp;AI&nbsp;safety&nbsp;researchers now working in academia and industry. &nbsp;(Iâll leave the formation of a Conservative branch of&nbsp;AI&nbsp;alignment, which reacts against the Reform branch by moving <em>slightly</em> back in the direction of the Orthodox branch, as a problem for the future â to say nothing of Reconstructionist or Marxist branches.)</p>



<p>Hereâs an incomplete but hopefully representative list of the differences in doctrine between Orthodox and Reform AI Risk:</p>



<p>(1) Orthodox&nbsp;AI-riskers tend to believe that humanity will survive or be destroyed based on the actions of a few elite engineers over the next decade or two.&nbsp; Everything else&#8212;climate change, droughts, the future of US democracy, war over Ukraine and maybe Taiwan&#8212;fades into insignificance except insofar as it affects those engineers.</p>



<p>We Reform&nbsp;AI-riskers, by contrast, believe that&nbsp;AI&nbsp;might well pose civilizational risks in the coming century, but so does all the other stuff, and it&#8217;s all tied together.&nbsp; An invasion of Taiwan might change which world power gets access to TSMC GPUs.&nbsp; Almost everything affects which entities pursue the&nbsp;AI&nbsp;scaling frontier and whether they&#8217;re cooperating or competing to be first.</p>



<p>(2) Orthodox&nbsp;AI-riskers believe that public outreach has limited value: most people can&#8217;t understand this issue anyway, and will need to be saved from&nbsp;AI&nbsp;despite themselves.</p>



<p>We Reform&nbsp;AI-riskers believe that trying to get a broad swath of the public on board with one&#8217;s preferred&nbsp;AI&nbsp;policy is something close to a deontological imperative.</p>



<p>(3) Orthodox&nbsp;AI-riskers worry almost entirely about an agentic, misaligned&nbsp;AI&nbsp;that deceives humans while it works to destroy them, along the way to maximizing its strange utility function.</p>



<p>We Reform&nbsp;AI-riskers entertain that possibility, but we worry at least as much about powerful AIs that are weaponized by bad humans, which we expect to pose existential risks much earlier in any case.</p>



<p>(4) Orthodox&nbsp;AI-riskers have limited interest in&nbsp;AI&nbsp;safety&nbsp;research applicable to actually-existing systems (LaMDA, GPT-3, DALL-E2, etc.), seeing the dangers posed by those systems as basically trivial compared to the looming danger of a misaligned agentic&nbsp;AI.</p>



<p>We Reform&nbsp;AI-riskers see research on actually-existing systems as one of the only ways to get feedback from the world about which&nbsp;AI&nbsp;safety&nbsp;ideas are or aren&#8217;t promising.</p>



<p>(5) Orthodox&nbsp;AI-riskers worry most about the &#8220;FOOM&#8221; scenario, where some&nbsp;AI&nbsp;might cross a threshold from innocuous-looking to plotting to kill all humans in the space of hours or days.</p>



<p>We Reform&nbsp;AI-riskers worry most about the &#8220;slow-moving trainwreck&#8221; scenario, where (just like with climate change) well-informed people can see the writing on the wall decades ahead, but just can&#8217;t line up everyone&#8217;s incentives to prevent it.</p>



<p>(6) Orthodox&nbsp;AI-riskers talk a lot about a &#8220;pivotal act&#8221; to prevent a misaligned&nbsp;AI&nbsp;from ever being developed, which might involve (e.g.) using an aligned&nbsp;AI&nbsp;to impose a worldwide surveillance regime.</p>



<p>We Reform&nbsp;AI-riskers worry more about such an act causing the very calamity that it was intended to prevent.</p>



<p>(7) Orthodox&nbsp;AI-riskers feel a strong need to repudiate the norms of mainstream science, seeing them as too slow-moving to react in time to the existential danger of&nbsp;AI.</p>



<p>We Reform&nbsp;AI-riskers feel a strong need to get mainstream science on board with the&nbsp;AI&nbsp;safety&nbsp;program.</p>



<p>(8) Orthodox&nbsp;AI-riskers are maximalists about the power of pure, unaided superintelligence to just figure out how to commandeer whatever physical resources it needs to take over the world (for example, by messaging some lab over the Internet, and tricking it into manufacturing nanobots that will do the superintelligence&#8217;s bidding).</p>



<p>We Reform&nbsp;AI-riskers believe that, here just like in high school, there are limits to the power of pure&nbsp;intelligence&nbsp;to achieve one&#8217;s goals.&nbsp; We&#8217;d expect even an agentic, misaligned&nbsp;AI, if such existed, to need a stable power source, robust interfaces to the physical world, and probably allied humans before it posed much of an existential threat.</p>



<p>What have I missed?</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-20T20:44:33Z">Sunday, November 20 2022, 20:44</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/164'>TR22-164 |  Learning versus Pseudorandom Generators in Constant Parallel Time | 

	Shuichi Hirahara, 

	Mikito Nanashima</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          A polynomial-stretch pseudorandom generator (PPRG) in NC$^0$ (i.e., constant parallel time) is one of the most important cryptographic primitives, especially for constructing highly efficient cryptography and indistinguishability obfuscation. The celebrated work (Applebaum, Ishai, and Kushilevitz, SIAM Journal on Computing, 2006) on randomized encodings yields the characterization of sublinear-stretch pseudorandom generators in NC$^0$ by the existence of logspace-computable one-way functions, but characterizing PPRGs in NC$^0$ seems out of reach at present. Therefore, it is natural to ask which sort of hardness notion is essential for constructing PPRGs in NC$^0$. Particularly, to the best of our knowledge, all the previously known candidates for PPRGs in NC$^0$ follow only one framework based on Goldreich&#39;s one-way function. 
		
In this paper, we present a new learning-theoretic characterization for PPRGs in NC$^0$ and related classes. Specifically, we consider the average-case hardness of learning for well-studied classes in parameterized settings, where the number of samples is restricted to fixed-parameter tractable (FPT), and show that the following are equivalent:
	(i) The existence of (a collection of) PPRGs in NC$^0$.
	(ii) The average-case hardness of learning sparse $\mathbb{F}_2$-polynomials on a sparse example distribution and an NC$^0$-samplable target distribution (i.e., a distribution on target functions).
	(iii) The average-case hardness of learning Fourier-sparse functions on a sparse example distribution and an NC$^0$-samplable target distribution.
	(iv) The average-case hardness of learning constant-depth parity decision trees on a sparse example distribution and an NC$^0$-samplable target distribution.
Furthermore, we characterize a (single) PPRG in $\oplus$-NC$^0$ by the average-case hardness of learning constant-degree $\mathbb{F}_2$-polynomials on a uniform example distribution with FPT samples. Based on our results, we propose new candidates for PPRGs in NC$^0$ and related classes under a hardness assumption on a natural learning problem. An important property of PPRGs in NC$^0$ constructed in our framework is that the output bits are computed by various predicates; thus, it seems to resist an attack that depends on a specific property of one fixed predicate.
	
Conceptually, the main contribution of this study is to formalize a theory of FPT dualization of concept classes, which yields a meta-theorem for the first result. For the second result on PPRGs in $\oplus$-NC$^0$, we use a different technique of pseudorandom $\mathbb{F}_2$-polynomials.
        
        </div>

        <div class='tr-article-summary'>
        
          
          A polynomial-stretch pseudorandom generator (PPRG) in NC$^0$ (i.e., constant parallel time) is one of the most important cryptographic primitives, especially for constructing highly efficient cryptography and indistinguishability obfuscation. The celebrated work (Applebaum, Ishai, and Kushilevitz, SIAM Journal on Computing, 2006) on randomized encodings yields the characterization of sublinear-stretch pseudorandom generators in NC$^0$ by the existence of logspace-computable one-way functions, but characterizing PPRGs in NC$^0$ seems out of reach at present. Therefore, it is natural to ask which sort of hardness notion is essential for constructing PPRGs in NC$^0$. Particularly, to the best of our knowledge, all the previously known candidates for PPRGs in NC$^0$ follow only one framework based on Goldreich&#39;s one-way function. 
		
In this paper, we present a new learning-theoretic characterization for PPRGs in NC$^0$ and related classes. Specifically, we consider the average-case hardness of learning for well-studied classes in parameterized settings, where the number of samples is restricted to fixed-parameter tractable (FPT), and show that the following are equivalent:
	(i) The existence of (a collection of) PPRGs in NC$^0$.
	(ii) The average-case hardness of learning sparse $\mathbb{F}_2$-polynomials on a sparse example distribution and an NC$^0$-samplable target distribution (i.e., a distribution on target functions).
	(iii) The average-case hardness of learning Fourier-sparse functions on a sparse example distribution and an NC$^0$-samplable target distribution.
	(iv) The average-case hardness of learning constant-depth parity decision trees on a sparse example distribution and an NC$^0$-samplable target distribution.
Furthermore, we characterize a (single) PPRG in $\oplus$-NC$^0$ by the average-case hardness of learning constant-degree $\mathbb{F}_2$-polynomials on a uniform example distribution with FPT samples. Based on our results, we propose new candidates for PPRGs in NC$^0$ and related classes under a hardness assumption on a natural learning problem. An important property of PPRGs in NC$^0$ constructed in our framework is that the output bits are computed by various predicates; thus, it seems to resist an attack that depends on a specific property of one fixed predicate.
	
Conceptually, the main contribution of this study is to formalize a theory of FPT dualization of concept classes, which yields a meta-theorem for the first result. For the second result on PPRGs in $\oplus$-NC$^0$, we use a different technique of pseudorandom $\mathbb{F}_2$-polynomials.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-20T11:26:40Z">Sunday, November 20 2022, 11:26</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/163'>TR22-163 |  Random Walks on Rotating Expanders | 

	Gil Cohen, 

	Gal Maor</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Random walks on expanders are a powerful tool which found applications in many areas of theoretical computer science, and beyond. However, they come with an inherent cost -- the spectral expansion of the corresponding power graph deteriorates at a rate that is exponential in the length of the walk. As an example, when $G$ is a $d$-regular Ramanujan graph, the power graph $G^t$ has spectral expansion $2^{\Omega(t)} \sqrt{D}$, where $D = d^t$ is the regularity of $G^t$, thus, $G^t$ is $2^{\Omega(t)}$ away from being Ramanujan. This exponential blowup manifests itself in many applications.

In this work we bypass this barrier by permuting the vertices of the given graph after each random step. We prove that there exists a sequence of permutations for which the spectral expansion deteriorates by only a linear factor in $t$. In the Ramanujan case this yields an expansion of $O(t \sqrt{D})$. We stress that the permutations are tailor-made to the graph at hand and require no randomness to generate.

Our proof, which holds for all sufficiently high girth graphs, makes heavy use of the powerful framework of finite free probability and interlacing families that was developed in a seminal sequence of works by Marcus, Spielman and Srivastava.
        
        </div>

        <div class='tr-article-summary'>
        
          
          Random walks on expanders are a powerful tool which found applications in many areas of theoretical computer science, and beyond. However, they come with an inherent cost -- the spectral expansion of the corresponding power graph deteriorates at a rate that is exponential in the length of the walk. As an example, when $G$ is a $d$-regular Ramanujan graph, the power graph $G^t$ has spectral expansion $2^{\Omega(t)} \sqrt{D}$, where $D = d^t$ is the regularity of $G^t$, thus, $G^t$ is $2^{\Omega(t)}$ away from being Ramanujan. This exponential blowup manifests itself in many applications.

In this work we bypass this barrier by permuting the vertices of the given graph after each random step. We prove that there exists a sequence of permutations for which the spectral expansion deteriorates by only a linear factor in $t$. In the Ramanujan case this yields an expansion of $O(t \sqrt{D})$. We stress that the permutations are tailor-made to the graph at hand and require no randomness to generate.

Our proof, which holds for all sufficiently high girth graphs, makes heavy use of the powerful framework of finite free probability and interlacing families that was developed in a seminal sequence of works by Marcus, Spielman and Srivastava.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-20T09:05:18Z">Sunday, November 20 2022, 09:05</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://decentralizedthoughts.github.io/2022-11-20-pbft-via-locked-braodcast/'>On PBFT from Locked Broadcast</a></h3>
        <p class='tr-article-feed'>from <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We describe a variation of the authenticated version of PBFT using Locked Broadcast that follows a similar path as our previous post on Paxos using Recoverable Broadcast. I call this protocol linear PBFT and variants of it are used by SBFT and Tusk. A later post will show how to...
        
        </div>

        <div class='tr-article-summary'>
        
          
          We describe a variation of the authenticated version of PBFT using Locked Broadcast that follows a similar path as our previous post on Paxos using Recoverable Broadcast. I call this protocol linear PBFT and variants of it are used by SBFT and Tusk. A later post will show how to...
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-20T09:00:00Z">Sunday, November 20 2022, 09:00</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/162'>TR22-162 |  Directed Isoperimetric Theorems for Boolean Functions on the Hypergrid and an $\widetilde{O}(n\sqrt{d})$ Monotonicity Tester | 

	Hadley Black, 

	Deeparnab Chakrabarty, 

	C. Seshadhri</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The problem of testing monotonicity for Boolean functions on the hypergrid, $f:[n]^d \to \{0,1\}$ is a classic topic in property testing. When $n=2$, the domain is the hypercube. For the hypercube case, a breakthrough result of Khot-Minzer-Safra (FOCS 2015) gave a non-adaptive, one-sided tester making $\widetilde{O}(\varepsilon^{-2}\sqrt{d})$ queries. Up to polylog $d$ and $\varepsilon$ factors, this bound matches the $\widetilde{\Omega}(\sqrt{d})$-query non-adaptive lower bound (Chen-De-Servedio-Tan (STOC 2015), Chen-Waingarten-Xie (STOC 2017)). For any $n &gt; 2$, the optimal non-adaptive complexity was unknown. A previous result of the authors achieves a $\widetilde{O}(d^{5/6})$-query upper bound (SODA 2020), quite far from the $\sqrt{d}$ bound for the hypercube.

In this paper, we resolve the non-adaptive complexity of monotonicity testing for all constant $n$, up to $\text{poly}(\varepsilon^{-1}\log d)$ factors. Specifically, we give a non-adaptive, one-sided monotonicity tester making $\widetilde{O}(\varepsilon^{-2}n\sqrt{d})$ queries. From a technical standpoint, we prove new directed isoperimetric theorems over the hypergrid $[n]^d$. These results generalize the celebrated directed Talagrand inequalities that were only known for the hypercube.
        
        </div>

        <div class='tr-article-summary'>
        
          
          The problem of testing monotonicity for Boolean functions on the hypergrid, $f:[n]^d \to \{0,1\}$ is a classic topic in property testing. When $n=2$, the domain is the hypercube. For the hypercube case, a breakthrough result of Khot-Minzer-Safra (FOCS 2015) gave a non-adaptive, one-sided tester making $\widetilde{O}(\varepsilon^{-2}\sqrt{d})$ queries. Up to polylog $d$ and $\varepsilon$ factors, this bound matches the $\widetilde{\Omega}(\sqrt{d})$-query non-adaptive lower bound (Chen-De-Servedio-Tan (STOC 2015), Chen-Waingarten-Xie (STOC 2017)). For any $n &gt; 2$, the optimal non-adaptive complexity was unknown. A previous result of the authors achieves a $\widetilde{O}(d^{5/6})$-query upper bound (SODA 2020), quite far from the $\sqrt{d}$ bound for the hypercube.

In this paper, we resolve the non-adaptive complexity of monotonicity testing for all constant $n$, up to $\text{poly}(\varepsilon^{-1}\log d)$ factors. Specifically, we give a non-adaptive, one-sided monotonicity tester making $\widetilde{O}(\varepsilon^{-2}n\sqrt{d})$ queries. From a technical standpoint, we prove new directed isoperimetric theorems over the hypergrid $[n]^d$. These results generalize the celebrated directed Talagrand inequalities that were only known for the hypercube.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-20T05:41:10Z">Sunday, November 20 2022, 05:41</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/161'>TR22-161 |  Towards Multi-Pass Streaming Lower Bounds for Optimal Approximation of Max-Cut | 

	Raghuvansh Saxena, 

	Lijie Chen, 

	Gillat Kol, 

	Dmitry Paramonov, 

	Zhao Song, 

	Huacheng Yu</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We consider the Max-Cut problem, asking how much space is needed by a streaming algorithm in order to estimate the value of the maximum cut in a graph. This problem has been extensively studied over the last decade, and we now have a near-optimal lower bound for one-pass streaming algorithms, showing that they require linear space to guarantee a better-than-$2$ approximation [KKS15, KK19]. The result relies on a lower bound for the cycle-finding problem, showing that it is hard for a one-pass streaming algorithm to find a cycle in a union of matchings.

The end-goal of our research is to prove a similar lower for multi-pass streaming algorithms that guarantee a better-than-$2$ approximation for Max-Cut, a highly challenging open problem. In this paper, we take a significant step in this direction, showing that even $o(\log n)$-pass streaming algorithms need $n^{\Omega(1)}$ space to solve the cycle-finding problem. Our proof is quite involved, dividing the cycles in the graph into &quot;short&quot; and &quot;long&quot; cycles, and using tailor-made lower bound techniques to handle each case.
        
        </div>

        <div class='tr-article-summary'>
        
          
          We consider the Max-Cut problem, asking how much space is needed by a streaming algorithm in order to estimate the value of the maximum cut in a graph. This problem has been extensively studied over the last decade, and we now have a near-optimal lower bound for one-pass streaming algorithms, showing that they require linear space to guarantee a better-than-$2$ approximation [KKS15, KK19]. The result relies on a lower bound for the cycle-finding problem, showing that it is hard for a one-pass streaming algorithm to find a cycle in a union of matchings.

The end-goal of our research is to prove a similar lower for multi-pass streaming algorithms that guarantee a better-than-$2$ approximation for Max-Cut, a highly challenging open problem. In this paper, we take a significant step in this direction, showing that even $o(\log n)$-pass streaming algorithms need $n^{\Omega(1)}$ space to solve the cycle-finding problem. Our proof is quite involved, dividing the cycles in the graph into &quot;short&quot; and &quot;long&quot; cycles, and using tailor-made lower bound techniques to handle each case.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-20T05:39:49Z">Sunday, November 20 2022, 05:39</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/160'>TR22-160 |  The Geometry of Rounding | 

	Jason Vander Woude, 

	Peter Dixon, 

	A.  Pavan, 

	Jamie Radcliffe, 

	N. V. Vinodchandran</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Rounding has proven to be a fundamental tool in theoretical computer science. By observing that rounding and partitioning of $\mathbb{R}^d$ are equivalent, we introduce the following natural partition problem which we call the secluded hypercube partition problem: Given $k\in\mathbb{N}$ (ideally small) and $\epsilon&gt;0$ (ideally large), is there a partition of $\mathbb{R}^d$ with unit hypercubes such that for every point $\vec{p} \in \mathbb{R}^d$, its closed $\epsilon$-neighborhood (in the $\ell_{\infty}$  norm) intersects at most $k$ hypercubes?

We undertake a comprehensive study of this partition problem. We prove that for every $d\in\mathbb{N}$, there is an explicit (and efficiently computable) hypercube partition of $\mathbb{R}^d$ with $k = d+1$ and $\epsilon = \frac{1}{2d}$. We complement this construction by proving that the value of $k=d+1$ is the best possible (for any $\epsilon$) for a broad class of &quot;reasonable&quot; partitions including hypercube partitions. We also investigate the optimality of the parameter $\epsilon$ and prove that any partition in this broad class that has $k=d+1$, must have $\epsilon\leq\frac{1}{2\sqrt{d}}$. These bounds imply limitations of certain deterministic rounding schemes existing in the literature. Furthermore, this general bound is based on the currently known lower bounds for the dissection number of the cube, and improvements to this bound will yield improvements to our bounds.

While our work is motivated by the desire to understand rounding algorithms, one of our main conceptual contributions is the introduction of the secluded hypercube partition problem, which fits well with a long history of investigations by mathematicians on various hypercube partitions/tilings of Euclidean  space.
        
        </div>

        <div class='tr-article-summary'>
        
          
          Rounding has proven to be a fundamental tool in theoretical computer science. By observing that rounding and partitioning of $\mathbb{R}^d$ are equivalent, we introduce the following natural partition problem which we call the secluded hypercube partition problem: Given $k\in\mathbb{N}$ (ideally small) and $\epsilon&gt;0$ (ideally large), is there a partition of $\mathbb{R}^d$ with unit hypercubes such that for every point $\vec{p} \in \mathbb{R}^d$, its closed $\epsilon$-neighborhood (in the $\ell_{\infty}$  norm) intersects at most $k$ hypercubes?

We undertake a comprehensive study of this partition problem. We prove that for every $d\in\mathbb{N}$, there is an explicit (and efficiently computable) hypercube partition of $\mathbb{R}^d$ with $k = d+1$ and $\epsilon = \frac{1}{2d}$. We complement this construction by proving that the value of $k=d+1$ is the best possible (for any $\epsilon$) for a broad class of &quot;reasonable&quot; partitions including hypercube partitions. We also investigate the optimality of the parameter $\epsilon$ and prove that any partition in this broad class that has $k=d+1$, must have $\epsilon\leq\frac{1}{2\sqrt{d}}$. These bounds imply limitations of certain deterministic rounding schemes existing in the literature. Furthermore, this general bound is based on the currently known lower bounds for the dissection number of the cube, and improvements to this bound will yield improvements to our bounds.

While our work is motivated by the desire to understand rounding algorithms, one of our main conceptual contributions is the introduction of the secluded hypercube partition problem, which fits well with a long history of investigations by mathematicians on various hypercube partitions/tilings of Euclidean  space.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-20T05:38:52Z">Sunday, November 20 2022, 05:38</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
