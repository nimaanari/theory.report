<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.6 (2023-03-30) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2023-04-19T07:30:41Z">Wednesday, April 19 2023, 07:30</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://minorfree.github.io/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://minorfree.github.io'>Hung Le</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>Sébastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Wednesday, April 19
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://rjlipton.wpcomstaging.com/2023/04/18/an-award-for-ellen-zegura/'>An Award For Ellen Zegura</a></h3>
        <p class='tr-article-feed'>from <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Ellen Zegura was just honored with the Class of 1934 Distinguished Professor Award. The Class of 1934 Distinguished Professor Award recognizes outstanding achievement in teaching, research, and service. It is the highest honor given to a Georgia Tech professor. Created in 1984 by the Class of 1934 in observance of its 50th reunion, the award [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>
Ellen Zegura was just honored with the Class of 1934 Distinguished Professor <a href="https://news.gatech.edu/news/2023/04/13/ellen-zegura-honored-class-1934-distinguished-professor-award">Award</a>. The Class of 1934 Distinguished Professor Award recognizes outstanding achievement in teaching, research, and service. It is the highest honor given to a Georgia Tech professor. </p>
<p><P><br />
<a href="https://rjlipton.wpcomstaging.com/2023/04/18/an-award-for-ellen-zegura/ez/" rel="attachment wp-att-21482"><img data-attachment-id="21482" data-permalink="https://rjlipton.wpcomstaging.com/2023/04/18/an-award-for-ellen-zegura/ez/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/ez.jpeg?fit=248%2C225&amp;ssl=1" data-orig-size="248,225" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="ez" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/ez.jpeg?fit=248%2C225&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/ez.jpeg?fit=248%2C225&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/ez.jpeg?resize=200%2C180&#038;ssl=1" alt="" width="200" height="180" class="aligncenter wp-image-21482" data-recalc-dims="1" /></a></p>
<p><P><br />
Created in 1984 by the Class of 1934 in observance of its 50th reunion, the award is presented to an active professor who has made significant, long-term contributions&#8212;contributions that have brought widespread recognition to the professor, to their school, and to the Institute. The GaTech <a href="https://news.gatech.edu/news/2023/04/13/ellen-zegura-honored-class-1934-distinguished-professor-award">story</a> on Ellen has many delightful personal details touching all aspects of her vocation.</p>
<p>
Two other people in our line have been so honored recently: <a href="https://www.cc.gatech.edu/people/james-foley">Jim Foley</a> from computing and <a href="https://news.gatech.edu/news/2016/04/11/thomas-earns-top-faculty-honor">Robin Thomas</a> from mathematics. As a past Tech professor I knew both of them well. I&#8217;ve also known other past winners of this great award. Congrats to Ellen. </p>
<p>
<p><H2> Voting Too </H2></p>
<p><p>
Zegura is getting this award for many reasons, but one prominent reason is her research with two other top Georgia Tech professors&#8212;Michael Best and Rich DeMillo on voting safety. Best is a professor of international affairs and interactive computing at Georgia Tech and has worked globally on election systems monitoring for more than 15 years. DeMillo is the chair of the School of Cybersecurity and Privacy at Georgia Tech and has also worked on voting for years. </p>
<p><P><br />
<a href="https://rjlipton.wpcomstaging.com/2023/04/18/an-award-for-ellen-zegura/trio/" rel="attachment wp-att-21483"><img data-attachment-id="21483" data-permalink="https://rjlipton.wpcomstaging.com/2023/04/18/an-award-for-ellen-zegura/trio/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/Trio.jpg?fit=605%2C219&amp;ssl=1" data-orig-size="605,219" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="Trio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/Trio.jpg?fit=300%2C109&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/Trio.jpg?fit=600%2C217&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/Trio.jpg?resize=550%2C200&#038;ssl=1" alt="" width="550" height="200" class="aligncenter wp-image-21483" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/Trio.jpg?w=605&amp;ssl=1 605w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/Trio.jpg?resize=300%2C109&amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/Trio.jpg?resize=600%2C219&amp;ssl=1 600w" sizes="(max-width: 550px) 100vw, 550px" data-recalc-dims="1" /></a></p>
<p><P><br />
The trio wrote an opinion piece on <a href="https://www.ajc.com/opinion/opinion-ga-voting-processes-should-be-both-secure-and-usable/H2N2RAET65GGTEPSJDS67ORXGQ/">elections</a> in the Atlanta Journal-Constitution newspaper. Also see <a href="https://scs.gatech.edu/news/640727/georgia-techs-secure-and-safe-elections-research-group-provide-live-wait-times-fulton">this</a> for comments by Tech&#8217;s Secure and Safe Elections Research Group. </p>
<p>
Here are some of the papers this was based on&#8212;Rich was a co-author of the first, Ellen and Michael on the others: </p>
<ul>
<li>
<a href="https://oar.princeton.edu/bitstream/88435/pr1qj9r/1/BallotMarkingDeviceVoters.pdf">Ballot-Marking Devices</a>&#8212;DeMillo </p>
<li>
<a href="https://dl.acm.org/doi/abs/10.1145/2909609.2909623">A First Look at &#8220;Eyes on the Vote&#8221;</a>&#8212;Best and Zegura </p>
<li>
<a href="https://dl.acm.org/doi/abs/10.1145/2909609.2909640">Lessons in Social Election Monitoring</a>&#8212;Best and Zegura
</ul>
<p><H2> Open Problems </H2></p>
<p><p>
Again congrats to Ellen on being honored. </p>
<p>
<p class="authors">By rjlipton</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-19T03:08:19Z">Wednesday, April 19 2023, 03:08</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.08615'>Revisiting Block-Diagonal SDP Relaxations for the Clique Number of the Paley Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Vladimir A. Kobzar, Krishnan Mody</p><p>This work addresses the block-diagonal semidefinite program (SDP) relaxations
for the clique number of the Paley graphs. The size of the maximal clique
(clique number) of a graph is a classic NP-complete problem; a Paley graph is a
deterministic graph where two vertices are connected if their difference is a
quadratic residue modulo certain prime powers. Improving the upper bound for
the Paley graph clique number for odd prime powers is an open problem in
combinatorics. Moreover, since quadratic residues exhibit pseudorandom
properties, Paley graphs are related to the construction of deterministic
restricted isometries, an open problem in compressed sensing and sparse
recovery. Recent work provides evidence that the current upper bounds can be
improved by the sum-of-squares (SOS) relaxations. In particular the bounds
given by the SOS relaxations of degree 4 (SOS-4) are asymptotically growing at
an order smaller than square root of the prime. However computations of SOS-4
become intractable with respect to large graphs. Gvozdenovic et al. introduced
a more computationally efficient block-diagonal hierarchy of SDPs that refines
the SOS hierarchy. They computed the values of these SDPs of degrees 2 and 3
(L2 and L3 respectively) for the Paley graph clique numbers associated with
primes p less or equal to 809. These values bound from the above the values of
the corresponding SOS-4 and SOS-6 relaxations respectively. We revisit these
computations and determine the values of the L2 relaxation for larger p's. Our
results provide additional numerical evidence that the L2 relaxations, and
therefore also the SOS-4 relaxations, are asymptotically growing at an order
smaller than the square root of p.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kobzar_V/0/1/0/all/0/1">Vladimir A. Kobzar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mody_K/0/1/0/all/0/1">Krishnan Mody</a></p><p>This work addresses the block-diagonal semidefinite program (SDP) relaxations
for the clique number of the Paley graphs. The size of the maximal clique
(clique number) of a graph is a classic NP-complete problem; a Paley graph is a
deterministic graph where two vertices are connected if their difference is a
quadratic residue modulo certain prime powers. Improving the upper bound for
the Paley graph clique number for odd prime powers is an open problem in
combinatorics. Moreover, since quadratic residues exhibit pseudorandom
properties, Paley graphs are related to the construction of deterministic
restricted isometries, an open problem in compressed sensing and sparse
recovery. Recent work provides evidence that the current upper bounds can be
improved by the sum-of-squares (SOS) relaxations. In particular the bounds
given by the SOS relaxations of degree 4 (SOS-4) are asymptotically growing at
an order smaller than square root of the prime. However computations of SOS-4
become intractable with respect to large graphs. Gvozdenovic et al. introduced
a more computationally efficient block-diagonal hierarchy of SDPs that refines
the SOS hierarchy. They computed the values of these SDPs of degrees 2 and 3
(L2 and L3 respectively) for the Paley graph clique numbers associated with
primes p less or equal to 809. These values bound from the above the values of
the corresponding SOS-4 and SOS-6 relaxations respectively. We revisit these
computations and determine the values of the L2 relaxation for larger p's. Our
results provide additional numerical evidence that the L2 relaxations, and
therefore also the SOS-4 relaxations, are asymptotically growing at an order
smaller than the square root of p.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-19T00:30:00Z">Wednesday, April 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.08745'>Super-Logarithmic Lower Bounds for Dynamic Graph Problems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Kasper Green Larsen, Huacheng Yu</p><p>In this work, we prove a $\tilde{\Omega}(\lg^{3/2} n )$ unconditional lower
bound on the maximum of the query time and update time for dynamic data
structures supporting reachability queries in $n$-node directed acyclic graphs
under edge insertions. This is the first super-logarithmic lower bound for any
natural graph problem. In proving the lower bound, we also make novel
contributions to the state-of-the-art data structure lower bound techniques
that we hope may lead to further progress in proving lower bounds.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Larsen_K/0/1/0/all/0/1">Kasper Green Larsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Huacheng Yu</a></p><p>In this work, we prove a $\tilde{\Omega}(\lg^{3/2} n )$ unconditional lower
bound on the maximum of the query time and update time for dynamic data
structures supporting reachability queries in $n$-node directed acyclic graphs
under edge insertions. This is the first super-logarithmic lower bound for any
natural graph problem. In proving the lower bound, we also make novel
contributions to the state-of-the-art data structure lower bound techniques
that we hope may lead to further progress in proving lower bounds.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-19T00:30:00Z">Wednesday, April 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.08534'>Lossy Compressor preserving variant calling through Extended BWT</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Veronica Guerrini, Felipe A. Louza, Giovanna Rosone</p><p>A standard format used for storing the output of high-throughput sequencing
experiments is the FASTQ format. It comprises three main components: (i)
headers, (ii) bases (nucleotide sequences), and (iii) quality scores. FASTQ
files are widely used for variant calling, where sequencing data are mapped
into a reference genome to discover variants that may be used for further
analysis. There are many specialized compressors that exploit redundancy in
FASTQ data with the focus only on either the bases or the quality scores
components. In this paper we consider the novel problem of lossy compressing,
in a reference-free way, FASTQ data by modifying both components at the same
time, while preserving the important information of the original FASTQ. We
introduce a general strategy, based on the Extended Burrows-Wheeler Transform
(EBWT) and positional clustering, and we present implementations in both
internal memory and external memory. Experimental results show that the lossy
compression performed by our tool is able to achieve good compression while
preserving information relating to variant calling more than the competitors.
Availability: the software is freely available at
github.com/veronicaguerrini/BFQzip.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Guerrini_V/0/1/0/all/0/1">Veronica Guerrini</a>, <a href="http://arxiv.org/find/cs/1/au:+Louza_F/0/1/0/all/0/1">Felipe A. Louza</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosone_G/0/1/0/all/0/1">Giovanna Rosone</a></p><p>A standard format used for storing the output of high-throughput sequencing
experiments is the FASTQ format. It comprises three main components: (i)
headers, (ii) bases (nucleotide sequences), and (iii) quality scores. FASTQ
files are widely used for variant calling, where sequencing data are mapped
into a reference genome to discover variants that may be used for further
analysis. There are many specialized compressors that exploit redundancy in
FASTQ data with the focus only on either the bases or the quality scores
components. In this paper we consider the novel problem of lossy compressing,
in a reference-free way, FASTQ data by modifying both components at the same
time, while preserving the important information of the original FASTQ. We
introduce a general strategy, based on the Extended Burrows-Wheeler Transform
(EBWT) and positional clustering, and we present implementations in both
internal memory and external memory. Experimental results show that the lossy
compression performed by our tool is able to achieve good compression while
preserving information relating to variant calling more than the competitors.
Availability: the software is freely available at
https://github.com/veronicaguerrini/BFQzip.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-19T00:30:00Z">Wednesday, April 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.08567'>Traversing combinatorial 0/1-polytopes via optimization</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Arturo Merino, Torsten M&#xfc;tze</p><p>In this paper, we present a new framework that exploits combinatorial
optimization for efficiently generating a large variety of combinatorial
objects based on graphs, matroids, posets and polytopes. Our method relies on a
simple and versatile algorithm for computing a Hamilton path on the skeleton of
any 0/1-polytope ${\rm conv}(X)$, where $X\subseteq \{0,1\}^n$. The algorithm
uses as a black box any algorithm that solves a variant of the classical linear
optimization problem $\min\{w\cdot x\mid x\in X\}$, and the resulting delay,
i.e., the running time per visited vertex on the Hamilton path, is only by a
factor of $\log n$ larger than the running time of the optimization algorithm.
When $X$ encodes a particular class of combinatorial objects, then traversing
the skeleton of the polytope ${\rm conv}(X)$ along a Hamilton path corresponds
to listing the combinatorial objects by local change operations, i.e., we
obtain Gray code listings. As concrete results of our general framework, we
obtain efficient algorithms for generating all ($c$-optimal) bases in a
matroid; ($c$-optimal) spanning trees, forests, ($c$-optimal) matchings in a
general graph; ($c$-optimal) vertex covers, ($c$-optimal) stable sets in a
bipartite graph; as well as ($c$-optimal) antichains and ideals of a poset. The
delay and space required by these algorithms are polynomial in the size of the
matroid, graph, or poset, respectively, and these listings correspond to
Hamilton paths on the corresponding combinatorial polytopes. We also obtain an
$O(t_{\rm LP} \log n)$ delay algorithm for the vertex enumeration problem on
0/1-polytopes $\{x\in\mathbb{R}^n\mid Ax\leq b\}$, where $A\in
\mathbb{R}^{m\times n}$ and $b\in\mathbb{R}^m$, and $t_{\rm LP}$ is the time
needed to solve the linear program $\min\{w\cdot x\mid Ax\leq b\}$. This
improves upon the 25-year old $O(t_{\rm LP}\,n)$ delay algorithm of Bussieck
and L\"ubbecke.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Merino_A/0/1/0/all/0/1">Arturo Merino</a>, <a href="http://arxiv.org/find/cs/1/au:+Mutze_T/0/1/0/all/0/1">Torsten M&#xfc;tze</a></p><p>In this paper, we present a new framework that exploits combinatorial
optimization for efficiently generating a large variety of combinatorial
objects based on graphs, matroids, posets and polytopes. Our method relies on a
simple and versatile algorithm for computing a Hamilton path on the skeleton of
any 0/1-polytope ${\rm conv}(X)$, where $X\subseteq \{0,1\}^n$. The algorithm
uses as a black box any algorithm that solves a variant of the classical linear
optimization problem $\min\{w\cdot x\mid x\in X\}$, and the resulting delay,
i.e., the running time per visited vertex on the Hamilton path, is only by a
factor of $\log n$ larger than the running time of the optimization algorithm.
When $X$ encodes a particular class of combinatorial objects, then traversing
the skeleton of the polytope ${\rm conv}(X)$ along a Hamilton path corresponds
to listing the combinatorial objects by local change operations, i.e., we
obtain Gray code listings. As concrete results of our general framework, we
obtain efficient algorithms for generating all ($c$-optimal) bases in a
matroid; ($c$-optimal) spanning trees, forests, ($c$-optimal) matchings in a
general graph; ($c$-optimal) vertex covers, ($c$-optimal) stable sets in a
bipartite graph; as well as ($c$-optimal) antichains and ideals of a poset. The
delay and space required by these algorithms are polynomial in the size of the
matroid, graph, or poset, respectively, and these listings correspond to
Hamilton paths on the corresponding combinatorial polytopes. We also obtain an
$O(t_{\rm LP} \log n)$ delay algorithm for the vertex enumeration problem on
0/1-polytopes $\{x\in\mathbb{R}^n\mid Ax\leq b\}$, where $A\in
\mathbb{R}^{m\times n}$ and $b\in\mathbb{R}^m$, and $t_{\rm LP}$ is the time
needed to solve the linear program $\min\{w\cdot x\mid Ax\leq b\}$. This
improves upon the 25-year old $O(t_{\rm LP}\,n)$ delay algorithm of Bussieck
and L\"ubbecke.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-19T00:30:00Z">Wednesday, April 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.08581'>Graph Sparsification by Approximate Matrix Multiplication</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Neophytos Charalambides, Alfred O. Hero III</p><p>Graphs arising in statistical problems, signal processing, large networks,
combinatorial optimization, and data analysis are often dense, which causes
both computational and storage bottlenecks. One way of \textit{sparsifying} a
\textit{weighted} graph, while sharing the same vertices as the original graph
but reducing the number of edges, is through \textit{spectral sparsification}.
We study this problem through the perspective of RandNLA. Specifically, we
utilize randomized matrix multiplication to give a clean and simple analysis of
how sampling according to edge weights gives a spectral approximation to graph
Laplacians. Through the $CR$-MM algorithm, we attain a simple and
computationally efficient sparsifier whose resulting Laplacian estimate is
unbiased and of minimum variance. Furthermore, we define a new notion of
\textit{additive spectral sparsifiers}, which has not been considered in the
literature.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Charalambides_N/0/1/0/all/0/1">Neophytos Charalambides</a>, <a href="http://arxiv.org/find/math/1/au:+Hero_A/0/1/0/all/0/1">Alfred O. Hero III</a></p><p>Graphs arising in statistical problems, signal processing, large networks,
combinatorial optimization, and data analysis are often dense, which causes
both computational and storage bottlenecks. One way of \textit{sparsifying} a
\textit{weighted} graph, while sharing the same vertices as the original graph
but reducing the number of edges, is through \textit{spectral sparsification}.
We study this problem through the perspective of RandNLA. Specifically, we
utilize randomized matrix multiplication to give a clean and simple analysis of
how sampling according to edge weights gives a spectral approximation to graph
Laplacians. Through the $CR$-MM algorithm, we attain a simple and
computationally efficient sparsifier whose resulting Laplacian estimate is
unbiased and of minimum variance. Furthermore, we define a new notion of
\textit{additive spectral sparsifiers}, which has not been considered in the
literature.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-19T00:30:00Z">Wednesday, April 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.08632'>Subcubic algorithm for (Unweighted) Unrooted Tree Edit Distance</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Krzysztof Pi&#xf3;ro</p><p>The tree edit distance problem is a natural generalization of the classic
string edit distance problem. Given two ordered, edge-labeled trees $T_1$ and
$T_2$, the edit distance between $T_1$ and $T_2$ is defined as the minimum
total cost of operations that transform $T_1$ into $T_2$. In one operation, we
can contract an edge, split a vertex into two or change the label of an edge.
For the weighted version of the problem, where the cost of each operation
depends on the type of the operation and the label on the edge involved,
$\mathcal{O}(n^3)$ time algorithms are known for both rooted and unrooted
trees. The existence of a truly subcubic $\mathcal{O}(n^{3-\epsilon})$ time
algorithm is unlikely, as it would imply a truly subcubic algorithm for the
APSP problem. However, recently Mao (FOCS'21) showed that if we assume that
each operation has a unit cost, then the tree edit distance between two rooted
trees can be computed in truly subcubic time. In this paper, we show how to
adapt Mao's algorithm to make it work for unrooted trees and we show an
$\widetilde{\mathcal{O}}(n^{(7\omega + 15)/(2\omega + 6)}) \leq
\mathcal{O}(n^{2.9417})$ time algorithm for the unweighted tree edit distance
between two unrooted trees, where $\omega \leq 2.373$ is the matrix
multiplication exponent. It is the first known subcubic algorithm for unrooted
trees. The main idea behind our algorithm is the fact that to compute the tree
edit distance between two unrooted trees, it is enough to compute the tree edit
distance between an arbitrary rooting of the first tree and every rooting of
the second tree.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Pioro_K/0/1/0/all/0/1">Krzysztof Pi&#xf3;ro</a></p><p>The tree edit distance problem is a natural generalization of the classic
string edit distance problem. Given two ordered, edge-labeled trees $T_1$ and
$T_2$, the edit distance between $T_1$ and $T_2$ is defined as the minimum
total cost of operations that transform $T_1$ into $T_2$. In one operation, we
can contract an edge, split a vertex into two or change the label of an edge.
For the weighted version of the problem, where the cost of each operation
depends on the type of the operation and the label on the edge involved,
$\mathcal{O}(n^3)$ time algorithms are known for both rooted and unrooted
trees. The existence of a truly subcubic $\mathcal{O}(n^{3-\epsilon})$ time
algorithm is unlikely, as it would imply a truly subcubic algorithm for the
APSP problem. However, recently Mao (FOCS'21) showed that if we assume that
each operation has a unit cost, then the tree edit distance between two rooted
trees can be computed in truly subcubic time. In this paper, we show how to
adapt Mao's algorithm to make it work for unrooted trees and we show an
$\widetilde{\mathcal{O}}(n^{(7\omega + 15)/(2\omega + 6)}) \leq
\mathcal{O}(n^{2.9417})$ time algorithm for the unweighted tree edit distance
between two unrooted trees, where $\omega \leq 2.373$ is the matrix
multiplication exponent. It is the first known subcubic algorithm for unrooted
trees. The main idea behind our algorithm is the fact that to compute the tree
edit distance between two unrooted trees, it is enough to compute the tree edit
distance between an arbitrary rooting of the first tree and every rooting of
the second tree.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-19T00:30:00Z">Wednesday, April 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.08648'>Dynamic Vector Bin Packing for Online Resource Allocation in the Cloud</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Aniket Murhekar, David Arbour, Tung Mai, Anup Rao</p><p>Several cloud-based applications, such as cloud gaming, rent servers to
execute jobs which arrive in an online fashion. Each job has a resource demand
and must be dispatched to a cloud server which has enough resources to execute
the job, which departs after its completion. Under the `pay-as-you-go' billing
model, the server rental cost is proportional to the total time that servers
are actively running jobs. The problem of efficiently allocating a sequence of
online jobs to servers without exceeding the resource capacity of any server
while minimizing total server usage time can be modelled as a variant of the
dynamic bin packing problem (DBP), called MinUsageTime DBP.
</p>
<p>In this work, we initiate the study of the problem with multi-dimensional
resource demands (e.g. CPU/GPU usage, memory requirement, bandwidth usage,
etc.), called MinUsageTime Dynamic Vector Bin Packing (DVBP). We study the
competitive ratio (CR) of Any Fit packing algorithms for this problem. We show
almost-tight bounds on the CR of three specific Any Fit packing algorithms,
namely First Fit, Next Fit, and Move To Front. We prove that the CR of Move To
Front is at most $(2\mu+1)d +1$, where $\mu$ is the ratio of the max/min item
durations. For $d=1$, this significantly improves the previously known upper
bound of $6\mu+7$ (Kamali &amp; Lopez-Ortiz, 2015). We then prove the CR of First
Fit and Next Fit are bounded by $(\mu+2)d+1$ and $2\mu d+1$, respectively.
Next, we prove a lower bound of $(\mu+1)d$ on the CR of any Any Fit packing
algorithm, an improved lower bound of $2\mu d$ for Next Fit, and a lower bound
of $2\mu$ for Move To Front in the 1-D case. All our bounds improve or match
the best-known bounds for the 1-D case. Finally, we experimentally study the
average-case performance of these algorithms on randomly generated synthetic
data, and observe that Move To Front outperforms other Any Fit packing
algorithms.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Murhekar_A/0/1/0/all/0/1">Aniket Murhekar</a>, <a href="http://arxiv.org/find/cs/1/au:+Arbour_D/0/1/0/all/0/1">David Arbour</a>, <a href="http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1">Tung Mai</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1">Anup Rao</a></p><p>Several cloud-based applications, such as cloud gaming, rent servers to
execute jobs which arrive in an online fashion. Each job has a resource demand
and must be dispatched to a cloud server which has enough resources to execute
the job, which departs after its completion. Under the `pay-as-you-go' billing
model, the server rental cost is proportional to the total time that servers
are actively running jobs. The problem of efficiently allocating a sequence of
online jobs to servers without exceeding the resource capacity of any server
while minimizing total server usage time can be modelled as a variant of the
dynamic bin packing problem (DBP), called MinUsageTime DBP.
</p>
<p>In this work, we initiate the study of the problem with multi-dimensional
resource demands (e.g. CPU/GPU usage, memory requirement, bandwidth usage,
etc.), called MinUsageTime Dynamic Vector Bin Packing (DVBP). We study the
competitive ratio (CR) of Any Fit packing algorithms for this problem. We show
almost-tight bounds on the CR of three specific Any Fit packing algorithms,
namely First Fit, Next Fit, and Move To Front. We prove that the CR of Move To
Front is at most $(2\mu+1)d +1$, where $\mu$ is the ratio of the max/min item
durations. For $d=1$, this significantly improves the previously known upper
bound of $6\mu+7$ (Kamali &amp; Lopez-Ortiz, 2015). We then prove the CR of First
Fit and Next Fit are bounded by $(\mu+2)d+1$ and $2\mu d+1$, respectively.
Next, we prove a lower bound of $(\mu+1)d$ on the CR of any Any Fit packing
algorithm, an improved lower bound of $2\mu d$ for Next Fit, and a lower bound
of $2\mu$ for Move To Front in the 1-D case. All our bounds improve or match
the best-known bounds for the 1-D case. Finally, we experimentally study the
average-case performance of these algorithms on randomly generated synthetic
data, and observe that Move To Front outperforms other Any Fit packing
algorithms.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-19T00:30:00Z">Wednesday, April 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.08746'>On Approximate Reconfigurability of Label Cover</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Naoto Ohsaka</p><p>Given a two-prover game $G$ and its two satisfying labelings
$\psi_\mathsf{s}$ and $\psi_\mathsf{t}$, the Label Cover Reconfiguration
problem asks whether $\psi_\mathsf{s}$ can be transformed into
$\psi_\mathsf{t}$ by repeatedly changing the value of a vertex while preserving
any intermediate labeling satisfying $G$. We consider an optimization variant
of Label Cover Reconfiguration by relaxing the feasibility of labelings,
referred to as Maxmin Label Cover Reconfiguration: we are allowed to transform
by passing through any non-satisfying labelings, but required to maximize the
minimum fraction of satisfied edges during transformation from
$\psi_\mathsf{s}$ to $\psi_\mathsf{t}$. Since the parallel repetition theorem
of Raz (SIAM J. Comput., 1998), which implies NP-hardness of Label Cover within
any constant factor, produces strong inapproximability results for many NP-hard
problems, one may think of using Maxmin Label Cover Reconfiguration to derive
inapproximability results for reconfiguration problems. We prove the following
results on Maxmin Label Cover Reconfiguration, which display different trends
from those of Label Cover and the parallel repetition theorem:
</p>
<p>(1) Maxmin Label Cover Reconfiguration can be approximated within a factor of
nearly $\frac{1}{4}$ for restricted graph classes, including slightly dense
graphs and balanced bipartite graphs.
</p>
<p>(2) A naive parallel repetition of Maxmin Label Cover Reconfiguration does
not decrease the optimal objective value.
</p>
<p>(3) Label Cover Reconfiguration on projection games can be decided in
polynomial time.
</p>
<p>The above results suggest that a reconfiguration analogue of the parallel
repetition theorem is unlikely.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ohsaka_N/0/1/0/all/0/1">Naoto Ohsaka</a></p><p>Given a two-prover game $G$ and its two satisfying labelings
$\psi_\mathsf{s}$ and $\psi_\mathsf{t}$, the Label Cover Reconfiguration
problem asks whether $\psi_\mathsf{s}$ can be transformed into
$\psi_\mathsf{t}$ by repeatedly changing the value of a vertex while preserving
any intermediate labeling satisfying $G$. We consider an optimization variant
of Label Cover Reconfiguration by relaxing the feasibility of labelings,
referred to as Maxmin Label Cover Reconfiguration: we are allowed to transform
by passing through any non-satisfying labelings, but required to maximize the
minimum fraction of satisfied edges during transformation from
$\psi_\mathsf{s}$ to $\psi_\mathsf{t}$. Since the parallel repetition theorem
of Raz (SIAM J. Comput., 1998), which implies NP-hardness of Label Cover within
any constant factor, produces strong inapproximability results for many NP-hard
problems, one may think of using Maxmin Label Cover Reconfiguration to derive
inapproximability results for reconfiguration problems. We prove the following
results on Maxmin Label Cover Reconfiguration, which display different trends
from those of Label Cover and the parallel repetition theorem:
</p>
<p>(1) Maxmin Label Cover Reconfiguration can be approximated within a factor of
nearly $\frac{1}{4}$ for restricted graph classes, including slightly dense
graphs and balanced bipartite graphs.
</p>
<p>(2) A naive parallel repetition of Maxmin Label Cover Reconfiguration does
not decrease the optimal objective value.
</p>
<p>(3) Label Cover Reconfiguration on projection games can be decided in
polynomial time.
</p>
<p>The above results suggest that a reconfiguration analogue of the parallel
repetition theorem is unlikely.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-19T00:30:00Z">Wednesday, April 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.08892'>Parallel Greedy Spanners</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Bernhard Haeupler, D Ellis Hershkowitz, Zihan Tan</p><p>A $t$-spanner of a graph is a subgraph that $t$-approximates pairwise
distances. The greedy algorithm is one of the simplest and most well-studied
algorithms for constructing a sparse spanner: it computes a $t$-spanner with
$n^{1+O(1/t)}$ edges by repeatedly choosing any edge which does not close a
cycle of chosen edges with $t+1$ or fewer edges.
</p>
<p>We demonstrate that the greedy algorithm computes a $t$-spanner with $n^{1 +
O(1/t)}$ edges even when a matching of such edges are added in parallel. In
particular, it suffices to repeatedly add any matching where each individual
edge does not close a cycle with $t +1$ or fewer edges but where adding the
entire matching might. Our analysis makes use of and illustrates the power of
new advances in length-constrained expander decompositions.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Haeupler_B/0/1/0/all/0/1">Bernhard Haeupler</a>, <a href="http://arxiv.org/find/cs/1/au:+Hershkowitz_D/0/1/0/all/0/1">D Ellis Hershkowitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1">Zihan Tan</a></p><p>A $t$-spanner of a graph is a subgraph that $t$-approximates pairwise
distances. The greedy algorithm is one of the simplest and most well-studied
algorithms for constructing a sparse spanner: it computes a $t$-spanner with
$n^{1+O(1/t)}$ edges by repeatedly choosing any edge which does not close a
cycle of chosen edges with $t+1$ or fewer edges.
</p>
<p>We demonstrate that the greedy algorithm computes a $t$-spanner with $n^{1 +
O(1/t)}$ edges even when a matching of such edges are added in parallel. In
particular, it suffices to repeatedly add any matching where each individual
edge does not close a cycle with $t +1$ or fewer edges but where adding the
entire matching might. Our analysis makes use of and illustrates the power of
new advances in length-constrained expander decompositions.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-19T00:30:00Z">Wednesday, April 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.09113'>Random Cuts are Optimal for Explainable k-Medians</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Konstantin Makarychev, Liren Shan</p><p>We show that the RandomCoordinateCut algorithm gives the optimal competitive
ratio for explainable k-medians in l1. The problem of explainable k-medians was
introduced by Dasgupta, Frost, Moshkovitz, and Rashtchian in 2020. Several
groups of authors independently proposed a simple polynomial-time randomized
algorithm for the problem and showed that this algorithm is O(log k loglog k)
competitive. We provide a tight analysis of the algorithm and prove that its
competitive ratio is upper bounded by 2ln k +2. This bound matches the
Omega(log k) lower bound by Dasgupta et al (2020).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Makarychev_K/0/1/0/all/0/1">Konstantin Makarychev</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_L/0/1/0/all/0/1">Liren Shan</a></p><p>We show that the RandomCoordinateCut algorithm gives the optimal competitive
ratio for explainable k-medians in l1. The problem of explainable k-medians was
introduced by Dasgupta, Frost, Moshkovitz, and Rashtchian in 2020. Several
groups of authors independently proposed a simple polynomial-time randomized
algorithm for the problem and showed that this algorithm is O(log k loglog k)
competitive. We provide a tight analysis of the algorithm and prove that its
competitive ratio is upper bounded by 2ln k +2. This bound matches the
Omega(log k) lower bound by Dasgupta et al (2020).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-19T00:30:00Z">Wednesday, April 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.09167'>Optimal PAC Bounds Without Uniform Convergence</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ishaq Aden-Ali, Yeshwanth Cherapanamjeri, Abhishek Shetty, Nikita Zhivotovskiy</p><p>In statistical learning theory, determining the sample complexity of
realizable binary classification for VC classes was a long-standing open
problem. The results of Simon and Hanneke established sharp upper bounds in
this setting. However, the reliance of their argument on the uniform
convergence principle limits its applicability to more general learning
settings such as multiclass classification. In this paper, we address this
issue by providing optimal high probability risk bounds through a framework
that surpasses the limitations of uniform convergence arguments.
</p>
<p>Our framework converts the leave-one-out error of permutation invariant
predictors into high probability risk bounds. As an application, by adapting
the one-inclusion graph algorithm of Haussler, Littlestone, and Warmuth, we
propose an algorithm that achieves an optimal PAC bound for binary
classification. Specifically, our result shows that certain aggregations of
one-inclusion graph algorithms are optimal, addressing a variant of a classic
question posed by Warmuth.
</p>
<p>We further instantiate our framework in three settings where uniform
convergence is provably suboptimal. For multiclass classification, we prove an
optimal risk bound that scales with the one-inclusion hypergraph density of the
class, addressing the suboptimality of the analysis of Daniely and
Shalev-Shwartz. For partial hypothesis classification, we determine the optimal
sample complexity bound, resolving a question posed by Alon, Hanneke, Holzman,
and Moran. For realizable bounded regression with absolute loss, we derive an
optimal risk bound that relies on a modified version of the scale-sensitive
dimension, refining the results of Bartlett and Long. Our rates surpass
standard uniform convergence-based results due to the smaller complexity
measure in our risk bound.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Aden_Ali_I/0/1/0/all/0/1">Ishaq Aden-Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+Cherapanamjeri_Y/0/1/0/all/0/1">Yeshwanth Cherapanamjeri</a>, <a href="http://arxiv.org/find/cs/1/au:+Shetty_A/0/1/0/all/0/1">Abhishek Shetty</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhivotovskiy_N/0/1/0/all/0/1">Nikita Zhivotovskiy</a></p><p>In statistical learning theory, determining the sample complexity of
realizable binary classification for VC classes was a long-standing open
problem. The results of Simon and Hanneke established sharp upper bounds in
this setting. However, the reliance of their argument on the uniform
convergence principle limits its applicability to more general learning
settings such as multiclass classification. In this paper, we address this
issue by providing optimal high probability risk bounds through a framework
that surpasses the limitations of uniform convergence arguments.
</p>
<p>Our framework converts the leave-one-out error of permutation invariant
predictors into high probability risk bounds. As an application, by adapting
the one-inclusion graph algorithm of Haussler, Littlestone, and Warmuth, we
propose an algorithm that achieves an optimal PAC bound for binary
classification. Specifically, our result shows that certain aggregations of
one-inclusion graph algorithms are optimal, addressing a variant of a classic
question posed by Warmuth.
</p>
<p>We further instantiate our framework in three settings where uniform
convergence is provably suboptimal. For multiclass classification, we prove an
optimal risk bound that scales with the one-inclusion hypergraph density of the
class, addressing the suboptimality of the analysis of Daniely and
Shalev-Shwartz. For partial hypothesis classification, we determine the optimal
sample complexity bound, resolving a question posed by Alon, Hanneke, Holzman,
and Moran. For realizable bounded regression with absolute loss, we derive an
optimal risk bound that relies on a modified version of the scale-sensitive
dimension, refining the results of Bartlett and Long. Our rates surpass
standard uniform convergence-based results due to the smaller complexity
measure in our risk bound.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-19T00:30:00Z">Wednesday, April 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Tuesday, April 18
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/048'>TR23-048 |  A $d^{1/2+o(1)}$ Monotonicity Tester for Boolean Functions on $d$-Dimensional Hypergrids | 

	Hadley Black, 

	Deeparnab Chakrabarty, 

	C. Seshadhri</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Monotonicity testing of Boolean functions on the hypergrid, $f:[n]^d \to \{0,1\}$, is a classic topic in property testing. Determining the non-adaptive complexity of this problem is an important open question. For arbitrary $n$, [Black-Chakrabarty-Seshadhri, SODA 2020] describe a tester with query complexity $\widetilde{O}(\varepsilon^{-4/3}d^{5/6})$. This complexity is independent of $n$, but has a suboptimal dependence on $d$. Recently, [Braverman-Khot-Kindler-Minzer, ITCS 2023] and [Black-Chakrabarty-Seshadhri, STOC 2023] describe $\widetilde{O}(\varepsilon^{-2} n^3\sqrt{d})$ and $\widetilde{O}(\varepsilon^{-2} n\sqrt{d})$-query testers, respectively. These testers have an almost optimal dependence on $d$, but a suboptimal polynomial dependence on $n$. 

In this paper, we describe a non-adaptive, one-sided monotonicity tester with query complexity
$O(\varepsilon^{-2} d^{1/2 + o(1)})$, independent of $n$. Up to the $d^{o(1)}$-factors, our result resolves the non-adaptive complexity of monotonicity testing for Boolean functions on hypergrids. The independence of $n$ yields a non-adaptive, one-sided $O(\varepsilon^{-2} d^{1/2 + o(1)})$-query monotonicity tester for Boolean functions $f:\mathbb{R}^d \to \{0,1\}$ associated with an arbitrary product measure.
        
        </div>

        <div class='tr-article-summary'>
        
          
          Monotonicity testing of Boolean functions on the hypergrid, $f:[n]^d \to \{0,1\}$, is a classic topic in property testing. Determining the non-adaptive complexity of this problem is an important open question. For arbitrary $n$, [Black-Chakrabarty-Seshadhri, SODA 2020] describe a tester with query complexity $\widetilde{O}(\varepsilon^{-4/3}d^{5/6})$. This complexity is independent of $n$, but has a suboptimal dependence on $d$. Recently, [Braverman-Khot-Kindler-Minzer, ITCS 2023] and [Black-Chakrabarty-Seshadhri, STOC 2023] describe $\widetilde{O}(\varepsilon^{-2} n^3\sqrt{d})$ and $\widetilde{O}(\varepsilon^{-2} n\sqrt{d})$-query testers, respectively. These testers have an almost optimal dependence on $d$, but a suboptimal polynomial dependence on $n$. 

In this paper, we describe a non-adaptive, one-sided monotonicity tester with query complexity
$O(\varepsilon^{-2} d^{1/2 + o(1)})$, independent of $n$. Up to the $d^{o(1)}$-factors, our result resolves the non-adaptive complexity of monotonicity testing for Boolean functions on hypergrids. The independence of $n$ yields a non-adaptive, one-sided $O(\varepsilon^{-2} d^{1/2 + o(1)})$-query monotonicity tester for Boolean functions $f:\mathbb{R}^d \to \{0,1\}$ associated with an arbitrary product measure.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-18T19:15:38Z">Tuesday, April 18 2023, 19:15</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.07825'>Regression and Algorithmic Information Theory</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Samuel Epstein</p><p>In this paper we prove a theorem about regression, in that the shortest
description of a function consistent with a finite sample of data is less than
the combined conditional Kolmogorov complexities over the data in the sample.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Epstein_S/0/1/0/all/0/1">Samuel Epstein</a></p><p>In this paper we prove a theorem about regression, in that the shortest
description of a function consistent with a finite sample of data is less than
the combined conditional Kolmogorov complexities over the data in the sample.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-18T00:30:00Z">Tuesday, April 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.07357'>Efficient Incremental Penetration Depth Estimation between Convex Geometries</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Wei Gao</p><p>Penetration depth (PD) is essential for robotics due to its extensive
applications in dynamic simulation, motion planning, haptic rendering, etc. The
Expanding Polytope Algorithm (EPA) is the de facto standard for this problem,
which estimates PD by expanding an inner polyhedral approximation of an
implicit set. In this paper, we propose a novel optimization-based algorithm
that incrementally estimates minimum penetration depth and its direction. One
major advantage of our method is that it can be warm-started by exploiting the
spatial and temporal coherence, which emerges naturally in many robotic
applications (e.g., the temporal coherence between adjacent simulation time
knots). As a result, our algorithm achieves substantial speedup -- we
demonstrate it is 5-30x faster than EPA on several benchmarks. Moreover, our
approach is built upon the same implicit geometry representation as EPA, which
enables easy integration and deployment into existing software stacks. We also
provide an open-source implementation for further evaluations and experiments.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1">Wei Gao</a></p><p>Penetration depth (PD) is essential for robotics due to its extensive
applications in dynamic simulation, motion planning, haptic rendering, etc. The
Expanding Polytope Algorithm (EPA) is the de facto standard for this problem,
which estimates PD by expanding an inner polyhedral approximation of an
implicit set. In this paper, we propose a novel optimization-based algorithm
that incrementally estimates minimum penetration depth and its direction. One
major advantage of our method is that it can be warm-started by exploiting the
spatial and temporal coherence, which emerges naturally in many robotic
applications (e.g., the temporal coherence between adjacent simulation time
knots). As a result, our algorithm achieves substantial speedup -- we
demonstrate it is 5-30x faster than EPA on several benchmarks. Moreover, our
approach is built upon the same implicit geometry representation as EPA, which
enables easy integration and deployment into existing software stacks. We also
provide an open-source implementation for further evaluations and experiments.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-18T00:30:00Z">Tuesday, April 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.07622'>Random $\epsilon$-Cover on Compact Symmetric Space</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Somnath Chakraborty</p><p>A randomized scheme that succeeds with probability $1-\delta$ (for any
$\delta&gt;0$) has been devised to construct (1) an equidistributed
$\epsilon$-cover of a compact Riemannian symmetric space $\mathbb M$ of
dimension $d_{\mathbb M}$ and antipodal dimension $\bar{d}_{\mathbb M}$, and
(2) an approximate $(\lambda_r,2)$-design, using $n(\epsilon,\delta)$-many
Haar-random isometries of $\mathbb M$, where
\begin{equation}n(\epsilon,\delta):=O_{\mathbb M}\left(d_{\mathbb M}\ln
\left(\frac 1\epsilon\right)+\log\left(\frac
1\delta\right)\right)\,,\end{equation} and $\lambda_r$ is the $r$-th smallest
eigenvalue of the Laplace-Beltrami operator on $\mathbb M$. The
$\epsilon$-cover so-produced can be used to compute the integral of 1-Lipschitz
functions within additive $\tilde O(\epsilon)$-error, as well as in comparing
persistence homology computed from data cloud to that of a hypothetical data
cloud sampled from the uniform measure.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Chakraborty_S/0/1/0/all/0/1">Somnath Chakraborty</a></p><p>A randomized scheme that succeeds with probability $1-\delta$ (for any
$\delta&gt;0$) has been devised to construct (1) an equidistributed
$\epsilon$-cover of a compact Riemannian symmetric space $\mathbb M$ of
dimension $d_{\mathbb M}$ and antipodal dimension $\bar{d}_{\mathbb M}$, and
(2) an approximate $(\lambda_r,2)$-design, using $n(\epsilon,\delta)$-many
Haar-random isometries of $\mathbb M$, where
\begin{equation}n(\epsilon,\delta):=O_{\mathbb M}\left(d_{\mathbb M}\ln
\left(\frac 1\epsilon\right)+\log\left(\frac
1\delta\right)\right)\,,\end{equation} and $\lambda_r$ is the $r$-th smallest
eigenvalue of the Laplace-Beltrami operator on $\mathbb M$. The
$\epsilon$-cover so-produced can be used to compute the integral of 1-Lipschitz
functions within additive $\tilde O(\epsilon)$-error, as well as in comparing
persistence homology computed from data cloud to that of a hypothetical data
cloud sampled from the uniform measure.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-18T00:30:00Z">Tuesday, April 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.07516'>Simple Combinatorial Construction of the $k^{o(1)}$-Lower Bound for Approximating the Parameterized $k$-Clique</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yijia Chen, Yi Feng, Bundit Laekhanukit, Yanlin Liu</p><p>In the parameterized $k$-clique problem, or $k$-Clique for short, we are
given a graph $G$ and a parameter $k\ge 1$. The goal is to decide whether there
exist $k$ vertices in $G$ that induce a complete subgraph (i.e., a $k$-clique).
This problem plays a central role in the theory of parameterized intractability
as one of the first W[1]-complete problems. Existing research has shown that
even an FPT-approximation algorithm for $k$-Clique with arbitrary ratio does
not exist, assuming the Gap-Exponential-Time Hypothesis (Gap-ETH) [Chalermsook
et al., FOCS'17 and SICOMP]. However, whether this inapproximability result can
be based on the standard assumption of $\mathrm{W} 1\ne \mathrm{FPT}$ remains
unclear. The recent breakthrough of Bingkai Lin [STOC'21] and subsequent works
by Karthik C.S. and Khot [CCC'22], and by Lin, Ren, Sun Wang [ICALP'22] give a
technique that bypasses Gap-ETH, thus leading to the inapproximability ratio of
$O(1)$ and $k^{o(1)}$ under $\mathrm{W}[1]$-hardness (the first two) and ETH
(for the latter one). All the work along this line follows the framework
developed by Lin, which starts from the $k$-vector-sum problem and requires
some involved algebraic techniques.
</p>
<p>This paper presents an alternative framework for proving the W[1]-hardness of
the $k^{o(1)}$-FPT-inapproximability of $k$-Clique. Using this framework, we
obtain a gap-producing self-reduction of $k$-Clique without any intermediate
algebraic problem. More precisely, we reduce from $(k,k-1)$-Gap Clique to
$(q^k, q^{k-1})$-Gap Clique, for any function $q$ depending only on the
parameter $k$, thus implying the $k^{o(1)}$-inapproximability result when $q$
is sufficiently large. Our proof is relatively simple and mostly combinatorial.
At the core of our construction is a novel encoding of $k$-element subset
stemming from the theory of "network coding" and a "Sidon set" representation
of a graph.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yijia Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yi Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Laekhanukit_B/0/1/0/all/0/1">Bundit Laekhanukit</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yanlin Liu</a></p><p>In the parameterized $k$-clique problem, or $k$-Clique for short, we are
given a graph $G$ and a parameter $k\ge 1$. The goal is to decide whether there
exist $k$ vertices in $G$ that induce a complete subgraph (i.e., a $k$-clique).
This problem plays a central role in the theory of parameterized intractability
as one of the first W[1]-complete problems. Existing research has shown that
even an FPT-approximation algorithm for $k$-Clique with arbitrary ratio does
not exist, assuming the Gap-Exponential-Time Hypothesis (Gap-ETH) [Chalermsook
et al., FOCS'17 and SICOMP]. However, whether this inapproximability result can
be based on the standard assumption of $\mathrm{W} 1\ne \mathrm{FPT}$ remains
unclear. The recent breakthrough of Bingkai Lin [STOC'21] and subsequent works
by Karthik C.S. and Khot [CCC'22], and by Lin, Ren, Sun Wang [ICALP'22] give a
technique that bypasses Gap-ETH, thus leading to the inapproximability ratio of
$O(1)$ and $k^{o(1)}$ under $\mathrm{W}[1]$-hardness (the first two) and ETH
(for the latter one). All the work along this line follows the framework
developed by Lin, which starts from the $k$-vector-sum problem and requires
some involved algebraic techniques.
</p>
<p>This paper presents an alternative framework for proving the W[1]-hardness of
the $k^{o(1)}$-FPT-inapproximability of $k$-Clique. Using this framework, we
obtain a gap-producing self-reduction of $k$-Clique without any intermediate
algebraic problem. More precisely, we reduce from $(k,k-1)$-Gap Clique to
$(q^k, q^{k-1})$-Gap Clique, for any function $q$ depending only on the
parameter $k$, thus implying the $k^{o(1)}$-inapproximability result when $q$
is sufficiently large. Our proof is relatively simple and mostly combinatorial.
At the core of our construction is a novel encoding of $k$-element subset
stemming from the theory of "network coding" and a "Sidon set" representation
of a graph.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-18T00:30:00Z">Tuesday, April 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.07716'>On modeling NP-Complete problems as polynomial-sized linear programs: Escaping/Side-stepping the "barriers"</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Moustapha Diaby, Mark Karwan, Lei Sun</p><p>In view of the extended formulations (EFs) developments (e.g. "Fiorini, S.,
S. Massar, S. Pokutta, H.R. Tiwary, and R. de Wolf [2015]. Exponential Lower
Bounds for Polytopes in Combinatorial Optimization. Journal of the ACM 62:2"),
we focus in this paper on the question of whether it is possible to model an
NP-Complete problem as a polynomial-sized linear program. For the sake of
simplicity of exposition, the discussions are focused on the TSP. We show that
a finding that there exists no polynomial-sized extended formulation of "the
TSP polytope" does not (necessarily) imply that it is "impossible" for a
polynomial-sized linear program to solve the TSP optimization problem. We show
that under appropriate conditions the TSP optimization problem can be solved
without recourse to the traditional city-to-city ("travel leg") variables,
thereby side-stepping/"escaping from" "the TSP polytope" and hence, the
barriers. Some illustrative examples are discussed.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Diaby_M/0/1/0/all/0/1">Moustapha Diaby</a>, <a href="http://arxiv.org/find/cs/1/au:+Karwan_M/0/1/0/all/0/1">Mark Karwan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lei Sun</a></p><p>In view of the extended formulations (EFs) developments (e.g. "Fiorini, S.,
S. Massar, S. Pokutta, H.R. Tiwary, and R. de Wolf [2015]. Exponential Lower
Bounds for Polytopes in Combinatorial Optimization. Journal of the ACM 62:2"),
we focus in this paper on the question of whether it is possible to model an
NP-Complete problem as a polynomial-sized linear program. For the sake of
simplicity of exposition, the discussions are focused on the TSP. We show that
a finding that there exists no polynomial-sized extended formulation of "the
TSP polytope" does not (necessarily) imply that it is "impossible" for a
polynomial-sized linear program to solve the TSP optimization problem. We show
that under appropriate conditions the TSP optimization problem can be solved
without recourse to the traditional city-to-city ("travel leg") variables,
thereby side-stepping/"escaping from" "the TSP polytope" and hence, the
barriers. Some illustrative examples are discussed.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-18T00:30:00Z">Tuesday, April 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.07441'>Fully Scalable Massively Parallel Algorithms for Embedded Planar Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yi-Jun Chang, Da Wei Zheng</p><p>We consider the \emph{massively parallel computation} (MPC) model, which is a
theoretical abstraction of large-scale parallel processing models such as
MapReduce. In this model, assuming the widely believed 1-vs-2-cycles
conjecture, it is not possible to solve many basic graph problems in constant
rounds with strongly sublinear memory size per machine.
</p>
<p>Recently, Holm and T\v{e}tek [SODA 2023] showed that it is possible to get
around this barrier for planar graphs when a planar straight-line embedding of
the graph is given. For such inputs on $n$ vertices, they obtained
constant-round MPC algorithms for connected components, minimum spanning tree
(MST), and $O(1)$-approximation of $st$-shortest path, diameter, and radius, as
long as the memory size per machine is $\mathcal{S} = n^{2/3 + \Omega(1)}$.
</p>
<p>In this work, we provide an improved recursive framework to obtain
constant-round algorithms in the more challenging \emph{fully scalable} regime
where memory size per machine can be $\mathcal{S} = n^\delta$ for any given
constant $\delta &gt; 0$. This gives the first constant-round algorithms in this
regime for fundamental problems such as connected components, MST, and EMST.
</p>
<p>Moreover, we show that $\varepsilon$-emulators can be incorporated into our
recursive framework to obtain constant-round $(1+\varepsilon)$-approximation
algorithms for single source shortest path (SSSP) and shortest cycle in
embedded planar graphs. We show that it is possible to construct a dual graph
of the given embedded planar graph in constant rounds, which allows us to solve
the $(1+\varepsilon)$-approximate $st$-maximum flow and minimum cut problem as
both reduce to a shortest cycle problem in the dual graph. Using $O(n^2)$ total
space, we also obtain constant-round algorithms for
$(1+\varepsilon)$-approximate all-pairs shortest paths (APSP), diameter, and
radius.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yi-Jun Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1">Da Wei Zheng</a></p><p>We consider the \emph{massively parallel computation} (MPC) model, which is a
theoretical abstraction of large-scale parallel processing models such as
MapReduce. In this model, assuming the widely believed 1-vs-2-cycles
conjecture, it is not possible to solve many basic graph problems in constant
rounds with strongly sublinear memory size per machine.
</p>
<p>Recently, Holm and T\v{e}tek [SODA 2023] showed that it is possible to get
around this barrier for planar graphs when a planar straight-line embedding of
the graph is given. For such inputs on $n$ vertices, they obtained
constant-round MPC algorithms for connected components, minimum spanning tree
(MST), and $O(1)$-approximation of $st$-shortest path, diameter, and radius, as
long as the memory size per machine is $\mathcal{S} = n^{2/3 + \Omega(1)}$.
</p>
<p>In this work, we provide an improved recursive framework to obtain
constant-round algorithms in the more challenging \emph{fully scalable} regime
where memory size per machine can be $\mathcal{S} = n^\delta$ for any given
constant $\delta &gt; 0$. This gives the first constant-round algorithms in this
regime for fundamental problems such as connected components, MST, and EMST.
</p>
<p>Moreover, we show that $\varepsilon$-emulators can be incorporated into our
recursive framework to obtain constant-round $(1+\varepsilon)$-approximation
algorithms for single source shortest path (SSSP) and shortest cycle in
embedded planar graphs. We show that it is possible to construct a dual graph
of the given embedded planar graph in constant rounds, which allows us to solve
the $(1+\varepsilon)$-approximate $st$-maximum flow and minimum cut problem as
both reduce to a shortest cycle problem in the dual graph. Using $O(n^2)$ total
space, we also obtain constant-round algorithms for
$(1+\varepsilon)$-approximate all-pairs shortest paths (APSP), diameter, and
radius.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-18T00:30:00Z">Tuesday, April 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.07403'>Fully Dynamic Shortest Path Reporting Against an Adaptive Adversary</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Anastasiia Alokhina, Jan van den Brand</p><p>Algebraic data structures are the main subroutine for maintaining distances
in fully dynamic graphs in subquadratic time. However, these dynamic algebraic
algorithms generally cannot maintain the shortest paths, especially against
adaptive adversaries. We present the first fully dynamic algorithm that
maintains the shortest paths against an adaptive adversary in subquadratic
update time. This is obtained via a combinatorial reduction that allows
reconstructing the shortest paths with only a few distance estimates. Using
this reduction, we obtain the following:
</p>
<p>On weighted directed graphs with real edge weights in $[1,W]$, we can
maintain $(1+\epsilon)$ approximate shortest paths in
$\tilde{O}(n^{1.816}\epsilon^{-2} \log W)$ update and $\tilde{O}(n^{1.741}
\epsilon^{-2} \log W)$ query time. This improves upon the approximate distance
data structures from [v.d.Brand, Nanongkai, FOCS'19], which only returned a
distance estimate, by matching their complexity and returning an approximate
shortest path.
</p>
<p>On unweighted directed graphs, we can maintain exact shortest paths in
$\tilde{O}(n^{1.823})$ update and $\tilde{O}(n^{1.747})$ query time. This
improves upon [Bergamaschi, Henzinger, P.Gutenberg, V.Williams, Wein, SODA'21]
who could report the path only against oblivious adversaries. We improve both
their update and query time while also handling adaptive adversaries.
</p>
<p>On unweighted undirected graphs, our reduction holds not just against
adaptive adversaries but is also deterministic. We maintain a
$(1+\epsilon)$-approximate $st$-shortest path in $O(n^{1.529} / \epsilon^2)$
time per update, and $(1+\epsilon)$-approximate single source shortest paths in
$O(n^{1.764} / \epsilon^2)$ time per update. Previous deterministic results by
[v.d.Brand, Nazari, Forster, FOCS'22] could only maintain distance estimates
but no paths.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Alokhina_A/0/1/0/all/0/1">Anastasiia Alokhina</a>, <a href="http://arxiv.org/find/cs/1/au:+Brand_J/0/1/0/all/0/1">Jan van den Brand</a></p><p>Algebraic data structures are the main subroutine for maintaining distances
in fully dynamic graphs in subquadratic time. However, these dynamic algebraic
algorithms generally cannot maintain the shortest paths, especially against
adaptive adversaries. We present the first fully dynamic algorithm that
maintains the shortest paths against an adaptive adversary in subquadratic
update time. This is obtained via a combinatorial reduction that allows
reconstructing the shortest paths with only a few distance estimates. Using
this reduction, we obtain the following:
</p>
<p>On weighted directed graphs with real edge weights in $[1,W]$, we can
maintain $(1+\epsilon)$ approximate shortest paths in
$\tilde{O}(n^{1.816}\epsilon^{-2} \log W)$ update and $\tilde{O}(n^{1.741}
\epsilon^{-2} \log W)$ query time. This improves upon the approximate distance
data structures from [v.d.Brand, Nanongkai, FOCS'19], which only returned a
distance estimate, by matching their complexity and returning an approximate
shortest path.
</p>
<p>On unweighted directed graphs, we can maintain exact shortest paths in
$\tilde{O}(n^{1.823})$ update and $\tilde{O}(n^{1.747})$ query time. This
improves upon [Bergamaschi, Henzinger, P.Gutenberg, V.Williams, Wein, SODA'21]
who could report the path only against oblivious adversaries. We improve both
their update and query time while also handling adaptive adversaries.
</p>
<p>On unweighted undirected graphs, our reduction holds not just against
adaptive adversaries but is also deterministic. We maintain a
$(1+\epsilon)$-approximate $st$-shortest path in $O(n^{1.529} / \epsilon^2)$
time per update, and $(1+\epsilon)$-approximate single source shortest paths in
$O(n^{1.764} / \epsilon^2)$ time per update. Previous deterministic results by
[v.d.Brand, Nazari, Forster, FOCS'22] could only maintain distance estimates
but no paths.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-18T00:30:00Z">Tuesday, April 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.07413'>Robust Algorithms on Adaptive Inputs from Bounded Adversaries</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yeshwanth Cherapanamjeri, Sandeep Silwal, David P. Woodruff, Fred Zhang, Qiuyi Zhang, Samson Zhou</p><p>We study dynamic algorithms robust to adaptive input generated from sources
with bounded capabilities, such as sparsity or limited interaction. For
example, we consider robust linear algebraic algorithms when the updates to the
input are sparse but given by an adversary with access to a query oracle. We
also study robust algorithms in the standard centralized setting, where an
adversary queries an algorithm in an adaptive manner, but the number of
interactions between the adversary and the algorithm is bounded. We first
recall a unified framework of [HKM+20, BKM+22, ACSS23] for answering $Q$
adaptive queries that incurs $\widetilde{\mathcal{O}}(\sqrt{Q})$ overhead in
space, which is roughly a quadratic improvement over the na\"{i}ve
implementation, and only incurs a logarithmic overhead in query time. Although
the general framework has diverse applications in machine learning and data
science, such as adaptive distance estimation, kernel density estimation,
linear regression, range queries, and point queries and serves as a preliminary
benchmark, we demonstrate even better algorithmic improvements for (1) reducing
the pre-processing time for adaptive distance estimation and (2) permitting an
unlimited number of adaptive queries for kernel density estimation. Finally, we
complement our theoretical results with additional empirical evaluations.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cherapanamjeri_Y/0/1/0/all/0/1">Yeshwanth Cherapanamjeri</a>, <a href="http://arxiv.org/find/cs/1/au:+Silwal_S/0/1/0/all/0/1">Sandeep Silwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1">David P. Woodruff</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fred Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qiuyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Samson Zhou</a></p><p>We study dynamic algorithms robust to adaptive input generated from sources
with bounded capabilities, such as sparsity or limited interaction. For
example, we consider robust linear algebraic algorithms when the updates to the
input are sparse but given by an adversary with access to a query oracle. We
also study robust algorithms in the standard centralized setting, where an
adversary queries an algorithm in an adaptive manner, but the number of
interactions between the adversary and the algorithm is bounded. We first
recall a unified framework of [HKM+20, BKM+22, ACSS23] for answering $Q$
adaptive queries that incurs $\widetilde{\mathcal{O}}(\sqrt{Q})$ overhead in
space, which is roughly a quadratic improvement over the na\"{i}ve
implementation, and only incurs a logarithmic overhead in query time. Although
the general framework has diverse applications in machine learning and data
science, such as adaptive distance estimation, kernel density estimation,
linear regression, range queries, and point queries and serves as a preliminary
benchmark, we demonstrate even better algorithmic improvements for (1) reducing
the pre-processing time for adaptive distance estimation and (2) permitting an
unlimited number of adaptive queries for kernel density estimation. Finally, we
complement our theoretical results with additional empirical evaluations.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-18T00:30:00Z">Tuesday, April 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.07537'>Gradient-less Federated Gradient Boosting Trees with Learnable Learning Rates</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Chenyang Ma, Xinchi Qiu, Daniel J. Beutel, Nicholas D. Lane</p><p>The privacy-sensitive nature of decentralized datasets and the robustness of
eXtreme Gradient Boosting (XGBoost) on tabular data raise the needs to train
XGBoost in the context of federated learning (FL). Existing works on federated
XGBoost in the horizontal setting rely on the sharing of gradients, which
induce per-node level communication frequency and serious privacy concerns. To
alleviate these problems, we develop an innovative framework for horizontal
federated XGBoost which does not depend on the sharing of gradients and
simultaneously boosts privacy and communication efficiency by making the
learning rates of the aggregated tree ensembles learnable. We conduct extensive
evaluations on various classification and regression datasets, showing our
approach achieves performance comparable to the state-of-the-art method and
effectively improves communication efficiency by lowering both communication
rounds and communication overhead by factors ranging from 25x to 700x.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chenyang Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xinchi Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Beutel_D/0/1/0/all/0/1">Daniel J. Beutel</a>, <a href="http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1">Nicholas D. Lane</a></p><p>The privacy-sensitive nature of decentralized datasets and the robustness of
eXtreme Gradient Boosting (XGBoost) on tabular data raise the needs to train
XGBoost in the context of federated learning (FL). Existing works on federated
XGBoost in the horizontal setting rely on the sharing of gradients, which
induce per-node level communication frequency and serious privacy concerns. To
alleviate these problems, we develop an innovative framework for horizontal
federated XGBoost which does not depend on the sharing of gradients and
simultaneously boosts privacy and communication efficiency by making the
learning rates of the aggregated tree ensembles learnable. We conduct extensive
evaluations on various classification and regression datasets, showing our
approach achieves performance comparable to the state-of-the-art method and
effectively improves communication efficiency by lowering both communication
rounds and communication overhead by factors ranging from 25x to 700x.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-18T00:30:00Z">Tuesday, April 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.07652'>Learned Interpolation for Better Streaming Quantile Approximation with Worst-Case Guarantees</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Nicholas Schiefer, Justin Y. Chen, Piotr Indyk, Shyam Narayanan, Sandeep Silwal, Tal Wagner</p><p>An $\varepsilon$-approximate quantile sketch over a stream of $n$ inputs
approximates the rank of any query point $q$ - that is, the number of input
points less than $q$ - up to an additive error of $\varepsilon n$, generally
with some probability of at least $1 - 1/\mathrm{poly}(n)$, while consuming
$o(n)$ space. While the celebrated KLL sketch of Karnin, Lang, and Liberty
achieves a provably optimal quantile approximation algorithm over worst-case
streams, the approximations it achieves in practice are often far from optimal.
Indeed, the most commonly used technique in practice is Dunning's t-digest,
which often achieves much better approximations than KLL on real-world data but
is known to have arbitrarily large errors in the worst case. We apply
interpolation techniques to the streaming quantiles problem to attempt to
achieve better approximations on real-world data sets than KLL while
maintaining similar guarantees in the worst case.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1">Nicholas Schiefer</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Justin Y. Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Indyk_P/0/1/0/all/0/1">Piotr Indyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1">Shyam Narayanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Silwal_S/0/1/0/all/0/1">Sandeep Silwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Wagner_T/0/1/0/all/0/1">Tal Wagner</a></p><p>An $\varepsilon$-approximate quantile sketch over a stream of $n$ inputs
approximates the rank of any query point $q$ - that is, the number of input
points less than $q$ - up to an additive error of $\varepsilon n$, generally
with some probability of at least $1 - 1/\mathrm{poly}(n)$, while consuming
$o(n)$ space. While the celebrated KLL sketch of Karnin, Lang, and Liberty
achieves a provably optimal quantile approximation algorithm over worst-case
streams, the approximations it achieves in practice are often far from optimal.
Indeed, the most commonly used technique in practice is Dunning's t-digest,
which often achieves much better approximations than KLL on real-world data but
is known to have arbitrarily large errors in the worst case. We apply
interpolation techniques to the streaming quantiles problem to attempt to
achieve better approximations on real-world data sets than KLL while
maintaining similar guarantees in the worst case.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-18T00:30:00Z">Tuesday, April 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.07674'>Thin trees for laminar families</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Nathan Klein, Neil Olver</p><p>In the laminar-constrained spanning tree problem, the goal is to find a
minimum-cost spanning tree which respects upper bounds on the number of times
each cut in a given laminar family is crossed. This generalizes the
well-studied degree-bounded spanning tree problem, as well as a previously
studied setting where a chain of cuts is given. We give the first
constant-factor approximation algorithm; in particular we show how to obtain a
multiplicative violation of the crossing bounds of less than 22 while losing
less than a factor of 5 in terms of cost.
</p>
<p>Our result compares to the natural LP relaxation. As a consequence, our
results show that given a $k$-edge-connected graph and a laminar family
$\mathcal{L} \subseteq 2^V$ of cuts, there exists a spanning tree which
contains only an $O(1/k)$ fraction of the edges across every cut in
$\mathcal{L}$. This can be viewed as progress towards the Thin Tree Conjecture,
which (in a strong form) states that this guarantee can be obtained for all
cuts simultaneously.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Klein_N/0/1/0/all/0/1">Nathan Klein</a>, <a href="http://arxiv.org/find/cs/1/au:+Olver_N/0/1/0/all/0/1">Neil Olver</a></p><p>In the laminar-constrained spanning tree problem, the goal is to find a
minimum-cost spanning tree which respects upper bounds on the number of times
each cut in a given laminar family is crossed. This generalizes the
well-studied degree-bounded spanning tree problem, as well as a previously
studied setting where a chain of cuts is given. We give the first
constant-factor approximation algorithm; in particular we show how to obtain a
multiplicative violation of the crossing bounds of less than 22 while losing
less than a factor of 5 in terms of cost.
</p>
<p>Our result compares to the natural LP relaxation. As a consequence, our
results show that given a $k$-edge-connected graph and a laminar family
$\mathcal{L} \subseteq 2^V$ of cuts, there exists a spanning tree which
contains only an $O(1/k)$ fraction of the edges across every cut in
$\mathcal{L}$. This can be viewed as progress towards the Thin Tree Conjecture,
which (in a strong form) states that this guarantee can be obtained for all
cuts simultaneously.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-18T00:30:00Z">Tuesday, April 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Monday, April 17
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=7243'>Will UT Austin and Texas A&#038;M survive beyond this week?</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          This week, the Texas Senate will take up SB 18, a bill to ban the granting of tenure at all public universities in Texas, including UT Austin and Texas A&#38;M. (Those of us who have tenure would retain it, for what little that’s worth.) [Update: I’ve learned that, even if this bill passes the Senate, [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>This week, the Texas Senate will take up <a href="https://capitol.texas.gov/tlodocs/88R/billtext/pdf/SB00018S.pdf">SB 18</a>, a bill to ban the granting of tenure at all public universities in Texas, including UT Austin and Texas A&amp;M.  (Those of us who have tenure would retain it, for what little that’s worth.)</p>



<p>[<strong><mark style="background-color:rgba(0, 0, 0, 0)" class="has-inline-color has-vivid-red-color">Update:</mark></strong> I’ve learned that, even if this bill passes the Senate, there’s a good chance that it will get watered down or die in the House, or found to be satisfied by UT’s existing system of post-tenure review.  That’s the only reason why people in the know aren’t panicking even more than they are.]</p>



<p>I find it hard to imagine that SB 18 will actually pass both houses and be enforced as written, simply because it&#8217;s obvious that if it did, it would be the end of UT Austin and Texas A&amp;M as leading research universities.  More precisely, it would be the immediate end of our ability to recruit competitively, and the slightly slower end of our competitiveness period, as faculty with options moved elsewhere.  This is so because of the economics of faculty hiring.  Particularly in STEM fields like computer science, those who become professors typically forgo <em>vastly</em> higher salaries in industry, not to mention equity in startup companies and so on.  Why would we do such a nutty thing?  Because we like a certain lifestyle.  We&#8217;re willing to move several economic strata downward in return for jobs where (in principle) no one can fire us without cause, or tell us what we&#8217;re allowed to say or publish.  The evidence from industry labs (Google, Facebook, Microsoft, etc.) suggests that, in competitive fields, for Texas to attract and retain top faculty <em>without</em> tenure would require paying them hundreds of thousands more per year.  In that sense, tenure is a <em>bargain</em> for universities and the state.  Of course the situation is different for art history and English literature, but in any case SB 18 makes no distinction between fields.</p>



<p>The Texas Senate is considering two other bills this week: <a href="https://capitol.texas.gov/tlodocs/88R/billtext/pdf/SB00017S.pdf">SB 17</a>, which would ban all DEI (Diversity, Equity, and Inclusion) programs, offices, and practices at public universities, and <a href="https://capitol.texas.gov/tlodocs/88R/billtext/pdf/SB00016E.pdf">SB 16</a>, which would require the firing of any professor if they &#8220;compel or attempt to compel a student &#8230; to adopt a belief that any race, sex, or ethnicity or social, political, or religious belief is inherently superior to any other race, sex, ethnicity, or belief.&#8221;  (The language here seems sloppy to me: is liberal democracy &#8220;inherently superior&#8221; to Nazism?  Would teaching students about the horrors of Nazism count as &#8220;attempting to compel them&#8221; to accept this superiority?)</p>



<p>Taken together, it&#8217;s clear that the goal is to hit back hard against &#8220;wokeness&#8221; in academia, and thereby satisfy the Republican base.</p>



<p>Here&#8217;s the thing: there <em>really is</em> an illiberal ideology that&#8217;s taken over parts of academia (not all of it)&#8212;an ideology that Tim Urban, in his wonderful recent book <a href="https://www.amazon.com/Whats-Our-Problem-Self-Help-Societies-ebook/dp/B0BTJCTR58"><em>What&#8217;s Our Problem?</em></a>, usefully terms &#8220;Social Justice Fundamentalism&#8221; or SJF, to distinguish it sharply from &#8220;Liberal Social Justice,&#8221; the ideology of (for example) the Civil Rights movement.  Now, I&#8217;m on record as <strong>not a fan</strong> of the SJF ideology, to put it mildly, and the SJF ideology is on record as not a fan of me.  In 2015, I was infamously dragged through the mud of <em>Salon</em>, <em>The New Republic</em>, <em>Raw Story</em>, and many other magazines and websites for a single blog comment criticizing a form of feminism that had contributed to making my life miserable, even while I proudly called myself a liberal feminist (and still do).  More recently, wokesters have written to my department chair trying to get me disciplined or fired, for everything from my use of the now-verboten term &#8220;quantum supremacy,&#8221; to a reference to female breasts in a poem I wrote as a student that was still on my homepage.  (These attempts thankfully went nowhere.  Notwithstanding what you read, sanity retains many strongholds in academia.)</p>



<p>Anyway, despite all of this, the Texas Republicans have somehow succeeded in making me more afraid of <em>them</em>, purely on the level of professional survival, than I&#8217;ve ever been of the Social Justice Fundamentalists.  In effect, the Republicans propose to solve the &#8220;problem of wokeness&#8221; by simply dropping thermonuclear weapons on all Texas public universities, thereby taking out me and my colleagues as collateral damage&#8212;regardless of our own views on wokeness or anything else, and regardless of what we&#8217;re doing for Texas&#8217; scientific competitiveness.</p>



<p>I don&#8217;t expect that most of my readers, in or out of Texas, will need to be persuaded about any of this&#8212;nor am I expecting to change many minds on the other side.  Mostly, I&#8217;m writing this post in the hope that some well-connected moderates here in Austin will link to it, and the post might thereby play a tiny role in helping Texas&#8217; first-rate public universities live one more day.  (And to any such moderates: <strong>yes</strong>, I&#8217;m happy to meet in person with you or your colleagues, if that would help!)  Some posts are here on this blog for no better reason than, y&#8217;know, moral obligation.</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-17T23:58:01Z">Monday, April 17 2023, 23:58</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://decentralizedthoughts.github.io/2023-04-17-blockchainsplustees-day2-summary/'>Blockchains + TEEs Day 2 Summary</a></h3>
        <p class='tr-article-feed'>from <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          This is the second of the two part post on the workshop on Blockchains + TEEs that concluded last week. Here are the key ideas from Day 2. You can find the post summarizing Day 1 here. Nick Hynes: Practical Secure Decentralized Computing Nick discussed the use of TEEs by...
        
        </div>

        <div class='tr-article-summary'>
        
          
          This is the second of the two part post on the workshop on Blockchains + TEEs that concluded last week. Here are the key ideas from Day 2. You can find the post summarizing Day 1 here. Nick Hynes: Practical Secure Decentralized Computing Nick discussed the use of TEEs by...
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-17T05:00:00Z">Monday, April 17 2023, 05:00</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://rjlipton.wpcomstaging.com/2023/04/16/schoolbook-error-discovery/'>Schoolbook Error/Discovery</a></h3>
        <p class='tr-article-feed'>from <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Liam Squires is a fifth-grader at Virginia&#8217;s HM Pearson Elementary. He is now famous&#8212;after a story that is in print in today&#8217;s Sunday New York Times. Squires saw that the diagrams of igneous and sedimentary rocks had the wrong labels. The book was a &#8220;Level 5&#8221; tome encompassing Earth Science. The book was approved by [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>
Liam Squires is a fifth-grader at Virginia&#8217;s HM Pearson Elementary. He is now famous&#8212;after a <a href="https://www.nytimes.com/2023/04/12/us/student-textbook-mistake.html?searchResultPosition=2">story</a> that is in print in today&#8217;s Sunday New York Times. Squires saw that the diagrams of igneous and sedimentary rocks had the wrong labels.</p>
<p><P><br />
<a href="https://rjlipton.wpcomstaging.com/2023/04/16/schoolbook-error-discovery/kid/" rel="attachment wp-att-21466"><img data-attachment-id="21466" data-permalink="https://rjlipton.wpcomstaging.com/2023/04/16/schoolbook-error-discovery/kid/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/kid.jpg?fit=634%2C806&amp;ssl=1" data-orig-size="634,806" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="kid" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/kid.jpg?fit=236%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/kid.jpg?fit=600%2C763&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/kid.jpg?resize=236%2C300&#038;ssl=1" alt="" width="236" height="300" class="aligncenter size-medium wp-image-21466" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/kid.jpg?resize=236%2C300&amp;ssl=1 236w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/kid.jpg?w=634&amp;ssl=1 634w" sizes="(max-width: 236px) 100vw, 236px" data-recalc-dims="1" /></a></p>
<p><P><br />
The book was a &#8220;Level 5&#8221; tome encompassing Earth Science. The book was approved by the school district back in 2015&#8212;but a diagram mistake had managed to creep through. Noticing the pictures of igneous and sedimentary rocks were miscaptioned, Squires told his teacher, who told the school, who then told Five Ponds Press. Who sent a wonderful personal letter back to Squires.</p>
<p>
Anthony Picciano&#8217;s blog has further <a href="https://apicciano.commons.gc.cuny.edu/2023/04/13/liam-squires-virginia-fifth-grader-is-celebrated-for-spotting-error-in-science-textbook/">details</a> of the story.</p>
<p><P><br />
<a href="https://rjlipton.wpcomstaging.com/2023/04/16/schoolbook-error-discovery/textbook/" rel="attachment wp-att-21467"><img data-attachment-id="21467" data-permalink="https://rjlipton.wpcomstaging.com/2023/04/16/schoolbook-error-discovery/textbook/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/textbook.jpg?fit=600%2C600&amp;ssl=1" data-orig-size="600,600" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="textbook" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/textbook.jpg?fit=300%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/textbook.jpg?fit=600%2C600&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/textbook.jpg?resize=400%2C400&#038;ssl=1" alt="" width="400" height="400" class="aligncenter wp-image-21467" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/textbook.jpg?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/textbook.jpg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/textbook.jpg?resize=400%2C400&amp;ssl=1 400w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/textbook.jpg?resize=200%2C200&amp;ssl=1 200w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/04/textbook.jpg?w=600&amp;ssl=1 600w" sizes="(max-width: 400px) 100vw, 400px" data-recalc-dims="1" /></a></p>
<p><P></p>
<p><H2> Open Problems </H2></p>
<p><p>
Let&#8217;s send Squires some &#8220;proofs&#8221; for checking? Would he catch that out of three references to the &#8220;Bell basis&#8221; in my <a href="https://mitpress.mit.edu/9780262045254/introduction-to-quantum-algorithms-via-linear-algebra/">textbook</a> with Ken, one of them is incorrect (should be &#8220;Hadamard basis&#8221;)? How about the next P=NP proof? </p>
<p>
<p class="authors">By rjlipton</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-17T02:36:23Z">Monday, April 17 2023, 02:36</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.07227'>On representations of real numbers and the computational complexity of converting between such representations</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Amir M. Ben-Amram, Lars Kristiansen, Jakob Grue Simonsen</p><p>We study the computational complexity of converting one representation of
real numbers into another representation. Typical examples of representations
are Cauchy sequences, base-10 expansions, Dedekind cuts and continued
fractions.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Ben_Amram_A/0/1/0/all/0/1">Amir M. Ben-Amram</a>, <a href="http://arxiv.org/find/math/1/au:+Kristiansen_L/0/1/0/all/0/1">Lars Kristiansen</a>, <a href="http://arxiv.org/find/math/1/au:+Simonsen_J/0/1/0/all/0/1">Jakob Grue Simonsen</a></p><p>We study the computational complexity of converting one representation of
real numbers into another representation. Typical examples of representations
are Cauchy sequences, base-10 expansions, Dedekind cuts and continued
fractions.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-17T00:30:00Z">Monday, April 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.06780'>Online Geometric Hitting Set and Set Cover Beyond Unit Balls in $\mathbb{R}^2$</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Minati De, Ratnadip Mandal, Satyam Singh</p><p>We investigate the geometric hitting set problem in the online setup for the
range space $\Sigma=({\cal P},{\cal S})$, where the set $\P\subset\mathbb{R}^2$
is a collection of $n$ points and the set $\cal S$ is a family of geometric
objects in $\mathbb{R}^2$. In the online setting, the geometric objects arrive
one by one. Upon the arrival of an object, an online algorithm must maintain a
valid hitting set by making an irreversible decision, i.e., once a point is
added to the hitting set by the algorithm, it can not be deleted in the future.
The objective of the geometric hitting set problem is to find a hitting set of
the minimum cardinality. Even and Smorodinsky (Discret. Appl. Math., 2014)
considered an online model (Model-I) in which the range space $\Sigma$ is known
in advance, but the order of arrival of the input objects in $\cal S$ is
unknown. They proposed online algorithms having optimal competitive ratios of
$\Theta(\log n)$ for intervals, half-planes and unit disks in $\mathbb{R}^2$.
Whether such an algorithm exists for unit squares remained open for a long
time. This paper considers an online model (Model-II) in which the entire range
space $\Sigma$ is not known in advance. We only know the set $\cal P$ but not
the set $\cal S$ in advance. Note that any algorithm for Model-II will also
work for Model-I, but not vice-versa. In Model-II, we obtain an optimal
competitive ratio of $\Theta(\log(n))$ for unit disks and regular $k$-gon with
$k\geq 4$ in $\mathbb{R}^2$. All the above-mentioned results also hold for the
equivalent geometric set cover problem in Model-II.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+De_M/0/1/0/all/0/1">Minati De</a>, <a href="http://arxiv.org/find/cs/1/au:+Mandal_R/0/1/0/all/0/1">Ratnadip Mandal</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Satyam Singh</a></p><p>We investigate the geometric hitting set problem in the online setup for the
range space $\Sigma=({\cal P},{\cal S})$, where the set $\P\subset\mathbb{R}^2$
is a collection of $n$ points and the set $\cal S$ is a family of geometric
objects in $\mathbb{R}^2$. In the online setting, the geometric objects arrive
one by one. Upon the arrival of an object, an online algorithm must maintain a
valid hitting set by making an irreversible decision, i.e., once a point is
added to the hitting set by the algorithm, it can not be deleted in the future.
The objective of the geometric hitting set problem is to find a hitting set of
the minimum cardinality. Even and Smorodinsky (Discret. Appl. Math., 2014)
considered an online model (Model-I) in which the range space $\Sigma$ is known
in advance, but the order of arrival of the input objects in $\cal S$ is
unknown. They proposed online algorithms having optimal competitive ratios of
$\Theta(\log n)$ for intervals, half-planes and unit disks in $\mathbb{R}^2$.
Whether such an algorithm exists for unit squares remained open for a long
time. This paper considers an online model (Model-II) in which the entire range
space $\Sigma$ is not known in advance. We only know the set $\cal P$ but not
the set $\cal S$ in advance. Note that any algorithm for Model-II will also
work for Model-I, but not vice-versa. In Model-II, we obtain an optimal
competitive ratio of $\Theta(\log(n))$ for unit disks and regular $k$-gon with
$k\geq 4$ in $\mathbb{R}^2$. All the above-mentioned results also hold for the
equivalent geometric set cover problem in Model-II.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-17T00:30:00Z">Monday, April 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.06862'>The Longest Subsequence-Repeated Subsequence Problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mahuel Lafond, Wenfeng Lai, Adiesha Liyanage, Binhai Zhu</p><p>Motivated by computing duplication patterns in sequences, a new fundamental
problem called the longest subsequence-repeated subsequence (LSRS) is proposed.
Given a sequence $S$ of length $n$, a letter-repeated subsequence is a
subsequence of $S$ in the form of $x_1^{d_1}x_2^{d_2}\cdots x_k^{d_k}$ with
$x_i$ a subsequence of $S$, $x_j\neq x_{j+1}$ and $d_i\geq 2$ for all $i$ in
$[k]$ and $j$ in $[k-1]$. We first present an $O(n^6)$ time algorithm to
compute the longest cubic subsequences of all the $O(n^2)$ substrings of $S$,
improving the trivial $O(n^7)$ bound. Then, an $O(n^6)$ time algorithm for
computing the longest subsequence-repeated subsequence (LSRS) of $S$ is
obtained. Finally we focus on two variants of this problem. We first consider
the constrained version when $\Sigma$ is unbounded, each letter appears in $S$
at most $d$ times and all the letters in $\Sigma$ must appear in the solution.
We show that the problem is NP-hard for $d=4$, via a reduction from a special
version of SAT (which is obtained from 3-COLORING). We then show that when each
letter appears in $S$ at most $d=3$ times, then the problem is solvable in
$O(n^5)$ time.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lafond_M/0/1/0/all/0/1">Mahuel Lafond</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1">Wenfeng Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liyanage_A/0/1/0/all/0/1">Adiesha Liyanage</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Binhai Zhu</a></p><p>Motivated by computing duplication patterns in sequences, a new fundamental
problem called the longest subsequence-repeated subsequence (LSRS) is proposed.
Given a sequence $S$ of length $n$, a letter-repeated subsequence is a
subsequence of $S$ in the form of $x_1^{d_1}x_2^{d_2}\cdots x_k^{d_k}$ with
$x_i$ a subsequence of $S$, $x_j\neq x_{j+1}$ and $d_i\geq 2$ for all $i$ in
$[k]$ and $j$ in $[k-1]$. We first present an $O(n^6)$ time algorithm to
compute the longest cubic subsequences of all the $O(n^2)$ substrings of $S$,
improving the trivial $O(n^7)$ bound. Then, an $O(n^6)$ time algorithm for
computing the longest subsequence-repeated subsequence (LSRS) of $S$ is
obtained. Finally we focus on two variants of this problem. We first consider
the constrained version when $\Sigma$ is unbounded, each letter appears in $S$
at most $d$ times and all the letters in $\Sigma$ must appear in the solution.
We show that the problem is NP-hard for $d=4$, via a reduction from a special
version of SAT (which is obtained from 3-COLORING). We then show that when each
letter appears in $S$ at most $d=3$ times, then the problem is solvable in
$O(n^5)$ time.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-17T00:30:00Z">Monday, April 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.07107'>Near Tight Shortest Paths in the Hybrid Model</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Philipp Schneider</p><p>Shortest paths problems are subject to extensive studies in classic
distributed models such as the CONGEST or congested clique, which describe the
way in which nodes may communicate in order to solve such a problem. %where
nodes initially know only the distance to their neighbors in some graph and
must compute distances to any other node with as few communication rounds as
possible. This article focuses on hybrid networks, which give nodes access to
multiple, different modes of communication, in particular the HYBRID model
which combines unrestricted local communication along edges of the input graph
alongside heavily restricted global communication between arbitrary pairs of
nodes.
</p>
<p>Previous work [Augustine et al, SODA'20, Kuhn et al. PODC'20] showed that
each node learning its distance to $k$ dedicated source nodes (aka the $k$-SSP
problem) takes at least $\tilOm(\!\sqrt{k})$ rounds in the HYBRID model, even
for polynomial approximations. This lower bound was matched with algorithmic
solutions for $k \geq n^{2/3}$. However, as $k$ gets smaller, the gap between
the known upper and lower bounds diverges and even becomes exponential for the
single source shortest paths problem (SSSP). In this work we plug this gap for
the whole range of $k$ (up to terms that are polylogarithmic in $n$), by giving
algorithmic solutions for $k$-SSP in $\tilO\big(\!\sqrt k\big)$ rounds for any
$k$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1">Philipp Schneider</a></p><p>Shortest paths problems are subject to extensive studies in classic
distributed models such as the CONGEST or congested clique, which describe the
way in which nodes may communicate in order to solve such a problem. %where
nodes initially know only the distance to their neighbors in some graph and
must compute distances to any other node with as few communication rounds as
possible. This article focuses on hybrid networks, which give nodes access to
multiple, different modes of communication, in particular the HYBRID model
which combines unrestricted local communication along edges of the input graph
alongside heavily restricted global communication between arbitrary pairs of
nodes.
</p>
<p>Previous work [Augustine et al, SODA'20, Kuhn et al. PODC'20] showed that
each node learning its distance to $k$ dedicated source nodes (aka the $k$-SSP
problem) takes at least $\tilOm(\!\sqrt{k})$ rounds in the HYBRID model, even
for polynomial approximations. This lower bound was matched with algorithmic
solutions for $k \geq n^{2/3}$. However, as $k$ gets smaller, the gap between
the known upper and lower bounds diverges and even becomes exponential for the
single source shortest paths problem (SSSP). In this work we plug this gap for
the whole range of $k$ (up to terms that are polylogarithmic in $n$), by giving
algorithmic solutions for $k$-SSP in $\tilO\big(\!\sqrt k\big)$ rounds for any
$k$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-17T00:30:00Z">Monday, April 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.07284'>Solving Unique Games over Globally Hypercontractive Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mitali Bafna, Dor Minzer</p><p>We study the complexity of affine Unique-Games (UG) over globally
hypercontractive graphs, which are graphs that are not small set expanders but
admit a useful and succinct characterization of all small sets that violate the
small-set expansion property. This class of graphs includes the Johnson and
Grassmann graphs, which have played a pivotal role in recent PCP constructions
for UG, and their generalizations via high-dimensional expanders.
</p>
<p>Our algorithm shows how to round "low-entropy" solutions to sum-of-squares
(SoS) semidefinite programs, broadly extending the algorithmic framework of
[BBKSS'21]. We give a new rounding scheme for SoS, which eliminates global
correlations in a given pseudodistribution so that it retains various good
properties even after conditioning. Getting structural control over a
pseudodistribution after conditioning is a fundamental challenge in many SoS
based algorithms. Due to these challenges, [BBKSS] were not able to establish
strong algorithms for globally hypercontractive graphs, and could only do so
for certifiable small-set expanders. Our results improve upon the results of
[BBKSS] in various aspects: we are able to deal with instances with arbitrarily
small (but constant) completeness, and most importantly, their algorithm gets a
soundness guarantee that degrades with other parameters of the graph (which in
all PCP constructions grow with the alphabet size), whereas our doesn't.
</p>
<p>Our result suggests that UG is easy on globally hypercontractive graphs, and
therefore highlights the importance of graphs that lack such a characterization
in the context of PCP reductions for UG.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bafna_M/0/1/0/all/0/1">Mitali Bafna</a>, <a href="http://arxiv.org/find/cs/1/au:+Minzer_D/0/1/0/all/0/1">Dor Minzer</a></p><p>We study the complexity of affine Unique-Games (UG) over globally
hypercontractive graphs, which are graphs that are not small set expanders but
admit a useful and succinct characterization of all small sets that violate the
small-set expansion property. This class of graphs includes the Johnson and
Grassmann graphs, which have played a pivotal role in recent PCP constructions
for UG, and their generalizations via high-dimensional expanders.
</p>
<p>Our algorithm shows how to round "low-entropy" solutions to sum-of-squares
(SoS) semidefinite programs, broadly extending the algorithmic framework of
[BBKSS'21]. We give a new rounding scheme for SoS, which eliminates global
correlations in a given pseudodistribution so that it retains various good
properties even after conditioning. Getting structural control over a
pseudodistribution after conditioning is a fundamental challenge in many SoS
based algorithms. Due to these challenges, [BBKSS] were not able to establish
strong algorithms for globally hypercontractive graphs, and could only do so
for certifiable small-set expanders. Our results improve upon the results of
[BBKSS] in various aspects: we are able to deal with instances with arbitrarily
small (but constant) completeness, and most importantly, their algorithm gets a
soundness guarantee that degrades with other parameters of the graph (which in
all PCP constructions grow with the alphabet size), whereas our doesn't.
</p>
<p>Our result suggests that UG is easy on globally hypercontractive graphs, and
therefore highlights the importance of graphs that lack such a characterization
in the context of PCP reductions for UG.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-17T00:30:00Z">Monday, April 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.07274'>Beyond Planarity: A Spring-Based Approach</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Simon van Wageningen, Tamara Mchedlidze, Alexandru Telea</p><p>Planar drawings of graphs tend to be favored over non-planar drawings.
Testing planarity and creating a planar layout of a planar graph can be done in
linear time. However, creating readable drawings of nearly planar graphs
remains a challenge. We therefore seek to answer which edges of nearly planar
graphs create clutter in their drawings generated by mainstream graph drawing
algorithms. We present a heuristic to identify problematic edges in nearly
planar graphs and adjust their weights in order to produce higher quality
layouts with spring-based drawing algorithms. Our experiments show that our
heuristic produces significantly higher quality drawings for augmented grid
graphs, augmented triangulations, and deep triangulations.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Wageningen_S/0/1/0/all/0/1">Simon van Wageningen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mchedlidze_T/0/1/0/all/0/1">Tamara Mchedlidze</a>, <a href="http://arxiv.org/find/cs/1/au:+Telea_A/0/1/0/all/0/1">Alexandru Telea</a></p><p>Planar drawings of graphs tend to be favored over non-planar drawings.
Testing planarity and creating a planar layout of a planar graph can be done in
linear time. However, creating readable drawings of nearly planar graphs
remains a challenge. We therefore seek to answer which edges of nearly planar
graphs create clutter in their drawings generated by mainstream graph drawing
algorithms. We present a heuristic to identify problematic edges in nearly
planar graphs and adjust their weights in order to produce higher quality
layouts with spring-based drawing algorithms. Our experiments show that our
heuristic produces significantly higher quality drawings for augmented grid
graphs, augmented triangulations, and deep triangulations.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-17T00:30:00Z">Monday, April 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.06733'>Near-Optimal Degree Testing for Bayes Nets</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Vipul Arora, Arnab Bhattacharyya, Cl&#xe9;ment L. Canonne, Joy Qiping Yang</p><p>This paper considers the problem of testing the maximum in-degree of the
Bayes net underlying an unknown probability distribution $P$ over $\{0,1\}^n$,
given sample access to $P$. We show that the sample complexity of the problem
is $\tilde{\Theta}(2^{n/2}/\varepsilon^2)$. Our algorithm relies on a
testing-by-learning framework, previously used to obtain sample-optimal
testers; in order to apply this framework, we develop new algorithms for
``near-proper'' learning of Bayes nets, and high-probability learning under
$\chi^2$ divergence, which are of independent interest.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Arora_V/0/1/0/all/0/1">Vipul Arora</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1">Arnab Bhattacharyya</a>, <a href="http://arxiv.org/find/cs/1/au:+Canonne_C/0/1/0/all/0/1">Cl&#xe9;ment L. Canonne</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Joy Qiping Yang</a></p><p>This paper considers the problem of testing the maximum in-degree of the
Bayes net underlying an unknown probability distribution $P$ over $\{0,1\}^n$,
given sample access to $P$. We show that the sample complexity of the problem
is $\tilde{\Theta}(2^{n/2}/\varepsilon^2)$. Our algorithm relies on a
testing-by-learning framework, previously used to obtain sample-optimal
testers; in order to apply this framework, we develop new algorithms for
``near-proper'' learning of Bayes nets, and high-probability learning under
$\chi^2$ divergence, which are of independent interest.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-17T00:30:00Z">Monday, April 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.06787'>A Polynomial Time, Pure Differentially Private Estimator for Binary Product Distributions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Vikrant Singhal</p><p>We present the first $\varepsilon$-differentially private, computationally
efficient algorithm that estimates the means of product distributions over
$\{0,1\}^d$ accurately in total-variation distance, whilst attaining the
optimal sample complexity to within polylogarithmic factors. The prior work had
either solved this problem efficiently and optimally under weaker notions of
privacy, or had solved it optimally while having exponential running times.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Singhal_V/0/1/0/all/0/1">Vikrant Singhal</a></p><p>We present the first $\varepsilon$-differentially private, computationally
efficient algorithm that estimates the means of product distributions over
$\{0,1\}^d$ accurately in total-variation distance, whilst attaining the
optimal sample complexity to within polylogarithmic factors. The prior work had
either solved this problem efficiently and optimally under weaker notions of
privacy, or had solved it optimally while having exponential running times.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-17T00:30:00Z">Monday, April 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.06853'>Pseudorandom Hashing for Space-bounded Computation with Applications in Streaming</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Praneeth Kacham, Rasmus Pagh, Mikkel Thorup, David P. Woodruff</p><p>We revisit Nisan's classical pseudorandom generator (PRG) for space-bounded
computation (STOC 1990) and its applications in streaming algorithms. We
describe a new generator, HashPRG, that can be thought of as a symmetric
version of Nisan's generator over larger alphabets. Our generator allows a
trade-off between seed length and the time needed to compute a given block of
the generator's output. HashPRG can be used to obtain derandomizations with
much better update time and \emph{without sacrificing space} for a large number
of data stream algorithms, such as $F_p$ estimation in the parameter regimes $p
&gt; 2$ and $0 &lt; p &lt; 2$ and CountSketch with tight estimation guarantees as
analyzed by Minton and Price (SODA 2014) which assumed access to a random
oracle. We also show a recent analysis of Private CountSketch can be
derandomized using our techniques.
</p>
<p>For a $d$-dimensional vector $x$ being updated in a turnstile stream, we show
that $\|x\|_{\infty}$ can be estimated up to an additive error of
$\varepsilon\|x\|_{2}$ using $O(\varepsilon^{-2}\log(1/\varepsilon)\log d)$
bits of space. Additionally, the update time of this algorithm is $O(\log
1/\varepsilon)$ in the Word RAM model. We show that the space complexity of
this algorithm is optimal up to constant factors. However, for vectors $x$ with
$\|x\|_{\infty} = \Theta(\|x\|_{2})$, we show that the lower bound can be
broken by giving an algorithm that uses $O(\varepsilon^{-2}\log d)$ bits of
space which approximates $\|x\|_{\infty}$ up to an additive error of
$\varepsilon\|x\|_{2}$. We use our aforementioned derandomization of the
CountSketch data structure to obtain this algorithm, and using the time-space
trade off of HashPRG, we show that the update time of this algorithm is also
$O(\log 1/\varepsilon)$ in the Word RAM model.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kacham_P/0/1/0/all/0/1">Praneeth Kacham</a>, <a href="http://arxiv.org/find/cs/1/au:+Pagh_R/0/1/0/all/0/1">Rasmus Pagh</a>, <a href="http://arxiv.org/find/cs/1/au:+Thorup_M/0/1/0/all/0/1">Mikkel Thorup</a>, <a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1">David P. Woodruff</a></p><p>We revisit Nisan's classical pseudorandom generator (PRG) for space-bounded
computation (STOC 1990) and its applications in streaming algorithms. We
describe a new generator, HashPRG, that can be thought of as a symmetric
version of Nisan's generator over larger alphabets. Our generator allows a
trade-off between seed length and the time needed to compute a given block of
the generator's output. HashPRG can be used to obtain derandomizations with
much better update time and \emph{without sacrificing space} for a large number
of data stream algorithms, such as $F_p$ estimation in the parameter regimes $p
&gt; 2$ and $0 &lt; p &lt; 2$ and CountSketch with tight estimation guarantees as
analyzed by Minton and Price (SODA 2014) which assumed access to a random
oracle. We also show a recent analysis of Private CountSketch can be
derandomized using our techniques.
</p>
<p>For a $d$-dimensional vector $x$ being updated in a turnstile stream, we show
that $\|x\|_{\infty}$ can be estimated up to an additive error of
$\varepsilon\|x\|_{2}$ using $O(\varepsilon^{-2}\log(1/\varepsilon)\log d)$
bits of space. Additionally, the update time of this algorithm is $O(\log
1/\varepsilon)$ in the Word RAM model. We show that the space complexity of
this algorithm is optimal up to constant factors. However, for vectors $x$ with
$\|x\|_{\infty} = \Theta(\|x\|_{2})$, we show that the lower bound can be
broken by giving an algorithm that uses $O(\varepsilon^{-2}\log d)$ bits of
space which approximates $\|x\|_{\infty}$ up to an additive error of
$\varepsilon\|x\|_{2}$. We use our aforementioned derandomization of the
CountSketch data structure to obtain this algorithm, and using the time-space
trade off of HashPRG, we show that the update time of this algorithm is also
$O(\log 1/\varepsilon)$ in the Word RAM model.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-17T00:30:00Z">Monday, April 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.07109'>Optimal Uncoordinated Unique IDs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Peter C. Dillinger, Mart&#xed;n Farach-Colton, Guido Tagliavini, Stefan Walzer</p><p>In the Uncoordinated Unique Identifiers Problem (UUIDP) there are $n$
independent instances of an algorithm $\mathcal{A}$ that generates IDs from a
universe $\{1, \dots, m\}$, and there is an adversary that requests IDs from
these instances. The goal is to design $\mathcal{A}$ such that it minimizes the
probability that the same ID is ever generated twice across all instances, that
is, minimizes the collision probability. Crucially, no communication between
the instances of $\mathcal{A}$ is possible. Solutions to the UUIDP are often
used as mechanisms for surrogate key generation in distributed databases and
key-value stores. In spite of its practical relevance, we know of no prior
theoretical work on the UUIDP.
</p>
<p>In this paper we initiate the systematic study of the UUIDP. We analyze both
existing and novel algorithms for this problem, and evaluate their collision
probability using worst-case analysis and competitive analysis, against
oblivious and adaptive adversaries. In particular, we present an algorithm that
is optimal in the worst case against oblivious adversaries, an algorithm that
is at most a logarithmic factor away from optimal in the worst case against
adaptive adversaries, and an algorithm that is optimal in the competitive sense
against both oblivious and adaptive adversaries.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dillinger_P/0/1/0/all/0/1">Peter C. Dillinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Farach_Colton_M/0/1/0/all/0/1">Mart&#xed;n Farach-Colton</a>, <a href="http://arxiv.org/find/cs/1/au:+Tagliavini_G/0/1/0/all/0/1">Guido Tagliavini</a>, <a href="http://arxiv.org/find/cs/1/au:+Walzer_S/0/1/0/all/0/1">Stefan Walzer</a></p><p>In the Uncoordinated Unique Identifiers Problem (UUIDP) there are $n$
independent instances of an algorithm $\mathcal{A}$ that generates IDs from a
universe $\{1, \dots, m\}$, and there is an adversary that requests IDs from
these instances. The goal is to design $\mathcal{A}$ such that it minimizes the
probability that the same ID is ever generated twice across all instances, that
is, minimizes the collision probability. Crucially, no communication between
the instances of $\mathcal{A}$ is possible. Solutions to the UUIDP are often
used as mechanisms for surrogate key generation in distributed databases and
key-value stores. In spite of its practical relevance, we know of no prior
theoretical work on the UUIDP.
</p>
<p>In this paper we initiate the systematic study of the UUIDP. We analyze both
existing and novel algorithms for this problem, and evaluate their collision
probability using worst-case analysis and competitive analysis, against
oblivious and adaptive adversaries. In particular, we present an algorithm that
is optimal in the worst case against oblivious adversaries, an algorithm that
is at most a logarithmic factor away from optimal in the worst case against
adaptive adversaries, and an algorithm that is optimal in the competitive sense
against both oblivious and adaptive adversaries.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-17T00:30:00Z">Monday, April 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.07268'>Planar and Minor-Free Metrics Embed into Metrics of Polylogarithmic Treewidth with Expected Multiplicative Distortion Arbitrarily Close to 1</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Vincent Cohen-Addad, Hung Le, Marcin Pilipczuk, Micha&#x142; Pilipczuk</p><p>We prove that there is a randomized polynomial-time algorithm that given an
edge-weighted graph $G$ excluding a fixed-minor $Q$ on $n$ vertices and an
accuracy parameter $\varepsilon&gt;0$, constructs an edge-weighted graph~$H$ and
an embedding $\eta\colon V(G)\to V(H)$ with the following properties: * For any
constant size $Q$, the treewidth of $H$ is polynomial in $\varepsilon^{-1}$,
$\log n$, and the logarithm of the stretch of the distance metric in $G$. * The
expected multiplicative distortion is $(1+\varepsilon)$: for every pair of
vertices $u,v$ of $G$, we have $\mathrm{dist}_H(\eta(u),\eta(v))\geq
\mathrm{dist}_G(u,v)$ always and
$\mathrm{Exp}[\mathrm{dist}_H(\eta(u),\eta(v))]\leq
(1+\varepsilon)\mathrm{dist}_G(u,v)$.
</p>
<p>Our embedding is the first to achieve polylogarithmic treewidth of the host
graph and comes close to the lower bound by Carroll and Goel, who showed that
any embedding of a planar graph with $\mathcal{O}(1)$ expected distortion
requires the host graph to have treewidth $\Omega(\log n)$. It also provides a
unified framework for obtaining randomized quasi-polynomial-time approximation
schemes for a variety of problems including network design, clustering or
routing problems, in minor-free metrics where the optimization goal is the sum
of selected distances. Applications include the capacitated vehicle routing
problem, and capacitated clustering problems.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cohen_Addad_V/0/1/0/all/0/1">Vincent Cohen-Addad</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1">Hung Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Pilipczuk_M/0/1/0/all/0/1">Marcin Pilipczuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Pilipczuk_M/0/1/0/all/0/1">Micha&#x142; Pilipczuk</a></p><p>We prove that there is a randomized polynomial-time algorithm that given an
edge-weighted graph $G$ excluding a fixed-minor $Q$ on $n$ vertices and an
accuracy parameter $\varepsilon&gt;0$, constructs an edge-weighted graph~$H$ and
an embedding $\eta\colon V(G)\to V(H)$ with the following properties: * For any
constant size $Q$, the treewidth of $H$ is polynomial in $\varepsilon^{-1}$,
$\log n$, and the logarithm of the stretch of the distance metric in $G$. * The
expected multiplicative distortion is $(1+\varepsilon)$: for every pair of
vertices $u,v$ of $G$, we have $\mathrm{dist}_H(\eta(u),\eta(v))\geq
\mathrm{dist}_G(u,v)$ always and
$\mathrm{Exp}[\mathrm{dist}_H(\eta(u),\eta(v))]\leq
(1+\varepsilon)\mathrm{dist}_G(u,v)$.
</p>
<p>Our embedding is the first to achieve polylogarithmic treewidth of the host
graph and comes close to the lower bound by Carroll and Goel, who showed that
any embedding of a planar graph with $\mathcal{O}(1)$ expected distortion
requires the host graph to have treewidth $\Omega(\log n)$. It also provides a
unified framework for obtaining randomized quasi-polynomial-time approximation
schemes for a variety of problems including network design, clustering or
routing problems, in minor-free metrics where the optimization goal is the sum
of selected distances. Applications include the capacitated vehicle routing
problem, and capacitated clustering problems.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-17T00:30:00Z">Monday, April 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Sunday, April 16
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=7230'>AI safety: what should actually be done now?</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          So, I recorded a 2.5-hour-long podcast with Daniel Filan about &#8220;reform AI alignment,&#8221; and the work I’ve been doing this year at OpenAI.  The end result is &#8230; well, probably closer to my current views on this subject than anything else I&#8217;ve said or written! Listen here or read the transcript here. Here&#8217;s Daniel&#8217;s abstract: [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>So, I recorded a 2.5-hour-long podcast with <a href="https://danielfilan.com/">Daniel Filan</a> about &#8220;reform AI alignment,&#8221; and the work I’ve been doing this year at OpenAI.  The end result is &#8230; well, probably closer to my current views on this subject than anything else I&#8217;ve said or written! <a href="https://podcasts.google.com/feed/aHR0cHM6Ly9heHJwb2RjYXN0LmxpYnN5bi5jb20vcnNz/episode/NTM1YTQ1MzAtMDVlNS00OTE4LThlMjgtOWRmZWUzMjM1Mjk0">Listen here</a> or <a href="https://axrp.net/episode/2023/04/11/episode-20-reform-ai-alignment-scott-aaronson.html">read the transcript here</a>.  Here&#8217;s Daniel&#8217;s abstract:</p>



<blockquote class="wp-block-quote">
<p>How should we scientifically think about the impact of AI on human civilization, and whether or not it will doom us all? In this episode, I speak with Scott Aaronson about his views on how to make progress in AI alignment, as well as his work on watermarking the output of language models, and how he moved from a background in quantum complexity theory to working on AI.</p>
</blockquote>



<p>Thanks so much to Daniel for making this podcast happen.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Maybe I should make a broader comment, though.</p>



<p>From my recent posts, and from my declining to sign the <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">six-month AI pause letter</a> (even though I sympathize with many of its goals), many people seem to have goten the impression that I’m not worried about AI, or that (ironically, given my job this year) I&#8217;m basically in the &#8220;full speed ahead&#8221; camp.</p>



<p>This is not true.  In reality, I’m <em>full</em> <em>of</em> worry.  The issue is just that, in this case, I’m also full of <em>metaworry</em>&#8212;i.e., the worry that whichever things I worry about will turn out to have been the wrong things.</p>



<p>Even if we look at the pause letter, or more generally, at the people who wish to slow down AI research, we find that they wildly disagree <em>among themselves</em> about why a slowdown is called for.  One faction says that AI needs to be paused because it will spread misinformation and entrench social biases … or (this part is said aloud surprisingly often) because progress is being led by, you know, like, <em>totally gross</em> capitalistic Silicon Valley nerdbros, and might enhance those nerds&#8217; power.</p>



<p>A second faction, one that <em>contains</em> many of the gross nerdbros, is worried about AI because it might become superintelligent, recursively improve itself, and destroy all life on earth while optimizing for some alien goal.  Hopefully both factions agree that this scenario would be bad, so that the only disagreement is about its likelihood.</p>



<p>As I&#8217;ll never tire of pointing out, the two factions seem to have been converging on the same conclusion&#8212;namely, <em>AI progress urgently needs to be slowed down</em>&#8212;even while they sharply reject each other&#8217;s rationales and indeed are barely on speaking terms with each other.</p>



<p>OK, you might object, but that&#8217;s just sociology.  Why shouldn&#8217;t a rational person worry about near-term AI risk <em>and</em> long-term AI risk?  Why shouldn&#8217;t the ethics people focused on the former and the alignment people focused on the latter strategically join forces?  Such a hybrid Frankenpause is, it seems to me, precisely what the pause letter was trying to engineer.  Alas, the result was that, while a few people closer to the AI ethics camp (like Gary Marcus and Ernest Davis) agreed to sign, many others (Emily Bender, Timnit Gebru, Arvind Narayanan&#8230;) pointedly declined, because&#8212;as they explained on social media&#8212;to do so would be to legitimate the gross nerds and their sci-fi fantasies.</p>



<p>From my perspective, the problem is this:</p>



<ol>
<li><strong>Under the ethics people&#8217;s assumptions, I don&#8217;t see that an AI pause is called for.</strong>  Or rather, while I understand the arguments, the <em>same</em> arguments would seem to have justified stopping the development of the printing press, aviation, radio, computers, the Internet, and virtually every other nascent technology, until committees of academic experts had decided that the positive social effects would outweigh the negative ones, which might&#8217;ve been never.  The trouble is, well, how do you even <em>study</em> the social effects of a new technology, before society starts using it?  Aren&#8217;t we mostly <em>happy</em> that technological pioneers went ahead with all the previously-mentioned things, and dealt with the problems later as they arose?  But preventing the widespread societal adoption of GPT-like tools seems to be what the AI ethics camp <em>really</em> wants, much more than preventing further scaling for scientific research.  I reject any anti-AI argument that could be generalized and transplanted backwards to produce an argument against moving forward with, let&#8217;s say, agriculture or metallurgy.</li>



<li><strong>Under the alignment people&#8217;s assumptions, I <em>do</em> see that an AI pause is urgently called for&#8212;but I&#8217;m not yet on board with their assumptions.</strong>  The kind of relentlessly optimizing AI that could form the intention to doom humanity, still seems very different to me from the kind of AI that’s astonished the world these past couple years, to the point that it’s not obvious how much progress in the latter should increase our terror about the former.  Even Eliezer Yudkowsky <a href="https://www.youtube.com/watch?v=AaTRHFaaPG8">agrees</a> that GPT-4 doesn&#8217;t seem too dangerous in itself.  And an AI that was only <em>slightly</em> dangerous could presumably be recognized as such before it was too late.  So everything hinges on the conjecture that, in going from GPT-n to GPT-(n+1), there might be a &#8220;sharp turn&#8221; where an existential risk to humanity very suddenly emerged, with or without the cooperation of bad humans who used GPT-(n+1) for nefarious purposes.  I still don&#8217;t know how to think about the likelihood of this risk.  The empirical case for it is likely to be inadequate, by its proponents&#8217; own admission.  I admired how my friend Sarah Constantin thought through the issues in her recent essay <a href="https://sarahconstantin.substack.com/p/why-i-am-not-an-ai-doomer">Why I Am Not An AI Doomer</a>&#8212;but on the other hand, as others have pointed out, Sarah ends up conceding a staggering fraction of the doomers&#8217; case in the course of arguing against the rest of it.  What today passes for an &#8220;anti-doomer&#8221; might&#8217;ve been called a &#8220;doomer&#8221; just a few years ago.</li>
</ol>



<p>In short, one could say, the ethics and alignment communities are <em>both</em> building up cases for pausing AI progress, working at it from opposite ends, but their efforts haven&#8217;t yet met at any single argument that I wholeheartedly endorse.</p>



<p>This might just be a question of timing.  <em>If</em> AI is going become existentially dangerous, then I definitely want global coordination well <em>before</em> that happens.  And while it seems unlikely to me that we&#8217;re anywhere near the existential danger zone yet, the pace of progress over the past few years has been so astounding, and has upended so many previous confident assumptions, that caution seems well-advised.</p>



<p>But is a pause the right action?  How should we compare the risk of acceleration now to the risk of a so-called &#8220;overhang,&#8221; where capabilities might skyrocket even faster in the future, faster than society can react or adapt, <em>because</em> of a previous pause?  Also, would a pause even force OpenAI to change its plans from what they would&#8217;ve been otherwise?  (If I knew, I&#8217;d be prohibited from telling, which makes it convenient that I don&#8217;t!)  Or would the main purpose be symbolic, just to show that the main AI labs can coordinate on <em>something</em>?</p>



<p>If so, then one striking aspect of the pause letter is that it was written without consultation with the main entities who would need to agree to any such pause (OpenAI, DeepMind, Google, &#8230;).  Another striking aspect is that it applies only to systems &#8220;more powerful than&#8221; GPT-4.  There are two problems here.  Firstly, the concept &#8220;more powerful than&#8221; isn&#8217;t well-defined: presumably it rules out more parameters and more gradient descent, but what about more reinforcement learning or tuning of hyperparameters?  Secondly, to whatever extent it makes sense, it seems specifically tailored to tie the hands of OpenAI, while giving OpenAI&#8217;s competitors a chance to catch up to OpenAI.  The fact that the most famous signatory is Elon Musk, who&#8217;s now trying to build an &#8220;anti-woke&#8221; chatbot to compete against GPT, doesn&#8217;t help.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>So, if not this pause letter, <em>what do I think ought to happen instead?</em></p>



<p>I&#8217;ve been thinking about it a lot, and the most important thing I can come up with is: clear articulation of fire alarms, red lines, whatever you want to call them, along with what our responses to those fire alarms should be.  Two of my previous fire alarms were the first use of chatbots for academic cheating, and the first depressed person who commits suicide after interacting with a chatbot.  Both of those have now happened.  Here are some others:</p>



<ul>
<li>A chatbot is used to impersonate someone for fraudulent purposes, by imitating his or her writing style.</li>



<li>A chatbot helps a hacker find security vulnerabilities in code that are then actually exploited.</li>



<li>A child dies because his or her parents follow wrong chatbot-supplied medical advice.</li>



<li>Russian or Iranian or Chinese intelligence, or some other such organization, uses a chatbot to mass-manufacture disinformation and propaganda.</li>



<li>A chatbot helps a terrorist manufacture weapons that are used in a terrorist attack.</li>
</ul>



<p>I&#8217;m extremely curious: which fire alarms are <em>you</em> most worried about?  How do you think the AI companies and governments should respond if and when they happen?</p>



<p>In my view, articulating fire alarms actually provides multiple benefits.  Not only will it give us a playbook if and when any of the bad events happen, it will also give us clear <em>targets to try to forecast</em>.  If we&#8217;ve decided that behavior X is unacceptable, and if extrapolating the performance of GPT-1 through GPT-n on various metrics leads to the prediction that GPT-(n+1) will be capable of X, then we suddenly have a clear, legible case for delaying the release of GPT-(n+1).</p>



<p>Or&#8212;and this is yet a third benefit&#8212;we have something clear on which to <em>test</em> GPT-(n+1), in &#8220;sandboxes,&#8221; before releasing it.  I think the kinds of <a href="https://www.lesswrong.com/posts/4Gt42jX7RiaNaxCwP/more-information-about-the-dangerous-capability-evaluations">safety evals</a> that ARC (the Alignment Research Center) did on GPT-4 before it was released&#8212;for example, testing its ability to deceive Mechanical Turkers&#8212;were an extremely important prototype, something that we&#8217;ll need a lot more of before the release of future language models.   But all of society should have a say on what, specifically, <em>are</em> the dangerous behaviors that these evals are checking for.</p>



<p>So let&#8217;s get started on that!  Readers: which unaligned behaviors would you like GPT-5 to be tested for prior to its release?  Bonus points for plausibility and non-obviousness.</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-16T22:18:25Z">Sunday, April 16 2023, 22:18</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2023/04/16/fractal-arbelos.html'>A fractal arbelos</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          If you nest a triangle in a hexagon in a dodecagon, etc., doubling the number of sides each time, you get a triangulation. This sequence of nested polygons was already known to Archimedes, who took it all the way up to 96 sides in order to calculate an accurate approximation of \(\pi\).
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>If you nest a triangle in a hexagon in a dodecagon, etc., doubling the number of sides each time, you get a triangulation. This sequence of nested polygons was already known to Archimedes, who took it all the way up to 96 sides in order to calculate an accurate <a href="https://en.wikipedia.org/wiki/Approximations_of_%CF%80">approximation of \(\pi\)</a>.</p>

<p style="text-align:center"><img src="/blog/assets/2023/archimedes.svg" alt="Triangle in a hexagon in a dodecagon in a circle" style="width:100%;max-width:540px" /></p>

<p>Replacing each triangle by an <a href="https://en.wikipedia.org/wiki/Arbelos">arbelos</a>, the shape between one outer semicircle and two inner ones, and keeping the same pattern, with each triangle connected to one bigger one and two smaller ones, we get a nice tessellation of a semicircle, which can be extended outward in the same way to tile a halfplane. Alternatively, you can think of these as forming a triangulation of the <a href="https://en.wikipedia.org/wiki/Hyperbolic_geometry">hyperbolic plane</a> by <a href="https://en.wikipedia.org/wiki/Ideal_triangle">ideal triangles</a>. Under this reinterpretation, the distinction between bigger and smaller goes away; all the triangles are the same size.</p>

<p style="text-align:center"><img src="/blog/assets/2023/fractal-arbelos.svg" alt="A fractal arbelos" /></p>

<p>It looks like a <a href="https://en.wikipedia.org/wiki/Uniform_tilings_in_hyperbolic_plane">uniform tessellation of the hyperbolic plane</a>, but it’s not: it has at most a one-dimensional group of symmetries, not the full two-dimensional group that a uniform tiling would have. There is a uniform tessellation generated by reflections across the sides of an ideal triangle, but it’s not this one:</p>

<p style="text-align:center"><img src="/blog/assets/2023/uniform-ideal.svg" alt="Uniform tessellation by reflected ideal triangles" title="PD image by Saric from Wikimedia commons, File:Ideal-triangle hyperbolic tiling.svg" style="width:100%;max-width:540px" /></p>

<p>They have the same combinatorial structure (their dual graphs form the same infinite 3-regular tree), but their geometry is different. You can tell they’re not the same because in the uniform tessellation, each triangle is adjacent to its reflection across each of its sides. In the fractal arbelos, if you reflected any triangle across its top side, the other two sides would reflect into vertical rays, but we don’t see any vertical rays. Instead, the fractal arbelos is what you get when you contract the vertical sides of the <a href="https://en.wikipedia.org/wiki/Binary_tiling">aperiodic binary tiling of the hyperbolic plane</a> (below) into points at infinity, leaving the other sides straight (in the hyperbolic plane).</p>

<p style="text-align:center"><img src="/blog/assets/2018/binary-tiling.svg" alt="The binary tiling" /></p>

<p>Now let’s take it down to only some fixed level, rather than recursing infinitely all the way down, and number the vertices in binary.</p>

<p style="text-align:center"><img src="/blog/assets/2023/numbered-arbelos.svg" alt="Fewer levels of the fractal arbelos, with its vertices numbered in binary" /></p>

<p>We can read off a lot of information from these numbers:</p>

<ul>
  <li>
    <p>Each numbered point, with number \(x\), is connected by arcs to other numbers \(x\pm 2^i\), where \(2^i\) is less than or equal to the smallest power of two used in the binary representation of \(x\) (its rightmost one-bit). In the special case \(x=0\), any power of two is allowed.</p>
  </li>
  <li>
    <p>Each numbered point, with nonzero number \(x\), is the middle point of exactly one arbelos-shaped tile. If \(p\) is the largest power of two in the binary representation of \(x\), then the two smaller tiles under this one are numbered \(x\pm p/2\). The larger tile above it is either \(x+p\) or \(x-p\), depending on whether the next bit in the binary representation of \(x\) is a \(0\) or a \(1\), respectively.</p>
  </li>
</ul>

<p>The same numbering system can be extended to all tiles in the halfplane, using <a href="https://en.wikipedia.org/wiki/Dyadic_rational">dyadic rationals</a> in place of integers. The one-dimensional symmetry group is then apparent from the numbers: it’s just multiplication or division by finite powers of two.</p>

<p>If the existence of a special point \(0\), with no arbelos above it, is a concern, then you can replace \(0\) by any <a href="https://en.wikipedia.org/wiki/P-adic_number">2-adic integer</a> whose left-infinite binary representation has infinitely many zeros and infinitely many ones, and number from there in the same way. You can either choose a number with a periodic binary representation (in which case you still get a one-dimensional symmetry group) or an aperiodic representation (producing a tiling with no symmetries at all).</p>

<p>(<a href="https://mathstodon.xyz/@11011110/110211494756432130">Discuss on Mastodon</a>)</p><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-16T18:04:00Z">Sunday, April 16 2023, 18:04</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/047'>TR23-047 |  Ruling Out Short Proofs of Unprovable Sentences is Hard | 

	Hunter Monroe</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          If no optimal propositional proof system exists, we (and independently Pudlák) prove that ruling out length $t$ proofs of any unprovable sentence is hard. This mapping from unprovable to hard-to-prove sentences powerfully translates facts about noncomputability into complexity theory. For instance, because proving string $x$ is Kolmogorov random ($x{\in}R$) is typically impossible, it is typically hard to prove &quot;no length $t$ proof shows $x{\in}R$&quot;, or tautologies encoding this. Therefore, a proof system with one family of hard tautologies has these densely in an enumeration of families. The assumption also implies that a natural language is $\textbf{NP}$-intermediate: with $R$ redefined to have a sparse complement, the complement of the language $\{\langle x,1^t\rangle|$ no length $t$ proof exists of  $x{\in}R\}$ is also sparse. 

Efficiently ruling out length $t$ proofs of $x{\in}R$ might violate the constraint on using the fact of $x{\in}R$&#39;s unprovability. We conjecture: any computable predicate on $R$ that might be used in if-then statements (or case-based proofs) does no better than branching at random, because $R$ appears random by any effective test. This constraint could also inhibit the usefulness in circuits and propositional proofs of NOT gates and cancellation---needed to encode if-then statements. If $R$ defeats if-then logic, exhaustive search is necessary.
        
        </div>

        <div class='tr-article-summary'>
        
          
          If no optimal propositional proof system exists, we (and independently Pudlák) prove that ruling out length $t$ proofs of any unprovable sentence is hard. This mapping from unprovable to hard-to-prove sentences powerfully translates facts about noncomputability into complexity theory. For instance, because proving string $x$ is Kolmogorov random ($x{\in}R$) is typically impossible, it is typically hard to prove &quot;no length $t$ proof shows $x{\in}R$&quot;, or tautologies encoding this. Therefore, a proof system with one family of hard tautologies has these densely in an enumeration of families. The assumption also implies that a natural language is $\textbf{NP}$-intermediate: with $R$ redefined to have a sparse complement, the complement of the language $\{\langle x,1^t\rangle|$ no length $t$ proof exists of  $x{\in}R\}$ is also sparse. 

Efficiently ruling out length $t$ proofs of $x{\in}R$ might violate the constraint on using the fact of $x{\in}R$&#39;s unprovability. We conjecture: any computable predicate on $R$ that might be used in if-then statements (or case-based proofs) does no better than branching at random, because $R$ appears random by any effective test. This constraint could also inhibit the usefulness in circuits and propositional proofs of NOT gates and cancellation---needed to encode if-then statements. If $R$ defeats if-then logic, exhaustive search is necessary.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-16T12:20:26Z">Sunday, April 16 2023, 12:20</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/046'>TR23-046 |  NP-Hardness of Approximating Meta-Complexity: A Cryptographic Approach | 

	Hanlin Ren, 

	Rahul Ilango, 

	Yizhi Huang</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          It is a long-standing open problem whether the Minimum Circuit Size Problem ($\mathrm{MCSP}$) and related meta-complexity problems are NP-complete. Even for the rare cases where the NP-hardness of meta-complexity problems are known, we only know very weak hardness of approximation.

In this work, we prove NP-hardness of approximating meta-complexity with nearly-optimal approximation gaps. Our key idea is to use *cryptographic constructions* in our reductions, where the security of the cryptographic construction implies the correctness of the reduction. We present both conditional and unconditional hardness of approximation results as follows.

$\bullet$ Assuming subexponentially-secure witness encryption exists, we prove essentially optimal NP-hardness of approximating conditional time-bounded Kolmogorov complexity ($\mathrm{K}^t(x \mid y)$) in the regime where $t \gg |y|$. Previously, the best hardness of approximation known was a $|x|^{1/ \mathrm{poly}(\log \log |x|)}$ factor and only in the sublinear regime ($t \ll |y|$).
$\bullet$ Unconditionally, we show near-optimal NP-hardness of approximation for the Minimum Oracle Circuit Size Problem (MOCSP), where Yes instances have circuit complexity at most $2^{\varepsilon n}$, and No instances are essentially as hard as random truth tables. Our reduction builds on a witness encryption construction proposed by Garg, Gentry, Sahai, and Waters (STOC&#39;13). Previously, it was unknown whether it is NP-hard to distinguish between oracle circuit complexity $s$ versus $10s\log N$.
$\bullet$ Finally, we define a &quot;multi-valued&quot; version of $\mathrm{MCSP}$, called $\mathrm{mvMCSP}$, and show that w.p. $1$ over a random oracle $O$, $\mathrm{mvMCSP}^O$ is NP-hard to approximate under quasi-polynomial-time reductions with $O$ oracle access. Intriguingly, this result follows almost directly from the security of Micali&#39;s CS proofs (Micali, SICOMP&#39;00).

In conclusion, we give three results convincingly demonstrating the power of cryptographic techniques in proving NP-hardness of approximating meta-complexity.
        
        </div>

        <div class='tr-article-summary'>
        
          
          It is a long-standing open problem whether the Minimum Circuit Size Problem ($\mathrm{MCSP}$) and related meta-complexity problems are NP-complete. Even for the rare cases where the NP-hardness of meta-complexity problems are known, we only know very weak hardness of approximation.

In this work, we prove NP-hardness of approximating meta-complexity with nearly-optimal approximation gaps. Our key idea is to use *cryptographic constructions* in our reductions, where the security of the cryptographic construction implies the correctness of the reduction. We present both conditional and unconditional hardness of approximation results as follows.

$\bullet$ Assuming subexponentially-secure witness encryption exists, we prove essentially optimal NP-hardness of approximating conditional time-bounded Kolmogorov complexity ($\mathrm{K}^t(x \mid y)$) in the regime where $t \gg |y|$. Previously, the best hardness of approximation known was a $|x|^{1/ \mathrm{poly}(\log \log |x|)}$ factor and only in the sublinear regime ($t \ll |y|$).
$\bullet$ Unconditionally, we show near-optimal NP-hardness of approximation for the Minimum Oracle Circuit Size Problem (MOCSP), where Yes instances have circuit complexity at most $2^{\varepsilon n}$, and No instances are essentially as hard as random truth tables. Our reduction builds on a witness encryption construction proposed by Garg, Gentry, Sahai, and Waters (STOC&#39;13). Previously, it was unknown whether it is NP-hard to distinguish between oracle circuit complexity $s$ versus $10s\log N$.
$\bullet$ Finally, we define a &quot;multi-valued&quot; version of $\mathrm{MCSP}$, called $\mathrm{mvMCSP}$, and show that w.p. $1$ over a random oracle $O$, $\mathrm{mvMCSP}^O$ is NP-hard to approximate under quasi-polynomial-time reductions with $O$ oracle access. Intriguingly, this result follows almost directly from the security of Micali&#39;s CS proofs (Micali, SICOMP&#39;00).

In conclusion, we give three results convincingly demonstrating the power of cryptographic techniques in proving NP-hardness of approximating meta-complexity.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-16T06:00:20Z">Sunday, April 16 2023, 06:00</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Saturday, April 15
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2023/04/15/linkage.html'>Linkage</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Two recent posts by Dave Richeson on connections between stamp folding and the design of circular labyrinths and their combinatorial enumeration as meanders (\(\mathbb{M}\)). See also Wikipedia on map folding and meanders.
        
        </div>

        <div class='tr-article-summary'>
        
          
          <ul>
  <li>
    <p>Two recent posts by Dave Richeson on <a href="https://mathstodon.xyz/@divbyzero/110074607064920932">connections between stamp folding and the design of circular labyrinths</a> and <a href="https://mathstodon.xyz/@divbyzero/110107529366706667">their combinatorial enumeration as meanders</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110130128539526442">\(\mathbb{M}\)</a>).</span> 
See also Wikipedia on <a href="https://en.wikipedia.org/wiki/Map_folding">map folding</a> and <a href="https://en.wikipedia.org/wiki/Meander_(mathematics)">meanders</a>.</p>
  </li>
  <li>
    <p>Here’s a long-standing mistake in Wikipedia’s algorithms coverage <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110131646871368785">\(\mathbb{M}\)</a>).</span> From August 2013 until recently <a href="https://en.wikipedia.org/wiki/Quickselect">the quickselect article</a> stated “quickselect has almost certain \(O(n)\) time complexity” or “a random pivot, which yields almost certain linear time”, or some other form of this statement. This turns out to be false. There is no constant \(C\) such that the probability of performing fewer than \(Cn\) comparisons goes to one in the limit as \(n\) goes to infinity. Instead, if you are using quickselect to find the minimum element (say), you will perform more than \(Cn\) comparisons in the case that the first \(2C\) pivots all land in the top half of the inputs, in decreasing order. This happens with constant probability, exponentially small in \(C\log C\) but independent of \(n\). Luc Devroye in two papers showed upper bounds on the probability of many comparisons, exponential and then (as above) superexponential in \(C\): see “<a href="http://luc.devroye.org/devroye-selection1984.pdf">Exponential bounds for the running time of a selection algorithm</a>” (1984), and “<a href="http://luc.devroye.org/wcfind.pdf">On the probabilistic worst-case time of ‘Find’</a>” (2001). So the probability of linearity is indeed very close to one, but it is not almost certain.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@tao/110068572186339264">Thread by Terry Tao with images of historical manuscripts at the Institut de France</a>, including Poincaré’s anticipation of special relativity, Pascal’s triangle (in a book by Pascal), Galois’s last letter, a Gutenberg bible, and some historical portraits.</p>
  </li>
  <li>
    <p>Did you know that the first publication to connect carbon dioxide levels to global warming, by 19th century amateur scientist <a href="https://en.wikipedia.org/wiki/Eunice_Newton_Foote">Eunice Newton Foote</a>, went long forgotten until uncovered by feminist historians in the 1970s <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110144606239976056">\(\mathbb{M}\)</a>).</span> Via <a href="https://wikimediafoundation.org/news/2023/03/14/susunw-is-on-a-mission-to-write-women-into-history-with-wikipedia/">a profile of Wikipedia volunteer SusunW</a> who significantly expanded Foote’s article last year, bringing it to Featured Article status.</p>
  </li>
  <li>
    <p><a href="https:doi.org/10.1080/17513472.2023.2191572">Knitted origami</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@villares@ciberlandia.pt/110123876929220736">\(\mathbb{M}\)</a>),</span> techniques for embedding sharp creases as features in knitted fabric.</p>
  </li>
  <li>
    <p>Two more recent Wikipedia Good Articles <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110155594315017176">\(\mathbb{M}\)</a>):</span></p>

    <ul>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Unit_fraction">Unit fraction</a> – you know, like 1/2, 1/3, 1/27, etc.</p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Beckman%E2%80%93Quarles_theorem">Beckman–Quarles theorem</a>, according to which the only mappings from the Euclidean plane to itself that preserve unit distances are the obvious ones: translations, rotations, mirror reflections, and glide reflections. In contrast, mappings into higher dimensions can be much messier; for instance, there is a unit-distance-preserving map from the plane to a set of only seven distinct points in six dimensions.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="https://www.julianstanczak.com/work.php">Selected paintings by Julian Stanczek</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@joshmillard@mastodon.social/110113097820423511">\(\mathbb{M}\)</a>),</span> very geometric in their use of color, form, and line.</p>
  </li>
  <li>
    <p><a href="https://reason.com/tag/large-libel-models/">Large libel models</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110166208017935593">\(\mathbb{M}\)</a>,</span> <a href="https://www.metafilter.com/198888/ChatGPT-cooks-up-fake-sexual-harassment-scandal-and-names-real-professor">via</a>). Far more than you probably wanted to know about: when ChatGPT makes up fake scandals about real people, who is responsible for the resulting libel? Also via Wikipedia, where there have been multiple incidents of editors adding falsified LLM-written content to Wikipedia, and discussions about how to keep this junk out. But attempts to ban LLM content have been somewhat stymied by too many under-informed editors who think “isn’t it neat that computers can write Wikipedia articles” or “won’t someone think of the people who can’t write themselves but can get computers to write for them”.</p>
  </li>
  <li>
    <p><a href="https://im.icerm.brown.edu/portfolio/nonabelian-set/">Nonabelian Set</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110172037195803108">\(\mathbb{M}\)</a>),</span> a variation of the Set card game (in which one seeks triples of cards that form lines in a finite geometry) where the cards represent permutation group elements and the goal is to find sequences of tiles that compose to the identity. One of many neat mathematical projects and visualizations from a 2019 program, “<a href="https://im.icerm.brown.edu/">Illustrating Mathematics</a>”, at the Brown University Institute for Computational and Experimental Research in Mathematics.</p>
  </li>
  <li>
    <p><a href="https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/aperiodic-tilings/">Simon Tatham writes about two algorithms for randomly generating aperiodic tilings</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@fanf@mendeddrum.org/110175215669526845">\(\mathbb{M}\)</a>),</span> including the new hat tiling, which we might expect to see soon as one of the fields for the Loopy puzzle in Tatham’s puzzle collection.</p>
  </li>
  <li>
    <p><a href="https://ericneyman.wordpress.com/2020/11/29/an-elegant-proof-of-laplaces-rule-of-succession/">Laplace’s Rule of Succession</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@ccanonne/110183704348315532">\(\mathbb{M}\)</a>).</span> An intuitive explanation for why, after seeing \(k\) heads out of \(n\) flips of a coin with unknown bias, your estimate for the bias should be \((k+1)/(n+2)\).</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/how-randomness-improves-algorithms-20230403/"><em>Quanta</em> on why randomness is useful in algorithm design</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@QuantaMagazine@mstdn.social/110135300361305997">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://igorpak.wordpress.com/2023/04/12/the-journal-hall-of-shame/">Igor Pak names and shames high-level journals claiming to cover all of mathematics, but actually excluding all of combinatorics</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110195556643419780">\(\mathbb{M}\)</a>).</span> The follow-up discussion also concerns similar prejudices against category theory, and how a hypothetical book on categorical combinatorics would be treated.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Kaktovik_numerals">Kaktovik numerals</a>, an intuitive system of base-20 digits invented by <a href="https://en.wikipedia.org/wiki/I%C3%B1upiat">Iñupiat</a> schoolchildren, <a href="https://www.scientificamerican.com/article/a-number-system-invented-by-inuit-schoolchildren-will-make-its-silicon-valley-debut/">are getting added to Unicode</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@tokensane@mastodon.me.uk/110190269945900718">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p>Amusing to see <a href="https://cameroncounts.wordpress.com/2023/04/15/bases-2/">an application of Mornington Crescent in group theory</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110204407564525420">\(\mathbb{M}\)</a>).</span> Peter Cameron looks at bases, sequences of the elements permuted by a permutation group whose permuted values are sufficient to identify which group element permuted them. For bases that are minimal under taking prefixes, the possible base lengths form a contiguous interval of integers.</p>
  </li>
</ul><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-15T14:40:00Z">Saturday, April 15 2023, 14:40</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://tcsplus.wordpress.com/2023/04/14/tcs-talk-wednesday-april-19-michal-feldman-tel-aviv-university/'>TCS+ talk: Wednesday, April 19 — Michal Feldman, Tel Aviv University</a></h3>
        <p class='tr-article-feed'>from <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The next TCS+ talk will take place this coming Wednesday, April 19th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Michal Feldman from Tel Aviv University will speak about &#8220;Algorithmic Contract Design&#8221; (abstract below). You can reserve a spot as an individual or a group to join us [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The next TCS+ talk will take place this coming Wednesday, April 19th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Michal Feldman</strong> from Tel Aviv University will speak about &#8220;<em>Algorithmic Contract Design</em>&#8221; (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: Contract design captures situations where a principal delegates the execution of a costly task to an agent. To complete the task, the agent chooses an action from a set of costly actions. The principal can only observe the outcome, which is stochastically determined by the chosen action. The principal incentivizes the desired action through a contract, that specifies payments based on the observed outcome. In this talk, I will survey two papers on *combinatorial* contracts, which highlight different sources of complexity that arise in contract design. The first (FOCS’21) is where the agent can choose any subset of a given set of actions; the second (STOC’23) is where the principal motivates a team of agents. We provide (approximation) algorithms and hardness results for the optimal contract problem in these scenarios.</p>
<p>Based on joint work with Tomer Ezra, Paul Duetting and Thomas Kesselheim.</p></blockquote>
<p class="authors">By plustcs</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-15T00:18:14Z">Saturday, April 15 2023, 00:18</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Friday, April 14
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://emanueleviola.wordpress.com/2023/04/14/mathematics-of-the-impossible-chapter-8-three-impossibility-results-for-3sat/'>Mathematics of the impossible, Chapter 8, Three impossibility results for 3Sat</a></h3>
        <p class='tr-article-feed'>from <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We should turn back to a traditional separation technique – diagonalization.[21] In this chapter we put together many of the techniques we have seen to obtain several impossibility results for 3Sat. The template of all these results (and others, like those mentioned in section&#160;º5.1) is similar. All these results prove time bounds of the form [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <div class="quote">
<div class="flushright">
<p style="text-align:justify">  <em>We should turn back to a traditional separation technique – diagonalization.</em><span class="cite">[<a href="journals/jcss/Fortnow00">21</a>]</span></p>
</div>
</div>
<p style="text-align:justify">In this chapter we put together many of the techniques we have seen to obtain several impossibility results for 3Sat. The template of all these results (and others, like those mentioned in section&nbsp;º<a href="#x1-580005.1">5.1<!--tex4ht:ref: sec:Nondeterministic-computation --></a>) is similar. All these results prove time bounds of the form <img src="https://s0.wp.com/latex.php?latex=t%5Cge+n%5E%7B1%2B%5Cepsilon+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=t%5Cge+n%5E%7B1%2B%5Cepsilon+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=t%5Cge+n%5E%7B1%2B%5Cepsilon+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="t&#92;ge n^{1+&#92;epsilon }" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%5Cin+%280%2C1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cepsilon+%5Cin+%280%2C1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cepsilon+%5Cin+%280%2C1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;epsilon &#92;in (0,1)" class="latex" />. One can optimize the methods to push <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cepsilon+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cepsilon+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;epsilon " class="latex" /> close to <img src="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1" class="latex" />, but even establishing <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;epsilon =1" class="latex" /> seems out of reach, and there are known barriers for current techniques <span class="cite">[<a href="journals/cc/BussW15">16</a>]</span>.</p>
<h3 class="sectionHead"><span class="titlemark">8.1   </span> <a id="x1-970008.1"></a>Impossibility I</h3>
<p style="text-align:justify">We begin with the following remarkable result.</p>
<div class="newtheorem">
<p style="text-align:justify"><span class="head"> <a id="x1-97001r1"></a> <b>Theorem</b> 8.1.  </span><span class="cite">[<a href="journals/jcss/Fortnow00">21</a>]</span> Either <img src="https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cnot+%5Cin+%5Ctext+%7BL%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cnot+%5Cin+%5Ctext+%7BL%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cnot+%5Cin+%5Ctext+%7BL%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;text {3Sat}&#92;not &#92;in &#92;text {L}" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cnot+%5Cin+%5Ctext+%7BTime%7D%28n%5E%7B1%2B%5Cepsilon+%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cnot+%5Cin+%5Ctext+%7BTime%7D%28n%5E%7B1%2B%5Cepsilon+%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cnot+%5Cin+%5Ctext+%7BTime%7D%28n%5E%7B1%2B%5Cepsilon+%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;text {3Sat}&#92;not &#92;in &#92;text {Time}(n^{1+&#92;epsilon })" class="latex" /> for some constant <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cepsilon+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cepsilon+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;epsilon " class="latex" />.</p>
<p style="text-align:justify">
</div>
<p style="text-align:justify">   Note that we don’t know if <img src="https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cin+%5Ctext+%7BL%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cin+%5Ctext+%7BL%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cin+%5Ctext+%7BL%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;text {3Sat}&#92;in &#92;text {L}" class="latex" /> or if <img src="https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cin+%5Ctext+%7BTime%7D%28n%5Clog+%5E%7B10%7Dn%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cin+%5Ctext+%7BTime%7D%28n%5Clog+%5E%7B10%7Dn%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cin+%5Ctext+%7BTime%7D%28n%5Clog+%5E%7B10%7Dn%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;text {3Sat}&#92;in &#92;text {Time}(n&#92;log ^{10}n)" class="latex" />. In particular, Theorem <a href="#x1-97001r1">8.1<!--tex4ht:ref: thm:fortnow --></a> implies that any algorithm for 3Sat either must use super-logarithmic space or time <img src="https://s0.wp.com/latex.php?latex=n%5E%7B1%2Bc%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=n%5E%7B1%2Bc%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n%5E%7B1%2Bc%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="n^{1+c}" class="latex" />.</p>
<p style="text-align:justify">
<div class="proof">
<p style="text-align:justify">   <span class="head">    <b>Proof</b>.&nbsp;</span>We assume that what we want to prove is not true and derive the following striking contradiction with the hierarchy Theorem <a href="#x1-41003r4">3.4<!--tex4ht:ref: thm:TIME-hierarchy-TM --></a>:</p>
<div style="text-align: center"> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Ctext+%7BTime%7D%28n%5E%7B2%7D%29+%26+%5Csubseteq+%5Ctext+%7BL%7D%5C%5C+%26+%5Csubseteq+%5Cbigcup+_%7Bd%7D%5CSigma+_%7Bd%7D%5Ctext+%7BTime%7D%28n%29%5C%5C+%26+%5Csubseteq+%5Ctext+%7BTime%7D%28n%5E%7B1.9%7D%29.+%5Cend%7Baligned%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Ctext+%7BTime%7D%28n%5E%7B2%7D%29+%26+%5Csubseteq+%5Ctext+%7BL%7D%5C%5C+%26+%5Csubseteq+%5Cbigcup+_%7Bd%7D%5CSigma+_%7Bd%7D%5Ctext+%7BTime%7D%28n%29%5C%5C+%26+%5Csubseteq+%5Ctext+%7BTime%7D%28n%5E%7B1.9%7D%29.+%5Cend%7Baligned%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Ctext+%7BTime%7D%28n%5E%7B2%7D%29+%26+%5Csubseteq+%5Ctext+%7BL%7D%5C%5C+%26+%5Csubseteq+%5Cbigcup+_%7Bd%7D%5CSigma+_%7Bd%7D%5Ctext+%7BTime%7D%28n%29%5C%5C+%26+%5Csubseteq+%5Ctext+%7BTime%7D%28n%5E%7B1.9%7D%29.+%5Cend%7Baligned%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;begin{aligned} &#92;text {Time}(n^{2}) &amp; &#92;subseteq &#92;text {L}&#92;&#92; &amp; &#92;subseteq &#92;bigcup _{d}&#92;Sigma _{d}&#92;text {Time}(n)&#92;&#92; &amp; &#92;subseteq &#92;text {Time}(n^{1.9}). &#92;end{aligned}" class="latex" /></div>
<p style="text-align:justify">   The first inclusion holds by the assumption that <img src="https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cin+%5Ctext+%7BL%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cin+%5Ctext+%7BL%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cin+%5Ctext+%7BL%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;text {3Sat}&#92;in &#92;text {L}" class="latex" /> and the fact that any function in <img src="https://s0.wp.com/latex.php?latex=%5Ctext+%7BTime%7D%28n%5E%7B2%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Ctext+%7BTime%7D%28n%5E%7B2%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ctext+%7BTime%7D%28n%5E%7B2%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;text {Time}(n^{2})" class="latex" /> can be reduced to 3Sat in log-space, by Theorem <a href="#x1-57001r1">5.1<!--tex4ht:ref: thm:redux-ckt-2-3sat --></a> and the discussion after that.</p>
<p style="text-align:justify">   The second inclusion is Theorem <a href="#x1-91003r11">7.11<!--tex4ht:ref: thm:L-in-linear-hiearchy --></a>.</p>
<p style="text-align:justify">   For the third inclusion, the assumption that <img src="https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cin+%5Ctext+%7BTime%7D%28n%5E%7B1%2B%5Cepsilon+%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cin+%5Ctext+%7BTime%7D%28n%5E%7B1%2B%5Cepsilon+%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cin+%5Ctext+%7BTime%7D%28n%5E%7B1%2B%5Cepsilon+%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;text {3Sat}&#92;in &#92;text {Time}(n^{1+&#92;epsilon })" class="latex" /> for every <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cepsilon+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cepsilon+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;epsilon " class="latex" /> implies that <img src="https://s0.wp.com/latex.php?latex=%5Ctext+%7BNTime%7D%28dn%29%5Csubseteq+%5Ctext+%7BTime%7D%28n%5E%7B1%2B%5Cepsilon+%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Ctext+%7BNTime%7D%28dn%29%5Csubseteq+%5Ctext+%7BTime%7D%28n%5E%7B1%2B%5Cepsilon+%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ctext+%7BNTime%7D%28dn%29%5Csubseteq+%5Ctext+%7BTime%7D%28n%5E%7B1%2B%5Cepsilon+%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;text {NTime}(dn)&#92;subseteq &#92;text {Time}(n^{1+&#92;epsilon })" class="latex" /> for every <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="d" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cepsilon+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cepsilon+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;epsilon " class="latex" />, by the quasi-linear-time completeness of 3Sat, Theorem <a href="#x1-63006r4">5.4<!--tex4ht:ref: thm:redux-NTime-3Sat --></a>. Now apply Exercise <a href="#x1-73002r2">6.2<!--tex4ht:ref: xca:LIN-NTIME=00003DTIME-implies-LIN-H=00003DTIME --></a>. <b>QED</b></p>
</div>
<p style="text-align:justify">
<h3 class="sectionHead"><span class="titlemark">8.2   </span> <a id="x1-980008.2"></a>Impossibility II</h3>
<p style="text-align:justify">We now state and prove a closely related result for TiSp. We seek to rule out algorithms for 3Sat that simultaneously use little space and time, whereas in Theorem <a href="#x1-97001r1">8.1<!--tex4ht:ref: thm:fortnow --></a> we even ruled out the possibility that there are two distinct algorithms, one optimizing space and the other time. The main gain is that we will be able to handle much larger space: power rather than log.</p>
<div class="newtheorem">
<p style="text-align:justify"><span class="head"> <a id="x1-98001r2"></a> <b>Theorem</b> 8.2.  </span><img src="https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cnot+%5Cin+%5Ctext+%7BTiSp%7D%28n%5E%7B1%2Bc_%7B%5Cepsilon+%7D%7D%2Cn%5E%7B1-%5Cepsilon+%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cnot+%5Cin+%5Ctext+%7BTiSp%7D%28n%5E%7B1%2Bc_%7B%5Cepsilon+%7D%7D%2Cn%5E%7B1-%5Cepsilon+%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ctext+%7B3Sat%7D%5Cnot+%5Cin+%5Ctext+%7BTiSp%7D%28n%5E%7B1%2Bc_%7B%5Cepsilon+%7D%7D%2Cn%5E%7B1-%5Cepsilon+%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;text {3Sat}&#92;not &#92;in &#92;text {TiSp}(n^{1+c_{&#92;epsilon }},n^{1-&#92;epsilon })" class="latex" />, for any <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;epsilon &gt;0" class="latex" />.</p>
<p style="text-align:justify">
</div>
<p style="text-align:justify">   The important aspect of Theorem <a href="#x1-97001r1">8.1<!--tex4ht:ref: thm:fortnow --></a> is that it applies to the RAM model; stronger results can be shown for space-bounded TMs.</p>
<div class="newtheorem">
<p style="text-align:justify"><span class="head"> <a id="x1-98002r1"></a> <b>Exercise</b> 8.1.  </span>Prove that <img src="https://s0.wp.com/latex.php?latex=%5Ctext+%7BPalindromes%7D%5Cnot+%5Cin+%5Ctext+%7BTM-TiSp%7D%28n%5E%7B1%2Bc_%7B%5Cepsilon+%7D%7D%2Cn%5E%7B1-%5Cepsilon+%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Ctext+%7BPalindromes%7D%5Cnot+%5Cin+%5Ctext+%7BTM-TiSp%7D%28n%5E%7B1%2Bc_%7B%5Cepsilon+%7D%7D%2Cn%5E%7B1-%5Cepsilon+%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ctext+%7BPalindromes%7D%5Cnot+%5Cin+%5Ctext+%7BTM-TiSp%7D%28n%5E%7B1%2Bc_%7B%5Cepsilon+%7D%7D%2Cn%5E%7B1-%5Cepsilon+%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;text {Palindromes}&#92;not &#92;in &#92;text {TM-TiSp}(n^{1+c_{&#92;epsilon }},n^{1-&#92;epsilon })" class="latex" />, for any <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;epsilon &gt;0" class="latex" />. (TM-TiSp<img src="https://s0.wp.com/latex.php?latex=%28t%2Cs%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%28t%2Cs%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28t%2Cs%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="(t,s)" class="latex" /> is defined as Space<img src="https://s0.wp.com/latex.php?latex=%28s%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%28s%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28s%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="(s)" class="latex" />, cf.&nbsp;Definition <a href="#x1-80001r1">7.1<!--tex4ht:ref: def:Space --></a>, but moreover the machine runs in at most <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="t" class="latex" /> steps.) Hint: This problem has a simple solution. Give a suitable simulation of TM-Tisp by 1TM, then apply Theorem <a href="#x1-39001r1">3.1<!--tex4ht:ref: thm:TM-pal-requires-quadratic --></a>.</p>
<p style="text-align:justify">
</div>
<p style="text-align:justify">
<div class="proof">
<p style="text-align:justify">   <span class="head">    <b>Proof</b>.&nbsp;</span>We assume that what we want to prove is not true and derive the following contradiction with the hierarchy Theorem <a href="#x1-41003r4">3.4<!--tex4ht:ref: thm:TIME-hierarchy-TM --></a>:</p>
<div style="text-align: center"> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Ctext+%7BTime%7D%28n%5E%7B1%2B%5Cepsilon+%7D%29+%26+%5Csubseteq+%5Ctext+%7B%5Ctext+%7BTiSp%7D%28c%5Censuremath+%7Bn%5E%7B%281%2B%5Cepsilon+%29%281%2Bc_%7B%5Cepsilon+%7D%29%7D%7D%2Cc%5Censuremath+%7Bn%5E%7B%281%2B%5Cepsilon+%29%281-%5Cepsilon+%29%7D%7D%29%7D%5C%5C+%26+%5Csubseteq+%5Ctext+%7B%5Ctext+%7BTiSp%7D%28%5Censuremath+%7Bn%5E%7B1%2Bc_%7B%5Cepsilon+%7D%7D%7D%2Cc%5Censuremath+%7Bn%5E%7B1-%5Cepsilon+%5E%7B2%7D%7D%7D%29%7D%5C%5C+%26+%5Csubseteq+%5CSigma+_%7Bc_%7B%5Cepsilon+%7D%7D%5Ctext+%7BTime%7D%28n%29%5C%5C+%26+%5Csubseteq+%5Ctext+%7BTime%7D%28n%5E%7B1%2B%5Cepsilon+%2F2%7D%29.+%5Cend%7Baligned%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Ctext+%7BTime%7D%28n%5E%7B1%2B%5Cepsilon+%7D%29+%26+%5Csubseteq+%5Ctext+%7B%5Ctext+%7BTiSp%7D%28c%5Censuremath+%7Bn%5E%7B%281%2B%5Cepsilon+%29%281%2Bc_%7B%5Cepsilon+%7D%29%7D%7D%2Cc%5Censuremath+%7Bn%5E%7B%281%2B%5Cepsilon+%29%281-%5Cepsilon+%29%7D%7D%29%7D%5C%5C+%26+%5Csubseteq+%5Ctext+%7B%5Ctext+%7BTiSp%7D%28%5Censuremath+%7Bn%5E%7B1%2Bc_%7B%5Cepsilon+%7D%7D%7D%2Cc%5Censuremath+%7Bn%5E%7B1-%5Cepsilon+%5E%7B2%7D%7D%7D%29%7D%5C%5C+%26+%5Csubseteq+%5CSigma+_%7Bc_%7B%5Cepsilon+%7D%7D%5Ctext+%7BTime%7D%28n%29%5C%5C+%26+%5Csubseteq+%5Ctext+%7BTime%7D%28n%5E%7B1%2B%5Cepsilon+%2F2%7D%29.+%5Cend%7Baligned%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Ctext+%7BTime%7D%28n%5E%7B1%2B%5Cepsilon+%7D%29+%26+%5Csubseteq+%5Ctext+%7B%5Ctext+%7BTiSp%7D%28c%5Censuremath+%7Bn%5E%7B%281%2B%5Cepsilon+%29%281%2Bc_%7B%5Cepsilon+%7D%29%7D%7D%2Cc%5Censuremath+%7Bn%5E%7B%281%2B%5Cepsilon+%29%281-%5Cepsilon+%29%7D%7D%29%7D%5C%5C+%26+%5Csubseteq+%5Ctext+%7B%5Ctext+%7BTiSp%7D%28%5Censuremath+%7Bn%5E%7B1%2Bc_%7B%5Cepsilon+%7D%7D%7D%2Cc%5Censuremath+%7Bn%5E%7B1-%5Cepsilon+%5E%7B2%7D%7D%7D%29%7D%5C%5C+%26+%5Csubseteq+%5CSigma+_%7Bc_%7B%5Cepsilon+%7D%7D%5Ctext+%7BTime%7D%28n%29%5C%5C+%26+%5Csubseteq+%5Ctext+%7BTime%7D%28n%5E%7B1%2B%5Cepsilon+%2F2%7D%29.+%5Cend%7Baligned%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;begin{aligned} &#92;text {Time}(n^{1+&#92;epsilon }) &amp; &#92;subseteq &#92;text {&#92;text {TiSp}(c&#92;ensuremath {n^{(1+&#92;epsilon )(1+c_{&#92;epsilon })}},c&#92;ensuremath {n^{(1+&#92;epsilon )(1-&#92;epsilon )}})}&#92;&#92; &amp; &#92;subseteq &#92;text {&#92;text {TiSp}(&#92;ensuremath {n^{1+c_{&#92;epsilon }}},c&#92;ensuremath {n^{1-&#92;epsilon ^{2}}})}&#92;&#92; &amp; &#92;subseteq &#92;Sigma _{c_{&#92;epsilon }}&#92;text {Time}(n)&#92;&#92; &amp; &#92;subseteq &#92;text {Time}(n^{1+&#92;epsilon /2}). &#92;end{aligned}" class="latex" /></div>
<p style="text-align:justify">   The first inclusion holds by the assumption, padding, and the fact that 3Sat is complete under reductions s.t.&nbsp;each bit is computable in time (and hence space) <img src="https://s0.wp.com/latex.php?latex=n%5E%7Bo%281%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=n%5E%7Bo%281%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n%5E%7Bo%281%29%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="n^{o(1)}" class="latex" />, a fact we do not prove here. <b>QED</b></p>
</div>
<div class="newtheorem">
<p style="text-align:justify"><span class="head"> <a id="x1-98003r2"></a> <b>Exercise</b> 8.2.  </span>Finish the proof by justifying the remaining inclusions.</p>
<p style="text-align:justify">
</div>
<p style="text-align:justify">
<h3 class="sectionHead"><span class="titlemark">8.3   </span> <a id="x1-990008.3"></a>Impossibility III</h3>
<p style="text-align:justify">So far our impossibility results required bounds on space. We now state and prove a result that applies to time. Of course, as discussed in Chapter <a href="#x1-380003">3<!--tex4ht:ref: chap:The-grand-challenge --></a>, we don’t know how to prove that, say, 3Sat cannot be computed in linear time on a 2TM. For single-tape machines, we can prove quadratic bounds, for palindromes (Theorem <a href="#x1-39001r1">3.1<!--tex4ht:ref: thm:TM-pal-requires-quadratic --></a>) and 3Sat (Problem <a href="#x1-56002r2">4.2<!--tex4ht:ref: prob:3Sat-not-in-TM-Time1.99 --></a>). Next we consider an interesting model which is between 1TM and 2TM and is a good indication of the state of our knowledge.</p>
<div class="newtheorem">
<p style="text-align:justify"><span class="head"> <a id="x1-99001r1"></a> <b>Definition</b> 8.1.  </span>A <img src="https://s0.wp.com/latex.php?latex=1.5%5Ctext+%7BTM%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1.5%5Ctext+%7BTM%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1.5%5Ctext+%7BTM%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1.5&#92;text {TM}" class="latex" /> is like a 2TM except that the input tape is read-only.</p>
<p style="text-align:justify">
</div>
<div class="newtheorem">
<p style="text-align:justify"><span class="head"> <a id="x1-99002r3"></a> <b>Theorem</b> 8.3.  </span><span class="cite">[<a href="#XMaS87">43</a>,&nbsp;<a href="journals/tcs/MelkebeekR05">70</a>]</span> 3Sat requires time <img src="https://s0.wp.com/latex.php?latex=n%5E%7B1%2Bc%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=n%5E%7B1%2Bc%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n%5E%7B1%2Bc%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="n^{1+c}" class="latex" /> on a 1.5TM.</p>
<p style="text-align:justify">
</div>
<div class="newtheorem">
<p style="text-align:justify"><span class="head"> <a id="x1-99003r3"></a> <b>Exercise</b> 8.3.  </span>Prove Theorem <a href="#x1-99002r3">8.3<!--tex4ht:ref: thm:3Sat-requires-time-1.5TM --></a> following this guideline:</p>
<ol class="enumerate1">
<li class="enumerate" id="x1-99005x1">Let <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="M" class="latex" /> be a 1.5 TM running in tine <img src="https://s0.wp.com/latex.php?latex=t%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=t%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=t%28n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="t(n)" class="latex" />. Divide the read-write tape of <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="M" class="latex" /> into consecutive blocks of <img src="https://s0.wp.com/latex.php?latex=b&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=b&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=b&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="b" class="latex" /> cells, shifted by an offset <img src="https://s0.wp.com/latex.php?latex=i+%5Cleq+b&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=i+%5Cleq+b&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=i+%5Cleq+b&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="i &#92;leq b" class="latex" />.  Prove that for every input there is an offset s.t. the sum of the lengths of all the crossing sequences between adjacent blocks is small.</li>
<li class="enumerate" id="x1-99007x2">Prove that <img src="https://s0.wp.com/latex.php?latex=1.5%5Ctext+%7BTM-Time%7D%28n%5E%7B1.1%7D%29%5Csubseteq+%5Cexists+y%5Cin+%5C%7B0%2C1%5C%7D+%5E%7Bn%5E%7B1-c%7D%7D%5Ctext+%7BTiSp%7D%28n%5E%7Bc%7D%2Cn%5E%7B1-c%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1.5%5Ctext+%7BTM-Time%7D%28n%5E%7B1.1%7D%29%5Csubseteq+%5Cexists+y%5Cin+%5C%7B0%2C1%5C%7D+%5E%7Bn%5E%7B1-c%7D%7D%5Ctext+%7BTiSp%7D%28n%5E%7Bc%7D%2Cn%5E%7B1-c%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1.5%5Ctext+%7BTM-Time%7D%28n%5E%7B1.1%7D%29%5Csubseteq+%5Cexists+y%5Cin+%5C%7B0%2C1%5C%7D+%5E%7Bn%5E%7B1-c%7D%7D%5Ctext+%7BTiSp%7D%28n%5E%7Bc%7D%2Cn%5E%7B1-c%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1.5&#92;text {TM-Time}(n^{1.1})&#92;subseteq &#92;exists y&#92;in &#92;{0,1&#92;} ^{n^{1-c}}&#92;text {TiSp}(n^{c},n^{1-c})" class="latex" />. (The right-hand side is the class of functions <img src="https://s0.wp.com/latex.php?latex=f%3A%5C%7B0%2C1%5C%7D%5E%2A+%5Cto+%5C%7B0%2C1%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=f%3A%5C%7B0%2C1%5C%7D%5E%2A+%5Cto+%5C%7B0%2C1%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=f%3A%5C%7B0%2C1%5C%7D%5E%2A+%5Cto+%5C%7B0%2C1%5C%7D+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="f:&#92;{0,1&#92;}^* &#92;to &#92;{0,1&#92;} " class="latex" /> for which there is a RAM <img src="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=M&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="M" class="latex" />       that on input <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28x%2Cy%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="(x,y)" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%7Cx%7C%3Dn&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Cx%7C%3Dn&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Cx%7C%3Dn&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="|x|=n" class="latex" />, runs in time <img src="https://s0.wp.com/latex.php?latex=n%5E%7Bc%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=n%5E%7Bc%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n%5E%7Bc%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="n^{c}" class="latex" /> and uses memory cells <img src="https://s0.wp.com/latex.php?latex=0..n%5E%7B1-c%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=0..n%5E%7B1-c%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=0..n%5E%7B1-c%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="0..n^{1-c}" class="latex" /> and s.t.&nbsp;<img src="https://s0.wp.com/latex.php?latex=f%28x%29%3D1%5CLeftrightarrow+%5Cexists+y%5Cin+%5C%7B0%2C1%5C%7D+%5E%7Bn%5E%7B1-c%7D%7DM%28x%2Cy%29%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=f%28x%29%3D1%5CLeftrightarrow+%5Cexists+y%5Cin+%5C%7B0%2C1%5C%7D+%5E%7Bn%5E%7B1-c%7D%7DM%28x%2Cy%29%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=f%28x%29%3D1%5CLeftrightarrow+%5Cexists+y%5Cin+%5C%7B0%2C1%5C%7D+%5E%7Bn%5E%7B1-c%7D%7DM%28x%2Cy%29%3D1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="f(x)=1&#92;Leftrightarrow &#92;exists y&#92;in &#92;{0,1&#92;} ^{n^{1-c}}M(x,y)=1" class="latex" />.)</li>
<li class="enumerate" id="x1-99009x3">Conclude the proof.</li>
</ol>
</div>
<p style="text-align:justify">
<h3 class="likesectionHead"><a id="x1-1000008.3"></a>Notes</h3>
<p style="text-align:justify">For a survey (not up to date) of this type of impossibility results see <span class="cite">[<a href="#XMelkebeek06">69</a>]</span>.</p>
<div class="thebibliography">
<p class="bibitem"><span class="biblabel">   [1]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:conf/focs/AbboudBW15"></a>Amir Abboud, Arturs Backurs, and Virginia&nbsp;Vassilevska Williams. Tight hardness      results for LCS and other sequence similarity measures.  In Venkatesan Guruswami,      editor, IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS      2015, Berkeley, CA, USA, 17-20 October, 2015, pages 59–78. IEEE Computer Society,      2015.</p>
<p class="bibitem"><span class="biblabel">   [2]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XAdleman78"></a>Leonard  Adleman.   Two  theorems  on  random  polynomial  time.   In  19th IEEE      Symp.&nbsp;on Foundations of Computer Science (FOCS), pages 75–83. 1978.</p>
<p class="bibitem"><span class="biblabel">   [3]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XAjt83"></a>Mikl≤s Ajtai.  <img src="https://s0.wp.com/latex.php?latex=%5CSigma+%5Csp+%7B1%7D%5Csb+%7B1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5CSigma+%5Csp+%7B1%7D%5Csb+%7B1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5CSigma+%5Csp+%7B1%7D%5Csb+%7B1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;Sigma &#92;sp {1}&#92;sb {1}" class="latex" />-formulae on finite structures.  Annals of Pure and Applied Logic,      24(1):1–48, 1983.</p>
<p class="bibitem"><span class="biblabel">   [4]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XAjtai05"></a>Mikl≤s Ajtai. A non-linear time lower bound for boolean branching programs. Theory      of Computing, 1(1):149–176, 2005.</p>
<p class="bibitem"><span class="biblabel">   [5]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XAll89"></a>Eric  Allender.   A  note  on  the  power  of  threshold  circuits.   In  30th Symposium      on Foundations of Computer Science, pages 580–584, Research Triangle Park, North      Carolina, 30 October–1 November 1989. IEEE.</p>
<p class="bibitem"><span class="biblabel">   [6]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XAllender01"></a>Eric Allender. The division breakthroughs. Bulletin of the EATCS, 74:61–77, 2001.</p>
<p class="bibitem"><span class="biblabel">   [7]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XAllenderK10"></a>Eric  Allender  and  Michal  Koucký.     Amplifying  lower  bounds  by  means  of      self-reducibility. J.&nbsp;of the ACM, 57(3), 2010.</p>
<p class="bibitem"><span class="biblabel">   [8]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XAGHP92"></a>Noga Alon, Oded Goldreich, Johan Hσstad, and RenΘ Peralta. Simple constructions      of  almost  <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k" class="latex" />-wise  independent  random  variables.   Random  Structures  &amp;  Algorithms,      3(3):289–304, 1992.</p>
<p class="bibitem"><span class="biblabel">   [9]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:journals/jcss/AngluinV79"></a>Dana Angluin<br />
and Leslie&nbsp;G. Valiant. Fast probabilistic algorithms for hamiltonian      circuits and matchings. J. Comput. Syst. Sci., 18(2):155–193, 1979.</p>
<p class="bibitem"><span class="biblabel">  [10]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XAroraLuMoSuSz98"></a>Sanjeev Arora, Carsten Lund, Rajeev Motwani, Madhu Sudan, and Mario Szegedy.      Proof  verification  and  the  hardness  of  approximation  problems.    J.&nbsp;of  the  ACM,      45(3):501–555, May 1998.</p>
<p class="bibitem"><span class="biblabel">  [11]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:journals/siamcomp/BackursI18"></a>Arturs Backurs and Piotr Indyk.  Edit distance cannot be computed in strongly      subquadratic time (unless SETH is false). SIAM J. Comput., 47(3):1087–1097, 2018.</p>
<p class="bibitem"><span class="biblabel">  [12]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XBatcher68"></a>Kenneth&nbsp;E. Batcher.  Sorting networks and their applications.  In AFIPS Spring      Joint Computing Conference, volume&nbsp;32, pages 307–314, 1968.</p>
<p class="bibitem"><span class="biblabel">  [13]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XBeameCH86"></a>Paul  Beame,  Stephen&nbsp;A.  Cook,  and  H.&nbsp;James  Hoover.   Log  depth  circuits  for      division and related problems. SIAM J. Comput., 15(4):994–1003, 1986.</p>
<p class="bibitem"><span class="biblabel">  [14]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XBSSV03"></a>Paul Beame, Michael Saks, Xiaodong Sun, and Erik Vee.   Time-space trade-off      lower  bounds  for  randomized  computation  of  decision  problems.   J.&nbsp;of  the  ACM,      50(2):154–195, 2003.</p>
<p class="bibitem"><span class="biblabel">  [15]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XBen-OrC92"></a>Michael Ben-Or and Richard Cleve. Computing algebraic formulas using a constant      number of registers. SIAM J.&nbsp;on Computing, 21(1):54–58, 1992.</p>
<p class="bibitem"><span class="biblabel">  [16]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:journals/cc/BussW15"></a>Samuel&nbsp;R.  Buss  and  Ryan  Williams.   Limits  on  alternation  trading  proofs  for      time-space lower bounds. Comput. Complex., 24(3):533–600, 2015.</p>
<p class="bibitem"><span class="biblabel">  [17]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:conf/stoc/ChenT19"></a>Lijie Chen and Roei Tell. Bootstrapping results for threshold circuits &#8220;just beyond&#8221;      known lower bounds.  In Moses Charikar and Edith Cohen, editors, Proceedings of the      51st Annual ACM SIGACT Symposium on Theory of Computing, STOC 2019, Phoenix,      AZ, USA, June 23-26, 2019, pages 34–41. ACM, 2019.</p>
<p class="bibitem"><span class="biblabel">  [18]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XCleve91"></a>Richard  Cleve.    Towards  optimal  simulations  of  formulas  by  bounded-width                                                                                                                                                                                          programs. Computational Complexity, 1:91–105, 1991.</p>
<p class="bibitem"><span class="biblabel">  [19]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XCook73"></a>Stephen&nbsp;A. Cook. A hierarchy for nondeterministic time complexity. J.&nbsp;of Computer      and System Sciences, 7(4):343–353, 1973.</p>
<p class="bibitem"><span class="biblabel">  [20]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:journals/siamcomp/Csanky76"></a>L.&nbsp;Csanky.     Fast  parallel  matrix  inversion  algorithms.     SIAM  J.  Comput.,      5(4):618–623, 1976.</p>
<p class="bibitem"><span class="biblabel">  [21]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:journals/jcss/Fortnow00"></a>Lance  Fortnow.   Time-space  tradeoffs  for  satisfiability.   J.  Comput.  Syst.  Sci.,      60(2):337–353, 2000.</p>
<p class="bibitem"><span class="biblabel">  [22]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:journals/jct/FraenkelL81"></a>Aviezri&nbsp;S. Fraenkel and David Lichtenstein. Computing a perfect strategy for n x n      chess requires time exponential in n. J. Comb. Theory, Ser. A, 31(2):199–214, 1981.</p>
<p class="bibitem"><span class="biblabel">  [23]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XFredmanS89"></a>Michael&nbsp;L. Fredman and Michael&nbsp;E. Saks.  The cell probe complexity of dynamic      data structures. In ACM Symp.&nbsp;on the Theory of Computing (STOC), pages 345–354,      1989.</p>
<p class="bibitem"><span class="biblabel">  [24]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XGajentaanO95"></a>Anka Gajentaan and Mark&nbsp;H. Overmars. On a class of <img src="https://s0.wp.com/latex.php?latex=%7BO%7D%28n%5E2%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BO%7D%28n%5E2%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BO%7D%28n%5E2%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="{O}(n^2)" class="latex" /> problems in computational      geometry. Comput. Geom., 5:165–185, 1995.</p>
<p class="bibitem"><span class="biblabel">  [25]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XGareyJ79"></a>M.&nbsp;R. Garey and David&nbsp;S. Johnson. Computers and Intractability: A Guide to the      Theory of NP-Completeness. W. H. Freeman, 1979.</p>
<p class="bibitem"><span class="biblabel">  [26]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XMR1549939"></a>K.&nbsp;G÷del.   ▄ber  formal  unentscheidbare  sΣtze  der  Principia  Mathematica  und      verwandter systeme I. Monatsh. Math. Phys., 38, 1931.</p>
<p class="bibitem"><span class="biblabel">  [27]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XGoldreich08Complexity"></a>Oded Goldreich. Computational Complexity: A Conceptual Perspective. Cambridge      University Press, 2008.</p>
<p class="bibitem"><span class="biblabel">  [28]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XGreenlawHR-Limits"></a>Raymond  Greenlaw,  H.&nbsp;James  Hoover,  and  Walter  Ruzzo.   Limits  to  Parallel      Computation: P-Completeness Theory. 02 2001.</p>
<p class="bibitem"><span class="biblabel">  [29]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="X10.4007/annals.2021.193.2.4"></a>David Harvey and Joris van&nbsp;der Hoeven. Integer multiplication in time <img src="https://s0.wp.com/latex.php?latex=O%28n%5Cmathrm+%7Blog%7D%5C%2C+n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=O%28n%5Cmathrm+%7Blog%7D%5C%2C+n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=O%28n%5Cmathrm+%7Blog%7D%5C%2C+n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="O(n&#92;mathrm {log}&#92;, n)" class="latex" />. Annals of      Mathematics, 193(2):563 – 617, 2021.</p>
<p class="bibitem"><span class="biblabel">  [30]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:journals/iandc/Hennie65"></a>F.&nbsp;C. Hennie.  One-tape, off-line turing machine computations.  Information and      Control, 8(6):553–578, 1965.</p>
<p class="bibitem"><span class="biblabel">  [31]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XHennieS66"></a>Fred  Hennie  and  Richard  Stearns.    Two-tape  simulation  of  multitape  turing      machines. J.&nbsp;of the ACM, 13:533–546, October 1966.</p>
<p class="bibitem"><span class="biblabel">  [32]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:journals/jacm/HopcroftPV77"></a>John&nbsp;E. Hopcroft, Wolfgang&nbsp;J. Paul, and Leslie&nbsp;G. Valiant. On time versus space.      J. ACM, 24(2):332–337, 1977.</p>
<p class="bibitem"><span class="biblabel">  [33]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XIP99"></a>Russell Impagliazzo and Ramamohan Paturi.   The complexity of <img src="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k" class="latex" />-sat.   In IEEE      Conf.&nbsp;on Computational Complexity (CCC), pages 237–, 1999.</p>
<p class="bibitem"><span class="biblabel">  [34]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XImpagliazzoPS97"></a>Russell Impagliazzo, Ramamohan Paturi, and Michael&nbsp;E. Saks. Size-depth tradeoffs      for threshold circuits. SIAM J. Comput., 26(3):693–707, 1997.</p>
<p class="bibitem"><span class="biblabel">  [35]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XIPZ01"></a>Russell Impagliazzo, Ramamohan Paturi, and Francis Zane.  Which problems have      strongly exponential complexity? J. Computer &amp; Systems Sciences, 63(4):512–530, Dec      2001.</p>
<p class="bibitem"><span class="biblabel">  [36]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XImW97"></a>Russell  Impagliazzo  and  Avi  Wigderson.    <img src="https://s0.wp.com/latex.php?latex=%5Cmathit+%7BP%7D+%3D+%5Cmathit+%7BBPP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathit+%7BP%7D+%3D+%5Cmathit+%7BBPP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathit+%7BP%7D+%3D+%5Cmathit+%7BBPP%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathit {P} = &#92;mathit {BPP}" class="latex" />  if  <img src="https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="E" class="latex" />  requires  exponential  circuits:      Derandomizing the XOR lemma.  In 29th ACM Symp.&nbsp;on the Theory of Computing      (STOC), pages 220–229. ACM, 1997.</p>
<p class="bibitem"><span class="biblabel">  [37]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XKarpLi82"></a>Richard&nbsp;M.  Karp  and  Richard&nbsp;J.  Lipton.    Turing  machines  that  take  advice.      L’Enseignement MathΘmatique. Revue Internationale. IIe SΘrie, 28(3-4):191–209, 1982.</p>
<p class="bibitem"><span class="biblabel">  [38]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XKobayashi1985OnTS"></a>Kojiro Kobayashi.  On the structure of one-tape nondeterministic turing machine      time hierarchy. Theor. Comput. Sci., 40:175–193, 1985.</p>
<p class="bibitem"><span class="biblabel">  [39]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:journals/siamcomp/LarsenWY20"></a>Kasper&nbsp;Green Larsen, Omri Weinstein, and Huacheng Yu. Crossing the logarithmic      barrier for dynamic boolean data structure lower bounds.  SIAM J. Comput., 49(5),      2020.</p>
<p class="bibitem"><span class="biblabel">  [40]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XLevin73"></a>Leonid&nbsp;A.  Levin.    Universal  sequential  search  problems.    Problemy  Peredachi      Informatsii, 9(3):115–116, 1973.</p>
<p class="bibitem"><span class="biblabel">  [41]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XLundFoKaNi92"></a>Carsten Lund, Lance Fortnow, Howard Karloff, and Noam Nisan. Algebraic methods      for interactive proof systems. J.&nbsp;of the ACM, 39(4):859–868, October 1992.</p>
<p class="bibitem"><span class="biblabel">  [42]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XLupanov58"></a>O.&nbsp;B. Lupanov. A method of circuit synthesis. Izv. VUZ Radiofiz., 1:120–140, 1958.</p>
<p class="bibitem"><span class="biblabel">  [43]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XMaS87"></a>Wolfgang Maass and Amir Schorr. Speed-up of Turing machines with one work tape      and a two-way input tape. SIAM J.&nbsp;on Computing, 16(1):195–202, 1987.</p>
<p class="bibitem"><span class="biblabel">  [44]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XBarrington89"></a>David&nbsp;A.  Mix  Barrington.   Bounded-width  polynomial-size  branching  programs      recognize  exactly  those  languages  in  NC<img src="https://s0.wp.com/latex.php?latex=%5E1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5E1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5E1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="^1" class="latex" />.    J.&nbsp;of  Computer  and  System  Sciences,      38(1):150–164, 1989.</p>
<p class="bibitem"><span class="biblabel">  [45]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XNaN93"></a>Joseph Naor and Moni Naor.  Small-bias probability spaces: efficient constructions      and applications. SIAM J.&nbsp;on Computing, 22(4):838–856, 1993.</p>
<p class="bibitem"><span class="biblabel">  [46]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XNechiporuk66"></a>E.&nbsp;I. Nechiporuk. A boolean function. Soviet Mathematics-Doklady, 169(4):765–766,      1966.</p>
<p class="bibitem"><span class="biblabel">  [47]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XNep70"></a>Valery&nbsp;A. Nepomnjaščiĭ. Rudimentary predicates and Turing calculations. Soviet      Mathematics-Doklady, 11(6):1462–1465, 1970.</p>
<p class="bibitem"><span class="biblabel">  [48]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XViolaNEU-ram2sat-neu-author"></a>NEU. From RAM to SAT. Available at <a href="http://www.ccs.neu.edu/home/viola/" rel="nofollow">http://www.ccs.neu.edu/home/viola/</a>, 2012.</p>
<p class="bibitem"><span class="biblabel">                                                                                                                                                                                      [49]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:journals/jcss/PapadimitriouY91"></a>Christos&nbsp;H. Papadimitriou and Mihalis Yannakakis. Optimization, approximation,      and complexity classes. J. Comput. Syst. Sci., 43(3):425–440, 1991.</p>
<p class="bibitem"><span class="biblabel">  [50]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XPPST83"></a>Wolfgang&nbsp;J. Paul, Nicholas Pippenger, Endre SzemerΘdi, and William&nbsp;T. Trotter.      On determinism versus non-determinism and related problems (preliminary version). In      IEEE Symp.&nbsp;on Foundations of Computer Science (FOCS), pages 429–438, 1983.</p>
<p class="bibitem"><span class="biblabel">  [51]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XPippengerF79"></a>Nicholas Pippenger and Michael&nbsp;J. Fischer. Relations among complexity measures.      J.&nbsp;of the ACM, 26(2):361–381, 1979.</p>
<p class="bibitem"><span class="biblabel">  [52]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XRaz87"></a>Alexander Razborov. Lower bounds on the dimension of schemes of bounded depth      in a complete basis containing the logical addition function.  Akademiya Nauk SSSR.      Matematicheskie Zametki, 41(4):598–607, 1987.  English translation in Mathematical      Notes of the Academy of Sci. of the USSR, 41(4):333-338, 1987.</p>
<p class="bibitem"><span class="biblabel">  [53]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XReingold08"></a>Omer Reingold. Undirected connectivity in log-space. J.&nbsp;of the ACM, 55(4), 2008.</p>
<p class="bibitem"><span class="biblabel">  [54]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:journals/siamcomp/Robson84"></a>J.&nbsp;M.  Robson.    N  by  N  checkers  is  exptime  complete.    SIAM  J.  Comput.,      13(2):252–267, 1984.</p>
<p class="bibitem"><span class="biblabel">  [55]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:conf/coco/Santhanam01"></a>Rahul Santhanam.   On separators, segregators and time versus space.   In IEEE      Conf.&nbsp;on Computational Complexity (CCC), pages 286–294, 2001.</p>
<p class="bibitem"><span class="biblabel">  [56]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XSAVITCH1970177"></a>Walter&nbsp;J. Savitch.  Relationships between nondeterministic and deterministic tape      complexities. Journal of Computer and System Sciences, 4(2):177–192, 1970.</p>
<p class="bibitem"><span class="biblabel">  [57]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:journals/siamcomp/Schonhage80"></a>Arnold Sch÷nhage. Storage modification machines. SIAM J. Comput., 9(3):490–508,      1980.</p>
<p class="bibitem"><span class="biblabel">  [58]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XShamir92"></a>Adi Shamir. IP = PSPACE. J.&nbsp;of the ACM, 39(4):869–877, October 1992.</p>
<p class="bibitem"><span class="biblabel">  [59]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XMR29860"></a>Claude&nbsp;E. Shannon. The synthesis of two-terminal switching circuits. Bell System                                                                                                                                                                                          Tech. J., 28:59–98, 1949.</p>
<p class="bibitem"><span class="biblabel">  [60]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XSho90"></a>Victor Shoup. New algorithms for finding irreducible polynomials over finite fields.      Mathematics of Computation, 54(189):435–447, 1990.</p>
<p class="bibitem"><span class="biblabel">  [61]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XSiegel04"></a>Alan Siegel. On universal classes of extremely random constant-time hash functions.      SIAM J.&nbsp;on Computing, 33(3):505–543, 2004.</p>
<p class="bibitem"><span class="biblabel">  [62]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XSip83b"></a>Michael Sipser. A complexity theoretic approach to randomness. In ACM Symp.&nbsp;on      the Theory of Computing (STOC), pages 330–335, 1983.</p>
<p class="bibitem"><span class="biblabel">  [63]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XSmo87"></a>Roman Smolensky.  Algebraic methods in the theory of lower bounds for Boolean      circuit complexity.  In 19th ACM Symp.&nbsp;on the Theory of Computing (STOC), pages      77–82. ACM, 1987.</p>
<p class="bibitem"><span class="biblabel">  [64]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XMR2145856"></a>Larry Stockmeyer and Albert&nbsp;R. Meyer.  Cosmological lower bound on the circuit      complexity of a small problem in logic. J. ACM, 49(6):753–784, 2002.</p>
<p class="bibitem"><span class="biblabel">  [65]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XToda91"></a>Seinosuke Toda.   PP is as hard as the polynomial-time hierarchy.   SIAM J.&nbsp;on      Computing, 20(5):865–877, 1991.</p>
<p class="bibitem"><span class="biblabel">  [66]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:journals/x/Turing37"></a>Alan&nbsp;M.   Turing.      On   computable   numbers,   with   an   application   to   the      entscheidungsproblem. Proc. London Math. Soc., s2-42(1):230–265, 1937.</p>
<p class="bibitem"><span class="biblabel">  [67]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XVal77"></a>Leslie&nbsp;G.  Valiant.   Graph-theoretic  arguments  in  low-level  complexity.   In  6th      Symposium on Mathematical Foundations of Computer Science, volume&nbsp;53 of Lecture      Notes in Computer Science, pages 162–176. Springer, 1977.</p>
<p class="bibitem"><span class="biblabel">  [68]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:journals/tcs/ValiantV86"></a>Leslie&nbsp;G. Valiant and Vijay&nbsp;V. Vazirani. NP is as easy as detecting unique solutions.      Theor. Comput. Sci., 47(3):85–93, 1986.</p>
<p class="bibitem"><span class="biblabel">  [69]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XMelkebeek06"></a>Dieter  van  Melkebeek.   A  survey  of  lower  bounds  for  satisfiability  and  related                                                                                                                                                                                          problems. Foundations and Trends in Theoretical Computer Science, 2(3):197–303, 2006.</p>
<p class="bibitem"><span class="biblabel">  [70]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:journals/tcs/MelkebeekR05"></a>Dieter van Melkebeek and Ran Raz.  A time lower bound for satisfiability.  Theor.      Comput. Sci., 348(2-3):311–320, 2005.</p>
<p class="bibitem"><span class="biblabel">  [71]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XDBLP:journals/tcs/Vinodchandran05"></a>N.&nbsp;V. Vinodchandran.  A note on the circuit complexity of PP.  Theor. Comput.      Sci., 347(1-2):415–418, 2005.</p>
<p class="bibitem"><span class="biblabel">  [72]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XViolaBPvsE"></a>Emanuele Viola.  On approximate majority and probabilistic time.  Computational      Complexity, 18(3):337–375, 2009.</p>
<p class="bibitem"><span class="biblabel">  [73]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="Xviola-FTTCS09"></a>Emanuele Viola. On the power of small-depth computation. Foundations and Trends      in Theoretical Computer Science, 5(1):1–72, 2009.</p>
<p class="bibitem"><span class="biblabel">  [74]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="XViola-xxx"></a>Emanuele Viola.  Reducing 3XOR to listing triangles, an exposition.  Available at      <a href="http://www.ccs.neu.edu/home/viola/" rel="nofollow">http://www.ccs.neu.edu/home/viola/</a>, 2011.</p>
<p class="bibitem"><span class="biblabel">  [75]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="Xviola-datastructurelb-implies-cclb"></a>Emanuele Viola.  Lower bounds for data structures with space close to maximum      imply  circuit  lower  bounds.    Theory  of  Computing,  15:1–9,  2019.    Available  at      <a href="http://www.ccs.neu.edu/home/viola/" rel="nofollow">http://www.ccs.neu.edu/home/viola/</a>.</p>
<p class="bibitem"><span class="biblabel">  [76]<span class="bibsp">&nbsp;&nbsp;&nbsp;</span></span><a id="Xviola-tm"></a>Emanuele  Viola.   Pseudorandom  bits  and  lower  bounds  for  randomized  turing      machines. Theory of Computing, 18(10):1–12, 2022.</p>
</div>
<p class="authors">By Manu</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-14T14:41:26Z">Friday, April 14 2023, 14:41</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2023/04/14/junior-professor-chaire-de-professeur-junior-at-university-lyon-1-apply-by-may-12-2023/'>junior professor (chaire de professeur junior) at University Lyon 1 (apply by May 12, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Tenure track position with a low teaching load and an attractive financial package. Teaching at University Lyon 1 and research in theoretical computer science at the LIP laboratory, located at Ecole Normale Supérieure de Lyon. Teaching in English is possible. Contact person for research: Nicolas Trotignon. For teaching: Saida Bouakaz. Website: www.univ-lyon1.fr/universite/travailler-a-lyon-1/recrutement-chaires-de-professeurs-junior-2022#.ZDkD7exBz9G Email: nicolas.trotignon@ens-lyon.fr
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Tenure track position with a low teaching load and an attractive financial package. Teaching at University Lyon 1 and research in theoretical computer science at the LIP laboratory, located at Ecole Normale Supérieure de Lyon. Teaching in English is possible. Contact person for research: Nicolas Trotignon. For teaching: Saida Bouakaz.</p>
<p>Website: <a href="https://www.univ-lyon1.fr/universite/travailler-a-lyon-1/recrutement-chaires-de-professeurs-junior-2022#.ZDkD7exBz9G">https://www.univ-lyon1.fr/universite/travailler-a-lyon-1/recrutement-chaires-de-professeurs-junior-2022#.ZDkD7exBz9G</a><br />
Email: nicolas.trotignon@ens-lyon.fr</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-14T07:51:11Z">Friday, April 14 2023, 07:51</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.06523'>The 2-Attractor Problem is NP-Complete</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Janosch Fuchs, Philip Whittington</p><p>A $k$-attractor is a combinatorial object unifying dictionary-based
compression. It allows to compare the repetitiveness measures of different
dictionary compressors such as Lempel-Ziv 77, the Burrows-Wheeler transform,
straight line programs and macro schemes. For a string $ T \in \Sigma^n$, the
$k$-attractor is defined as a set of positions $\Gamma \subseteq [1,n]$, such
that every distinct substring of length at most $k$ is covered by at least one
of the selected positions. Thus, if a substring occurs multiple times in $T$,
one position suffices to cover it. A 1-attractor is easily computed in linear
time, while Kempa and Prezza [STOC 2018] have shown that for $k \geq 3$, it is
NP-complete to compute the smallest $k$-attractor by a reduction from $k$-set
cover.
</p>
<p>The main result of this paper answers the open question for the complexity of
the 2-attractor problem, showing that the problem remains NP-complete. Kempa
and Prezza's proof for $k \geq 3$ also reduces the 2-attractor problem to the
2-set cover problem, which is equivalent to edge cover, but that does not fully
capture the complexity of the 2-attractor problem. For this reason, we extend
edge cover by a color function on the edges, yielding the colorful edge cover
problem. Any edge cover must then satisfy the additional constraint that each
color is represented. This extension raises the complexity such that colorful
edge cover becomes NP-complete while also more precisely modeling the
2-attractor problem. We obtain a reduction showing $k$-attractor to be
NP-complete for any $k \geq 2$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Fuchs_J/0/1/0/all/0/1">Janosch Fuchs</a>, <a href="http://arxiv.org/find/cs/1/au:+Whittington_P/0/1/0/all/0/1">Philip Whittington</a></p><p>A $k$-attractor is a combinatorial object unifying dictionary-based
compression. It allows to compare the repetitiveness measures of different
dictionary compressors such as Lempel-Ziv 77, the Burrows-Wheeler transform,
straight line programs and macro schemes. For a string $ T \in \Sigma^n$, the
$k$-attractor is defined as a set of positions $\Gamma \subseteq [1,n]$, such
that every distinct substring of length at most $k$ is covered by at least one
of the selected positions. Thus, if a substring occurs multiple times in $T$,
one position suffices to cover it. A 1-attractor is easily computed in linear
time, while Kempa and Prezza [STOC 2018] have shown that for $k \geq 3$, it is
NP-complete to compute the smallest $k$-attractor by a reduction from $k$-set
cover.
</p>
<p>The main result of this paper answers the open question for the complexity of
the 2-attractor problem, showing that the problem remains NP-complete. Kempa
and Prezza's proof for $k \geq 3$ also reduces the 2-attractor problem to the
2-set cover problem, which is equivalent to edge cover, but that does not fully
capture the complexity of the 2-attractor problem. For this reason, we extend
edge cover by a color function on the edges, yielding the colorful edge cover
problem. Any edge cover must then satisfy the additional constraint that each
color is represented. This extension raises the complexity such that colorful
edge cover becomes NP-complete while also more precisely modeling the
2-attractor problem. We obtain a reduction showing $k$-attractor to be
NP-complete for any $k \geq 2$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-14T00:30:00Z">Friday, April 14 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.06713'>Influences of Fourier Completely Bounded Polynomials and Classical Simulation of Quantum Algorithms</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Francisco Escudero Guti&#xe9;rrez</p><p>We give a new presentation of the main result of Arunachalam, Bri\"et and
Palazuelos (SICOMP'19) and show that quantum query algorithms are characterized
by a new class of polynomials which we call Fourier completely bounded
polynomials. We conjecture that all such polynomials have an influential
variable. This conjecture is weaker than the famous Aaronson-Ambainis (AA)
conjecture (Theory of Computing'14), but has the same implications for
classical simulation of quantum query algorithms.
</p>
<p>We prove a new case of the AA conjecture by showing that it holds for
homogeneous Fourier completely bounded polynomials. This implies that if the
output of $d$-query quantum algorithm is a homogeneous polynomial $p$ of degree
$2d$, then it has a variable with influence at least $Var[p]^2$.
</p>
<p>In addition, we give an alternative proof of the results of Bansal, Sinha and
de Wolf (CCC'22 and QIP'23) showing that block-multilinear completely bounded
polynomials have influential variables. Our proof is simpler, obtains better
constants and does not use randomness.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Gutierrez_F/0/1/0/all/0/1">Francisco Escudero Guti&#xe9;rrez</a></p><p>We give a new presentation of the main result of Arunachalam, Bri\"et and
Palazuelos (SICOMP'19) and show that quantum query algorithms are characterized
by a new class of polynomials which we call Fourier completely bounded
polynomials. We conjecture that all such polynomials have an influential
variable. This conjecture is weaker than the famous Aaronson-Ambainis (AA)
conjecture (Theory of Computing'14), but has the same implications for
classical simulation of quantum query algorithms.
</p>
<p>We prove a new case of the AA conjecture by showing that it holds for
homogeneous Fourier completely bounded polynomials. This implies that if the
output of $d$-query quantum algorithm is a homogeneous polynomial $p$ of degree
$2d$, then it has a variable with influence at least $Var[p]^2$.
</p>
<p>In addition, we give an alternative proof of the results of Bansal, Sinha and
de Wolf (CCC'22 and QIP'23) showing that block-multilinear completely bounded
polynomials have influential variables. Our proof is simpler, obtains better
constants and does not use randomness.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-14T00:30:00Z">Friday, April 14 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.06704'>How Will It Drape Like? Capturing Fabric Mechanics from Depth Images</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Carlos Rodriguez-Pardo, Melania Prieto-Martin, Dan Casas, Elena Garces</p><p>We propose a method to estimate the mechanical parameters of fabrics using a
casual capture setup with a depth camera. Our approach enables to create
mechanically-correct digital representations of real-world textile materials,
which is a fundamental step for many interactive design and engineering
applications. As opposed to existing capture methods, which typically require
expensive setups, video sequences, or manual intervention, our solution can
capture at scale, is agnostic to the optical appearance of the textile, and
facilitates fabric arrangement by non-expert operators. To this end, we propose
a sim-to-real strategy to train a learning-based framework that can take as
input one or multiple images and outputs a full set of mechanical parameters.
Thanks to carefully designed data augmentation and transfer learning protocols,
our solution generalizes to real images despite being trained only on synthetic
data, hence successfully closing the sim-to-real loop.Key in our work is to
demonstrate that evaluating the regression accuracy based on the similarity at
parameter space leads to an inaccurate distances that do not match the human
perception. To overcome this, we propose a novel metric for fabric drape
similarity that operates on the image domain instead on the parameter space,
allowing us to evaluate our estimation within the context of a similarity rank.
We show that out metric correlates with human judgments about the perception of
drape similarity, and that our model predictions produce perceptually accurate
results compared to the ground truth parameters.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Pardo_C/0/1/0/all/0/1">Carlos Rodriguez-Pardo</a>, <a href="http://arxiv.org/find/cs/1/au:+Prieto_Martin_M/0/1/0/all/0/1">Melania Prieto-Martin</a>, <a href="http://arxiv.org/find/cs/1/au:+Casas_D/0/1/0/all/0/1">Dan Casas</a>, <a href="http://arxiv.org/find/cs/1/au:+Garces_E/0/1/0/all/0/1">Elena Garces</a></p><p>We propose a method to estimate the mechanical parameters of fabrics using a
casual capture setup with a depth camera. Our approach enables to create
mechanically-correct digital representations of real-world textile materials,
which is a fundamental step for many interactive design and engineering
applications. As opposed to existing capture methods, which typically require
expensive setups, video sequences, or manual intervention, our solution can
capture at scale, is agnostic to the optical appearance of the textile, and
facilitates fabric arrangement by non-expert operators. To this end, we propose
a sim-to-real strategy to train a learning-based framework that can take as
input one or multiple images and outputs a full set of mechanical parameters.
Thanks to carefully designed data augmentation and transfer learning protocols,
our solution generalizes to real images despite being trained only on synthetic
data, hence successfully closing the sim-to-real loop.Key in our work is to
demonstrate that evaluating the regression accuracy based on the similarity at
parameter space leads to an inaccurate distances that do not match the human
perception. To overcome this, we propose a novel metric for fabric drape
similarity that operates on the image domain instead on the parameter space,
allowing us to evaluate our estimation within the context of a similarity rank.
We show that out metric correlates with human judgments about the perception of
drape similarity, and that our model predictions produce perceptually accurate
results compared to the ground truth parameters.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-14T00:30:00Z">Friday, April 14 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2304.06715'>Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jonathan Crabb&#xe9;, Mihaela van der Schaar</p><p>Interpretability methods are valuable only if their explanations faithfully
describe the explained model. In this work, we consider neural networks whose
predictions are invariant under a specific symmetry group. This includes
popular architectures, ranging from convolutional to graph neural networks. Any
explanation that faithfully explains this type of model needs to be in
agreement with this invariance property. We formalize this intuition through
the notion of explanation invariance and equivariance by leveraging the
formalism from geometric deep learning. Through this rigorous formalism, we
derive (1) two metrics to measure the robustness of any interpretability method
with respect to the model symmetry group; (2) theoretical robustness guarantees
for some popular interpretability methods and (3) a systematic approach to
increase the invariance of any interpretability method with respect to a
symmetry group. By empirically measuring our metrics for explanations of models
associated with various modalities and symmetry groups, we derive a set of 5
guidelines to allow users and developers of interpretability methods to produce
robust explanations.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Crabbe_J/0/1/0/all/0/1">Jonathan Crabb&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1">Mihaela van der Schaar</a></p><p>Interpretability methods are valuable only if their explanations faithfully
describe the explained model. In this work, we consider neural networks whose
predictions are invariant under a specific symmetry group. This includes
popular architectures, ranging from convolutional to graph neural networks. Any
explanation that faithfully explains this type of model needs to be in
agreement with this invariance property. We formalize this intuition through
the notion of explanation invariance and equivariance by leveraging the
formalism from geometric deep learning. Through this rigorous formalism, we
derive (1) two metrics to measure the robustness of any interpretability method
with respect to the model symmetry group; (2) theoretical robustness guarantees
for some popular interpretability methods and (3) a systematic approach to
increase the invariance of any interpretability method with respect to a
symmetry group. By empirically measuring our metrics for explanations of models
associated with various modalities and symmetry groups, we derive a set of 5
guidelines to allow users and developers of interpretability methods to produce
robust explanations.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-04-14T00:30:00Z">Friday, April 14 2023, 00:30</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
