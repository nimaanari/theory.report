<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>
  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.4 (2022-04-12) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <link rel='stylesheet' type='text/css' href='css/font-awesome.css'>
  <link rel='stylesheet' type='text/css' href='css/blank.css'>
</head>
<body>
  <div id='navwrap'>
    <div id='nav'>
      <p>
        Last Update
      </p>
      <p class='small'>
        
          <time class='timeago' datetime="2022-09-21T06:34:50Z">Wednesday, September 21 2022, 06:34</time>
        
      </p>

      <p>Feeds</p>
      <ul class='subscriptions small' >
      
        <li>
          <a href='http://arxiv.org/rss/cs.CC'><img src='i/feed.png'></a>
          <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
          
        </li>
      
        <li>
          <a href='http://arxiv.org/rss/cs.CG'><img src='i/feed.png'></a>
          <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
          
        </li>
      
        <li>
          <a href='http://arxiv.org/rss/cs.DS'><img src='i/feed.png'></a>
          <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
          
        </li>
      
        <li>
          <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='i/feed.png'></a>
          <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
          
        </li>
      
        <li>
          <a href='https://adamsheffer.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
          
        </li>
      
        <li>
          <a href='https://adamdsmith.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
          
        </li>
      
        <li>
          <a href='https://polylogblog.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
          
        </li>
      
        <li>
          <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='i/feed.png'></a>
          <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
          
        </li>
      
        <li>
          <a href='http://www.argmin.net/feed.xml'><img src='i/feed.png'></a>
          <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
          
        </li>
      
        <li>
          <a href='http://bit-player.org/feed/atom/'><img src='i/feed.png'></a>
          <a href='http://bit-player.org'>bit-player</a>
          
        </li>
      
        <li>
          <a href='https://cstheory-jobs.org/feed/'><img src='i/feed.png'></a>
          <a href='https://cstheory-jobs.org'>CCI: jobs</a>
          
        </li>
      
        <li>
          <a href='https://cstheory-events.org/feed/'><img src='i/feed.png'></a>
          <a href='https://cstheory-events.org'>CS Theory Events</a>
          
        </li>
      
        <li>
          <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='i/feed.png'></a>
          <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
          
        </li>
      
        <li>
          <a href='https://11011110.github.io/blog/feed.xml'><img src='i/feed.png'></a>
          <a href='https://11011110.github.io/blog/'>David Eppstein</a>
          
        </li>
      
        <li>
          <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='i/feed.png'></a>
          <a href='https://daveagp.wordpress.com'>David Pritchard</a>
          
        </li>
      
        <li>
          <a href='https://decentdescent.org/feed.xml'><img src='i/feed.png'></a>
          <a href='https://decentdescent.org/'>Decent Descent</a>
          
        </li>
      
        <li>
          <a href='https://decentralizedthoughts.github.io/feed'><img src='i/feed.png'></a>
          <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
          
        </li>
      
        <li>
          <a href='https://differentialprivacy.org/feed.xml'><img src='i/feed.png'></a>
          <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
          
        </li>
      
        <li>
          <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='i/feed.png'></a>
          <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
          
        </li>
      
        <li>
          <a href='https://emanueleviola.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
          
        </li>
      
        <li>
          <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='i/feed.png'></a>
          <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
          
        </li>
      
        <li>
          <a href='https://dstheory.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
          
        </li>
      
        <li>
          <a href='https://francisbach.com/feed/'><img src='i/feed.png'></a>
          <a href='https://francisbach.com'>Francis Bach</a>
          
        </li>
      
        <li>
          <a href='https://gilkalai.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
          
        </li>
      
        <li>
          <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='i/feed.png'></a>
          <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
          
        </li>
      
        <li>
          <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='i/feed.png'></a>
          <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
          
        </li>
      
        <li>
          <a href='https://gradientscience.org/feed.xml'><img src='i/feed.png'></a>
          <a href='https://gradientscience.org/'>Gradient Science</a>
          
        </li>
      
        <li>
          <a href='http://grigory.us/blog/feed.xml'><img src='i/feed.png'></a>
          <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
          
        </li>
      
        <li>
          <a href='https://tcsmath.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
          
        </li>
      
        <li>
          <a href='https://kamathematics.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
          
        </li>
      
        <li>
          <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='i/feed.png'></a>
          <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
          
        </li>
      
        <li>
          <a href='https://lucatrevisan.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
          
        </li>
      
        <li>
          <a href='https://mittheory.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
          
        </li>
      
        <li>
          <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='i/feed.png'></a>
          <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
          
        </li>
      
        <li>
          <a href='http://blog.mrtz.org/feed.xml'><img src='i/feed.png'></a>
          <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
          
        </li>
      
        <li>
          <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='i/feed.png'></a>
          <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
          
        </li>
      
        <li>
          <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
          
        </li>
      
        <li>
          <a href='http://www.solipsistslog.com/feed/'><img src='i/feed.png'></a>
          <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
          
        </li>
      
        <li>
          <a href='http://www.offconvex.org/feed.xml'><img src='i/feed.png'></a>
          <a href='http://offconvex.github.io/'>Off the Convex Path</a>
          
        </li>
      
        <li>
          <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='i/feed.png'></a>
          <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
          
        </li>
      
        <li>
          <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='i/feed.png'></a>
          <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
          
        </li>
      
        <li>
          <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='i/feed.png'></a>
          <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
          
        </li>
      
        <li>
          <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='i/feed.png'></a>
          <a href='https://blogs.princeton.edu/imabandit'>SÃ©bastien Bubeck</a>
          
        </li>
      
        <li>
          <a href='https://scottaaronson.blog/?feed=atom'><img src='i/feed.png'></a>
          <a href='https://scottaaronson.blog'>Scott Aaronson</a>
          
        </li>
      
        <li>
          <a href='https://blog.simons.berkeley.edu/feed/'><img src='i/feed.png'></a>
          <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
          
        </li>
      
        <li>
          <a href='https://tcsplus.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
          
        </li>
      
        <li>
          <a href='https://toc4fairness.org/feed/'><img src='i/feed.png'></a>
          <a href='https://toc4fairness.org'>TOC for Fairness</a>
          
        </li>
      
        <li>
          <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='i/feed.png'></a>
          <a href='http://blog.geomblog.org/'>The Geomblog</a>
          
        </li>
      
        <li>
          <a href='https://www.let-all.com/blog/feed/'><img src='i/feed.png'></a>
          <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
          
        </li>
      
        <li>
          <a href='https://theorydish.blog/feed/'><img src='i/feed.png'></a>
          <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
          
        </li>
      
        <li>
          <a href='https://thmatters.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://thmatters.wordpress.com'>Theory Matters</a>
          
        </li>
      
        <li>
          <a href='https://mycqstate.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
          
        </li>
      
        <li>
          <a href='https://agtb.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
          
        </li>
      
        <li>
          <a href='https://windowsontheory.org/feed/'><img src='i/feed.png'></a>
          <a href='https://windowsontheory.org'>Windows on Theory</a>
          
        </li>
      
      </ul>

      <p class='small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
      <p class='small'>Subscribe to the <a href="atom.xml">Atom feed</a> or <a href="rss20.xml">RSS feed</a> to stay up to date.</p>
      <p class='small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
      <p class='small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
    </div>
  </div>

  <div id='opts'>
    <div style='width: 100%; text-align: right;'>
    <img src='i/view-headlines.png' id='show-headlines' title='Show Headlines Only' width='24' height='24'>
    <img src='i/view-snippets.png' id='show-snippets' title='Show Snippets' width='24' height='24'>
    <img src='i/view-standard.png' id='show-fulltext' title='Show Full Text' width='24' height='24'>
    </div>
  </div>

  <h1>
    Theory of Computing Report
  </h1>

  <div id="articles">
    
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Wednesday, September 21
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://cstheory-jobs.org/2022/09/21/teaching-professor-at-uc-san-diego-apply-by-october-15-2022/'>Teaching professor at UC San Diego (apply by October 15, 2022)</a></h3>
          <p class='item-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          UC San Diego Computer Science department seeks applications for an Assistant Teaching Professor. Teaching Professors are full members of the academic senate and are eligible for Security of Employment, analogous to tenure. Teaching Professors have an increased emphasis on teaching, while maintaining an active program of research, in their research area and/or education. Website: apol-recruit.ucsd.edu/JPF03253 [&#8230;]
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p>UC San Diego Computer Science department seeks applications for an Assistant Teaching Professor. Teaching Professors are full members of the academic senate and are eligible for Security of Employment, analogous to tenure. Teaching Professors have an increased emphasis on teaching, while maintaining an active program of research, in their research area and/or education.</p>
<p>Website: <a href="https://apol-recruit.ucsd.edu/JPF03253">https://apol-recruit.ucsd.edu/JPF03253</a><br />
Email: shachar.lovett@gmail.com</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-21T06:20:52Z">Wednesday, September 21 2022, 06:20</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.09527'>Intrinsic Simulations and Universality in Automata Networks</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Mart&#xed;n R&#xed;os-Wilson, Guillaume Theyssier (I2M)</p><p>An automata network (AN) is a finite graph where each node holds a state from
a finite alphabet and is equipped with a local map defining the evolution of
the state of the node depending on its neighbors. They are studied both from
the dynamical and the computational complexity point of view. Inspired from
well-established notions in the context of cellular automata, we develop a
theory of intrinsic simulations and universality for families of automata
networks. We establish many consequences of intrinsic universality in terms of
complexity of orbits (periods of attractors, transients, etc) as well as
hardness of the standard well-studied decision problems for automata networks
(short/long term prediction, reachability, etc). In the way, we prove
orthogonality results for these problems: the hardness of a single one does not
imply hardness of the others, while intrinsic universality implies hardness of
all of them. As a complement, we develop a proof technique to establish
intrinsic simulation and universality results which is suitable to deal with
families of symmetric networks were connections are non-oriented. It is based
on an operation of glueing of networks, which allows to produce complex orbits
in large networks from compatible pseudo-orbits in small networks. As an
illustration, we give a short proof that the family of networks were each node
obeys the rule of the 'game of life' cellular automaton is strongly universal.
This formalism and proof technique is also applied in a companion paper devoted
to studying the effect of update schedules on intrinsic universality for
concrete symmetric families of automata networks.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Rios_Wilson_M/0/1/0/all/0/1">Mart&#xed;n R&#xed;os-Wilson</a>, <a href="http://arxiv.org/find/cs/1/au:+Theyssier_G/0/1/0/all/0/1">Guillaume Theyssier</a> (I2M)</p><p>An automata network (AN) is a finite graph where each node holds a state from
a finite alphabet and is equipped with a local map defining the evolution of
the state of the node depending on its neighbors. They are studied both from
the dynamical and the computational complexity point of view. Inspired from
well-established notions in the context of cellular automata, we develop a
theory of intrinsic simulations and universality for families of automata
networks. We establish many consequences of intrinsic universality in terms of
complexity of orbits (periods of attractors, transients, etc) as well as
hardness of the standard well-studied decision problems for automata networks
(short/long term prediction, reachability, etc). In the way, we prove
orthogonality results for these problems: the hardness of a single one does not
imply hardness of the others, while intrinsic universality implies hardness of
all of them. As a complement, we develop a proof technique to establish
intrinsic simulation and universality results which is suitable to deal with
families of symmetric networks were connections are non-oriented. It is based
on an operation of glueing of networks, which allows to produce complex orbits
in large networks from compatible pseudo-orbits in small networks. As an
illustration, we give a short proof that the family of networks were each node
obeys the rule of the 'game of life' cellular automaton is strongly universal.
This formalism and proof technique is also applied in a companion paper devoted
to studying the effect of update schedules on intrinsic universality for
concrete symmetric families of automata networks.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-21T00:30:00Z">Wednesday, September 21 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.09788'>VEST is W[2]-hard</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Michael Skotnica</p><p>In this short note, we show that the problem of VEST is $W[2]$-hard for
parameter $k$. This strengthens a result of Matou\v{s}ek, who showed
$W[1]$-hardness of that problem. The consequence of this result is that
computing the $k$-th homotopy group of a $d$-dimensional space for $d &gt; 3$ is
$W[2]$-hard for parameter $k$.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Skotnica_M/0/1/0/all/0/1">Michael Skotnica</a></p><p>In this short note, we show that the problem of VEST is $W[2]$-hard for
parameter $k$. This strengthens a result of Matou\v{s}ek, who showed
$W[1]$-hardness of that problem. The consequence of this result is that
computing the $k$-th homotopy group of a $d$-dimensional space for $d &gt; 3$ is
$W[2]$-hard for parameter $k$.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-21T00:30:00Z">Wednesday, September 21 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.09800'>A tight bound for the number of edges of matchstick graphs</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: J&#xe9;r&#xe9;my Lavoll&#xe9;e, Konrad Swanepoel</p><p>A matchstick graph is a plane graph with edges drawn as unit-distance line
segments. Harborth introduced these graphs in 1986 and conjectured that the
maximum number of edges for a matchstick graph on $n$ vertices is $\lfloor
3n-\sqrt{12n-3} \rfloor$. In this paper we prove this conjecture for all $n\geq
1$. The main geometric ingredient of the proof is an isoperimetric inequality
related to Lhuilier's inequality.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Lavollee_J/0/1/0/all/0/1">J&#xe9;r&#xe9;my Lavoll&#xe9;e</a>, <a href="http://arxiv.org/find/math/1/au:+Swanepoel_K/0/1/0/all/0/1">Konrad Swanepoel</a></p><p>A matchstick graph is a plane graph with edges drawn as unit-distance line
segments. Harborth introduced these graphs in 1986 and conjectured that the
maximum number of edges for a matchstick graph on $n$ vertices is $\lfloor
3n-\sqrt{12n-3} \rfloor$. In this paper we prove this conjecture for all $n\geq
1$. The main geometric ingredient of the proof is an isoperimetric inequality
related to Lhuilier's inequality.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-21T00:30:00Z">Wednesday, September 21 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.09313'>Natural Wave Numbers, Natural Wave Co-numbers, and the Computation of the Primes</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Terence R. Smith</p><p>The paper exploits an isomorphism between the natural numbers N and a space U
of periodic sequences of the roots of unity in constructing a recursive
procedure for representing and computing the prime numbers. The nth wave number
${\bf u}_n$ is the countable sequence of the nth roots of unity having
frequencies k/n for all integer phases k. The space U is closed under a
commutative and associative binary operation ${\bf u}_m \odot{\bf u}_n={\bf
u}_{mn}$, termed the circular product, and is isomorphic with N under their
respective product operators. Functions are defined on U that partition wave
numbers into two complementary sequences, of which the co-number $ {\overset
{\bf \ast }{ \bf u}}_n$ is a function of a wave number in which zeros replace
its positive roots of unity. The recursive procedure $ {\overset {\bf \ast }{
\bf U}}_{N+1}= {\overset {\bf \ast }{ \bf U}}_{N}\odot{\overset {\bf \ast }{\bf
u}}_{{N+1}}$ represents prime numbers explicitly in terms of preceding prime
numbers, starting with $p_1=2$, and is shown never to terminate. If ${p}_1, ...
, { p}_{N+1}$ are the first $N+1$ prime phases, then the phases in the range
$p_{N+1} \leq k &lt; p^2_{N+1}$ that are associated with the non-zero terms of $
{\overset {\bf \ast }{\bf U}}_{N}$ are, together with $ p_1, ...,p_N$, all of
the prime phases less than $p^2_{N+1}$. When applied with all of the primes
identified at the previous step, the recursive procedure identifies
approximately $7^{2(N-1)}/(2(N-1)ln7)$ primes at each iteration for $ N&gt;1$.
When the phases of wave numbers are represented in modular arithmetic, the
prime phases are representable in terms of sums of reciprocals of the initial
set of prime phases and have a relation with the zeta-function.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Smith_T/0/1/0/all/0/1">Terence R. Smith</a></p><p>The paper exploits an isomorphism between the natural numbers N and a space U
of periodic sequences of the roots of unity in constructing a recursive
procedure for representing and computing the prime numbers. The nth wave number
${\bf u}_n$ is the countable sequence of the nth roots of unity having
frequencies k/n for all integer phases k. The space U is closed under a
commutative and associative binary operation ${\bf u}_m \odot{\bf u}_n={\bf
u}_{mn}$, termed the circular product, and is isomorphic with N under their
respective product operators. Functions are defined on U that partition wave
numbers into two complementary sequences, of which the co-number $ {\overset
{\bf \ast }{ \bf u}}_n$ is a function of a wave number in which zeros replace
its positive roots of unity. The recursive procedure $ {\overset {\bf \ast }{
\bf U}}_{N+1}= {\overset {\bf \ast }{ \bf U}}_{N}\odot{\overset {\bf \ast }{\bf
u}}_{{N+1}}$ represents prime numbers explicitly in terms of preceding prime
numbers, starting with $p_1=2$, and is shown never to terminate. If ${p}_1, ...
, { p}_{N+1}$ are the first $N+1$ prime phases, then the phases in the range
$p_{N+1} \leq k &lt; p^2_{N+1}$ that are associated with the non-zero terms of $
{\overset {\bf \ast }{\bf U}}_{N}$ are, together with $ p_1, ...,p_N$, all of
the prime phases less than $p^2_{N+1}$. When applied with all of the primes
identified at the previous step, the recursive procedure identifies
approximately $7^{2(N-1)}/(2(N-1)ln7)$ primes at each iteration for $ N&gt;1$.
When the phases of wave numbers are represented in modular arithmetic, the
prime phases are representable in terms of sums of reciprocals of the initial
set of prime phases and have a relation with the zeta-function.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-21T00:30:00Z">Wednesday, September 21 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.09509'>Data structures for topologically sound higher-dimensional diagram rewriting</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Amar Hadzihasanovic, Diana Kessler</p><p>We present a computational implementation of diagrammatic sets, a model of
higher-dimensional diagram rewriting that is "topologically sound": diagrams
admit a functorial interpretation as homotopies in cell complexes. This has
potential applications both in the formalisation of higher algebra and category
theory and in computational algebraic topology. We describe data structures for
well-formed shapes of diagrams of arbitrary dimensions and provide a solution
to their isomorphism problem in time $O(n^3 \log n)$. On top of this, we define
a type theory for rewriting in diagrammatic sets and provide a semantic
characterisation of its syntactic category. All data structures and algorithms
are implemented in the Python library rewalt, which also supports various
visualisations of diagrams.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Hadzihasanovic_A/0/1/0/all/0/1">Amar Hadzihasanovic</a>, <a href="http://arxiv.org/find/math/1/au:+Kessler_D/0/1/0/all/0/1">Diana Kessler</a></p><p>We present a computational implementation of diagrammatic sets, a model of
higher-dimensional diagram rewriting that is "topologically sound": diagrams
admit a functorial interpretation as homotopies in cell complexes. This has
potential applications both in the formalisation of higher algebra and category
theory and in computational algebraic topology. We describe data structures for
well-formed shapes of diagrams of arbitrary dimensions and provide a solution
to their isomorphism problem in time $O(n^3 \log n)$. On top of this, we define
a type theory for rewriting in diagrammatic sets and provide a semantic
characterisation of its syntactic category. All data structures and algorithms
are implemented in the Python library rewalt, which also supports various
visualisations of diagrams.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-21T00:30:00Z">Wednesday, September 21 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.09661'>Exact Matching and the Top-k Perfect Matching Problem</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Nicolas El Maalouly, Lasse Wulf</p><p>The aim of this note is to provide a reduction of the Exact Matching problem
to the Top-$k$ Perfect Matching Problem. Together with earlier work by El
Maalouly, this shows that the two problems are polynomial-time equivalent.
</p>
<p>The Exact Matching Problem is a well-known 40 years old problem for which a
randomized, but no deterministic poly-time algorithm has been discovered. The
Top-$k$ Perfect Matching Problem is the problem of finding a perfect matching
which maximizes the total weight of the $k$ heaviest edges contained in it.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Maalouly_N/0/1/0/all/0/1">Nicolas El Maalouly</a>, <a href="http://arxiv.org/find/cs/1/au:+Wulf_L/0/1/0/all/0/1">Lasse Wulf</a></p><p>The aim of this note is to provide a reduction of the Exact Matching problem
to the Top-$k$ Perfect Matching Problem. Together with earlier work by El
Maalouly, this shows that the two problems are polynomial-time equivalent.
</p>
<p>The Exact Matching Problem is a well-known 40 years old problem for which a
randomized, but no deterministic poly-time algorithm has been discovered. The
Top-$k$ Perfect Matching Problem is the problem of finding a perfect matching
which maximizes the total weight of the $k$ heaviest edges contained in it.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-21T00:30:00Z">Wednesday, September 21 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.09668'>Maximizing a Submodular Function with Bounded Curvature under an Unknown Knapsack Constraint</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Max Klimm, Martin Knaack</p><p>This paper studies the problem of maximizing a monotone submodular function
under an unknown knapsack constraint. A solution to this problem is a policy
that decides which item to pack next based on the past packing history. The
robustness factor of a policy is the worst case ratio of the solution obtained
by following the policy and an optimal solution that knows the knapsack
capacity. We develop an algorithm with a robustness factor that is decreasing
in the curvature $B$ of the submodular function. For the extreme cases $c=0$
corresponding to a modular objective, it matches a previously known and best
possible robustness factor of $1/2$. For the other extreme case of $c=1$ it
yields a robustness factor of $\approx 0.35$ improving over the best previously
known robustness factor of $\approx 0.06$.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Klimm_M/0/1/0/all/0/1">Max Klimm</a>, <a href="http://arxiv.org/find/cs/1/au:+Knaack_M/0/1/0/all/0/1">Martin Knaack</a></p><p>This paper studies the problem of maximizing a monotone submodular function
under an unknown knapsack constraint. A solution to this problem is a policy
that decides which item to pack next based on the past packing history. The
robustness factor of a policy is the worst case ratio of the solution obtained
by following the policy and an optimal solution that knows the knapsack
capacity. We develop an algorithm with a robustness factor that is decreasing
in the curvature $B$ of the submodular function. For the extreme cases $c=0$
corresponding to a modular objective, it matches a previously known and best
possible robustness factor of $1/2$. For the other extreme case of $c=1$ it
yields a robustness factor of $\approx 0.35$ improving over the best previously
known robustness factor of $\approx 0.06$.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-21T00:30:00Z">Wednesday, September 21 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.09711'>Development of a Parallel BAT and Its Applications in Binary-state Network Reliability Problems</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Wei-Chang Yeh</p><p>Various networks are broadly and deeply applied in real-life applications.
Reliability is the most important index for measuring the performance of all
network types. Among the various algorithms, only implicit enumeration
algorithms, such as depth-first-search, breadth-search-first, universal
generating function methodology, binary-decision diagram, and
binary-addition-tree algorithm (BAT), can be used to calculate the exact
network reliability. However, implicit enumeration algorithms can only be used
to solve small-scale network reliability problems. The BAT was recently
proposed as a simple, fast, easy-to-code, and flexible make-to-fit
exact-solution algorithm. Based on the experimental results, the BAT and its
variants outperformed other implicit enumeration algorithms. Hence, to overcome
the above-mentioned obstacle as a result of the size problem, a new parallel
BAT (PBAT) was proposed to improve the BAT based on compute multithread
architecture to calculate the binary-state network reliability problem, which
is fundamental for all types of network reliability problems. From the analysis
of the time complexity and experiments conducted on 20 benchmarks of
binary-state network reliability problems, PBAT was able to efficiently solve
medium-scale network reliability problems.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Yeh_W/0/1/0/all/0/1">Wei-Chang Yeh</a></p><p>Various networks are broadly and deeply applied in real-life applications.
Reliability is the most important index for measuring the performance of all
network types. Among the various algorithms, only implicit enumeration
algorithms, such as depth-first-search, breadth-search-first, universal
generating function methodology, binary-decision diagram, and
binary-addition-tree algorithm (BAT), can be used to calculate the exact
network reliability. However, implicit enumeration algorithms can only be used
to solve small-scale network reliability problems. The BAT was recently
proposed as a simple, fast, easy-to-code, and flexible make-to-fit
exact-solution algorithm. Based on the experimental results, the BAT and its
variants outperformed other implicit enumeration algorithms. Hence, to overcome
the above-mentioned obstacle as a result of the size problem, a new parallel
BAT (PBAT) was proposed to improve the BAT based on compute multithread
architecture to calculate the binary-state network reliability problem, which
is fundamental for all types of network reliability problems. From the analysis
of the time complexity and experiments conducted on 20 benchmarks of
binary-state network reliability problems, PBAT was able to efficiently solve
medium-scale network reliability problems.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-21T00:30:00Z">Wednesday, September 21 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.09888'>Modeling the Small-World Phenomenon with Road Networks</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Michael T. Goodrich, Evrim Ozel</p><p>Dating back to two famous experiments by the social-psychologist, Stanley
Milgram, in the 1960s, the small-world phenomenon is the idea that all people
are connected through a short chain of acquaintances that can be used to route
messages. Many subsequent papers have attempted to model this phenomenon, with
most concentrating on the "short chain" of acquaintances rather than their
ability to efficiently route messages. In this paper, we study the small-world
navigability of the U.S. road network, with the goal of providing a model that
explains how messages in the original small-world experiments could be routed
along short paths using U.S. roads. To this end, we introduce the Neighborhood
Preferential Attachment model, which combines elements from Kleinberg's model
and the Barab\'asi-Albert model, such that long-range links are chosen
according to both the degrees and (road-network) distances of vertices in the
network. We empirically evaluate all three models by running a decentralized
routing algorithm, where each vertex only has knowledge of its own neighbors,
and find that our model outperforms both of these models in terms of the
average hop length. Moreover, our experiments indicate that similar to the
Barab\'asi-Albert model, networks generated by our model are scale-free, which
could be a more realistic representation of acquaintanceship links in the
original small-world experiment.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Goodrich_M/0/1/0/all/0/1">Michael T. Goodrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozel_E/0/1/0/all/0/1">Evrim Ozel</a></p><p>Dating back to two famous experiments by the social-psychologist, Stanley
Milgram, in the 1960s, the small-world phenomenon is the idea that all people
are connected through a short chain of acquaintances that can be used to route
messages. Many subsequent papers have attempted to model this phenomenon, with
most concentrating on the "short chain" of acquaintances rather than their
ability to efficiently route messages. In this paper, we study the small-world
navigability of the U.S. road network, with the goal of providing a model that
explains how messages in the original small-world experiments could be routed
along short paths using U.S. roads. To this end, we introduce the Neighborhood
Preferential Attachment model, which combines elements from Kleinberg's model
and the Barab\'asi-Albert model, such that long-range links are chosen
according to both the degrees and (road-network) distances of vertices in the
network. We empirically evaluate all three models by running a decentralized
routing algorithm, where each vertex only has knowledge of its own neighbors,
and find that our model outperforms both of these models in terms of the
average hop length. Moreover, our experiments indicate that similar to the
Barab\'asi-Albert model, networks generated by our model are scale-free, which
could be a more realistic representation of acquaintanceship links in the
original small-world experiment.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-21T00:30:00Z">Wednesday, September 21 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.09896'>On the Correlation Gap of Matroids</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Edin Husi&#x107;, Zhuan Khye Koh, Georg Loho, L&#xe1;szl&#xf3; A. V&#xe9;gh</p><p>A set function can be extended to the unit cube in various ways; the
correlation gap measures the ratio between two natural extensions. This
quantity has been identified as the performance guarantee in a range of
approximation algorithms and mechanism design settings. It is known that the
correlation gap of a monotone submodular function is $1-1/e$, and this is tight
even for simple matroid rank functions.
</p>
<p>We initiate a fine-grained study of correlation gaps of matroid rank
functions. In particular, we present improved lower bounds on the correlation
gap as parametrized by the rank and the girth of the matroid. We also show that
the worst correlation gap of a weighted matroid rank function is achieved under
uniform weights. Such improved lower bounds have direct applications for
submodular maximization under matroid constraints, mechanism design, and
contention resolution schemes. Previous work relied on implicit correlation gap
bounds for problems such as list decoding and approval voting.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Husic_E/0/1/0/all/0/1">Edin Husi&#x107;</a>, <a href="http://arxiv.org/find/math/1/au:+Koh_Z/0/1/0/all/0/1">Zhuan Khye Koh</a>, <a href="http://arxiv.org/find/math/1/au:+Loho_G/0/1/0/all/0/1">Georg Loho</a>, <a href="http://arxiv.org/find/math/1/au:+Vegh_L/0/1/0/all/0/1">L&#xe1;szl&#xf3; A. V&#xe9;gh</a></p><p>A set function can be extended to the unit cube in various ways; the
correlation gap measures the ratio between two natural extensions. This
quantity has been identified as the performance guarantee in a range of
approximation algorithms and mechanism design settings. It is known that the
correlation gap of a monotone submodular function is $1-1/e$, and this is tight
even for simple matroid rank functions.
</p>
<p>We initiate a fine-grained study of correlation gaps of matroid rank
functions. In particular, we present improved lower bounds on the correlation
gap as parametrized by the rank and the girth of the matroid. We also show that
the worst correlation gap of a weighted matroid rank function is achieved under
uniform weights. Such improved lower bounds have direct applications for
submodular maximization under matroid constraints, mechanism design, and
contention resolution schemes. Previous work relied on implicit correlation gap
bounds for problems such as list decoding and approval voting.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-21T00:30:00Z">Wednesday, September 21 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Tuesday, September 20
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://eccc.weizmann.ac.il/report/2022/133'>TR22-133 |  Downward Self-Reducibility in TFNP | 

	Prahladh Harsha, 

	Daniel Mitropolsky, 

	Alon Rosen</a></h3>
          <p class='item-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          A problem is downward self-reducible if it can be solved efficiently given an oracle that returns
solutions for strictly smaller instances. In the decisional landscape, downward self-reducibility is
well studied and it is known that all downward self-reducible problems are in PSPACE. In this
paper, we initiate the study of downward self-reducible search problems which are guaranteed to
have a solution â that is, the downward self-reducible problems in TFNP. We show that most
natural PLS-complete problems are downward self-reducible and any downward self-reducible
problem in TFNP is contained in PLS. Furthermore, if the downward self-reducible problem
is in UTFNP (i.e. it has a unique solution), then it is actually contained in CLS. This implies
that if integer factoring is downward self-reducible then it is in fact in CLS, suggesting that no
efficient factoring algorithm exists using the factorization of smaller numbers.
        
        </div>

        <div class='item-content item-summary'>
        
          
          A problem is downward self-reducible if it can be solved efficiently given an oracle that returns
solutions for strictly smaller instances. In the decisional landscape, downward self-reducibility is
well studied and it is known that all downward self-reducible problems are in PSPACE. In this
paper, we initiate the study of downward self-reducible search problems which are guaranteed to
have a solution â that is, the downward self-reducible problems in TFNP. We show that most
natural PLS-complete problems are downward self-reducible and any downward self-reducible
problem in TFNP is contained in PLS. Furthermore, if the downward self-reducible problem
is in UTFNP (i.e. it has a unique solution), then it is actually contained in CLS. This implies
that if integer factoring is downward self-reducible then it is in fact in CLS, suggesting that no
efficient factoring algorithm exists using the factorization of smaller numbers.
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-20T18:28:17Z">Tuesday, September 20 2022, 18:28</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.08219'>Better Hardness Results for the Minimum Spanning Tree Congestion Problem</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Huong Luu, Marek Chrobak</p><p>In the spanning tree congestion problem, given a connected graph $G$, the
objective is to compute a spanning tree $T$ in $G$ for which the maximum edge
congestion is minimized, where the congestion of an edge $e$ of $T$ is the
number of vertex pairs adjacent in $G$ for which the path connecting them in
$T$ traverses $e$. The problem is known to be NP-hard, but its approximability
is still poorly understood, and it is not even known whether the optimum can be
efficiently approximated with ratio $o(n)$. In the decision version of this
problem, denoted STC-$K$, we need to determine if $G$ has a spanning tree with
congestion at most $K$. It is known that STC-$K$ is NP-complete for $K\ge 8$,
and this implies a lower bound of $1.125$ on the approximation ratio of
minimizing congestion. On the other hand, $3$-STC can be solved in polynomial
time, with the complexity status of this problem for $K\in \{4,5,6,7\}$
remaining an open problem. We substantially improve the earlier hardness result
by proving that STC-$K$ is NP-complete for $K\ge 5$. This leaves only the case
$K=4$ open, and improves the lower bound on the approximation ratio to $1.2$.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Luu_H/0/1/0/all/0/1">Huong Luu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chrobak_M/0/1/0/all/0/1">Marek Chrobak</a></p><p>In the spanning tree congestion problem, given a connected graph $G$, the
objective is to compute a spanning tree $T$ in $G$ for which the maximum edge
congestion is minimized, where the congestion of an edge $e$ of $T$ is the
number of vertex pairs adjacent in $G$ for which the path connecting them in
$T$ traverses $e$. The problem is known to be NP-hard, but its approximability
is still poorly understood, and it is not even known whether the optimum can be
efficiently approximated with ratio $o(n)$. In the decision version of this
problem, denoted STC-$K$, we need to determine if $G$ has a spanning tree with
congestion at most $K$. It is known that STC-$K$ is NP-complete for $K\ge 8$,
and this implies a lower bound of $1.125$ on the approximation ratio of
minimizing congestion. On the other hand, $3$-STC can be solved in polynomial
time, with the complexity status of this problem for $K\in \{4,5,6,7\}$
remaining an open problem. We substantially improve the earlier hardness result
by proving that STC-$K$ is NP-complete for $K\ge 5$. This leaves only the case
$K=4$ open, and improves the lower bound on the approximation ratio to $1.2$.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-20T00:30:00Z">Tuesday, September 20 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.08688'>On Relaxed Locally Decodable Codes for Hamming and Insertion-Deletion Errors</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Alex Block, Jeremiah Blocki, Kuan Cheng, Elena Grigorescu, Xin Li, Yu Zheng, Minshen Zhu</p><p>Locally Decodable Codes (LDCs) are error-correcting codes
$C:\Sigma^n\rightarrow \Sigma^m$ with super-fast decoding algorithms. They are
important mathematical objects in many areas of theoretical computer science,
yet the best constructions so far have codeword length $m$ that is
super-polynomial in $n$, for codes with constant query complexity and constant
alphabet size. In a very surprising result, Ben-Sasson et al. showed how to
construct a relaxed version of LDCs (RLDCs) with constant query complexity and
almost linear codeword length over the binary alphabet, and used them to obtain
significantly-improved constructions of Probabilistically Checkable Proofs. In
this work, we study RLDCs in the standard Hamming-error setting, and introduce
their variants in the insertion and deletion (Insdel) error setting. Insdel
LDCs were first studied by Ostrovsky and Paskin-Cherniavsky, and are further
motivated by recent advances in DNA random access bio-technologies, in which
the goal is to retrieve individual files from a DNA storage database. Our first
result is an exponential lower bound on the length of Hamming RLDCs making 2
queries, over the binary alphabet. This answers a question explicitly raised by
Gur and Lachish. Our result exhibits a "phase-transition"-type behavior on the
codeword length for constant-query Hamming RLDCs. We further define two
variants of RLDCs in the Insdel-error setting, a weak and a strong version. On
the one hand, we construct weak Insdel RLDCs with with parameters matching
those of the Hamming variants. On the other hand, we prove exponential lower
bounds for strong Insdel RLDCs. These results demonstrate that, while these
variants are equivalent in the Hamming setting, they are significantly
different in the insdel setting. Our results also prove a strict separation
between Hamming RLDCs and Insdel RLDCs.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Block_A/0/1/0/all/0/1">Alex Block</a>, <a href="http://arxiv.org/find/cs/1/au:+Blocki_J/0/1/0/all/0/1">Jeremiah Blocki</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1">Kuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Grigorescu_E/0/1/0/all/0/1">Elena Grigorescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Minshen Zhu</a></p><p>Locally Decodable Codes (LDCs) are error-correcting codes
$C:\Sigma^n\rightarrow \Sigma^m$ with super-fast decoding algorithms. They are
important mathematical objects in many areas of theoretical computer science,
yet the best constructions so far have codeword length $m$ that is
super-polynomial in $n$, for codes with constant query complexity and constant
alphabet size. In a very surprising result, Ben-Sasson et al. showed how to
construct a relaxed version of LDCs (RLDCs) with constant query complexity and
almost linear codeword length over the binary alphabet, and used them to obtain
significantly-improved constructions of Probabilistically Checkable Proofs. In
this work, we study RLDCs in the standard Hamming-error setting, and introduce
their variants in the insertion and deletion (Insdel) error setting. Insdel
LDCs were first studied by Ostrovsky and Paskin-Cherniavsky, and are further
motivated by recent advances in DNA random access bio-technologies, in which
the goal is to retrieve individual files from a DNA storage database. Our first
result is an exponential lower bound on the length of Hamming RLDCs making 2
queries, over the binary alphabet. This answers a question explicitly raised by
Gur and Lachish. Our result exhibits a "phase-transition"-type behavior on the
codeword length for constant-query Hamming RLDCs. We further define two
variants of RLDCs in the Insdel-error setting, a weak and a strong version. On
the one hand, we construct weak Insdel RLDCs with with parameters matching
those of the Hamming variants. On the other hand, we prove exponential lower
bounds for strong Insdel RLDCs. These results demonstrate that, while these
variants are equivalent in the Hamming setting, they are significantly
different in the insdel setting. Our results also prove a strict separation
between Hamming RLDCs and Insdel RLDCs.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-20T00:30:00Z">Tuesday, September 20 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.08114'>Asymptotically Optimal Bounds for Estimating H-Index in Sublinear Time with Applications to Subgraph Counting</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Sepehr Assadi, Hoai-An Nguyen</p><p>The $h$-index is a metric used to measure the impact of a user in a
publication setting, such as a member of a social network with many highly
liked posts or a researcher in an academic domain with many highly cited
publications. Specifically, the $h$-index of a user is the largest integer $h$
such that at least $h$ publications of the user have at least $h$ units of
positive feedback.
</p>
<p>We design an algorithm that, given query access to the $n$ publications of a
user and each publication's corresponding positive feedback number, outputs a
$(1\pm \varepsilon)$-approximation of the $h$-index of this user with
probability at least $1-\delta$ in time \[
</p>
<p>O(\frac{n \cdot \ln{(1/\delta)}}{\varepsilon^2 \cdot h}),
</p>
<p>\] where $h$ is the actual $h$-index which is unknown to the algorithm
a-priori. We then design a novel lower bound technique that allows us to prove
that this bound is in fact asymptotically optimal for this problem in all
parameters $n,h,\varepsilon,$ and $\delta$.
</p>
<p>Our work is one of the first in sublinear time algorithms that addresses
obtaining asymptotically optimal bounds, especially in terms of the error and
confidence parameters. As such, we focus on designing novel techniques for this
task. In particular, our lower bound technique seems quite general -- to
showcase this, we also use our approach to prove an asymptotically optimal
lower bound for the problem of estimating the number of triangles in a graph in
sublinear time, which now is also optimal in the error and confidence
parameters. This result improves upon prior lower bounds of Eden, Levi, Ron,
and Seshadhri (FOCS'15) for this problem, as well as multiple follow-ups that
extended this lower bound to other subgraph counting problems.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Assadi_S/0/1/0/all/0/1">Sepehr Assadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hoai-An Nguyen</a></p><p>The $h$-index is a metric used to measure the impact of a user in a
publication setting, such as a member of a social network with many highly
liked posts or a researcher in an academic domain with many highly cited
publications. Specifically, the $h$-index of a user is the largest integer $h$
such that at least $h$ publications of the user have at least $h$ units of
positive feedback.
</p>
<p>We design an algorithm that, given query access to the $n$ publications of a
user and each publication's corresponding positive feedback number, outputs a
$(1\pm \varepsilon)$-approximation of the $h$-index of this user with
probability at least $1-\delta$ in time \[
</p>
<p>O(\frac{n \cdot \ln{(1/\delta)}}{\varepsilon^2 \cdot h}),
</p>
<p>\] where $h$ is the actual $h$-index which is unknown to the algorithm
a-priori. We then design a novel lower bound technique that allows us to prove
that this bound is in fact asymptotically optimal for this problem in all
parameters $n,h,\varepsilon,$ and $\delta$.
</p>
<p>Our work is one of the first in sublinear time algorithms that addresses
obtaining asymptotically optimal bounds, especially in terms of the error and
confidence parameters. As such, we focus on designing novel techniques for this
task. In particular, our lower bound technique seems quite general -- to
showcase this, we also use our approach to prove an asymptotically optimal
lower bound for the problem of estimating the number of triangles in a graph in
sublinear time, which now is also optimal in the error and confidence
parameters. This result improves upon prior lower bounds of Eden, Levi, Ron,
and Seshadhri (FOCS'15) for this problem, as well as multiple follow-ups that
extended this lower bound to other subgraph counting problems.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-20T00:30:00Z">Tuesday, September 20 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.08166'>The trace reconstruction problem for spider graphs</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Alec Sun, William Yue</p><p>We study the trace reconstruction problem for spider graphs. Let $n$ be the
number of nodes of a spider and $d$ be the length of each leg, and suppose that
we are given independent traces of the spider from a deletion channel in which
each non-root node is deleted with probability $q$. This is a natural
generalization of the string trace reconstruction problem in theoretical
computer science, which corresponds to the special case where the spider has
one leg. In the regime where $d\ge \log_{1/q}(n)$, the problem can be reduced
to the vanilla string trace reconstruction problem. We thus study the more
interesting regime $d\le \log_{1/q}(n)$, in which entire legs of the spider are
deleted with non-negligible probability. We describe an algorithm that
reconstructs spiders with high probability using
$\exp\left(\mathcal{O}\left(\frac{(nq^d)^{1/3}}{d^{1/3}}(\log
n)^{2/3}\right)\right)$ traces. Our algorithm works for all deletion
probabilities $q\in(0,1)$.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1">Alec Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_W/0/1/0/all/0/1">William Yue</a></p><p>We study the trace reconstruction problem for spider graphs. Let $n$ be the
number of nodes of a spider and $d$ be the length of each leg, and suppose that
we are given independent traces of the spider from a deletion channel in which
each non-root node is deleted with probability $q$. This is a natural
generalization of the string trace reconstruction problem in theoretical
computer science, which corresponds to the special case where the spider has
one leg. In the regime where $d\ge \log_{1/q}(n)$, the problem can be reduced
to the vanilla string trace reconstruction problem. We thus study the more
interesting regime $d\le \log_{1/q}(n)$, in which entire legs of the spider are
deleted with non-negligible probability. We describe an algorithm that
reconstructs spiders with high probability using
$\exp\left(\mathcal{O}\left(\frac{(nq^d)^{1/3}}{d^{1/3}}(\log
n)^{2/3}\right)\right)$ traces. Our algorithm works for all deletion
probabilities $q\in(0,1)$.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-20T00:30:00Z">Tuesday, September 20 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.08281'>Improved Generalization Bound and Learning of Sparsity Patterns for Data-Driven Low-Rank Approximation</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Shinsaku Sakaue, Taihei Oki</p><p>Learning sketching matrices for fast and accurate low-rank approximation
(LRA) has gained increasing attention. Recently, Bartlett, Indyk, and Wagner
(COLT 2022) presented a generalization bound for the learning-based LRA.
Specifically, for rank-$k$ approximation using an $m \times n$ learned
sketching matrix with $s$ non-zeros in each column, they proved an
$\tilde{\mathrm{O}}(nsm)$ bound on the \emph{fat shattering dimension}
($\tilde{\mathrm{O}}$ hides logarithmic factors). We build on their work and
make two contributions.
</p>
<p>1. We present a better $\tilde{\mathrm{O}}(nsk)$ bound ($k \le m$). En route
to obtaining the bound, we give a low-complexity \emph{Goldberg--Jerrum
algorithm} for computing pseudo-inverse matrices, which would be of independent
interest.
</p>
<p>2. We alleviate an assumption of the previous study that the sparsity pattern
of sketching matrices is fixed. We prove that learning positions of non-zeros
increases the fat shattering dimension only by ${\mathrm{O}}(ns\log n)$. Also,
experiments confirm the practical benefit of learning sparsity patterns.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Sakaue_S/0/1/0/all/0/1">Shinsaku Sakaue</a>, <a href="http://arxiv.org/find/cs/1/au:+Oki_T/0/1/0/all/0/1">Taihei Oki</a></p><p>Learning sketching matrices for fast and accurate low-rank approximation
(LRA) has gained increasing attention. Recently, Bartlett, Indyk, and Wagner
(COLT 2022) presented a generalization bound for the learning-based LRA.
Specifically, for rank-$k$ approximation using an $m \times n$ learned
sketching matrix with $s$ non-zeros in each column, they proved an
$\tilde{\mathrm{O}}(nsm)$ bound on the \emph{fat shattering dimension}
($\tilde{\mathrm{O}}$ hides logarithmic factors). We build on their work and
make two contributions.
</p>
<p>1. We present a better $\tilde{\mathrm{O}}(nsk)$ bound ($k \le m$). En route
to obtaining the bound, we give a low-complexity \emph{Goldberg--Jerrum
algorithm} for computing pseudo-inverse matrices, which would be of independent
interest.
</p>
<p>2. We alleviate an assumption of the previous study that the sparsity pattern
of sketching matrices is fixed. We prove that learning positions of non-zeros
increases the fat shattering dimension only by ${\mathrm{O}}(ns\log n)$. Also,
experiments confirm the practical benefit of learning sparsity patterns.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-20T00:30:00Z">Tuesday, September 20 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.08427'>A Nearly Tight Lower Bound for the $d$-Dimensional Cow-Path Problem</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Nikhil Bansal, John Kuszmaul, William Kuszmaul</p><p>In the $d$-dimensional cow-path problem, a cow living in $\mathbb{R}^d$ must
locate a $(d - 1)$-dimensional hyperplane $H$ whose location is unknown. The
only way that the cow can find $H$ is to roam $\mathbb{R}^d$ until it
intersects $\mathcal{H}$. If the cow travels a total distance $s$ to locate a
hyperplane $H$ whose distance from the origin was $r \ge 1$, then the cow is
said to achieve competitive ratio $s / r$.
</p>
<p>It is a classic result that, in $\mathbb{R}^2$, the optimal (deterministic)
competitive ratio is $9$. In $\mathbb{R}^3$, the optimal competitive ratio is
known to be at most $\approx 13.811$. But in higher dimensions, the asymptotic
relationship between $d$ and the optimal competitive ratio remains an open
question. The best upper and lower bounds, due to Antoniadis et al., are
$O(d^{3/2})$ and $\Omega(d)$, leaving a gap of roughly $\sqrt{d}$. In this
note, we achieve a stronger lower bound of $\tilde{\Omega}(d^{3/2})$.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bansal_N/0/1/0/all/0/1">Nikhil Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuszmaul_J/0/1/0/all/0/1">John Kuszmaul</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuszmaul_W/0/1/0/all/0/1">William Kuszmaul</a></p><p>In the $d$-dimensional cow-path problem, a cow living in $\mathbb{R}^d$ must
locate a $(d - 1)$-dimensional hyperplane $H$ whose location is unknown. The
only way that the cow can find $H$ is to roam $\mathbb{R}^d$ until it
intersects $\mathcal{H}$. If the cow travels a total distance $s$ to locate a
hyperplane $H$ whose distance from the origin was $r \ge 1$, then the cow is
said to achieve competitive ratio $s / r$.
</p>
<p>It is a classic result that, in $\mathbb{R}^2$, the optimal (deterministic)
competitive ratio is $9$. In $\mathbb{R}^3$, the optimal competitive ratio is
known to be at most $\approx 13.811$. But in higher dimensions, the asymptotic
relationship between $d$ and the optimal competitive ratio remains an open
question. The best upper and lower bounds, due to Antoniadis et al., are
$O(d^{3/2})$ and $\Omega(d)$, leaving a gap of roughly $\sqrt{d}$. In this
note, we achieve a stronger lower bound of $\tilde{\Omega}(d^{3/2})$.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-20T00:30:00Z">Tuesday, September 20 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.08600'>GenPIP: In-Memory Acceleration of Genome Analysis via Tight Integration of Basecalling and Read Mapping</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Haiyu Mao, Mohammed Alser, Mohammad Sadrosadati, Can Firtina, Akanksha Baranwal, Damla Senol Cali, Aditya Manglik, Nour Almadhoun Alserr, Onur Mutlu</p><p>Nanopore sequencing is a widely-used high-throughput genome sequencing
technology that can sequence long fragments of a genome into raw electrical
signals at low cost. Nanopore sequencing requires two computationally-costly
processing steps for accurate downstream genome analysis. The first step,
basecalling, translates the raw electrical signals into nucleotide bases (i.e.,
A, C, G, T). The second step, read mapping, finds the correct location of a
read in a reference genome. In existing genome analysis pipelines, basecalling
and read mapping are executed separately. We observe in this work that such
separate execution of the two most time-consuming steps inherently leads to (1)
significant data movement and (2) redundant computations on the data, slowing
down the genome analysis pipeline. This paper proposes GenPIP, an in-memory
genome analysis accelerator that tightly integrates basecalling and read
mapping. GenPIP improves the performance of the genome analysis pipeline with
two key mechanisms: (1) in-memory fine-grained collaborative execution of the
major genome analysis steps in parallel; (2) a new technique for
early-rejection of low-quality and unmapped reads to timely stop the execution
of genome analysis for such reads, reducing inefficient computation. Our
experiments show that, for the execution of the genome analysis pipeline,
GenPIP provides 41.6X (8.4X) speedup and 32.8X (20.8X) energy savings with
negligible accuracy loss compared to the state-of-the-art software genome
analysis tools executed on a state-of-the-art CPU (GPU). Compared to a design
that combines state-of-the-art in-memory basecalling and read mapping
accelerators, GenPIP provides 1.39X speedup and 1.37X energy savings.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1">Haiyu Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Alser_M/0/1/0/all/0/1">Mohammed Alser</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadrosadati_M/0/1/0/all/0/1">Mohammad Sadrosadati</a>, <a href="http://arxiv.org/find/cs/1/au:+Firtina_C/0/1/0/all/0/1">Can Firtina</a>, <a href="http://arxiv.org/find/cs/1/au:+Baranwal_A/0/1/0/all/0/1">Akanksha Baranwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Cali_D/0/1/0/all/0/1">Damla Senol Cali</a>, <a href="http://arxiv.org/find/cs/1/au:+Manglik_A/0/1/0/all/0/1">Aditya Manglik</a>, <a href="http://arxiv.org/find/cs/1/au:+Alserr_N/0/1/0/all/0/1">Nour Almadhoun Alserr</a>, <a href="http://arxiv.org/find/cs/1/au:+Mutlu_O/0/1/0/all/0/1">Onur Mutlu</a></p><p>Nanopore sequencing is a widely-used high-throughput genome sequencing
technology that can sequence long fragments of a genome into raw electrical
signals at low cost. Nanopore sequencing requires two computationally-costly
processing steps for accurate downstream genome analysis. The first step,
basecalling, translates the raw electrical signals into nucleotide bases (i.e.,
A, C, G, T). The second step, read mapping, finds the correct location of a
read in a reference genome. In existing genome analysis pipelines, basecalling
and read mapping are executed separately. We observe in this work that such
separate execution of the two most time-consuming steps inherently leads to (1)
significant data movement and (2) redundant computations on the data, slowing
down the genome analysis pipeline. This paper proposes GenPIP, an in-memory
genome analysis accelerator that tightly integrates basecalling and read
mapping. GenPIP improves the performance of the genome analysis pipeline with
two key mechanisms: (1) in-memory fine-grained collaborative execution of the
major genome analysis steps in parallel; (2) a new technique for
early-rejection of low-quality and unmapped reads to timely stop the execution
of genome analysis for such reads, reducing inefficient computation. Our
experiments show that, for the execution of the genome analysis pipeline,
GenPIP provides 41.6X (8.4X) speedup and 32.8X (20.8X) energy savings with
negligible accuracy loss compared to the state-of-the-art software genome
analysis tools executed on a state-of-the-art CPU (GPU). Compared to a design
that combines state-of-the-art in-memory basecalling and read mapping
accelerators, GenPIP provides 1.39X speedup and 1.37X energy savings.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-20T00:30:00Z">Tuesday, September 20 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Monday, September 19
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://blog.computationalcomplexity.org/2022/09/there-are-two-different-definitions-of.html'>There are two different definitions of Integer Programming. Why?</a></h3>
          <p class='item-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Alice and Bob have the following conversation.</p><p>===============================</p><p>ALICE: In your book you define INT PROG as, given a matrix A and vectors b,c,</p><p>find the integer vector x such that Ax\le b and c DOT x is maximized.</p><p>This is not correct! You also need x\ge 0.</p><p><br></p><p>BOB: Really? I always heard it without that extra constraint, though I am</p><p>sure they are equivalent and both NP-complete (Alice nods).</p><p>Where did you see it defined with that extra constraint?</p><p><br></p><p>ALICE:</p><p>Wikipedia entry in IP<br></p><p>Chapter of a book at an MIT website<br></p><p>Something on Science Direct<br></p><p>A course at Duke<br></p><p>An article by Papadimitriou&nbsp;<br></p><p>An article on arxiv<br></p><p>The book&nbsp;Graphs, Networks and Algorithms by Dieter Jungnickel</p><p>Bob, do you have examples where they do not use that extra constraint.&nbsp;</p><p>BOB:&nbsp;</p><p>Math Works<br></p><p>Lecture notes from UIUC<br></p><p>Lecture notes from Lehigh Univ.<br></p><p>The book Parameterized Complexity Theory&nbsp;by Flum and Grohe</p><p>The book Computers and Intractability : A Guide to the Theory of NP-Completeness by Garey and Johnson</p><p>ALICE: Both of our lists are impressive. So now what?&nbsp;</p><p>--------------------------------------------------------------------</p><p>(This is Bill again.)</p><p>What indeed!</p><p>1) Why are there two definitions of Int Prog?</p><p>2) When is it good to use which one?&nbsp;</p><p><br></p><p><br></p><p>By gasarch</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p>Alice and Bob have the following conversation.</p><p>===============================</p><p>ALICE: In your book you define INT PROG as, given a matrix A and vectors b,c,</p><p>find the integer vector x such that Ax\le b and c DOT x is maximized.</p><p>This is not correct! You also need x\ge 0.</p><p><br /></p><p>BOB: Really? I always heard it without that extra constraint, though I am</p><p>sure they are equivalent and both NP-complete (Alice nods).</p><p>Where did you see it defined with that extra constraint?</p><p><br /></p><p>ALICE:</p><p><a href="https://en.wikipedia.org/wiki/Integer_programming">Wikipedia entry in IP</a><br /></p><p><a href="https://web.mit.edu/15.053/www/AMP-Chapter-09.pdf">Chapter of a book at an MIT website</a><br /></p><p><a href="https://www.sciencedirect.com/topics/mathematics/integer-programming-problem">Something on Science Direct</a><br /></p><p><a href="https://courses.cs.duke.edu/fall12/compsci590.1/introduction.pdf">A course at Duke</a><br /></p><p><a href="https://lara.epfl.ch/w/_media/papadimitriou81complexityintegerprogramming.pdf">An article by Papadimitriou</a>&nbsp;<br /></p><p><a href="https://arxiv.org/pdf/2012.00079.pdf">An article on arxiv</a><br /></p><p>The book&nbsp;<i>Graphs, Networks and Algorithms</i> by Dieter Jungnickel</p><p>Bob, do you have examples where they do not use that extra constraint.&nbsp;</p><p>BOB:&nbsp;</p><p><a href="https://www.mathworks.com/discovery/integer-programming.html">Math Works</a><br /></p><p><a href="https://faculty.math.illinois.edu/~mlavrov/docs/482-fall-2019/lecture33.pdf">Lecture notes from UIUC</a><br /></p><p><a href="https://coral.ise.lehigh.edu/~ted/files/ie418/lectures/Lecture1.pdf">Lecture notes from Lehigh Univ.</a><br /></p><p>The book <i>Parameterized Complexity Theory</i>&nbsp;by Flum and Grohe</p><p>The book <i>Computers and Intractability : A Guide to the Theory of NP-Completeness</i> by Garey and Johnson</p><p>ALICE: Both of our lists are impressive. So now what?&nbsp;</p><p>--------------------------------------------------------------------</p><p>(This is Bill again.)</p><p>What indeed!</p><p>1) Why are there two definitions of Int Prog?</p><p>2) When is it good to use which one?&nbsp;</p><p><br /></p><p><br /></p><p class="authors">By gasarch</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T19:02:00Z">Monday, September 19 2022, 19:02</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://processalgebra.blogspot.com/2022/09/dean-of-school-of-technology-at.html'>Dean of the School of Technology at Reykjavik University: Call for applications</a></h3>
          <p class='item-feed'>from <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Reykjavik University is looking for a new dean of the School of Technology, which comprises the Department of Applied Engineering, the Department of Computer Science, and the Department of Engineering.&nbsp;</p><p>If you have a strong academic career, a vision of how our school can improve its standing and impact, and would enjoy living in Iceland, I encourage you to consider this opportunity!  See the ad at the link below for more information:&nbsp;</p><p>jobs.50skills.com/ru/en/15613&nbsp;</p><p>Spread the news through your network and encourage excellent candidates to apply.</p><p>By Luca Aceto</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p>Reykjavik University is looking for a new dean of the School of Technology, which comprises the Department of Applied Engineering, the Department of Computer Science, and the Department of Engineering.&nbsp;</p><p>If you have a strong academic career, a vision of how our school can improve its standing and impact, and would enjoy living in Iceland, I encourage you to consider this opportunity!  See the ad at the link below for more information:&nbsp;</p><p><a href="https://jobs.50skills.com/ru/en/15613">https://jobs.50skills.com/ru/en/15613</a>&nbsp;</p><p>Spread the news through your network and encourage excellent candidates to apply.</p><p class="authors">By Luca Aceto</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T16:58:00Z">Monday, September 19 2022, 16:58</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07625'>Extremal combinatorics, iterated pigeonhole arguments, and generalizations of PPP</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Amol Pasarkar, Mihalis Yannakakis, Christos Papadimitriou</p><p>We study the complexity of computational problems arising from existence
theorems in extremal combinatorics. For some of these problems, a solution is
guaranteed to exist based on an iterated application of the Pigeonhole
Principle. This results in the definition of a new complexity class within
TFNP, which we call PLC (for "polynomial long choice"). PLC includes all of
PPP, as well as numerous previously unclassified total problems, including
search problems related to Ramsey's theorem, the Sunflower theorem, the
Erd\H{o}s-Ko-Rado lemma, and K\"onig's lemma. Whether the first two of these
four problems are PLC-complete is an important open question which we pursue;
in contrast, we show that the latter two are PPP-complete. Finally, we reframe
PPP as an optimization problem, and define a hierarchy of such problems related
to Tur\'an's theorem.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Pasarkar_A/0/1/0/all/0/1">Amol Pasarkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Yannakakis_M/0/1/0/all/0/1">Mihalis Yannakakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Papadimitriou_C/0/1/0/all/0/1">Christos Papadimitriou</a></p><p>We study the complexity of computational problems arising from existence
theorems in extremal combinatorics. For some of these problems, a solution is
guaranteed to exist based on an iterated application of the Pigeonhole
Principle. This results in the definition of a new complexity class within
TFNP, which we call PLC (for "polynomial long choice"). PLC includes all of
PPP, as well as numerous previously unclassified total problems, including
search problems related to Ramsey's theorem, the Sunflower theorem, the
Erd\H{o}s-Ko-Rado lemma, and K\"onig's lemma. Whether the first two of these
four problems are PLC-complete is an important open question which we pursue;
in contrast, we show that the latter two are PPP-complete. Finally, we reframe
PPP as an optimization problem, and define a hierarchy of such problems related
to Tur\'an's theorem.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.08009'>Approximate traces on groups and the quantum complexity class $\operatorname{MIP}^{co,s}$</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Isaac Goldbring, Bradd Hart</p><p>An open question in quantum complexity theory is whether or not the class
$\operatorname{MIP}^{co}$, consisting of languages that can be efficiently
verified using interacting provers sharing quantum resources according to the
quantum commuting model, coincides with the class $coRE$ of languages with
recursively enumerable complement. We introduce the notion of a qc-modulus,
which encodes approximations to quantum commuting correlations, and show that
the existence of a computable qc-modulus gives a negative answer to a natural
variant of the aforementioned question.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Goldbring_I/0/1/0/all/0/1">Isaac Goldbring</a>, <a href="http://arxiv.org/find/cs/1/au:+Hart_B/0/1/0/all/0/1">Bradd Hart</a></p><p>An open question in quantum complexity theory is whether or not the class
$\operatorname{MIP}^{co}$, consisting of languages that can be efficiently
verified using interacting provers sharing quantum resources according to the
quantum commuting model, coincides with the class $coRE$ of languages with
recursively enumerable complement. We introduce the notion of a qc-modulus,
which encodes approximations to quantum commuting correlations, and show that
the existence of a computable qc-modulus gives a negative answer to a natural
variant of the aforementioned question.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.08042'>Decision Tree Complexity versus Block Sensitivity and Degree</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Rahul Chugh, Supartha Podder, Swagato Sanyal</p><p>Relations between the decision tree complexity and various other complexity
measures of Boolean functions is a thriving topic of research in computational
complexity. It is known that decision tree complexity is bounded above by the
cube of block sensitivity, and the cube of polynomial degree. However, the
widest separation between decision tree complexity and each of block
sensitivity and degree that is witnessed by known Boolean functions is
quadratic. In this work, we investigate the tightness of the existing cubic
upper bounds.
</p>
<p>We improve the cubic upper bounds for many interesting classes of Boolean
functions. We show that for graph properties and for functions with a constant
number of alternations, both of the cubic upper bounds can be improved to
quadratic. We define a class of Boolean functions, which we call the zebra
functions, that comprises Boolean functions where each monotone path from 0^n
to 1^n has an equal number of alternations. This class contains the symmetric
and monotone functions as its subclasses. We show that for any zebra function,
decision tree complexity is at most the square of block sensitivity, and
certificate complexity is at most the square of degree.
</p>
<p>Finally, we show using a lifting theorem of communication complexity by
G{\"{o}}{\"{o}}s, Pitassi and Watson that the task of proving an improved upper
bound on the decision tree complexity for all functions is in a sense
equivalent to the potentially easier task of proving a similar upper bound on
communication complexity for each bi-partition of the input variables, for all
functions. In particular, this implies that to bound the decision tree
complexity it suffices to bound smaller measures like parity decision tree
complexity, subcube decision tree complexity and decision tree rank, that are
defined in terms of models that can be efficiently simulated by communication
protocols.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chugh_R/0/1/0/all/0/1">Rahul Chugh</a>, <a href="http://arxiv.org/find/cs/1/au:+Podder_S/0/1/0/all/0/1">Supartha Podder</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1">Swagato Sanyal</a></p><p>Relations between the decision tree complexity and various other complexity
measures of Boolean functions is a thriving topic of research in computational
complexity. It is known that decision tree complexity is bounded above by the
cube of block sensitivity, and the cube of polynomial degree. However, the
widest separation between decision tree complexity and each of block
sensitivity and degree that is witnessed by known Boolean functions is
quadratic. In this work, we investigate the tightness of the existing cubic
upper bounds.
</p>
<p>We improve the cubic upper bounds for many interesting classes of Boolean
functions. We show that for graph properties and for functions with a constant
number of alternations, both of the cubic upper bounds can be improved to
quadratic. We define a class of Boolean functions, which we call the zebra
functions, that comprises Boolean functions where each monotone path from 0^n
to 1^n has an equal number of alternations. This class contains the symmetric
and monotone functions as its subclasses. We show that for any zebra function,
decision tree complexity is at most the square of block sensitivity, and
certificate complexity is at most the square of degree.
</p>
<p>Finally, we show using a lifting theorem of communication complexity by
G{\"{o}}{\"{o}}s, Pitassi and Watson that the task of proving an improved upper
bound on the decision tree complexity for all functions is in a sense
equivalent to the potentially easier task of proving a similar upper bound on
communication complexity for each bi-partition of the input variables, for all
functions. In particular, this implies that to bound the decision tree
complexity it suffices to bound smaller measures like parity decision tree
complexity, subcube decision tree complexity and decision tree rank, that are
defined in terms of models that can be efficiently simulated by communication
protocols.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07557'>On Optimal Coverage of a Tree with Multiple Robots</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: I. Aldana-Galv&#xe1;n, J.C. Catana-Salazar, J.M. D&#xed;az-B&#xe1;&#xf1;ez, F. Duque, R. Fabila-Monroy, M.A. Heredia, A. Ram&#xed;rez-Vigueras, J. Urrutia</p><p>We study the algorithmic problem of optimally covering a tree with $k$ mobile
robots. The tree is known to all robots, and our goal is to assign a walk to
each robot in such a way that the union of these walks covers the whole tree.
We assume that the edges have the same length, and that traveling along an edge
takes a unit of time. Two objective functions are considered: the cover time
and the cover length. The cover time is the maximum time a robot needs to
finish its assigned walk and the cover length is the sum of the lengths of all
the walks. We also consider a variant in which the robots must rendezvous
periodically at the same vertex in at most a certain number of moves. We show
that the problem is different for the two cost functions. For the cover time
minimization problem, we prove that the problem is NP-hard when $k$ is part of
the input, regardless of whether periodic rendezvous are required or not. For
the cover length minimization problem, we show that it can be solved in
polynomial time when periodic rendezvous are not required, and it is NP-hard
otherwise.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Aldana_Galvan_I/0/1/0/all/0/1">I. Aldana-Galv&#xe1;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Catana_Salazar_J/0/1/0/all/0/1">J.C. Catana-Salazar</a>, <a href="http://arxiv.org/find/cs/1/au:+Diaz_Banez_J/0/1/0/all/0/1">J.M. D&#xed;az-B&#xe1;&#xf1;ez</a>, <a href="http://arxiv.org/find/cs/1/au:+Duque_F/0/1/0/all/0/1">F. Duque</a>, <a href="http://arxiv.org/find/cs/1/au:+Fabila_Monroy_R/0/1/0/all/0/1">R. Fabila-Monroy</a>, <a href="http://arxiv.org/find/cs/1/au:+Heredia_M/0/1/0/all/0/1">M.A. Heredia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramirez_Vigueras_A/0/1/0/all/0/1">A. Ram&#xed;rez-Vigueras</a>, <a href="http://arxiv.org/find/cs/1/au:+Urrutia_J/0/1/0/all/0/1">J. Urrutia</a></p><p>We study the algorithmic problem of optimally covering a tree with $k$ mobile
robots. The tree is known to all robots, and our goal is to assign a walk to
each robot in such a way that the union of these walks covers the whole tree.
We assume that the edges have the same length, and that traveling along an edge
takes a unit of time. Two objective functions are considered: the cover time
and the cover length. The cover time is the maximum time a robot needs to
finish its assigned walk and the cover length is the sum of the lengths of all
the walks. We also consider a variant in which the robots must rendezvous
periodically at the same vertex in at most a certain number of moves. We show
that the problem is different for the two cost functions. For the cover time
minimization problem, we prove that the problem is NP-hard when $k$ is part of
the input, regardless of whether periodic rendezvous are required or not. For
the cover length minimization problem, we show that it can be solved in
polynomial time when periodic rendezvous are not required, and it is NP-hard
otherwise.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07772'>$b$-Coloring Parameterized by Pathwidth is XNLP-complete</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Lars Jaffke, Paloma T. Lima, Roohani Sharma</p><p>We show that the $b$-Coloring problem is complete for the class XNLP when
parameterized by the pathwidth of the input graph. Besides determining the
precise parameterized complexity of this problem, this implies that b-Coloring
parameterized by pathwidth is $W[t]$-hard for all $t$, and resolves the
parameterized complexity of $b$-Coloring parameterized by treewidth.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jaffke_L/0/1/0/all/0/1">Lars Jaffke</a>, <a href="http://arxiv.org/find/cs/1/au:+Lima_P/0/1/0/all/0/1">Paloma T. Lima</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1">Roohani Sharma</a></p><p>We show that the $b$-Coloring problem is complete for the class XNLP when
parameterized by the pathwidth of the input graph. Besides determining the
precise parameterized complexity of this problem, this implies that b-Coloring
parameterized by pathwidth is $W[t]$-hard for all $t$, and resolves the
parameterized complexity of $b$-Coloring parameterized by treewidth.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07580'>Exploring the Tradeoff between Competitive Ratio and Variance in Online-Matching Markets</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Pan Xu</p><p>In this paper, we propose an online-matching-based model to study the
assignment problems arising in a wide range of online-matching markets,
including online recommendations, ride-hailing platforms, and crowdsourcing
markets. It features that each assignment can request a random set of resources
and yield a random utility, and the two (cost and utility) can be arbitrarily
correlated with each other. We present two linear-programming-based
parameterized policies to study the tradeoff between the \emph{competitive
ratio} (CR) on the total utilities and the \emph{variance} on the total number
of matches (unweighted version). The first one (SAMP) is to sample an edge
according to the distribution extracted from the clairvoyant optimal, while the
second (ATT) features a time-adaptive attenuation framework that leads to an
improvement over the state-of-the-art competitive-ratio result. We also
consider the problem under a large-budget assumption and show that SAMP
achieves asymptotically optimal performance in terms of competitive ratio.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1">Pan Xu</a></p><p>In this paper, we propose an online-matching-based model to study the
assignment problems arising in a wide range of online-matching markets,
including online recommendations, ride-hailing platforms, and crowdsourcing
markets. It features that each assignment can request a random set of resources
and yield a random utility, and the two (cost and utility) can be arbitrarily
correlated with each other. We present two linear-programming-based
parameterized policies to study the tradeoff between the \emph{competitive
ratio} (CR) on the total utilities and the \emph{variance} on the total number
of matches (unweighted version). The first one (SAMP) is to sample an edge
according to the distribution extracted from the clairvoyant optimal, while the
second (ATT) features a time-adaptive attenuation framework that leads to an
improvement over the state-of-the-art competitive-ratio result. We also
consider the problem under a large-budget assumption and show that SAMP
achieves asymptotically optimal performance in terms of competitive ratio.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07729'>On Weighted Graph Sparsification by Linear Sketching</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Yu Chen, Sanjeev Khanna, Huan Li</p><p>A seminal work of [Ahn-Guha-McGregor, PODS'12] showed that one can compute a
cut sparsifier of an unweighted undirected graph by taking a near-linear number
of linear measurements on the graph. Subsequent works also studied computing
other graph sparsifiers using linear sketching, and obtained near-linear upper
bounds for spectral sparsifiers [Kapralov-Lee-Musco-Musco-Sidford, FOCS'14] and
first non-trivial upper bounds for spanners [Filtser-Kapralov-Nouri, SODA'21].
All these linear sketching algorithms, however, only work on unweighted graphs.
</p>
<p>In this paper, we initiate the study of weighted graph sparsification by
linear sketching by investigating a natural class of linear sketches that we
call incidence sketches, in which each measurement is a linear combination of
the weights of edges incident on a single vertex. Our results are:
</p>
<p>1. Weighted cut sparsification: We give an algorithm that computes a $(1 +
\epsilon)$-cut sparsifier using $\tilde{O}(n \epsilon^{-3})$ linear
measurements, which is nearly optimal.
</p>
<p>2. Weighted spectral sparsification: We give an algorithm that computes a $(1
+ \epsilon)$-spectral sparsifier using $\tilde{O}(n^{6/5} \epsilon^{-4})$
linear measurements. Complementing our algorithm, we then prove a superlinear
lower bound of $\Omega(n^{21/20-o(1)})$ measurements for computing some
$O(1)$-spectral sparsifier using incidence sketches.
</p>
<p>3. Weighted spanner computation: We focus on graphs whose largest/smallest
edge weights differ by an $O(1)$ factor, and prove that, for incidence
sketches, the upper bounds obtained by~[Filtser-Kapralov-Nouri, SODA'21] are
optimal up to an $n^{o(1)}$ factor.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Khanna_S/0/1/0/all/0/1">Sanjeev Khanna</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huan Li</a></p><p>A seminal work of [Ahn-Guha-McGregor, PODS'12] showed that one can compute a
cut sparsifier of an unweighted undirected graph by taking a near-linear number
of linear measurements on the graph. Subsequent works also studied computing
other graph sparsifiers using linear sketching, and obtained near-linear upper
bounds for spectral sparsifiers [Kapralov-Lee-Musco-Musco-Sidford, FOCS'14] and
first non-trivial upper bounds for spanners [Filtser-Kapralov-Nouri, SODA'21].
All these linear sketching algorithms, however, only work on unweighted graphs.
</p>
<p>In this paper, we initiate the study of weighted graph sparsification by
linear sketching by investigating a natural class of linear sketches that we
call incidence sketches, in which each measurement is a linear combination of
the weights of edges incident on a single vertex. Our results are:
</p>
<p>1. Weighted cut sparsification: We give an algorithm that computes a $(1 +
\epsilon)$-cut sparsifier using $\tilde{O}(n \epsilon^{-3})$ linear
measurements, which is nearly optimal.
</p>
<p>2. Weighted spectral sparsification: We give an algorithm that computes a $(1
+ \epsilon)$-spectral sparsifier using $\tilde{O}(n^{6/5} \epsilon^{-4})$
linear measurements. Complementing our algorithm, we then prove a superlinear
lower bound of $\Omega(n^{21/20-o(1)})$ measurements for computing some
$O(1)$-spectral sparsifier using incidence sketches.
</p>
<p>3. Weighted spanner computation: We focus on graphs whose largest/smallest
edge weights differ by an $O(1)$ factor, and prove that, for incidence
sketches, the upper bounds obtained by~[Filtser-Kapralov-Nouri, SODA'21] are
optimal up to an $n^{o(1)}$ factor.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07860'>A $(1.5+\epsilon)$-Approximation Algorithm for Weighted Connectivity Augmentation</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Vera Traub, Rico Zenklusen</p><p>Connectivity augmentation problems are among the most elementary questions in
Network Design. Many of these problems admit natural $2$-approximation
algorithms, often through various classic techniques, whereas it remains open
whether approximation factors below $2$ can be achieved. One of the most basic
examples thereof is the Weighted Connectivity Augmentation Problem (WCAP). In
WCAP, one is given an undirected graph together with a set of additional
weighted candidate edges, and the task is to find a cheapest set of candidate
edges whose addition to the graph increases its edge-connectivity. We present a
$(1.5+\varepsilon)$-approximation algorithm for WCAP, showing for the first
time that factors below $2$ are achievable.
</p>
<p>On a high level, we design a well-chosen local search algorithm, inspired by
recent advances for Weighted Tree Augmentation. To measure progress, we
consider a directed weakening of WCAP and show that it has highly structured
planar solutions. Interpreting a solution of the original problem as one of
this directed weakening allows us to describe local exchange steps in a clean
and algorithmically amenable way. Leveraging these insights, we show that we
can efficiently search for good exchange steps within a component class for
link sets that is closely related to bounded treewidth subgraphs of circle
graphs. Moreover, we prove that an optimum solution can be decomposed into
smaller components, at least one of which leads to a good local search step as
long as we did not yet achieve the claimed approximation guarantee.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Traub_V/0/1/0/all/0/1">Vera Traub</a>, <a href="http://arxiv.org/find/cs/1/au:+Zenklusen_R/0/1/0/all/0/1">Rico Zenklusen</a></p><p>Connectivity augmentation problems are among the most elementary questions in
Network Design. Many of these problems admit natural $2$-approximation
algorithms, often through various classic techniques, whereas it remains open
whether approximation factors below $2$ can be achieved. One of the most basic
examples thereof is the Weighted Connectivity Augmentation Problem (WCAP). In
WCAP, one is given an undirected graph together with a set of additional
weighted candidate edges, and the task is to find a cheapest set of candidate
edges whose addition to the graph increases its edge-connectivity. We present a
$(1.5+\varepsilon)$-approximation algorithm for WCAP, showing for the first
time that factors below $2$ are achievable.
</p>
<p>On a high level, we design a well-chosen local search algorithm, inspired by
recent advances for Weighted Tree Augmentation. To measure progress, we
consider a directed weakening of WCAP and show that it has highly structured
planar solutions. Interpreting a solution of the original problem as one of
this directed weakening allows us to describe local exchange steps in a clean
and algorithmically amenable way. Leveraging these insights, we show that we
can efficiently search for good exchange steps within a component class for
link sets that is closely related to bounded treewidth subgraphs of circle
graphs. Moreover, we prove that an optimum solution can be decomposed into
smaller components, at least one of which leads to a good local search step as
long as we did not yet achieve the claimed approximation guarantee.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07988'>Prophet Inequalities for Cost Minimization</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Vasilis Livanos, Ruta Mehta</p><p>Prophet inequalities for rewards maximization are fundamental results from
optimal stopping theory with several applications to mechanism design and
online optimization. We study the cost minimization counterpart of the
classical prophet inequality, where one is facing a sequence of costs $X_1,
X_2, \dots, X_n$ in an online manner and must ''stop'' at some point and take
the last cost seen. Given that the $X_i$'s are independent, drawn from known
distributions, the goal is to devise a stopping strategy $S$ (online algorithm)
that minimizes the expected cost.
</p>
<p>We first observe that if the $X_i$'s are not identically distributed, then no
strategy can achieve a bounded approximation, no matter if the arrival order is
adversarial or random. This leads us to consider the case where the $X_i$'s are
I.I.D.. For the I.I.D. case, we give a complete characterization of the optimal
stopping strategy. We show that it achieves a (distribution-dependent)
constant-factor approximation to the prophet's cost for almost all
distributions and that this constant is tight. In particular, for distributions
for which the integral of the hazard rate is a polynomial $H(x) = \sum_{i=1}^k
a_i x^{d_i}$, where $d_1 &lt; \dots &lt; d_k$, the approximation factor is
$\lambda(d_1)$, a decreasing function of $d_1$. Furthermore, for MHR
distributions, we show that this constant is at most $2$, and this is again
tight.
</p>
<p>We also analyze single-threshold strategies for the cost prophet inequality
problem. We design a threshold that achieves a
$\operatorname{O}(\operatorname{polylog}n)$-factor approximation, where the
exponent in the logarithmic factor is a distribution-dependent constant, and we
show a matching lower bound.
</p>
<p>We believe that our results are of independent interest for analyzing
approximately optimal (posted price-style) mechanisms for procuring items.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Livanos_V/0/1/0/all/0/1">Vasilis Livanos</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1">Ruta Mehta</a></p><p>Prophet inequalities for rewards maximization are fundamental results from
optimal stopping theory with several applications to mechanism design and
online optimization. We study the cost minimization counterpart of the
classical prophet inequality, where one is facing a sequence of costs $X_1,
X_2, \dots, X_n$ in an online manner and must ''stop'' at some point and take
the last cost seen. Given that the $X_i$'s are independent, drawn from known
distributions, the goal is to devise a stopping strategy $S$ (online algorithm)
that minimizes the expected cost.
</p>
<p>We first observe that if the $X_i$'s are not identically distributed, then no
strategy can achieve a bounded approximation, no matter if the arrival order is
adversarial or random. This leads us to consider the case where the $X_i$'s are
I.I.D.. For the I.I.D. case, we give a complete characterization of the optimal
stopping strategy. We show that it achieves a (distribution-dependent)
constant-factor approximation to the prophet's cost for almost all
distributions and that this constant is tight. In particular, for distributions
for which the integral of the hazard rate is a polynomial $H(x) = \sum_{i=1}^k
a_i x^{d_i}$, where $d_1 &lt; \dots &lt; d_k$, the approximation factor is
$\lambda(d_1)$, a decreasing function of $d_1$. Furthermore, for MHR
distributions, we show that this constant is at most $2$, and this is again
tight.
</p>
<p>We also analyze single-threshold strategies for the cost prophet inequality
problem. We design a threshold that achieves a
$\operatorname{O}(\operatorname{polylog}n)$-factor approximation, where the
exponent in the logarithmic factor is a distribution-dependent constant, and we
show a matching lower bound.
</p>
<p>We believe that our results are of independent interest for analyzing
approximately optimal (posted price-style) mechanisms for procuring items.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.08024'>Fast approximation of search trees on trees with centroid trees</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Benjamin Aram Berendsohn, Ishay Golinsky, Haim Kaplan, L&#xe1;szl&#xf3; Kozma</p><p>Search trees on trees (STTs) generalize the fundamental binary search tree
(BST) data structure: in STTs the underlying search space is an arbitrary tree,
whereas in BSTs it is a path. An optimal BST of size $n$ can be computed for a
given distribution of queries in $O(n^2)$ time [Knuth 1971] and centroid BSTs
provide a nearly-optimal alternative, computable in $O(n)$ time [Mehlhorn
1977].
</p>
<p>By contrast, optimal STTs are not known to be computable in polynomial time,
and the fastest constant-approximation algorithm runs in $O(n^3)$ time
[Berendsohn, Kozma 2022]. Centroid trees can be defined for STTs analogously to
BSTs, and they have been used in a wide range of algorithmic applications. In
the unweighted case (i.e., for a uniform distribution of queries), a centroid
tree can be computed in $O(n)$ time [Brodal et al. 2001; Della Giustina et al.
2019]. These algorithms, however, do not readily extend to the weighted case.
Moreover, no approximation guarantees were previously known for centroid trees
in either the unweighted or weighted cases.
</p>
<p>In this paper we revisit centroid trees in a general, weighted setting, and
we settle both the algorithmic complexity of constructing them, and the quality
of their approximation. For constructing a weighted centroid tree, we give an
output-sensitive $O(n\log h)\subseteq O(n\log n)$ time algorithm, where $h$ is
the height of the resulting centroid tree. If the weights are of polynomial
complexity, the running time is $O(n\log\log n)$. We show these bounds to be
optimal, in a general decision tree model of computation. For approximation, we
prove that the cost of a centroid tree is at most twice the optimum, and this
guarantee is best possible, both in the weighted and unweighted cases. We also
give tight, fine-grained bounds on the approximation-ratio for bounded-degree
trees and on the approximation-ratio of more general $\alpha$-centroid trees.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Berendsohn_B/0/1/0/all/0/1">Benjamin Aram Berendsohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Golinsky_I/0/1/0/all/0/1">Ishay Golinsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaplan_H/0/1/0/all/0/1">Haim Kaplan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kozma_L/0/1/0/all/0/1">L&#xe1;szl&#xf3; Kozma</a></p><p>Search trees on trees (STTs) generalize the fundamental binary search tree
(BST) data structure: in STTs the underlying search space is an arbitrary tree,
whereas in BSTs it is a path. An optimal BST of size $n$ can be computed for a
given distribution of queries in $O(n^2)$ time [Knuth 1971] and centroid BSTs
provide a nearly-optimal alternative, computable in $O(n)$ time [Mehlhorn
1977].
</p>
<p>By contrast, optimal STTs are not known to be computable in polynomial time,
and the fastest constant-approximation algorithm runs in $O(n^3)$ time
[Berendsohn, Kozma 2022]. Centroid trees can be defined for STTs analogously to
BSTs, and they have been used in a wide range of algorithmic applications. In
the unweighted case (i.e., for a uniform distribution of queries), a centroid
tree can be computed in $O(n)$ time [Brodal et al. 2001; Della Giustina et al.
2019]. These algorithms, however, do not readily extend to the weighted case.
Moreover, no approximation guarantees were previously known for centroid trees
in either the unweighted or weighted cases.
</p>
<p>In this paper we revisit centroid trees in a general, weighted setting, and
we settle both the algorithmic complexity of constructing them, and the quality
of their approximation. For constructing a weighted centroid tree, we give an
output-sensitive $O(n\log h)\subseteq O(n\log n)$ time algorithm, where $h$ is
the height of the resulting centroid tree. If the weights are of polynomial
complexity, the running time is $O(n\log\log n)$. We show these bounds to be
optimal, in a general decision tree model of computation. For approximation, we
prove that the cost of a centroid tree is at most twice the optimum, and this
guarantee is best possible, both in the weighted and unweighted cases. We also
give tight, fine-grained bounds on the approximation-ratio for bounded-degree
trees and on the approximation-ratio of more general $\alpha$-centroid trees.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Sunday, September 18
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://eccc.weizmann.ac.il/report/2022/132'>TR22-132 |  Fooling polynomials using invariant theory | 

	Harm Derksen, 

	Emanuele Viola</a></h3>
          <p class='item-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          We revisit the problem of constructing explicit pseudorandom generators
that fool with error $\epsilon$ degree-$d$ polynomials in $n$ variables
over the field $F_q$, in the case of large $q$. Previous constructions
either have seed length at least $2^{d}\log q$, and thus are only non-trivial
when the degree is less than $\log n$, or else rely on a seminal reduction by Bogdanov
(STOC 2005). This reduction yields seed length not less than $d^{4} \log n + \log q$
and requires fields of size at least $d^{6}/\epsilon^{2}$; and explicit generators
meeting such bounds are known.

Departing from Bogdanov&#39;s reduction, we develop an algebraic analogue
of the Bogdanov-Viola paradigm (FOCS 2007, SICOMP 2010) of summing
generators for degree-one polynomials. Whereas previous analyses of
the paradigm are restricted to degree less than $\log n$, we give a new analysis
which handles large degrees. A main new idea is to show that the construction
preserves indecomposability of polynomials. Apparently for the first
time in the area, the proof uses invariant theory.

Our approach in particular yields several new pseudorandom generators.
In particular, for large enough fields we obtain seed length $O(d\log n+\log q)$
which is optimal up to constant factors. We also construct generators
for fields of size as small as $O(d^{4})$. Further reducing the field
size requires a significant change in techniques: Most or all generators
for large-degree polynomials rely on Weil bounds; but such bounds
are only applicable when $q&gt;d^{4}$.
        
        </div>

        <div class='item-content item-summary'>
        
          
          We revisit the problem of constructing explicit pseudorandom generators
that fool with error $\epsilon$ degree-$d$ polynomials in $n$ variables
over the field $F_q$, in the case of large $q$. Previous constructions
either have seed length at least $2^{d}\log q$, and thus are only non-trivial
when the degree is less than $\log n$, or else rely on a seminal reduction by Bogdanov
(STOC 2005). This reduction yields seed length not less than $d^{4} \log n + \log q$
and requires fields of size at least $d^{6}/\epsilon^{2}$; and explicit generators
meeting such bounds are known.

Departing from Bogdanov&#39;s reduction, we develop an algebraic analogue
of the Bogdanov-Viola paradigm (FOCS 2007, SICOMP 2010) of summing
generators for degree-one polynomials. Whereas previous analyses of
the paradigm are restricted to degree less than $\log n$, we give a new analysis
which handles large degrees. A main new idea is to show that the construction
preserves indecomposability of polynomials. Apparently for the first
time in the area, the proof uses invariant theory.

Our approach in particular yields several new pseudorandom generators.
In particular, for large enough fields we obtain seed length $O(d\log n+\log q)$
which is optimal up to constant factors. We also construct generators
for fields of size as small as $O(d^{4})$. Further reducing the field
size requires a significant change in techniques: Most or all generators
for large-degree polynomials rely on Weil bounds; but such bounds
are only applicable when $q&gt;d^{4}$.
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-18T20:32:26Z">Sunday, September 18 2022, 20:32</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://adamsheffer.wordpress.com/2022/09/18/new-cs-tenure-track-positions-new-program-in-manhattan/'>New CS Tenure Track Positions, New program in Manhattan</a></h3>
          <p class='item-feed'>from <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          Do you happen to know anyone who is looking for a CS Tenure Track position in Manhattan? Someone who might be interested in being part of a new CS program and help decide the directions it will grow into? Please share this with them: geometrynyc.wixsite.com/csjobs.<p>By Adam Sheffer</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          Do you happen to know anyone who is looking for a CS Tenure Track position in Manhattan? Someone who might be interested in being part of a new CS program and help decide the directions it will grow into? Please share this with them: https://geometrynyc.wixsite.com/csjobs.<p class="authors">By Adam Sheffer</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-18T18:01:54Z">Sunday, September 18 2022, 18:01</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://eccc.weizmann.ac.il/report/2022/131'>TR22-131 |  Radical Sylvester-Gallai for Cubics | 

	Rafael Mendes de Oliveira, 

	Akash Sengupta</a></h3>
          <p class='item-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          Let $\mathcal{F} = \{F_1, \ldots, F_m\}$ be a finite set of irreducible homogeneous multivariate polynomials of degree at most $3$ such that $F_i$ does not divide $F_j$ for $i\neq j$. 
We say that $\mathcal{F}$ is a cubic radical Sylvester-Gallai configuration if for any two distinct $F_i,F_j$ there exists a third polynomial $F_k$ such that whenever $F_i,F_j$ vanish, $F_k$ also vanishes. In particular, for any two indices $i, j \in [m]$, there exists $k \in [m] \setminus \{i,j\}$ such that $F_k \in \rad(F_i, F_j)$.

We prove that any cubic radical Sylvester-Gallai configuration is low-dimensional, that is
$$ \dim span_{\mathbb{K}}{\mathcal{F}} = O(1).$$
This solves a conjecture of Gupta [G14] in degree $3$ and generalizes the result in [S20], which proved that quadratic radical Sylvester-Gallai configurations are low-dimensional. 
Our result takes us one step closer towards solving the non-linear Sylvester-Gallai conjectures of Gupta [G14], which would yield the first deterministic polynomial time algorithm for the PIT problem for depth-4 circuits of bounded top and bottom fanins.


To prove our Sylvester-Gallai theorem, we develop several new tools combining techniques from algebraic geometry and elimination theory.
Among our technical contributions, we prove a structure theorem characterizing non-radical ideals generated by two cubic forms, generalizing the structure theorems of [HP94, CTSSD87, S20].
Moreover, building upon the groundbreaking work [AH20a], we introduce the notion of wide Ananyan-Hochster algebras and show that these algebras allow us to transfer the local conditions of Sylvester-Gallai configurations into global conditions.
        
        </div>

        <div class='item-content item-summary'>
        
          
          Let $\mathcal{F} = \{F_1, \ldots, F_m\}$ be a finite set of irreducible homogeneous multivariate polynomials of degree at most $3$ such that $F_i$ does not divide $F_j$ for $i\neq j$. 
We say that $\mathcal{F}$ is a cubic radical Sylvester-Gallai configuration if for any two distinct $F_i,F_j$ there exists a third polynomial $F_k$ such that whenever $F_i,F_j$ vanish, $F_k$ also vanishes. In particular, for any two indices $i, j \in [m]$, there exists $k \in [m] \setminus \{i,j\}$ such that $F_k \in \rad(F_i, F_j)$.

We prove that any cubic radical Sylvester-Gallai configuration is low-dimensional, that is
$$ \dim span_{\mathbb{K}}{\mathcal{F}} = O(1).$$
This solves a conjecture of Gupta [G14] in degree $3$ and generalizes the result in [S20], which proved that quadratic radical Sylvester-Gallai configurations are low-dimensional. 
Our result takes us one step closer towards solving the non-linear Sylvester-Gallai conjectures of Gupta [G14], which would yield the first deterministic polynomial time algorithm for the PIT problem for depth-4 circuits of bounded top and bottom fanins.


To prove our Sylvester-Gallai theorem, we develop several new tools combining techniques from algebraic geometry and elimination theory.
Among our technical contributions, we prove a structure theorem characterizing non-radical ideals generated by two cubic forms, generalizing the structure theorems of [HP94, CTSSD87, S20].
Moreover, building upon the groundbreaking work [AH20a], we introduce the notion of wide Ananyan-Hochster algebras and show that these algebras allow us to transfer the local conditions of Sylvester-Gallai configurations into global conditions.
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-18T08:52:56Z">Sunday, September 18 2022, 08:52</time>
        </div>
      </div>
    </article>
  
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Friday, September 16
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://eccc.weizmann.ac.il/report/2022/130'>TR22-130 |  A Borsuk-Ulam lower bound for sign-rank and its application | 

	Hamed Hatami, 

	Kaave Hosseini, 

	Xiang Meng</a></h3>
          <p class='item-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          We introduce a new topological argument based on the Borsuk-Ulam theorem to prove a lower bound on sign-rank.  

      This result implies the strongest possible separation between randomized and unbounded-error communication complexity. More precisely, we show that for a particular range of parameters, the randomized communication complexity of the Gap Hamming Distance problem is $O(1)$ while its unbounded-error communication complexity is  $\Omega(\log(n))$. 
     
 Previously, it was unknown whether the unbounded-error communication complexity could be asymptotically larger than the randomized communication complexity.
    
In connection to learning theory, we prove that,  despite its learnability properties, the class of large margin half-spaces in $\mathbb{R}^d$ is genuinely high-dimensional, i.e., it cannot be embedded in $\mathbb{R}^{d-1}$. This result is closely related to a recent conjecture of Alon, Hanneke, Holzman, and Moran (FOCS 2021) about the VC dimension of this class.
    
Our final application is to the theory of dimension reductions. The Johnson-Lindenstrauss theorem implies that any set of $N$ unit vectors is embeddable in dimension  $O(\gamma^{-2}\log N)$ without altering the signs of those pairwise inner products that have absolute values at least $\gamma&gt;0$.  Our result establishes the tightness of this bound, which answers a question of Linial, Mendelson, Schechtman, and Shraibman (Combinatorica, 27(2007)) in the case of partial functions.
        
        </div>

        <div class='item-content item-summary'>
        
          
          We introduce a new topological argument based on the Borsuk-Ulam theorem to prove a lower bound on sign-rank.  

      This result implies the strongest possible separation between randomized and unbounded-error communication complexity. More precisely, we show that for a particular range of parameters, the randomized communication complexity of the Gap Hamming Distance problem is $O(1)$ while its unbounded-error communication complexity is  $\Omega(\log(n))$. 
     
 Previously, it was unknown whether the unbounded-error communication complexity could be asymptotically larger than the randomized communication complexity.
    
In connection to learning theory, we prove that,  despite its learnability properties, the class of large margin half-spaces in $\mathbb{R}^d$ is genuinely high-dimensional, i.e., it cannot be embedded in $\mathbb{R}^{d-1}$. This result is closely related to a recent conjecture of Alon, Hanneke, Holzman, and Moran (FOCS 2021) about the VC dimension of this class.
    
Our final application is to the theory of dimension reductions. The Johnson-Lindenstrauss theorem implies that any set of $N$ unit vectors is embeddable in dimension  $O(\gamma^{-2}\log N)$ without altering the signs of those pairwise inner products that have absolute values at least $\gamma&gt;0$.  Our result establishes the tightness of this bound, which answers a question of Linial, Mendelson, Schechtman, and Shraibman (Combinatorica, 27(2007)) in the case of partial functions.
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T02:54:31Z">Friday, September 16 2022, 02:54</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.06930'>How Much Structure Is Needed for Huge Quantum Speedups?</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Scott Aaronson</p><p>I survey, for a general scientific audience, three decades of research into
which sorts of problems admit exponential speedups via quantum computers --
from the classics (like the algorithms of Simon and Shor), to the breakthrough
of Yamakawa and Zhandry from April 2022. I discuss both the quantum circuit
model, which is what we ultimately care about in practice but where our
knowledge is radically incomplete, and the so-called oracle or black-box or
query complexity model, where we've managed to achieve a much more thorough
understanding that then informs our conjectures about the circuit model. I
discuss the strengths and weaknesses of switching attention to sampling tasks,
as was done in the recent quantum supremacy experiments. I make some skeptical
remarks about widely-repeated claims of exponential quantum speedups for
practical machine learning and optimization problems. Through many examples, I
try to convey the "law of conservation of weirdness," according to which every
problem admitting an exponential quantum speedup must have some unusual
property to allow the amplitude to be concentrated on the unknown right
answer(s).
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Aaronson_S/0/1/0/all/0/1">Scott Aaronson</a></p><p>I survey, for a general scientific audience, three decades of research into
which sorts of problems admit exponential speedups via quantum computers --
from the classics (like the algorithms of Simon and Shor), to the breakthrough
of Yamakawa and Zhandry from April 2022. I discuss both the quantum circuit
model, which is what we ultimately care about in practice but where our
knowledge is radically incomplete, and the so-called oracle or black-box or
query complexity model, where we've managed to achieve a much more thorough
understanding that then informs our conjectures about the circuit model. I
discuss the strengths and weaknesses of switching attention to sampling tasks,
as was done in the recent quantum supremacy experiments. I make some skeptical
remarks about widely-repeated claims of exponential quantum speedups for
practical machine learning and optimization problems. Through many examples, I
try to convey the "law of conservation of weirdness," according to which every
problem admitting an exponential quantum speedup must have some unusual
property to allow the amplitude to be concentrated on the unknown right
answer(s).
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.06939'>The Complexity Classes of Hamming Distance Recoverable Robust Problems</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Christoph Gr&#xfc;ne</p><p>In the well-known complexity class NP, many combinatorial problems can be
found, whose optimization counterpart are important for many practical
settings. Those problems usually consider full knowledge about the input and
optimize on this specific input. In a practical setting, however, uncertainty
in the input data is a usual phenomenon, whereby this is normally not covered
in optimization versions of NP problems. One concept to model the uncertainty
in the input data, is \textit{recoverable robustness}. In this setting, a
solution on the input is calculated, whereby a possible recovery to a good
solution should be guaranteed, whenever uncertainty manifests itself. That is,
a solution $\texttt{s}_0$ for the base scenario $\textsf{S}_0$ as well as a
solution \texttt{s} for every possible scenario of scenario set \textsf{S} has
to be calculated. In other words, not only solution $\texttt{s}_0$ for instance
$\textsf{S}_0$ is calculated but solutions \texttt{s} for all scenarios from
\textsf{S} are prepared to correct possible errors through uncertainty. This
paper introduces a specific concept of recoverable robust problems: Hamming
Distance Recoverable Robust Problems. In this setting, solutions $\texttt{s}_0$
and \texttt{s} have to be calculated, such that $\texttt{s}_0$ and \texttt{s}
may only differ in at most $\kappa$ elements. That is, one can recover from a
harmful scenario by choosing a different solution, which is not too far away
from the first solution. This paper surveys the complexity of Hamming distance
recoverable robust version of optimization problems, typically found in NP for
different types of scenarios. The complexity is primarily situated in the lower
levels of the polynomial hierarchy. The main contribution of the paper is that
recoverable robust problems with compression-encoded scenarios and $m \in
\mathbb{N}$ recoveries are $\Sigma^P_{2m+1}$-complete.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Grune_C/0/1/0/all/0/1">Christoph Gr&#xfc;ne</a></p><p>In the well-known complexity class NP, many combinatorial problems can be
found, whose optimization counterpart are important for many practical
settings. Those problems usually consider full knowledge about the input and
optimize on this specific input. In a practical setting, however, uncertainty
in the input data is a usual phenomenon, whereby this is normally not covered
in optimization versions of NP problems. One concept to model the uncertainty
in the input data, is \textit{recoverable robustness}. In this setting, a
solution on the input is calculated, whereby a possible recovery to a good
solution should be guaranteed, whenever uncertainty manifests itself. That is,
a solution $\texttt{s}_0$ for the base scenario $\textsf{S}_0$ as well as a
solution \texttt{s} for every possible scenario of scenario set \textsf{S} has
to be calculated. In other words, not only solution $\texttt{s}_0$ for instance
$\textsf{S}_0$ is calculated but solutions \texttt{s} for all scenarios from
\textsf{S} are prepared to correct possible errors through uncertainty. This
paper introduces a specific concept of recoverable robust problems: Hamming
Distance Recoverable Robust Problems. In this setting, solutions $\texttt{s}_0$
and \texttt{s} have to be calculated, such that $\texttt{s}_0$ and \texttt{s}
may only differ in at most $\kappa$ elements. That is, one can recover from a
harmful scenario by choosing a different solution, which is not too far away
from the first solution. This paper surveys the complexity of Hamming distance
recoverable robust version of optimization problems, typically found in NP for
different types of scenarios. The complexity is primarily situated in the lower
levels of the polynomial hierarchy. The main contribution of the paper is that
recoverable robust problems with compression-encoded scenarios and $m \in
\mathbb{N}$ recoveries are $\Sigma^P_{2m+1}$-complete.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07497'>On Power Set Axiom</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Leonid A. Levin</p><p>Usual math sets have special types: countable, compact, open, occasionally
Borel, rarely projective, etc. Generic sets dependent on Power Set axiom appear
mostly in esoteric areas, ST logic, etc. Dropping that Axiom may greatly
simplify the foundations of mainstream math. Meanwhile dependence on it of a
theorem is worth noting, as dependence on Choice often is.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Levin_L/0/1/0/all/0/1">Leonid A. Levin</a></p><p>Usual math sets have special types: countable, compact, open, occasionally
Borel, rarely projective, etc. Generic sets dependent on Power Set axiom appear
mostly in esoteric areas, ST logic, etc. Dropping that Axiom may greatly
simplify the foundations of mainstream math. Meanwhile dependence on it of a
theorem is worth noting, as dependence on Choice often is.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.06968'>Stochastic strategies for patrolling a terrain with a synchronized multi-robot system</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Luis E. Caraballo, Jos&#xe9; M. D&#xed;az-B&#xe1;&#xf1;ez, Ruy Fabila-Monroy, Carlos Hidalgo-Toscan</p><p>A group of cooperative aerial robots can be deployed to efficiently patrol a
terrain, in which each robot flies around an assigned area and shares
information with the neighbors periodically in order to protect or supervise
it. To ensure robustness, previous works on these synchronized systems propose
sending a robot to the neighboring area in case it detects a failure. In order
to deal with unpredictability and to improve on the efficiency in the
deterministic patrolling scheme, this paper proposes random strategies to cover
the areas distributed among the agents. First, a theoretical study of the
stochastic process is addressed in this paper for two metrics: the \emph{idle
time}, the expected time between two consecutive observations of any point of
the terrain and the \emph{isolation time}, the expected time that a robot is
without communication with any other robot. After that, the random strategies
are experimentally compared with the deterministic strategy adding another
metric: the \emph{broadcast time}, the expected time elapsed from the moment a
robot emits a message until it is received by all the other robots of the team.
The simulations show that theoretical results are in good agreement with the
simulations and the random strategies outperform the behavior obtained with the
deterministic protocol proposed in the literature.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Caraballo_L/0/1/0/all/0/1">Luis E. Caraballo</a>, <a href="http://arxiv.org/find/cs/1/au:+Diaz_Banez_J/0/1/0/all/0/1">Jos&#xe9; M. D&#xed;az-B&#xe1;&#xf1;ez</a>, <a href="http://arxiv.org/find/cs/1/au:+Fabila_Monroy_R/0/1/0/all/0/1">Ruy Fabila-Monroy</a>, <a href="http://arxiv.org/find/cs/1/au:+Hidalgo_Toscan_C/0/1/0/all/0/1">Carlos Hidalgo-Toscan</a></p><p>A group of cooperative aerial robots can be deployed to efficiently patrol a
terrain, in which each robot flies around an assigned area and shares
information with the neighbors periodically in order to protect or supervise
it. To ensure robustness, previous works on these synchronized systems propose
sending a robot to the neighboring area in case it detects a failure. In order
to deal with unpredictability and to improve on the efficiency in the
deterministic patrolling scheme, this paper proposes random strategies to cover
the areas distributed among the agents. First, a theoretical study of the
stochastic process is addressed in this paper for two metrics: the \emph{idle
time}, the expected time between two consecutive observations of any point of
the terrain and the \emph{isolation time}, the expected time that a robot is
without communication with any other robot. After that, the random strategies
are experimentally compared with the deterministic strategy adding another
metric: the \emph{broadcast time}, the expected time elapsed from the moment a
robot emits a message until it is received by all the other robots of the team.
The simulations show that theoretical results are in good agreement with the
simulations and the random strategies outperform the behavior obtained with the
deterministic protocol proposed in the literature.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07517'>Spectral Total-Variation Processing of Shapes -- Theory and Applications</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Jonathan Brokman, Martin Burger, Guy Gilboa</p><p>In this work we present a comprehensive analysis of total variation (TV) on
non Euclidean domains and its eigenfunctions. We specifically address
parameterized surfaces, a natural representation of the shapes used in 3D
graphics. Our work sheds new light on the celebrated Beltrami and Anisotropic
TV flows, and explains experimental findings from recent years on shape
spectral TV [Fumero et al. 2020] and adaptive anisotropic spectral TV [Biton
and Gilboa 2022]. A new notion of convexity on manifolds is derived, by
characterizing structures that are stable throughout the TV flow, performed on
manifolds. We further propose a time efficient nonlinear and non Euclidean
spectral framework for shape processing that is based on zero homogeneous
flows, and propose three different such methods. Each method satisfies distinct
characteristics, demonstrated through smoothing, enhancing and exaggerating
filters.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Brokman_J/0/1/0/all/0/1">Jonathan Brokman</a>, <a href="http://arxiv.org/find/cs/1/au:+Burger_M/0/1/0/all/0/1">Martin Burger</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilboa_G/0/1/0/all/0/1">Guy Gilboa</a></p><p>In this work we present a comprehensive analysis of total variation (TV) on
non Euclidean domains and its eigenfunctions. We specifically address
parameterized surfaces, a natural representation of the shapes used in 3D
graphics. Our work sheds new light on the celebrated Beltrami and Anisotropic
TV flows, and explains experimental findings from recent years on shape
spectral TV [Fumero et al. 2020] and adaptive anisotropic spectral TV [Biton
and Gilboa 2022]. A new notion of convexity on manifolds is derived, by
characterizing structures that are stable throughout the TV flow, performed on
manifolds. We further propose a time efficient nonlinear and non Euclidean
spectral framework for shape processing that is based on zero homogeneous
flows, and propose three different such methods. Each method satisfies distinct
characteristics, demonstrated through smoothing, enhancing and exaggerating
filters.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.06909'>Multiway Powersort</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: William Cawley Gelling, Markus E. Nebel, Benjamin Smith, Sebastian Wild</p><p>Powersort (Munro &amp; Wild, ESA2018) has recently replaced Timsort's suboptimal
merge policy in the CPython reference implementation of Python, as well as in
PyPy and further libraries. We present a stable mergesort variant, Multiway
Powersort, that exploits existing runs and finds nearly-optimal merging orders
for k-way merges with negligible overhead. As observed with Multiway Quicksort
(Kushagra et al., ALENEX 2014; Aum\"uller &amp; Dietzfelbinger, TALG 2016; Wild,
PhD thesis 2016) and the inclusion of Dual-Pivot Quicksort in the Java runtime
library, memory transfers increasingly determine the cost of internal sorting.
We demonstrate that our 4-way Powersort implementation can achieve substantial
speedups over standard (2-way) Powersort and other stable sorting methods
without compromising the optimally run-adaptive performance of Powersort.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gelling_W/0/1/0/all/0/1">William Cawley Gelling</a>, <a href="http://arxiv.org/find/cs/1/au:+Nebel_M/0/1/0/all/0/1">Markus E. Nebel</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_B/0/1/0/all/0/1">Benjamin Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Wild_S/0/1/0/all/0/1">Sebastian Wild</a></p><p>Powersort (Munro &amp; Wild, ESA2018) has recently replaced Timsort's suboptimal
merge policy in the CPython reference implementation of Python, as well as in
PyPy and further libraries. We present a stable mergesort variant, Multiway
Powersort, that exploits existing runs and finds nearly-optimal merging orders
for k-way merges with negligible overhead. As observed with Multiway Quicksort
(Kushagra et al., ALENEX 2014; Aum\"uller &amp; Dietzfelbinger, TALG 2016; Wild,
PhD thesis 2016) and the inclusion of Dual-Pivot Quicksort in the Java runtime
library, memory transfers increasingly determine the cost of internal sorting.
We demonstrate that our 4-way Powersort implementation can achieve substantial
speedups over standard (2-way) Powersort and other stable sorting methods
without compromising the optimally run-adaptive performance of Powersort.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07016'>Algorithms and Lower Bounds for Replacement Paths under Multiple Edge Failures</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Virginia Vassilevska Williams, Eyob Woldeghebriel, Yinzhan Xu</p><p>This paper considers a natural fault-tolerant shortest paths problem: for
some constant integer $f$, given a directed weighted graph with no negative
cycles and two fixed vertices $s$ and $t$, compute (either explicitly or
implicitly) for every tuple of $f$ edges, the distance from $s$ to $t$ if these
edges fail. We call this problem $f$-Fault Replacement Paths ($f$FRP).
</p>
<p>We first present an $\tilde{O}(n^3)$ time algorithm for $2$FRP in $n$-vertex
directed graphs with arbitrary edge weights and no negative cycles. As $2$FRP
is a generalization of the well-studied Replacement Paths problem (RP) that
asks for the distances between $s$ and $t$ for any single edge failure, $2$FRP
is at least as hard as RP. Since RP in graphs with arbitrary weights is
equivalent in a fine-grained sense to All-Pairs Shortest Paths (APSP)
[Vassilevska Williams and Williams FOCS'10, J.~ACM'18], $2$FRP is at least as
hard as APSP, and thus a substantially subcubic time algorithm in the number of
vertices for $2$FRP would be a breakthrough. Therefore, our algorithm in
$\tilde{O}(n^3)$ time is conditionally nearly optimal. Our algorithm implies an
$\tilde{O}(n^{f+1})$ time algorithm for the $f$FRP problem, giving the first
improvement over the straightforward $O(n^{f+2})$ time algorithm.
</p>
<p>Then we focus on the restriction of $2$FRP to graphs with small integer
weights bounded by $M$ in absolute values. Using fast rectangular matrix
multiplication, we obtain a randomized algorithm that runs in
$\tilde{O}(M^{2/3}n^{2.9153})$ time. This implies an improvement over our
$\tilde{O}(n^{f+1})$ time arbitrary weight algorithm for all $f&gt;1$. We also
present a data structure variant of the algorithm that can trade off
pre-processing and query time. In addition to the algebraic algorithms, we also
give an $n^{8/3-o(1)}$ conditional lower bound for combinatorial $2$FRP
algorithms in directed unweighted graphs.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Williams_V/0/1/0/all/0/1">Virginia Vassilevska Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Woldeghebriel_E/0/1/0/all/0/1">Eyob Woldeghebriel</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yinzhan Xu</a></p><p>This paper considers a natural fault-tolerant shortest paths problem: for
some constant integer $f$, given a directed weighted graph with no negative
cycles and two fixed vertices $s$ and $t$, compute (either explicitly or
implicitly) for every tuple of $f$ edges, the distance from $s$ to $t$ if these
edges fail. We call this problem $f$-Fault Replacement Paths ($f$FRP).
</p>
<p>We first present an $\tilde{O}(n^3)$ time algorithm for $2$FRP in $n$-vertex
directed graphs with arbitrary edge weights and no negative cycles. As $2$FRP
is a generalization of the well-studied Replacement Paths problem (RP) that
asks for the distances between $s$ and $t$ for any single edge failure, $2$FRP
is at least as hard as RP. Since RP in graphs with arbitrary weights is
equivalent in a fine-grained sense to All-Pairs Shortest Paths (APSP)
[Vassilevska Williams and Williams FOCS'10, J.~ACM'18], $2$FRP is at least as
hard as APSP, and thus a substantially subcubic time algorithm in the number of
vertices for $2$FRP would be a breakthrough. Therefore, our algorithm in
$\tilde{O}(n^3)$ time is conditionally nearly optimal. Our algorithm implies an
$\tilde{O}(n^{f+1})$ time algorithm for the $f$FRP problem, giving the first
improvement over the straightforward $O(n^{f+2})$ time algorithm.
</p>
<p>Then we focus on the restriction of $2$FRP to graphs with small integer
weights bounded by $M$ in absolute values. Using fast rectangular matrix
multiplication, we obtain a randomized algorithm that runs in
$\tilde{O}(M^{2/3}n^{2.9153})$ time. This implies an improvement over our
$\tilde{O}(n^{f+1})$ time arbitrary weight algorithm for all $f&gt;1$. We also
present a data structure variant of the algorithm that can trade off
pre-processing and query time. In addition to the algebraic algorithms, we also
give an $n^{8/3-o(1)}$ conditional lower bound for combinatorial $2$FRP
algorithms in directed unweighted graphs.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07024'>Almost Ramanujan Expanders from Arbitrary Expanders via Operator Amplification</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Fernando Granha Jeronimo, Tushant Mittal, Sourya Roy, Avi Wigderson</p><p>We give an efficient algorithm that transforms any bounded degree expander
graph into another that achieves almost optimal (namely, near-quadratic, $d
\leq 1/\lambda^{2+o(1)}$) trade-off between (any desired) spectral expansion
$\lambda$ and degree $d$. Furthermore, the algorithm is local: every vertex can
compute its new neighbors as a subset of its original neighborhood of radius
$O(\log(1/\lambda))$. The optimal quadratic trade-off is known as the Ramanujan
bound, so our construction gives almost Ramanujan expanders from arbitrary
expanders.
</p>
<p>The locality of the transformation preserves structural properties of the
original graph, and thus has many consequences. Applied to Cayley graphs, our
transformation shows that any expanding finite group has almost Ramanujan
expanding generators. Similarly, one can obtain almost optimal explicit
constructions of quantum expanders, dimension expanders, monotone expanders,
etc., from existing (suboptimal) constructions of such objects. Another
consequence is a "derandomized" random walk on the original (suboptimal)
expander with almost optimal convergence rate. Our transformation also applies
when the degree is not bounded or the expansion is not constant.
</p>
<p>We obtain our results by a generalization of Ta-Shma's technique in his
breakthrough paper [STOC 2017], used to obtain explicit almost optimal binary
codes. Specifically, our spectral amplification extends Ta-Shma's analysis of
bias amplification from scalars to matrices of arbitrary dimension in a very
natural way. Curiously, while Ta-Shma's explicit bias amplification
derandomizes a well-known probabilistic argument (underlying the
Gilbert--Varshamov bound), there seems to be no known probabilistic (or other
existential) way of achieving our explicit ("high-dimensional") spectral
amplification.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jeronimo_F/0/1/0/all/0/1">Fernando Granha Jeronimo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_T/0/1/0/all/0/1">Tushant Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1">Sourya Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Wigderson_A/0/1/0/all/0/1">Avi Wigderson</a></p><p>We give an efficient algorithm that transforms any bounded degree expander
graph into another that achieves almost optimal (namely, near-quadratic, $d
\leq 1/\lambda^{2+o(1)}$) trade-off between (any desired) spectral expansion
$\lambda$ and degree $d$. Furthermore, the algorithm is local: every vertex can
compute its new neighbors as a subset of its original neighborhood of radius
$O(\log(1/\lambda))$. The optimal quadratic trade-off is known as the Ramanujan
bound, so our construction gives almost Ramanujan expanders from arbitrary
expanders.
</p>
<p>The locality of the transformation preserves structural properties of the
original graph, and thus has many consequences. Applied to Cayley graphs, our
transformation shows that any expanding finite group has almost Ramanujan
expanding generators. Similarly, one can obtain almost optimal explicit
constructions of quantum expanders, dimension expanders, monotone expanders,
etc., from existing (suboptimal) constructions of such objects. Another
consequence is a "derandomized" random walk on the original (suboptimal)
expander with almost optimal convergence rate. Our transformation also applies
when the degree is not bounded or the expansion is not constant.
</p>
<p>We obtain our results by a generalization of Ta-Shma's technique in his
breakthrough paper [STOC 2017], used to obtain explicit almost optimal binary
codes. Specifically, our spectral amplification extends Ta-Shma's analysis of
bias amplification from scalars to matrices of arbitrary dimension in a very
natural way. Curiously, while Ta-Shma's explicit bias amplification
derandomizes a well-known probabilistic argument (underlying the
Gilbert--Varshamov bound), there seems to be no known probabilistic (or other
existential) way of achieving our explicit ("high-dimensional") spectral
amplification.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07100'>Concurrent Size</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Gal Sela, Erez Petrank</p><p>The size of a data structure (i.e., the number of elements in it) is a widely
used property of a data set. However, for concurrent programs, obtaining a
correct size efficiently is non-trivial. In fact, the literature does not offer
a mechanism to obtain a correct (linearizable) size of a concurrent data set
without resorting to inefficient solutions, such as taking a full snapshot of
the data structure to count the elements, or acquiring one global lock in all
update and size operations. This paper presents a methodology for adding a
concurrent linearizable size operation to sets and dictionaries with a
relatively low performance overhead. Theoretically, the proposed size operation
is wait-free with asymptotic complexity linear in the number of threads
(independently of data-structure size). Practically, we evaluated the
performance overhead by adding size to various concurrent data structures in
Java$-$a skip list, a hash table and a tree. The proposed linearizable size
operation executes faster by orders of magnitude compared to the existing
option of taking a snapshot, while incurring a throughput loss of $1\%-20\%$ on
the original data structure's operations.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Sela_G/0/1/0/all/0/1">Gal Sela</a>, <a href="http://arxiv.org/find/cs/1/au:+Petrank_E/0/1/0/all/0/1">Erez Petrank</a></p><p>The size of a data structure (i.e., the number of elements in it) is a widely
used property of a data set. However, for concurrent programs, obtaining a
correct size efficiently is non-trivial. In fact, the literature does not offer
a mechanism to obtain a correct (linearizable) size of a concurrent data set
without resorting to inefficient solutions, such as taking a full snapshot of
the data structure to count the elements, or acquiring one global lock in all
update and size operations. This paper presents a methodology for adding a
concurrent linearizable size operation to sets and dictionaries with a
relatively low performance overhead. Theoretically, the proposed size operation
is wait-free with asymptotic complexity linear in the number of threads
(independently of data-structure size). Practically, we evaluated the
performance overhead by adding size to various concurrent data structures in
Java$-$a skip list, a hash table and a tree. The proposed linearizable size
operation executes faster by orders of magnitude compared to the existing
option of taking a snapshot, while incurring a throughput loss of $1\%-20\%$ on
the original data structure's operations.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07312'>Multicalibrated Regression for Downstream Fairness</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Ira Globus-Harris, Varun Gupta, Christopher Jung, Michael Kearns, Jamie Morgenstern, Aaron Roth</p><p>We show how to take a regression function $\hat{f}$ that is appropriately
``multicalibrated'' and efficiently post-process it into an approximately error
minimizing classifier satisfying a large variety of fairness constraints. The
post-processing requires no labeled data, and only a modest amount of unlabeled
data and computation. The computational and sample complexity requirements of
computing $\hat f$ are comparable to the requirements for solving a single fair
learning task optimally, but it can in fact be used to solve many different
downstream fairness-constrained learning problems efficiently. Our
post-processing method easily handles intersecting groups, generalizing prior
work on post-processing regression functions to satisfy fairness constraints
that only applied to disjoint groups. Our work extends recent work showing that
multicalibrated regression functions are ``omnipredictors'' (i.e. can be
post-processed to optimally solve unconstrained ERM problems) to constrained
optimization.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Globus_Harris_I/0/1/0/all/0/1">Ira Globus-Harris</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1">Varun Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1">Christopher Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Kearns_M/0/1/0/all/0/1">Michael Kearns</a>, <a href="http://arxiv.org/find/cs/1/au:+Morgenstern_J/0/1/0/all/0/1">Jamie Morgenstern</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1">Aaron Roth</a></p><p>We show how to take a regression function $\hat{f}$ that is appropriately
``multicalibrated'' and efficiently post-process it into an approximately error
minimizing classifier satisfying a large variety of fairness constraints. The
post-processing requires no labeled data, and only a modest amount of unlabeled
data and computation. The computational and sample complexity requirements of
computing $\hat f$ are comparable to the requirements for solving a single fair
learning task optimally, but it can in fact be used to solve many different
downstream fairness-constrained learning problems efficiently. Our
post-processing method easily handles intersecting groups, generalizing prior
work on post-processing regression functions to satisfy fairness constraints
that only applied to disjoint groups. Our work extends recent work showing that
multicalibrated regression functions are ``omnipredictors'' (i.e. can be
post-processed to optimally solve unconstrained ERM problems) to constrained
optimization.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07440'>Envy-freeness in 3D Hedonic Games</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: &#xc1;gnes Cseh, Michael McKay, David Manlove</p><p>We study the problem of partitioning a set of agents into coalitions based on
the agents' additively separable preferences, which can also be viewed as a
hedonic game. We apply three successively weaker solution concepts, namely
envy-freeness, weakly justified envy-freeness, and justified envy-freeness.
</p>
<p>In a model in which coalitions may have any size, trivial solutions exist for
these concepts, which provides a strong motivation for placing restrictions on
coalition size. In this paper, we require feasible coalitions to have size
three. We study the existence of partitions that are envy-free, weakly
justified envy-free, and justified envy-free, and the computational complexity
of finding such partitions, if they exist.
</p>
<p>We present a comprehensive complexity classification, in terms of the
restrictions placed on the agents' preferences. From this, we identify a
general trend that for the three successively weaker solution concepts,
existence and polynomial-time solvability hold under successively weaker
restrictions.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cseh_A/0/1/0/all/0/1">&#xc1;gnes Cseh</a>, <a href="http://arxiv.org/find/cs/1/au:+McKay_M/0/1/0/all/0/1">Michael McKay</a>, <a href="http://arxiv.org/find/cs/1/au:+Manlove_D/0/1/0/all/0/1">David Manlove</a></p><p>We study the problem of partitioning a set of agents into coalitions based on
the agents' additively separable preferences, which can also be viewed as a
hedonic game. We apply three successively weaker solution concepts, namely
envy-freeness, weakly justified envy-freeness, and justified envy-freeness.
</p>
<p>In a model in which coalitions may have any size, trivial solutions exist for
these concepts, which provides a strong motivation for placing restrictions on
coalition size. In this paper, we require feasible coalitions to have size
three. We study the existence of partitions that are envy-free, weakly
justified envy-free, and justified envy-free, and the computational complexity
of finding such partitions, if they exist.
</p>
<p>We present a comprehensive complexity classification, in terms of the
restrictions placed on the agents' preferences. From this, we identify a
general trend that for the three successively weaker solution concepts,
existence and polynomial-time solvability hold under successively weaker
restrictions.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07463'>Omnipredictors for Constrained Optimization</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Lunjia Hu, Inbal Livni-Navon, Omer Reingold, Chutong Yang</p><p>The notion of omnipredictors (Gopalan, Kalai, Reingold, Sharan and Wieder
ITCS 2021), suggested a new paradigm for loss minimization. Rather than
learning a predictor based on a known loss function, omnipredictors can easily
be post-processed to minimize any one of a rich family of loss functions
compared with the loss of a class $C$. It has been shown that such
omnipredictors exist and are implied (for all convex and Lipschitz loss
functions) by the notion of multicalibration from the algorithmic fairness
literature. Nevertheless, it is often the case that the action selected must
obey some additional constraints (such as capacity or parity constraints). In
itself, the original notion of omnipredictors does not apply in this
well-motivated and heavily studied the context of constrained loss
minimization.
</p>
<p>In this paper, we introduce omnipredictors for constrained optimization and
study their complexity and implications. The notion that we introduce allows
the learner to be unaware of the loss function that will be later assigned as
well as the constraints that will be later imposed, as long as the
subpopulations that are used to define these constraints are known.
</p>
<p>The paper shows how to obtain omnipredictors for constrained optimization
problems, relying on appropriate variants of multicalibration. For some
interesting constraints and general loss functions and for general constraints
and some interesting loss functions, we show how omnipredictors are implied by
a variant of multicalibration that is similar in complexity to standard
multicalibration. We demonstrate that in the general case, standard
multicalibration is insufficient and show that omnipredictors are implied by
multicalibration with respect to a class containing all the level sets of
hypotheses in $C$. We also investigate the implications when the constraints
are group fairness notions.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Lunjia Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Livni_Navon_I/0/1/0/all/0/1">Inbal Livni-Navon</a>, <a href="http://arxiv.org/find/cs/1/au:+Reingold_O/0/1/0/all/0/1">Omer Reingold</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chutong Yang</a></p><p>The notion of omnipredictors (Gopalan, Kalai, Reingold, Sharan and Wieder
ITCS 2021), suggested a new paradigm for loss minimization. Rather than
learning a predictor based on a known loss function, omnipredictors can easily
be post-processed to minimize any one of a rich family of loss functions
compared with the loss of a class $C$. It has been shown that such
omnipredictors exist and are implied (for all convex and Lipschitz loss
functions) by the notion of multicalibration from the algorithmic fairness
literature. Nevertheless, it is often the case that the action selected must
obey some additional constraints (such as capacity or parity constraints). In
itself, the original notion of omnipredictors does not apply in this
well-motivated and heavily studied the context of constrained loss
minimization.
</p>
<p>In this paper, we introduce omnipredictors for constrained optimization and
study their complexity and implications. The notion that we introduce allows
the learner to be unaware of the loss function that will be later assigned as
well as the constraints that will be later imposed, as long as the
subpopulations that are used to define these constraints are known.
</p>
<p>The paper shows how to obtain omnipredictors for constrained optimization
problems, relying on appropriate variants of multicalibration. For some
interesting constraints and general loss functions and for general constraints
and some interesting loss functions, we show how omnipredictors are implied by
a variant of multicalibration that is similar in complexity to standard
multicalibration. We demonstrate that in the general case, standard
multicalibration is insufficient and show that omnipredictors are implied by
multicalibration with respect to a class containing all the level sets of
hypotheses in $C$. We also investigate the implications when the constraints
are group fairness notions.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07520'>On (Random-order) Online Contention Resolution Schemes for the Matching Polytope of (Bipartite) Graphs</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Calum MacRury, Will Ma, Nathaniel Grammel</p><p>We present new results for online contention resolution schemes for the
matching polytope of graphs, in the random-order (RCRS) and adversarial (OCRS)
arrival models. Our results include improved selectability guarantees (i.e.,
lower bounds), as well as new impossibility results (i.e., upper bounds). By
well-known reductions to the prophet (secretary) matching problem, a
$c$-selectable OCRS (RCRS) implies a $c$-competitive algorithm for adversarial
(random order) edge arrivals. Similar reductions are also known for the
query-commit matching problem. For the adversarial arrival model, we present a
new analysis of the OCRS of Ezra et al.~(EC, 2020). We show that this scheme is
$0.344$-selectable for general graphs and $0.349$-selectable for bipartite
graphs, improving on the previous $0.337$ selectability result for this
algorithm. We also show that the selectability of this scheme cannot be greater
than $0.361$ for general graphs and $0.382$ for bipartite graphs. We further
show that no OCRS can achieve a selectability greater than $0.4$ for general
graphs, and $0.433$ for bipartite graphs.
</p>
<p>For random-order arrivals, we present two attenuation-based schemes which use
new attenuation functions. Our first RCRS is $0.474$-selectable for general
graphs, and our second is $0.476$-selectable for bipartite graphs. These
results improve upon the recent $0.45$ (and $0.456$) selectability results for
general graphs (respectively, bipartite graphs) due to Pollner et al.~(EC,
2022). On general graphs, our 0.474-selectable RCRS provides the best known
positive result even for offline contention resolution, and also for the
correlation gap. We conclude by proving a fundamental upper bound of 0.5 on the
selectability of RCRS, using bipartite graphs.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+MacRury_C/0/1/0/all/0/1">Calum MacRury</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Will Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Grammel_N/0/1/0/all/0/1">Nathaniel Grammel</a></p><p>We present new results for online contention resolution schemes for the
matching polytope of graphs, in the random-order (RCRS) and adversarial (OCRS)
arrival models. Our results include improved selectability guarantees (i.e.,
lower bounds), as well as new impossibility results (i.e., upper bounds). By
well-known reductions to the prophet (secretary) matching problem, a
$c$-selectable OCRS (RCRS) implies a $c$-competitive algorithm for adversarial
(random order) edge arrivals. Similar reductions are also known for the
query-commit matching problem. For the adversarial arrival model, we present a
new analysis of the OCRS of Ezra et al.~(EC, 2020). We show that this scheme is
$0.344$-selectable for general graphs and $0.349$-selectable for bipartite
graphs, improving on the previous $0.337$ selectability result for this
algorithm. We also show that the selectability of this scheme cannot be greater
than $0.361$ for general graphs and $0.382$ for bipartite graphs. We further
show that no OCRS can achieve a selectability greater than $0.4$ for general
graphs, and $0.433$ for bipartite graphs.
</p>
<p>For random-order arrivals, we present two attenuation-based schemes which use
new attenuation functions. Our first RCRS is $0.474$-selectable for general
graphs, and our second is $0.476$-selectable for bipartite graphs. These
results improve upon the recent $0.45$ (and $0.456$) selectability results for
general graphs (respectively, bipartite graphs) due to Pollner et al.~(EC,
2022). On general graphs, our 0.474-selectable RCRS provides the best known
positive result even for offline contention resolution, and also for the
correlation gap. We conclude by proving a fundamental upper bound of 0.5 on the
selectability of RCRS, using bipartite graphs.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07524'>$\tilde{O}(n+\mathrm{poly}(k))$-time Algorithm for Bounded Tree Edit Distance</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Debarati Das, Jacob Gilbert, MohammadTaghi Hajiaghayi, Tomasz Kociumaka, Barna Saha, Hamed Saleh</p><p>Computing the edit distance of two strings is one of the most basic problems
in computer science and combinatorial optimization. Tree edit distance is a
natural generalization of edit distance in which the task is to compute a
measure of dissimilarity between two (unweighted) rooted trees with node
labels. Perhaps the most notable recent application of tree edit distance is in
NoSQL big databases, such as MongoDB, where each row of the database is a JSON
document represented as a labeled rooted tree, and finding dissimilarity
between two rows is a basic operation. Until recently, the fastest algorithm
for tree edit distance ran in cubic time (Demaine, Mozes, Rossman, Weimann;
TALG'10); however, Mao (FOCS'21) broke the cubic barrier for the tree edit
distance problem using fast matrix multiplication.
</p>
<p>Given a parameter $k$ as an upper bound on the distance, an $O(n+k^2)$-time
algorithm for edit distance has been known since the 1980s due to the works of
Myers (Algorithmica'86) and Landau and Vishkin (JCSS'88). The existence of an
$\tilde{O}(n+\mathrm{poly}(k))$-time algorithm for tree edit distance has been
posed as an open question, e.g., by Akmal and Jin (ICALP'21), who gave a
state-of-the-art $\tilde{O}(nk^2)$-time algorithm. In this paper, we answer
this question positively.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1">Debarati Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilbert_J/0/1/0/all/0/1">Jacob Gilbert</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1">MohammadTaghi Hajiaghayi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kociumaka_T/0/1/0/all/0/1">Tomasz Kociumaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_B/0/1/0/all/0/1">Barna Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Saleh_H/0/1/0/all/0/1">Hamed Saleh</a></p><p>Computing the edit distance of two strings is one of the most basic problems
in computer science and combinatorial optimization. Tree edit distance is a
natural generalization of edit distance in which the task is to compute a
measure of dissimilarity between two (unweighted) rooted trees with node
labels. Perhaps the most notable recent application of tree edit distance is in
NoSQL big databases, such as MongoDB, where each row of the database is a JSON
document represented as a labeled rooted tree, and finding dissimilarity
between two rows is a basic operation. Until recently, the fastest algorithm
for tree edit distance ran in cubic time (Demaine, Mozes, Rossman, Weimann;
TALG'10); however, Mao (FOCS'21) broke the cubic barrier for the tree edit
distance problem using fast matrix multiplication.
</p>
<p>Given a parameter $k$ as an upper bound on the distance, an $O(n+k^2)$-time
algorithm for edit distance has been known since the 1980s due to the works of
Myers (Algorithmica'86) and Landau and Vishkin (JCSS'88). The existence of an
$\tilde{O}(n+\mathrm{poly}(k))$-time algorithm for tree edit distance has been
posed as an open question, e.g., by Akmal and Jin (ICALP'21), who gave a
state-of-the-art $\tilde{O}(nk^2)$-time algorithm. In this paper, we answer
this question positively.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Thursday, September 15
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://11011110.github.io/blog/2022/09/15/linkage.html'>Linkage</a></h3>
          <p class='item-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          Lithophanes, a 19th-century art medium involving backlit translucent engravings, revived via 3d printing as a single format for scientific images that blind people can read by feeling and sighted people can see (\(\mathbb{M}\), original research paper).
        
        </div>

        <div class='item-content item-summary'>
        
          
          <ul>
  <li>
    <p>Lithophanes, a 19th-century art medium involving backlit translucent engravings, <a href="https://arstechnica.com/science/2022/08/19th-century-art-form-revived-to-make-tactile-science-graphics-for-the-blind/">revived via 3d printing as a single format for scientific images that blind people can read by feeling and sighted people can see</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108927524393418764">\(\mathbb{M}\)</a>,</span> <a href="https://doi.org/10.1126/sciadv.abq2640">original research paper</a>).</p>
  </li>
  <li>
    <p>A quick ânot a Reuleaux triangleâ link: <a href="https://www.instructables.com/Lego-Triangle/">Lego triangles</a>, incorrectly embellished as <a href="https://makezine.com/article/maker-news/lego-reuleaux-triangles/">Lego Reuleaux triangles</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108933430914328377">\(\mathbb{M}\)</a>).</span> The second link even goes on to say âI donât think [they] are quite convex enough to be proper Reuleaux trianglesâ. And in this itâs correct, if âconvexâ is interpreted to mean âbulgyâ. You can tell because the corners are right angles. Proper Reuleaux triangles would have \(120^\circ\) corners.</p>
  </li>
  <li>
    <p>As part of a search for real-world applications of combinatorial design theory, Jeremy Kun asks: â<a href="https://mathstodon.xyz/@j2kun/108936354352902797">What is the authoritative text on combinatorial designs, anyway?</a>â I suspect that the shakeup in the area caused by Peter Keevashâs use of the probabilistic method has caused what used to be the authoritative texts to become obsolete.</p>
  </li>
  <li>
    <p><a href="https://youtube.com/shorts/KYMYshbhKcw">Squaring the circle illusion</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@henryseg/108940642070353471">\(\mathbb{M}\)</a>).</span> Henry Segerman 3d-prints a shape that, when rotated \(180^\circ\), changes appearance from a circle to a square. With the same area in the viewing frame, even.</p>
  </li>
  <li>
    <p>Finding a vertex-to-vertex and edge-to-edge mapping (âhomomorphismâ) from an input directed graph to a fixed oriented tree or cycle can be either polynomial-time or <span style="white-space:nowrap">\(\mathsf{NP}\)-complete,</span> depending on the target. But <a href="https://cstheory.stackexchange.com/questions/33836/complexity-of-digraph-homomorphism-to-an-oriented-cycle">the targets for which it is <span style="white-space:nowrap">\(\mathsf{NP}\)-complete</span> are surprisingly complicated!</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108950099488030475">\(\mathbb{M}\)</a>)</span>  <a href="https://arxiv.org/abs/2205.07528">A recent search</a> found the smallest hard tree to have <span style="white-space:nowrap">\(20\) vertices,</span> and the smallest hard cycle to <span style="white-space:nowrap">have \(26\).</span></p>
  </li>
  <li>
    <p>Tamal Dey and Yusu Wang have a new graduate-level textbook on computational topology <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108953135639829468">\(\mathbb{M}\)</a>):</span> <em><a href="https://www.cs.purdue.edu/homes/tamaldey/book/CTDAbook/CTDAbook.pdf">Computational Topology for Data Analysis</a></em> (<a href="https://doi.org/10.1017/9781009099950">Cambridge University Press, 2022</a>). <a href="https://www.maa.org/press/maa-reviews/computational-topology-for-data-analysis">Ellen Gasparovic reviews it for the MAA</a>.</p>
  </li>
  <li>
    <p><a href="https://web.archive.org/web/20220908210937/https://www.nytimes.com/interactive/2022/09/08/world/europe/succession-royal-family.html">In case anyone else is looking for a topical real-world example of a depth-first traversal of a tree</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108965503136254522">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p>Chegg stops pretending not to have coursework-cheating as their main business; <a href="https://www.chronicle.com/article/some-students-use-chegg-to-cheat-the-site-has-stopped-helping-colleges-catch-them">will no longer cooperate with university cheating investigations</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108970379605304595">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="http://www.math.brown.edu/reschwar/PosterPappus/pappus.png">Schwartzâs Pappus fractal</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108972366289751471">\(\mathbb{M}\)</a>,</span> <a href="http://www.math.brown.edu/reschwar/PosterPappus/pappus.pdf">explanation</a>). Start with two separate lines of three points \(abc\) <span style="white-space:nowrap">and \(ABC\).</span> By <a href="https://en.wikipedia.org/wiki/Pappus%27s_hexagon_theorem">Pappusâs theorem</a> the diagonal crossing points <span style="white-space:nowrap">\(\alpha=aB\cdot Ab\),</span> <span style="white-space:nowrap">\(\beta=aC\cdot Ac\),</span> and \(\gamma=bC\cdot Bc\) form another line of three <span style="white-space:nowrap">points \(\alpha\beta\gamma\).</span> Now recurse with \(abc\) <span style="white-space:nowrap">and \(\alpha\beta\gamma\),</span> and again with \(\alpha\beta\gamma\) <span style="white-space:nowrap">and \(ABC\).</span> You get a nice lightning-bolt fractal shape.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/html/2209.04402">This yearâs <em>Graph Drawing</em> conference proceedings are online</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108982719390301261">\(\mathbb{M}\)</a>).</span>  As in past years, theyâre using a system where the proceedings are published both on arXiv and in Springer LNCS.</p>
  </li>
  <li>
    <p><a href="https://ris.utwente.nl/ws/portalfiles/portal/275927505/3e2a9e5b2fad237a3d35f36fa2c5f44552f2.pdf">An analysis of online exam-proctoring tool Proctorio finds it completely ineffective at catching cheaters</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108984339518916646">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=32744976">via</a>). âThe use of online proctoring is therefore best compared to taking a placebo: it has some positive influence, not because it works but because people believe that it works â¦ policy makers would do well to balance the cost of deploying it (which can be
considerable) against the marginal benefits of this placebo effect.â</p>
  </li>
  <li>
    <p><a href="https://langorigami.com/publication/paper-pentasia-an-aperiodic-surface-in-modular-origami/">Langâs paper pentasia</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108995556838713267">\(\mathbb{M}\)</a>).</span> The kite and dart Penrose tiling lifts to a surface in 3d with two equilateral triangles per tile, overhanging for the darts. Robert Lang made these nice physical models. Explanations by <a href="https://link.springer.com/article/10.1007/s00283-021-10088-4">Barry Cipra on the 1993 discovery of this surface with Conway</a> and <a href="https://langorigami.com/wp-content/uploads/2015/09/paper-pentasia.pdf">Lang and Barry Hayes, also on a related 3d surface for the rhombic Penrose tiling</a>.</p>
  </li>
  <li>
    <p>Three more new Wikipedia Good Articles <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108998160700266138">\(\mathbb{M}\)</a>):</span></p>

    <ul>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Euclidean_minimum_spanning_tree">Euclidean minimum spanning tree</a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Doyle_spiral">Doyle spiral</a>, spiraling circle packings in which each circle is surrounded by a ring of six tangent circles</p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Laves_graph">Laves graph</a>, a highly-symmetric 3-regular infinite graph in 3d</p>
      </li>
    </ul>
  </li>
  <li>
    <p>An algorithmic version of <a href="en.wikipedia.org/wiki/Mantel's theorem">Mantelâs theorem</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109004244575186009">\(\mathbb{M}\)</a>):</span> If an <span style="white-space:nowrap">\(n\)-vertex</span> graph has more than <span style="white-space:nowrap">\(\bigl\lfloor\tfrac{n^2}{4}\bigr\rfloor\) edges</span> then we can find a triangle in it in linear time.</p>

    <p><em>Proof:</em> Delete min-degree vertices, maintaining the edge excess, until the remaining \(k\) vertices all have <span style="white-space:nowrap">degree \(\ge\tfrac{k}{2}\).</span> If <span style="white-space:nowrap">\(k\) is odd,</span> all <span style="white-space:nowrap">have \(\ge \tfrac{k+1}{2}\);</span> if <span style="white-space:nowrap">\(k\) is even,</span> at least one <span style="white-space:nowrap">has \(\ge \tfrac{k}{2}+1\).</span> Then a max-degree vertex and any neighbor have together <span style="white-space:nowrap">\(\ge k+1\) incidences,</span> so by the pigeonhole principle they have a common neighbor forming a <span style="white-space:nowrap">triangle. \(\Box\)</span></p>
  </li>
</ul><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-15T22:36:00Z">Thursday, September 15 2022, 22:36</time>
        </div>
      </div>
    </article>
  
  </div>

  <script src='js/jquery-2.0.3.min.js'></script>
  <script src="js/jquery.timeago.js" type="text/javascript"></script>
  <script>
    jQuery(document).ready(function() {
      jQuery("time.timeago").timeago();
    });
  </script>
  <script src='js/blank.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
