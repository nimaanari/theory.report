<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.6 (2023-03-30) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2023-05-09T01:45:49Z">Tuesday, May 09 2023, 01:45</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://minorfree.github.io/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://minorfree.github.io'>Hung Le</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>Sébastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Tuesday, May 09
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.03946'>An Improved PTAS for Covering Targets with Mobile Sensors</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Nonthaphat Wongwattanakij, Nattawut Phetmak, Chaiporn Jaikaeo, Jittat Fakcharoenphol</p><p>This paper considers a movement minimization problem for mobile sensors.
Given a set of $n$ point targets, the $k$-Sink Minimum Movement Target Coverage
Problem is to schedule mobile sensors, initially located at $k$ base stations,
to cover all targets minimizing the total moving distance of the sensors. We
present a polynomial-time approximation scheme for finding a $(1+\epsilon)$
approximate solution running in time $n^{O(1/\epsilon)}$ for this problem when
$k$, the number of base stations, is constant. Our algorithm improves the
running time exponentially from the previous work that runs in time
$n^{O(1/\epsilon^2)}$, without any target distribution assumption. To devise a
faster algorithm, we prove a stronger bound on the number of sensors in any
unit area in the optimal solution and employ a more refined dynamic programming
algorithm whose complexity depends only on the width of the problem.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Wongwattanakij_N/0/1/0/all/0/1">Nonthaphat Wongwattanakij</a>, <a href="http://arxiv.org/find/cs/1/au:+Phetmak_N/0/1/0/all/0/1">Nattawut Phetmak</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaikaeo_C/0/1/0/all/0/1">Chaiporn Jaikaeo</a>, <a href="http://arxiv.org/find/cs/1/au:+Fakcharoenphol_J/0/1/0/all/0/1">Jittat Fakcharoenphol</a></p><p>This paper considers a movement minimization problem for mobile sensors.
Given a set of $n$ point targets, the $k$-Sink Minimum Movement Target Coverage
Problem is to schedule mobile sensors, initially located at $k$ base stations,
to cover all targets minimizing the total moving distance of the sensors. We
present a polynomial-time approximation scheme for finding a $(1+\epsilon)$
approximate solution running in time $n^{O(1/\epsilon)}$ for this problem when
$k$, the number of base stations, is constant. Our algorithm improves the
running time exponentially from the previous work that runs in time
$n^{O(1/\epsilon^2)}$, without any target distribution assumption. To devise a
faster algorithm, we prove a stronger bound on the number of sensors in any
unit area in the optimal solution and employ a more refined dynamic programming
algorithm whose complexity depends only on the width of the problem.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-09T00:30:00Z">Tuesday, May 09 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.03985'>Minimum-Membership Geometric Set Cover, Revisited</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sayan Bandyapadhyay, William Lochet, Saket Saurabh, Jie Xue</p><p>We revisit a natural variant of geometric set cover, called
minimum-membership geometric set cover (MMGSC). In this problem, the input
consists of a set $S$ of points and a set $\mathcal{R}$ of geometric objects,
and the goal is to find a subset $\mathcal{R}^*\subseteq\mathcal{R}$ to cover
all points in $S$ such that the \textit{membership} of $S$ with respect to
$\mathcal{R}^*$, denoted by $\mathsf{memb}(S,\mathcal{R}^*)$, is minimized,
where $\mathsf{memb}(S,\mathcal{R}^*)=\max_{p\in S}|\{R\in\mathcal{R}^*: p\in
R\}|$. We achieve the following two main results.
</p>
<p>* We give the first polynomial-time constant-approximation algorithm for
MMGSC with unit squares. This answers a question left open since the work of
Erlebach and Leeuwen [SODA'08], who gave a constant-approximation algorithm
with running time $n^{O(\mathsf{opt})}$ where $\mathsf{opt}$ is the optimum of
the problem (i.e., the minimum membership).
</p>
<p>* We give the first polynomial-time approximation scheme (PTAS) for MMGSC
with halfplanes. Prior to this work, it was even unknown whether the problem
can be approximated with a factor of $o(\log n)$ in polynomial time, while it
is well-known that the minimum-size set cover problem with halfplanes can be
solved in polynomial time.
</p>
<p>We also consider a problem closely related to MMGSC, called minimum-ply
geometric set cover (MPGSC), in which the goal is to find
$\mathcal{R}^*\subseteq\mathcal{R}$ to cover $S$ such that the ply of
$\mathcal{R}^*$ is minimized, where the ply is defined as the maximum number of
objects in $\mathcal{R}^*$ which have a nonempty common intersection. Very
recently, Durocher et al. gave the first constant-approximation algorithm for
MPGSC with unit squares which runs in $O(n^{12})$ time. We give a significantly
simpler constant-approximation algorithm with near-linear running time.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bandyapadhyay_S/0/1/0/all/0/1">Sayan Bandyapadhyay</a>, <a href="http://arxiv.org/find/cs/1/au:+Lochet_W/0/1/0/all/0/1">William Lochet</a>, <a href="http://arxiv.org/find/cs/1/au:+Saurabh_S/0/1/0/all/0/1">Saket Saurabh</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1">Jie Xue</a></p><p>We revisit a natural variant of geometric set cover, called
minimum-membership geometric set cover (MMGSC). In this problem, the input
consists of a set $S$ of points and a set $\mathcal{R}$ of geometric objects,
and the goal is to find a subset $\mathcal{R}^*\subseteq\mathcal{R}$ to cover
all points in $S$ such that the \textit{membership} of $S$ with respect to
$\mathcal{R}^*$, denoted by $\mathsf{memb}(S,\mathcal{R}^*)$, is minimized,
where $\mathsf{memb}(S,\mathcal{R}^*)=\max_{p\in S}|\{R\in\mathcal{R}^*: p\in
R\}|$. We achieve the following two main results.
</p>
<p>* We give the first polynomial-time constant-approximation algorithm for
MMGSC with unit squares. This answers a question left open since the work of
Erlebach and Leeuwen [SODA'08], who gave a constant-approximation algorithm
with running time $n^{O(\mathsf{opt})}$ where $\mathsf{opt}$ is the optimum of
the problem (i.e., the minimum membership).
</p>
<p>* We give the first polynomial-time approximation scheme (PTAS) for MMGSC
with halfplanes. Prior to this work, it was even unknown whether the problem
can be approximated with a factor of $o(\log n)$ in polynomial time, while it
is well-known that the minimum-size set cover problem with halfplanes can be
solved in polynomial time.
</p>
<p>We also consider a problem closely related to MMGSC, called minimum-ply
geometric set cover (MPGSC), in which the goal is to find
$\mathcal{R}^*\subseteq\mathcal{R}$ to cover $S$ such that the ply of
$\mathcal{R}^*$ is minimized, where the ply is defined as the maximum number of
objects in $\mathcal{R}^*$ which have a nonempty common intersection. Very
recently, Durocher et al. gave the first constant-approximation algorithm for
MPGSC with unit squares which runs in $O(n^{12})$ time. We give a significantly
simpler constant-approximation algorithm with near-linear running time.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-09T00:30:00Z">Tuesday, May 09 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Monday, May 08
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://blog.computationalcomplexity.org/2023/05/other-ramseys.html'>Other Ramsey's</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>&nbsp;I often Google Ramsey stuff to find something. I often end up back on my own collection of Ramsey theory&nbsp; papers. But I sometimes find OTHER uses of the phrase&nbsp; Ramsey.&nbsp;</p><p>To tell these Ramsey's apart we will call the math Ramsey guy Frank Ramsey since that was his name.&nbsp;</p><p>1) Frank's brother Arthur Michael Ramsey (usually just Michael Ramsey). He was the Archbishop of Canterbury from 1961 until 1974.&nbsp; &nbsp;He was ahead of his time in being Ecumenical and supporting women clergy. His Wikipedia page is&nbsp;here.&nbsp;</p><p>2) The Ramsey Effect. Aaron Ramsey is a football player (what Americans call Soccer). There were four time when he scored a goal and soon after an important person died. This was dubbed The Ramsey Effect. This is of course silly, and if he asked Frank about the probabilities (unlikely-Frank died in the 1930's and Aaron was born in 1990) I am sure Frank would tell you that four is too small a number to make anything out of this. The Ramsey Effect is discussed&nbsp;here. There is no Wikipedia page about it, and I found it by accident so, as the kids say, its NOT a think.&nbsp;</p><p>3) Jon Bennett Ramsey. A girl who was murdered when she was six. Case still unsolved. See the Wikipedia page on this&nbsp;here.</p><p>4) First name Ramsey:&nbsp;here. The only one I had heard of is Ramsey Clark.</p><p>5) Last name Ramsey:&nbsp;here. Lots of people! Most I had not heard of.&nbsp;</p><p>(With regard to 4 and 5: there are so many famous people you can't have heard of all of them)</p><p>6) For more Ramsey Stuff see&nbsp;here</p><p><br></p><p><br></p><p>By gasarch</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>&nbsp;I often Google Ramsey stuff to find something. I often end up back on my own collection of Ramsey theory&nbsp; papers. But I sometimes find OTHER uses of the phrase&nbsp; <i>Ramsey.&nbsp;</i></p><p>To tell these Ramsey's apart we will call the math Ramsey guy <i>Frank Ramsey </i>since that was his name.&nbsp;</p><p>1) Frank's brother Arthur Michael Ramsey (usually just Michael Ramsey). He was the Archbishop of Canterbury from 1961 until 1974.&nbsp; &nbsp;He was ahead of his time in being Ecumenical and supporting women clergy. His Wikipedia page is&nbsp;<a href="https://en.wikipedia.org/wiki/Michael_Ramsey">here</a>.&nbsp;</p><p>2) The Ramsey Effect. Aaron Ramsey is a football player (what Americans call Soccer). There were four time when he scored a goal and soon after an important person died. This was dubbed <i>The Ramsey Effect. </i>This is of course silly, and if he asked Frank about the probabilities (unlikely-Frank died in the 1930's and Aaron was born in 1990) I am sure Frank would tell you that four is too small a number to make anything out of this. The Ramsey Effect is discussed&nbsp;<a href="https://knowyourmeme.com/memes/the-ramsey-effect">here</a>. There is no Wikipedia page about it, and I found it by accident so, as the kids say, its NOT a think.&nbsp;</p><p>3) Jon Bennett Ramsey. A girl who was murdered when she was six. Case still unsolved. See the Wikipedia page on this&nbsp;<a href="https://en.wikipedia.org/wiki/Killing_of_JonBen%C3%A9t_Ramsey">here</a>.</p><p>4) First name Ramsey:&nbsp;<a href="https://en.wikipedia.org/wiki/Ramsey_(given_name)">here</a>. The only one I had heard of is Ramsey Clark.</p><p>5) Last name Ramsey:&nbsp;<a href="https://en.wikipedia.org/wiki/Ramsey_(surname)">here</a>. Lots of people! Most I had not heard of.&nbsp;</p><p>(With regard to 4 and 5: there are so many famous people you can't have heard of all of them)</p><p>6) For more Ramsey Stuff see&nbsp;<a href="https://en.wikipedia.org/wiki/Ramsey">here</a></p><p><br /></p><p><br /></p><p class="authors">By gasarch</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-08T23:06:00Z">Monday, May 08 2023, 23:06</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://ptreview.sublinear.info/2023/05/news-for-april-2023/'>News for April 2023</a></h3>
        <p class='tr-article-feed'>from <a href='https://ptreview.sublinear.info'>Property Testing Review</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          After an empty month, the engines of PTReview are roaring back to life with a fresh batch of papers for this month&#8217;s edition. In total, we have four papers that are sure to pique your interest. It&#8217;s been an action-packed month with a diverse range of topics covered in the featured papers. The first paper [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>After an empty month, the engines of PTReview are roaring back to life with a fresh batch of papers for this month&#8217;s edition. In total, we have four papers that are sure to pique your interest. It&#8217;s been an action-packed month with a diverse range of topics covered in the featured papers. The first paper delves into new variations in distribution testing, while the second paper discusses optimal testers for Bayes Nets. The third paper focuses on optimal tolerant junta-testers, and the final paper introduces a cool monotonicity tester over hypergrids.</p>



<p><strong>Distribution Testing Under the Parity Trace</strong> by Renato Ferreira Pinto Jr and Nathaniel Harms (<a href="https://arxiv.org/abs/2304.01374">arXiv</a>) The featured paper considers the classic setup in distribution testing with <em>a twist</em>. To explain the results, let me introduce the framework considered in this work. Consider distributions supported over \([n]\). Suppose I want to design distribution testers where instead of obtaining samples in the usual way, I first obtain an ordered list of samples, I store it in a sequence \(S\) and only the least significant bit of each element of \(S\) is made available to your distribution testing algorithm. This is called a parity trace.  Note that with this access model, suddenly a bunch of standard tasks become non-trivial. To take an example from the paper, you can no longer distinguish between the uniform distribution supported on \(\{1,2, \ldots, n\}\) and the uniform distribution supported on \(\{n+1, n+2, \ldots 2n\}\) in this access model. Nevertheless, the paper shows, you can still obtain testers which require only sublinear number of accesses for testing uniformity in this model.</p>



<p>Another cool feature of this <em>big</em> paper is an unexpected foray into the trace reconstruction literature from a property testing viewpoint. I wish I understood the formal connection better to describe a bit more about it. But for now, let me leave that as an appetizer which (hopefully) encourages you to take a look at the paper.</p>



<p></p>



<p><strong>New Lower Bounds for Adaptive Tolerant Junta Testing</strong> by Xi Chen and Shyamal Patel (<a href="https://arxiv.org/abs/2304.10647">arXiv</a>) If you are a regular here on the PTReview corner, you are probably no stranger to the <em>tolerant junta testing</em> problem. As a corollary to the main result, the paper in question proves a lower bound of \(k^{\Omega(\log k)}\) queries on any adaptive algorithm that wishes to test whether the input function \(f\) is \(\varepsilon_1\) close to being a \(k\)-junta or whether it is \(\varepsilon_2 \geq \varepsilon_1 + \displaystyle\frac{1}{poly(k)}\). Indeed, another remarkable achievement of the paper is that it achieves a superpolynomial separation between non-tolerant versions and the tolerant versions of any natural property of boolean functions under the adaptive setting.</p>



<p></p>



<p><strong>Near-Optimal Degree Testing for Bayes Nets</strong> by Vipul Arora, Arnab Bhattacharyya, Clément L. Canonne (our own!) and Joy Qiping Yang (<a href="https://arxiv.org/abs/2304.06733">arXiv</a>) This paper continues a line of investigation which a subset of the authors were a part of (which we also covered in our <a href="https://ptreview.sublinear.info/2022/05/news-for-april-2022/">News for April 2022</a>). Let us remind ourselves of the setup. You are given a probability distribution \(\mathcal{P}\) supported over the Boolean Hypercube. Suppose \(\mathcal{P}\) can be generated by a Bayseian Network. You may think of a Bayesian Network as a DAG where each vertex tosses a coin (with different heads probabilities). The question seeks to test whether \(\mathcal{P}\) admits a sparse Bayesian Net (in the sense of each vertex having small in-degree). The main result of the paper gives an algorithm for this task which requires \(\Theta(2^{n/2}/\varepsilon^2)\) samples. The paper also proves an almost matching lower bound.</p>



<p></p>



<p><strong>A \(d^{1/2+o(1)}\) Monotonicity Tester for Boolean Functions on \(d\)-Dimensional Hypergrids</strong> by Hadley Black, Deeparnab Chakrabarty and C. Seshadhri (again, our own!) (<a href="https://arxiv.org/abs/2304.01416">arXiv</a>) Again, the problem of monotonicity testing of boolean functions hardly requires any detailing to the regular readers of PTReview. As you can see in our <a href="https://ptreview.sublinear.info/2022/12/news-for-october-2022-2/">News from November 2022</a> there were two concurrent papers mulling over this problem over the hypergrid domain. The featured paper is the result of a dedicated pursuit by the authors and the key result is what the title says. Namely, you can test monotonicity with a number of (non-adaptive, one-sided) queries that has no dependence on \(n\).</p>
<p class="authors">By Akash</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-08T13:54:49Z">Monday, May 08 2023, 13:54</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://gilkalai.wordpress.com/2023/05/08/mathematics-mainly-combinatorics-related-matters-a-lot-of-activity/'>Mathematics (mainly combinatorics) related matters: A lot of activity.</a></h3>
        <p class='tr-article-feed'>from <a href='https://gilkalai.wordpress.com'>Gil Kalai</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Plan for next weeks blogging There are various things to blog about and let me give a quick preview for the plan for the next few posts. The purpose of this post is to give an impression about the hectic &#8230; Continue reading &#8594;
        
        </div>

        <div class='tr-article-summary'>
        
          
          <h3>Plan for next weeks blogging</h3>
<p>There are various things to blog about and let me give a quick preview for the plan for the next few posts. The purpose of this post is to give an impression about the hectic mathematical activities around here with special emphasis on combinatorics and early-in-the-week activities. There is so much action around that I feel tired just to write about it. Next post will be around <a href="https://youtu.be/Qz3q6jcknHE">Amnon Shashua&#8217;s lecture</a> at Reichman university giving a deep dive on LLMs. Following it I will tell you about developments around physics with special emphasis to the <a href="https://www.youtube.com/live/mg3gy4Et_xY?feature=share">evening at the Israeli Academy</a> celebrating Israel entrance to CERN. Fourth in line is a post about my<a href="https://arxiv.org/abs/2305.01064"> recent paper with Yosef Rinott and Tomer Shoham</a> on the Google 2019 quantum supremacy experiment (this is our third paper on the subject).</p>
<p>Let&#8217;s move on to today&#8217;s post which will include more than the usual dose of Hebrew.</p>
<h2>Avinoam</h2>
<p>A few weeks ago, Avinoam Mann, a dear member of our department in Jerusalem, passed away. Avinoam was a famous group theorist working on many aspects of this theory. I have many warm memories of Avinoam since I was a student when I took with Avinoam two very demanding reading courses, and later as colleagues and friends for many decades. Avinoam was also a poet and here is a poem he wrote.</p>
<p><a href="https://gilkalai.files.wordpress.com/2023/05/avinoam.jpg"><img data-attachment-id="24262" data-permalink="https://gilkalai.wordpress.com/2023/05/08/mathematics-mainly-combinatorics-related-matters-a-lot-of-activity/avinoam/" data-orig-file="https://gilkalai.files.wordpress.com/2023/05/avinoam.jpg" data-orig-size="1036,723" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;SM-G986B&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1680451480&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;5.4&quot;,&quot;iso&quot;:&quot;500&quot;,&quot;shutter_speed&quot;:&quot;0.02&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="Avinoam" data-image-description="" data-image-caption="" data-medium-file="https://gilkalai.files.wordpress.com/2023/05/avinoam.jpg?w=300" data-large-file="https://gilkalai.files.wordpress.com/2023/05/avinoam.jpg?w=640" class="alignnone size-full wp-image-24262" src="https://gilkalai.files.wordpress.com/2023/05/avinoam.jpg?w=640" alt="Avinoam" srcset="https://gilkalai.files.wordpress.com/2023/05/avinoam.jpg?w=640 640w, https://gilkalai.files.wordpress.com/2023/05/avinoam.jpg?w=150 150w, https://gilkalai.files.wordpress.com/2023/05/avinoam.jpg?w=300 300w, https://gilkalai.files.wordpress.com/2023/05/avinoam.jpg?w=768 768w, https://gilkalai.files.wordpress.com/2023/05/avinoam.jpg?w=1024 1024w, https://gilkalai.files.wordpress.com/2023/05/avinoam.jpg 1036w" sizes="(max-width: 640px) 100vw, 640px"   /></a></p>
<h2>Many many many Seminars</h2>
<h3>HUJI Combinatorics  seminars</h3>
<p>The seminar takes place on Mondays between 11-13. The 2-hour format allows ample discussions. There were brilliant talks at the HUJI (Hebrew University of Jerusalem) Combinatorics Seminar. The last four were given by  Illay Hoshen, Yuval Filmus, Igor Balla, and Nathan Keller. <strong>Ilay Hoshen</strong> spoke about his paper with <strong>Wojtek Samotij</strong> on Simonovits&#8217;s theorem for random graphs, and presented a (partial) resolution to a conjecture by DeMarco and Kahn.  <strong>Yuval Filmus</strong> talked about a joint work with <strong>Nathan Lindzey</strong>, where the starting point was Fourier expansion for function on the Boolean cube  and asked what happens if we study functions on other domains, such as the &#8220;slice&#8221; or the symmetric group? (very elegant connection with representation theory.) Once it&#8217;s on the arxive we will add the link. <strong>Igor Balla</strong> talked about <span style="color: initial">Equiangular lines via matrix projection. This work presents a definite progress on the classical problem of equiangular lines as well as some connections to problems in quantum information theory.  <strong>Nathan Keller</strong> talked about <a href="https://arxiv.org/abs/2303.15755">his joint work</a> with </span><strong>Noam Lifshitz, Dor Minzer</strong>, and <strong>Ohad Sheinfeld. </strong>Hypercontractivity for global functions is used for far reaching Erdos-Ko-Rado theorems for permutations. Nati Linial wrote to me about the lecture: <span style="color: #ff0000"><strong>מצויינת, מדוייקת, אינפורמטיבית, א-מחיה</strong></span> . (which roughly translates to: “Outstanding, Accurate, Informative – Oh the Joy!”). Today <b></b><strong>Amir Yehudayoff</strong> will talk about <a href="https://arxiv.org/abs/2304.05456">his work</a> with <strong>Dan Carmon</strong> on dual systolic graphs.</p>
<p>This is not the only HUJI combinatorics seminar, on Thursday afternoon we have a joint Jerusalem-Copenhagen combinatorics seminar and at noon we have a joint Jerusalem Copenhagen lunch seminar. I gave a lecture in the lunch seminar a couple of weeks ago about challenges in the combinatorial theory of convex polytopes and spheres beyond the <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="g" class="latex" />-theorem. Of course, our CS-theory Wednesday seminars have plenty of lectures of combinatorial flavour.</p>
<h3>Shachar Lovett&#8217;s lecture at Tel Aviv Combinatorics Seminar</h3>
<p>Things in TAU (Tel Aviv University) are not calmer.</p>
<p>TAU combinatorics seminar is on Sundays 10:05-11:05, and last  week (April 30) the legendary <strong>Shachar Lovett</strong> gave a talk about his paper with <strong>Alexander Knop</strong>, <strong>Sam McGuire</strong>, and <strong>Weiqiang Yuan </strong>about <a title="Structure of monomials of Boolean functions" href="https://gilkalai.files.wordpress.com/2023/05/structure-of-monomials-of-boolean-functions.pptx">Structure of monomials of Boolean functions.</a> (Click for the slides.)</p>
<p>The main theorem is the following one. <strong>Theorem:</strong> Let <img src="https://s0.wp.com/latex.php?latex=f%3A%5C%7B0%2C1%5C%7D%5En+%5Cto+%5C%7B0%2C1%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=f%3A%5C%7B0%2C1%5C%7D%5En+%5Cto+%5C%7B0%2C1%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=f%3A%5C%7B0%2C1%5C%7D%5En+%5Cto+%5C%7B0%2C1%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="f:&#92;{0,1&#92;}^n &#92;to &#92;{0,1&#92;}" class="latex" /> be a Boolean function with <img src="https://s0.wp.com/latex.php?latex=%7CM%28f%29%7C%3Dm&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7CM%28f%29%7C%3Dm&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7CM%28f%29%7C%3Dm&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="|M(f)|=m" class="latex" />. (<img src="https://s0.wp.com/latex.php?latex=M%28f%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=M%28f%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=M%28f%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="M(f)" class="latex" /> is the number of monomials in the presentation of <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="f" class="latex" />) Then f can be computed by an AND decision tree of depth <img src="https://s0.wp.com/latex.php?latex=d%3DO%28%5Clog+%5E5m%5Clog+n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=d%3DO%28%5Clog+%5E5m%5Clog+n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=d%3DO%28%5Clog+%5E5m%5Clog+n%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="d=O(&#92;log ^5m&#92;log n)" class="latex" />.</p>
<p>The proof uses an auxiliary (sharp) result about hitting sets (aka transversals) for monomials. It is not known if the <img src="https://s0.wp.com/latex.php?latex=%5Clog+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Clog+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clog+n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;log n" class="latex" /> factor in the main theorem is needed. Shachar described exciting connections with the log-rank conjecture and with Frankl&#8217;s union-closed conjecture. He also described the analogous (Fourier) question for functions  <img src="https://s0.wp.com/latex.php?latex=f%3A%5C%7B-1%2C1%5C%7D%5En+%5Cto+%5C%7B0%2C1%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=f%3A%5C%7B-1%2C1%5C%7D%5En+%5Cto+%5C%7B0%2C1%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=f%3A%5C%7B-1%2C1%5C%7D%5En+%5Cto+%5C%7B0%2C1%5C%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="f:&#92;{-1,1&#92;}^n &#92;to &#92;{0,1&#92;}" class="latex" />.</p>
<p>After the lecture Shachar told me about some basic details of the recent amazing proof of Kelly and Meka for sharp bound for Roth&#8217;s theorems. Shachar promised me that the crucial new ideas giving a Fourier proof for similar bounds for the cup set problem could be presented in four to six hours. (He started with two hours but I flatly disbelieved it.) We also came back to the idea of a polymath project devoted to Frankl&#8217;s conjecture.</p>
<p>This is not the only TAU combinatorics seminar.  Two days later, on May 2 <strong>Patrick Morris</strong> gave a special seminar about <a href="https://arxiv.org/abs/2209.01116"> a robust Corrádi–Hajnal Theorem</a> (joint work with <strong>Peter Allen, Julia Böttcher, Jan Corsten, Ewan Davies, Matthew Jenssen, Barnaby Roberts</strong> and <strong>Jozef Skokan</strong>.) As far as I know there are plans to have special seminars dedicated to both the new ultimate Roth bounds and the Ramsey breakthrough.</p>
<p>Of course, there is also the weekly discrete and computational geometry seminar, and a few weeks ago I gave a talk about &#8220;covering problems&#8221; which was well accepted and is in line with this year&#8217;s theme for the seminar.</p>
<h3>More combinatorics seminars</h3>
<p>There are many other combinatorics seminars around. If you have an urge for combinatorics lectures between the Tel Aviv seminar and the one in Jerusalem,  on Sundays at 2 o&#8217;clock there is the Bar Ilan weekly Combinatorics Seminar, and on May 7  <b>Yelena Yuditsky </b>talked about <a href="https://arxiv.org/abs/2207.01041">Conflict-free colouring of subsets</a> (joint work with <strong>Bruno Jartoux, Chaya Keller</strong> and <strong>Shakhar Smorodinsky</strong>.) On Mondays Martin Golumbic runs the seminar: &#8220;Monday with Marty and Students of Sunil&#8221; devoted to algorithmic graph theory.  Tomorrow, <strong>Pradeesha Ashok</strong> talks about: Exact and Parameterised algorithms for Graph Burning (joint work with <strong>Avi Tomar, Shaily Verma, Sayani Das, Lawqueen Kanesh</strong> and <strong>Saket Saurabh</strong>).</p>
<h3>Shmuel Weinberger&#8217;s lectures</h3>
<p>Of course, things are just as amazingly intense in other fields of mathematics as well. Last week I attended two great talks by <strong>Shmuel Weinberger</strong>. The first talk gave the answer to the question which groups act without fixed points on some aspherical topological space. Shmuel said that his talk will be structured like a Tarantino&#8217;s movie, and at the end he expressed hope that the talk was as entertaining but not as violent.  This is based on joint work with <strong>Sylvain Cappell,</strong> and <strong>Min Yan.</strong> The second talk gave (among other things) a lower bound for the number of vertices needed to triangulate <img src="https://s0.wp.com/latex.php?latex=%282d-1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%282d-1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%282d-1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="(2d-1)" class="latex" />-dimensional lens spaces which is the quotient space of an action of <img src="https://s0.wp.com/latex.php?latex=Z_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=Z_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=Z_n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="Z_n" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+C%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb+C%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+C%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathbb C^d" class="latex" />. The proof goes via the notions of <img src="https://s0.wp.com/latex.php?latex=%5Cell_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cell_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cell_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;ell_2" class="latex" />-homology and certain invariants of Cheeger and Gromov and it would be really nice to have some simpler proofs. (This is based on an old work with <strong>Stanley Chang</strong>, and a new work with <strong>Geunho Lim</strong>.</p>
<p><iframe class="youtube-player" width="640" height="360" src="https://www.youtube.com/embed/qwojzqAcHqw?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></p>
<p><span style="color: #ff0000">Here is a lecture on calculus on extraordinary spaces by Yael Karshon (Hebrew).</span></p>
<h2>Kazhdan&#8217;s Sunday seminars &#8211; a plan for fall 2024 (maybe 2023).</h2>
<p>Since David Kazhdan moved from Harvard to HUJI he is running four-five semester-long seminars every  year on Sundays, and a basic notion seminar on Thursday afternoons. This semester, for example, in one of the seminars, Udi de Shalit presents Wiles&#8217; proof of Fermat&#8217;s last theorem (taking Ribet&#8217;s part for granted).</p>
<p>Once every decade or so, I serve as a co-teacher in a Kazhdan seminar. In fall 2003 David Kazhdan and I ran a <a href="http://www.ma.huji.ac.il/~kalai/polytopes.html">seminar on polytopes</a>, toric varieties, and related combinatorics and algebra.  In 2013 David and I felt that it was time to run another such event in 2014, perhaps establishing a tradition for a decennial joint seminar.  I announced this coming event in my <a href="https://gilkalai.wordpress.com/2013/01/01/symplectic-geometry-quantization-and-quantum-noise/">January 2013 post</a> and wrote: &#8220;So next spring, the plan is &#8230;[to] devote one of David’s Sunday seminars to computation, quantumness, symplectic geometry, and information.&#8221; Alas, David had a terrible car accident and we had to delay the plan to <a href="https://gilkalai.wordpress.com/2019/10/27/starting-today-kazhdan-sunday-seminar-computation-quantumness-symplectic-geometry-and-information/">a fall 2019 seminar</a> that Leonid Polterovich, Dorit Aharonov, Guy Kindler and I ran.  Also, in fall 2018, Karim Adiprasito gave a Kazhdan seminar on  &#8220;Positivity in combinatorics and beyond&#8221; where Karim presented his proof for the g-conjecture. We are now planning a  Kazhdan seminar in fall 2024 around &#8220;global hypercontractivity&#8221; with Noam Lifshitz, myself and perhaps also Guy Kindler and others. (Kazhdan&#8217;s 2023/2024 schedule was fully booked, but come to think of it, since Dor Minzer is in town in fall 2023, maybe we will do something then.)</p>
<h2>60th birthday conferences for the young: Gunter Ziegler and Leonid Polterovich</h2>
<p>Noga Alon recently complained that &#8220;younger and younger people are celebrating their 60th birthdays&#8221;. Indeed, two weeks from now there will be a <a href="http://ehrhart.math.fu-berlin.de/gmz60/#arnau">day-and-a half workshop</a> in Berlin celebrating Günter Ziegler&#8217;s birthday and in June there will be a <a href="https://math.ethz.ch/fim/activities/conferences/lp60-geometry-and-dynamics-polterovich.html">Leonid Polterovich fest</a> in Zurich. Happy birthdays, kids!</p>
<h2>Maybe it is over</h2>
<p>A well-known Israeli poet and writer Yonatan Gefen passed away recently and here is a nice song he wrote (performed by Arik Einstein): <a href="https://youtu.be/R384MOGq7HM">Yhachol lihyot sheze nigmar</a>  (It is possible that it is over.)</p>
<p><iframe class="youtube-player" width="640" height="360" src="https://www.youtube.com/embed/R384MOGq7HM?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent" allowfullscreen="true" style="border:0;" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></p>
<p class="authors">By Gil Kalai</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-08T06:50:10Z">Monday, May 08 2023, 06:50</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.03439'>Degrees of Second and Higher-Order Polynomials</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Donghyun Lim, Martin Ziegler</p><p>Second-order polynomials generalize classical first-order ones in allowing
for additional variables that range over functions rather than values. We are
motivated by their applications in higher-order computational complexity
theory, extending for example classical classes like P or PSPACE to operators
in Analysis [doi:10.1137/S0097539794263452, doi:10.1145/2189778.2189780]. The
degree subclassifies ordinary polynomial growth into linear, quadratic, cubic
etc. In order to similarly classify second-order polynomials, define their
degree to be an 'arctic' first-order polynomial (namely a term/expression over
variable $D$ and operations $+$ and $\cdot$ and $\max$). Our normal form and
semantic uniqueness results for second-order polynomials assert said
second-order degree to be well-defined; and it turns out to transform well
under (now two kinds of) polynomial composition. More generally we define the
degree of a third-order polynomial to be an arctic second-order polynomial, and
establish its transformation under three kinds of composition.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lim_D/0/1/0/all/0/1">Donghyun Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziegler_M/0/1/0/all/0/1">Martin Ziegler</a></p><p>Second-order polynomials generalize classical first-order ones in allowing
for additional variables that range over functions rather than values. We are
motivated by their applications in higher-order computational complexity
theory, extending for example classical classes like P or PSPACE to operators
in Analysis [doi:10.1137/S0097539794263452, doi:10.1145/218<a href="/abs/9778.21897">9778.21897</a>80]. The
degree subclassifies ordinary polynomial growth into linear, quadratic, cubic
etc. In order to similarly classify second-order polynomials, define their
degree to be an 'arctic' first-order polynomial (namely a term/expression over
variable $D$ and operations $+$ and $\cdot$ and $\max$). Our normal form and
semantic uniqueness results for second-order polynomials assert said
second-order degree to be well-defined; and it turns out to transform well
under (now two kinds of) polynomial composition. More generally we define the
degree of a third-order polynomial to be an arctic second-order polynomial, and
establish its transformation under three kinds of composition.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-08T00:30:00Z">Monday, May 08 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.03180'>On Range Summary Queries</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Peyman Afshani, Pingan Cheng, Aniket Basu Roy, Zhewei Wei</p><p>We study the query version of the approximate heavy hitter and quantile
problems. In the former problem, the input is a parameter $\varepsilon$ and a
set $P$ of $n$ points in $\mathbb{R}^d$ where each point is assigned a color
from a set $C$, and we want to build a structure s.t. given any geometric range
$\gamma$, we can efficiently find a list of approximate heavy hitters in
$\gamma\cap P$, i.e., colors that appear at least $\varepsilon |\gamma \cap P|$
times in $\gamma \cap P$, as well as their frequencies with an additive error
of $\varepsilon |\gamma \cap P|$. In the latter problem, each point is assigned
a weight from a totally ordered universe and the query must output a sequence
$S$ of $1+1/\varepsilon$ weights s.t. the $i$-th weight in $S$ has approximate
rank $i\varepsilon|\gamma\cap P|$, meaning, rank $i\varepsilon|\gamma\cap P|$
up to an additive error of $\varepsilon|\gamma\cap P|$. Previously, optimal
results were only known in 1D [WY11] but a few sub-optimal methods were
available in higher dimensions [AW17, ACH+12].
</p>
<p>We study the problems for 3D halfspace and dominance queries. We consider the
real RAM model with integer registers of size $w=\Theta(\log n)$ bits. For
dominance queries, we show optimal solutions for both heavy hitter and quantile
problems: using linear space, we can answer both queries in time $O(\log n +
1/\varepsilon)$. Note that as the output size is $\frac{1}{\varepsilon}$, after
investing the initial $O(\log n)$ searching time, our structure takes on
average $O(1)$ time to find a heavy hitter or a quantile! For more general
halfspace heavy hitter queries, the same optimal query time can be achieved by
increasing the space by an extra $\log_w\frac{1}{\varepsilon}$ (resp.
$\log\log_w\frac{1}{\varepsilon}$) factor in 3D (resp. 2D). By spending extra
$\log^{O(1)}\frac{1}{\varepsilon}$ factors in time and space, we can also
support quantile queries.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Afshani_P/0/1/0/all/0/1">Peyman Afshani</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1">Pingan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1">Aniket Basu Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1">Zhewei Wei</a></p><p>We study the query version of the approximate heavy hitter and quantile
problems. In the former problem, the input is a parameter $\varepsilon$ and a
set $P$ of $n$ points in $\mathbb{R}^d$ where each point is assigned a color
from a set $C$, and we want to build a structure s.t. given any geometric range
$\gamma$, we can efficiently find a list of approximate heavy hitters in
$\gamma\cap P$, i.e., colors that appear at least $\varepsilon |\gamma \cap P|$
times in $\gamma \cap P$, as well as their frequencies with an additive error
of $\varepsilon |\gamma \cap P|$. In the latter problem, each point is assigned
a weight from a totally ordered universe and the query must output a sequence
$S$ of $1+1/\varepsilon$ weights s.t. the $i$-th weight in $S$ has approximate
rank $i\varepsilon|\gamma\cap P|$, meaning, rank $i\varepsilon|\gamma\cap P|$
up to an additive error of $\varepsilon|\gamma\cap P|$. Previously, optimal
results were only known in 1D [WY11] but a few sub-optimal methods were
available in higher dimensions [AW17, ACH+12].
</p>
<p>We study the problems for 3D halfspace and dominance queries. We consider the
real RAM model with integer registers of size $w=\Theta(\log n)$ bits. For
dominance queries, we show optimal solutions for both heavy hitter and quantile
problems: using linear space, we can answer both queries in time $O(\log n +
1/\varepsilon)$. Note that as the output size is $\frac{1}{\varepsilon}$, after
investing the initial $O(\log n)$ searching time, our structure takes on
average $O(1)$ time to find a heavy hitter or a quantile! For more general
halfspace heavy hitter queries, the same optimal query time can be achieved by
increasing the space by an extra $\log_w\frac{1}{\varepsilon}$ (resp.
$\log\log_w\frac{1}{\varepsilon}$) factor in 3D (resp. 2D). By spending extra
$\log^{O(1)}\frac{1}{\varepsilon}$ factors in time and space, we can also
support quantile queries.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-08T00:30:00Z">Monday, May 08 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.03609'>Differentially Private Topological Data Analysis</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Taegyu Kang, Sehwan Kim, Jinwon Sohn, Jordan Awan</p><p>This paper is the first to attempt differentially private (DP) topological
data analysis (TDA), producing near-optimal private persistence diagrams. We
analyze the sensitivity of persistence diagrams in terms of the bottleneck
distance, and we show that the commonly used \v{C}ech complex has sensitivity
that does not decrease as the sample size $n$ increases. This makes it
challenging for the persistence diagrams of \v{C}ech complexes to be
privatized. As an alternative, we show that the persistence diagram obtained by
the $L^1$-distance to measure (DTM) has sensitivity $O(1/n)$. Based on the
sensitivity analysis, we propose using the exponential mechanism whose utility
function is defined in terms of the bottleneck distance of the $L^1$-DTM
persistence diagrams. We also derive upper and lower bounds of the accuracy of
our privacy mechanism; the obtained bounds indicate that the privacy error of
our mechanism is near-optimal. We demonstrate the performance of our privatized
persistence diagrams through simulations as well as on a real dataset tracking
human movement.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/stat/1/au:+Kang_T/0/1/0/all/0/1">Taegyu Kang</a>, <a href="http://arxiv.org/find/stat/1/au:+Kim_S/0/1/0/all/0/1">Sehwan Kim</a>, <a href="http://arxiv.org/find/stat/1/au:+Sohn_J/0/1/0/all/0/1">Jinwon Sohn</a>, <a href="http://arxiv.org/find/stat/1/au:+Awan_J/0/1/0/all/0/1">Jordan Awan</a></p><p>This paper is the first to attempt differentially private (DP) topological
data analysis (TDA), producing near-optimal private persistence diagrams. We
analyze the sensitivity of persistence diagrams in terms of the bottleneck
distance, and we show that the commonly used \v{C}ech complex has sensitivity
that does not decrease as the sample size $n$ increases. This makes it
challenging for the persistence diagrams of \v{C}ech complexes to be
privatized. As an alternative, we show that the persistence diagram obtained by
the $L^1$-distance to measure (DTM) has sensitivity $O(1/n)$. Based on the
sensitivity analysis, we propose using the exponential mechanism whose utility
function is defined in terms of the bottleneck distance of the $L^1$-DTM
persistence diagrams. We also derive upper and lower bounds of the accuracy of
our privacy mechanism; the obtained bounds indicate that the privacy error of
our mechanism is near-optimal. We demonstrate the performance of our privatized
persistence diagrams through simulations as well as on a real dataset tracking
human movement.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-08T00:30:00Z">Monday, May 08 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.03146'>Testing Convex Truncation</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Anindya De, Shivam Nadimpalli, Rocco A. Servedio</p><p>We study the basic statistical problem of testing whether normally
distributed $n$-dimensional data has been truncated, i.e. altered by only
retaining points that lie in some unknown truncation set $S \subseteq
\mathbb{R}^n$. As our main algorithmic results,
</p>
<p>(1) We give a computationally efficient $O(n)$-sample algorithm that can
distinguish the standard normal distribution $N(0,I_n)$ from $N(0,I_n)$
conditioned on an unknown and arbitrary convex set $S$.
</p>
<p>(2) We give a different computationally efficient $O(n)$-sample algorithm
that can distinguish $N(0,I_n)$ from $N(0,I_n)$ conditioned on an unknown and
arbitrary mixture of symmetric convex sets.
</p>
<p>These results stand in sharp contrast with known results for learning or
testing convex bodies with respect to the normal distribution or learning
convex-truncated normal distributions, where state-of-the-art algorithms
require essentially $n^{\sqrt{n}}$ samples. An easy argument shows that no
finite number of samples suffices to distinguish $N(0,I_n)$ from an unknown and
arbitrary mixture of general (not necessarily symmetric) convex sets, so no
common generalization of results (1) and (2) above is possible.
</p>
<p>We also prove that any algorithm (computationally efficient or otherwise)
that can distinguish $N(0,I_n)$ from $N(0,I_n)$ conditioned on an unknown
symmetric convex set must use $\Omega(n)$ samples. This shows that the sample
complexity of each of our algorithms is optimal up to a constant factor.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1">Anindya De</a>, <a href="http://arxiv.org/find/cs/1/au:+Nadimpalli_S/0/1/0/all/0/1">Shivam Nadimpalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Servedio_R/0/1/0/all/0/1">Rocco A. Servedio</a></p><p>We study the basic statistical problem of testing whether normally
distributed $n$-dimensional data has been truncated, i.e. altered by only
retaining points that lie in some unknown truncation set $S \subseteq
\mathbb{R}^n$. As our main algorithmic results,
</p>
<p>(1) We give a computationally efficient $O(n)$-sample algorithm that can
distinguish the standard normal distribution $N(0,I_n)$ from $N(0,I_n)$
conditioned on an unknown and arbitrary convex set $S$.
</p>
<p>(2) We give a different computationally efficient $O(n)$-sample algorithm
that can distinguish $N(0,I_n)$ from $N(0,I_n)$ conditioned on an unknown and
arbitrary mixture of symmetric convex sets.
</p>
<p>These results stand in sharp contrast with known results for learning or
testing convex bodies with respect to the normal distribution or learning
convex-truncated normal distributions, where state-of-the-art algorithms
require essentially $n^{\sqrt{n}}$ samples. An easy argument shows that no
finite number of samples suffices to distinguish $N(0,I_n)$ from an unknown and
arbitrary mixture of general (not necessarily symmetric) convex sets, so no
common generalization of results (1) and (2) above is possible.
</p>
<p>We also prove that any algorithm (computationally efficient or otherwise)
that can distinguish $N(0,I_n)$ from $N(0,I_n)$ conditioned on an unknown
symmetric convex set must use $\Omega(n)$ samples. This shows that the sample
complexity of each of our algorithms is optimal up to a constant factor.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-08T00:30:00Z">Monday, May 08 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.03307'>On Optimization and Counting of Non-Broken Bases of Matroids</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Dorna Abdolazimi, Kasper Lindberg, Shayan Oveis Gharan</p><p>Given a matroid $M=(E,{\cal I})$, and a total ordering over the elements $E$,
a broken circuit is a circuit where the smallest element is removed and an NBC
independent set is an independent set in ${\cal I}$ with no broken circuit. The
set of NBC independent sets of any matroid $M$ define a simplicial complex
called the broken circuit complex which has been the subject of intense study
in combinatorics. Recently, Adiprasito, Huh and Katz showed that the face of
numbers of any broken circuit complex form a log-concave sequence, proving a
long-standing conjecture of Rota.
</p>
<p>We study counting and optimization problems on NBC bases of a generic
matroid. We find several fundamental differences with the independent set
complex: for example, we show that it is NP-hard to find the max-weight NBC
base of a matroid or that the convex hull of NBC bases of a matroid has edges
of arbitrary large length. We also give evidence that the natural down-up walk
on the space of NBC bases of a matroid may not mix rapidly by showing that for
some family of matroids it is NP-hard to count the number of NBC bases after
certain conditionings.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Abdolazimi_D/0/1/0/all/0/1">Dorna Abdolazimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lindberg_K/0/1/0/all/0/1">Kasper Lindberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Gharan_S/0/1/0/all/0/1">Shayan Oveis Gharan</a></p><p>Given a matroid $M=(E,{\cal I})$, and a total ordering over the elements $E$,
a broken circuit is a circuit where the smallest element is removed and an NBC
independent set is an independent set in ${\cal I}$ with no broken circuit. The
set of NBC independent sets of any matroid $M$ define a simplicial complex
called the broken circuit complex which has been the subject of intense study
in combinatorics. Recently, Adiprasito, Huh and Katz showed that the face of
numbers of any broken circuit complex form a log-concave sequence, proving a
long-standing conjecture of Rota.
</p>
<p>We study counting and optimization problems on NBC bases of a generic
matroid. We find several fundamental differences with the independent set
complex: for example, we show that it is NP-hard to find the max-weight NBC
base of a matroid or that the convex hull of NBC bases of a matroid has edges
of arbitrary large length. We also give evidence that the natural down-up walk
on the space of NBC bases of a matroid may not mix rapidly by showing that for
some family of matroids it is NP-hard to count the number of NBC bases after
certain conditionings.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-08T00:30:00Z">Monday, May 08 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.03110'>A Sparse Johnson-Lindenstrauss Transform using Fast Hashing</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jakob B&#xe6;k Tejs Houen, Mikkel Thorup</p><p>The \emph{Sparse Johnson-Lindenstrauss Transform} of Kane and Nelson (SODA
2012) provides a linear dimensionality-reducing map $A \in \mathbb{R}^{m \times
u}$ in $\ell_2$ that preserves distances up to distortion of $1 + \varepsilon$
with probability $1 - \delta$, where $m = O(\varepsilon^{-2} \log 1/\delta)$
and each column of $A$ has $O(\varepsilon m)$ non-zero entries. The previous
analyses of the Sparse Johnson-Lindenstrauss Transform all assumed access to a
$\Omega(\log 1/\delta)$-wise independent hash function. The main contribution
of this paper is a more general analysis of the Sparse Johnson-Lindenstrauss
Transform with less assumptions on the hash function. We also show that the
\emph{Mixed Tabulation hash function} of Dahlgaard, Knudsen, Rotenberg, and
Thorup (FOCS 2015) satisfies the conditions of our analysis, thus giving us the
first analysis of a Sparse Johnson-Lindenstrauss Transform that works with a
practical hash function.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Houen_J/0/1/0/all/0/1">Jakob B&#xe6;k Tejs Houen</a>, <a href="http://arxiv.org/find/cs/1/au:+Thorup_M/0/1/0/all/0/1">Mikkel Thorup</a></p><p>The \emph{Sparse Johnson-Lindenstrauss Transform} of Kane and Nelson (SODA
2012) provides a linear dimensionality-reducing map $A \in \mathbb{R}^{m \times
u}$ in $\ell_2$ that preserves distances up to distortion of $1 + \varepsilon$
with probability $1 - \delta$, where $m = O(\varepsilon^{-2} \log 1/\delta)$
and each column of $A$ has $O(\varepsilon m)$ non-zero entries. The previous
analyses of the Sparse Johnson-Lindenstrauss Transform all assumed access to a
$\Omega(\log 1/\delta)$-wise independent hash function. The main contribution
of this paper is a more general analysis of the Sparse Johnson-Lindenstrauss
Transform with less assumptions on the hash function. We also show that the
\emph{Mixed Tabulation hash function} of Dahlgaard, Knudsen, Rotenberg, and
Thorup (FOCS 2015) satisfies the conditions of our analysis, thus giving us the
first analysis of a Sparse Johnson-Lindenstrauss Transform that works with a
practical hash function.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-08T00:30:00Z">Monday, May 08 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.03194'>Testing Convexity of Discrete Sets in High Dimensions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Hadley Black, Eric Blais, Nathaniel Harms</p><p>We study the problem of testing whether an unknown set $S$ in $n$ dimensions
is convex or far from convex, using membership queries. The simplest
high-dimensional discrete domain where the problem of testing convexity is
non-trivial is the domain $\{-1,0,1\}^n$. Our main results are nearly-tight
upper and lower bounds of $3^{\widetilde \Theta( \sqrt n)}$ for one-sided error
testing of convex sets over this domain with non-adaptive queries. Together
with our $3^{\Omega(n)}$ lower bound on one-sided error testing with samples,
this shows that non-adaptive queries are significantly more powerful than
samples for this problem.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Black_H/0/1/0/all/0/1">Hadley Black</a>, <a href="http://arxiv.org/find/cs/1/au:+Blais_E/0/1/0/all/0/1">Eric Blais</a>, <a href="http://arxiv.org/find/cs/1/au:+Harms_N/0/1/0/all/0/1">Nathaniel Harms</a></p><p>We study the problem of testing whether an unknown set $S$ in $n$ dimensions
is convex or far from convex, using membership queries. The simplest
high-dimensional discrete domain where the problem of testing convexity is
non-trivial is the domain $\{-1,0,1\}^n$. Our main results are nearly-tight
upper and lower bounds of $3^{\widetilde \Theta( \sqrt n)}$ for one-sided error
testing of convex sets over this domain with non-adaptive queries. Together
with our $3^{\Omega(n)}$ lower bound on one-sided error testing with samples,
this shows that non-adaptive queries are significantly more powerful than
samples for this problem.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-08T00:30:00Z">Monday, May 08 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.03240'>Sum-of-Local-Effects Data Structures for Separable Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xing Lyu, Travis Gagie, Meng He, Yakov Nekrich, Norbert Zeh</p><p>It is not difficult to think of applications that can be modelled as graph
problems in which placing some facility or commodity at a vertex has some
positive or negative effect on the values of all the vertices out to some
distance, and we want to be able to calculate quickly the cumulative effect on
any vertex's value at any time or the list of the most beneficial or most
detrimential effects on a vertex. In this paper we show how, given an
edge-weighted graph with constant-size separators, we can support the following
operations on it in time polylogarithmic in the number of vertices and the
number of facilities placed on the vertices, where distances between vertices
are measured with respect to the edge weights:
</p>
<p>Add (v, f, w, d) places a facility of weight w and with effect radius d onto
vertex v.
</p>
<p>Remove (v, f) removes a facility f previously placed on v using Add from v.
</p>
<p>Sum (v) or Sum (v, d) returns the total weight of all facilities affecting v
or, with a distance parameter d, the total weight of all facilities whose
effect region intersects the ``circle'' with radius d around v.
</p>
<p>Top (v, k) or Top (v, k, d) returns the k facilities of greatest weight that
affect v or, with a distance parameter d, whose effect region intersects the
``circle'' with radius d around v.
</p>
<p>The weights of the facilities and the operation that Sum uses to ``sum'' them
must form a semigroup. For Top queries, the weights must be drawn from a total
order.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1">Xing Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gagie_T/0/1/0/all/0/1">Travis Gagie</a>, <a href="http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1">Meng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Nekrich_Y/0/1/0/all/0/1">Yakov Nekrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeh_N/0/1/0/all/0/1">Norbert Zeh</a></p><p>It is not difficult to think of applications that can be modelled as graph
problems in which placing some facility or commodity at a vertex has some
positive or negative effect on the values of all the vertices out to some
distance, and we want to be able to calculate quickly the cumulative effect on
any vertex's value at any time or the list of the most beneficial or most
detrimential effects on a vertex. In this paper we show how, given an
edge-weighted graph with constant-size separators, we can support the following
operations on it in time polylogarithmic in the number of vertices and the
number of facilities placed on the vertices, where distances between vertices
are measured with respect to the edge weights:
</p>
<p>Add (v, f, w, d) places a facility of weight w and with effect radius d onto
vertex v.
</p>
<p>Remove (v, f) removes a facility f previously placed on v using Add from v.
</p>
<p>Sum (v) or Sum (v, d) returns the total weight of all facilities affecting v
or, with a distance parameter d, the total weight of all facilities whose
effect region intersects the ``circle'' with radius d around v.
</p>
<p>Top (v, k) or Top (v, k, d) returns the k facilities of greatest weight that
affect v or, with a distance parameter d, whose effect region intersects the
``circle'' with radius d around v.
</p>
<p>The weights of the facilities and the operation that Sum uses to ``sum'' them
must form a semigroup. For Top queries, the weights must be drawn from a total
order.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-08T00:30:00Z">Monday, May 08 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.03381'>Tighter Approximation for the Uniform Cost-Distance Steiner Tree Problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Josefine Foos, Stephan Held, Yannik Kyle Dustin Spitzley</p><p>Uniform cost-distance Steiner trees minimize the sum of the total length and
weighted path lengths from a dedicated root to the other terminals. They are
applied when the tree is intended for signal transmission, e.g. in chip design
or telecommunication networks. They are a special case of general cost-distance
Steiner trees, where different distance functions are used for total length and
path lengths.
</p>
<p>We improve the best published approximation factor for the uniform
cost-distance Steiner tree problem from 2.39 to 2.05. If we can approximate the
minimum-length Steiner tree problem arbitrarily well, our algorithm achieves an
approximation factor arbitrarily close to $ 1 + \frac{1}{\sqrt{2}} $. This
bound is tight in the following sense. We also prove the gap $ 1 +
\frac{1}{\sqrt{2}} $ between optimum solutions and the lower bound which we and
all previous approximation algorithms for this problem use.
</p>
<p>Similarly to previous approaches, we start with an approximate minimum-length
Steiner tree and split it into subtrees that are later re-connected. To improve
the approximation factor, we split it into components more carefully, taking
the cost structure into account, and we significantly enhance the analysis.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Foos_J/0/1/0/all/0/1">Josefine Foos</a>, <a href="http://arxiv.org/find/cs/1/au:+Held_S/0/1/0/all/0/1">Stephan Held</a>, <a href="http://arxiv.org/find/cs/1/au:+Spitzley_Y/0/1/0/all/0/1">Yannik Kyle Dustin Spitzley</a></p><p>Uniform cost-distance Steiner trees minimize the sum of the total length and
weighted path lengths from a dedicated root to the other terminals. They are
applied when the tree is intended for signal transmission, e.g. in chip design
or telecommunication networks. They are a special case of general cost-distance
Steiner trees, where different distance functions are used for total length and
path lengths.
</p>
<p>We improve the best published approximation factor for the uniform
cost-distance Steiner tree problem from 2.39 to 2.05. If we can approximate the
minimum-length Steiner tree problem arbitrarily well, our algorithm achieves an
approximation factor arbitrarily close to $ 1 + \frac{1}{\sqrt{2}} $. This
bound is tight in the following sense. We also prove the gap $ 1 +
\frac{1}{\sqrt{2}} $ between optimum solutions and the lower bound which we and
all previous approximation algorithms for this problem use.
</p>
<p>Similarly to previous approaches, we start with an approximate minimum-length
Steiner tree and split it into subtrees that are later re-connected. To improve
the approximation factor, we split it into components more carefully, taking
the cost structure into account, and we significantly enhance the analysis.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-08T00:30:00Z">Monday, May 08 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.03440'>Tight Bounds for Chordal/Interval Vertex Deletion Parameterized by Treewidth</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Michal Wlodarczyk</p><p>In Chordal/Interval Vertex Deletion we ask how many vertices one needs to
remove from a graph to make it chordal (respectively: interval). We study these
problems under the parameterization by treewidth $tw$ of the input graph $G$.
On the one hand, we present an algorithm for Chordal Vertex Deletion with
running time $2^{O(tw)} \cdot |V(G)|$, improving upon the running time
$2^{O(tw^2)} \cdot |V(G)|^{O(1)}$ by Jansen, de Kroon, and Wlodarczyk
(STOC'21). When a tree decomposition of width $tw$ is given, then the base of
the exponent equals $2^{\omega-1}\cdot 3 + 1$. Our algorithm is based on a
novel link between chordal graphs and graphic matroids, which allows us to
employ the framework of representative families. On the other hand, we prove
that the known $2^{O(tw \log tw)} \cdot |V(G)|$-time algorithm for Interval
Vertex Deletion cannot be improved assuming Exponential Time Hypothesis.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Wlodarczyk_M/0/1/0/all/0/1">Michal Wlodarczyk</a></p><p>In Chordal/Interval Vertex Deletion we ask how many vertices one needs to
remove from a graph to make it chordal (respectively: interval). We study these
problems under the parameterization by treewidth $tw$ of the input graph $G$.
On the one hand, we present an algorithm for Chordal Vertex Deletion with
running time $2^{O(tw)} \cdot |V(G)|$, improving upon the running time
$2^{O(tw^2)} \cdot |V(G)|^{O(1)}$ by Jansen, de Kroon, and Wlodarczyk
(STOC'21). When a tree decomposition of width $tw$ is given, then the base of
the exponent equals $2^{\omega-1}\cdot 3 + 1$. Our algorithm is based on a
novel link between chordal graphs and graphic matroids, which allows us to
employ the framework of representative families. On the other hand, we prove
that the known $2^{O(tw \log tw)} \cdot |V(G)|$-time algorithm for Interval
Vertex Deletion cannot be improved assuming Exponential Time Hypothesis.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-08T00:30:00Z">Monday, May 08 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.03693'>Fast Dynamic Programming in Trees in the MPC Model</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Chetan Gupta, Rustam Latypov, Yannic Maus, Shreyas Pai, Simo S&#xe4;rkk&#xe4;, Jan Studen&#xfd;, Jukka Suomela, Jara Uitto, Hossein Vahidi</p><p>We present a deterministic algorithm for solving a wide range of dynamic
programming problems in trees in $O(\log D)$ rounds in the massively parallel
computation model (MPC), with $O(n^\delta)$ words of local memory per machine,
for any given constant $0 &lt; \delta &lt; 1$. Here $D$ is the diameter of the tree
and $n$ is the number of nodes--we emphasize that our running time is
independent of $n$.
</p>
<p>Our algorithm can solve many classical graph optimization problems such as
maximum weight independent set, maximum weight matching, minimum weight
dominating set, and minimum weight vertex cover. It can also be used to solve
many accumulation tasks in which some aggregate information is propagated
upwards or downwards in the tree--this includes, for example, computing the
sum, minimum, or maximum of the input labels in each subtree, as well as many
inference tasks commonly solved with belief propagation. Our algorithm can also
solve any locally checkable labeling problem (LCLs) in trees. Our algorithm
works for any reasonable representation of the input tree; for example, the
tree can be represented as a list of edges or as a string with nested
parentheses or tags. The running time of $O(\log D)$ rounds is also known to be
necessary, assuming the widely-believed $2$-cycle conjecture.
</p>
<p>Our algorithm strictly improves on two prior algorithms: (i) Bateni,
Behnezhad, Derakhshan, Hajiaghayi, and Mirrokni [ICALP'18] solve problems of
these flavors in $O(\log n)$ rounds, while our algorithm is much faster in
low-diameter trees. Furthermore, their algorithm also uses randomness, while
our algorithm is deterministic. (ii) Balliu, Latypov, Maus, Olivetti, and Uitto
[SODA'23] solve only locally checkable labeling problems in $O(\log D)$ rounds,
while our algorithm can be applied to a much broader family of problems.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gupta_C/0/1/0/all/0/1">Chetan Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Latypov_R/0/1/0/all/0/1">Rustam Latypov</a>, <a href="http://arxiv.org/find/cs/1/au:+Maus_Y/0/1/0/all/0/1">Yannic Maus</a>, <a href="http://arxiv.org/find/cs/1/au:+Pai_S/0/1/0/all/0/1">Shreyas Pai</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarkka_S/0/1/0/all/0/1">Simo S&#xe4;rkk&#xe4;</a>, <a href="http://arxiv.org/find/cs/1/au:+Studeny_J/0/1/0/all/0/1">Jan Studen&#xfd;</a>, <a href="http://arxiv.org/find/cs/1/au:+Suomela_J/0/1/0/all/0/1">Jukka Suomela</a>, <a href="http://arxiv.org/find/cs/1/au:+Uitto_J/0/1/0/all/0/1">Jara Uitto</a>, <a href="http://arxiv.org/find/cs/1/au:+Vahidi_H/0/1/0/all/0/1">Hossein Vahidi</a></p><p>We present a deterministic algorithm for solving a wide range of dynamic
programming problems in trees in $O(\log D)$ rounds in the massively parallel
computation model (MPC), with $O(n^\delta)$ words of local memory per machine,
for any given constant $0 &lt; \delta &lt; 1$. Here $D$ is the diameter of the tree
and $n$ is the number of nodes--we emphasize that our running time is
independent of $n$.
</p>
<p>Our algorithm can solve many classical graph optimization problems such as
maximum weight independent set, maximum weight matching, minimum weight
dominating set, and minimum weight vertex cover. It can also be used to solve
many accumulation tasks in which some aggregate information is propagated
upwards or downwards in the tree--this includes, for example, computing the
sum, minimum, or maximum of the input labels in each subtree, as well as many
inference tasks commonly solved with belief propagation. Our algorithm can also
solve any locally checkable labeling problem (LCLs) in trees. Our algorithm
works for any reasonable representation of the input tree; for example, the
tree can be represented as a list of edges or as a string with nested
parentheses or tags. The running time of $O(\log D)$ rounds is also known to be
necessary, assuming the widely-believed $2$-cycle conjecture.
</p>
<p>Our algorithm strictly improves on two prior algorithms: (i) Bateni,
Behnezhad, Derakhshan, Hajiaghayi, and Mirrokni [ICALP'18] solve problems of
these flavors in $O(\log n)$ rounds, while our algorithm is much faster in
low-diameter trees. Furthermore, their algorithm also uses randomness, while
our algorithm is deterministic. (ii) Balliu, Latypov, Maus, Olivetti, and Uitto
[SODA'23] solve only locally checkable labeling problems in $O(\log D)$ rounds,
while our algorithm can be applied to a much broader family of problems.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-08T00:30:00Z">Monday, May 08 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.03697'>Fault-Tolerant ST-Diameter Oracles</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Davide Bil&#xf2;, Keerti Choudhary, Sarel Cohen, Tobias Friedrich, Simon Krogmann, Martin Schirneck</p><p>We study the problem of estimating the $ST$-diameter of a graph that is
subject to a bounded number of edge failures. An $f$-edge fault-tolerant
$ST$-diameter oracle ($f$-FDO-$ST$) is a data structure that preprocesses a
given graph $G$, two sets of vertices $S,T$, and positive integer $f$. When
queried with a set $F$ of at most $f$ edges, the oracle returns an estimate
$\widehat{D}$ of the $ST$-diameter $\operatorname{diam}(G-F,S,T)$, the maximum
distance between vertices in $S$ and $T$ in $G-F$. The oracle has stretch
$\sigma \geq 1$ if $\operatorname{diam}(G-F,S,T) \leq \widehat{D} \leq \sigma
\operatorname{diam}(G-F,S,T)$. If $S$ and $T$ both contain all vertices, the
data structure is called an $f$-edge fault-tolerant diameter oracle ($f$-FDO).
An $f$-edge fault-tolerant distance sensitivity oracles ($f$-DSO) estimates the
pairwise graph distances under up to $f$ failures.
</p>
<p>We design new $f$-FDOs and $f$-FDO-$ST$s by reducing their construction to
that of all-pairs and single-source $f$-DSOs. We obtain several new tradeoffs
between the size of the data structure, stretch guarantee, query and
preprocessing times for diameter oracles by combining our black-box reductions
with known results from the literature.
</p>
<p>We also provide an information-theoretic lower bound on the space requirement
of approximate $f$-FDOs. We show that there exists a family of graphs for which
any $f$-FDO with sensitivity $f \ge 2$ and stretch less than $5/3$ requires
$\Omega(n^{3/2})$ bits of space, regardless of the query time.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bilo_D/0/1/0/all/0/1">Davide Bil&#xf2;</a>, <a href="http://arxiv.org/find/cs/1/au:+Choudhary_K/0/1/0/all/0/1">Keerti Choudhary</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1">Sarel Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Friedrich_T/0/1/0/all/0/1">Tobias Friedrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Krogmann_S/0/1/0/all/0/1">Simon Krogmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Schirneck_M/0/1/0/all/0/1">Martin Schirneck</a></p><p>We study the problem of estimating the $ST$-diameter of a graph that is
subject to a bounded number of edge failures. An $f$-edge fault-tolerant
$ST$-diameter oracle ($f$-FDO-$ST$) is a data structure that preprocesses a
given graph $G$, two sets of vertices $S,T$, and positive integer $f$. When
queried with a set $F$ of at most $f$ edges, the oracle returns an estimate
$\widehat{D}$ of the $ST$-diameter $\operatorname{diam}(G-F,S,T)$, the maximum
distance between vertices in $S$ and $T$ in $G-F$. The oracle has stretch
$\sigma \geq 1$ if $\operatorname{diam}(G-F,S,T) \leq \widehat{D} \leq \sigma
\operatorname{diam}(G-F,S,T)$. If $S$ and $T$ both contain all vertices, the
data structure is called an $f$-edge fault-tolerant diameter oracle ($f$-FDO).
An $f$-edge fault-tolerant distance sensitivity oracles ($f$-DSO) estimates the
pairwise graph distances under up to $f$ failures.
</p>
<p>We design new $f$-FDOs and $f$-FDO-$ST$s by reducing their construction to
that of all-pairs and single-source $f$-DSOs. We obtain several new tradeoffs
between the size of the data structure, stretch guarantee, query and
preprocessing times for diameter oracles by combining our black-box reductions
with known results from the literature.
</p>
<p>We also provide an information-theoretic lower bound on the space requirement
of approximate $f$-FDOs. We show that there exists a family of graphs for which
any $f$-FDO with sensitivity $f \ge 2$ and stretch less than $5/3$ requires
$\Omega(n^{3/2})$ bits of space, regardless of the query time.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-08T00:30:00Z">Monday, May 08 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Sunday, May 07
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=7287'>Brief Update on Texan Tenure</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Update (May 8): Some tentative good news! It looks like there&#8217;s now a compromise bill in the House that would preserve tenure, insisting only on the sort of post-tenure review that UT (like most universities) already has. I blogged a few weeks ago about SB 18, a bill that would end tenure at Texas public [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p><strong><mark style="background-color:rgba(0, 0, 0, 0)" class="has-inline-color has-vivid-red-color">Update (May 8):</mark></strong> Some <a href="https://www.texastribune.org/2023/05/07/texas-house-tenure-bill/">tentative good news</a>!  It looks like there&#8217;s now a compromise bill in the House that would preserve tenure, insisting only on the sort of post-tenure review that UT (like most universities) already has.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>I <a href="https://scottaaronson.blog/?p=7243">blogged</a> a few weeks ago about SB 18, a bill that would end tenure at Texas public universities, including UT Austin and Texas A&amp;M.  The bad news is that SB 18 passed the Texas Senate.  The good news is that I&#8217;m told&#8212;I don&#8217;t know how reliably&#8212;that it has little chance of passing the House.</p>



<p>But it&#8217;s going to be discussed in the House tomorrow.  Any Texas residents reading this can, and are strongly urged, to <a href="https://comments.house.texas.gov/home?c=c290">submit brief comments here</a>. <strong> Please note that the deadline is tomorrow (Monday) morning.</strong></p>



<p>I just submitted the comment below.  Obviously, among the arguments that I genuinely believe, I made only those that I expect might have some purchase on a Texas Republican.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>I’m a professor of computer science at UT Austin, specializing in quantum computing.&nbsp; I am however writing this statement strictly in my capacity as a private citizen and Texas resident, not in my professional capacity.</p>



<p>Like the supporters of SB 18, I too see leftist ideological indoctrination on college campuses as a serious problem.&nbsp; It’s something that I and many other moderates and classical liberals in academia have been pushing back on for years.</p>



<p>But my purpose in this comment is to explain why eliminating tenure at UT Austin and Texas A&amp;M is NOT the solution — indeed, it would be the equivalent of treating a tumor by murdering the patient.</p>



<p>I’ve seen firsthand how already, just the *threat* that SB 18 might pass has seriously hampered our ability to recruit the best scientists and engineers to become faculty at UT Austin.&nbsp; If this bill were actually to pass, I expect that the impact on our recruiting would be total and catastrophic.&nbsp; It would effectively mean the end of UT Austin as one of the top public universities in the country.&nbsp; Hundreds of scientists who were lured to Texas by UT’s excellence, including me and my wife, would start looking for jobs elsewhere — even those whose own tenure was “grandfathered in.” &nbsp;They’d leave en masse for California and Massachusetts and anywhere else they could continue the lives they’d planned.</p>



<p>The reality is this: the sorts of scientists and engineers we’re talking about could typically make vastly higher incomes, in the high six figures or even seven figures, by working in private industry or forming their own startups.&nbsp; Yet they choose to accept much lower salaries to spend their careers in academia.&nbsp; Why?&nbsp; Because of the promise of a certain way of life: one where they can speak freely as scholars and individuals without worrying about how it will affect their employment.&nbsp; Tenure is a central part of that promise.&nbsp; Remove it, and the value proposition collapses.</p>



<p>In some sense, the state of Texas (like nearly every other state) actually gets a bargain through tenure.&nbsp; It couldn’t possibly afford to retain top-caliber scientists and engineers — working on medical breakthroughs, revolutionary advances in AI, and all the other stuff — if it DIDN’T offer tenure.</p>



<p>For this reason, I hope that even conservatives in the Texas House will see that we have a common interest here, in ensuring SB 18 never even makes it out of committee — for the sake of the future of innovation in Texas.&nbsp; I’m open to other possible responses to the problem of political indoctrination on campus.</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-07T23:39:34Z">Sunday, May 07 2023, 23:39</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/067'>TR23-067 |  Linear Relaxed Locally Decodable and Correctable Codes Do Not Need Adaptivity and Two-Sided Error | 

	Guy Goldberg</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Relaxed locally decodable codes (RLDCs) are error-correcting codes in which individual bits of the message can be recovered by querying only a few bits from a noisy codeword.
Unlike standard (non-relaxed) decoders, a relaxed one is allowed to output a ``rejection&#39;&#39; symbol, indicating that the decoding failed.
To prevent the decoder from always rejecting, we demand that if its input is a valid codeword, then for every bit, the decoder is correct with high probability.

We study the power of adaptivity and two-sided error for RLDCs.
Our main result is that if the underlying code is linear, *adaptivity and two-sided error do not give any power to relaxed local decoding.*
We construct a reduction from adaptive, two-sided error relaxed decoders to non-adaptive, one-sided error ones.
That is, the reduction produces a relaxed decoder that never errs or rejects if its input is a valid codeword and makes queries based on its internal randomness (and the requested index to decode), independently of the input.
The reduction does not change the query complexity (nor the underlying code), and for any input, the decoder&#39;s error probability increases at most two-fold.

The idea behind the reduction is our new notion of *additive* promise problem.
A promise problem is additive if the sum of any two YES-instances is a YES-instance (i.e., the YES instances are a subspace) and the sum of any NO-instance and a YES-instance is a NO-instance (i.e., the NO-instances are a collection of cosets).

We prove that relaxed decoding, interpreted as a promise problem, satisfies this definition.
We construct a reduction that applies to *any* additive promise problem, allowing us to obtain the result for RLDCs.
Our result also holds for relaxed locally *correctable* codes (RLCCs), where a *codeword* bit should be recovered.
        
        </div>

        <div class='tr-article-summary'>
        
          
          Relaxed locally decodable codes (RLDCs) are error-correcting codes in which individual bits of the message can be recovered by querying only a few bits from a noisy codeword.
Unlike standard (non-relaxed) decoders, a relaxed one is allowed to output a ``rejection&#39;&#39; symbol, indicating that the decoding failed.
To prevent the decoder from always rejecting, we demand that if its input is a valid codeword, then for every bit, the decoder is correct with high probability.

We study the power of adaptivity and two-sided error for RLDCs.
Our main result is that if the underlying code is linear, *adaptivity and two-sided error do not give any power to relaxed local decoding.*
We construct a reduction from adaptive, two-sided error relaxed decoders to non-adaptive, one-sided error ones.
That is, the reduction produces a relaxed decoder that never errs or rejects if its input is a valid codeword and makes queries based on its internal randomness (and the requested index to decode), independently of the input.
The reduction does not change the query complexity (nor the underlying code), and for any input, the decoder&#39;s error probability increases at most two-fold.

The idea behind the reduction is our new notion of *additive* promise problem.
A promise problem is additive if the sum of any two YES-instances is a YES-instance (i.e., the YES instances are a subspace) and the sum of any NO-instance and a YES-instance is a NO-instance (i.e., the NO-instances are a collection of cosets).

We prove that relaxed decoding, interpreted as a promise problem, satisfies this definition.
We construct a reduction that applies to *any* additive promise problem, allowing us to obtain the result for RLDCs.
Our result also holds for relaxed locally *correctable* codes (RLCCs), where a *codeword* bit should be recovered.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-07T19:30:37Z">Sunday, May 07 2023, 19:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Friday, May 05
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/066'>TR23-066 |  Protecting Single-Hop Radio Networks from Message Drops | 

	Dmitry Paramonov, 

	Gillat Kol, 

	Raghuvansh Saxena, 

	Klim Efremenko</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Single-hop radio networks (SHRN) are a well studied abstraction of communication over a wireless channel. In this model, in every round, each of the $n$ participating parties may decide to broadcast a message to all the others, potentially causing collisions. We consider the SHRN model in the presence of stochastic message drops (i.e., erasures), where in every round, the message received by each party is erased (replaced by $\bot$) with some small constant probability, independently.

Our main result is a constant rate coding scheme, allowing one to run protocols designed to work over the (noiseless) SHRN model over the SHRN model with erasures. Our scheme converts any protocol $\Pi$ of length at most exponential in $n$ over the SHRN model to a protocol $\Pi&#39;$ that is resilient to constant fraction of erasures and has length linear in the length of $\Pi$.

We mention that for the special case where the protocol $\Pi$ is non-adaptive, i.e., the order of communication is fixed in advance, such a scheme was known. Nevertheless, adaptivity is widely used and is known to hugely boost the power of wireless channels, which makes handling the general case of adaptive protocols $\Pi$ both important and more challenging. Indeed, to the best of our knowledge, our result is the first constant rate scheme that converts adaptive protocols to noise resilient ones in any multi-party model.
        
        </div>

        <div class='tr-article-summary'>
        
          
          Single-hop radio networks (SHRN) are a well studied abstraction of communication over a wireless channel. In this model, in every round, each of the $n$ participating parties may decide to broadcast a message to all the others, potentially causing collisions. We consider the SHRN model in the presence of stochastic message drops (i.e., erasures), where in every round, the message received by each party is erased (replaced by $\bot$) with some small constant probability, independently.

Our main result is a constant rate coding scheme, allowing one to run protocols designed to work over the (noiseless) SHRN model over the SHRN model with erasures. Our scheme converts any protocol $\Pi$ of length at most exponential in $n$ over the SHRN model to a protocol $\Pi&#39;$ that is resilient to constant fraction of erasures and has length linear in the length of $\Pi$.

We mention that for the special case where the protocol $\Pi$ is non-adaptive, i.e., the order of communication is fixed in advance, such a scheme was known. Nevertheless, adaptivity is widely used and is known to hugely boost the power of wireless channels, which makes handling the general case of adaptive protocols $\Pi$ both important and more challenging. Indeed, to the best of our knowledge, our result is the first constant rate scheme that converts adaptive protocols to noise resilient ones in any multi-party model.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T17:23:04Z">Friday, May 05 2023, 17:23</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=7278'>AI and Aaronson&#8217;s Law of Dark Irony</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The major developments in human history are always steeped in dark ironies. Yes, that&#8217;s my Law of Dark Irony, the whole thing. I don&#8217;t know why it&#8217;s true, but it certainly seems to be. Taking WWII as the archetypal example, let&#8217;s enumerate just the more obvious ones: When I think about the scenarios where superintelligent [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The major developments in human history are <em>always</em> steeped in dark ironies.  Yes, that&#8217;s my Law of Dark Irony, the whole thing.</p>



<p>I don&#8217;t know why it&#8217;s true, but it certainly seems to be.  Taking WWII as the archetypal example, let&#8217;s enumerate just the more obvious ones:</p>



<ul>
<li>After the carnage of WWI, the world&#8217;s most sensitive and thoughtful people (many of them) learned the lesson that they should oppose war at any cost.  This attitude let Germany rearm and set the stage for WWII.</li>



<li>Hitler, who was neither tall nor blond, wished to establish the worldwide domination of tall, blond Aryans &#8230; and do so via an alliance with the Japanese.</li>



<li>The Nazis touted the dream of eugenically perfecting the human race, then perpetrated a genocide against a tiny group that had produced Einstein, von Neumann, Wigner, Ulam, and Tarski.</li>



<li>The Jews were murdered using a chemical&#8212;<a href="https://en.wikipedia.org/wiki/Zyklon_B">Zyklon B</a>&#8212;developed in part by the Jewish chemist Fritz Haber.</li>



<li>The Allied force that made the greatest sacrifice in lives to defeat Hitler was Stalin&#8217;s USSR, <em>another</em> of history&#8217;s most murderous and horrifying regimes.</li>



<li>The man who rallied the free world to defeat Nazism, Winston Churchill, was himself a racist colonialist, whose views would be (and regularly are) denounced as &#8220;Nazi&#8221; on modern college campuses.</li>



<li>The WWII legacy that would go on to threaten humanity&#8217;s existence&#8212;the Bomb&#8212;was created in what the scientists believed was a desperate race to <em>save</em> humanity.  Then Hitler was defeated before the Bomb was ready, and it turned out the Nazis were never even close to building their own Bomb, and the Bomb was used instead against Japan.</li>
</ul>



<p>When I think about the scenarios where superintelligent AI destroys the world, they rarely seem to do enough justice to the Law of Dark Irony.  It&#8217;s like: OK, AI is created to serve humanity, and instead it turns on humanity and destroys it.  Great, that&#8217;s <em>one</em> dark irony.  One.  What other dark ironies could there be?  How about:</p>



<ul>
<li>For decades, the Yudkowskyans warned about the dangers of superintelligence.  So far, by all accounts, the great practical effect of these warnings has been to inspire the founding of both DeepMind and OpenAI, the entities that Yudkowskyans believe are locked into a race to <em>realize</em> those dangers.</li>



<li>Maybe AIs will displace humans &#8230; and they&#8217;ll <em>deserve</em> to, since they won&#8217;t be quite as wretched and cruel as we are.  (This is basically the plot of <em>Westworld</em>, or at least of its first couple seasons, which Dana and I are now belatedly watching.)</li>



<li>Maybe the world will get destroyed by what Yudkowsky calls a <a href="https://arbital.com/p/pivotal/">&#8220;pivotal act&#8221;</a>: an act meant to <em>safeguard</em> the world from takeover from an unaligned AGI, for example by taking it over with an aligned AGI first.  (I seriously worry about this; it&#8217;s a pretty obvious one.)</li>



<li>Maybe AI will get the idea to take over the world, but only because it&#8217;s been trained on generations of science fiction and decades of Internet discussion worrying about the possibility of AI taking over the world.  (I&#8217;m far from the first to notice this possibility.)</li>



<li>Maybe AI will indeed destroy the world, but it will do so &#8220;by mistake,&#8221; while trying to <em>save</em> the world, or by taking a calculated gamble to save the world that fails.  (A commenter on my last post <a href="https://scottaaronson.blog/?p=7266#comment-1949890">brought this one up</a>.)</li>



<li>Maybe humanity will successfully coordinate to pause AGI development, and then promptly be destroyed by <em>something else</em>&#8212;runaway climate change, an accidental nuclear exchange&#8212;that the AGI, had it been created, would&#8217;ve prevented.  (This, of course, would be directly analogous to one of the great dark ironies of all time: the one where decades of antinuclear activism, intended to save the planet, has instead doomed us to destroy the earth by oil and coal.)</li>
</ul>



<p>Readers: which <em>other</em> possible dark ironies have I missed?</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T02:27:52Z">Friday, May 05 2023, 02:27</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02553'>Complexity and asymptotics of structure constants</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Greta Panova</p><p>Kostka, Littlewood-Richardson, Kronecker, and plethysm coefficients are
fundamental quantities in algebraic combinatorics, yet many natural questions
about them stay unanswered for more than 80 years. Kronecker and plethysm
coefficients lack ``nice formulas'', a notion that can be formalized using
computational complexity theory. Beyond formulas and combinatorial
interpretations, we can attempt to understand their asymptotic behavior in
various regimes, and inequalities they could satisfy. Understanding these
quantities has applications beyond combinatorics. On the one hand, the
asymptotics of structure constants is closely related to understanding the
[limit] behavior of vertex and tiling models in statistical mechanics. More
recently, these structure constants have been involved in establishing
computational complexity lower bounds and separation of complexity classes like
VP vs VNP, the algebraic analogs of P vs NP in arithmetic complexity theory.
Here we discuss the outstanding problems related to asymptotics, positivity,
and complexity of structure constants focusing mostly on the Kronecker
coefficients of the symmetric group and, less so, on the plethysm coefficients.
</p>
<p>This expository paper is based on the talk presented at the Open Problems in
Algebraic Combinatorics coneference in May 2022.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Panova_G/0/1/0/all/0/1">Greta Panova</a></p><p>Kostka, Littlewood-Richardson, Kronecker, and plethysm coefficients are
fundamental quantities in algebraic combinatorics, yet many natural questions
about them stay unanswered for more than 80 years. Kronecker and plethysm
coefficients lack ``nice formulas'', a notion that can be formalized using
computational complexity theory. Beyond formulas and combinatorial
interpretations, we can attempt to understand their asymptotic behavior in
various regimes, and inequalities they could satisfy. Understanding these
quantities has applications beyond combinatorics. On the one hand, the
asymptotics of structure constants is closely related to understanding the
[limit] behavior of vertex and tiling models in statistical mechanics. More
recently, these structure constants have been involved in establishing
computational complexity lower bounds and separation of complexity classes like
VP vs VNP, the algebraic analogs of P vs NP in arithmetic complexity theory.
Here we discuss the outstanding problems related to asymptotics, positivity,
and complexity of structure constants focusing mostly on the Kronecker
coefficients of the symmetric group and, less so, on the plethysm coefficients.
</p>
<p>This expository paper is based on the talk presented at the Open Problems in
Algebraic Combinatorics coneference in May 2022.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02800'>On the parameterized complexity of the Perfect Phylogeny problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jorke M. de Vlas</p><p>This paper categorizes the parameterized complexity of the algorithmic
problems Perfect Phylogeny and Triangulating Colored Graphs. We show that they
are complete for the parameterized complexity class XALP using a reduction from
Tree-chained Multicolor Independent Set and a proof of membership. We introduce
the problem Triangulating Multicolored Graphs as a stepping stone and prove
XALP-completeness for this problem as well. We also show that, assuming the
Exponential Time Hypothesis, there exists no algorithm that solves any of these
problems in time $f(k) n^{o(k)}$, where $n$ is the input size, $k$ the
parameter, and $f$ any computable function.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Vlas_J/0/1/0/all/0/1">Jorke M. de Vlas</a></p><p>This paper categorizes the parameterized complexity of the algorithmic
problems Perfect Phylogeny and Triangulating Colored Graphs. We show that they
are complete for the parameterized complexity class XALP using a reduction from
Tree-chained Multicolor Independent Set and a proof of membership. We introduce
the problem Triangulating Multicolored Graphs as a stepping stone and prove
XALP-completeness for this problem as well. We also show that, assuming the
Exponential Time Hypothesis, there exists no algorithm that solves any of these
problems in time $f(k) n^{o(k)}$, where $n$ is the input size, $k$ the
parameter, and $f$ any computable function.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.03003'>All Kronecker coefficients are reduced Kronecker coefficients</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Christian Ikenmeyer, Greta Panova</p><p>We settle the question of where exactly the reduced Kronecker coefficients
lie on the spectrum between the Littlewood-Richardson and Kronecker
coefficients by showing that every Kronecker coefficient of the symmetric group
is equal to a reduced Kronecker coefficient by an explicit construction. This
implies the equivalence of a question by Stanley from 2000 and a question by
Kirillov from 2004 about combinatorial interpretations of these two families of
coefficients. Moreover, as a corollary, we deduce that deciding the positivity
of reduced Kronecker coefficients is $NP$-hard, and computing them is
$\#P$-hard under parsimonious many-one reductions.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Ikenmeyer_C/0/1/0/all/0/1">Christian Ikenmeyer</a>, <a href="http://arxiv.org/find/math/1/au:+Panova_G/0/1/0/all/0/1">Greta Panova</a></p><p>We settle the question of where exactly the reduced Kronecker coefficients
lie on the spectrum between the Littlewood-Richardson and Kronecker
coefficients by showing that every Kronecker coefficient of the symmetric group
is equal to a reduced Kronecker coefficient by an explicit construction. This
implies the equivalence of a question by Stanley from 2000 and a question by
Kirillov from 2004 about combinatorial interpretations of these two families of
coefficients. Moreover, as a corollary, we deduce that deciding the positivity
of reduced Kronecker coefficients is $NP$-hard, and computing them is
$\#P$-hard under parsimonious many-one reductions.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02445'>Online Geometric Covering and Piercing</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Minati De, Saksham Jain, Sarat Varma Kallepalli, Satyam Singh</p><p>We consider the online version of the piercing set problem, where geometric
objects arrive one by one, and the online algorithm must maintain a valid
piercing set for the already arrived objects by making irrevocable decisions.
It is easy to observe that any deterministic online algorithm that solves this
problem has a competitive ratio of at least $\Omega(n)$, which even holds when
the objects are intervals. This paper considers the piercing set problem when
objects are bounded scaled. We propose deterministic algorithms for bounded
scaled fat objects. Piercing translated copies of an object is equivalent to
the unit covering problem, which is well-studied in the online setup.
Surprisingly, no upper bound of the competitive ratio was known for the unit
covering problem when unit objects are anything other than balls and
hypercubes. Our result gives an upper bound of the competitive ratio for the
unit covering problem for various unit objects. For fixed-oriented hypercubes
in $\mathbb{R}^d$ with the scaling factor in the range $[1,k]$, we propose an
algorithm having a competitive ratio of at most~$3^d\log_2 k+2^d$. In the end,
we show a lower bound of the competitive ratio for bounded scaled objects of
various types like $\alpha$-fat objects in $\mathbb{R}^2$, axis-aligned
hypercubes in $\mathbb{R}^d$, and balls in $\mathbb{R}^2$ and~$\mathbb{R}^3$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+De_M/0/1/0/all/0/1">Minati De</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Saksham Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Kallepalli_S/0/1/0/all/0/1">Sarat Varma Kallepalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Satyam Singh</a></p><p>We consider the online version of the piercing set problem, where geometric
objects arrive one by one, and the online algorithm must maintain a valid
piercing set for the already arrived objects by making irrevocable decisions.
It is easy to observe that any deterministic online algorithm that solves this
problem has a competitive ratio of at least $\Omega(n)$, which even holds when
the objects are intervals. This paper considers the piercing set problem when
objects are bounded scaled. We propose deterministic algorithms for bounded
scaled fat objects. Piercing translated copies of an object is equivalent to
the unit covering problem, which is well-studied in the online setup.
Surprisingly, no upper bound of the competitive ratio was known for the unit
covering problem when unit objects are anything other than balls and
hypercubes. Our result gives an upper bound of the competitive ratio for the
unit covering problem for various unit objects. For fixed-oriented hypercubes
in $\mathbb{R}^d$ with the scaling factor in the range $[1,k]$, we propose an
algorithm having a competitive ratio of at most~$3^d\log_2 k+2^d$. In the end,
we show a lower bound of the competitive ratio for bounded scaled objects of
various types like $\alpha$-fat objects in $\mathbb{R}^2$, axis-aligned
hypercubes in $\mathbb{R}^d$, and balls in $\mathbb{R}^2$ and~$\mathbb{R}^3$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02724'>Towards Stratified Space Learning: 2-complexes</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yossi Bokor Bleile</p><p>In this paper, we consider a simple class of stratified spaces --
2-complexes. We present an algorithm that learns the abstract structure of an
embedded 2-complex from a point cloud sampled from it. We use tools and
inspiration from computational geometry, algebraic topology, and topological
data analysis and prove the correctness of the identified abstract structure
under assumptions on the embedding.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bleile_Y/0/1/0/all/0/1">Yossi Bokor Bleile</a></p><p>In this paper, we consider a simple class of stratified spaces --
2-complexes. We present an algorithm that learns the abstract structure of an
embedded 2-complex from a point cloud sampled from it. We use tools and
inspiration from computational geometry, algebraic topology, and topological
data analysis and prove the correctness of the identified abstract structure
under assumptions on the embedding.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02850'>Impossibility of Depth Reduction in Explainable Clustering</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Chengyuan Deng, Surya Teja Gavva, Karthik C. S., Parth Patel, Adarsh Srinivasan</p><p>Over the last few years Explainable Clustering has gathered a lot of
attention. Dasgupta et al. [ICML'20] initiated the study of explainable k-means
and k-median clustering problems where the explanation is captured by a
threshold decision tree which partitions the space at each node using axis
parallel hyperplanes. Recently, Laber et al. [Pattern Recognition'23] made a
case to consider the depth of the decision tree as an additional complexity
measure of interest.
</p>
<p>In this work, we prove that even when the input points are in the Euclidean
plane, then any depth reduction in the explanation incurs unbounded loss in the
k-means and k-median cost. Formally, we show that there exists a data set X in
the Euclidean plane, for which there is a decision tree of depth k-1 whose
k-means/k-median cost matches the optimal clustering cost of X, but every
decision tree of depth less than k-1 has unbounded cost w.r.t. the optimal cost
of clustering. We extend our results to the k-center objective as well, albeit
with weaker guarantees.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1">Chengyuan Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gavva_S/0/1/0/all/0/1">Surya Teja Gavva</a>, <a href="http://arxiv.org/find/cs/1/au:+S%2E_K/0/1/0/all/0/1">Karthik C. S.</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_P/0/1/0/all/0/1">Parth Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1">Adarsh Srinivasan</a></p><p>Over the last few years Explainable Clustering has gathered a lot of
attention. Dasgupta et al. [ICML'20] initiated the study of explainable k-means
and k-median clustering problems where the explanation is captured by a
threshold decision tree which partitions the space at each node using axis
parallel hyperplanes. Recently, Laber et al. [Pattern Recognition'23] made a
case to consider the depth of the decision tree as an additional complexity
measure of interest.
</p>
<p>In this work, we prove that even when the input points are in the Euclidean
plane, then any depth reduction in the explanation incurs unbounded loss in the
k-means and k-median cost. Formally, we show that there exists a data set X in
the Euclidean plane, for which there is a decision tree of depth k-1 whose
k-means/k-median cost matches the optimal clustering cost of X, but every
decision tree of depth less than k-1 has unbounded cost w.r.t. the optimal cost
of clustering. We extend our results to the k-center objective as well, albeit
with weaker guarantees.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02450'>Perfect Sampling for Hard Spheres from Strong Spatial Mixing</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Konrad Anand, Andreas G&#xf6;bel, Marcus Pappik, Will Perkins</p><p>We provide a perfect sampling algorithm for the hard-sphere model on subsets
of $\mathbb{R}^d$ with expected running time linear in the volume under the
assumption of strong spatial mixing. A large number of perfect and approximate
sampling algorithms have been devised to sample from the hard-sphere model, and
our perfect sampling algorithm is efficient for a range of parameters for which
only efficient approximate samplers were previously known and is faster than
these known approximate approaches. Our methods also extend to the more general
setting of Gibbs point processes interacting via finite-range, repulsive
potentials.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Anand_K/0/1/0/all/0/1">Konrad Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Gobel_A/0/1/0/all/0/1">Andreas G&#xf6;bel</a>, <a href="http://arxiv.org/find/cs/1/au:+Pappik_M/0/1/0/all/0/1">Marcus Pappik</a>, <a href="http://arxiv.org/find/cs/1/au:+Perkins_W/0/1/0/all/0/1">Will Perkins</a></p><p>We provide a perfect sampling algorithm for the hard-sphere model on subsets
of $\mathbb{R}^d$ with expected running time linear in the volume under the
assumption of strong spatial mixing. A large number of perfect and approximate
sampling algorithms have been devised to sample from the hard-sphere model, and
our perfect sampling algorithm is efficient for a range of parameters for which
only efficient approximate samplers were previously known and is faster than
these known approximate approaches. Our methods also extend to the more general
setting of Gibbs point processes interacting via finite-range, repulsive
potentials.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02508'>Efficient Caching with Reserves via Marking</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sharat Ibrahimpur, Manish Purohit, Zoya Svitkina, Erik Vee, Joshua R. Wang</p><p>Online caching is among the most fundamental and well-studied problems in the
area of online algorithms. Innovative algorithmic ideas and analysis --
including potential functions and primal-dual techniques -- give insight into
this still-growing area. Here, we introduce a new analysis technique that first
uses a potential function to upper bound the cost of an online algorithm and
then pairs that with a new dual-fitting strategy to lower bound the cost of an
offline optimal algorithm. We apply these techniques to the Caching with
Reserves problem recently introduced by Ibrahimpur et al. [10] and give an
O(log k)-competitive fractional online algorithm via a marking strategy, where
k denotes the size of the cache. We also design a new online rounding algorithm
that runs in polynomial time to obtain an O(log k)-competitive randomized
integral algorithm. Additionally, we provide a new, simple proof for randomized
marking for the classical unweighted paging problem.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ibrahimpur_S/0/1/0/all/0/1">Sharat Ibrahimpur</a>, <a href="http://arxiv.org/find/cs/1/au:+Purohit_M/0/1/0/all/0/1">Manish Purohit</a>, <a href="http://arxiv.org/find/cs/1/au:+Svitkina_Z/0/1/0/all/0/1">Zoya Svitkina</a>, <a href="http://arxiv.org/find/cs/1/au:+Vee_E/0/1/0/all/0/1">Erik Vee</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Joshua R. Wang</a></p><p>Online caching is among the most fundamental and well-studied problems in the
area of online algorithms. Innovative algorithmic ideas and analysis --
including potential functions and primal-dual techniques -- give insight into
this still-growing area. Here, we introduce a new analysis technique that first
uses a potential function to upper bound the cost of an online algorithm and
then pairs that with a new dual-fitting strategy to lower bound the cost of an
offline optimal algorithm. We apply these techniques to the Caching with
Reserves problem recently introduced by Ibrahimpur et al. [10] and give an
O(log k)-competitive fractional online algorithm via a marking strategy, where
k denotes the size of the cache. We also design a new online rounding algorithm
that runs in polynomial time to obtain an O(log k)-competitive randomized
integral algorithm. Additionally, we provide a new, simple proof for randomized
marking for the classical unweighted paging problem.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02526'>Prefix Sorting DFAs: a Recursive Algorithm</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Nicola Cotumaccio</p><p>In the past thirty years, numerous algorithms for building the suffix array
of a string have been proposed. In 2021, the notion of suffix array was
extended from strings to DFAs, and it was shown that the resulting data
structure can be built in $ O(m^2 + n^{5/2}) $ time, where $ n $ is the number
of states and $ m $ is the number of edges [SODA 2021]. Recently, algorithms
running in $ O(mn) $ and $ O(n^2\log n) $ time have been described [CPM 2023].
</p>
<p>In this paper, we improve the previous bounds by proposing an $ O(n^2) $
recursive algorithm inspired by Farach's algorithm for building a suffix tree
[FOCS 1997]. To this end, we provide insight into the rich lexicographic and
combinatorial structure of a graph, so contributing to the fascinating journey
which might lead to solve the long-standing open problem of building the suffix
tree of a graph.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cotumaccio_N/0/1/0/all/0/1">Nicola Cotumaccio</a></p><p>In the past thirty years, numerous algorithms for building the suffix array
of a string have been proposed. In 2021, the notion of suffix array was
extended from strings to DFAs, and it was shown that the resulting data
structure can be built in $ O(m^2 + n^{5/2}) $ time, where $ n $ is the number
of states and $ m $ is the number of edges [SODA 2021]. Recently, algorithms
running in $ O(mn) $ and $ O(n^2\log n) $ time have been described [CPM 2023].
</p>
<p>In this paper, we improve the previous bounds by proposing an $ O(n^2) $
recursive algorithm inspired by Farach's algorithm for building a suffix tree
[FOCS 1997]. To this end, we provide insight into the rich lexicographic and
combinatorial structure of a graph, so contributing to the fascinating journey
which might lead to solve the long-standing open problem of building the suffix
tree of a graph.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02535'>On the Unreasonable Effectiveness of Single Vector Krylov Methods for Low-Rank Approximation</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Raphael A. Meyer, Cameron Musco, Christopher Musco</p><p>Krylov subspace methods are a ubiquitous tool for computing near-optimal rank
$k$ approximations of large matrices. While "large block" Krylov methods with
block size at least $k$ give the best known theoretical guarantees, block size
one (a single vector) or a small constant is often preferred in practice.
Despite their popularity, we lack theoretical bounds on the performance of such
"small block" Krylov methods for low-rank approximation.
</p>
<p>We address this gap between theory and practice by proving that small block
Krylov methods essentially match all known low-rank approximation guarantees
for large block methods. Via a black-box reduction we show, for example, that
the standard single vector Krylov method run for $t$ iterations obtains the
same spectral norm and Frobenius norm error bounds as a Krylov method with
block size $\ell \geq k$ run for $O(t/\ell)$ iterations, up to a logarithmic
dependence on the smallest gap between sequential singular values. That is, for
a given number of matrix-vector products, single vector methods are essentially
as effective as any choice of large block size.
</p>
<p>By combining our result with tail-bounds on eigenvalue gaps in random
matrices, we prove that the dependence on the smallest singular value gap can
be eliminated if the input matrix is perturbed by a small random matrix.
Further, we show that single vector methods match the more complex algorithm of
[Bakshi et al. `22], which combines the results of multiple block sizes to
achieve an improved algorithm for Schatten $p$-norm low-rank approximation.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Meyer_R/0/1/0/all/0/1">Raphael A. Meyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1">Cameron Musco</a>, <a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1">Christopher Musco</a></p><p>Krylov subspace methods are a ubiquitous tool for computing near-optimal rank
$k$ approximations of large matrices. While "large block" Krylov methods with
block size at least $k$ give the best known theoretical guarantees, block size
one (a single vector) or a small constant is often preferred in practice.
Despite their popularity, we lack theoretical bounds on the performance of such
"small block" Krylov methods for low-rank approximation.
</p>
<p>We address this gap between theory and practice by proving that small block
Krylov methods essentially match all known low-rank approximation guarantees
for large block methods. Via a black-box reduction we show, for example, that
the standard single vector Krylov method run for $t$ iterations obtains the
same spectral norm and Frobenius norm error bounds as a Krylov method with
block size $\ell \geq k$ run for $O(t/\ell)$ iterations, up to a logarithmic
dependence on the smallest gap between sequential singular values. That is, for
a given number of matrix-vector products, single vector methods are essentially
as effective as any choice of large block size.
</p>
<p>By combining our result with tail-bounds on eigenvalue gaps in random
matrices, we prove that the dependence on the smallest singular value gap can
be eliminated if the input matrix is perturbed by a small random matrix.
Further, we show that single vector methods match the more complex algorithm of
[Bakshi et al. `22], which combines the results of multiple block sizes to
achieve an improved algorithm for Schatten $p$-norm low-rank approximation.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02544'>Nearly-Linear Time and Streaming Algorithms for Outlier-Robust PCA</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ilias Diakonikolas, Daniel M. Kane, Ankit Pensia, Thanasis Pittas</p><p>We study principal component analysis (PCA), where given a dataset in
$\mathbb{R}^d$ from a distribution, the task is to find a unit vector $v$ that
approximately maximizes the variance of the distribution after being projected
along $v$. Despite being a classical task, standard estimators fail drastically
if the data contains even a small fraction of outliers, motivating the problem
of robust PCA. Recent work has developed computationally-efficient algorithms
for robust PCA that either take super-linear time or have sub-optimal error
guarantees. Our main contribution is to develop a nearly-linear time algorithm
for robust PCA with near-optimal error guarantees. We also develop a
single-pass streaming algorithm for robust PCA with memory usage nearly-linear
in the dimension.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1">Ilias Diakonikolas</a>, <a href="http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1">Daniel M. Kane</a>, <a href="http://arxiv.org/find/cs/1/au:+Pensia_A/0/1/0/all/0/1">Ankit Pensia</a>, <a href="http://arxiv.org/find/cs/1/au:+Pittas_T/0/1/0/all/0/1">Thanasis Pittas</a></p><p>We study principal component analysis (PCA), where given a dataset in
$\mathbb{R}^d$ from a distribution, the task is to find a unit vector $v$ that
approximately maximizes the variance of the distribution after being projected
along $v$. Despite being a classical task, standard estimators fail drastically
if the data contains even a small fraction of outliers, motivating the problem
of robust PCA. Recent work has developed computationally-efficient algorithms
for robust PCA that either take super-linear time or have sub-optimal error
guarantees. Our main contribution is to develop a nearly-linear time algorithm
for robust PCA with near-optimal error guarantees. We also develop a
single-pass streaming algorithm for robust PCA with memory usage nearly-linear
in the dimension.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02545'>$\alpha_i$-Metric Graphs: Radius, Diameter and all Eccentricities</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Feodor F. Dragan, Guillaume Ducoffe</p><p>We extend known results on chordal graphs and distance-hereditary graphs to
much larger graph classes by using only a common metric property of these
graphs. Specifically, a graph is called $\alpha_i$-metric ($i\in \mathcal{N}$)
if it satisfies the following $\alpha_i$-metric property for every vertices
$u,w,v$ and $x$: if a shortest path between $u$ and $w$ and a shortest path
between $x$ and $v$ share a terminal edge $vw$, then $d(u,x)\geq d(u,v) +
d(v,x)-i$. Roughly, gluing together any two shortest paths along a common
terminal edge may not necessarily result in a shortest path but yields a
``near-shortest'' path with defect at most $i$. It is known that
$\alpha_0$-metric graphs are exactly ptolemaic graphs, and that chordal graphs
and distance-hereditary graphs are $\alpha_i$-metric for $i=1$ and $i=2$,
respectively. We show that an additive $O(i)$-approximation of the radius, of
the diameter, and in fact of all vertex eccentricities of an $\alpha_i$-metric
graph can be computed in total linear time. Our strongest results are obtained
for $\alpha_1$-metric graphs, for which we prove that a central vertex can be
computed in subquadratic time, and even better in linear time for so-called
$(\alpha_1,\Delta)$-metric graphs (a superclass of chordal graphs and of plane
triangulations with inner vertices of degree at least $7$). The latter answers
a question raised in (Dragan, IPL, 2020). Our algorithms follow from new
results on centers and metric intervals of $\alpha_i$-metric graphs. In
particular, we prove that the diameter of the center is at most $3i+2$ (at most
$3$, if $i=1$). The latter partly answers a question raised in (Yushmanov &amp;
Chepoi, Mathematical Problems in Cybernetics, 1991).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dragan_F/0/1/0/all/0/1">Feodor F. Dragan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ducoffe_G/0/1/0/all/0/1">Guillaume Ducoffe</a></p><p>We extend known results on chordal graphs and distance-hereditary graphs to
much larger graph classes by using only a common metric property of these
graphs. Specifically, a graph is called $\alpha_i$-metric ($i\in \mathcal{N}$)
if it satisfies the following $\alpha_i$-metric property for every vertices
$u,w,v$ and $x$: if a shortest path between $u$ and $w$ and a shortest path
between $x$ and $v$ share a terminal edge $vw$, then $d(u,x)\geq d(u,v) +
d(v,x)-i$. Roughly, gluing together any two shortest paths along a common
terminal edge may not necessarily result in a shortest path but yields a
``near-shortest'' path with defect at most $i$. It is known that
$\alpha_0$-metric graphs are exactly ptolemaic graphs, and that chordal graphs
and distance-hereditary graphs are $\alpha_i$-metric for $i=1$ and $i=2$,
respectively. We show that an additive $O(i)$-approximation of the radius, of
the diameter, and in fact of all vertex eccentricities of an $\alpha_i$-metric
graph can be computed in total linear time. Our strongest results are obtained
for $\alpha_1$-metric graphs, for which we prove that a central vertex can be
computed in subquadratic time, and even better in linear time for so-called
$(\alpha_1,\Delta)$-metric graphs (a superclass of chordal graphs and of plane
triangulations with inner vertices of degree at least $7$). The latter answers
a question raised in (Dragan, IPL, 2020). Our algorithms follow from new
results on centers and metric intervals of $\alpha_i$-metric graphs. In
particular, we prove that the diameter of the center is at most $3i+2$ (at most
$3$, if $i=1$). The latter partly answers a question raised in (Yushmanov &amp;
Chepoi, Mathematical Problems in Cybernetics, 1991).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02566'>A Hyperbolic Extension of Kadison-Singer Type Results</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ruizhe Zhang, Xinzhi Zhang</p><p>In 2013, Marcus, Spielman, and Srivastava resolved the famous Kadison-Singer
conjecture. It states that for $n$ independent random vectors $v_1,\cdots, v_n$
that have expected squared norm bounded by $\epsilon$ and are in the isotropic
position in expectation, there is a positive probability that the determinant
polynomial $\det(xI - \sum_{i=1}^n v_iv_i^\top)$ has roots bounded by $(1 +
\sqrt{\epsilon})^2$. An interpretation of the Kadison-Singer theorem is that we
can always find a partition of the vectors $v_1,\cdots,v_n$ into two sets with
a low discrepancy in terms of the spectral norm (in other words, rely on the
determinant polynomial).
</p>
<p>In this paper, we provide two results for a broader class of polynomials, the
hyperbolic polynomials. Furthermore, our results are in two generalized
settings:
</p>
<p>$\bullet$ The first one shows that the Kadison-Singer result requires a
weaker assumption that the vectors have a bounded sum of hyperbolic norms.
</p>
<p>$\bullet$ The second one relaxes the Kadison-Singer result's distribution
assumption to the Strongly Rayleigh distribution.
</p>
<p>To the best of our knowledge, the previous results only support determinant
polynomials [Anari and Oveis Gharan'14, Kyng, Luh and Song'20]. It is unclear
whether they can be generalized to a broader class of polynomials. In addition,
we also provide a sub-exponential time algorithm for constructing our results.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Zhang_R/0/1/0/all/0/1">Ruizhe Zhang</a>, <a href="http://arxiv.org/find/math/1/au:+Zhang_X/0/1/0/all/0/1">Xinzhi Zhang</a></p><p>In 2013, Marcus, Spielman, and Srivastava resolved the famous Kadison-Singer
conjecture. It states that for $n$ independent random vectors $v_1,\cdots, v_n$
that have expected squared norm bounded by $\epsilon$ and are in the isotropic
position in expectation, there is a positive probability that the determinant
polynomial $\det(xI - \sum_{i=1}^n v_iv_i^\top)$ has roots bounded by $(1 +
\sqrt{\epsilon})^2$. An interpretation of the Kadison-Singer theorem is that we
can always find a partition of the vectors $v_1,\cdots,v_n$ into two sets with
a low discrepancy in terms of the spectral norm (in other words, rely on the
determinant polynomial).
</p>
<p>In this paper, we provide two results for a broader class of polynomials, the
hyperbolic polynomials. Furthermore, our results are in two generalized
settings:
</p>
<p>$\bullet$ The first one shows that the Kadison-Singer result requires a
weaker assumption that the vectors have a bounded sum of hyperbolic norms.
</p>
<p>$\bullet$ The second one relaxes the Kadison-Singer result's distribution
assumption to the Strongly Rayleigh distribution.
</p>
<p>To the best of our knowledge, the previous results only support determinant
polynomials [Anari and Oveis Gharan'14, Kyng, Luh and Song'20]. It is unclear
whether they can be generalized to a broader class of polynomials. In addition,
we also provide a sub-exponential time algorithm for constructing our results.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02816'>Shannon meets Gray: Noise-robust, Low-sensitivity Codes with Applications in Differential Privacy</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: David Rasmussen Lolck, Rasmus Pagh</p><p>Integer data is typically made differentially private by adding noise from a
Discrete Laplace (or Discrete Gaussian) distribution. We study the setting
where differential privacy of a counting query is achieved using bit-wise
randomized response, i.e., independent, random bit flips on the encoding of the
query answer.
</p>
<p>Binary error-correcting codes transmitted through noisy channels with
independent bit flips are well-studied in information theory. However, such
codes are unsuitable for differential privacy since they have (by design) high
sensitivity, i.e., neighboring integers have encodings with a large Hamming
distance. Gray codes show that it is possible to create an efficient
sensitivity 1 encoding, but are also not suitable for differential privacy due
to lack of noise-robustness.
</p>
<p>Our main result is that it is possible, with a constant rate code, to
simultaneously achieve the sensitivity of Gray codes and the noise-robustness
of error-correcting codes (down to the noise level required for differential
privacy). An application of this new encoding of the integers is a faster,
space-optimal differentially private data structure for histograms.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lolck_D/0/1/0/all/0/1">David Rasmussen Lolck</a>, <a href="http://arxiv.org/find/cs/1/au:+Pagh_R/0/1/0/all/0/1">Rasmus Pagh</a></p><p>Integer data is typically made differentially private by adding noise from a
Discrete Laplace (or Discrete Gaussian) distribution. We study the setting
where differential privacy of a counting query is achieved using bit-wise
randomized response, i.e., independent, random bit flips on the encoding of the
query answer.
</p>
<p>Binary error-correcting codes transmitted through noisy channels with
independent bit flips are well-studied in information theory. However, such
codes are unsuitable for differential privacy since they have (by design) high
sensitivity, i.e., neighboring integers have encodings with a large Hamming
distance. Gray codes show that it is possible to create an efficient
sensitivity 1 encoding, but are also not suitable for differential privacy due
to lack of noise-robustness.
</p>
<p>Our main result is that it is possible, with a constant rate code, to
simultaneously achieve the sensitivity of Gray codes and the noise-robustness
of error-correcting codes (down to the noise level required for differential
privacy). An application of this new encoding of the integers is a faster,
space-optimal differentially private data structure for histograms.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02831'>Local Computation Algorithms for Hypergraph Coloring -- following Beck's approach (full version)</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Andrzej Dorobisz, Jakub Kozik</p><p>We investigate local computation algorithms (LCA) for two-coloring of
$k$-uniform hypergraphs. We focus on hypergraph instances that satisfy
strengthened assumption of the Lov\'{a}sz Local Lemma of the form $2^{1-\alpha
k} (\Delta+1) \mathrm{e} &lt; 1$, where $\Delta$ is the bound on the maximum edge
degree. The main question which arises here is for how large $\alpha$ there
exists an LCA that is able to properly color such hypergraphs in
polylogarithmic time per query. We describe briefly how upgrading the classical
sequential procedure of Beck from 1991 with Moser and Tardos' RESAMPLE yields
polylogarithmic LCA that works for $\alpha$ up to $1/4$. Then, we present an
improved procedure that solves wider range of instances by allowing $\alpha$ up
to $1/3$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dorobisz_A/0/1/0/all/0/1">Andrzej Dorobisz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kozik_J/0/1/0/all/0/1">Jakub Kozik</a></p><p>We investigate local computation algorithms (LCA) for two-coloring of
$k$-uniform hypergraphs. We focus on hypergraph instances that satisfy
strengthened assumption of the Lov\'{a}sz Local Lemma of the form $2^{1-\alpha
k} (\Delta+1) \mathrm{e} &lt; 1$, where $\Delta$ is the bound on the maximum edge
degree. The main question which arises here is for how large $\alpha$ there
exists an LCA that is able to properly color such hypergraphs in
polylogarithmic time per query. We describe briefly how upgrading the classical
sequential procedure of Beck from 1991 with Moser and Tardos' RESAMPLE yields
polylogarithmic LCA that works for $\alpha$ up to $1/4$. Then, we present an
improved procedure that solves wider range of instances by allowing $\alpha$ up
to $1/3$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02854'>Near-Optimal Compact Routing and Decomposition Schemes for Planar Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jinfeng Dou, Thorsten G&#xf6;tte, Henning Hillebrandt, Christian Scheideler, Julian Werthmann</p><p>We consider the problem of computing compact routing tables for a (weighted)
planar graph $G:= (V, E,w)$ in the PRAM, CONGEST, and the novel HYBRID
communication model. We present algorithms with polylogarithmic work and
communication that are almost optimal in all relevant parameters, i.e.,
computation time, table sizes, and stretch. All algorithms are heavily
randomized, and all our bounds hold w.h.p. For a given parameter $\epsilon&gt;0$,
our scheme computes labels of size $\widetilde{O}(\epsilon^{-1})$ and is
computed in $\widetilde{O}(\epsilon^{-2})$ time and $\widetilde{O}(n)$ work in
the PRAM and a HYBRID model and $\widetilde{O}(\epsilon^{-2} \cdot HD)$ (Here,
$HD$ denotes the network's hop-diameter) time in CONGEST. The stretch of the
resulting routing scheme is $1+\epsilon$. To achieve these results, we extend
the divide-and-conquer framework of Li and Parter [STOC '19] and combine it
with state-of-the-art distributed distance approximation algorithms [STOC '22].
Furthermore, we provide a distributed decomposition scheme, which may be of
independent interest.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dou_J/0/1/0/all/0/1">Jinfeng Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gotte_T/0/1/0/all/0/1">Thorsten G&#xf6;tte</a>, <a href="http://arxiv.org/find/cs/1/au:+Hillebrandt_H/0/1/0/all/0/1">Henning Hillebrandt</a>, <a href="http://arxiv.org/find/cs/1/au:+Scheideler_C/0/1/0/all/0/1">Christian Scheideler</a>, <a href="http://arxiv.org/find/cs/1/au:+Werthmann_J/0/1/0/all/0/1">Julian Werthmann</a></p><p>We consider the problem of computing compact routing tables for a (weighted)
planar graph $G:= (V, E,w)$ in the PRAM, CONGEST, and the novel HYBRID
communication model. We present algorithms with polylogarithmic work and
communication that are almost optimal in all relevant parameters, i.e.,
computation time, table sizes, and stretch. All algorithms are heavily
randomized, and all our bounds hold w.h.p. For a given parameter $\epsilon&gt;0$,
our scheme computes labels of size $\widetilde{O}(\epsilon^{-1})$ and is
computed in $\widetilde{O}(\epsilon^{-2})$ time and $\widetilde{O}(n)$ work in
the PRAM and a HYBRID model and $\widetilde{O}(\epsilon^{-2} \cdot HD)$ (Here,
$HD$ denotes the network's hop-diameter) time in CONGEST. The stretch of the
resulting routing scheme is $1+\epsilon$. To achieve these results, we extend
the divide-and-conquer framework of Li and Parter [STOC '19] and combine it
with state-of-the-art distributed distance approximation algorithms [STOC '22].
Furthermore, we provide a distributed decomposition scheme, which may be of
independent interest.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02922'>Coloring tournaments with few colors: Algorithms and complexity</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Felix Klingelhoefer, Alantha Newman</p><p>A k-coloring of a tournament is a partition of its vertices into k acyclic
sets. Deciding if a tournament is 2-colorable is NP-hard. A natural problem,
akin to that of coloring a 3-colorable graph with few colors, is to color a
2-colorable tournament with few colors. This problem does not seem to have been
addressed before, although it is a special case of coloring a 2-colorable
3-uniform hypergraph with few colors, which is a well-studied problem with
super-constant lower bounds.
</p>
<p>We present an efficient decomposition lemma for tournaments and show that it
can be used to design polynomial-time algorithms to color various classes of
tournaments with few colors, including an algorithm to color a 2-colorable
tournament with ten colors. For the classes of tournaments considered, we
complement our upper bounds with strengthened lower bounds, painting a
comprehensive picture of the algorithmic and complexity aspects of coloring
tournaments.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Klingelhoefer_F/0/1/0/all/0/1">Felix Klingelhoefer</a>, <a href="http://arxiv.org/find/cs/1/au:+Newman_A/0/1/0/all/0/1">Alantha Newman</a></p><p>A k-coloring of a tournament is a partition of its vertices into k acyclic
sets. Deciding if a tournament is 2-colorable is NP-hard. A natural problem,
akin to that of coloring a 3-colorable graph with few colors, is to color a
2-colorable tournament with few colors. This problem does not seem to have been
addressed before, although it is a special case of coloring a 2-colorable
3-uniform hypergraph with few colors, which is a well-studied problem with
super-constant lower bounds.
</p>
<p>We present an efficient decomposition lemma for tournaments and show that it
can be used to design polynomial-time algorithms to color various classes of
tournaments with few colors, including an algorithm to color a 2-colorable
tournament with ten colors. For the classes of tournaments considered, we
complement our upper bounds with strengthened lower bounds, painting a
comprehensive picture of the algorithmic and complexity aspects of coloring
tournaments.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02946'>What Else Can Voronoi Diagrams Do For Diameter In Planar Graphs?</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Amir Abboud, Shay Mozes, Oren Weimann</p><p>The Voronoi diagrams technique was introduced by Cabello to compute the
diameter of planar graphs in subquadratic time. We present novel applications
of this technique in static, fault-tolerant, and partially-dynamic undirected
unweighted planar graphs, as well as some new limitations.
</p>
<p>1. In the static case, we give $n^{3+o(1)}/D^2$ and $\tilde{O}(n\cdot D^2)$
time algorithms for computing the diameter of a planar graph $G$ with diameter
$D$. These are faster than the state of the art $\tilde{O}(n^{5/3})$ when
$D&lt;n^{1/3}$ or $D&gt;n^{2/3}$.
</p>
<p>2. In the fault-tolerant setting, we give an $n^{7/3+o(1)}$ time algorithm
for computing the diameter of $G\setminus \{e\}$ for every edge $e$ in $G$ (the
replacement diameter problem). Compared to the naive $\tilde{O}(n^{8/3})$ time
algorithm that runs the static algorithm for every edge.
</p>
<p>3. In the incremental setting, where we wish to maintain the diameter while
while adding edges, we present an algorithm with total running time
$n^{7/3+o(1)}$. Compared to the naive $\tilde{O}(n^{8/3})$ time algorithm that
runs the static algorithm after every update.
</p>
<p>4. We give a lower bound (conditioned on the SETH) ruling out an amortized
$O(n^{1-\varepsilon})$ update time for maintaining the diameter in {\em
weighted} planar graph. The lower bound holds even for incremental or
decremental updates.
</p>
<p>Our upper bounds are obtained by novel uses and manipulations of Voronoi
diagrams. These include maintaining the Voronoi diagram when edges of the graph
are deleted, allowing the sites of the Voronoi diagram to lie on a BFS tree
level (rather than on boundaries of $r$-division), and a new reduction from
incremental diameter to incremental {\em distance oracles} that could be of
interest beyond planar graphs. Our lower bound is the first lower bound for a
dynamic planar graph problem that is conditioned on the SETH.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Abboud_A/0/1/0/all/0/1">Amir Abboud</a>, <a href="http://arxiv.org/find/cs/1/au:+Mozes_S/0/1/0/all/0/1">Shay Mozes</a>, <a href="http://arxiv.org/find/cs/1/au:+Weimann_O/0/1/0/all/0/1">Oren Weimann</a></p><p>The Voronoi diagrams technique was introduced by Cabello to compute the
diameter of planar graphs in subquadratic time. We present novel applications
of this technique in static, fault-tolerant, and partially-dynamic undirected
unweighted planar graphs, as well as some new limitations.
</p>
<p>1. In the static case, we give $n^{3+o(1)}/D^2$ and $\tilde{O}(n\cdot D^2)$
time algorithms for computing the diameter of a planar graph $G$ with diameter
$D$. These are faster than the state of the art $\tilde{O}(n^{5/3})$ when
$D&lt;n^{1/3}$ or $D&gt;n^{2/3}$.
</p>
<p>2. In the fault-tolerant setting, we give an $n^{7/3+o(1)}$ time algorithm
for computing the diameter of $G\setminus \{e\}$ for every edge $e$ in $G$ (the
replacement diameter problem). Compared to the naive $\tilde{O}(n^{8/3})$ time
algorithm that runs the static algorithm for every edge.
</p>
<p>3. In the incremental setting, where we wish to maintain the diameter while
while adding edges, we present an algorithm with total running time
$n^{7/3+o(1)}$. Compared to the naive $\tilde{O}(n^{8/3})$ time algorithm that
runs the static algorithm after every update.
</p>
<p>4. We give a lower bound (conditioned on the SETH) ruling out an amortized
$O(n^{1-\varepsilon})$ update time for maintaining the diameter in {\em
weighted} planar graph. The lower bound holds even for incremental or
decremental updates.
</p>
<p>Our upper bounds are obtained by novel uses and manipulations of Voronoi
diagrams. These include maintaining the Voronoi diagram when edges of the graph
are deleted, allowing the sites of the Voronoi diagram to lie on a BFS tree
level (rather than on boundaries of $r$-division), and a new reduction from
incremental diameter to incremental {\em distance oracles} that could be of
interest beyond planar graphs. Our lower bound is the first lower bound for a
dynamic planar graph problem that is conditioned on the SETH.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02987'>Convergence to Lexicographically Optimal Base in a (Contra)Polymatroid and Applications to Densest Subgraph and Tree Packing</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Elfarouk Harb, Kent Quanrud, Chandra Chekuri</p><p>Boob et al. [1] described an iterative peeling algorithm called Greedy++ for
the Densest Subgraph Problem (DSG) and conjectured that it converges to an
optimum solution. Chekuri, Quanrud, and Torres [2] extended the algorithm to
general supermodular density problems (of which DSG is a special case) and
proved that the resulting algorithm Super-Greedy++ (and hence also Greedy++)
converges. In this paper, we revisit the convergence proof and provide a
different perspective. This is done via a connection to Fujishige's quadratic
program for finding a lexicographically optimal base in a (contra)polymatroid
[3], and a noisy version of the Frank-Wolfe method from convex optimisation
[4,5]. This gives us a simpler convergence proof, and also shows a stronger
property that Super-Greedy++ converges to the optimal dense decomposition
vector, answering a question raised in Harb et al. [6]. A second contribution
of the paper is to understand Thorup's work on ideal tree packing and greedy
tree packing [7,8] via the Frank-Wolfe algorithm applied to find a
lexicographically optimum base in the graphic matroid. This yields a simpler
and transparent proof. The two results appear disparate but are unified via
Fujishige's result and convex optimisation.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Harb_E/0/1/0/all/0/1">Elfarouk Harb</a>, <a href="http://arxiv.org/find/cs/1/au:+Quanrud_K/0/1/0/all/0/1">Kent Quanrud</a>, <a href="http://arxiv.org/find/cs/1/au:+Chekuri_C/0/1/0/all/0/1">Chandra Chekuri</a></p><p>Boob et al. [1] described an iterative peeling algorithm called Greedy++ for
the Densest Subgraph Problem (DSG) and conjectured that it converges to an
optimum solution. Chekuri, Quanrud, and Torres [2] extended the algorithm to
general supermodular density problems (of which DSG is a special case) and
proved that the resulting algorithm Super-Greedy++ (and hence also Greedy++)
converges. In this paper, we revisit the convergence proof and provide a
different perspective. This is done via a connection to Fujishige's quadratic
program for finding a lexicographically optimal base in a (contra)polymatroid
[3], and a noisy version of the Frank-Wolfe method from convex optimisation
[4,5]. This gives us a simpler convergence proof, and also shows a stronger
property that Super-Greedy++ converges to the optimal dense decomposition
vector, answering a question raised in Harb et al. [6]. A second contribution
of the paper is to understand Thorup's work on ideal tree packing and greedy
tree packing [7,8] via the Frank-Wolfe algorithm applied to find a
lexicographically optimum base in the graphic matroid. This yields a simpler
and transparent proof. The two results appear disparate but are unified via
Fujishige's result and convex optimisation.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.03000'>Ranking and unranking bordered and unbordered words</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Daniel Gabric</p><p>A \emph{border} of a word $w$ is a word that is both a non-empty proper
prefix and suffix of $w$. If $w$ has a border, then it is said to be
\emph{bordered}; otherwise, it is said to be \emph{unbordered}. The main
results of this paper are the first algorithms to rank and unrank length-$n$
bordered and unbordered words over a $k$-letter alphabet. We show that ranking
bordered and unbordered words can be done in $O(kn^3)$ time using $O(n)$ space,
and unranking them can be done in $O(n^4k\log k)$ time using $O(n)$ space.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gabric_D/0/1/0/all/0/1">Daniel Gabric</a></p><p>A \emph{border} of a word $w$ is a word that is both a non-empty proper
prefix and suffix of $w$. If $w$ has a border, then it is said to be
\emph{bordered}; otherwise, it is said to be \emph{unbordered}. The main
results of this paper are the first algorithms to rank and unrank length-$n$
bordered and unbordered words over a $k$-letter alphabet. We show that ranking
bordered and unbordered words can be done in $O(kn^3)$ time using $O(n)$ space,
and unranking them can be done in $O(n^4k\log k)$ time using $O(n)$ space.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-05T00:30:00Z">Friday, May 05 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Thursday, May 04
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://rjlipton.wpcomstaging.com/2023/05/04/a-ttic-talk/'>A TTIC Talk</a></h3>
        <p class='tr-article-feed'>from <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Julia Chuzhoy is a professor at TTIC&#8212;the Toyota Technological Institute at Chicago. She is giving a talk this Friday at TTIC, 6045 S. Kenwood Avenue in the 5th Floor, Room 530. Her title is, &#8220;On Fixing Some Issues with Expanders.&#8221; An associated paper by her, however, has a much longer title. Ken and I wrote [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>
Julia Chuzhoy is a professor at <a href="https://home.ttic.edu/~cjulia/">TTIC</a>&#8212;the Toyota Technological Institute at Chicago. She is giving a talk this Friday at TTIC, 6045 S. Kenwood Avenue in the 5th Floor, Room 530. Her title is, &#8220;On Fixing Some Issues with Expanders.&#8221; An associated <a href="https://home.ttic.edu/~cjulia/papers/APSP-expanders.pdf">paper</a> by her, however, has a much longer title. </p>
<p><P><br />
<a href="https://rjlipton.wpcomstaging.com/2023/05/04/a-ttic-talk/jc-2/" rel="attachment wp-att-21600"><img data-attachment-id="21600" data-permalink="https://rjlipton.wpcomstaging.com/2023/05/04/a-ttic-talk/jc-2/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/jc.jpeg?fit=221%2C228&amp;ssl=1" data-orig-size="221,228" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="jc" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/jc.jpeg?fit=221%2C228&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/jc.jpeg?fit=221%2C228&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/jc.jpeg?resize=221%2C228&#038;ssl=1" alt="" width="221" height="228" class="aligncenter size-full wp-image-21600" data-recalc-dims="1" /></a></p>
<p><P><br />
Ken and I wrote a <a href="https://rjlipton.wpcomstaging.com/2015/06/08/minor-insights-are-useful/">post</a> on her work a while ago in 2015. That was on a joint result with Chandra Chekuri. He is the Paul and Cynthia Sayles Professor at UIUC. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2023/05/04/a-ttic-talk/cc-2/" rel="attachment wp-att-21601"><img data-attachment-id="21601" data-permalink="https://rjlipton.wpcomstaging.com/2023/05/04/a-ttic-talk/cc-2/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/cc.jpeg?fit=201%2C251&amp;ssl=1" data-orig-size="201,251" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cc" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/cc.jpeg?fit=201%2C251&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/cc.jpeg?fit=201%2C251&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/cc.jpeg?resize=160%2C200&#038;ssl=1" alt="" width="160" height="200" class="aligncenter wp-image-21601" data-recalc-dims="1" /></a></p>
<p><H2> A Well-Connected Talk </H2></p>
<p><p>
I am visiting TTIC till Friday and have to then return to home. So I will have to miss her talk. I hope it is well-connected enough that some of it gets back to me. Let me know how it goes. </p>
<p>
I believe the main thrust of her talk should interest many since it attends to an important issue. The concept of expander graph is wonderful and has great connections to other areas of combinatorics, but the condition is costly to instantiate and verify and may go beyond what many algorithms truly need for good performance. Her method to weaken-and-sidestep the requirements, centered around a notion of <em>well-connected</em> graphs, should help extend applications of the general technology. She says:</p>
<blockquote><p><b> </b> <em> We believe that the use of well-connected graphs instead of expanders in various dynamic distance-based problems (such as APSP in general graphs) has the potential of providing much stronger guarantees, since we are no longer necessarily restricted to superlogarithmic approximation factors. </em>
</p></blockquote>
<p>
<p><H2> Open Problems </H2></p>
<p><p>
Enjoy the talk. I think the results that follow from weakening expanders could be quite powerful. </p>
<p>
<p class="authors">By rjlipton</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-04T23:25:49Z">Thursday, May 04 2023, 23:25</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/065'>TR23-065 |  From Grassmannian to Simplicial High-Dimensional Expanders | 

	Louis Golowich</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          In this paper, we present a new construction of simplicial complexes of subpolynomial degree with arbitrarily good local spectral expansion. Previously, the only known high-dimensional expanders (HDXs) with arbitrarily good expansion and less than polynomial degree were based on one of two constructions, namely Ramanujan complexes and coset complexes. In contrast, our construction is a Cayley complex over the group $\mathbb{F}_2^k$, with Cayley generating set given by a Grassmannian HDX.

  Our construction is in part motivated by a coding-theoretic interpretation of Grassmannian HDXs that we present, which provides a formal connection between Grassmannian HDXs, simplicial HDXs, and LDPC codes. We apply this interpretation to prove a general characterization of the 1-homology groups over $\mathbb{F}_2$ of Cayley simplicial complexes over $\mathbb{F}_2^k$. Using this result, we construct simplicial complexes on $N$ vertices with arbitrarily good local expansion for which the dimension of the 1-homology group grows as $\Omega(\log^2N)$. No prior constructions in the literature have been shown to achieve as large a 1-homology group.
        
        </div>

        <div class='tr-article-summary'>
        
          
          In this paper, we present a new construction of simplicial complexes of subpolynomial degree with arbitrarily good local spectral expansion. Previously, the only known high-dimensional expanders (HDXs) with arbitrarily good expansion and less than polynomial degree were based on one of two constructions, namely Ramanujan complexes and coset complexes. In contrast, our construction is a Cayley complex over the group $\mathbb{F}_2^k$, with Cayley generating set given by a Grassmannian HDX.

  Our construction is in part motivated by a coding-theoretic interpretation of Grassmannian HDXs that we present, which provides a formal connection between Grassmannian HDXs, simplicial HDXs, and LDPC codes. We apply this interpretation to prove a general characterization of the 1-homology groups over $\mathbb{F}_2$ of Cayley simplicial complexes over $\mathbb{F}_2^k$. Using this result, we construct simplicial complexes on $N$ vertices with arbitrarily good local expansion for which the dimension of the 1-homology group grows as $\Omega(\log^2N)$. No prior constructions in the literature have been shown to achieve as large a 1-homology group.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-04T13:54:49Z">Thursday, May 04 2023, 13:54</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://blog.computationalcomplexity.org/2023/05/breaking-ground-in-isomorphism-testing.html'>Breaking Ground in Isomorphism Testing: A Leap Forward for a Bottleneck Case of Group Isomorphism</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Guest post by Josh Grochow and Youming Qiao</p><p>There has, quietly, been somewhat of a breakthrough in isomorphism testing. No, not as big as Babai's 2016 Graph Isomorphism in Quasipolynomial Time. But a first foothold in climbing a wall for which no one had gotten much off the ground before. The result, due to Xiaorui Sun in this year's STOC, is an algorithm for testing isomorphism of a certain class of groups - p-groups of class 2 and exponent p if you must know, but we'll get to that - in time \(n^{O(\log^{5/6} n)}\) where n is the order of the group. To understand why we're excited about this we have to tell a bit of a story.&nbsp;</p><p>In the 1970s, when Graph Isomorphism was still a mystery, people also thought more widely about isomorphism testing of other combinatorial and algebraic structures. For finite groups of order n, Robert Tarjan realized that there is an \(n^{\log n+O(1)}\)-time algorithm, simply because a group of order n has a generating set of size \(\log n\). This observation was recorded by Gary Miller in a paper in STOC'78, and independently realized by Felsch and Neubüser. A natural question is then whether Group Isomorphism can be solved in time poly(n) where n is the group order.</p><br><p>Not only is this question natural from the perspective of studying groups computationally, it is also natural from the perspective of Graph Isomorphism. For Group Isomorphism reduces to Graph Isomorphism in polynomial-time (as does the isomorphism problem for any finite algebraic or relational structure, see Zemlyachenko, Korneenko, &amp; Tyshkevich). While this has been known for a long time, Babai’s result on Graph Isomorphism brings the running times quite close: \(n^{O(\log^2 n)}\) for graphs, and \(n^{O(\log n)}\) for groups. So not only does Group Isomorphism stand in the way of getting Graph Isomorphism into P, but in our current state of knowledge, it even stands in the way of shaving off more than a single log in the exponent of the runtime.</p><br><p>Since the general Group Isomorphism problem seems difficult, attention turned to special classes of groups. It was not hard to see that isomorphism of Abelian groups could be computed in polynomial time. However, a group class that is just “one step away” from Abelian - groups G where, when you mod out by the center Z(G), what’s left is Abelian -&nbsp; turned out to be difficult. Such groups are called class-2 nilpotent, and in one sense, their&nbsp; group-theoretic structure is relatively straightforward: both G/Z(G) and Z(G) are Abelian. Yet, to devise an efficient isomorphism testing procedure turned out to be extremely difficult (see e.g. Garzon-Zalcstein, Rosenbaum-Wagner, O’Brien, Wilson), to the point that this is usually considered as a bottleneck for putting Group Isomorphism in P.&nbsp;</p><br><p>Among class-2 nilpotent groups, the “key case” to resolve is widely believed, for several reasons, to be p-groups of class 2 and exponent p. In such groups, both the center Z(G) and quotient G/Z(G) are elementary abelian, i.e., of the form \((Z_p)^d\). Despite having an even simpler group-theoretic structure, this group class still turns out to be difficult! For a long time, the asymptotic growth of the exponent of the runtime for solving this restricted problem has not improved over the \(n^{\log n+O(1)}\)-time algorithm, which works for all groups.1</p><br><p>Xiaorui Sun’s result represents the first substantial improvement, cracking open this decades-old quest. His algorithm runs in time \(n^{O(\log^{5/6} n)}\), and its techniques are indeed novel. The starting point of this algorithm is to consider the following equivalent problem in (multi)linear algebra: let \(f, g:Z_p^d \times Z_p^d \rightarrow Z_p^e\) be two skew-symmetric bilinear maps. Do there exist change of bases A in \(GL(d, p)\) and B in \(GL(e, p)\), such that for all \(u, v\) in \(Z_p^d\), \(f(A(u), A(v))=B(g(u, v))\)?</p><br><p>Baer’s Correspondence sets up an equivalence of categories between p-groups of class 2 and exponent p, and skew-symmetric bilinear maps over \(Z_p\). This viewpoint allows Xiaorui to use multilinear algebra to study the structure of these bilinear maps. He also crucially depends on a result of Ivanyos and Qiao, which built on Wilson’s use of involutive algebras in this context. He also uses the individualization-and-refinement technique (but for matrix spaces, not graphs!), a characterization of spaces of matrices of low rank, and reducing a tensor to a “semi-canonical” form part of which is somewhat reminiscent of the Tucker decomposition.</p><br><p>All this results in an algorithm which solves the above problem on bilinear maps in time \(p^{(d+e)^{1.8} \log p}\). For groups of order \(p^n\) with \(\log_p(n)\) larger than \(\log^5 p\), Baer’s Correspondence then says that this algorithm does it; when \(\log_p n\) is smaller than \(log^5 p,\) he can fall back on the generator-enumerator algorithm, since the number of generators is at most \(log_p n\).</p><br><p>For us, who have been working on Group Isomorphism for more than a decade, Xiaorui’s result represents an exciting development on this classic algorithmic problem, and we look forward to seeing more progress in this direction in the near future.&nbsp;</p><p></p>1Rosenbaum &amp; Wagner improved the exponent to \(\frac{1}{2}\log {p(n)} + O(1)\), and later improved to \(\frac{1}{4}\log {p(n)} + O(1)\) for all groups, see p.5 of Le Gall &amp; Rosenbaum. In 2014, at a conference on Groups, Computation, and Geometry organized by Wilson, Brooksbank, Hulpke, Kantor, and Penttila, it was concluded that modern practical methods, such as those used in GAP and MAGMA, still take \(n^{O(\log n)}\) steps in the worst case.<p></p><p>By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;"><i>Guest post by Josh Grochow and </i></span><span style="font-family: Arial;"><span style="font-size: 14.6667px; white-space: pre-wrap;"><i>Youming Qiao</i></span></span></p><p><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">There has, quietly, been somewhat of a breakthrough in isomorphism testing. No, not as big as Babai's 2016 </span><a href="https://arxiv.org/abs/1512.03547" style="text-decoration-line: none;"><span style="color: #1155cc; font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; text-decoration-line: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">Graph Isomorphism in Quasipolynomial Time</span></a><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">. But a first foothold in climbing a wall for which no one had gotten much off the ground before. The result, due to </span><a href="https://arxiv.org/abs/2303.15412" style="text-decoration-line: none;"><span style="color: #1155cc; font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; text-decoration-line: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">Xiaorui Sun in this year's STOC</span></a><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">, is an algorithm for testing isomorphism of a certain class of groups - p-groups of class 2 and exponent p if you must know, but we'll get to that - in time \(n^{O(\log^{5/6} n)}\) where n is the order of the group. To understand why we're excited about this we have to tell a bit of a story.&nbsp;</span></p><span id="docs-internal-guid-259ecb6b-7fff-b1a3-e5ab-283b928791b9"><p dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">In the 1970s, when Graph Isomorphism was still a mystery, people also thought more widely about isomorphism testing of other combinatorial and algebraic structures. For finite groups of order n, Robert Tarjan realized that there is an \(n^{\log n+O(1)}\)-time algorithm, simply because a group of order n has a generating set of size \(\log n\). This observation was recorded by Gary Miller in a </span><a href="https://doi.org/10.1145/800133.804331" style="text-decoration-line: none;"><span style="color: #1155cc; font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; text-decoration-line: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">paper in STOC'78</span></a><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">, and independently realized by </span><a href="https://www.sciencedirect.com/science/article/abs/pii/B9780080129754500114" style="text-decoration-line: none;"><span style="color: #1155cc; font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; text-decoration-line: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">Felsch and Neubüser</span></a><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">. A natural question is then whether Group Isomorphism can be solved in time poly(n) where n is the group order.</span></p><br /><p dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">Not only is this question natural from the perspective of studying groups computationally, it is also natural from the perspective of </span><span style="font-family: Arial; font-size: 11pt; font-style: italic; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">Graph</span><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;"> Isomorphism. For Group Isomorphism reduces to Graph Isomorphism in polynomial-time (as does the isomorphism problem for any finite algebraic or relational structure, see </span><a href="https://doi.org/10.1007/BF02104746" style="text-decoration-line: none;"><span style="color: #1155cc; font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; text-decoration-line: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">Zemlyachenko, Korneenko, &amp; Tyshkevich</span></a><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">). While this has been known for a long time, Babai’s result on Graph Isomorphism brings the running times quite close: \(n^{O(\log^2 n)}\) for graphs, and \(n^{O(\log n)}\) for groups. So not only does Group Isomorphism stand in the way of getting Graph Isomorphism into P, but in our current state of knowledge, it even stands in the way of shaving off more than a single log in the exponent of the runtime.</span></p><br /><p dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">Since the general Group Isomorphism problem seems difficult, attention turned to special classes of groups. It was not hard to see that isomorphism of Abelian groups could be computed in polynomial time. However, a group class that is just “one step away” from Abelian - groups G where, when you mod out by the center Z(G), what’s left is Abelian -&nbsp; turned out to be difficult. Such groups are called class-2 nilpotent, and in one sense, their&nbsp; group-theoretic structure is relatively straightforward: both G/Z(G) and Z(G) are Abelian. Yet, to devise an efficient isomorphism testing procedure turned out to be extremely difficult (see e.g. </span><a href="https://doi.org/10.1016/0022-0000(91)90012-T" style="text-decoration-line: none;"><span style="color: #1155cc; font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; text-decoration-line: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">Garzon-Zalcstein</span></a><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">, </span><a href="https://doi.org/10.1016/j.tcs.2015.05.036" style="text-decoration-line: none;"><span style="color: #1155cc; font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; text-decoration-line: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">Rosenbaum-Wagner</span></a><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">, </span><a href="https://www.math.auckland.ac.nz/~obrien/research/isom.pdf" style="text-decoration-line: none;"><span style="color: #1155cc; font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; text-decoration-line: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">O’Brien</span></a><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">, </span><a href="https://www.sciencedirect.com/science/article/pii/S0021869309004463" style="text-decoration-line: none;"><span style="color: #1155cc; font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; text-decoration-line: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">Wilson</span></a><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">), to the point that this is usually considered as a bottleneck for putting Group Isomorphism in P.&nbsp;</span></p><br /><p dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">Among class-2 nilpotent groups, the “key case” to resolve is widely believed, </span><a href="https://cstheory.stackexchange.com/a/42551/129" style="text-decoration-line: none;"><span style="color: #1155cc; font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; text-decoration-line: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">for several reasons</span></a><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">, to be p-groups of class 2 and exponent p. In such groups, both the center Z(G) and quotient G/Z(G) are elementary abelian, i.e., of the form \((Z_p)^d\). Despite having an even simpler group-theoretic structure, this group class still turns out to be difficult! For a long time, the asymptotic growth of the exponent of the runtime for solving this restricted problem has not improved over the \(n^{\log n+O(1)}\)-time algorithm, which works for all groups.<sup>1</sup></span></p><br /><p dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">Xiaorui Sun’s result represents the first substantial improvement, cracking open this decades-old quest. His algorithm runs in time \(n^{O(\log^{5/6} n)}\), and its techniques are indeed novel. The starting point of this algorithm is to consider the following equivalent problem in (multi)linear algebra: let \(f, g:Z_p^d \times Z_p^d \rightarrow Z_p^e\) be two skew-symmetric bilinear maps. Do there exist change of bases A in \(GL(d, p)\) and B in \(GL(e, p)\), such that for all \(u, v\) in \(Z_p^d\), \(f(A(u), A(v))=B(g(u, v))\)?</span></p><br /><p dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><a href="https://www.jstor.org/stable/1989886" style="text-decoration-line: none;"><span style="color: #1155cc; font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; text-decoration-line: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">Baer’s Correspondence</span></a><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;"> sets up an equivalence of categories between p-groups of class 2 and exponent p, and skew-symmetric bilinear maps over \(Z_p\). This viewpoint allows Xiaorui to use multilinear algebra to study the structure of these bilinear maps. He also crucially depends on a result of </span><a href="https://doi.org/10.1137/18M1165682" style="text-decoration-line: none;"><span style="color: #1155cc; font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; text-decoration-line: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">Ivanyos and Qiao</span></a><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">, which built on </span><a href="https://doi.org/10.1016/j.jalgebra.2009.07.029" style="text-decoration-line: none;"><span style="color: #1155cc; font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; text-decoration-line: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">Wilson’s use</span></a><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;"> of involutive algebras in this context. He also uses the individualization-and-refinement technique (but for matrix spaces, not graphs!), a characterization of spaces of matrices of low rank, and reducing a tensor to a “semi-canonical” form part of which is somewhat reminiscent of the Tucker decomposition.</span></p><br /><p dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">All this results in an algorithm which solves the above problem on bilinear maps in time \(p^{(d+e)^{1.8} \log p}\). For groups of order \(p^n\) with \(\log_p(n)\) larger than \(\log^5 p\), Baer’s Correspondence then says that this algorithm does it; when \(\log_p n\) is smaller than \(log^5 p,\) he can fall back on the generator-enumerator algorithm, since the number of generators is at most \(log_p n\).</span></p><br /><p dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;">For us, who have been working on Group Isomorphism for more than a decade, Xiaorui’s result represents an exciting development on this classic algorithmic problem, and we look forward to seeing more progress in this direction in the near future.&nbsp;</span></p><p dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;"><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline;"></span></span></p><hr /><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;"><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline;"><sup>1</sup></span><a href="https://doi.org/10.1016/j.tcs.2015.05.036" style="font-family: &quot;Times New Roman&quot;; font-size: medium; text-decoration-line: none; white-space: normal;"><span style="color: #1155cc; font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; text-decoration-line: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">Rosenbaum &amp; Wagner</span></a><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline;"> improved the exponent to \(\frac{1}{2}\log {p(n)} + O(1)\), and later improved to \(\frac{1}{4}\log {p(n)} + O(1)\) for all groups, see p.5 of </span><a href="https://arxiv.org/abs/1609.08253" style="font-family: &quot;Times New Roman&quot;; font-size: medium; text-decoration-line: none; white-space: normal;"><span style="color: #1155cc; font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; text-decoration-line: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">Le Gall &amp; Rosenbaum</span></a><span style="font-family: Arial; font-size: 11pt; font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline;">. In 2014, at a conference on Groups, Computation, and Geometry organized by Wilson, Brooksbank, Hulpke, Kantor, and Penttila, it was concluded that modern practical methods, such as those used in GAP and MAGMA, still take \(n^{O(\log n)}\) steps in the worst case.</span></span><p></p></span><p class="authors">By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-04T11:12:00Z">Thursday, May 04 2023, 11:12</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2023/05/04/phd-student-at-department-of-computer-and-information-science-linkoping-university-apply-by-may-28-2023/'>PhD Student at Department of Computer and Information Science, Linköping University (apply by May 28, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Linköping University advertises one (1) position as PhD student in Computer Science. The PhD student will be supervised by prof. Peter Jonsson. The research for the advertised position is in the area of parameterized complexity of constraint satisfaction problems (CSP). Website: liu.se/en/work-at-liu/vacancies/21872 Email: peter.jonsson@liu.se
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Linköping University advertises one (1) position as PhD student in Computer Science. The PhD student will be supervised by prof. Peter Jonsson. The research for the advertised position is in the area of parameterized complexity of constraint satisfaction problems (CSP).</p>
<p>Website: <a href="https://liu.se/en/work-at-liu/vacancies/21872">https://liu.se/en/work-at-liu/vacancies/21872</a><br />
Email: peter.jonsson@liu.se</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-04T09:18:29Z">Thursday, May 04 2023, 09:18</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.01721'>Construction of Decision Trees and Acyclic Decision Graphs from Decision Rule Systems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Kerven Durdymyradov, Mikhail Moshkov</p><p>Decision trees and systems of decision rules are widely used as classifiers,
as a means for knowledge representation, and as algorithms. They are among the
most interpretable models for data analysis. The study of the relationships
between these two models can be seen as an important task of computer science.
Methods for transforming decision trees into systems of decision rules are
simple and well-known. In this paper, we consider the inverse transformation
problem, which is not trivial. We study the complexity of constructing decision
trees and acyclic decision graphs representing decision trees from decision
rule systems, and we discuss the possibility of not building the entire
decision tree, but describing the computation path in this tree for the given
input.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Durdymyradov_K/0/1/0/all/0/1">Kerven Durdymyradov</a>, <a href="http://arxiv.org/find/cs/1/au:+Moshkov_M/0/1/0/all/0/1">Mikhail Moshkov</a></p><p>Decision trees and systems of decision rules are widely used as classifiers,
as a means for knowledge representation, and as algorithms. They are among the
most interpretable models for data analysis. The study of the relationships
between these two models can be seen as an important task of computer science.
Methods for transforming decision trees into systems of decision rules are
simple and well-known. In this paper, we consider the inverse transformation
problem, which is not trivial. We study the complexity of constructing decision
trees and acyclic decision graphs representing decision trees from decision
rule systems, and we discuss the possibility of not building the entire
decision tree, but describing the computation path in this tree for the given
input.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-04T00:30:00Z">Thursday, May 04 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.01851'>Complexity and Enumeration in Models of Genome Rearrangement</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Lora Bailey, Heather Smith Blake, Garner Cochran, Nathan Fox, Michael Levet, Reem Mahmoud, Elizabeth Matson, Inne Singgih, Grace Stadnyk, Xinyi Wang, Alexander Widemann</p><p>In this paper, we examine the computational complexity of enumeration in
certain genome rearrangement models. We first show that the Pairwise
Rearrangement problem in the Single Cut-and-Join model (Bergeron, Medvedev, &amp;
Stoye, J. Comput. Biol. 2010) is $\#\textsf{P}$-complete under polynomial-time
Turing reductions. Next, we show that in the Single Cut or Join model (Feijao &amp;
Meidanis, IEEE ACM Trans. Comp. Biol. Bioinf. 2011), the problem of enumerating
all medians ($\#$Median) is logspace-computable ($\textsf{FL}$), improving upon
the previous polynomial-time ($\textsf{FP}$) bound of Mikl\'os &amp; Smith (RECOMB
2015).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/q-bio/1/au:+Bailey_L/0/1/0/all/0/1">Lora Bailey</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Blake_H/0/1/0/all/0/1">Heather Smith Blake</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Cochran_G/0/1/0/all/0/1">Garner Cochran</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Fox_N/0/1/0/all/0/1">Nathan Fox</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Levet_M/0/1/0/all/0/1">Michael Levet</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Mahmoud_R/0/1/0/all/0/1">Reem Mahmoud</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Matson_E/0/1/0/all/0/1">Elizabeth Matson</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Singgih_I/0/1/0/all/0/1">Inne Singgih</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Stadnyk_G/0/1/0/all/0/1">Grace Stadnyk</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wang_X/0/1/0/all/0/1">Xinyi Wang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Widemann_A/0/1/0/all/0/1">Alexander Widemann</a></p><p>In this paper, we examine the computational complexity of enumeration in
certain genome rearrangement models. We first show that the Pairwise
Rearrangement problem in the Single Cut-and-Join model (Bergeron, Medvedev, &amp;
Stoye, J. Comput. Biol. 2010) is $\#\textsf{P}$-complete under polynomial-time
Turing reductions. Next, we show that in the Single Cut or Join model (Feijao &amp;
Meidanis, IEEE ACM Trans. Comp. Biol. Bioinf. 2011), the problem of enumerating
all medians ($\#$Median) is logspace-computable ($\textsf{FL}$), improving upon
the previous polynomial-time ($\textsf{FP}$) bound of Mikl\'os &amp; Smith (RECOMB
2015).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-04T00:30:00Z">Thursday, May 04 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.02226'>$P\not=NP$ relative to a $P$-complete oracle</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Reiner Czerwinski</p><p>The $P$ versus $NP$ problem is still unsolved. But there are several oracles
with $P$ unequal $NP$ relative to them. Here we will prove, that $P\not=NP$
relative to a $P$-complete oracle. In this paper, we use padding arguments as
the proof method. The padding arguments are not bounded by a computable
function. Such as we can use methods from computability theory to separate
complexity classes.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Czerwinski_R/0/1/0/all/0/1">Reiner Czerwinski</a></p><p>The $P$ versus $NP$ problem is still unsolved. But there are several oracles
with $P$ unequal $NP$ relative to them. Here we will prove, that $P\not=NP$
relative to a $P$-complete oracle. In this paper, we use padding arguments as
the proof method. The padding arguments are not bounded by a computable
function. Such as we can use methods from computability theory to separate
complexity classes.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-04T00:30:00Z">Thursday, May 04 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.01877'>The Impacts of Dimensionality, Diffusion, and Directedness on Intrinsic Cross-Model Simulation in Tile-Based Self-Assembly</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Daniel Hader, Matthew J. Patitz</p><p>Algorithmic self-assembly occurs when disorganized components autonomously
combine to form structures and, by their design and the dynamics of the system,
are forced to follow the execution of algorithms. Motivated by applications in
DNA-nanotechnology, investigations in algorithmic tile-based self-assembly have
blossomed into a mature theory with research leveraging tools from
computability theory, complexity theory, information theory, and graph theory
to develop a wide range of models and show that many are computationally
universal, while also exposing powers and limitations of each. Beyond
computational universality, the abstract Tile Assembly Model (aTAM) was shown
to be intrinsically universal (IU), a strong notion of completeness where a
single tile set is capable of simulating all systems within the model; however,
this result required non-deterministic tile attachments. This was later
confirmed necessary when it was shown that the class of directed aTAM systems
is not IU. Building on these results to further investigate the impacts of
other dynamics, Hader et al. examined several tile-assembly models which varied
across (1) the numbers of dimensions used, (2) restrictions based on diffusion
of tiles through space, and (3) whether each system is directed, and showed
which models are IU. Such results have shed much light on the roles of various
aspects of the dynamics of tile-assembly and their effects on the intrinsic
universality of each model. Here we provide direct comparisons of the various
models by considering intrinsic simulations between models. We show that in
some cases one model is more powerful than another, and in others, pairs of
models have mutually exclusive capabilities. This comparison helps to expose
the impacts of these three important aspects and further helps define a
hierarchy of tile-assembly models.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Hader_D/0/1/0/all/0/1">Daniel Hader</a>, <a href="http://arxiv.org/find/cs/1/au:+Patitz_M/0/1/0/all/0/1">Matthew J. Patitz</a></p><p>Algorithmic self-assembly occurs when disorganized components autonomously
combine to form structures and, by their design and the dynamics of the system,
are forced to follow the execution of algorithms. Motivated by applications in
DNA-nanotechnology, investigations in algorithmic tile-based self-assembly have
blossomed into a mature theory with research leveraging tools from
computability theory, complexity theory, information theory, and graph theory
to develop a wide range of models and show that many are computationally
universal, while also exposing powers and limitations of each. Beyond
computational universality, the abstract Tile Assembly Model (aTAM) was shown
to be intrinsically universal (IU), a strong notion of completeness where a
single tile set is capable of simulating all systems within the model; however,
this result required non-deterministic tile attachments. This was later
confirmed necessary when it was shown that the class of directed aTAM systems
is not IU. Building on these results to further investigate the impacts of
other dynamics, Hader et al. examined several tile-assembly models which varied
across (1) the numbers of dimensions used, (2) restrictions based on diffusion
of tiles through space, and (3) whether each system is directed, and showed
which models are IU. Such results have shed much light on the roles of various
aspects of the dynamics of tile-assembly and their effects on the intrinsic
universality of each model. Here we provide direct comparisons of the various
models by considering intrinsic simulations between models. We show that in
some cases one model is more powerful than another, and in others, pairs of
models have mutually exclusive capabilities. This comparison helps to expose
the impacts of these three important aspects and further helps define a
hierarchy of tile-assembly models.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-04T00:30:00Z">Thursday, May 04 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.01883'>A Lightweight CNN-Transformer Model for Learning Traveling Salesman Problems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Minseop Jung, Jaeseung Lee, Jibum Kim</p><p>Transformer-based models show state-of-the-art performance even for
large-scale Traveling Salesman Problems (TSPs). However, they are based on
fully-connected attention models and suffer from large computational complexity
and GPU memory usage. We propose a lightweight CNN-Transformer model based on a
CNN embedding layer and partial self-attention. Our CNN-Transformer model is
able to better learn spatial features from input data using a CNN embedding
layer compared with the standard Transformer models. It also removes
considerable redundancy in fully connected attention models using the proposed
partial self-attention. Experiments show that the proposed model outperforms
other state-of-the-art Transformer-based models in terms of TSP solution
quality, GPU memory usage, and inference time. Our model consumes approximately
20% less GPU memory usage and has 45% faster inference time compared with other
state-of-the-art Transformer-based models. Our code is publicly available at
github.com/cm8908/CNN_Transformer3
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1">Minseop Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaeseung Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jibum Kim</a></p><p>Transformer-based models show state-of-the-art performance even for
large-scale Traveling Salesman Problems (TSPs). However, they are based on
fully-connected attention models and suffer from large computational complexity
and GPU memory usage. We propose a lightweight CNN-Transformer model based on a
CNN embedding layer and partial self-attention. Our CNN-Transformer model is
able to better learn spatial features from input data using a CNN embedding
layer compared with the standard Transformer models. It also removes
considerable redundancy in fully connected attention models using the proposed
partial self-attention. Experiments show that the proposed model outperforms
other state-of-the-art Transformer-based models in terms of TSP solution
quality, GPU memory usage, and inference time. Our model consumes approximately
20% less GPU memory usage and has 45% faster inference time compared with other
state-of-the-art Transformer-based models. Our code is publicly available at
https://github.com/cm8908/CNN_Transformer3
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-04T00:30:00Z">Thursday, May 04 2023, 00:30</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
