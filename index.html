<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>
  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.4 (2022-04-12) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <link rel='stylesheet' type='text/css' href='css/font-awesome.css'>
  <link rel='stylesheet' type='text/css' href='css/blank.css'>
</head>
<body>
  <div id='navwrap'>
    <div id='nav'>
      <p>
        Last Update
      </p>
      <p class='small'>
        
          <time class='timeago' datetime="2022-09-19T14:04:58Z">Monday, September 19 2022, 14:04</time>
        
      </p>

      <p>Feeds</p>
      <ul class='subscriptions small' >
      
        <li>
          <a href='http://arxiv.org/rss/cs.CC'><img src='i/feed.png'></a>
          <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
          
        </li>
      
        <li>
          <a href='http://arxiv.org/rss/cs.CG'><img src='i/feed.png'></a>
          <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
          
        </li>
      
        <li>
          <a href='http://arxiv.org/rss/cs.DS'><img src='i/feed.png'></a>
          <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
          
        </li>
      
        <li>
          <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='i/feed.png'></a>
          <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
          
        </li>
      
        <li>
          <a href='https://adamsheffer.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
          
        </li>
      
        <li>
          <a href='https://adamdsmith.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
          
        </li>
      
        <li>
          <a href='https://polylogblog.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
          
        </li>
      
        <li>
          <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='i/feed.png'></a>
          <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
          
        </li>
      
        <li>
          <a href='http://www.argmin.net/feed.xml'><img src='i/feed.png'></a>
          <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
          
        </li>
      
        <li>
          <a href='http://bit-player.org/feed/atom/'><img src='i/feed.png'></a>
          <a href='http://bit-player.org'>bit-player</a>
          
        </li>
      
        <li>
          <a href='https://cstheory-jobs.org/feed/'><img src='i/feed.png'></a>
          <a href='https://cstheory-jobs.org'>CCI: jobs</a>
          
        </li>
      
        <li>
          <a href='https://cstheory-events.org/feed/'><img src='i/feed.png'></a>
          <a href='https://cstheory-events.org'>CS Theory Events</a>
          
        </li>
      
        <li>
          <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='i/feed.png'></a>
          <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
          
        </li>
      
        <li>
          <a href='https://11011110.github.io/blog/feed.xml'><img src='i/feed.png'></a>
          <a href='https://11011110.github.io/blog/'>David Eppstein</a>
          
        </li>
      
        <li>
          <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='i/feed.png'></a>
          <a href='https://daveagp.wordpress.com'>David Pritchard</a>
          
        </li>
      
        <li>
          <a href='https://decentdescent.org/feed.xml'><img src='i/feed.png'></a>
          <a href='https://decentdescent.org/'>Decent Descent</a>
          
        </li>
      
        <li>
          <a href='https://decentralizedthoughts.github.io/feed'><img src='i/feed.png'></a>
          <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
          
        </li>
      
        <li>
          <a href='https://differentialprivacy.org/feed.xml'><img src='i/feed.png'></a>
          <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
          
        </li>
      
        <li>
          <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='i/feed.png'></a>
          <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
          
        </li>
      
        <li>
          <a href='https://emanueleviola.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
          
        </li>
      
        <li>
          <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='i/feed.png'></a>
          <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
          
        </li>
      
        <li>
          <a href='https://dstheory.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
          
        </li>
      
        <li>
          <a href='https://francisbach.com/feed/'><img src='i/feed.png'></a>
          <a href='https://francisbach.com'>Francis Bach</a>
          
        </li>
      
        <li>
          <a href='https://gilkalai.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
          
        </li>
      
        <li>
          <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='i/feed.png'></a>
          <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
          
        </li>
      
        <li>
          <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='i/feed.png'></a>
          <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
          
        </li>
      
        <li>
          <a href='https://gradientscience.org/feed.xml'><img src='i/feed.png'></a>
          <a href='https://gradientscience.org/'>Gradient Science</a>
          
        </li>
      
        <li>
          <a href='http://grigory.us/blog/feed.xml'><img src='i/feed.png'></a>
          <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
          
        </li>
      
        <li>
          <a href='https://tcsmath.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
          
        </li>
      
        <li>
          <a href='https://kamathematics.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
          
        </li>
      
        <li>
          <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='i/feed.png'></a>
          <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
          
        </li>
      
        <li>
          <a href='https://lucatrevisan.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
          
        </li>
      
        <li>
          <a href='https://mittheory.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
          
        </li>
      
        <li>
          <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='i/feed.png'></a>
          <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
          
        </li>
      
        <li>
          <a href='http://blog.mrtz.org/feed.xml'><img src='i/feed.png'></a>
          <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
          
        </li>
      
        <li>
          <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='i/feed.png'></a>
          <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
          
        </li>
      
        <li>
          <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
          
        </li>
      
        <li>
          <a href='http://www.solipsistslog.com/feed/'><img src='i/feed.png'></a>
          <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
          
        </li>
      
        <li>
          <a href='http://www.offconvex.org/feed.xml'><img src='i/feed.png'></a>
          <a href='http://offconvex.github.io/'>Off the Convex Path</a>
          
        </li>
      
        <li>
          <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='i/feed.png'></a>
          <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
          
        </li>
      
        <li>
          <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='i/feed.png'></a>
          <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
          
        </li>
      
        <li>
          <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='i/feed.png'></a>
          <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
          
        </li>
      
        <li>
          <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='i/feed.png'></a>
          <a href='https://blogs.princeton.edu/imabandit'>SÃ©bastien Bubeck</a>
          
        </li>
      
        <li>
          <a href='https://scottaaronson.blog/?feed=atom'><img src='i/feed.png'></a>
          <a href='https://scottaaronson.blog'>Scott Aaronson</a>
          
        </li>
      
        <li>
          <a href='https://blog.simons.berkeley.edu/feed/'><img src='i/feed.png'></a>
          <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
          
        </li>
      
        <li>
          <a href='https://tcsplus.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
          
        </li>
      
        <li>
          <a href='https://toc4fairness.org/feed/'><img src='i/feed.png'></a>
          <a href='https://toc4fairness.org'>TOC for Fairness</a>
          
        </li>
      
        <li>
          <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='i/feed.png'></a>
          <a href='http://blog.geomblog.org/'>The Geomblog</a>
          
        </li>
      
        <li>
          <a href='https://www.let-all.com/blog/feed/'><img src='i/feed.png'></a>
          <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
          
        </li>
      
        <li>
          <a href='https://theorydish.blog/feed/'><img src='i/feed.png'></a>
          <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
          
        </li>
      
        <li>
          <a href='https://thmatters.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://thmatters.wordpress.com'>Theory Matters</a>
          
        </li>
      
        <li>
          <a href='https://mycqstate.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
          
        </li>
      
        <li>
          <a href='https://agtb.wordpress.com/feed/'><img src='i/feed.png'></a>
          <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
          
        </li>
      
        <li>
          <a href='https://windowsontheory.org/feed/'><img src='i/feed.png'></a>
          <a href='https://windowsontheory.org'>Windows on Theory</a>
          
        </li>
      
      </ul>

      <p class='small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
      <p class='small'>Subscribe to the <a href="atom.xml">Atom feed</a> or <a href="rss20.xml">RSS feed</a> to stay up to date.</p>
      <p class='small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
      <p class='small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
    </div>
  </div>

  <div id='opts'>
    <div style='width: 100%; text-align: right;'>
    <img src='i/view-headlines.png' id='show-headlines' title='Show Headlines Only' width='24' height='24'>
    <img src='i/view-snippets.png' id='show-snippets' title='Show Snippets' width='24' height='24'>
    <img src='i/view-standard.png' id='show-fulltext' title='Show Full Text' width='24' height='24'>
    </div>
  </div>

  <h1>
    Theory of Computing Report
  </h1>

  <div id="articles">
    
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Monday, September 19
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07625'>Extremal combinatorics, iterated pigeonhole arguments, and generalizations of PPP</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Amol Pasarkar, Mihalis Yannakakis, Christos Papadimitriou</p><p>We study the complexity of computational problems arising from existence
theorems in extremal combinatorics. For some of these problems, a solution is
guaranteed to exist based on an iterated application of the Pigeonhole
Principle. This results in the definition of a new complexity class within
TFNP, which we call PLC (for "polynomial long choice"). PLC includes all of
PPP, as well as numerous previously unclassified total problems, including
search problems related to Ramsey's theorem, the Sunflower theorem, the
Erd\H{o}s-Ko-Rado lemma, and K\"onig's lemma. Whether the first two of these
four problems are PLC-complete is an important open question which we pursue;
in contrast, we show that the latter two are PPP-complete. Finally, we reframe
PPP as an optimization problem, and define a hierarchy of such problems related
to Tur\'an's theorem.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Pasarkar_A/0/1/0/all/0/1">Amol Pasarkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Yannakakis_M/0/1/0/all/0/1">Mihalis Yannakakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Papadimitriou_C/0/1/0/all/0/1">Christos Papadimitriou</a></p><p>We study the complexity of computational problems arising from existence
theorems in extremal combinatorics. For some of these problems, a solution is
guaranteed to exist based on an iterated application of the Pigeonhole
Principle. This results in the definition of a new complexity class within
TFNP, which we call PLC (for "polynomial long choice"). PLC includes all of
PPP, as well as numerous previously unclassified total problems, including
search problems related to Ramsey's theorem, the Sunflower theorem, the
Erd\H{o}s-Ko-Rado lemma, and K\"onig's lemma. Whether the first two of these
four problems are PLC-complete is an important open question which we pursue;
in contrast, we show that the latter two are PPP-complete. Finally, we reframe
PPP as an optimization problem, and define a hierarchy of such problems related
to Tur\'an's theorem.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.08009'>Approximate traces on groups and the quantum complexity class $\operatorname{MIP}^{co,s}$</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Isaac Goldbring, Bradd Hart</p><p>An open question in quantum complexity theory is whether or not the class
$\operatorname{MIP}^{co}$, consisting of languages that can be efficiently
verified using interacting provers sharing quantum resources according to the
quantum commuting model, coincides with the class $coRE$ of languages with
recursively enumerable complement. We introduce the notion of a qc-modulus,
which encodes approximations to quantum commuting correlations, and show that
the existence of a computable qc-modulus gives a negative answer to a natural
variant of the aforementioned question.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Goldbring_I/0/1/0/all/0/1">Isaac Goldbring</a>, <a href="http://arxiv.org/find/cs/1/au:+Hart_B/0/1/0/all/0/1">Bradd Hart</a></p><p>An open question in quantum complexity theory is whether or not the class
$\operatorname{MIP}^{co}$, consisting of languages that can be efficiently
verified using interacting provers sharing quantum resources according to the
quantum commuting model, coincides with the class $coRE$ of languages with
recursively enumerable complement. We introduce the notion of a qc-modulus,
which encodes approximations to quantum commuting correlations, and show that
the existence of a computable qc-modulus gives a negative answer to a natural
variant of the aforementioned question.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.08042'>Decision Tree Complexity versus Block Sensitivity and Degree</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Rahul Chugh, Supartha Podder, Swagato Sanyal</p><p>Relations between the decision tree complexity and various other complexity
measures of Boolean functions is a thriving topic of research in computational
complexity. It is known that decision tree complexity is bounded above by the
cube of block sensitivity, and the cube of polynomial degree. However, the
widest separation between decision tree complexity and each of block
sensitivity and degree that is witnessed by known Boolean functions is
quadratic. In this work, we investigate the tightness of the existing cubic
upper bounds.
</p>
<p>We improve the cubic upper bounds for many interesting classes of Boolean
functions. We show that for graph properties and for functions with a constant
number of alternations, both of the cubic upper bounds can be improved to
quadratic. We define a class of Boolean functions, which we call the zebra
functions, that comprises Boolean functions where each monotone path from 0^n
to 1^n has an equal number of alternations. This class contains the symmetric
and monotone functions as its subclasses. We show that for any zebra function,
decision tree complexity is at most the square of block sensitivity, and
certificate complexity is at most the square of degree.
</p>
<p>Finally, we show using a lifting theorem of communication complexity by
G{\"{o}}{\"{o}}s, Pitassi and Watson that the task of proving an improved upper
bound on the decision tree complexity for all functions is in a sense
equivalent to the potentially easier task of proving a similar upper bound on
communication complexity for each bi-partition of the input variables, for all
functions. In particular, this implies that to bound the decision tree
complexity it suffices to bound smaller measures like parity decision tree
complexity, subcube decision tree complexity and decision tree rank, that are
defined in terms of models that can be efficiently simulated by communication
protocols.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chugh_R/0/1/0/all/0/1">Rahul Chugh</a>, <a href="http://arxiv.org/find/cs/1/au:+Podder_S/0/1/0/all/0/1">Supartha Podder</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1">Swagato Sanyal</a></p><p>Relations between the decision tree complexity and various other complexity
measures of Boolean functions is a thriving topic of research in computational
complexity. It is known that decision tree complexity is bounded above by the
cube of block sensitivity, and the cube of polynomial degree. However, the
widest separation between decision tree complexity and each of block
sensitivity and degree that is witnessed by known Boolean functions is
quadratic. In this work, we investigate the tightness of the existing cubic
upper bounds.
</p>
<p>We improve the cubic upper bounds for many interesting classes of Boolean
functions. We show that for graph properties and for functions with a constant
number of alternations, both of the cubic upper bounds can be improved to
quadratic. We define a class of Boolean functions, which we call the zebra
functions, that comprises Boolean functions where each monotone path from 0^n
to 1^n has an equal number of alternations. This class contains the symmetric
and monotone functions as its subclasses. We show that for any zebra function,
decision tree complexity is at most the square of block sensitivity, and
certificate complexity is at most the square of degree.
</p>
<p>Finally, we show using a lifting theorem of communication complexity by
G{\"{o}}{\"{o}}s, Pitassi and Watson that the task of proving an improved upper
bound on the decision tree complexity for all functions is in a sense
equivalent to the potentially easier task of proving a similar upper bound on
communication complexity for each bi-partition of the input variables, for all
functions. In particular, this implies that to bound the decision tree
complexity it suffices to bound smaller measures like parity decision tree
complexity, subcube decision tree complexity and decision tree rank, that are
defined in terms of models that can be efficiently simulated by communication
protocols.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07557'>On Optimal Coverage of a Tree with Multiple Robots</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: I. Aldana-Galv&#xe1;n, J.C. Catana-Salazar, J.M. D&#xed;az-B&#xe1;&#xf1;ez, F. Duque, R. Fabila-Monroy, M.A. Heredia, A. Ram&#xed;rez-Vigueras, J. Urrutia</p><p>We study the algorithmic problem of optimally covering a tree with $k$ mobile
robots. The tree is known to all robots, and our goal is to assign a walk to
each robot in such a way that the union of these walks covers the whole tree.
We assume that the edges have the same length, and that traveling along an edge
takes a unit of time. Two objective functions are considered: the cover time
and the cover length. The cover time is the maximum time a robot needs to
finish its assigned walk and the cover length is the sum of the lengths of all
the walks. We also consider a variant in which the robots must rendezvous
periodically at the same vertex in at most a certain number of moves. We show
that the problem is different for the two cost functions. For the cover time
minimization problem, we prove that the problem is NP-hard when $k$ is part of
the input, regardless of whether periodic rendezvous are required or not. For
the cover length minimization problem, we show that it can be solved in
polynomial time when periodic rendezvous are not required, and it is NP-hard
otherwise.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Aldana_Galvan_I/0/1/0/all/0/1">I. Aldana-Galv&#xe1;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Catana_Salazar_J/0/1/0/all/0/1">J.C. Catana-Salazar</a>, <a href="http://arxiv.org/find/cs/1/au:+Diaz_Banez_J/0/1/0/all/0/1">J.M. D&#xed;az-B&#xe1;&#xf1;ez</a>, <a href="http://arxiv.org/find/cs/1/au:+Duque_F/0/1/0/all/0/1">F. Duque</a>, <a href="http://arxiv.org/find/cs/1/au:+Fabila_Monroy_R/0/1/0/all/0/1">R. Fabila-Monroy</a>, <a href="http://arxiv.org/find/cs/1/au:+Heredia_M/0/1/0/all/0/1">M.A. Heredia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramirez_Vigueras_A/0/1/0/all/0/1">A. Ram&#xed;rez-Vigueras</a>, <a href="http://arxiv.org/find/cs/1/au:+Urrutia_J/0/1/0/all/0/1">J. Urrutia</a></p><p>We study the algorithmic problem of optimally covering a tree with $k$ mobile
robots. The tree is known to all robots, and our goal is to assign a walk to
each robot in such a way that the union of these walks covers the whole tree.
We assume that the edges have the same length, and that traveling along an edge
takes a unit of time. Two objective functions are considered: the cover time
and the cover length. The cover time is the maximum time a robot needs to
finish its assigned walk and the cover length is the sum of the lengths of all
the walks. We also consider a variant in which the robots must rendezvous
periodically at the same vertex in at most a certain number of moves. We show
that the problem is different for the two cost functions. For the cover time
minimization problem, we prove that the problem is NP-hard when $k$ is part of
the input, regardless of whether periodic rendezvous are required or not. For
the cover length minimization problem, we show that it can be solved in
polynomial time when periodic rendezvous are not required, and it is NP-hard
otherwise.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07772'>$b$-Coloring Parameterized by Pathwidth is XNLP-complete</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Lars Jaffke, Paloma T. Lima, Roohani Sharma</p><p>We show that the $b$-Coloring problem is complete for the class XNLP when
parameterized by the pathwidth of the input graph. Besides determining the
precise parameterized complexity of this problem, this implies that b-Coloring
parameterized by pathwidth is $W[t]$-hard for all $t$, and resolves the
parameterized complexity of $b$-Coloring parameterized by treewidth.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jaffke_L/0/1/0/all/0/1">Lars Jaffke</a>, <a href="http://arxiv.org/find/cs/1/au:+Lima_P/0/1/0/all/0/1">Paloma T. Lima</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1">Roohani Sharma</a></p><p>We show that the $b$-Coloring problem is complete for the class XNLP when
parameterized by the pathwidth of the input graph. Besides determining the
precise parameterized complexity of this problem, this implies that b-Coloring
parameterized by pathwidth is $W[t]$-hard for all $t$, and resolves the
parameterized complexity of $b$-Coloring parameterized by treewidth.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07580'>Exploring the Tradeoff between Competitive Ratio and Variance in Online-Matching Markets</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Pan Xu</p><p>In this paper, we propose an online-matching-based model to study the
assignment problems arising in a wide range of online-matching markets,
including online recommendations, ride-hailing platforms, and crowdsourcing
markets. It features that each assignment can request a random set of resources
and yield a random utility, and the two (cost and utility) can be arbitrarily
correlated with each other. We present two linear-programming-based
parameterized policies to study the tradeoff between the \emph{competitive
ratio} (CR) on the total utilities and the \emph{variance} on the total number
of matches (unweighted version). The first one (SAMP) is to sample an edge
according to the distribution extracted from the clairvoyant optimal, while the
second (ATT) features a time-adaptive attenuation framework that leads to an
improvement over the state-of-the-art competitive-ratio result. We also
consider the problem under a large-budget assumption and show that SAMP
achieves asymptotically optimal performance in terms of competitive ratio.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1">Pan Xu</a></p><p>In this paper, we propose an online-matching-based model to study the
assignment problems arising in a wide range of online-matching markets,
including online recommendations, ride-hailing platforms, and crowdsourcing
markets. It features that each assignment can request a random set of resources
and yield a random utility, and the two (cost and utility) can be arbitrarily
correlated with each other. We present two linear-programming-based
parameterized policies to study the tradeoff between the \emph{competitive
ratio} (CR) on the total utilities and the \emph{variance} on the total number
of matches (unweighted version). The first one (SAMP) is to sample an edge
according to the distribution extracted from the clairvoyant optimal, while the
second (ATT) features a time-adaptive attenuation framework that leads to an
improvement over the state-of-the-art competitive-ratio result. We also
consider the problem under a large-budget assumption and show that SAMP
achieves asymptotically optimal performance in terms of competitive ratio.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07729'>On Weighted Graph Sparsification by Linear Sketching</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Yu Chen, Sanjeev Khanna, Huan Li</p><p>A seminal work of [Ahn-Guha-McGregor, PODS'12] showed that one can compute a
cut sparsifier of an unweighted undirected graph by taking a near-linear number
of linear measurements on the graph. Subsequent works also studied computing
other graph sparsifiers using linear sketching, and obtained near-linear upper
bounds for spectral sparsifiers [Kapralov-Lee-Musco-Musco-Sidford, FOCS'14] and
first non-trivial upper bounds for spanners [Filtser-Kapralov-Nouri, SODA'21].
All these linear sketching algorithms, however, only work on unweighted graphs.
</p>
<p>In this paper, we initiate the study of weighted graph sparsification by
linear sketching by investigating a natural class of linear sketches that we
call incidence sketches, in which each measurement is a linear combination of
the weights of edges incident on a single vertex. Our results are:
</p>
<p>1. Weighted cut sparsification: We give an algorithm that computes a $(1 +
\epsilon)$-cut sparsifier using $\tilde{O}(n \epsilon^{-3})$ linear
measurements, which is nearly optimal.
</p>
<p>2. Weighted spectral sparsification: We give an algorithm that computes a $(1
+ \epsilon)$-spectral sparsifier using $\tilde{O}(n^{6/5} \epsilon^{-4})$
linear measurements. Complementing our algorithm, we then prove a superlinear
lower bound of $\Omega(n^{21/20-o(1)})$ measurements for computing some
$O(1)$-spectral sparsifier using incidence sketches.
</p>
<p>3. Weighted spanner computation: We focus on graphs whose largest/smallest
edge weights differ by an $O(1)$ factor, and prove that, for incidence
sketches, the upper bounds obtained by~[Filtser-Kapralov-Nouri, SODA'21] are
optimal up to an $n^{o(1)}$ factor.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Khanna_S/0/1/0/all/0/1">Sanjeev Khanna</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huan Li</a></p><p>A seminal work of [Ahn-Guha-McGregor, PODS'12] showed that one can compute a
cut sparsifier of an unweighted undirected graph by taking a near-linear number
of linear measurements on the graph. Subsequent works also studied computing
other graph sparsifiers using linear sketching, and obtained near-linear upper
bounds for spectral sparsifiers [Kapralov-Lee-Musco-Musco-Sidford, FOCS'14] and
first non-trivial upper bounds for spanners [Filtser-Kapralov-Nouri, SODA'21].
All these linear sketching algorithms, however, only work on unweighted graphs.
</p>
<p>In this paper, we initiate the study of weighted graph sparsification by
linear sketching by investigating a natural class of linear sketches that we
call incidence sketches, in which each measurement is a linear combination of
the weights of edges incident on a single vertex. Our results are:
</p>
<p>1. Weighted cut sparsification: We give an algorithm that computes a $(1 +
\epsilon)$-cut sparsifier using $\tilde{O}(n \epsilon^{-3})$ linear
measurements, which is nearly optimal.
</p>
<p>2. Weighted spectral sparsification: We give an algorithm that computes a $(1
+ \epsilon)$-spectral sparsifier using $\tilde{O}(n^{6/5} \epsilon^{-4})$
linear measurements. Complementing our algorithm, we then prove a superlinear
lower bound of $\Omega(n^{21/20-o(1)})$ measurements for computing some
$O(1)$-spectral sparsifier using incidence sketches.
</p>
<p>3. Weighted spanner computation: We focus on graphs whose largest/smallest
edge weights differ by an $O(1)$ factor, and prove that, for incidence
sketches, the upper bounds obtained by~[Filtser-Kapralov-Nouri, SODA'21] are
optimal up to an $n^{o(1)}$ factor.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07860'>A $(1.5+\epsilon)$-Approximation Algorithm for Weighted Connectivity Augmentation</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Vera Traub, Rico Zenklusen</p><p>Connectivity augmentation problems are among the most elementary questions in
Network Design. Many of these problems admit natural $2$-approximation
algorithms, often through various classic techniques, whereas it remains open
whether approximation factors below $2$ can be achieved. One of the most basic
examples thereof is the Weighted Connectivity Augmentation Problem (WCAP). In
WCAP, one is given an undirected graph together with a set of additional
weighted candidate edges, and the task is to find a cheapest set of candidate
edges whose addition to the graph increases its edge-connectivity. We present a
$(1.5+\varepsilon)$-approximation algorithm for WCAP, showing for the first
time that factors below $2$ are achievable.
</p>
<p>On a high level, we design a well-chosen local search algorithm, inspired by
recent advances for Weighted Tree Augmentation. To measure progress, we
consider a directed weakening of WCAP and show that it has highly structured
planar solutions. Interpreting a solution of the original problem as one of
this directed weakening allows us to describe local exchange steps in a clean
and algorithmically amenable way. Leveraging these insights, we show that we
can efficiently search for good exchange steps within a component class for
link sets that is closely related to bounded treewidth subgraphs of circle
graphs. Moreover, we prove that an optimum solution can be decomposed into
smaller components, at least one of which leads to a good local search step as
long as we did not yet achieve the claimed approximation guarantee.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Traub_V/0/1/0/all/0/1">Vera Traub</a>, <a href="http://arxiv.org/find/cs/1/au:+Zenklusen_R/0/1/0/all/0/1">Rico Zenklusen</a></p><p>Connectivity augmentation problems are among the most elementary questions in
Network Design. Many of these problems admit natural $2$-approximation
algorithms, often through various classic techniques, whereas it remains open
whether approximation factors below $2$ can be achieved. One of the most basic
examples thereof is the Weighted Connectivity Augmentation Problem (WCAP). In
WCAP, one is given an undirected graph together with a set of additional
weighted candidate edges, and the task is to find a cheapest set of candidate
edges whose addition to the graph increases its edge-connectivity. We present a
$(1.5+\varepsilon)$-approximation algorithm for WCAP, showing for the first
time that factors below $2$ are achievable.
</p>
<p>On a high level, we design a well-chosen local search algorithm, inspired by
recent advances for Weighted Tree Augmentation. To measure progress, we
consider a directed weakening of WCAP and show that it has highly structured
planar solutions. Interpreting a solution of the original problem as one of
this directed weakening allows us to describe local exchange steps in a clean
and algorithmically amenable way. Leveraging these insights, we show that we
can efficiently search for good exchange steps within a component class for
link sets that is closely related to bounded treewidth subgraphs of circle
graphs. Moreover, we prove that an optimum solution can be decomposed into
smaller components, at least one of which leads to a good local search step as
long as we did not yet achieve the claimed approximation guarantee.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07988'>Prophet Inequalities for Cost Minimization</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Vasilis Livanos, Ruta Mehta</p><p>Prophet inequalities for rewards maximization are fundamental results from
optimal stopping theory with several applications to mechanism design and
online optimization. We study the cost minimization counterpart of the
classical prophet inequality, where one is facing a sequence of costs $X_1,
X_2, \dots, X_n$ in an online manner and must ''stop'' at some point and take
the last cost seen. Given that the $X_i$'s are independent, drawn from known
distributions, the goal is to devise a stopping strategy $S$ (online algorithm)
that minimizes the expected cost.
</p>
<p>We first observe that if the $X_i$'s are not identically distributed, then no
strategy can achieve a bounded approximation, no matter if the arrival order is
adversarial or random. This leads us to consider the case where the $X_i$'s are
I.I.D.. For the I.I.D. case, we give a complete characterization of the optimal
stopping strategy. We show that it achieves a (distribution-dependent)
constant-factor approximation to the prophet's cost for almost all
distributions and that this constant is tight. In particular, for distributions
for which the integral of the hazard rate is a polynomial $H(x) = \sum_{i=1}^k
a_i x^{d_i}$, where $d_1 &lt; \dots &lt; d_k$, the approximation factor is
$\lambda(d_1)$, a decreasing function of $d_1$. Furthermore, for MHR
distributions, we show that this constant is at most $2$, and this is again
tight.
</p>
<p>We also analyze single-threshold strategies for the cost prophet inequality
problem. We design a threshold that achieves a
$\operatorname{O}(\operatorname{polylog}n)$-factor approximation, where the
exponent in the logarithmic factor is a distribution-dependent constant, and we
show a matching lower bound.
</p>
<p>We believe that our results are of independent interest for analyzing
approximately optimal (posted price-style) mechanisms for procuring items.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Livanos_V/0/1/0/all/0/1">Vasilis Livanos</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1">Ruta Mehta</a></p><p>Prophet inequalities for rewards maximization are fundamental results from
optimal stopping theory with several applications to mechanism design and
online optimization. We study the cost minimization counterpart of the
classical prophet inequality, where one is facing a sequence of costs $X_1,
X_2, \dots, X_n$ in an online manner and must ''stop'' at some point and take
the last cost seen. Given that the $X_i$'s are independent, drawn from known
distributions, the goal is to devise a stopping strategy $S$ (online algorithm)
that minimizes the expected cost.
</p>
<p>We first observe that if the $X_i$'s are not identically distributed, then no
strategy can achieve a bounded approximation, no matter if the arrival order is
adversarial or random. This leads us to consider the case where the $X_i$'s are
I.I.D.. For the I.I.D. case, we give a complete characterization of the optimal
stopping strategy. We show that it achieves a (distribution-dependent)
constant-factor approximation to the prophet's cost for almost all
distributions and that this constant is tight. In particular, for distributions
for which the integral of the hazard rate is a polynomial $H(x) = \sum_{i=1}^k
a_i x^{d_i}$, where $d_1 &lt; \dots &lt; d_k$, the approximation factor is
$\lambda(d_1)$, a decreasing function of $d_1$. Furthermore, for MHR
distributions, we show that this constant is at most $2$, and this is again
tight.
</p>
<p>We also analyze single-threshold strategies for the cost prophet inequality
problem. We design a threshold that achieves a
$\operatorname{O}(\operatorname{polylog}n)$-factor approximation, where the
exponent in the logarithmic factor is a distribution-dependent constant, and we
show a matching lower bound.
</p>
<p>We believe that our results are of independent interest for analyzing
approximately optimal (posted price-style) mechanisms for procuring items.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.08024'>Fast approximation of search trees on trees with centroid trees</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Benjamin Aram Berendsohn, Ishay Golinsky, Haim Kaplan, L&#xe1;szl&#xf3; Kozma</p><p>Search trees on trees (STTs) generalize the fundamental binary search tree
(BST) data structure: in STTs the underlying search space is an arbitrary tree,
whereas in BSTs it is a path. An optimal BST of size $n$ can be computed for a
given distribution of queries in $O(n^2)$ time [Knuth 1971] and centroid BSTs
provide a nearly-optimal alternative, computable in $O(n)$ time [Mehlhorn
1977].
</p>
<p>By contrast, optimal STTs are not known to be computable in polynomial time,
and the fastest constant-approximation algorithm runs in $O(n^3)$ time
[Berendsohn, Kozma 2022]. Centroid trees can be defined for STTs analogously to
BSTs, and they have been used in a wide range of algorithmic applications. In
the unweighted case (i.e., for a uniform distribution of queries), a centroid
tree can be computed in $O(n)$ time [Brodal et al. 2001; Della Giustina et al.
2019]. These algorithms, however, do not readily extend to the weighted case.
Moreover, no approximation guarantees were previously known for centroid trees
in either the unweighted or weighted cases.
</p>
<p>In this paper we revisit centroid trees in a general, weighted setting, and
we settle both the algorithmic complexity of constructing them, and the quality
of their approximation. For constructing a weighted centroid tree, we give an
output-sensitive $O(n\log h)\subseteq O(n\log n)$ time algorithm, where $h$ is
the height of the resulting centroid tree. If the weights are of polynomial
complexity, the running time is $O(n\log\log n)$. We show these bounds to be
optimal, in a general decision tree model of computation. For approximation, we
prove that the cost of a centroid tree is at most twice the optimum, and this
guarantee is best possible, both in the weighted and unweighted cases. We also
give tight, fine-grained bounds on the approximation-ratio for bounded-degree
trees and on the approximation-ratio of more general $\alpha$-centroid trees.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Berendsohn_B/0/1/0/all/0/1">Benjamin Aram Berendsohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Golinsky_I/0/1/0/all/0/1">Ishay Golinsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaplan_H/0/1/0/all/0/1">Haim Kaplan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kozma_L/0/1/0/all/0/1">L&#xe1;szl&#xf3; Kozma</a></p><p>Search trees on trees (STTs) generalize the fundamental binary search tree
(BST) data structure: in STTs the underlying search space is an arbitrary tree,
whereas in BSTs it is a path. An optimal BST of size $n$ can be computed for a
given distribution of queries in $O(n^2)$ time [Knuth 1971] and centroid BSTs
provide a nearly-optimal alternative, computable in $O(n)$ time [Mehlhorn
1977].
</p>
<p>By contrast, optimal STTs are not known to be computable in polynomial time,
and the fastest constant-approximation algorithm runs in $O(n^3)$ time
[Berendsohn, Kozma 2022]. Centroid trees can be defined for STTs analogously to
BSTs, and they have been used in a wide range of algorithmic applications. In
the unweighted case (i.e., for a uniform distribution of queries), a centroid
tree can be computed in $O(n)$ time [Brodal et al. 2001; Della Giustina et al.
2019]. These algorithms, however, do not readily extend to the weighted case.
Moreover, no approximation guarantees were previously known for centroid trees
in either the unweighted or weighted cases.
</p>
<p>In this paper we revisit centroid trees in a general, weighted setting, and
we settle both the algorithmic complexity of constructing them, and the quality
of their approximation. For constructing a weighted centroid tree, we give an
output-sensitive $O(n\log h)\subseteq O(n\log n)$ time algorithm, where $h$ is
the height of the resulting centroid tree. If the weights are of polynomial
complexity, the running time is $O(n\log\log n)$. We show these bounds to be
optimal, in a general decision tree model of computation. For approximation, we
prove that the cost of a centroid tree is at most twice the optimum, and this
guarantee is best possible, both in the weighted and unweighted cases. We also
give tight, fine-grained bounds on the approximation-ratio for bounded-degree
trees and on the approximation-ratio of more general $\alpha$-centroid trees.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-19T00:30:00Z">Monday, September 19 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Sunday, September 18
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://eccc.weizmann.ac.il/report/2022/132'>TR22-132 |  Fooling polynomials using invariant theory | 

	Harm Derksen, 

	Emanuele Viola</a></h3>
          <p class='item-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          We revisit the problem of constructing explicit pseudorandom generators
that fool with error $\epsilon$ degree-$d$ polynomials in $n$ variables
over the field $F_q$, in the case of large $q$. Previous constructions
either have seed length at least $2^{d}\log q$, and thus are only non-trivial
when the degree is less than $\log n$, or else rely on a seminal reduction by Bogdanov
(STOC 2005). This reduction yields seed length not less than $d^{4} \log n + \log q$
and requires fields of size at least $d^{6}/\epsilon^{2}$; and explicit generators
meeting such bounds are known.

Departing from Bogdanov&#39;s reduction, we develop an algebraic analogue
of the Bogdanov-Viola paradigm (FOCS 2007, SICOMP 2010) of summing
generators for degree-one polynomials. Whereas previous analyses of
the paradigm are restricted to degree less than $\log n$, we give a new analysis
which handles large degrees. A main new idea is to show that the construction
preserves indecomposability of polynomials. Apparently for the first
time in the area, the proof uses invariant theory.

Our approach in particular yields several new pseudorandom generators.
In particular, for large enough fields we obtain seed length $O(d\log n+\log q)$
which is optimal up to constant factors. We also construct generators
for fields of size as small as $O(d^{4})$. Further reducing the field
size requires a significant change in techniques: Most or all generators
for large-degree polynomials rely on Weil bounds; but such bounds
are only applicable when $q&gt;d^{4}$.
        
        </div>

        <div class='item-content item-summary'>
        
          
          We revisit the problem of constructing explicit pseudorandom generators
that fool with error $\epsilon$ degree-$d$ polynomials in $n$ variables
over the field $F_q$, in the case of large $q$. Previous constructions
either have seed length at least $2^{d}\log q$, and thus are only non-trivial
when the degree is less than $\log n$, or else rely on a seminal reduction by Bogdanov
(STOC 2005). This reduction yields seed length not less than $d^{4} \log n + \log q$
and requires fields of size at least $d^{6}/\epsilon^{2}$; and explicit generators
meeting such bounds are known.

Departing from Bogdanov&#39;s reduction, we develop an algebraic analogue
of the Bogdanov-Viola paradigm (FOCS 2007, SICOMP 2010) of summing
generators for degree-one polynomials. Whereas previous analyses of
the paradigm are restricted to degree less than $\log n$, we give a new analysis
which handles large degrees. A main new idea is to show that the construction
preserves indecomposability of polynomials. Apparently for the first
time in the area, the proof uses invariant theory.

Our approach in particular yields several new pseudorandom generators.
In particular, for large enough fields we obtain seed length $O(d\log n+\log q)$
which is optimal up to constant factors. We also construct generators
for fields of size as small as $O(d^{4})$. Further reducing the field
size requires a significant change in techniques: Most or all generators
for large-degree polynomials rely on Weil bounds; but such bounds
are only applicable when $q&gt;d^{4}$.
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-18T20:32:26Z">Sunday, September 18 2022, 20:32</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://adamsheffer.wordpress.com/2022/09/18/new-cs-tenure-track-positions-new-program-in-manhattan/'>New CS Tenure Track Positions, New program in Manhattan</a></h3>
          <p class='item-feed'>from <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          Do you happen to know anyone who is looking for a CS Tenure Track position in Manhattan? Someone who might be interested in being part of a new CS program and help decide the directions it will grow into? Please share this with them: geometrynyc.wixsite.com/csjobs.<p>By Adam Sheffer</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          Do you happen to know anyone who is looking for a CS Tenure Track position in Manhattan? Someone who might be interested in being part of a new CS program and help decide the directions it will grow into? Please share this with them: https://geometrynyc.wixsite.com/csjobs.<p class="authors">By Adam Sheffer</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-18T18:01:54Z">Sunday, September 18 2022, 18:01</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://eccc.weizmann.ac.il/report/2022/131'>TR22-131 |  Radical Sylvester-Gallai for Cubics | 

	Rafael Mendes de Oliveira, 

	Akash Sengupta</a></h3>
          <p class='item-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          Let $\mathcal{F} = \{F_1, \ldots, F_m\}$ be a finite set of irreducible homogeneous multivariate polynomials of degree at most $3$ such that $F_i$ does not divide $F_j$ for $i\neq j$. 
We say that $\mathcal{F}$ is a cubic radical Sylvester-Gallai configuration if for any two distinct $F_i,F_j$ there exists a third polynomial $F_k$ such that whenever $F_i,F_j$ vanish, $F_k$ also vanishes. In particular, for any two indices $i, j \in [m]$, there exists $k \in [m] \setminus \{i,j\}$ such that $F_k \in \rad(F_i, F_j)$.

We prove that any cubic radical Sylvester-Gallai configuration is low-dimensional, that is
$$ \dim span_{\mathbb{K}}{\mathcal{F}} = O(1).$$
This solves a conjecture of Gupta [G14] in degree $3$ and generalizes the result in [S20], which proved that quadratic radical Sylvester-Gallai configurations are low-dimensional. 
Our result takes us one step closer towards solving the non-linear Sylvester-Gallai conjectures of Gupta [G14], which would yield the first deterministic polynomial time algorithm for the PIT problem for depth-4 circuits of bounded top and bottom fanins.


To prove our Sylvester-Gallai theorem, we develop several new tools combining techniques from algebraic geometry and elimination theory.
Among our technical contributions, we prove a structure theorem characterizing non-radical ideals generated by two cubic forms, generalizing the structure theorems of [HP94, CTSSD87, S20].
Moreover, building upon the groundbreaking work [AH20a], we introduce the notion of wide Ananyan-Hochster algebras and show that these algebras allow us to transfer the local conditions of Sylvester-Gallai configurations into global conditions.
        
        </div>

        <div class='item-content item-summary'>
        
          
          Let $\mathcal{F} = \{F_1, \ldots, F_m\}$ be a finite set of irreducible homogeneous multivariate polynomials of degree at most $3$ such that $F_i$ does not divide $F_j$ for $i\neq j$. 
We say that $\mathcal{F}$ is a cubic radical Sylvester-Gallai configuration if for any two distinct $F_i,F_j$ there exists a third polynomial $F_k$ such that whenever $F_i,F_j$ vanish, $F_k$ also vanishes. In particular, for any two indices $i, j \in [m]$, there exists $k \in [m] \setminus \{i,j\}$ such that $F_k \in \rad(F_i, F_j)$.

We prove that any cubic radical Sylvester-Gallai configuration is low-dimensional, that is
$$ \dim span_{\mathbb{K}}{\mathcal{F}} = O(1).$$
This solves a conjecture of Gupta [G14] in degree $3$ and generalizes the result in [S20], which proved that quadratic radical Sylvester-Gallai configurations are low-dimensional. 
Our result takes us one step closer towards solving the non-linear Sylvester-Gallai conjectures of Gupta [G14], which would yield the first deterministic polynomial time algorithm for the PIT problem for depth-4 circuits of bounded top and bottom fanins.


To prove our Sylvester-Gallai theorem, we develop several new tools combining techniques from algebraic geometry and elimination theory.
Among our technical contributions, we prove a structure theorem characterizing non-radical ideals generated by two cubic forms, generalizing the structure theorems of [HP94, CTSSD87, S20].
Moreover, building upon the groundbreaking work [AH20a], we introduce the notion of wide Ananyan-Hochster algebras and show that these algebras allow us to transfer the local conditions of Sylvester-Gallai configurations into global conditions.
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-18T08:52:56Z">Sunday, September 18 2022, 08:52</time>
        </div>
      </div>
    </article>
  
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Friday, September 16
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://eccc.weizmann.ac.il/report/2022/130'>TR22-130 |  A Borsuk-Ulam lower bound for sign-rank and its application | 

	Hamed Hatami, 

	Kaave Hosseini, 

	Xiang Meng</a></h3>
          <p class='item-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          We introduce a new topological argument based on the Borsuk-Ulam theorem to prove a lower bound on sign-rank.  

      This result implies the strongest possible separation between randomized and unbounded-error communication complexity. More precisely, we show that for a particular range of parameters, the randomized communication complexity of the Gap Hamming Distance problem is $O(1)$ while its unbounded-error communication complexity is  $\Omega(\log(n))$. 
     
 Previously, it was unknown whether the unbounded-error communication complexity could be asymptotically larger than the randomized communication complexity.
    
In connection to learning theory, we prove that,  despite its learnability properties, the class of large margin half-spaces in $\mathbb{R}^d$ is genuinely high-dimensional, i.e., it cannot be embedded in $\mathbb{R}^{d-1}$. This result is closely related to a recent conjecture of Alon, Hanneke, Holzman, and Moran (FOCS 2021) about the VC dimension of this class.
    
Our final application is to the theory of dimension reductions. The Johnson-Lindenstrauss theorem implies that any set of $N$ unit vectors is embeddable in dimension  $O(\gamma^{-2}\log N)$ without altering the signs of those pairwise inner products that have absolute values at least $\gamma&gt;0$.  Our result establishes the tightness of this bound, which answers a question of Linial, Mendelson, Schechtman, and Shraibman (Combinatorica, 27(2007)) in the case of partial functions.
        
        </div>

        <div class='item-content item-summary'>
        
          
          We introduce a new topological argument based on the Borsuk-Ulam theorem to prove a lower bound on sign-rank.  

      This result implies the strongest possible separation between randomized and unbounded-error communication complexity. More precisely, we show that for a particular range of parameters, the randomized communication complexity of the Gap Hamming Distance problem is $O(1)$ while its unbounded-error communication complexity is  $\Omega(\log(n))$. 
     
 Previously, it was unknown whether the unbounded-error communication complexity could be asymptotically larger than the randomized communication complexity.
    
In connection to learning theory, we prove that,  despite its learnability properties, the class of large margin half-spaces in $\mathbb{R}^d$ is genuinely high-dimensional, i.e., it cannot be embedded in $\mathbb{R}^{d-1}$. This result is closely related to a recent conjecture of Alon, Hanneke, Holzman, and Moran (FOCS 2021) about the VC dimension of this class.
    
Our final application is to the theory of dimension reductions. The Johnson-Lindenstrauss theorem implies that any set of $N$ unit vectors is embeddable in dimension  $O(\gamma^{-2}\log N)$ without altering the signs of those pairwise inner products that have absolute values at least $\gamma&gt;0$.  Our result establishes the tightness of this bound, which answers a question of Linial, Mendelson, Schechtman, and Shraibman (Combinatorica, 27(2007)) in the case of partial functions.
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T02:54:31Z">Friday, September 16 2022, 02:54</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.06930'>How Much Structure Is Needed for Huge Quantum Speedups?</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Scott Aaronson</p><p>I survey, for a general scientific audience, three decades of research into
which sorts of problems admit exponential speedups via quantum computers --
from the classics (like the algorithms of Simon and Shor), to the breakthrough
of Yamakawa and Zhandry from April 2022. I discuss both the quantum circuit
model, which is what we ultimately care about in practice but where our
knowledge is radically incomplete, and the so-called oracle or black-box or
query complexity model, where we've managed to achieve a much more thorough
understanding that then informs our conjectures about the circuit model. I
discuss the strengths and weaknesses of switching attention to sampling tasks,
as was done in the recent quantum supremacy experiments. I make some skeptical
remarks about widely-repeated claims of exponential quantum speedups for
practical machine learning and optimization problems. Through many examples, I
try to convey the "law of conservation of weirdness," according to which every
problem admitting an exponential quantum speedup must have some unusual
property to allow the amplitude to be concentrated on the unknown right
answer(s).
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Aaronson_S/0/1/0/all/0/1">Scott Aaronson</a></p><p>I survey, for a general scientific audience, three decades of research into
which sorts of problems admit exponential speedups via quantum computers --
from the classics (like the algorithms of Simon and Shor), to the breakthrough
of Yamakawa and Zhandry from April 2022. I discuss both the quantum circuit
model, which is what we ultimately care about in practice but where our
knowledge is radically incomplete, and the so-called oracle or black-box or
query complexity model, where we've managed to achieve a much more thorough
understanding that then informs our conjectures about the circuit model. I
discuss the strengths and weaknesses of switching attention to sampling tasks,
as was done in the recent quantum supremacy experiments. I make some skeptical
remarks about widely-repeated claims of exponential quantum speedups for
practical machine learning and optimization problems. Through many examples, I
try to convey the "law of conservation of weirdness," according to which every
problem admitting an exponential quantum speedup must have some unusual
property to allow the amplitude to be concentrated on the unknown right
answer(s).
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.06939'>The Complexity Classes of Hamming Distance Recoverable Robust Problems</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Christoph Gr&#xfc;ne</p><p>In the well-known complexity class NP, many combinatorial problems can be
found, whose optimization counterpart are important for many practical
settings. Those problems usually consider full knowledge about the input and
optimize on this specific input. In a practical setting, however, uncertainty
in the input data is a usual phenomenon, whereby this is normally not covered
in optimization versions of NP problems. One concept to model the uncertainty
in the input data, is \textit{recoverable robustness}. In this setting, a
solution on the input is calculated, whereby a possible recovery to a good
solution should be guaranteed, whenever uncertainty manifests itself. That is,
a solution $\texttt{s}_0$ for the base scenario $\textsf{S}_0$ as well as a
solution \texttt{s} for every possible scenario of scenario set \textsf{S} has
to be calculated. In other words, not only solution $\texttt{s}_0$ for instance
$\textsf{S}_0$ is calculated but solutions \texttt{s} for all scenarios from
\textsf{S} are prepared to correct possible errors through uncertainty. This
paper introduces a specific concept of recoverable robust problems: Hamming
Distance Recoverable Robust Problems. In this setting, solutions $\texttt{s}_0$
and \texttt{s} have to be calculated, such that $\texttt{s}_0$ and \texttt{s}
may only differ in at most $\kappa$ elements. That is, one can recover from a
harmful scenario by choosing a different solution, which is not too far away
from the first solution. This paper surveys the complexity of Hamming distance
recoverable robust version of optimization problems, typically found in NP for
different types of scenarios. The complexity is primarily situated in the lower
levels of the polynomial hierarchy. The main contribution of the paper is that
recoverable robust problems with compression-encoded scenarios and $m \in
\mathbb{N}$ recoveries are $\Sigma^P_{2m+1}$-complete.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Grune_C/0/1/0/all/0/1">Christoph Gr&#xfc;ne</a></p><p>In the well-known complexity class NP, many combinatorial problems can be
found, whose optimization counterpart are important for many practical
settings. Those problems usually consider full knowledge about the input and
optimize on this specific input. In a practical setting, however, uncertainty
in the input data is a usual phenomenon, whereby this is normally not covered
in optimization versions of NP problems. One concept to model the uncertainty
in the input data, is \textit{recoverable robustness}. In this setting, a
solution on the input is calculated, whereby a possible recovery to a good
solution should be guaranteed, whenever uncertainty manifests itself. That is,
a solution $\texttt{s}_0$ for the base scenario $\textsf{S}_0$ as well as a
solution \texttt{s} for every possible scenario of scenario set \textsf{S} has
to be calculated. In other words, not only solution $\texttt{s}_0$ for instance
$\textsf{S}_0$ is calculated but solutions \texttt{s} for all scenarios from
\textsf{S} are prepared to correct possible errors through uncertainty. This
paper introduces a specific concept of recoverable robust problems: Hamming
Distance Recoverable Robust Problems. In this setting, solutions $\texttt{s}_0$
and \texttt{s} have to be calculated, such that $\texttt{s}_0$ and \texttt{s}
may only differ in at most $\kappa$ elements. That is, one can recover from a
harmful scenario by choosing a different solution, which is not too far away
from the first solution. This paper surveys the complexity of Hamming distance
recoverable robust version of optimization problems, typically found in NP for
different types of scenarios. The complexity is primarily situated in the lower
levels of the polynomial hierarchy. The main contribution of the paper is that
recoverable robust problems with compression-encoded scenarios and $m \in
\mathbb{N}$ recoveries are $\Sigma^P_{2m+1}$-complete.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07497'>On Power Set Axiom</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Leonid A. Levin</p><p>Usual math sets have special types: countable, compact, open, occasionally
Borel, rarely projective, etc. Generic sets dependent on Power Set axiom appear
mostly in esoteric areas, ST logic, etc. Dropping that Axiom may greatly
simplify the foundations of mainstream math. Meanwhile dependence on it of a
theorem is worth noting, as dependence on Choice often is.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Levin_L/0/1/0/all/0/1">Leonid A. Levin</a></p><p>Usual math sets have special types: countable, compact, open, occasionally
Borel, rarely projective, etc. Generic sets dependent on Power Set axiom appear
mostly in esoteric areas, ST logic, etc. Dropping that Axiom may greatly
simplify the foundations of mainstream math. Meanwhile dependence on it of a
theorem is worth noting, as dependence on Choice often is.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.06968'>Stochastic strategies for patrolling a terrain with a synchronized multi-robot system</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Luis E. Caraballo, Jos&#xe9; M. D&#xed;az-B&#xe1;&#xf1;ez, Ruy Fabila-Monroy, Carlos Hidalgo-Toscan</p><p>A group of cooperative aerial robots can be deployed to efficiently patrol a
terrain, in which each robot flies around an assigned area and shares
information with the neighbors periodically in order to protect or supervise
it. To ensure robustness, previous works on these synchronized systems propose
sending a robot to the neighboring area in case it detects a failure. In order
to deal with unpredictability and to improve on the efficiency in the
deterministic patrolling scheme, this paper proposes random strategies to cover
the areas distributed among the agents. First, a theoretical study of the
stochastic process is addressed in this paper for two metrics: the \emph{idle
time}, the expected time between two consecutive observations of any point of
the terrain and the \emph{isolation time}, the expected time that a robot is
without communication with any other robot. After that, the random strategies
are experimentally compared with the deterministic strategy adding another
metric: the \emph{broadcast time}, the expected time elapsed from the moment a
robot emits a message until it is received by all the other robots of the team.
The simulations show that theoretical results are in good agreement with the
simulations and the random strategies outperform the behavior obtained with the
deterministic protocol proposed in the literature.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Caraballo_L/0/1/0/all/0/1">Luis E. Caraballo</a>, <a href="http://arxiv.org/find/cs/1/au:+Diaz_Banez_J/0/1/0/all/0/1">Jos&#xe9; M. D&#xed;az-B&#xe1;&#xf1;ez</a>, <a href="http://arxiv.org/find/cs/1/au:+Fabila_Monroy_R/0/1/0/all/0/1">Ruy Fabila-Monroy</a>, <a href="http://arxiv.org/find/cs/1/au:+Hidalgo_Toscan_C/0/1/0/all/0/1">Carlos Hidalgo-Toscan</a></p><p>A group of cooperative aerial robots can be deployed to efficiently patrol a
terrain, in which each robot flies around an assigned area and shares
information with the neighbors periodically in order to protect or supervise
it. To ensure robustness, previous works on these synchronized systems propose
sending a robot to the neighboring area in case it detects a failure. In order
to deal with unpredictability and to improve on the efficiency in the
deterministic patrolling scheme, this paper proposes random strategies to cover
the areas distributed among the agents. First, a theoretical study of the
stochastic process is addressed in this paper for two metrics: the \emph{idle
time}, the expected time between two consecutive observations of any point of
the terrain and the \emph{isolation time}, the expected time that a robot is
without communication with any other robot. After that, the random strategies
are experimentally compared with the deterministic strategy adding another
metric: the \emph{broadcast time}, the expected time elapsed from the moment a
robot emits a message until it is received by all the other robots of the team.
The simulations show that theoretical results are in good agreement with the
simulations and the random strategies outperform the behavior obtained with the
deterministic protocol proposed in the literature.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07517'>Spectral Total-Variation Processing of Shapes -- Theory and Applications</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Jonathan Brokman, Martin Burger, Guy Gilboa</p><p>In this work we present a comprehensive analysis of total variation (TV) on
non Euclidean domains and its eigenfunctions. We specifically address
parameterized surfaces, a natural representation of the shapes used in 3D
graphics. Our work sheds new light on the celebrated Beltrami and Anisotropic
TV flows, and explains experimental findings from recent years on shape
spectral TV [Fumero et al. 2020] and adaptive anisotropic spectral TV [Biton
and Gilboa 2022]. A new notion of convexity on manifolds is derived, by
characterizing structures that are stable throughout the TV flow, performed on
manifolds. We further propose a time efficient nonlinear and non Euclidean
spectral framework for shape processing that is based on zero homogeneous
flows, and propose three different such methods. Each method satisfies distinct
characteristics, demonstrated through smoothing, enhancing and exaggerating
filters.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Brokman_J/0/1/0/all/0/1">Jonathan Brokman</a>, <a href="http://arxiv.org/find/cs/1/au:+Burger_M/0/1/0/all/0/1">Martin Burger</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilboa_G/0/1/0/all/0/1">Guy Gilboa</a></p><p>In this work we present a comprehensive analysis of total variation (TV) on
non Euclidean domains and its eigenfunctions. We specifically address
parameterized surfaces, a natural representation of the shapes used in 3D
graphics. Our work sheds new light on the celebrated Beltrami and Anisotropic
TV flows, and explains experimental findings from recent years on shape
spectral TV [Fumero et al. 2020] and adaptive anisotropic spectral TV [Biton
and Gilboa 2022]. A new notion of convexity on manifolds is derived, by
characterizing structures that are stable throughout the TV flow, performed on
manifolds. We further propose a time efficient nonlinear and non Euclidean
spectral framework for shape processing that is based on zero homogeneous
flows, and propose three different such methods. Each method satisfies distinct
characteristics, demonstrated through smoothing, enhancing and exaggerating
filters.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.06909'>Multiway Powersort</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: William Cawley Gelling, Markus E. Nebel, Benjamin Smith, Sebastian Wild</p><p>Powersort (Munro &amp; Wild, ESA2018) has recently replaced Timsort's suboptimal
merge policy in the CPython reference implementation of Python, as well as in
PyPy and further libraries. We present a stable mergesort variant, Multiway
Powersort, that exploits existing runs and finds nearly-optimal merging orders
for k-way merges with negligible overhead. As observed with Multiway Quicksort
(Kushagra et al., ALENEX 2014; Aum\"uller &amp; Dietzfelbinger, TALG 2016; Wild,
PhD thesis 2016) and the inclusion of Dual-Pivot Quicksort in the Java runtime
library, memory transfers increasingly determine the cost of internal sorting.
We demonstrate that our 4-way Powersort implementation can achieve substantial
speedups over standard (2-way) Powersort and other stable sorting methods
without compromising the optimally run-adaptive performance of Powersort.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gelling_W/0/1/0/all/0/1">William Cawley Gelling</a>, <a href="http://arxiv.org/find/cs/1/au:+Nebel_M/0/1/0/all/0/1">Markus E. Nebel</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_B/0/1/0/all/0/1">Benjamin Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Wild_S/0/1/0/all/0/1">Sebastian Wild</a></p><p>Powersort (Munro &amp; Wild, ESA2018) has recently replaced Timsort's suboptimal
merge policy in the CPython reference implementation of Python, as well as in
PyPy and further libraries. We present a stable mergesort variant, Multiway
Powersort, that exploits existing runs and finds nearly-optimal merging orders
for k-way merges with negligible overhead. As observed with Multiway Quicksort
(Kushagra et al., ALENEX 2014; Aum\"uller &amp; Dietzfelbinger, TALG 2016; Wild,
PhD thesis 2016) and the inclusion of Dual-Pivot Quicksort in the Java runtime
library, memory transfers increasingly determine the cost of internal sorting.
We demonstrate that our 4-way Powersort implementation can achieve substantial
speedups over standard (2-way) Powersort and other stable sorting methods
without compromising the optimally run-adaptive performance of Powersort.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07016'>Algorithms and Lower Bounds for Replacement Paths under Multiple Edge Failures</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Virginia Vassilevska Williams, Eyob Woldeghebriel, Yinzhan Xu</p><p>This paper considers a natural fault-tolerant shortest paths problem: for
some constant integer $f$, given a directed weighted graph with no negative
cycles and two fixed vertices $s$ and $t$, compute (either explicitly or
implicitly) for every tuple of $f$ edges, the distance from $s$ to $t$ if these
edges fail. We call this problem $f$-Fault Replacement Paths ($f$FRP).
</p>
<p>We first present an $\tilde{O}(n^3)$ time algorithm for $2$FRP in $n$-vertex
directed graphs with arbitrary edge weights and no negative cycles. As $2$FRP
is a generalization of the well-studied Replacement Paths problem (RP) that
asks for the distances between $s$ and $t$ for any single edge failure, $2$FRP
is at least as hard as RP. Since RP in graphs with arbitrary weights is
equivalent in a fine-grained sense to All-Pairs Shortest Paths (APSP)
[Vassilevska Williams and Williams FOCS'10, J.~ACM'18], $2$FRP is at least as
hard as APSP, and thus a substantially subcubic time algorithm in the number of
vertices for $2$FRP would be a breakthrough. Therefore, our algorithm in
$\tilde{O}(n^3)$ time is conditionally nearly optimal. Our algorithm implies an
$\tilde{O}(n^{f+1})$ time algorithm for the $f$FRP problem, giving the first
improvement over the straightforward $O(n^{f+2})$ time algorithm.
</p>
<p>Then we focus on the restriction of $2$FRP to graphs with small integer
weights bounded by $M$ in absolute values. Using fast rectangular matrix
multiplication, we obtain a randomized algorithm that runs in
$\tilde{O}(M^{2/3}n^{2.9153})$ time. This implies an improvement over our
$\tilde{O}(n^{f+1})$ time arbitrary weight algorithm for all $f&gt;1$. We also
present a data structure variant of the algorithm that can trade off
pre-processing and query time. In addition to the algebraic algorithms, we also
give an $n^{8/3-o(1)}$ conditional lower bound for combinatorial $2$FRP
algorithms in directed unweighted graphs.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Williams_V/0/1/0/all/0/1">Virginia Vassilevska Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Woldeghebriel_E/0/1/0/all/0/1">Eyob Woldeghebriel</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yinzhan Xu</a></p><p>This paper considers a natural fault-tolerant shortest paths problem: for
some constant integer $f$, given a directed weighted graph with no negative
cycles and two fixed vertices $s$ and $t$, compute (either explicitly or
implicitly) for every tuple of $f$ edges, the distance from $s$ to $t$ if these
edges fail. We call this problem $f$-Fault Replacement Paths ($f$FRP).
</p>
<p>We first present an $\tilde{O}(n^3)$ time algorithm for $2$FRP in $n$-vertex
directed graphs with arbitrary edge weights and no negative cycles. As $2$FRP
is a generalization of the well-studied Replacement Paths problem (RP) that
asks for the distances between $s$ and $t$ for any single edge failure, $2$FRP
is at least as hard as RP. Since RP in graphs with arbitrary weights is
equivalent in a fine-grained sense to All-Pairs Shortest Paths (APSP)
[Vassilevska Williams and Williams FOCS'10, J.~ACM'18], $2$FRP is at least as
hard as APSP, and thus a substantially subcubic time algorithm in the number of
vertices for $2$FRP would be a breakthrough. Therefore, our algorithm in
$\tilde{O}(n^3)$ time is conditionally nearly optimal. Our algorithm implies an
$\tilde{O}(n^{f+1})$ time algorithm for the $f$FRP problem, giving the first
improvement over the straightforward $O(n^{f+2})$ time algorithm.
</p>
<p>Then we focus on the restriction of $2$FRP to graphs with small integer
weights bounded by $M$ in absolute values. Using fast rectangular matrix
multiplication, we obtain a randomized algorithm that runs in
$\tilde{O}(M^{2/3}n^{2.9153})$ time. This implies an improvement over our
$\tilde{O}(n^{f+1})$ time arbitrary weight algorithm for all $f&gt;1$. We also
present a data structure variant of the algorithm that can trade off
pre-processing and query time. In addition to the algebraic algorithms, we also
give an $n^{8/3-o(1)}$ conditional lower bound for combinatorial $2$FRP
algorithms in directed unweighted graphs.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07024'>Almost Ramanujan Expanders from Arbitrary Expanders via Operator Amplification</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Fernando Granha Jeronimo, Tushant Mittal, Sourya Roy, Avi Wigderson</p><p>We give an efficient algorithm that transforms any bounded degree expander
graph into another that achieves almost optimal (namely, near-quadratic, $d
\leq 1/\lambda^{2+o(1)}$) trade-off between (any desired) spectral expansion
$\lambda$ and degree $d$. Furthermore, the algorithm is local: every vertex can
compute its new neighbors as a subset of its original neighborhood of radius
$O(\log(1/\lambda))$. The optimal quadratic trade-off is known as the Ramanujan
bound, so our construction gives almost Ramanujan expanders from arbitrary
expanders.
</p>
<p>The locality of the transformation preserves structural properties of the
original graph, and thus has many consequences. Applied to Cayley graphs, our
transformation shows that any expanding finite group has almost Ramanujan
expanding generators. Similarly, one can obtain almost optimal explicit
constructions of quantum expanders, dimension expanders, monotone expanders,
etc., from existing (suboptimal) constructions of such objects. Another
consequence is a "derandomized" random walk on the original (suboptimal)
expander with almost optimal convergence rate. Our transformation also applies
when the degree is not bounded or the expansion is not constant.
</p>
<p>We obtain our results by a generalization of Ta-Shma's technique in his
breakthrough paper [STOC 2017], used to obtain explicit almost optimal binary
codes. Specifically, our spectral amplification extends Ta-Shma's analysis of
bias amplification from scalars to matrices of arbitrary dimension in a very
natural way. Curiously, while Ta-Shma's explicit bias amplification
derandomizes a well-known probabilistic argument (underlying the
Gilbert--Varshamov bound), there seems to be no known probabilistic (or other
existential) way of achieving our explicit ("high-dimensional") spectral
amplification.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jeronimo_F/0/1/0/all/0/1">Fernando Granha Jeronimo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_T/0/1/0/all/0/1">Tushant Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1">Sourya Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Wigderson_A/0/1/0/all/0/1">Avi Wigderson</a></p><p>We give an efficient algorithm that transforms any bounded degree expander
graph into another that achieves almost optimal (namely, near-quadratic, $d
\leq 1/\lambda^{2+o(1)}$) trade-off between (any desired) spectral expansion
$\lambda$ and degree $d$. Furthermore, the algorithm is local: every vertex can
compute its new neighbors as a subset of its original neighborhood of radius
$O(\log(1/\lambda))$. The optimal quadratic trade-off is known as the Ramanujan
bound, so our construction gives almost Ramanujan expanders from arbitrary
expanders.
</p>
<p>The locality of the transformation preserves structural properties of the
original graph, and thus has many consequences. Applied to Cayley graphs, our
transformation shows that any expanding finite group has almost Ramanujan
expanding generators. Similarly, one can obtain almost optimal explicit
constructions of quantum expanders, dimension expanders, monotone expanders,
etc., from existing (suboptimal) constructions of such objects. Another
consequence is a "derandomized" random walk on the original (suboptimal)
expander with almost optimal convergence rate. Our transformation also applies
when the degree is not bounded or the expansion is not constant.
</p>
<p>We obtain our results by a generalization of Ta-Shma's technique in his
breakthrough paper [STOC 2017], used to obtain explicit almost optimal binary
codes. Specifically, our spectral amplification extends Ta-Shma's analysis of
bias amplification from scalars to matrices of arbitrary dimension in a very
natural way. Curiously, while Ta-Shma's explicit bias amplification
derandomizes a well-known probabilistic argument (underlying the
Gilbert--Varshamov bound), there seems to be no known probabilistic (or other
existential) way of achieving our explicit ("high-dimensional") spectral
amplification.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07100'>Concurrent Size</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Gal Sela, Erez Petrank</p><p>The size of a data structure (i.e., the number of elements in it) is a widely
used property of a data set. However, for concurrent programs, obtaining a
correct size efficiently is non-trivial. In fact, the literature does not offer
a mechanism to obtain a correct (linearizable) size of a concurrent data set
without resorting to inefficient solutions, such as taking a full snapshot of
the data structure to count the elements, or acquiring one global lock in all
update and size operations. This paper presents a methodology for adding a
concurrent linearizable size operation to sets and dictionaries with a
relatively low performance overhead. Theoretically, the proposed size operation
is wait-free with asymptotic complexity linear in the number of threads
(independently of data-structure size). Practically, we evaluated the
performance overhead by adding size to various concurrent data structures in
Java$-$a skip list, a hash table and a tree. The proposed linearizable size
operation executes faster by orders of magnitude compared to the existing
option of taking a snapshot, while incurring a throughput loss of $1\%-20\%$ on
the original data structure's operations.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Sela_G/0/1/0/all/0/1">Gal Sela</a>, <a href="http://arxiv.org/find/cs/1/au:+Petrank_E/0/1/0/all/0/1">Erez Petrank</a></p><p>The size of a data structure (i.e., the number of elements in it) is a widely
used property of a data set. However, for concurrent programs, obtaining a
correct size efficiently is non-trivial. In fact, the literature does not offer
a mechanism to obtain a correct (linearizable) size of a concurrent data set
without resorting to inefficient solutions, such as taking a full snapshot of
the data structure to count the elements, or acquiring one global lock in all
update and size operations. This paper presents a methodology for adding a
concurrent linearizable size operation to sets and dictionaries with a
relatively low performance overhead. Theoretically, the proposed size operation
is wait-free with asymptotic complexity linear in the number of threads
(independently of data-structure size). Practically, we evaluated the
performance overhead by adding size to various concurrent data structures in
Java$-$a skip list, a hash table and a tree. The proposed linearizable size
operation executes faster by orders of magnitude compared to the existing
option of taking a snapshot, while incurring a throughput loss of $1\%-20\%$ on
the original data structure's operations.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07312'>Multicalibrated Regression for Downstream Fairness</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Ira Globus-Harris, Varun Gupta, Christopher Jung, Michael Kearns, Jamie Morgenstern, Aaron Roth</p><p>We show how to take a regression function $\hat{f}$ that is appropriately
``multicalibrated'' and efficiently post-process it into an approximately error
minimizing classifier satisfying a large variety of fairness constraints. The
post-processing requires no labeled data, and only a modest amount of unlabeled
data and computation. The computational and sample complexity requirements of
computing $\hat f$ are comparable to the requirements for solving a single fair
learning task optimally, but it can in fact be used to solve many different
downstream fairness-constrained learning problems efficiently. Our
post-processing method easily handles intersecting groups, generalizing prior
work on post-processing regression functions to satisfy fairness constraints
that only applied to disjoint groups. Our work extends recent work showing that
multicalibrated regression functions are ``omnipredictors'' (i.e. can be
post-processed to optimally solve unconstrained ERM problems) to constrained
optimization.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Globus_Harris_I/0/1/0/all/0/1">Ira Globus-Harris</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1">Varun Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1">Christopher Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Kearns_M/0/1/0/all/0/1">Michael Kearns</a>, <a href="http://arxiv.org/find/cs/1/au:+Morgenstern_J/0/1/0/all/0/1">Jamie Morgenstern</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1">Aaron Roth</a></p><p>We show how to take a regression function $\hat{f}$ that is appropriately
``multicalibrated'' and efficiently post-process it into an approximately error
minimizing classifier satisfying a large variety of fairness constraints. The
post-processing requires no labeled data, and only a modest amount of unlabeled
data and computation. The computational and sample complexity requirements of
computing $\hat f$ are comparable to the requirements for solving a single fair
learning task optimally, but it can in fact be used to solve many different
downstream fairness-constrained learning problems efficiently. Our
post-processing method easily handles intersecting groups, generalizing prior
work on post-processing regression functions to satisfy fairness constraints
that only applied to disjoint groups. Our work extends recent work showing that
multicalibrated regression functions are ``omnipredictors'' (i.e. can be
post-processed to optimally solve unconstrained ERM problems) to constrained
optimization.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07440'>Envy-freeness in 3D Hedonic Games</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: &#xc1;gnes Cseh, Michael McKay, David Manlove</p><p>We study the problem of partitioning a set of agents into coalitions based on
the agents' additively separable preferences, which can also be viewed as a
hedonic game. We apply three successively weaker solution concepts, namely
envy-freeness, weakly justified envy-freeness, and justified envy-freeness.
</p>
<p>In a model in which coalitions may have any size, trivial solutions exist for
these concepts, which provides a strong motivation for placing restrictions on
coalition size. In this paper, we require feasible coalitions to have size
three. We study the existence of partitions that are envy-free, weakly
justified envy-free, and justified envy-free, and the computational complexity
of finding such partitions, if they exist.
</p>
<p>We present a comprehensive complexity classification, in terms of the
restrictions placed on the agents' preferences. From this, we identify a
general trend that for the three successively weaker solution concepts,
existence and polynomial-time solvability hold under successively weaker
restrictions.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cseh_A/0/1/0/all/0/1">&#xc1;gnes Cseh</a>, <a href="http://arxiv.org/find/cs/1/au:+McKay_M/0/1/0/all/0/1">Michael McKay</a>, <a href="http://arxiv.org/find/cs/1/au:+Manlove_D/0/1/0/all/0/1">David Manlove</a></p><p>We study the problem of partitioning a set of agents into coalitions based on
the agents' additively separable preferences, which can also be viewed as a
hedonic game. We apply three successively weaker solution concepts, namely
envy-freeness, weakly justified envy-freeness, and justified envy-freeness.
</p>
<p>In a model in which coalitions may have any size, trivial solutions exist for
these concepts, which provides a strong motivation for placing restrictions on
coalition size. In this paper, we require feasible coalitions to have size
three. We study the existence of partitions that are envy-free, weakly
justified envy-free, and justified envy-free, and the computational complexity
of finding such partitions, if they exist.
</p>
<p>We present a comprehensive complexity classification, in terms of the
restrictions placed on the agents' preferences. From this, we identify a
general trend that for the three successively weaker solution concepts,
existence and polynomial-time solvability hold under successively weaker
restrictions.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07463'>Omnipredictors for Constrained Optimization</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Lunjia Hu, Inbal Livni-Navon, Omer Reingold, Chutong Yang</p><p>The notion of omnipredictors (Gopalan, Kalai, Reingold, Sharan and Wieder
ITCS 2021), suggested a new paradigm for loss minimization. Rather than
learning a predictor based on a known loss function, omnipredictors can easily
be post-processed to minimize any one of a rich family of loss functions
compared with the loss of a class $C$. It has been shown that such
omnipredictors exist and are implied (for all convex and Lipschitz loss
functions) by the notion of multicalibration from the algorithmic fairness
literature. Nevertheless, it is often the case that the action selected must
obey some additional constraints (such as capacity or parity constraints). In
itself, the original notion of omnipredictors does not apply in this
well-motivated and heavily studied the context of constrained loss
minimization.
</p>
<p>In this paper, we introduce omnipredictors for constrained optimization and
study their complexity and implications. The notion that we introduce allows
the learner to be unaware of the loss function that will be later assigned as
well as the constraints that will be later imposed, as long as the
subpopulations that are used to define these constraints are known.
</p>
<p>The paper shows how to obtain omnipredictors for constrained optimization
problems, relying on appropriate variants of multicalibration. For some
interesting constraints and general loss functions and for general constraints
and some interesting loss functions, we show how omnipredictors are implied by
a variant of multicalibration that is similar in complexity to standard
multicalibration. We demonstrate that in the general case, standard
multicalibration is insufficient and show that omnipredictors are implied by
multicalibration with respect to a class containing all the level sets of
hypotheses in $C$. We also investigate the implications when the constraints
are group fairness notions.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Lunjia Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Livni_Navon_I/0/1/0/all/0/1">Inbal Livni-Navon</a>, <a href="http://arxiv.org/find/cs/1/au:+Reingold_O/0/1/0/all/0/1">Omer Reingold</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chutong Yang</a></p><p>The notion of omnipredictors (Gopalan, Kalai, Reingold, Sharan and Wieder
ITCS 2021), suggested a new paradigm for loss minimization. Rather than
learning a predictor based on a known loss function, omnipredictors can easily
be post-processed to minimize any one of a rich family of loss functions
compared with the loss of a class $C$. It has been shown that such
omnipredictors exist and are implied (for all convex and Lipschitz loss
functions) by the notion of multicalibration from the algorithmic fairness
literature. Nevertheless, it is often the case that the action selected must
obey some additional constraints (such as capacity or parity constraints). In
itself, the original notion of omnipredictors does not apply in this
well-motivated and heavily studied the context of constrained loss
minimization.
</p>
<p>In this paper, we introduce omnipredictors for constrained optimization and
study their complexity and implications. The notion that we introduce allows
the learner to be unaware of the loss function that will be later assigned as
well as the constraints that will be later imposed, as long as the
subpopulations that are used to define these constraints are known.
</p>
<p>The paper shows how to obtain omnipredictors for constrained optimization
problems, relying on appropriate variants of multicalibration. For some
interesting constraints and general loss functions and for general constraints
and some interesting loss functions, we show how omnipredictors are implied by
a variant of multicalibration that is similar in complexity to standard
multicalibration. We demonstrate that in the general case, standard
multicalibration is insufficient and show that omnipredictors are implied by
multicalibration with respect to a class containing all the level sets of
hypotheses in $C$. We also investigate the implications when the constraints
are group fairness notions.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07520'>On (Random-order) Online Contention Resolution Schemes for the Matching Polytope of (Bipartite) Graphs</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Calum MacRury, Will Ma, Nathaniel Grammel</p><p>We present new results for online contention resolution schemes for the
matching polytope of graphs, in the random-order (RCRS) and adversarial (OCRS)
arrival models. Our results include improved selectability guarantees (i.e.,
lower bounds), as well as new impossibility results (i.e., upper bounds). By
well-known reductions to the prophet (secretary) matching problem, a
$c$-selectable OCRS (RCRS) implies a $c$-competitive algorithm for adversarial
(random order) edge arrivals. Similar reductions are also known for the
query-commit matching problem. For the adversarial arrival model, we present a
new analysis of the OCRS of Ezra et al.~(EC, 2020). We show that this scheme is
$0.344$-selectable for general graphs and $0.349$-selectable for bipartite
graphs, improving on the previous $0.337$ selectability result for this
algorithm. We also show that the selectability of this scheme cannot be greater
than $0.361$ for general graphs and $0.382$ for bipartite graphs. We further
show that no OCRS can achieve a selectability greater than $0.4$ for general
graphs, and $0.433$ for bipartite graphs.
</p>
<p>For random-order arrivals, we present two attenuation-based schemes which use
new attenuation functions. Our first RCRS is $0.474$-selectable for general
graphs, and our second is $0.476$-selectable for bipartite graphs. These
results improve upon the recent $0.45$ (and $0.456$) selectability results for
general graphs (respectively, bipartite graphs) due to Pollner et al.~(EC,
2022). On general graphs, our 0.474-selectable RCRS provides the best known
positive result even for offline contention resolution, and also for the
correlation gap. We conclude by proving a fundamental upper bound of 0.5 on the
selectability of RCRS, using bipartite graphs.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+MacRury_C/0/1/0/all/0/1">Calum MacRury</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Will Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Grammel_N/0/1/0/all/0/1">Nathaniel Grammel</a></p><p>We present new results for online contention resolution schemes for the
matching polytope of graphs, in the random-order (RCRS) and adversarial (OCRS)
arrival models. Our results include improved selectability guarantees (i.e.,
lower bounds), as well as new impossibility results (i.e., upper bounds). By
well-known reductions to the prophet (secretary) matching problem, a
$c$-selectable OCRS (RCRS) implies a $c$-competitive algorithm for adversarial
(random order) edge arrivals. Similar reductions are also known for the
query-commit matching problem. For the adversarial arrival model, we present a
new analysis of the OCRS of Ezra et al.~(EC, 2020). We show that this scheme is
$0.344$-selectable for general graphs and $0.349$-selectable for bipartite
graphs, improving on the previous $0.337$ selectability result for this
algorithm. We also show that the selectability of this scheme cannot be greater
than $0.361$ for general graphs and $0.382$ for bipartite graphs. We further
show that no OCRS can achieve a selectability greater than $0.4$ for general
graphs, and $0.433$ for bipartite graphs.
</p>
<p>For random-order arrivals, we present two attenuation-based schemes which use
new attenuation functions. Our first RCRS is $0.474$-selectable for general
graphs, and our second is $0.476$-selectable for bipartite graphs. These
results improve upon the recent $0.45$ (and $0.456$) selectability results for
general graphs (respectively, bipartite graphs) due to Pollner et al.~(EC,
2022). On general graphs, our 0.474-selectable RCRS provides the best known
positive result even for offline contention resolution, and also for the
correlation gap. We conclude by proving a fundamental upper bound of 0.5 on the
selectability of RCRS, using bipartite graphs.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.07524'>$\tilde{O}(n+\mathrm{poly}(k))$-time Algorithm for Bounded Tree Edit Distance</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Debarati Das, Jacob Gilbert, MohammadTaghi Hajiaghayi, Tomasz Kociumaka, Barna Saha, Hamed Saleh</p><p>Computing the edit distance of two strings is one of the most basic problems
in computer science and combinatorial optimization. Tree edit distance is a
natural generalization of edit distance in which the task is to compute a
measure of dissimilarity between two (unweighted) rooted trees with node
labels. Perhaps the most notable recent application of tree edit distance is in
NoSQL big databases, such as MongoDB, where each row of the database is a JSON
document represented as a labeled rooted tree, and finding dissimilarity
between two rows is a basic operation. Until recently, the fastest algorithm
for tree edit distance ran in cubic time (Demaine, Mozes, Rossman, Weimann;
TALG'10); however, Mao (FOCS'21) broke the cubic barrier for the tree edit
distance problem using fast matrix multiplication.
</p>
<p>Given a parameter $k$ as an upper bound on the distance, an $O(n+k^2)$-time
algorithm for edit distance has been known since the 1980s due to the works of
Myers (Algorithmica'86) and Landau and Vishkin (JCSS'88). The existence of an
$\tilde{O}(n+\mathrm{poly}(k))$-time algorithm for tree edit distance has been
posed as an open question, e.g., by Akmal and Jin (ICALP'21), who gave a
state-of-the-art $\tilde{O}(nk^2)$-time algorithm. In this paper, we answer
this question positively.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1">Debarati Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilbert_J/0/1/0/all/0/1">Jacob Gilbert</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1">MohammadTaghi Hajiaghayi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kociumaka_T/0/1/0/all/0/1">Tomasz Kociumaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_B/0/1/0/all/0/1">Barna Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Saleh_H/0/1/0/all/0/1">Hamed Saleh</a></p><p>Computing the edit distance of two strings is one of the most basic problems
in computer science and combinatorial optimization. Tree edit distance is a
natural generalization of edit distance in which the task is to compute a
measure of dissimilarity between two (unweighted) rooted trees with node
labels. Perhaps the most notable recent application of tree edit distance is in
NoSQL big databases, such as MongoDB, where each row of the database is a JSON
document represented as a labeled rooted tree, and finding dissimilarity
between two rows is a basic operation. Until recently, the fastest algorithm
for tree edit distance ran in cubic time (Demaine, Mozes, Rossman, Weimann;
TALG'10); however, Mao (FOCS'21) broke the cubic barrier for the tree edit
distance problem using fast matrix multiplication.
</p>
<p>Given a parameter $k$ as an upper bound on the distance, an $O(n+k^2)$-time
algorithm for edit distance has been known since the 1980s due to the works of
Myers (Algorithmica'86) and Landau and Vishkin (JCSS'88). The existence of an
$\tilde{O}(n+\mathrm{poly}(k))$-time algorithm for tree edit distance has been
posed as an open question, e.g., by Akmal and Jin (ICALP'21), who gave a
state-of-the-art $\tilde{O}(nk^2)$-time algorithm. In this paper, we answer
this question positively.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-16T00:30:00Z">Friday, September 16 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Thursday, September 15
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://11011110.github.io/blog/2022/09/15/linkage.html'>Linkage</a></h3>
          <p class='item-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          Lithophanes, a 19th-century art medium involving backlit translucent engravings, revived via 3d printing as a single format for scientific images that blind people can read by feeling and sighted people can see (\(\mathbb{M}\), original research paper).
        
        </div>

        <div class='item-content item-summary'>
        
          
          <ul>
  <li>
    <p>Lithophanes, a 19th-century art medium involving backlit translucent engravings, <a href="https://arstechnica.com/science/2022/08/19th-century-art-form-revived-to-make-tactile-science-graphics-for-the-blind/">revived via 3d printing as a single format for scientific images that blind people can read by feeling and sighted people can see</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108927524393418764">\(\mathbb{M}\)</a>,</span> <a href="https://doi.org/10.1126/sciadv.abq2640">original research paper</a>).</p>
  </li>
  <li>
    <p>A quick ânot a Reuleaux triangleâ link: <a href="https://www.instructables.com/Lego-Triangle/">Lego triangles</a>, incorrectly embellished as <a href="https://makezine.com/article/maker-news/lego-reuleaux-triangles/">Lego Reuleaux triangles</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108933430914328377">\(\mathbb{M}\)</a>).</span> The second link even goes on to say âI donât think [they] are quite convex enough to be proper Reuleaux trianglesâ. And in this itâs correct, if âconvexâ is interpreted to mean âbulgyâ. You can tell because the corners are right angles. Proper Reuleaux triangles would have \(120^\circ\) corners.</p>
  </li>
  <li>
    <p>As part of a search for real-world applications of combinatorial design theory, Jeremy Kun asks: â<a href="https://mathstodon.xyz/@j2kun/108936354352902797">What is the authoritative text on combinatorial designs, anyway?</a>â I suspect that the shakeup in the area caused by Peter Keevashâs use of the probabilistic method has caused what used to be the authoritative texts to become obsolete.</p>
  </li>
  <li>
    <p><a href="https://youtube.com/shorts/KYMYshbhKcw">Squaring the circle illusion</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@henryseg/108940642070353471">\(\mathbb{M}\)</a>).</span> Henry Segerman 3d-prints a shape that, when rotated \(180^\circ\), changes appearance from a circle to a square. With the same area in the viewing frame, even.</p>
  </li>
  <li>
    <p>Finding a vertex-to-vertex and edge-to-edge mapping (âhomomorphismâ) from an input directed graph to a fixed oriented tree or cycle can be either polynomial-time or <span style="white-space:nowrap">\(\mathsf{NP}\)-complete,</span> depending on the target. But <a href="https://cstheory.stackexchange.com/questions/33836/complexity-of-digraph-homomorphism-to-an-oriented-cycle">the targets for which it is <span style="white-space:nowrap">\(\mathsf{NP}\)-complete</span> are surprisingly complicated!</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108950099488030475">\(\mathbb{M}\)</a>)</span>  <a href="https://arxiv.org/abs/2205.07528">A recent search</a> found the smallest hard tree to have <span style="white-space:nowrap">\(20\) vertices,</span> and the smallest hard cycle to <span style="white-space:nowrap">have \(26\).</span></p>
  </li>
  <li>
    <p>Tamal Dey and Yusu Wang have a new graduate-level textbook on computational topology <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108953135639829468">\(\mathbb{M}\)</a>):</span> <em><a href="https://www.cs.purdue.edu/homes/tamaldey/book/CTDAbook/CTDAbook.pdf">Computational Topology for Data Analysis</a></em> (<a href="https://doi.org/10.1017/9781009099950">Cambridge University Press, 2022</a>). <a href="https://www.maa.org/press/maa-reviews/computational-topology-for-data-analysis">Ellen Gasparovic reviews it for the MAA</a>.</p>
  </li>
  <li>
    <p><a href="https://web.archive.org/web/20220908210937/https://www.nytimes.com/interactive/2022/09/08/world/europe/succession-royal-family.html">In case anyone else is looking for a topical real-world example of a depth-first traversal of a tree</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108965503136254522">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p>Chegg stops pretending not to have coursework-cheating as their main business; <a href="https://www.chronicle.com/article/some-students-use-chegg-to-cheat-the-site-has-stopped-helping-colleges-catch-them">will no longer cooperate with university cheating investigations</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108970379605304595">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="http://www.math.brown.edu/reschwar/PosterPappus/pappus.png">Schwartzâs Pappus fractal</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108972366289751471">\(\mathbb{M}\)</a>,</span> <a href="http://www.math.brown.edu/reschwar/PosterPappus/pappus.pdf">explanation</a>). Start with two separate lines of three points \(abc\) <span style="white-space:nowrap">and \(ABC\).</span> By <a href="https://en.wikipedia.org/wiki/Pappus%27s_hexagon_theorem">Pappusâs theorem</a> the diagonal crossing points <span style="white-space:nowrap">\(\alpha=aB\cdot Ab\),</span> <span style="white-space:nowrap">\(\beta=aC\cdot Ac\),</span> and \(\gamma=bC\cdot Bc\) form another line of three <span style="white-space:nowrap">points \(\alpha\beta\gamma\).</span> Now recurse with \(abc\) <span style="white-space:nowrap">and \(\alpha\beta\gamma\),</span> and again with \(\alpha\beta\gamma\) <span style="white-space:nowrap">and \(ABC\).</span> You get a nice lightning-bolt fractal shape.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/html/2209.04402">This yearâs <em>Graph Drawing</em> conference proceedings are online</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108982719390301261">\(\mathbb{M}\)</a>).</span>  As in past years, theyâre using a system where the proceedings are published both on arXiv and in Springer LNCS.</p>
  </li>
  <li>
    <p><a href="https://ris.utwente.nl/ws/portalfiles/portal/275927505/3e2a9e5b2fad237a3d35f36fa2c5f44552f2.pdf">An analysis of online exam-proctoring tool Proctorio finds it completely ineffective at catching cheaters</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108984339518916646">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=32744976">via</a>). âThe use of online proctoring is therefore best compared to taking a placebo: it has some positive influence, not because it works but because people believe that it works â¦ policy makers would do well to balance the cost of deploying it (which can be
considerable) against the marginal benefits of this placebo effect.â</p>
  </li>
  <li>
    <p><a href="https://langorigami.com/publication/paper-pentasia-an-aperiodic-surface-in-modular-origami/">Langâs paper pentasia</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108995556838713267">\(\mathbb{M}\)</a>).</span> The kite and dart Penrose tiling lifts to a surface in 3d with two equilateral triangles per tile, overhanging for the darts. Robert Lang made these nice physical models. Explanations by <a href="https://link.springer.com/article/10.1007/s00283-021-10088-4">Barry Cipra on the 1993 discovery of this surface with Conway</a> and <a href="https://langorigami.com/wp-content/uploads/2015/09/paper-pentasia.pdf">Lang and Barry Hayes, also on a related 3d surface for the rhombic Penrose tiling</a>.</p>
  </li>
  <li>
    <p>Three more new Wikipedia Good Articles <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/108998160700266138">\(\mathbb{M}\)</a>):</span></p>

    <ul>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Euclidean_minimum_spanning_tree">Euclidean minimum spanning tree</a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Doyle_spiral">Doyle spiral</a>, spiraling circle packings in which each circle is surrounded by a ring of six tangent circles</p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Laves_graph">Laves graph</a>, a highly-symmetric 3-regular infinite graph in 3d</p>
      </li>
    </ul>
  </li>
  <li>
    <p>An algorithmic version of <a href="en.wikipedia.org/wiki/Mantel's theorem">Mantelâs theorem</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109004244575186009">\(\mathbb{M}\)</a>):</span> If an <span style="white-space:nowrap">\(n\)-vertex</span> graph has more than <span style="white-space:nowrap">\(\bigl\lfloor\tfrac{n^2}{4}\bigr\rfloor\) edges</span> then we can find a triangle in it in linear time.</p>

    <p><em>Proof:</em> Delete min-degree vertices, maintaining the edge excess, until the remaining \(k\) vertices all have <span style="white-space:nowrap">degree \(\ge\tfrac{k}{2}\).</span> If <span style="white-space:nowrap">\(k\) is odd,</span> all <span style="white-space:nowrap">have \(\ge \tfrac{k+1}{2}\);</span> if <span style="white-space:nowrap">\(k\) is even,</span> at least one <span style="white-space:nowrap">has \(\ge \tfrac{k}{2}+1\).</span> Then a max-degree vertex and any neighbor have together <span style="white-space:nowrap">\(\ge k+1\) incidences,</span> so by the pigeonhole principle they have a common neighbor forming a <span style="white-space:nowrap">triangle. \(\Box\)</span></p>
  </li>
</ul><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-15T22:36:00Z">Thursday, September 15 2022, 22:36</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://cstheory-jobs.org/2022/09/15/postdoc-at-technion-faculty-of-computer-science-israel-apply-by-october-31-2022/'>PostDoc at Technion faculty of Computer Science (Israel) (apply by October 31, 2022)</a></h3>
          <p class='item-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          Looking for a PostDoc interested in the area of sublinear algorithms and property testing, for up to four years. PhD applications will also be considered. Website: eldar.cswp.cs.technion.ac.il/positions/ Email: eldar@cs.technion.ac.il
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p>Looking for a PostDoc interested in the area of sublinear algorithms and property testing, for up to four years. PhD applications will also be considered.</p>
<p>Website: <a href="https://eldar.cswp.cs.technion.ac.il/positions/">https://eldar.cswp.cs.technion.ac.il/positions/</a><br />
Email: eldar@cs.technion.ac.il</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-15T21:13:46Z">Thursday, September 15 2022, 21:13</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://blog.computationalcomplexity.org/2022/09/monarachy-problem-with-definitions.html'>Monarachy: A Problem with Definitions</a></h3>
          <p class='item-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>&nbsp;As I am sure you know, Queen Elizabeth II passed away at the age of 96 recently.&nbsp; I am not a royal-watcher, but I am a royal-watcher-watcher. That is, the question of why people care about the lives of these people intrigues me. A few notes</p><p>1) Was she a good Queen? People tend to think so; however, since the job is somewhat ill-defined its hard to say.&nbsp;</p><p>2) The Queen is supposed to be above politics (she does not vote- I was surprised to find out that legally she can, but she really can't). We know very few of Queen Elizabeth II's opinions on political events. But the notion of political is not well defined. One would think that if she did an appeal for people to take the COVID vax that would not be political, but somehow it is (I do not know if she did such an appeal). King Charles III believes in global warming and that we need to do something about it. This again should not be political but is.&nbsp;</p><p>3) She is the second longest reigning Monarch. First is King Louis XIV who first became king at the age of 4. I had a blog complaining about this&nbsp;here. However, there is a more interesting point I want to make. From the first to the last day of King Louis XIV reign not much had changed. Technology, politics, other things just didn't change much. By contrast the world changed A LOT between Queen Elizabeth II first and last day:</p><p>a) The British were an important power in 1952. Less so now.</p><p>b) When her father died she was in Kenya and it took 4 hours to get the news to her. Now that would be immediate.&nbsp;</p><p>c) Divorce was considered bad in 1952 and is why King Edmond VIII could not be king (he wanted to marry a twice-divorced women whose ex-husbands were still alive). And now three of the Queen's children have been divorced.</p><p>d) Gay people.. enough said. There has even been a royal gay wedding, see&nbsp;here</p><p>Black people (can't call them African-Americans), Women,... you fill it in.&nbsp;</p><p>e) When Charles wanted to get married it seemed to be important that he marry a virgin. We cannot imagine this mentality anymore. When Prince William and Kate got married they were already living together and this was NOT an issue for ANYONE. I looked up what the Church of England thought of it and all I got was some very bland comments like That's what young people do nowadays.&nbsp;</p><p>3) Is the monarchy a good thing? As an American I feel I do not have a right to an opinion. If the citizens of the United Kingdom approve of the monarch (polls show they do) then who am I do tell them they are wrong? Even so, lets look at reasons for it</p><p>a) Tourism. It has been said that the Monarchy leads to MONEY from tourism. So it is worth the price? Nobody seems to know and it would be hard to tell. However, I don't think the citizens of the United Kingdom view&nbsp; money as the reason for Monarchy. The American analog is giving Disneyland tax breaks to be in Florida which generates jobs. I doubt they think of the Monarchy in those mundane transactional terms.&nbsp;</p><p>b) CS Lewis said&nbsp;</p><p>Where men are forbidden to honour a king they honour millionaires, athletes, or film stars instead: even famous prostitutes and gangsters. For spiritual nature, like bodily nature, will be served; deny it food and it will gobble poison.</p><p>This is&nbsp; bit odd- they must all pretend to like the monarchy to make it work. A long time ago when Charles and Dianna were both having affairs, 80% of the citizens the United Kingdom thought that was okay so long as they are discrete so the people&nbsp;don't find out. But- those ARE the people.</p><p>Also odd- CS Lewis was a theologian and a&nbsp; believing Christian; however, his comment above can apply to God as well as to Kings.&nbsp;</p><p><br></p><p><br></p><p><br></p><p><br></p><p>By gasarch</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p>&nbsp;As I am sure you know, Queen Elizabeth II passed away at the age of 96 recently.&nbsp; I am not a royal-watcher, but I am a royal-watcher-watcher. That is, the question of why people care about the lives of these people intrigues me. A few notes</p><p>1) Was she a <i>good Queen?</i> People tend to think so; however, since the job is somewhat ill-defined its hard to say.&nbsp;</p><p>2) The Queen is supposed to be above politics (she does not vote- I was surprised to find out that legally she can, but she really can't). We know very few of Queen Elizabeth II's opinions on political events. But the notion of <i>political </i>is not well defined. One would think that if she did an appeal for people to take the COVID vax that would not be political, but somehow it is (I do not know if she did such an appeal). King Charles III believes in global warming and that we need to do something about it. This again should not be political but is.&nbsp;</p><p>3) She is the second longest reigning Monarch. First is King Louis XIV who first became king at the age of 4. I had a blog complaining about this&nbsp;<a href="https://blog.computationalcomplexity.org/2022/05/queen-elizabeth-is-3rd-longest-reigning.html">here</a>. However, there is a more interesting point I want to make. From the first to the last day of King Louis XIV reign not much had changed. Technology, politics, other things just didn't change much. By contrast the world changed A LOT between Queen Elizabeth II first and last day:</p><p>a) The British were an important power in 1952. Less so now.</p><p>b) When her father died she was in Kenya and it took 4 hours to get the news to her. Now that would be immediate.&nbsp;</p><p>c) Divorce was considered bad in 1952 and is why King Edmond VIII could not be king (he wanted to marry a twice-divorced women whose ex-husbands were still alive). And now three of the Queen's children have been divorced.</p><p>d) Gay people.. enough said. There has even been a royal gay wedding, see&nbsp;<a href="https://www.dailymail.co.uk/news/article-5849971/Royal-familys-gay-wedding-story-Queens-cousin-Lord-Ivar-Mountbatten.html">here</a></p><p>Black people (can't call them African-Americans), Women,... you fill it in.&nbsp;</p><p>e) When Charles wanted to get married it seemed to be important that he marry a virgin. We cannot imagine this mentality anymore. When Prince William and Kate got married they were already living together and this was NOT an issue for ANYONE. I looked up what the Church of England thought of it and all I got was some very bland comments like <i>That's what young people do nowadays.&nbsp;</i></p><p>3) Is the monarchy a good thing? As an American I feel I do not have a right to an opinion. If the citizens of the United Kingdom approve of the monarch (polls show they do) then who am I do tell them they are wrong? Even so, lets look at reasons for it</p><p>a) Tourism. It has been said that the Monarchy leads to MONEY from tourism. So it is worth the price? Nobody seems to know and it would be hard to tell. However, I don't think the citizens of the United Kingdom view&nbsp; money as the reason for Monarchy. The American analog is giving Disneyland tax breaks to be in Florida which generates jobs. I doubt they think of the Monarchy in those mundane transactional terms.&nbsp;</p><p>b) CS Lewis said&nbsp;</p><p><i>Where men are forbidden to honour a king they honour millionaires, athletes, or film stars instead: even famous prostitutes and gangsters. For spiritual nature, like bodily nature, will be served; deny it food and it will gobble poison.</i></p><p>This is&nbsp; bit odd- they must all pretend to like the monarchy to make it work. A long time ago when Charles and Dianna were both having affairs, 80% of the citizens the United Kingdom thought that was okay so long as they are discrete so <i>the people</i>&nbsp;don't find out. But- those ARE the people.</p><p>Also odd- CS Lewis was a theologian and a&nbsp; believing Christian; however, his comment above can apply to God as well as to Kings.&nbsp;</p><p><i><br /></i></p><p><br /></p><p><br /></p><p><br /></p><p class="authors">By gasarch</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-15T20:41:00Z">Thursday, September 15 2022, 20:41</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://eccc.weizmann.ac.il/report/2022/129'>TR22-129 |  Binary Codes with Resilience Beyond 1/4 via Interaction | 

	Klim Efremenko, 

	Gillat Kol, 

	Raghuvansh Saxena, 

	Zhijun Zhang</a></h3>
          <p class='item-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          In the reliable transmission problem, a sender, Alice, wishes to transmit a bit-string x to a remote receiver, Bob, over a binary channel with adversarial noise. The solution to this problem is to encode x using an error correcting code. As it is long known that the distance of binary codes is at most 1/2, reliable transmission is possible only if the channel corrupts (flips) at most a 1/4-fraction of the communicated bits.
We revisit the reliable transmission problem in the two-way setting, where both Alice and Bob can send bits to each other. Our main result is the construction of two-way error correcting codes that are resilient to a constant fraction of corruptions strictly larger than 1/4. Moreover, our code has constant rate and requires Bob to only send one short message. We mention that our result resolves an open problem by Haeupler, Kamath, and Velingker [APPROX-RANDOM, 2015] and by Gupta, Kalai, and Zhang [STOC, 2022].
Curiously, our new two-way code requires a fresh perspective on classical error correcting codes: While classical codes have only one distance guarantee for all pairs of codewords (i.e., the minimum distance), we construct codes where the distance between a pair of codewords depends on the âcompatibilityâ of the messages they encode. We also prove that such codes are necessary for our result.
        
        </div>

        <div class='item-content item-summary'>
        
          
          In the reliable transmission problem, a sender, Alice, wishes to transmit a bit-string x to a remote receiver, Bob, over a binary channel with adversarial noise. The solution to this problem is to encode x using an error correcting code. As it is long known that the distance of binary codes is at most 1/2, reliable transmission is possible only if the channel corrupts (flips) at most a 1/4-fraction of the communicated bits.
We revisit the reliable transmission problem in the two-way setting, where both Alice and Bob can send bits to each other. Our main result is the construction of two-way error correcting codes that are resilient to a constant fraction of corruptions strictly larger than 1/4. Moreover, our code has constant rate and requires Bob to only send one short message. We mention that our result resolves an open problem by Haeupler, Kamath, and Velingker [APPROX-RANDOM, 2015] and by Gupta, Kalai, and Zhang [STOC, 2022].
Curiously, our new two-way code requires a fresh perspective on classical error correcting codes: While classical codes have only one distance guarantee for all pairs of codewords (i.e., the minimum distance), we construct codes where the distance between a pair of codewords depends on the âcompatibilityâ of the messages they encode. We also prove that such codes are necessary for our result.
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-15T19:10:17Z">Thursday, September 15 2022, 19:10</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.06349'>Structure and Complexity of Graphical Designs for Weighted Graphs through Eigenpolytopes</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Catherine Babecki, David Shiroma</p><p>We extend the theory of graphical designs, which are quadrature rules for
graphs, to positively weighted graphs. Through Gale duality for polytopes, we
show that there is a bijection between graphical designs and the faces of
eigenpolytopes associated to the graph. This bijection proves the existence of
graphical designs with positive quadrature weights, and upper bounds the size
of a graphical design. We further show that any combinatorial polytope appears
as the eigenpolytope of a positively weighted graph. Through this universality,
we establish two complexity results for graphical designs: it is strongly
NP-complete to determine if there is a graphical design smaller than the
mentioned upper bound, and it is #P-complete to count the number of minimal
graphical designs.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Babecki_C/0/1/0/all/0/1">Catherine Babecki</a>, <a href="http://arxiv.org/find/math/1/au:+Shiroma_D/0/1/0/all/0/1">David Shiroma</a></p><p>We extend the theory of graphical designs, which are quadrature rules for
graphs, to positively weighted graphs. Through Gale duality for polytopes, we
show that there is a bijection between graphical designs and the faces of
eigenpolytopes associated to the graph. This bijection proves the existence of
graphical designs with positive quadrature weights, and upper bounds the size
of a graphical design. We further show that any combinatorial polytope appears
as the eigenpolytope of a positively weighted graph. Through this universality,
we establish two complexity results for graphical designs: it is strongly
NP-complete to determine if there is a graphical design smaller than the
mentioned upper bound, and it is #P-complete to count the number of minimal
graphical designs.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-15T00:30:00Z">Thursday, September 15 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.06661'>Red Blue Set Cover Problem on Axis-Parallel Hyperplanes and Other Objects</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: V P Abidha, Pradeesha Ashok</p><p>Given a universe $\mathcal{U}=R \cup B$ of a finite set of red elements $R$,
and a finite set of blue elements $B$ and a family $\mathcal{F}$ of subsets of
$\mathcal{U}$, the \RBSC problem is to find a subset $\mathcal{F}'$ of
$\mathcal{F}$ that covers all blue elements of $B$ and minimum number of red
elements from $R$.
</p>
<p>We prove that the \RBSC problem is NP-hard even when $R$ and $B$ respectively
are sets of red and blue points in ${\rm I\!R}^2$ and the sets in $\mathcal{F}$
are defined by axis-parallel lines i.e, every set is a maximal set of points
with the same $x$ or $y$ coordinate.
</p>
<p>We then study the parameterized complexity of a generalization of this
problem, where $\mathcal{U}$ is a set of points in ${\rm I\!R}^d$ and
$\mathcal{F}$ is a collection of set of axis-parallel hyperplanes in ${\rm
I\!R}^d$, under different parameterizations. For every parameter, we show that
the problem is fixed-parameter tractable and also show the existence of a
polynomial kernel.
</p>
<p>We further consider the \RBSC problem for some special types of rectangles in
${\rm I\!R}^2$.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Abidha_V/0/1/0/all/0/1">V P Abidha</a>, <a href="http://arxiv.org/find/cs/1/au:+Ashok_P/0/1/0/all/0/1">Pradeesha Ashok</a></p><p>Given a universe $\mathcal{U}=R \cup B$ of a finite set of red elements $R$,
and a finite set of blue elements $B$ and a family $\mathcal{F}$ of subsets of
$\mathcal{U}$, the \RBSC problem is to find a subset $\mathcal{F}'$ of
$\mathcal{F}$ that covers all blue elements of $B$ and minimum number of red
elements from $R$.
</p>
<p>We prove that the \RBSC problem is NP-hard even when $R$ and $B$ respectively
are sets of red and blue points in ${\rm I\!R}^2$ and the sets in $\mathcal{F}$
are defined by axis-parallel lines i.e, every set is a maximal set of points
with the same $x$ or $y$ coordinate.
</p>
<p>We then study the parameterized complexity of a generalization of this
problem, where $\mathcal{U}$ is a set of points in ${\rm I\!R}^d$ and
$\mathcal{F}$ is a collection of set of axis-parallel hyperplanes in ${\rm
I\!R}^d$, under different parameterizations. For every parameter, we show that
the problem is fixed-parameter tractable and also show the existence of a
polynomial kernel.
</p>
<p>We further consider the \RBSC problem for some special types of rectangles in
${\rm I\!R}^2$.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-15T00:30:00Z">Thursday, September 15 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.06210'>Scheduling Algorithms for Federated Learning with Minimal Energy Consumption</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: La&#xe9;rcio Lima Pilla (STORM)</p><p>Federated Learning (FL) has opened the opportunity for collaboratively
training machine learning models on heterogeneous mobile or Edge devices while
keeping local data private.With an increase in its adoption, a growing concern
is related to its economic and environmental cost (as is also the case for
other machine learning techniques).Unfortunately, little work has been done to
optimize its energy consumption or emissions of carbon dioxide or equivalents,
as energy minimization is usually left as a secondary objective.In this paper,
we investigate the problem of minimizing the energy consumption of FL training
on heterogeneous devices by controlling the workload distribution.We model this
as the Minimal Cost FL Schedule problem, a total cost minimization problem with
identical, independent, and atomic tasks that have to be assigned to
heterogeneous resources with arbitrary cost functions.We propose a
pseudo-polynomial optimal solution to the problem based on the previously
unexplored Multiple-Choice Minimum-Cost Maximal Knapsack Packing Problem.We
also provide four algorithms for scenarios where cost functions are
monotonically increasing and follow the same behavior.These solutions are
likewise applicable on the minimization of other kinds of costs, and in other
one-dimensional data partition problems.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Pilla_L/0/1/0/all/0/1">La&#xe9;rcio Lima Pilla</a> (STORM)</p><p>Federated Learning (FL) has opened the opportunity for collaboratively
training machine learning models on heterogeneous mobile or Edge devices while
keeping local data private.With an increase in its adoption, a growing concern
is related to its economic and environmental cost (as is also the case for
other machine learning techniques).Unfortunately, little work has been done to
optimize its energy consumption or emissions of carbon dioxide or equivalents,
as energy minimization is usually left as a secondary objective.In this paper,
we investigate the problem of minimizing the energy consumption of FL training
on heterogeneous devices by controlling the workload distribution.We model this
as the Minimal Cost FL Schedule problem, a total cost minimization problem with
identical, independent, and atomic tasks that have to be assigned to
heterogeneous resources with arbitrary cost functions.We propose a
pseudo-polynomial optimal solution to the problem based on the previously
unexplored Multiple-Choice Minimum-Cost Maximal Knapsack Packing Problem.We
also provide four algorithms for scenarios where cost functions are
monotonically increasing and follow the same behavior.These solutions are
likewise applicable on the minimization of other kinds of costs, and in other
one-dimensional data partition problems.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-15T00:30:00Z">Thursday, September 15 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.06374'>Algorithmic (Semi-)Conjugacy via Koopman Operator Theory</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: William T. Redman, Maria Fonoberova, Ryan Mohr, Ioannis G. Kevrekidis, Igor Mezi&#x107;</p><p>Iterative algorithms are of utmost importance in decision and control. With
an ever growing number of algorithms being developed, distributed, and
proprietarized, there is a similarly growing need for methods that can provide
classification and comparison. By viewing iterative algorithms as discrete-time
dynamical systems, we leverage Koopman operator theory to identify
(semi-)conjugacies between algorithms using their spectral properties. This
provides a general framework with which to classify and compare algorithms.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Redman_W/0/1/0/all/0/1">William T. Redman</a>, <a href="http://arxiv.org/find/cs/1/au:+Fonoberova_M/0/1/0/all/0/1">Maria Fonoberova</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohr_R/0/1/0/all/0/1">Ryan Mohr</a>, <a href="http://arxiv.org/find/cs/1/au:+Kevrekidis_I/0/1/0/all/0/1">Ioannis G. Kevrekidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mezic_I/0/1/0/all/0/1">Igor Mezi&#x107;</a></p><p>Iterative algorithms are of utmost importance in decision and control. With
an ever growing number of algorithms being developed, distributed, and
proprietarized, there is a similarly growing need for methods that can provide
classification and comparison. By viewing iterative algorithms as discrete-time
dynamical systems, we leverage Koopman operator theory to identify
(semi-)conjugacies between algorithms using their spectral properties. This
provides a general framework with which to classify and compare algorithms.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-15T00:30:00Z">Thursday, September 15 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.06450'>Performance Evaluation of Parallel Algorithms</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Donald Ene Vincent Ike Anireh</p><p>Evaluating how well a whole system or set of subsystems performs is one of
the primary objectives of performance testing. We can tell via performance
assessment if the architecture implementation meets the design objectives.
Performance evaluations of several parallel algorithms are compared in this
study. Both theoretical and experimental methods are used in performance
assessment as a subdiscipline in computer science. The parallel method
outperforms its sequential counterpart in terms of throughput. The parallel
algorithm's performance (speedup) is examined, as shown in the result.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Anireh_D/0/1/0/all/0/1">Donald Ene Vincent Ike Anireh</a></p><p>Evaluating how well a whole system or set of subsystems performs is one of
the primary objectives of performance testing. We can tell via performance
assessment if the architecture implementation meets the design objectives.
Performance evaluations of several parallel algorithms are compared in this
study. Both theoretical and experimental methods are used in performance
assessment as a subdiscipline in computer science. The parallel method
outperforms its sequential counterpart in terms of throughput. The parallel
algorithm's performance (speedup) is examined, as shown in the result.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-15T00:30:00Z">Thursday, September 15 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.06695'>Parameterized algorithms for node connectivity augmentation problems</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Zeev Nutov</p><p>A graph $G$ is $k$-out-connected from its node $s$ if it contains $k$
internally disjoint $sv$-paths to every node $v$; $G$ is $k$-connected if it is
$k$-out-connected from every node. In connectivity augmentation problems the
goal is to augment a graph $G_0=(V,E_0)$ by a minimum costs edge set $J$ such
that $G_0 \cup J$ has higher connectivity than $G_0$. In the
$k$-Out-Connectivity Augmentation ($k$-OCA) problem, $G_0$ is
$(k-1)$-out-connected from $s$ and $G_0 \cup J$ should be $k$-out-connected
from $s$; in the $k$-Connectivity Augmentation ($k$-CA) problem $G_0$ is
$(k-1)$-connected and $G_0 \cup J$ should be $k$-connected. The parameterized
complexity status of these problems was open even for $k=3$ and unit costs. We
will show that $k$-OCA and $3$-CA can be solved in time $9^p \cdot n^{O(1)}$,
where $p$ is the size of an optimal solution. Our paper is the first that shows
fixed parameter tractability of a $k$-node-connectivity augmentation problem
with high values of $k$. We will also consider the $(2,k)$-Connectivity
Augmentation problem where $G_0$ is $(k-1)$-edge-connected and $G_0 \cup J$
should be both $k$-edge-connected and $2$-connected. We will show that this
problem can be solved in time $9^p \cdot n^{O(1)}$, and for unit costs
approximated within $1.892$.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Nutov_Z/0/1/0/all/0/1">Zeev Nutov</a></p><p>A graph $G$ is $k$-out-connected from its node $s$ if it contains $k$
internally disjoint $sv$-paths to every node $v$; $G$ is $k$-connected if it is
$k$-out-connected from every node. In connectivity augmentation problems the
goal is to augment a graph $G_0=(V,E_0)$ by a minimum costs edge set $J$ such
that $G_0 \cup J$ has higher connectivity than $G_0$. In the
$k$-Out-Connectivity Augmentation ($k$-OCA) problem, $G_0$ is
$(k-1)$-out-connected from $s$ and $G_0 \cup J$ should be $k$-out-connected
from $s$; in the $k$-Connectivity Augmentation ($k$-CA) problem $G_0$ is
$(k-1)$-connected and $G_0 \cup J$ should be $k$-connected. The parameterized
complexity status of these problems was open even for $k=3$ and unit costs. We
will show that $k$-OCA and $3$-CA can be solved in time $9^p \cdot n^{O(1)}$,
where $p$ is the size of an optimal solution. Our paper is the first that shows
fixed parameter tractability of a $k$-node-connectivity augmentation problem
with high values of $k$. We will also consider the $(2,k)$-Connectivity
Augmentation problem where $G_0$ is $(k-1)$-edge-connected and $G_0 \cup J$
should be both $k$-edge-connected and $2$-connected. We will show that this
problem can be solved in time $9^p \cdot n^{O(1)}$, and for unit costs
approximated within $1.892$.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-15T00:30:00Z">Thursday, September 15 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    
    <h2 class='new-date'>
      <i class='icon-calendar-empty'></i> Wednesday, September 14
    </h2>
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://emanueleviola.wordpress.com/2022/09/14/myth-creation-the-switching-lemma/'>Myth creation: The switching lemma</a></h3>
          <p class='item-feed'>from <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          The history of science is littered with anecdotes about misplaced credit. Because it does not matter if it was A or B who did it; it only matters if it was I or not I. In this spirit I am starting a series of posts about such misplaced credit, which I hesitated before calling more [&#8230;]
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p><!--?xml version="1.0" encoding="iso-8859-1" ?--> <!--http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd--> <!-- html,xhtml,-css,NoFonts --></p>
<p style="text-align:justify;">The history of science is littered with anecdotes about misplaced credit. Because it does not matter if it was A or B who did it; it only matters if it was I or not I. In this spirit I am starting a series of posts about such misplaced credit, which I hesitated before calling more colorfully &#8220;myth creation.&#8221; Before starting, I want to make absolutely clear that I am in no way criticizing the works themselves or their authors. In fact, many are among my favorites. Moreover, at least in the examples I have in mind right now, the authors do place their work in the appropriate context with the use of citations etc. My only point is the credit that the work has received within and without our community (typically due to inertia and snowball effects rather than anything else).</p>
<p style="text-align:justify;">Of course, at some level this doesnât matter. You can call Chebichevâs polynomials rainbow sprinkles and the math doesnât change. And yet at some other level maybe it does matter a little, for science isnât yet a purely robotic activity. With these posts I will advertise unpopular points of views that might be useful, for example to researchers who are junior or from different communities.</p>
<p style="text-align:justify;">
<h3 class="sectionHead">The switching lemma</h3>
<p style="text-align:justify;">
<div class="quote">
<p style="text-align:justify;"><em>I must admit I had a good run </em>&#8212; Johan Hastad (privately to the blogger)</p>
</div>
<p style="text-align:justify;">Random restrictions have been used in complexity theory since at least the 60âs <span class="cite">[<a href="#XSubbotovskaya61">Sub61</a>]</span>. The first dramatic use in the context of AC0 is due to <span class="cite">[<a href="#XFSS84">FSS84</a>,Â <a href="#XAjt83">Ajt83</a>]</span>. These works proved a <em>switching lemma</em> the amazing fact that a DNF gets simplified by a random restriction to the point that it can be written as a CNF, so you can collapse layers and induct. (An exposition is given below.) Using it, they proved super-polynomial lower bounds for AC0. The proof in <span class="cite">[<a href="#XFSS84">FSS84</a>]</span> is very nice, and if I want to get a quick intuition of why switching is at all possible, I often go back to it. <span class="cite">[<a href="#XAjt83">Ajt83</a>]</span> is also a brilliant paper, and long, unavailable online for free, filled with a logical notation which makes some people twitch. The first symbol of the title says it all, and may be the most obscene ever chosen:</p>
<div style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5CSigma+_%7B1%7D%5E%7B1%7D.+%5Cend%7Baligned%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5CSigma+_%7B1%7D%5E%7B1%7D.+%5Cend%7Baligned%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5CSigma+_%7B1%7D%5E%7B1%7D.+%5Cend%7Baligned%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;begin{aligned} &#92;Sigma _{1}^{1}. &#92;end{aligned}" class="latex" /></div>
<p>Subsequently, <span class="cite">[<a href="#XYao85">Yao85</a>]</span> proved exponential lower bounds of the form <img src="https://s0.wp.com/latex.php?latex=2%5E%7Bn%5E%7Bc%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=2%5E%7Bn%5E%7Bc%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=2%5E%7Bn%5E%7Bc%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="2^{n^{c}}" class="latex" />, with a refined analysis of the switching lemma. The bounds are tight, except for the constant <img src="https://s0.wp.com/latex.php?latex=c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=c&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="c" class="latex" /> which depends on the depth of the circuit. Finally, the star of this post <span class="cite">[<a href="conf/stoc/Hastad86">Has86</a>,Â <a href="#XHas87">Has87</a>]</span> obtained <img src="https://s0.wp.com/latex.php?latex=c%3D1%2F%28depth-1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=c%3D1%2F%28depth-1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=c%3D1%2F%28depth-1%29&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="c=1/(depth-1)" class="latex" />.</p>
<p style="text-align:justify;">Yaoâs paper doesnât quite state that a DNF can be written exactly as a CNF, but it states that it can be approximated. Hastadâs work is the first to prove that a DNF can be written as a CNF, and in this sense his statement is cleaner than Yaoâs. However, Yaoâs paper states explicitly that a small circuit, after being hit by a restriction, can be set to constant by fixing few more bits.</p>
<p style="text-align:justify;">The modern formulation of the switching lemma says that a DNF can be written as a <em>shallow decision tree</em> (and hence a small CNF). This formulation in terms of decision trees is actually not explicit in Hastadâs work. Beame, in his primer <span class="cite">[<a href="#XBea94">Bea94</a>]</span>, credits Cai with this idea and mentions several researchers noted Hastadâs proof works in this way.</p>
<p style="text-align:justify;">Another switching lemma trivia is that the proof in Hastadâs thesis is actually due to Boppana; Hastadâs original argument &#8212; of which apparently no written record exists &#8212; was closer to Razborovâs later proof.</p>
<p style="text-align:justify;">So, letâs recap. Random restrictions are already in <span class="cite">[<a href="#XSubbotovskaya61">Sub61</a>]</span>. The idea of switching is already in <span class="cite">[<a href="#XFSS84">FSS84</a>,Â <a href="#XAjt83">Ajt83</a>]</span>. You already had three analyses of these ideas, two giving superpolynomial lower bounds and one <span class="cite">[<a href="#XYao85">Yao85</a>]</span> giving exponential. The formulation in terms of decision trees isnât in <span class="cite">[<a href="#XHas87">Has87</a>]</span>, and the proof that appears in <span class="cite">[<a href="#XHas87">Has87</a>]</span> is due to Boppana.</p>
<p style="text-align:justify;">Still, I would guess <span class="cite">[<a href="#XHas87">Has87</a>]</span> is more well known than all the other works above combined. <span class="cite">[<a href="#XYao85">Yao85</a>]</span> did have a following at the time &#8212; I think it appeared in the pop news. But hey &#8212; have you ever heard of Yaoâs switching lemma?</p>
<p style="text-align:justify;">The current citation counts offer mixed support for my thesis:</p>
<p style="text-align:justify;">FSS: 1351</p>
<p style="text-align:justify;">Y: 732</p>
<p style="text-align:justify;">H &#8211; paper &#8220;Almost optimal&#8230;:&#8221; 867</p>
<p style="text-align:justify;">H &#8211; thesis: 582</p>
<p style="text-align:justify;">But it is very hard to use citation information. The two H citations overlap, and papers are cited for various reasons. For example FSS got a ton of citations for the connection to oracles (which has nothing to do with switching lemmas).</p>
<p style="text-align:justify;">Instead itâs instructive to note the type of citations that you can find in the literature:</p>
<div class="quote">
<p style="text-align:justify;"><em>Hastadâs switching lemma is a cornerstone of circuit complexity</em> [No mention of FSS, A, Y]</p>
</div>
<p style="text-align:justify;">
<div class="quote">
<p style="text-align:justify;"><em>Hastadâs Switching Lemma is one of the gems of computational complexity</em> [Notes below in passing it builds on FSS, A, Y]</p>
</div>
<p style="text-align:justify;">The wikipedia entry is also telling:</p>
<table class="quotation" border="0" cellspacing="15" cellpadding="0">
<tbody>
<tr>
<td>
<div class="quotation">
<p style="text-align:justify;"><em>In computational complexity theory, Hastadâs switching lemma is a key tool for proving lower bounds on the size of constant-depth Boolean circuits. Using the switching lemma, Johan Hastad (1987) showed that.</em>.. [No mention of FSS,A,Y]</p>
</div>
</td>
</tr>
</tbody>
</table>
<p>I think that 99% of the contribution of this line of research is the <em>amazing idea</em> that random restrictions simplify a DNF so that you can write it as a CNF and collapse. 90% of the rest is analyzing this to get superpolynomial lower bounds. And 90% of whatever is left is analyzing this to get exponential lower bounds.</p>
<p style="text-align:justify;">Going back to something I mentioned at the beginning, I want to emphasize that Hastad during talks makes a point of reminding the audience that the idea of random restrictions is due to Sipser, and of Boppanaâs contribution. And I also would like to thank him for his help with this post.</p>
<p style="text-align:justify;">OK &#8212; so maybe this is so, but it must then be the case that <span class="cite">[<a href="#XHas87">Has87</a>]</span> is the final word on this stuff, like the ultimate tightest analysis that kills the problem. Actually, it is not tight in some regimes of interest, and several cool works of past and recent times address that. In the end, I can only think of one reason why <span class="cite">[<a href="#XHas87">Has87</a>]</span> entered the mythology in ways that other works did not, the reason that I carefully sidestepped while composing this post: Ã¥.</p>
<p style="text-align:justify;">Perhaps one reason behind the aura of the switching lemma is that itâs hard to find examples. It would be nice to read: If you have this extreme DNF hereâs what happens, on the other hand for this other extreme DNF hereâs what happens, and in general this always works and hereâs the switching lemma. <em>Examples are forever</em> â Erdos. Instead the switching lemma is typically presented as <em>blam</em>!: an example-free encoding argument which feels <em>deus ex machina</em>, as in this <a href="https://arxiv.org/abs/2202.05651">crisp presentation by Thapen</a>. For a little more discussion, I liked <a href="https://www.cse.cuhk.edu.hk/~andrejb/csci5170/notes/19L02.pdf">Bogdanovâs lecture notes</a>. Next I give a slightly different exposition of the encoding argument.</p>
<p style="text-align:justify;"><b>The simplest case: Or of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="n" class="latex" /> bits</b>.</p>
<p style="text-align:justify;">Here the circuit <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C" class="latex" /> is simply the Or of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="n" class="latex" /> bits <img src="https://s0.wp.com/latex.php?latex=x_%7B1%7D%2Cx_%7B2%7D%2C%5Cldots+%2Cx_%7Bn%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=x_%7B1%7D%2Cx_%7B2%7D%2C%5Cldots+%2Cx_%7Bn%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=x_%7B1%7D%2Cx_%7B2%7D%2C%5Cldots+%2Cx_%7Bn%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="x_{1},x_{2},&#92;ldots ,x_{n}" class="latex" />. This and the next case can be analyzed in more familiar ways, but the benefit of the encoding argument presented next is that it will extend to the general case more easily&#8230; arguably. Anyway, itâs also just fun to learn a different argument.</p>
<p style="text-align:justify;">So, letâs take a random restriction <img src="https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;rho " class="latex" /> with exactly <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="s" class="latex" /> stars. Some of the bits may become <img src="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="0" class="latex" />, others <img src="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1" class="latex" />, and others yet may remain unfixed, i.e., assigned to stars. Those that become <img src="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="0" class="latex" /> you can ignore, while if some become <img src="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1" class="latex" /> then the whole circuit becomes <img src="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1" class="latex" />.</p>
<p style="text-align:justify;">We will show that the number of restrictions for which the restricted circuit <img src="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C|_{&#92;rho }" class="latex" /> requires decision trees of depth <img src="https://s0.wp.com/latex.php?latex=%5Cge+d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cge+d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cge+d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;ge d" class="latex" /> is small. To accomplish this, we are going to encode/map such restrictions using/to a restriction&#8230; with no stars (that is, just a 0/1 assignment to the variables). The gain is clear: just think of a restriction with zero stars versus a restriction with one star. The latter are more by a factor about the number <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="n" class="latex" /> of variables.</p>
<p style="text-align:justify;">A critical observation is that we only want to encode restrictions for which <img src="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C|_{&#92;rho }" class="latex" /> requires large depth. So <img src="https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;rho " class="latex" /> does not map any variable to <img src="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1" class="latex" />, for else the Or is <img src="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1" class="latex" /> which has decision trees of depth <img src="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="0" class="latex" />.</p>
<p style="text-align:justify;">The way we are going to encode <img src="https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;rho " class="latex" /> is this: <em>Simply replace the stars with ones</em>. To go back, replace the ones with stars. We are using the ones in the encoding to âsignalâ where the stars are.</p>
<p style="text-align:justify;">Hence, the number of bad restrictions is at most <img src="https://s0.wp.com/latex.php?latex=2%5E%7Bn%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=2%5E%7Bn%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=2%5E%7Bn%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="2^{n}" class="latex" />, which is tiny compared to the number <img src="https://s0.wp.com/latex.php?latex=%5Cbinom+%7Bn%7D%7Bs%7D2%5E%7Bn-s%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cbinom+%7Bn%7D%7Bs%7D2%5E%7Bn-s%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cbinom+%7Bn%7D%7Bs%7D2%5E%7Bn-s%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;binom {n}{s}2^{n-s}" class="latex" /> of restrictions with <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="s" class="latex" /> stars.</p>
<p style="text-align:justify;"><b>The medium case: Or of functions on disjoint inputs</b>.</p>
<p style="text-align:justify;">Instead of working with DNFs, I will consider a circuit <img src="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C" class="latex" /> which is the Or of arbitrary functions <img src="https://s0.wp.com/latex.php?latex=f_%7Bi%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=f_%7Bi%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=f_%7Bi%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="f_{i}" class="latex" /> each on <img src="https://s0.wp.com/latex.php?latex=w&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=w&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=w&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="w" class="latex" /> bits. You can immediately get this formulation from the usual one for DNFs, but I still find it a little useful since otherwise you might think there is something special about DNFs. What <em>is </em>special is that you take the Or of the functions, and we will exploit this again shortly.</p>
<p style="text-align:justify;">In this warm-up case, we start with functions on <em>disjoint</em> inputs. So, again, letâs take a random restriction <img src="https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;rho " class="latex" /> with exactly <img src="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=s&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="s" class="latex" /> stars. Some of the functions may become <img src="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="0" class="latex" />, others <img src="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1" class="latex" />, and others yet may remain unfixed. Those that become <img src="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="0" class="latex" /> you can ignore, while if some become <img src="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1" class="latex" /> then the whole circuit becomes <img src="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1" class="latex" />.</p>
<p style="text-align:justify;">As before, we will show that the number of restrictions for which the restricted circuit <img src="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C|_{&#92;rho }" class="latex" /> requires decision trees of depth <img src="https://s0.wp.com/latex.php?latex=%5Cge+d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cge+d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cge+d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;ge d" class="latex" /> is small. To accomplish this, we are going to encode/map such restrictions using/to a restriction with just <img src="https://s0.wp.com/latex.php?latex=s-d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=s-d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=s-d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="s-d" class="latex" /> stars, plus a little more information. As we saw already, the gain in reducing the number of stars is clear. In particular, standard calculations show that saving <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="d" class="latex" /> stars reduces the number of restrictions by a factor <img src="https://s0.wp.com/latex.php?latex=O%28s%2Fn%29%5E%7Bd%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=O%28s%2Fn%29%5E%7Bd%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=O%28s%2Fn%29%5E%7Bd%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="O(s/n)^{d}" class="latex" />. The auxiliary information will give us a factor of <img src="https://s0.wp.com/latex.php?latex=w%5E%7Bd%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=w%5E%7Bd%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=w%5E%7Bd%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="w^{d}" class="latex" />, leading to the familiar bound <img src="https://s0.wp.com/latex.php?latex=O%28ws%2Fn%29%5E%7Bd%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=O%28ws%2Fn%29%5E%7Bd%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=O%28ws%2Fn%29%5E%7Bd%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="O(ws/n)^{d}" class="latex" />.</p>
<p style="text-align:justify;">As before, recall that we only want to encode restrictions for which <img src="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C|_{&#92;rho }" class="latex" /> requires large depth. So no function in <img src="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C|_{&#92;rho }" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1" class="latex" />, for else the circuit is <img src="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1" class="latex" /> and has decision trees of depth <img src="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="0" class="latex" />. Also, you have <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="d" class="latex" /> stars among inputs to functions that are unfixed (i.e., not even fixed to <img src="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="0" class="latex" />), for else again you can compute the function reading less than <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="d" class="latex" /> bits. Because the functions are unfixed, there is a setting for those <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="d" class="latex" /> stars (and possibly a few more stars â that would only help the argument) that make the corresponding functions <img src="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1" class="latex" />. We are going to pick precisely that setting in our restriction <img src="https://s0.wp.com/latex.php?latex=%5Crho+%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Crho+%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Crho+%27&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;rho &#039;" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=s-d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=s-d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=s-d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="s-d" class="latex" /> stars. This allows us to âsignalâ which functions had inputs with the stars we are saving (namely, those that are the constant <img src="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1" class="latex" />). To completely recover <img src="https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;rho " class="latex" />, we simply add extra information to indicate where the stars were. The saving here is that we only have to say where the stars are among <img src="https://s0.wp.com/latex.php?latex=w&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=w&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=w&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="w" class="latex" /> symbols, not <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="n" class="latex" />.</p>
<p style="text-align:justify;"><b>The general case: Or of functions on any subset of <img src="https://s0.wp.com/latex.php?latex=w&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=w&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=w&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="w" class="latex" /> bits</b>.</p>
<p style="text-align:justify;">First, the number of functions does not play a role, so you can think you have functions on any possible subset of <img src="https://s0.wp.com/latex.php?latex=w&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=w&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=w&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="w" class="latex" /> bits, where some functions may be constant. The idea is the same, except we have to be slightly more careful because when we set values for the stars in one function we may also affect other functions. The idea is simply to fix one function at the time. Specifically, starting with <img src="https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;rho " class="latex" />, consider the first function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="f" class="latex" /> thatâs not made constant by <img src="https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;rho " class="latex" />. So the inputs to <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="f" class="latex" /> have some stars. As before, let us replace the stars with constants that make the function <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="f" class="latex" /> equal to the constant 1, and append the extra information that allows us to recover where these stars were in <img src="https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;rho " class="latex" />.</p>
<p style="text-align:justify;">Weâd like to repeat the argument. Note however we only have guarantees about <img src="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C|_{&#92;rho }" class="latex" />, not <img src="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C|_{&#92;rho }" class="latex" /> with some stars replaced with constants that make <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="f" class="latex" /> equal to <img src="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1" class="latex" />. We also canât just jump to the 2nd function thatâs not constant in <img src="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C|_{&#92;rho }" class="latex" />, since the âsignalâ fixing for that might clash with the fixing for the first â this is where the overlap in inputs makes things slightly more involved. Instead, because <img src="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C|_{&#92;rho }" class="latex" /> required decision tree depth at least <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="d" class="latex" />, we note there have to be some assignments to the <img src="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="m" class="latex" /> stars in the input to <img src="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=f&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="f" class="latex" /> so that the resulting, further restricted circuit still requires decision tree depth <img src="https://s0.wp.com/latex.php?latex=%5Cge+d-m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cge+d-m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cge+d-m&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;ge d-m" class="latex" /> (else <img src="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C%7C_%7B%5Crho+%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="C|_{&#92;rho }" class="latex" /> has decision trees of depth <img src="https://s0.wp.com/latex.php?latex=%3Cd&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%3Cd&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%3Cd&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&lt;d" class="latex" />).Â  We append this assignment to the auxiliary information and we continue the argument using the further restricted circuit.</p>
<p style="text-align:justify;">
<h3 class="likesectionHead"><a id="x1-30001"></a>References</h3>
<p style="text-align:justify;">
<div class="thebibliography">
<p class="bibitem"><span class="biblabel"> [Ajt83] <span class="bibsp">Â Â Â </span></span><a id="XAjt83"></a>Miklï¿½s Ajtai. <img src="https://s0.wp.com/latex.php?latex=%5CSigma+%5Csp+%7B1%7D%5Csb+%7B1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5CSigma+%5Csp+%7B1%7D%5Csb+%7B1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5CSigma+%5Csp+%7B1%7D%5Csb+%7B1%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;Sigma &#92;sp {1}&#92;sb {1}" class="latex" />-formulae on finite structures. Annals of Pure and Applied Logic, 24(1):1â48, 1983.</p>
<p class="bibitem"><span class="biblabel"> [Bea94]<span class="bibsp">Â Â Â </span></span><a id="XBea94"></a>Paul Beame. A switching lemma primer. Technical Report UW-CSE-95-07-01, Department of Computer Science and Engineering, University of Washington, November 1994. Available from <a href="http://www.cs.washington.edu/homes/beame/" rel="nofollow">http://www.cs.washington.edu/homes/beame/</a>.</p>
<p class="bibitem"><span class="biblabel"> [FSS84]<span class="bibsp">Â Â Â </span></span><a id="XFSS84"></a>MerrickÂ L. Furst, JamesÂ B. Saxe, and Michael Sipser. Parity, circuits, and the polynomial-time hierarchy. Mathematical Systems Theory, 17(1):13â27, 1984.</p>
<p class="bibitem"><span class="biblabel"> [Has86]<span class="bibsp">Â Â Â </span></span><a id="XDBLP:conf/stoc/Hastad86"></a>Johan Hï¿½stad. Almost optimal lower bounds for small depth circuits. In Juris Hartmanis, editor, Proceedings of the 18th Annual ACM Symposium on Theory of Computing, May 28-30, 1986, Berkeley, California, USA, pages 6â20. ACM, 1986.</p>
<p class="bibitem"><span class="biblabel"> [Hï¿½s87]<span class="bibsp">Â Â Â </span></span><a id="XHas87"></a>Johan Hï¿½stad. Computational limitations of small-depth circuits. MIT Press, 1987.</p>
<p class="bibitem"><span class="biblabel"> [Sub61]<span class="bibsp">Â Â Â </span></span><a id="XSubbotovskaya61"></a>B.Â A. Subbotovskaya. Realizations of linear functions by formulas using +, *, -. Soviet Mathematics-Doklady, 2:110â112, 1961.</p>
<p class="bibitem"><span class="biblabel"> [Yao85]<span class="bibsp">Â Â Â </span></span><a id="XYao85"></a>Andrew Yao. Separating the polynomial-time hierarchy by oracles. In 26th IEEE Symp.Â on Foundations of Computer Science (FOCS), pages 1â10, 1985.</p>
</div>
<p class="authors">By Manu</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-14T17:11:22Z">Wednesday, September 14 2022, 17:11</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://scottaaronson.blog/?p=6718'>I had a dream</a></h3>
          <p class='item-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          As I slept fitfully, still recovering from COVID, I had one of the more interesting dreams of my life: I was desperately trying to finish some PowerPoint slides in time to give a talk. Uncharacteristically for me, one of the slides displayed actual code. This was a dream, so nothing was as clear as I&#8217;d [&#8230;]
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p>As I slept fitfully, still recovering from COVID, I had one of the more interesting dreams of my life:</p>



<p>I was desperately trying to finish some PowerPoint slides in time to give a talk. Uncharacteristically for me, one of the slides displayed actual code. This was a dream, so nothing was as clear as I&#8217;d like, but the code did <em>something</em> vaguely reminiscent of <a href="https://scottaaronson.blog/?p=710">Rosserâs Theorem</a>âe.g., enumerating all proofs in ZFC until it finds the lexicographically first proof or disproof of a certain statement, then branching into cases depending on whether it&#8217;s a proof or a disproof. In any case, it was simple enough to fit on one slide.</p>



<p>Suddenly, though, my whole presentation was deleted. Everything was ruined!</p>



<p>One of the developers of PowerPoint happened to be right there in the lecture hall (of course!), so I confronted him with my laptop and angrily demanded an explanation. He said that I must have triggered the section of Microsoft Office that tries to detect and prevent any discussion of logical paradoxes that are too dangerous for humankindâthe ones that would cause people to realize that our entire universe is just an illusion, a sandbox being run inside an AI, a glitch-prone Matrix. He said it patronizingly, as if it should&#8217;ve been obvious: &#8220;you and I both know that the Paradoxes are not to be talked about, so why would you be so <em>stupid</em> as to put one in your presentation?&#8221;</p>



<p>My reaction was to jab my finger in the guyâs face, shove him, scream, and curse him out. At that moment, I wasnât concerned in the slightest about the universe being an illusion, or about glitches in the Matrix. I was concerned about my embarrassment when Iâd be called in 10 minutes to give my talk and would have nothing to show.</p>



<p>My last thought, before I woke with a start, was to wonder whether Greg Kuperberg was right and I should give my presentations in <a href="https://en.wikipedia.org/wiki/Beamer_(LaTeX)">Beamer</a>, or some other open-source software, and then I wouldnât have had this problem.</p>



<p>A coda: I woke a bit after 7AM Central and started to write this down. But then&#8212;this is now real life (!)&#8212;I saw an email saying that a dozen people were waiting for me in a conference room in Europe for an important Zoom meeting. We&#8217;d gotten the time zones wrong; I&#8217;d thought that it wasn&#8217;t until 8AM my time. If not for this dream causing me to wake up, I would&#8217;ve missed the meeting entirely.</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-14T16:52:05Z">Wednesday, September 14 2022, 16:52</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='https://windowsontheory.org/2022/09/14/quick-reminders-masters-postdocs-faculty-etc/'>Quick reminders: masters, postdocs, faculty, etc.</a></h3>
          <p class='item-feed'>from <a href='https://windowsontheory.org'>Windows on Theory</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          As we&#8217;re getting closer to the season when undergraduate students are considering graduate school, and graduate students are considering the next steps such as postdoc or faculty positions, I wanted to remind people of two resources for such positions: the TCS jobs and crowd-sourced masters pages. The process and market for both graduate studies and &#8230; Continue reading Quick reminders: masters, postdocs, faculty,&#160;etc.
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p>As we&#8217;re getting closer to the season when undergraduate students are considering graduate school, and graduate students are considering the next steps such as postdoc or faculty positions,  I wanted to remind people of two resources for such positions: the <a href="https://cstheory-jobs.org/">TCS jobs</a> and <a href="https://www.cs.princeton.edu/~smattw/masters/masters.html">crowd-sourced masters</a> pages. </p>



<p>The process and market for both graduate studies and faculty positions (at least in the US) is fairly standard, with more or less a common timeline, and general ideas of where to look for positions (universities&#8217; websites are always a good start, as are the websites of <a href="https://jobs.acm.org/jobs/products/">ACM</a> and <a href="https://cra.org/ads/">CRA</a>).  Even so, it&#8217;s not always clear which areas a university is searching for at any given year, and also these resources are very US-centric, while many great places are located outside the US.</p>



<p>The <strong>postdoc market</strong> is much more &#8220;ad hoc&#8221;. Some places such as the <a href="https://simons.berkeley.edu/programs/participate">Simons institute</a> and the <a href="https://www.ias.edu/math/csdm/postdocs">IAS</a> search for postdocs yearly and have several positions. (Our own <a href="https://www.harvard.edu/kempner-institute/">Kempner Institute</a> will also be having regular searches after it launches this year.)  But in many other cases, postdoc positions are with an individual researcher that might have availability only every few years, which makes it harder for candidates to find out about this.  For such positions, the <a href="https://cstheory-jobs.org/"><strong>Theoretical Computer Science jobs</strong></a> page is a great way to both advertise any position you have to offer, as well as find out about opportunities.  Please post any postdoc or faculty positions relevant to TCS in your institution, as well as advertise it to your students as a place to look for jobs.</p>



<p>Finding information about <strong>research-oriented Masters programs</strong> is also sometimes challenging. In the US it&#8217;s common for students to apply straight to a Ph.D from undergraduate, and Masters programs are often intended more for professional development. But, as I <a href="https://windowsontheory.org/2018/02/20/research-masters/">wrote in the past,</a> <em>research-oriented</em> Masters programs can actually be a great fit for many students. A Ph.D is a huge commitment on both the student and advisor side. If you have not had a chance to do research during your undergraduate studies,  it may be better to start with a Masters before taking such a commitment. Some research Masters programs do not charge any tuition, and several offer a stipend. To post and look for such opportunities, see the <a href="https://www.cs.princeton.edu/~smattw/masters/masters.html"><strong>crowdsourced TCS research masters website</strong></a>, managed by Aviad Rubinstein and Matt Weinberg.</p>



<p>If there are other great resources or opportunities, please post them in the comments!</p>



<p>In particular, the resources above are geared for theoretical CS. If you have suggestions of analogous resources for other fields, please post them as well.</p>



<p> </p>
<p class="authors">By Boaz Barak</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-14T13:50:02Z">Wednesday, September 14 2022, 13:50</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.05839'>On bounded depth proofs for Tseitin formulas on the grid; revisited</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Johan H&#xe5;stad, Kilian Risse</p><p>We study Frege proofs using depth-$d$ Boolean formulas for the Tseitin
contradiction on $n \times n$ grids. We prove that if each line in the proof is
of size $M$ then the number of lines is exponential in $n/(\log M)^{O(d)}$.
This strengthens a recent result of Pitassi et al. [PRT22]. The key technical
step is a multi-switching lemma extending the switching lemma of H\r{a}stad
[H\r{a}s20] for a space of restrictions related to the Tseitin contradiction.
The strengthened lemma also allows us to improve the lower bound for standard
proof size of bounded depth Frege refutations from exponential in $\tilde
\Omega (n^{1/59d})$ to exponential in $\tilde \Omega (n^{1/(2d-1)})$.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+H%5Cr%7Ba%7Dstad_J/0/1/0/all/0/1">Johan H&#xe5;stad</a>, <a href="http://arxiv.org/find/cs/1/au:+Risse_K/0/1/0/all/0/1">Kilian Risse</a></p><p>We study Frege proofs using depth-$d$ Boolean formulas for the Tseitin
contradiction on $n \times n$ grids. We prove that if each line in the proof is
of size $M$ then the number of lines is exponential in $n/(\log M)^{O(d)}$.
This strengthens a recent result of Pitassi et al. [PRT22]. The key technical
step is a multi-switching lemma extending the switching lemma of H\r{a}stad
[H\r{a}s20] for a space of restrictions related to the Tseitin contradiction.
The strengthened lemma also allows us to improve the lower bound for standard
proof size of bounded depth Frege refutations from exponential in $\tilde
\Omega (n^{1/59d})$ to exponential in $\tilde \Omega (n^{1/(2d-1)})$.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-14T00:30:00Z">Wednesday, September 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.06142'>What is a combinatorial interpretation?</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Igor Pak</p><p>In this survey we discuss the notion of combinatorial interpretation in the
context of Algebraic Combinatorics and related areas. We approach the subject
from the Computational Complexity perspective. We review many examples, state a
workable definition, discuss many open problems, and present recent results on
the subject.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Pak_I/0/1/0/all/0/1">Igor Pak</a></p><p>In this survey we discuss the notion of combinatorial interpretation in the
context of Algebraic Combinatorics and related areas. We approach the subject
from the Computational Complexity perspective. We review many examples, state a
workable definition, discuss many open problems, and present recent results on
the subject.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-14T00:30:00Z">Wednesday, September 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.05531'>Topological Measures for Pattern quantification of Impact Centers in Piezo Vibration Striking Treatment (PVST)</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Melih C. Yesilli, Max M. Chumley, Jisheng Chen, Firas A. Khasawneh, Yang Guo</p><p>Surface texture influences wear and tribological properties of manufactured
parts, and it plays a critical role in end-user products. Therefore,
quantifying the order or structure of a manufactured surface provides important
information on the quality and life expectancy of the product. Although texture
can be intentionally introduced to enhance aesthetics or to satisfy a design
function, sometimes it is an inevitable byproduct of surface treatment
processes such as Piezo Vibration Striking Treatment (PVST). Measures of order
for surfaces have been characterized using statistical, spectral, and geometric
approaches. For nearly hexagonal lattices, topological tools have also been
used to measure the surface order. This paper utilizes tools from Topological
Data Analysis for quantifying the impact centers' pattern in PVST. We compute
measures of order based on optical digital microscope images of surfaces
treated using PVST. These measures are applied to the grid obtained from
estimating the centers of tool impacts, and they quantify the grid's deviations
from the nominal one. Our results show that TDA provides a convenient framework
for the characterization of pattern type that bypasses some limitations of
existing tools such as difficult manual processing of the data and the need for
an expert user to analyze and interpret the surface images.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Yesilli_M/0/1/0/all/0/1">Melih C. Yesilli</a>, <a href="http://arxiv.org/find/cs/1/au:+Chumley_M/0/1/0/all/0/1">Max M. Chumley</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jisheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Khasawneh_F/0/1/0/all/0/1">Firas A. Khasawneh</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yang Guo</a></p><p>Surface texture influences wear and tribological properties of manufactured
parts, and it plays a critical role in end-user products. Therefore,
quantifying the order or structure of a manufactured surface provides important
information on the quality and life expectancy of the product. Although texture
can be intentionally introduced to enhance aesthetics or to satisfy a design
function, sometimes it is an inevitable byproduct of surface treatment
processes such as Piezo Vibration Striking Treatment (PVST). Measures of order
for surfaces have been characterized using statistical, spectral, and geometric
approaches. For nearly hexagonal lattices, topological tools have also been
used to measure the surface order. This paper utilizes tools from Topological
Data Analysis for quantifying the impact centers' pattern in PVST. We compute
measures of order based on optical digital microscope images of surfaces
treated using PVST. These measures are applied to the grid obtained from
estimating the centers of tool impacts, and they quantify the grid's deviations
from the nominal one. Our results show that TDA provides a convenient framework
for the characterization of pattern type that bypasses some limitations of
existing tools such as difficult manual processing of the data and the need for
an expert user to analyze and interpret the surface images.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-14T00:30:00Z">Wednesday, September 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.06020'>Rectilinear Convex Hull of Points in 3D</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Pablo P&#xe9;rez-Lantero, Carlos Seara, Jorge Urrutia</p><p>Let $P$ be a set of $n$ points in $\mathbb{R}^3$ in general position, and let
$RCH(P)$ be the rectilinear convex hull of $P$. In this paper we obtain an
optimal $O(n\log n)$-time and $O(n)$-space algorithm to compute $RCH(P)$. We
also obtain an efficient $O(n\log^2 n)$-time and $O(n\log n)$-space algorithm
to compute and maintain the set of vertices of the rectilinear convex hull of
$P$ as we rotate $\mathbb R^3$ around the $z$-axis. Finally we study some
properties of the rectilinear convex hulls of point sets in $\mathbb{R}^3$.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Perez_Lantero_P/0/1/0/all/0/1">Pablo P&#xe9;rez-Lantero</a>, <a href="http://arxiv.org/find/cs/1/au:+Seara_C/0/1/0/all/0/1">Carlos Seara</a>, <a href="http://arxiv.org/find/cs/1/au:+Urrutia_J/0/1/0/all/0/1">Jorge Urrutia</a></p><p>Let $P$ be a set of $n$ points in $\mathbb{R}^3$ in general position, and let
$RCH(P)$ be the rectilinear convex hull of $P$. In this paper we obtain an
optimal $O(n\log n)$-time and $O(n)$-space algorithm to compute $RCH(P)$. We
also obtain an efficient $O(n\log^2 n)$-time and $O(n\log n)$-space algorithm
to compute and maintain the set of vertices of the rectilinear convex hull of
$P$ as we rotate $\mathbb R^3$ around the $z$-axis. Finally we study some
properties of the rectilinear convex hulls of point sets in $\mathbb{R}^3$.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-14T00:30:00Z">Wednesday, September 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.05676'>Recovery from Non-Decomposable Distance Oracles</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Zhuangfei Hu, Xinda Li, David P. Woodruff, Hongyang Zhang, Shufan Zhang</p><p>A line of work has looked at the problem of recovering an input from distance
queries. In this setting, there is an unknown sequence $s \in \{0,1\}^{\leq
n}$, and one chooses a set of queries $y \in \{0,1\}^{\mathcal{O}(n)}$ and
receives $d(s,y)$ for a distance function $d$. The goal is to make as few
queries as possible to recover $s$. Although this problem is well-studied for
decomposable distances, i.e., distances of the form $d(s,y) = \sum_{i=1}^n
f(s_i, y_i)$ for some function $f$, which includes the important cases of
Hamming distance, $\ell_p$-norms, and $M$-estimators, to the best of our
knowledge this problem has not been studied for non-decomposable distances, for
which there are important special cases such as edit distance, dynamic time
warping (DTW), Frechet distance, earth mover's distance, and so on. We initiate
the study and develop a general framework for such distances. Interestingly,
for some distances such as DTW or Frechet, exact recovery of the sequence $s$
is provably impossible, and so we show by allowing the characters in $y$ to be
drawn from a slightly larger alphabet this then becomes possible. In a number
of cases we obtain optimal or near-optimal query complexity. We also study the
role of adaptivity for a number of different distance functions. One motivation
for understanding non-adaptivity is that the query sequence can be fixed and
the distances of the input to the queries provide a non-linear embedding of the
input, which can be used in downstream applications involving, e.g., neural
networks for natural language processing.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zhuangfei Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xinda Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1">David P. Woodruff</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shufan Zhang</a></p><p>A line of work has looked at the problem of recovering an input from distance
queries. In this setting, there is an unknown sequence $s \in \{0,1\}^{\leq
n}$, and one chooses a set of queries $y \in \{0,1\}^{\mathcal{O}(n)}$ and
receives $d(s,y)$ for a distance function $d$. The goal is to make as few
queries as possible to recover $s$. Although this problem is well-studied for
decomposable distances, i.e., distances of the form $d(s,y) = \sum_{i=1}^n
f(s_i, y_i)$ for some function $f$, which includes the important cases of
Hamming distance, $\ell_p$-norms, and $M$-estimators, to the best of our
knowledge this problem has not been studied for non-decomposable distances, for
which there are important special cases such as edit distance, dynamic time
warping (DTW), Frechet distance, earth mover's distance, and so on. We initiate
the study and develop a general framework for such distances. Interestingly,
for some distances such as DTW or Frechet, exact recovery of the sequence $s$
is provably impossible, and so we show by allowing the characters in $y$ to be
drawn from a slightly larger alphabet this then becomes possible. In a number
of cases we obtain optimal or near-optimal query complexity. We also study the
role of adaptivity for a number of different distance functions. One motivation
for understanding non-adaptivity is that the query sequence can be fixed and
the distances of the input to the queries provide a non-linear embedding of the
input, which can be used in downstream applications involving, e.g., neural
networks for natural language processing.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-14T00:30:00Z">Wednesday, September 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.05520'>Unsplittable Euclidean Capacitated Vehicle Routing: A $(2+\epsilon)$-Approximation Algorithm</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Fabrizio Grandoni, Claire Mathieu, Hang Zhou</p><p>In the unsplittable capacitated vehicle routing problem, we are given a
metric space with a vertex called depot and a set of vertices called terminals.
Each terminal is associated with a positive demand between 0 and 1. The goal is
to find a minimum length collection of tours starting and ending at the depot
such that the demand of each terminal is covered by a single tour (i.e., the
demand cannot be split), and the total demand of the terminals in each tour
does not exceed the capacity of 1.
</p>
<p>Our main result is a polynomial-time $(2+\epsilon)$-approximation algorithm
for this problem in the two-dimensional Euclidean plane, i.e., for the special
case where the terminals and the depot are associated with points in the
Euclidean plane and their distances are defined accordingly. This improves on
recent work by Blauth, Traub, and Vygen [IPCO'21] and Friggstad, Mousavi,
Rahgoshay, and Salavatipour [IPCO'22].
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Grandoni_F/0/1/0/all/0/1">Fabrizio Grandoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Mathieu_C/0/1/0/all/0/1">Claire Mathieu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hang Zhou</a></p><p>In the unsplittable capacitated vehicle routing problem, we are given a
metric space with a vertex called depot and a set of vertices called terminals.
Each terminal is associated with a positive demand between 0 and 1. The goal is
to find a minimum length collection of tours starting and ending at the depot
such that the demand of each terminal is covered by a single tour (i.e., the
demand cannot be split), and the total demand of the terminals in each tour
does not exceed the capacity of 1.
</p>
<p>Our main result is a polynomial-time $(2+\epsilon)$-approximation algorithm
for this problem in the two-dimensional Euclidean plane, i.e., for the special
case where the terminals and the depot are associated with points in the
Euclidean plane and their distances are defined accordingly. This improves on
recent work by Blauth, Traub, and Vygen [IPCO'21] and Friggstad, Mousavi,
Rahgoshay, and Salavatipour [IPCO'22].
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-14T00:30:00Z">Wednesday, September 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.05558'>A Note on the Quickest Minimum Cost Transshipment Problem</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Martin Skutella</p><p>Klinz and Woeginger (1995) prove that the minimum cost quickest flow problem
is NP-hard. On the other hand, the quickest minimum cost flow problem can be
solved efficiently via a straightforward reduction to the quickest flow problem
without costs. More generally, we show how the quickest minimum cost
transshipment problem can be reduced to the efficiently solvable quickest
transshipment problem, thus adding another mosaic tile to the rich complexity
landscape of flows over time.
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Skutella_M/0/1/0/all/0/1">Martin Skutella</a></p><p>Klinz and Woeginger (1995) prove that the minimum cost quickest flow problem
is NP-hard. On the other hand, the quickest minimum cost flow problem can be
solved efficiently via a straightforward reduction to the quickest flow problem
without costs. More generally, we show how the quickest minimum cost
transshipment problem can be reduced to the efficiently solvable quickest
transshipment problem, thus adding another mosaic tile to the rich complexity
landscape of flows over time.
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-14T00:30:00Z">Wednesday, September 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.05614'>An Improved Lower Bound for Matroid Intersection Prophet Inequalities</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Raghuvansh R. Saxena, Santhoshini Velusamy, S. Matthew Weinberg</p><p>We consider prophet inequalities subject to feasibility constraints that are
the intersection of $q$ matroids. The best-known algorithms achieve a
$\Theta(q)$-approximation, even when restricted to instances that are the
intersection of $q$ partition matroids, and with i.i.d.~Bernoulli random
variables. The previous best-known lower bound is $\Theta(\sqrt{q})$ due to a
simple construction of [Kleinberg-Weinberg STOC 2012] (which uses
i.i.d.~Bernoulli random variables, and writes the construction as the
intersection of partition matroids).
</p>
<p>We establish an improved lower bound of $q^{1/2+\Omega(1/\log \log q)}$ by
writing the construction of [Kleinberg-Weinberg STOC 2012] as the intersection
of asymptotically fewer partition matroids. We accomplish this via an improved
upper bound on the product dimension of a graph with $p^p$ disjoint cliques of
size $p$, using recent techniques developed in [Alon-Alweiss European Journal
of Combinatorics 2020].
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Saxena_R/0/1/0/all/0/1">Raghuvansh R. Saxena</a>, <a href="http://arxiv.org/find/cs/1/au:+Velusamy_S/0/1/0/all/0/1">Santhoshini Velusamy</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinberg_S/0/1/0/all/0/1">S. Matthew Weinberg</a></p><p>We consider prophet inequalities subject to feasibility constraints that are
the intersection of $q$ matroids. The best-known algorithms achieve a
$\Theta(q)$-approximation, even when restricted to instances that are the
intersection of $q$ partition matroids, and with i.i.d.~Bernoulli random
variables. The previous best-known lower bound is $\Theta(\sqrt{q})$ due to a
simple construction of [Kleinberg-Weinberg STOC 2012] (which uses
i.i.d.~Bernoulli random variables, and writes the construction as the
intersection of partition matroids).
</p>
<p>We establish an improved lower bound of $q^{1/2+\Omega(1/\log \log q)}$ by
writing the construction of [Kleinberg-Weinberg STOC 2012] as the intersection
of asymptotically fewer partition matroids. We accomplish this via an improved
upper bound on the product dimension of a graph with $p^p$ disjoint cliques of
size $p$, using recent techniques developed in [Alon-Alweiss European Journal
of Combinatorics 2020].
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-14T00:30:00Z">Wednesday, September 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
    

    <article class='item'>
      <div class='item-header'>
        <span class='item-caret'>
          <i class='icon-large icon-caret-down item-collapse item-close item-opened'></i>
          <i class='icon-large icon-caret-right item-expand item-open item-closed'></i>
        </span>
        <span class='item-info'>
          <h3 class='item-title'><a href='http://arxiv.org/abs/2209.05623'>Space Optimal Vertex Cover in Dynamic Streams</a></h3>
          <p class='item-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        </span>
      </div>

      <div class='item-body'>
        <div class='item-snippet'>
        
          <p>Authors: Kheeran K. Naidu, Vihan Shah</p><p>We optimally resolve the space complexity for the problem of finding an
$\alpha$-approximate minimum vertex cover ($\alpha$MVC) in dynamic graph
streams. We give a randomised algorithm for $\alpha$MVC which uses
$O(n^2/\alpha^2)$ bits of space matching Dark and Konrad's lower bound [CCC
2020] up to constant factors. By computing a random greedy matching, we
identify `easy' instances of the problem which can trivially be solved by
returning the entire vertex set. The remaining `hard' instances, then have
sparse induced subgraphs which we exploit to get our space savings and solve
$\alpha$MVC.
</p>
<p>Achieving this type of optimality result is crucial for providing a complete
understanding of a problem, and it has been gaining interest within the dynamic
graph streaming community. For connectivity, Nelson and Yu [SODA 2019] improved
the lower bound showing that $\Omega(n \log^3 n)$ bits of space is necessary
while Ahn, Guha, and McGregor [SODA 2012] have shown that $O(n \log^3 n)$ bits
is sufficient. For finding an $\alpha$-approximate maximum matching, the upper
bound was improved by Assadi and Shah [ITCS 2022] showing that
$O(n^2/\alpha^3)$ bits is sufficient while Dark and Konrad [CCC 2020] have
shown that $\Omega(n^2/\alpha^3)$ bits is necessary. The space complexity,
however, remains unresolved for many other dynamic graph streaming problems
where further improvements can still be made. \end{abstract}
</p>
        
        </div>

        <div class='item-content item-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Naidu_K/0/1/0/all/0/1">Kheeran K. Naidu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1">Vihan Shah</a></p><p>We optimally resolve the space complexity for the problem of finding an
$\alpha$-approximate minimum vertex cover ($\alpha$MVC) in dynamic graph
streams. We give a randomised algorithm for $\alpha$MVC which uses
$O(n^2/\alpha^2)$ bits of space matching Dark and Konrad's lower bound [CCC
2020] up to constant factors. By computing a random greedy matching, we
identify `easy' instances of the problem which can trivially be solved by
returning the entire vertex set. The remaining `hard' instances, then have
sparse induced subgraphs which we exploit to get our space savings and solve
$\alpha$MVC.
</p>
<p>Achieving this type of optimality result is crucial for providing a complete
understanding of a problem, and it has been gaining interest within the dynamic
graph streaming community. For connectivity, Nelson and Yu [SODA 2019] improved
the lower bound showing that $\Omega(n \log^3 n)$ bits of space is necessary
while Ahn, Guha, and McGregor [SODA 2012] have shown that $O(n \log^3 n)$ bits
is sufficient. For finding an $\alpha$-approximate maximum matching, the upper
bound was improved by Assadi and Shah [ITCS 2022] showing that
$O(n^2/\alpha^3)$ bits is sufficient while Dark and Konrad [CCC 2020] have
shown that $\Omega(n^2/\alpha^3)$ bits is necessary. The space complexity,
however, remains unresolved for many other dynamic graph streaming problems
where further improvements can still be made. \end{abstract}
</p>
        
        </div>

        <div class='item-footer'>
          <time class='timeago' datetime="2022-09-14T00:30:00Z">Wednesday, September 14 2022, 00:30</time>
        </div>
      </div>
    </article>
  
  </div>

  <script src='js/jquery-2.0.3.min.js'></script>
  <script src="js/jquery.timeago.js" type="text/javascript"></script>
  <script>
    jQuery(document).ready(function() {
      jQuery("time.timeago").timeago();
    });
  </script>
  <script src='js/blank.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
