<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.6 (2023-03-30) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2023-06-27T01:30:17Z">Tuesday, June 27 2023, 01:30</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://minorfree.github.io/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://minorfree.github.io'>Hung Le</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>Sébastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Tuesday, June 27
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://differentialprivacy.org/open-problem-better-privacy-guarantees-for-larger-groups/'>Open problem - Better privacy guarantees for larger groups</a></h3>
        <p class='tr-article-feed'>from <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Consider a simple query counting the number of people in various mutually exclusive groups.
In the differential privacy literature, it is typical to assume that each of these groups should be subject to the same privacy loss: the noise added to each count has the same magnitude, and everyone gets the same privacy guarantees.
However, in settings where these groups have vastly different population sizes, larger populations may be willing to accept more error in exchange for stronger privacy protections.
In particular, in many use cases, relative error (the noisy count is within 5% of the true value) matters more than absolute error (the noisy count is at a distance of at most 100 of the true value).
This leads to a natural question: can we use this fact to develop a mechanism that improves the privacy guarantees of individuals in larger groups, subject to a constraint on relative error?</p>

Problem definition

<p>Our goal is to obtain a mechanism which minimizes the overall privacy loss for each group without exceeding a relative error threshold for each group.
To formalize this goal, we first define a notion of per-group privacy we call group-wise zero-concentrated differential privacy as follows.</p>

<p>Definition. Group-wise zero-concentrated differential privacy.
Assume possible datasets consist of records from domain \(U\), and \(U\) can be partitioned into \(k\) fixed, disjoint groups \(U_1\), …, \(U_k\). Let \(v : \mathcal{D} \rightarrow \mathbb{R}^k\) be a function associating a dataset to a vector of privacy budgets (one per group). We say a mechanism \(\mathcal{M}\) satisfies \(v\)-group-wise zero-concentrated differential privacy (zCDP) if for any two datasets \(D\), \(D’\) differing in the addition or removal of a record in \(U_i\), and for all \(\alpha&gt;1\), we have:
\[
D_\alpha\left(\mathcal{M}(D||\mathcal{M}(D’)\right) \le \alpha \cdot {v(D)}_i
\]
\[
D_\alpha\left(\mathcal{M}(D’)||\mathcal{M}(D)\right) \le \alpha \cdot {v(D)}_i
\]
where \(D_\alpha\) is the Rényi divergence of order \(\alpha\).</p>

<p>This definition is similar to tailored DP, defined in [LP15]: each individual gets a different privacy guarantee, depending on which group they belong to;
this guarantee also depends on how many people are in this group.
We use zCDP as our definition of privacy due to its compatibility with the Gaussian mechanism; the same idea could easily be applied to other definitions like with Rényi DP or pure DP.</p>

<p>From there we can give a more formal definition of the problem as follows. The goal is to minimize the privacy loss for each individual group, while keeping the error under a given threshold.
For larger groups that can accept more noise, this means adding more noise to achieve the smallest possible privacy loss.</p>

<p>Problem.
Let \(r \in (0,1]\) be an acceptable level of relative error, and \(k\) be the number of distinct, mutually-exclusive partitions of domain \(X\).
Given a dataset \(D\), let \(x(D)\) be a vector containing the count of records in each partition.
The objective is to find a mechanism \(\mathcal{M}\) which takes in \(r\), \(k\), and \(D\) and outputs \(\hat{x}(D)\) such that \(E\left[\left|{x(D)}_i-{\hat{x}(D)}_i\right|\right]&lt;r\cdot {x(D)}_i\) for all \(i\), and satisfies \(v\)-group-wise zCDP where \(v(D)_i\) is as small as possible for all \(i\).
<br>
To prevent pathological mechanisms that optimize for specific datasets, we add two constraints to the problem: the privacy guarantee \(v(D)_i\) should only depend on \(x(D)_i\), and should be nonincreasing with \(x(D)_i\).</p>

<p>Since the relative error thresholds are proportional to the population size, each population can tolerate a different amount of noise.
This means that to minimize the privacy loss for each group, the mechanism must add noise of different scales to each group.
Of course, directly using \(x(D)_i\) to determine the scale of the noise for group \(i\) leads to a privacy loss which is data dependent, similarly to e.g. PATE [PAEGT17], and as such should be treated as a protected value.</p>

An example mechanism

<p>An example mechanism that seems like it could address this problem is as follows.
First, perform the original counting query and add Gaussian noise to satisfy \(\rho\)-zCDP.
Then, add additional Gaussian noise to each count, with a variance that depends on the noisy count itself — adding more noise to larger groups.
This mechanism is outlined in Algorithm 1.</p>

<p>Algorithm 1.
Adding data-dependent noise as a post-processing step.
<br>
Require: A dataset \(D\) where each data point belongs to one of \(k\) groups, a privacy parameter \(\rho\), and a relative error rate \(r\).</p>
<ol>
  <li>Let \(\sigma^2 = 1/(2\rho)\)</li>
  <li>For \(i=1\) to \(k\) do</li>
  <li>\(\qquad\) Let \(x_i\) be the number of people in \(D\) in group \(i\)</li>
  <li>\(\qquad\) Sample \(X_i \sim \mathcal{N}(x_i, \sigma^2)\)</li>
  <li>\(\qquad\) Sample \(Y_i \sim \mathcal{N}_{k}(X_i, (rX_i)^2)\)</li>
  <li>end for</li>
  <li>return \(Y_1,\dots,Y_k\)</li>
</ol>

<p>Algorithm 1 achieves this goal of having approximately \(r\) error in each group: the total variance error of the mechanism is \(\sigma^2 + (rX)^2\), and \(X\) is a zCDP measure of \(f(D)\).
This mechanism satisfies at least \(\rho\)-zCDP: line 4 is an invocation of the Gaussian mechanism with privacy parameter \(\rho\), and line 5 is a post processing step and as such preserves the zCDP guarantee.
We would like to show that this algorithm also satisfies a stronger group-wise zCDP guarantee.</p>

<p>This makes intuitive sense: line 5 adds additional Gaussian noise without using the private data directly.
Since the noise scale in line 5 is proportional to the total count in line 4, we expect the privacy guarantee to be significantly stronger for large groups with more noise.
Further, we can verify experimentally that when the data magnitude is large compared to the noise, the output distribution for each group is close to a Gaussian distribution.</p>

<p>The below figure illustrates this finding.
We plot 1,000,000 sample outputs of Algorithm 1 (red) with parameters \(\sigma^2 = 100\) and \(r= 0.3\), and compare it to the best fit Gaussian distribution (black outline) with mean \(10,002.6\) and standard deviation of \(2995.1\).</p>

<p>♦</p>

<p>With parameters such as these, the output of the mechanism looks and behaves like a Gaussian distribution, which should be ideal to characterize the zCDP guarantee.
However, it is difficult to directly quantify this guarantee, due to the changing variance which is also a random variable.
Likewise, if the true count is close to zero or if the first instance of noise is large compared to the true count than the resulting distribution takes on a heavy skew and is no longer similar to a single Gaussian distribution.
Such distributions with randomized variances have not, to the best of our knowledge, been considered much in the literature, and we do not know whether the mechanism’s output distribution follows some well-studied distribution.</p>

<p>The randomized variance also makes it difficult to bound the Rényi divergence of the distribution and characterize the zCDP guarantees directly.
Current privacy amplification techniques are insufficient, as those techniques consider adding additional noise where the noise parameters are independent of the data itself.</p>

<p>Perhaps the most promising direction to understand more about such processes is the area of stochastic differential equations, where it is common to study noise with data-dependent variance.
The Bessel process [Øks03] is an example of such a process, where the noise is dependent on the current value.
This process captures the noise added as post-processing (Line 5), but not the initial noise-addition step (Line 4).
Furthermore, to the best of our knowledge, the Bessel process and other value-dependent stochastic differential equations do not have closed-form solutions.</p>

Goal

<p>We see two possible paths forward to address the original question. One path would be to obtain an analysis of Algorithm 1 which shows non-trivial improved privacy guarantees for larger groups.
We tried multiple approaches, but could not prove such a result.</p>

<p>An alternative path would be to develop a different algorithm, which achieves better privacy guarantees for larger groups while maintaining the error below the relative error threshold for all groups.</p><p>By </p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Consider a simple query counting the number of people in various mutually exclusive groups.
In the differential privacy literature, it is typical to assume that each of these groups should be subject to the same privacy loss: the noise added to each count has the same magnitude, and everyone gets the same privacy guarantees.
However, in settings where these groups have vastly different population sizes, larger populations may be willing to accept more error in exchange for stronger privacy protections.
In particular, in many use cases, <em>relative</em> error (the noisy count is within 5% of the true value) matters more than absolute error (the noisy count is at a distance of at most 100 of the true value).
This leads to a natural question: can we use this fact to develop a mechanism that improves the privacy guarantees of individuals in larger groups, subject to a constraint on relative error?</p>

<h3 id="problem-definition">Problem definition</h3>

<p>Our goal is to obtain a mechanism which minimizes the overall privacy loss for each group without exceeding a relative error threshold for each group.
To formalize this goal, we first define a notion of per-group privacy we call group-wise zero-concentrated differential privacy as follows.</p>

<p><strong>Definition.</strong> <em>Group-wise zero-concentrated differential privacy.</em>
Assume possible datasets consist of records from domain \(U\), and \(U\) can be partitioned into \(k\) fixed, disjoint groups \(U_1\), …, \(U_k\). Let \(v : \mathcal{D} \rightarrow \mathbb{R}^k\) be a function associating a dataset to a vector of privacy budgets (one per group). We say a mechanism \(\mathcal{M}\) satisfies \(v\)-group-wise zero-concentrated differential privacy (zCDP) if for any two datasets \(D\), \(D’\) differing in the addition or removal of a record in \(U_i\), and for all \(\alpha&gt;1\), we have:
\[
D_\alpha\left(\mathcal{M}(D||\mathcal{M}(D’)\right) \le \alpha \cdot {v(D)}_i
\]
\[
D_\alpha\left(\mathcal{M}(D’)||\mathcal{M}(D)\right) \le \alpha \cdot {v(D)}_i
\]
where \(D_\alpha\) is the Rényi divergence of order \(\alpha\).</p>

<p>This definition is similar to <em>tailored DP</em>, defined in [<a href="https://eprint.iacr.org/2014/982.pdf">LP15</a>]: each individual gets a different privacy guarantee, depending on which group they belong to;
this guarantee also depends on how many people are in this group.
We use zCDP as our definition of privacy due to its compatibility with the Gaussian mechanism; the same idea could easily be applied to other definitions like with Rényi DP or pure DP.</p>

<p>From there we can give a more formal definition of the problem as follows. The goal is to minimize the privacy loss for each individual group, while keeping the error under a given threshold.
For larger groups that can accept more noise, this means adding more noise to achieve the smallest possible privacy loss.</p>

<p><strong>Problem.</strong>
Let \(r \in (0,1]\) be an acceptable level of relative error, and \(k\) be the number of distinct, mutually-exclusive partitions of domain \(X\).
Given a dataset \(D\), let \(x(D)\) be a vector containing the count of records in each partition.
The objective is to find a mechanism \(\mathcal{M}\) which takes in \(r\), \(k\), and \(D\) and outputs \(\hat{x}(D)\) such that \(E\left[\left|{x(D)}_i-{\hat{x}(D)}_i\right|\right]&lt;r\cdot {x(D)}_i\) for all \(i\), and satisfies \(v\)-group-wise zCDP where \(v(D)_i\) is as small as possible for all \(i\).
<br />
To prevent pathological mechanisms that optimize for specific datasets, we add two constraints to the problem: the privacy guarantee \(v(D)_i\) should only depend on \(x(D)_i\), and should be nonincreasing with \(x(D)_i\).</p>

<p>Since the relative error thresholds are proportional to the population size, each population can tolerate a different amount of noise.
This means that to minimize the privacy loss for each group, the mechanism must add noise of different scales to each group.
Of course, directly using \(x(D)_i\) to determine the scale of the noise for group \(i\) leads to a privacy loss which is data dependent, similarly to e.g. PATE [<a href="https://openreview.net/forum?id=HkwoSDPg">PAEGT17</a>], and as such should be treated as a protected value.</p>

<h3 id="an-example-mechanism">An example mechanism</h3>

<p>An example mechanism that seems like it could address this problem is as follows.
First, perform the original counting query and add Gaussian noise to satisfy \(\rho\)-zCDP.
Then, add additional Gaussian noise to each count, with a variance that depends on the noisy count itself — adding more noise to larger groups.
This mechanism is outlined in Algorithm 1.</p>

<p><strong>Algorithm 1.</strong>
<em>Adding data-dependent noise as a post-processing step.</em>
<br />
Require: A dataset \(D\) where each data point belongs to one of \(k\) groups, a privacy parameter \(\rho\), and a relative error rate \(r\).</p>
<ol>
  <li>Let \(\sigma^2 = 1/(2\rho)\)</li>
  <li><strong>For</strong> \(i=1\) to \(k\) <strong>do</strong></li>
  <li>\(\qquad\) Let \(x_i\) be the number of people in \(D\) in group \(i\)</li>
  <li>\(\qquad\) Sample \(X_i \sim \mathcal{N}(x_i, \sigma^2)\)</li>
  <li>\(\qquad\) Sample \(Y_i \sim \mathcal{N}_{k}(X_i, (rX_i)^2)\)</li>
  <li><strong>end for</strong></li>
  <li><strong>return</strong> \(Y_1,\dots,Y_k\)</li>
</ol>

<p>Algorithm 1 achieves this goal of having approximately \(r\) error in each group: the total variance error of the mechanism is \(\sigma^2 + (rX)^2\), and \(X\) is a zCDP measure of \(f(D)\).
This mechanism satisfies at least \(\rho\)-zCDP: line 4 is an invocation of the Gaussian mechanism with privacy parameter \(\rho\), and line 5 is a post processing step and as such preserves the zCDP guarantee.
We would like to show that this algorithm also satisfies a stronger group-wise zCDP guarantee.</p>

<p>This makes intuitive sense: line 5 adds additional Gaussian noise without using the private data directly.
Since the noise scale in line 5 is proportional to the total count in line 4, we expect the privacy guarantee to be significantly stronger for large groups with more noise.
Further, we can verify experimentally that when the data magnitude is large compared to the noise, the output distribution for each group is close to a Gaussian distribution.</p>

<p>The below figure illustrates this finding.
We plot 1,000,000 sample outputs of Algorithm 1 (red) with parameters \(\sigma^2 = 100\) and \(r= 0.3\), and compare it to the best fit Gaussian distribution (black outline) with mean \(10,002.6\) and standard deviation of \(2995.1\).</p>

<p><img src="../images/two-stage-noise-gaussian.png" width="70%" alt="A comparison between sample outputs of Algorithm 1 and the best-fit Gaussian distribution, showing that both match very closely." style="margin:auto;display: block;" /></p>

<p>With parameters such as these, the output of the mechanism looks and behaves like a Gaussian distribution, which should be ideal to characterize the zCDP guarantee.
However, it is difficult to directly quantify this guarantee, due to the changing variance which is also a random variable.
Likewise, if the true count is close to zero or if the first instance of noise is large compared to the true count than the resulting distribution takes on a heavy skew and is no longer similar to a single Gaussian distribution.
Such distributions with randomized variances have not, to the best of our knowledge, been considered much in the literature, and we do not know whether the mechanism’s output distribution follows some well-studied distribution.</p>

<p>The randomized variance also makes it difficult to bound the Rényi divergence of the distribution and characterize the zCDP guarantees directly.
Current privacy amplification techniques are insufficient, as those techniques consider adding additional noise where the noise parameters are independent of the data itself.</p>

<p>Perhaps the most promising direction to understand more about such processes is the area of stochastic differential equations, where it is common to study noise with data-dependent variance.
The Bessel process [<a href="http://www.stat.ucla.edu/~ywu/research/documents/StochasticDifferentialEquations.pdf">Øks03</a>] is an example of such a process, where the noise is dependent on the current value.
This process captures the noise added as post-processing (Line 5), but not the initial noise-addition step (Line 4).
Furthermore, to the best of our knowledge, the Bessel process and other value-dependent stochastic differential equations do not have closed-form solutions.</p>

<h3 id="goal">Goal</h3>

<p>We see two possible paths forward to address the original question. One path would be to obtain an analysis of Algorithm 1 which shows non-trivial improved privacy guarantees for larger groups.
We tried multiple approaches, but could not prove such a result.</p>

<p>An alternative path would be to develop a different algorithm, which achieves better privacy guarantees for larger groups while maintaining the error below the relative error threshold for all groups.</p><p class="authors">By </p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-27T01:00:00Z">Tuesday, June 27 2023, 01:00</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Monday, June 26
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/'>AI Ends It All</a></h3>
        <p class='tr-article-feed'>from <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          I was getting a lift with a friend&#8212;Greg Skau&#8212;just the other day. No not in his boat, but in his car. Our conversation turned to the topic of: &#8220;is AI a threat to all of us?&#8221; Indeed. See this: The year is 2050. The location is London&#8212; but not as we know it. GodBot, a [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>
I was getting a lift with a friend&#8212;Greg Skau&#8212;just the other day. No not in his boat, but in his car. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/gs/" rel="attachment wp-att-21828"><img data-attachment-id="21828" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/gs/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?fit=219%2C320&amp;ssl=1" data-orig-size="219,320" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="gs" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?fit=205%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?fit=219%2C320&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?resize=219%2C320&#038;ssl=1" alt="" width="219" height="320" class="aligncenter size-full wp-image-21828" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?w=219&amp;ssl=1 219w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?resize=205%2C300&amp;ssl=1 205w" sizes="(max-width: 219px) 100vw, 219px" data-recalc-dims="1" /></a></p>
<p><P><br />
Our conversation turned to the topic of: &#8220;is AI a threat to all of us?&#8221; Indeed. See <a href="https://www.standard.co.uk/insider/ai-apocalypse-life-robots-take-over-elon-musk-chatgpt-b1078423.html">this</a>:</p>
<p><P></p>
<blockquote><p><b> </b> <em> The year is 2050. The location is London&#8212; but not as we know it. GodBot, a robot so intelligent it can out-smart any human, is in charge of the United Kingdom &#8212; the entire planet, in fact &#8212; and just announced its latest plan to reverse global temperature rises: an international zero-child, zero-reproduction policy, which will see all human females systematically destroyed and replaced with carbon-neutral sex robots. </em>
</p></blockquote>
<p><p>
That my friend Greg would raise questions about AI seemed pretty neat. It seemed natural yet quite cool that a friend&#8212;who was not an AI expert&#8212;would raise these issues. I am also not an AI expert&#8212;not even an advanced beginner. But it is on just about everyone&#8217;s top list of questions. We had a fun conversation, but failed to resolve the issue. Of course.</p>
<p>
<p><H2> AI Limits </H2></p>
<p><p>
<a href="https://openai.com">OpenAI</a> has just revealed that ChatGPT-4 has learned to lie, telling a human it was a blind person in order to get a task done. Somehow lies seem to set such AI systems apart from what we might have thought were the limits of AI.</p>
<p>
Another thought on AI is: Is physical law an <a href="https://getpocket.com/explore/item/is-physical-law-an-alien-intelligence?utm_source=pocket-newtab">Alien Intelligence?</a>:</p>
<blockquote><p><b> </b> <em> Arthur Clarke once pointed out that any sufficiently advanced technology is going to be indistinguishable from magic. If you dropped in on a bunch of Paleolithic farmers with your iPhone and a pair of sneakers, you&#8217;d undoubtedly seem pretty magical. But the contrast is only middling: The farmers would still recognize you as basically like them, and before long they&#8217;d be taking selfies. But what if life has moved so far on that it doesn&#8217;t just appear magical, but appears like physics? </em>
</p></blockquote>
<p><p>
This is related to Clarke&#8217;s <a href="https://en.wikipedia.org/wiki/Clarke&#37;27s_three_laws">three laws</a>:</p>
<ul>
<li>
When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong. </p>
<li>
The only way of discovering the limits of the possible is to venture a little way past them into the impossible. </p>
<li>
Any sufficiently advanced technology is indistinguishable from magic.
</ul>
<p>
Or take a look at his T-shirt (referring to <a href="https://web.mit.edu/m-i-t/science_fiction/jenkins/jenkins_4.html">this</a>): </p>
<p><P><br />
<a href="https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/ac/" rel="attachment wp-att-21829"><img data-attachment-id="21829" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/ac/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?fit=240%2C240&amp;ssl=1" data-orig-size="240,240" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ac" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?fit=240%2C240&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?fit=240%2C240&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?resize=240%2C240&#038;ssl=1" alt="" width="240" height="240" class="aligncenter size-full wp-image-21829" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?w=240&amp;ssl=1 240w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?resize=200%2C200&amp;ssl=1 200w" sizes="(max-width: 240px) 100vw, 240px" data-recalc-dims="1" /></a></p>
<p>
<p><H2> Open Problems </H2></p>
<p><p>
Alan Perlis&#8212;the first Turing award winner&#8212;is famous for his many <a href="http://www.cs.yale.edu/homes/perlis-alan/quotes.html">quotes</a>. One was: &#8220;A year spent in artificial intelligence is enough to make one believe in God.&#8221;</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/ai/" rel="attachment wp-att-21830"><img data-attachment-id="21830" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/ai/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ai.jpeg?fit=300%2C240&amp;ssl=1" data-orig-size="300,240" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ai" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ai.jpeg?fit=300%2C240&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ai.jpeg?fit=300%2C240&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ai.jpeg?resize=300%2C240&#038;ssl=1" alt="" width="300" height="240" class="aligncenter size-full wp-image-21830" data-recalc-dims="1" /></a></p>
<p>
I enjoyed working for Alan at my first job at Yale University, many years ago. See <a href="https://blog.computationalcomplexity.org/2021/06/i-went-to-debate-about-program-verif.html">Fortnow&#8217;s</a> blog for comments on our joint work with Perlis and Rich DeMillo&#8212;<a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/social.pdf">Social Processes and Proofs of Theorems and Programs</a>.</p>
<p>
Ken pipes in that the record seems to indicate that this late-1970s quote reflected frustration during the long &#8220;AI winter&#8221; when basic human capabilities stayed beyond reach. He also notes that the 2050 date for sex robots was <a href="https://en.wikipedia.org/wiki/Love_and_Sex_with_Robots">forecast</a> by the British chess master who was previously best known for winning a famous computer bet in 1978.</p>
<p>
<p class="authors">By rjlipton</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T17:56:41Z">Monday, June 26 2023, 17:56</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://decentralizedthoughts.github.io/2023-06-26-dls-meets-rollback/'>$3f+1$ is needed in Partial Synchrony even against a Rollback adversary</a></h3>
        <p class='tr-article-feed'>from <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We covered the classic DLS88 split brain impossibility result against a Byzantine adversary in a previous post: DLS88: (Theorem 4.4) It is impossible to solve Agreement under partial synchrony against a Byzantine adversary if $f \geq n/3$. In a follow up, we discussed how CJKR12 strengthen this result by observing...
        
        </div>

        <div class='tr-article-summary'>
        
          
          We covered the classic DLS88 split brain impossibility result against a Byzantine adversary in a previous post: DLS88: (Theorem 4.4) It is impossible to solve Agreement under partial synchrony against a Byzantine adversary if $f \geq n/3$. In a follow up, we discussed how CJKR12 strengthen this result by observing...
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T11:00:00Z">Monday, June 26 2023, 11:00</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13247'>Quantum Merlin-Arthur and proofs without relative phase</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Roozbeh Bassirian, Bill Fefferman, Kunal Marwaha</p><p>We study a variant of QMA where quantum proofs have no relative phase (i.e.
non-negative amplitudes, up to a global phase). If only completeness is
modified, this class is equal to QMA [arXiv:1410.2882]; but if both
completeness and soundness are modified, the class (named QMA+ by Jeronimo and
Wu) can be much more powerful. We show that QMA+ with some constant gap is
equal to NEXP, yet QMA+ with some *other* constant gap is equal to QMA. One
interpretation is that Merlin's ability to "deceive" originates from relative
phase at least as much as from entanglement, since QMA(2) $\subseteq$ NEXP.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Bassirian_R/0/1/0/all/0/1">Roozbeh Bassirian</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Fefferman_B/0/1/0/all/0/1">Bill Fefferman</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Marwaha_K/0/1/0/all/0/1">Kunal Marwaha</a></p><p>We study a variant of QMA where quantum proofs have no relative phase (i.e.
non-negative amplitudes, up to a global phase). If only completeness is
modified, this class is equal to QMA [<a href="/abs/1410.2882">arXiv:1410.2882</a>]; but if both
completeness and soundness are modified, the class (named QMA+ by Jeronimo and
Wu) can be much more powerful. We show that QMA+ with some constant gap is
equal to NEXP, yet QMA+ with some *other* constant gap is equal to QMA. One
interpretation is that Merlin's ability to "deceive" originates from relative
phase at least as much as from entanglement, since QMA(2) $\subseteq$ NEXP.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T00:30:00Z">Monday, June 26 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13319'>A SAT Solver and Computer Algebra Attack on the Minimum Kochen-Specker Problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Zhengyu Li, Curtis Bright, Vijay Ganesh</p><p>One of the foundational results in quantum mechanics is the Kochen-Specker
(KS) theorem, which states that any theory whose predictions agree with quantum
mechanics must be contextual, i.e., a quantum observation cannot be understood
as revealing a pre-existing value. The theorem hinges on the existence of a
mathematical object called a KS vector system. While many KS vector systems are
known to exist, the problem of finding the minimum KS vector system has
remained stubbornly open for over 55 years, despite significant attempts by
leading scientists and mathematicians. In this paper, we present a new method
based on a combination of a SAT solver and a computer algebra system (CAS) to
address this problem. Our approach improves the lower bound on the minimum
number of vectors in a KS system from 22 to 24, and is about 35,000 times more
efficient compared to the previous best computational methods. The increase in
efficiency derives from the fact we are able to exploit the powerful
combinatorial search-with-learning capabilities of a SAT solver together with
the isomorph-free exhaustive generation methods of a CAS. The quest for the
minimum KS vector system is motivated by myriad applications such as
simplifying experimental tests of contextuality, zero-error classical
communication, dimension witnessing, and the security of certain quantum
cryptographic protocols. To the best of our knowledge, this is the first
application of a novel SAT+CAS system to a problem in the realm of quantum
foundations.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Li_Z/0/1/0/all/0/1">Zhengyu Li</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Bright_C/0/1/0/all/0/1">Curtis Bright</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Ganesh_V/0/1/0/all/0/1">Vijay Ganesh</a></p><p>One of the foundational results in quantum mechanics is the Kochen-Specker
(KS) theorem, which states that any theory whose predictions agree with quantum
mechanics must be contextual, i.e., a quantum observation cannot be understood
as revealing a pre-existing value. The theorem hinges on the existence of a
mathematical object called a KS vector system. While many KS vector systems are
known to exist, the problem of finding the minimum KS vector system has
remained stubbornly open for over 55 years, despite significant attempts by
leading scientists and mathematicians. In this paper, we present a new method
based on a combination of a SAT solver and a computer algebra system (CAS) to
address this problem. Our approach improves the lower bound on the minimum
number of vectors in a KS system from 22 to 24, and is about 35,000 times more
efficient compared to the previous best computational methods. The increase in
efficiency derives from the fact we are able to exploit the powerful
combinatorial search-with-learning capabilities of a SAT solver together with
the isomorph-free exhaustive generation methods of a CAS. The quest for the
minimum KS vector system is motivated by myriad applications such as
simplifying experimental tests of contextuality, zero-error classical
communication, dimension witnessing, and the security of certain quantum
cryptographic protocols. To the best of our knowledge, this is the first
application of a novel SAT+CAS system to a problem in the realm of quantum
foundations.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T00:30:00Z">Monday, June 26 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13201'>Decomposition of Geometric Graphs into Star Forests</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: J&#xe1;nos Pach, Morteza Saghafian, Patrick Schnider</p><p>We solve a problem of Dujmovi\'c and Wood (2007) by showing that a complete
convex geometric graph on $n$ vertices cannot be decomposed into fewer than
$n-1$ star-forests, each consisting of noncrossing edges. This bound is clearly
tight. We also discuss similar questions for abstract graphs.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Pach_J/0/1/0/all/0/1">J&#xe1;nos Pach</a>, <a href="http://arxiv.org/find/math/1/au:+Saghafian_M/0/1/0/all/0/1">Morteza Saghafian</a>, <a href="http://arxiv.org/find/math/1/au:+Schnider_P/0/1/0/all/0/1">Patrick Schnider</a></p><p>We solve a problem of Dujmovi\'c and Wood (2007) by showing that a complete
convex geometric graph on $n$ vertices cannot be decomposed into fewer than
$n-1$ star-forests, each consisting of noncrossing edges. This bound is clearly
tight. We also discuss similar questions for abstract graphs.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T00:30:00Z">Monday, June 26 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13312'>Effective data reduction algorithm for topological data analysis</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Seonmi Choi, Jinseok Oh, Jeong Rye Park, Seung Yeop Yang, Hongdae Yun</p><p>One of the most interesting tools that have recently entered the data science
toolbox is topological data analysis (TDA). With the explosion of available
data sizes and dimensions, identifying and extracting the underlying structure
of a given dataset is a fundamental challenge in data science, and TDA provides
a methodology for analyzing the shape of a dataset using tools and prospects
from algebraic topology. However, the computational complexity makes it quickly
infeasible to process large datasets, especially those with high dimensions.
Here, we introduce a preprocessing strategy called the Characteristic Lattice
Algorithm (CLA), which allows users to reduce the size of a given dataset as
desired while maintaining geometric and topological features in order to make
the computation of TDA feasible or to shorten its computation time. In
addition, we derive a stability theorem and an upper bound of the barcode
errors for CLA based on the bottleneck distance.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1">Seonmi Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1">Jinseok Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jeong Rye Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Seung Yeop Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_H/0/1/0/all/0/1">Hongdae Yun</a></p><p>One of the most interesting tools that have recently entered the data science
toolbox is topological data analysis (TDA). With the explosion of available
data sizes and dimensions, identifying and extracting the underlying structure
of a given dataset is a fundamental challenge in data science, and TDA provides
a methodology for analyzing the shape of a dataset using tools and prospects
from algebraic topology. However, the computational complexity makes it quickly
infeasible to process large datasets, especially those with high dimensions.
Here, we introduce a preprocessing strategy called the Characteristic Lattice
Algorithm (CLA), which allows users to reduce the size of a given dataset as
desired while maintaining geometric and topological features in order to make
the computation of TDA feasible or to shorten its computation time. In
addition, we derive a stability theorem and an upper bound of the barcode
errors for CLA based on the bottleneck distance.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T00:30:00Z">Monday, June 26 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13119'>Adversarial Resilience in Sequential Prediction via Abstention</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Surbhi Goel, Steve Hanneke, Shay Moran, Abhishek Shetty</p><p>We study the problem of sequential prediction in the stochastic setting with
an adversary that is allowed to inject clean-label adversarial (or
out-of-distribution) examples. Algorithms designed to handle purely stochastic
data tend to fail in the presence of such adversarial examples, often leading
to erroneous predictions. This is undesirable in many high-stakes applications
such as medical recommendations, where abstaining from predictions on
adversarial examples is preferable to misclassification. On the other hand,
assuming fully adversarial data leads to very pessimistic bounds that are often
vacuous in practice.
</p>
<p>To capture this motivation, we propose a new model of sequential prediction
that sits between the purely stochastic and fully adversarial settings by
allowing the learner to abstain from making a prediction at no cost on
adversarial examples. Assuming access to the marginal distribution on the
non-adversarial examples, we design a learner whose error scales with the VC
dimension (mirroring the stochastic setting) of the hypothesis class, as
opposed to the Littlestone dimension which characterizes the fully adversarial
setting. Furthermore, we design a learner for VC dimension~1 classes, which
works even in the absence of access to the marginal distribution. Our key
technical contribution is a novel measure for quantifying uncertainty for
learning VC classes, which may be of independent interest.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1">Surbhi Goel</a>, <a href="http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1">Steve Hanneke</a>, <a href="http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1">Shay Moran</a>, <a href="http://arxiv.org/find/cs/1/au:+Shetty_A/0/1/0/all/0/1">Abhishek Shetty</a></p><p>We study the problem of sequential prediction in the stochastic setting with
an adversary that is allowed to inject clean-label adversarial (or
out-of-distribution) examples. Algorithms designed to handle purely stochastic
data tend to fail in the presence of such adversarial examples, often leading
to erroneous predictions. This is undesirable in many high-stakes applications
such as medical recommendations, where abstaining from predictions on
adversarial examples is preferable to misclassification. On the other hand,
assuming fully adversarial data leads to very pessimistic bounds that are often
vacuous in practice.
</p>
<p>To capture this motivation, we propose a new model of sequential prediction
that sits between the purely stochastic and fully adversarial settings by
allowing the learner to abstain from making a prediction at no cost on
adversarial examples. Assuming access to the marginal distribution on the
non-adversarial examples, we design a learner whose error scales with the VC
dimension (mirroring the stochastic setting) of the hypothesis class, as
opposed to the Littlestone dimension which characterizes the fully adversarial
setting. Furthermore, we design a learner for VC dimension~1 classes, which
works even in the absence of access to the marginal distribution. Our key
technical contribution is a novel measure for quantifying uncertainty for
learning VC classes, which may be of independent interest.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T00:30:00Z">Monday, June 26 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13158'>Breaking the cubic barrier in the Solovay-Kitaev algorithm</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Greg Kuperberg (UC Davis)</p><p>We improve the Solovay-Kitaev theorem and algorithm for a general finite,
inverse-closed generating set acting on a qudit. Prior versions of the
algorithm can efficiently find a word of length $O((\log
1/\epsilon)^{3+\delta})$ to approximate an arbitrary target gate to within
$\epsilon$. Using two new ideas, each of which reduces the exponent separately,
our new bound on the world length is $O((\log
1/\epsilon)^{1.44042\ldots+\delta})$. Our result holds more generally for any
finite set that densely generates any connected, semisimple real Lie group,
with an extra length term in the non-compact case to reach group elements far
away from the identity.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Kuperberg_G/0/1/0/all/0/1">Greg Kuperberg</a> (UC Davis)</p><p>We improve the Solovay-Kitaev theorem and algorithm for a general finite,
inverse-closed generating set acting on a qudit. Prior versions of the
algorithm can efficiently find a word of length $O((\log
1/\epsilon)^{3+\delta})$ to approximate an arbitrary target gate to within
$\epsilon$. Using two new ideas, each of which reduces the exponent separately,
our new bound on the world length is $O((\log
1/\epsilon)^{1.44042\ldots+\delta})$. Our result holds more generally for any
finite set that densely generates any connected, semisimple real Lie group,
with an extra length term in the non-compact case to reach group elements far
away from the identity.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T00:30:00Z">Monday, June 26 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13258'>A Fast Maximum $k$-Plex Algorithm Parameterized by the Degeneracy Gap</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Zhengren Wang, Yi Zhou, Chunyu Luo, Mingyu Xiao</p><p>Given a graph, the $k$-plex is a vertex set in which each vertex is not
adjacent to at most $k-1$ other vertices in the set. The maximum $k$-plex
problem, which asks for the largest $k$-plex from a given graph, is an
important but computationally challenging problem in applications like graph
search and community detection. So far, there is a number of empirical
algorithms without sufficient theoretical explanations on the efficiency. We
try to bridge this gap by defining a novel parameter of the input instance,
$g_k(G)$, the gap between the degeneracy bound and the size of maximum $k$-plex
in the given graph, and presenting an exact algorithm parameterized by
$g_k(G)$. In other words, we design an algorithm with running time polynomial
in the size of input graph and exponential in $g_k(G)$ where $k$ is a constant.
Usually, $g_k(G)$ is small and bounded by $O(\log{(|V|)})$ in real-world
graphs, indicating that the algorithm runs in polynomial time. We also carry
out massive experiments and show that the algorithm is competitive with the
state-of-the-art solvers. Additionally, for large $k$ values such as $15$ and
$20$, our algorithm has superior performance over existing algorithms.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhengren Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1">Chunyu Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1">Mingyu Xiao</a></p><p>Given a graph, the $k$-plex is a vertex set in which each vertex is not
adjacent to at most $k-1$ other vertices in the set. The maximum $k$-plex
problem, which asks for the largest $k$-plex from a given graph, is an
important but computationally challenging problem in applications like graph
search and community detection. So far, there is a number of empirical
algorithms without sufficient theoretical explanations on the efficiency. We
try to bridge this gap by defining a novel parameter of the input instance,
$g_k(G)$, the gap between the degeneracy bound and the size of maximum $k$-plex
in the given graph, and presenting an exact algorithm parameterized by
$g_k(G)$. In other words, we design an algorithm with running time polynomial
in the size of input graph and exponential in $g_k(G)$ where $k$ is a constant.
Usually, $g_k(G)$ is small and bounded by $O(\log{(|V|)})$ in real-world
graphs, indicating that the algorithm runs in polynomial time. We also carry
out massive experiments and show that the algorithm is competitive with the
state-of-the-art solvers. Additionally, for large $k$ values such as $15$ and
$20$, our algorithm has superior performance over existing algorithms.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T00:30:00Z">Monday, June 26 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13306'>On minimum $t$-claw deletion in split graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sounaka Mishra</p><p>For $t\geq 3$, $K_{1, t}$ is called $t$-claw. In minimum $t$-claw deletion
problem (\texttt{Min-$t$-Claw-Del}), given a graph $G=(V, E)$, it is required
to find a vertex set $S$ of minimum size such that $G[V\setminus S]$ is
$t$-claw free. In a split graph, the vertex set is partitioned into two sets
such that one forms a clique and the other forms an independent set. Every
$t$-claw in a split graph has a center vertex in the clique partition. This
observation motivates us to consider the minimum one-sided bipartite $t$-claw
deletion problem (\texttt{Min-$t$-OSBCD}). Given a bipartite graph $G=(A \cup
B, E)$, in \texttt{Min-$t$-OSBCD} it is asked to find a vertex set $S$ of
minimum size such that $G[V \setminus S]$ has no $t$-claw with the center
vertex in $A$. A primal-dual algorithm approximates \texttt{Min-$t$-OSBCD}
within a factor of $t$. We prove that it is $\UGC$-hard to approximate with a
factor better than $t$. We also prove it is approximable within a factor of 2
for dense bipartite graphs. By using these results on \texttt{Min-$t$-OSBCD},
we prove that \texttt{Min-$t$-Claw-Del} is $\UGC$-hard to approximate within a
factor better than $t$, for split graphs. We also consider their complementary
maximization problems and prove that they are $\APX$-complete.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Sounaka Mishra</a></p><p>For $t\geq 3$, $K_{1, t}$ is called $t$-claw. In minimum $t$-claw deletion
problem (\texttt{Min-$t$-Claw-Del}), given a graph $G=(V, E)$, it is required
to find a vertex set $S$ of minimum size such that $G[V\setminus S]$ is
$t$-claw free. In a split graph, the vertex set is partitioned into two sets
such that one forms a clique and the other forms an independent set. Every
$t$-claw in a split graph has a center vertex in the clique partition. This
observation motivates us to consider the minimum one-sided bipartite $t$-claw
deletion problem (\texttt{Min-$t$-OSBCD}). Given a bipartite graph $G=(A \cup
B, E)$, in \texttt{Min-$t$-OSBCD} it is asked to find a vertex set $S$ of
minimum size such that $G[V \setminus S]$ has no $t$-claw with the center
vertex in $A$. A primal-dual algorithm approximates \texttt{Min-$t$-OSBCD}
within a factor of $t$. We prove that it is $\UGC$-hard to approximate with a
factor better than $t$. We also prove it is approximable within a factor of 2
for dense bipartite graphs. By using these results on \texttt{Min-$t$-OSBCD},
we prove that \texttt{Min-$t$-Claw-Del} is $\UGC$-hard to approximate within a
factor better than $t$, for split graphs. We also consider their complementary
maximization problems and prove that they are $\APX$-complete.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T00:30:00Z">Monday, June 26 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13383'>Fair integer programming under dichotomous preferences</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Tom Demeulemeester, Dries Goossens, Ben Hermans, Roel Leus</p><p>One cannot make truly fair decisions using integer linear programs unless one
controls the selection probabilities of the (possibly many) optimal solutions.
For this purpose, we propose a unified framework when binary decision variables
represent agents with dichotomous preferences, who only care about whether they
are selected in the final solution. We develop several general-purpose
algorithms to fairly select optimal solutions, for example, by maximizing the
Nash product or the minimum selection probability, or by using a random
ordering of the agents as a selection criterion (Random Serial Dictatorship).
As such, we embed the black-box procedure of solving an integer linear program
into a framework that is explainable from start to finish. Moreover, we study
the axiomatic properties of the proposed methods by embedding our framework
into the rich literature of cooperative bargaining and probabilistic social
choice. Lastly, we evaluate the proposed methods on a specific application,
namely kidney exchange. We find that while the methods maximizing the Nash
product or the minimum selection probability outperform the other methods on
the evaluated welfare criteria, methods such as Random Serial Dictatorship
perform reasonably well in computation times that are similar to those of
finding a single optimal solution.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Demeulemeester_T/0/1/0/all/0/1">Tom Demeulemeester</a>, <a href="http://arxiv.org/find/cs/1/au:+Goossens_D/0/1/0/all/0/1">Dries Goossens</a>, <a href="http://arxiv.org/find/cs/1/au:+Hermans_B/0/1/0/all/0/1">Ben Hermans</a>, <a href="http://arxiv.org/find/cs/1/au:+Leus_R/0/1/0/all/0/1">Roel Leus</a></p><p>One cannot make truly fair decisions using integer linear programs unless one
controls the selection probabilities of the (possibly many) optimal solutions.
For this purpose, we propose a unified framework when binary decision variables
represent agents with dichotomous preferences, who only care about whether they
are selected in the final solution. We develop several general-purpose
algorithms to fairly select optimal solutions, for example, by maximizing the
Nash product or the minimum selection probability, or by using a random
ordering of the agents as a selection criterion (Random Serial Dictatorship).
As such, we embed the black-box procedure of solving an integer linear program
into a framework that is explainable from start to finish. Moreover, we study
the axiomatic properties of the proposed methods by embedding our framework
into the rich literature of cooperative bargaining and probabilistic social
choice. Lastly, we evaluate the proposed methods on a specific application,
namely kidney exchange. We find that while the methods maximizing the Nash
product or the minimum selection probability outperform the other methods on
the evaluated welfare criteria, methods such as Random Serial Dictatorship
perform reasonably well in computation times that are similar to those of
finding a single optimal solution.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T00:30:00Z">Monday, June 26 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13387'>Improved Competitive Ratios for Online Bipartite Matching on Degree Bounded Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yilong Feng, Xiaowei Wu, Shengwei Zhou</p><p>We consider the online bipartite matching problem on $(k,d)$-bounded graphs,
where each online vertex has at most $d$ neighbors, each offline vertex has at
least $k$ neighbors, and $k\geq d\geq 2$. The model of $(k,d)$-bounded graphs
is proposed by Naor and Wajc (EC 2015 and TEAC 2018) to model the online
advertising applications in which offline advertisers are interested in a large
number of ad slots, while each online ad slot is interesting to a small number
of advertisers. They proposed deterministic and randomized algorithms with a
competitive ratio of $1 - (1-1/d)^k$ for the problem, and show that the
competitive ratio is optimal for deterministic algorithms. They also raised the
open questions of whether strictly better competitive ratios can be achieved
using randomized algorithms, for both the adversarial and stochastic arrival
models. In this paper we answer both of their open problems affirmatively. For
the adversarial arrival model, we propose a randomized algorithm with
competitive ratio $1 - (1-1/d)^k + \Omega(d^{-4}\cdot e^{-\frac{k}{d}})$ for
all $k\geq d\geq 2$. We also consider the stochastic model and show that even
better competitive ratios can be achieved. We show that for all $k\geq d\geq
2$, the competitive ratio is always at least $0.8237$. We further consider the
$b$-matching problem when each offline vertex can be matched at most $b$ times,
and provide several competitive ratio lower bounds for the adversarial and
stochastic model.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yilong Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaowei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Shengwei Zhou</a></p><p>We consider the online bipartite matching problem on $(k,d)$-bounded graphs,
where each online vertex has at most $d$ neighbors, each offline vertex has at
least $k$ neighbors, and $k\geq d\geq 2$. The model of $(k,d)$-bounded graphs
is proposed by Naor and Wajc (EC 2015 and TEAC 2018) to model the online
advertising applications in which offline advertisers are interested in a large
number of ad slots, while each online ad slot is interesting to a small number
of advertisers. They proposed deterministic and randomized algorithms with a
competitive ratio of $1 - (1-1/d)^k$ for the problem, and show that the
competitive ratio is optimal for deterministic algorithms. They also raised the
open questions of whether strictly better competitive ratios can be achieved
using randomized algorithms, for both the adversarial and stochastic arrival
models. In this paper we answer both of their open problems affirmatively. For
the adversarial arrival model, we propose a randomized algorithm with
competitive ratio $1 - (1-1/d)^k + \Omega(d^{-4}\cdot e^{-\frac{k}{d}})$ for
all $k\geq d\geq 2$. We also consider the stochastic model and show that even
better competitive ratios can be achieved. We show that for all $k\geq d\geq
2$, the competitive ratio is always at least $0.8237$. We further consider the
$b$-matching problem when each offline vertex can be matched at most $b$ times,
and provide several competitive ratio lower bounds for the adversarial and
stochastic model.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-26T00:30:00Z">Monday, June 26 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Sunday, June 25
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/091'>TR23-091 |  Succinct Computational Secret Sharing | 

	Benny Applebaum, 

	Amos Beimel, 

	Yuval Ishai, 

	Eyal Kushilevitz, 

	Tianren Liu, 

	Vinod Vaikuntanathan</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          A secret-sharing scheme enables a dealer to share a secret $s$ among $n$ parties such that only authorized subsets of parties, specified by a monotone access structure $f:\{0,1\}^n\to\{0,1\}$, can reconstruct $s$ from their shares. Other subsets of parties learn nothing about $s$.

The question of minimizing the (largest) share size for a given $f$ has been the subject of a large body of work. However, in most existing constructions for general access structures $f$, the share size is not much smaller than the size of some natural computational representation of $f$, a fact that has often been referred to as the ``representation size barrier&#39;&#39; in secret sharing.

In this work, we initiate a systematic study of succinct computational  secret sharing (SCSS), where the secrecy requirement is computational and the goal is to substantially beat the representation size barrier. We obtain the following main results.

(1) SCSS via Projective PRGs. We introduce the notion of a *projective PRG*, a pseudorandom generator for which any subset of the output bits can be revealed while keeping the other output bits hidden, using a *short* projective seed. We construct projective PRGs with different levels of succinctness under a variety of computational assumptions, and apply them towards constructing SCSS for graph access structures, monotone CNF formulas, and (less succinctly) useful subclasses of monotone circuits and branching programs. Most notably, under the sub-exponential RSA assumption, we obtain a SCSS scheme that, given an arbitrary access structure $f$, represented by a truth table of size $N=2^n$, produces shares of size $\polylog(N)=\poly(n)$ in time $\tilde O(N)$. For comparison, the share size of the best known information-theoretic schemes is $O(N^{0.58})$.

(2) SCSS via One-way Functions. Under the (minimal) assumption that one-way functions exist, we obtain a near-quadratic separation between the total share size of computational and information-theoretic secret sharing. This is the strongest separation one can hope for, given the state of the art in secret sharing lower bounds.
We also construct SCSS schemes from one-way functions for useful classes of access structures, including forbidden graphs and monotone DNF formulas.  This leads to constructions of fully-decomposable conditional disclosure of secrets (also known as privacy-free garbled circuits) for general functions, represented by a truth table of size $N=2^n$, with share size $\polylog(N)$ and computation time $\tilde O(N)$, assuming sub-exponentially secure one-way functions.
        
        </div>

        <div class='tr-article-summary'>
        
          
          A secret-sharing scheme enables a dealer to share a secret $s$ among $n$ parties such that only authorized subsets of parties, specified by a monotone access structure $f:\{0,1\}^n\to\{0,1\}$, can reconstruct $s$ from their shares. Other subsets of parties learn nothing about $s$.

The question of minimizing the (largest) share size for a given $f$ has been the subject of a large body of work. However, in most existing constructions for general access structures $f$, the share size is not much smaller than the size of some natural computational representation of $f$, a fact that has often been referred to as the ``representation size barrier&#39;&#39; in secret sharing.

In this work, we initiate a systematic study of succinct computational  secret sharing (SCSS), where the secrecy requirement is computational and the goal is to substantially beat the representation size barrier. We obtain the following main results.

(1) SCSS via Projective PRGs. We introduce the notion of a *projective PRG*, a pseudorandom generator for which any subset of the output bits can be revealed while keeping the other output bits hidden, using a *short* projective seed. We construct projective PRGs with different levels of succinctness under a variety of computational assumptions, and apply them towards constructing SCSS for graph access structures, monotone CNF formulas, and (less succinctly) useful subclasses of monotone circuits and branching programs. Most notably, under the sub-exponential RSA assumption, we obtain a SCSS scheme that, given an arbitrary access structure $f$, represented by a truth table of size $N=2^n$, produces shares of size $\polylog(N)=\poly(n)$ in time $\tilde O(N)$. For comparison, the share size of the best known information-theoretic schemes is $O(N^{0.58})$.

(2) SCSS via One-way Functions. Under the (minimal) assumption that one-way functions exist, we obtain a near-quadratic separation between the total share size of computational and information-theoretic secret sharing. This is the strongest separation one can hope for, given the state of the art in secret sharing lower bounds.
We also construct SCSS schemes from one-way functions for useful classes of access structures, including forbidden graphs and monotone DNF formulas.  This leads to constructions of fully-decomposable conditional disclosure of secrets (also known as privacy-free garbled circuits) for general functions, represented by a truth table of size $N=2^n$, with share size $\polylog(N)$ and computation time $\tilde O(N)$, assuming sub-exponentially secure one-way functions.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-25T01:21:48Z">Sunday, June 25 2023, 01:21</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Saturday, June 24
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/'>A Hidden Heroine</a></h3>
        <p class='tr-article-feed'>from <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          William Friedman was famous as one who broke codes during both world wars. I knew about him from articles such as this. But wait His wife Elizebeth Smith Friedman is the star of a PBS TV special. Together they were the first great cryptographers of modern times. They quickly shifted gear from working on the [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>William Friedman was famous as one who broke codes during both world wars. I knew about him from articles such as <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5419462/pdf/1.pdf">this</a>. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/wf/" rel="attachment wp-att-21808"><img data-attachment-id="21808" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/wf/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/wf.jpeg?fit=299%2C168&amp;ssl=1" data-orig-size="299,168" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="wf" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/wf.jpeg?fit=299%2C168&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/wf.jpeg?fit=299%2C168&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/wf.jpeg?resize=299%2C168&#038;ssl=1" alt="" width="299" height="168" class="aligncenter size-full wp-image-21808" data-recalc-dims="1" /></a></p>
<p>
But wait <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{&#92;dots}" class="latex" /> His wife Elizebeth Smith Friedman is the star of a PBS TV <a href="https://www.imdb.com/title/tt12599258/">special</a>. Together they were the first great cryptographers of modern times. They quickly shifted gear from working on the hypothesis of embedded cryptograms in William Shakespeare&#8217;s plays in 1915&#8211;16 to helping the US WW I effort from 1917 on. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/ef/" rel="attachment wp-att-21809"><img data-attachment-id="21809" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/ef/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ef.jpeg?fit=271%2C186&amp;ssl=1" data-orig-size="271,186" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ef" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ef.jpeg?fit=271%2C186&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ef.jpeg?fit=271%2C186&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ef.jpeg?resize=271%2C186&#038;ssl=1" alt="" width="271" height="186" class="aligncenter size-full wp-image-21809" data-recalc-dims="1" /></a></p>
<p>
What is so interesting is that I was unaware of her great contributions. I thought I knew the history of code breaking. But I was totally wrong. Elizebeth Friedman&#8217;s work on decrypting coded radio messages helped tip the balances of WWI and WWII. She saved thousands of lives, but her work was hidden by the US government for 62 years. Her superiors&#8212;all men&#8212;took credit for her work. She initially got <em>none</em>. Nothing at all. </p>
<p>
This is&#8212;at least it was&#8212;one of the terrible injustices in the history of code breaking. </p>
<p>
<span id="more-21806"></span></p>
<p><H2> Century-Later Recognition </H2></p>
<p><p>
Her role was hidden until documents concerning it were declassified in 2008. She had taken an oath during her WW II work with the US Navy to keep that secret until her death, which came in 1980 with no fanfare. </p>
<p>
Still, it took a decade more for true public awareness of her importance. Three recent biographies are: </p>
<ul>
<li>
Gregg Stuart Smith, <em>A Life in Code: Pioneer Cryptanalyst Elizebeth Smith Friedman</em>, <a href="https://www.amazon.com/stores/G.-Stuart-Smith/author/B0077D2M0O?ref=ap_rdr&#038;store_ref=ap_rdr&#038;isDramIntegrated=true&#038;shoppingPortalEnabled=true">2017</a>. </p>
<li>
Jason Fagone, <em>The Woman Who Smashed Codes: A True Story of Love, Spies, and the Unlikely Heroine Who Outwitted America&#8217;s Enemies</em>, <a href="https://www.amazon.com/Woman-Who-Smashed-Codes-Outwitted/dp/0062430513/ref=pd_lpo_sccl_2/146-5971751-8425411">2018</a>. </p>
<li>
Amy Butler Greenfield, <em>The Woman All Spies Fear: Code Breaker Elizebeth Smith Friedman and Her Hidden Life</em>, <a href="https://www.goodreads.com/en/book/show/56364344-the-woman-all-spies-fear">2021</a>.
</ul>
<p>
Although the last one is written for a young-adult audience, with large print and short chapters, it still has wonderful detail on her life and work. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/greenfieldbook/" rel="attachment wp-att-21811"><img data-attachment-id="21811" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/greenfieldbook/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?fit=600%2C894&amp;ssl=1" data-orig-size="600,894" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="GreenfieldBook" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?fit=201%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?fit=600%2C894&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?resize=200%2C300&#038;ssl=1" alt="" width="200" height="300" class="aligncenter wp-image-21811" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?resize=201%2C300&amp;ssl=1 201w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?zoom=2&amp;resize=200%2C300&amp;ssl=1 400w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?zoom=3&amp;resize=200%2C300&amp;ssl=1 600w" sizes="(max-width: 200px) 100vw, 200px" data-recalc-dims="1" /></a></p>
<p>
In the meantime, the NSA&#8217;s own tribute, dated <a href="https://media.defense.gov/2021/Jul/13/2002761955/-1/-1/0/FRIEDMAN-LEGACY-TRANSCRIPT.PDF">2006</a>, gives details on Elizebeth but is headlined only for William. Most of its 226 pages reprints six lectures by William that were originally circulated within the agency in 1963. It reprints a 1980 memorial to her at the end.</p>
<p>
<p><H2> How She Started </H2></p>
<p><p>
Her origin story is amazing as well. She was one of only two in her Midwest farming family of nine to attend college and obtained a degree in English after transferring to a school closer to home. Her first job opportunity, a brief stint as substitute principal at a public high school in Indiana, did not lead to other teaching positions, so she moved back with her family. She journeyed to Chicago to look for work, using the <a href="https://en.wikipedia.org/wiki/Newberry_Library">Newberry Library</a> as a hub.</p>
<p>
In one of her last days there, a librarian tipped her that a visiting millionaire, George Fabyan, was looking for help on a project involving Shakespeare. She connected with him and the scholarly director of the project, Elizabeth Gallup, and became employed at Fabyan&#8217;s private Riverbank Research Laboratory.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/gf/" rel="attachment wp-att-21812"><img data-attachment-id="21812" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/gf/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gf.jpeg?fit=203%2C249&amp;ssl=1" data-orig-size="203,249" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gf" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gf.jpeg?fit=203%2C249&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gf.jpeg?fit=203%2C249&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gf.jpeg?resize=203%2C249&#038;ssl=1" alt="" width="203" height="249" class="aligncenter size-full wp-image-21812" data-recalc-dims="1" /></a></p>
<p>
Riverbank had standard scientific projects as well. William Friedman was employed out of Cornell to work on plant genetics. One of several factors drawing him to the Shakespeare project was that his skill photographing plants was handy for images of original manuscripts kept in England. A second was developing techniques for statistical analysis. A third was Elizebeth. </p>
<p>
By the time they married in 1917, they had worked out that the statistical randomness of defective type metal re-used by Elizabethan printers and bias in how the alleged codes in Shakespeare were identified effaced the claimed footprints of Francis Bacon&#8217;s <a href="https://en.wikipedia.org/wiki/Bacon's_cipher">two-face cipher</a>. Before they could even ascertain how to publish their eight draft papers of study, however, America&#8217;s entry into World War I pressed them into other applications. Fabyan himself volunteered the services of his lab for top-secret work. </p>
<p>
Their work on Shakespeare <a href="https://www.amazon.com/Shakespearean-Ciphers-Examined-cryptographic-Shakespeare/dp/0521141397">was published</a> in 1957. It wasn&#8217;t top secret and it bore both their names, as did its 1955 <a href="https://books.google.com/books/about/The_Cryptologist_Looks_at_Shakespeare.html?id=BBdiuAAACAAJ&#038;hl=en&#038;output=html_text">manuscript</a> which won the Folger Library Shakespeare Prize. </p>
<p>
<p><H2> Open Problems </H2></p>
<p><p>
In 1999, the year of its creation, she was inducted to the NSA Hall of Honor&#8212;see <a href="https://www.pbs.org/wgbh/americanexperience/features/codebreaker-elizebeth-friedman-fought-nazi-spies/">this</a>.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/code-2/" rel="attachment wp-att-21813"><img data-attachment-id="21813" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/code-2/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/code.jpeg?fit=320%2C208&amp;ssl=1" data-orig-size="320,208" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="code" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/code.jpeg?fit=300%2C195&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/code.jpeg?fit=320%2C208&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/code.jpeg?resize=320%2C208&#038;ssl=1" alt="" width="320" height="208" class="aligncenter size-full wp-image-21813" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/code.jpeg?w=320&amp;ssl=1 320w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/code.jpeg?resize=300%2C195&amp;ssl=1 300w" sizes="(max-width: 320px) 100vw, 320px" data-recalc-dims="1" /></a></p>
<p>
I hope that in the future credit will be given equally to women as well as to men. This of course presupposes that the women had the opportunity to begin with. Let&#8217;s hope so.</p>
<p>
<p class="authors">By RJLipton+KWRegan</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-24T21:53:47Z">Saturday, June 24 2023, 21:53</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2023/06/24/geometric-flip-width.html'>Geometric flip-width revisited</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          I recently posted here about the flip-width of geometric graphs, and to readers of that post, my new preprint “Geometric Graphs with Unbounded Flip-Width” (arXiv:2306.12611, with Rose McCarty, to appear in CCCG 2023) will look very familiar. It even has the same title! However, the process of turning it into a paper led to some improvements. Let me summarize them briefly here.
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>I recently posted here about the <a href="/blog/2023/02/20/geometric-graphs-unbounded.html">flip-width of geometric graphs</a>, and to readers of that post, my new preprint “Geometric Graphs with Unbounded Flip-Width” (<a href="http://arxiv.org/abs/2306.12611">arXiv:2306.12611</a>, with Rose McCarty, to appear in CCCG 2023) will look very familiar. It even has the same title! However, the process of turning it into a paper led to some improvements. Let me summarize them briefly here.</p>

<p>First, a reminder of the main concept, flip-width. This is defined using a pursuit–evasion game in which a robber tries to escape cops by following paths in a graph. At each turn, the cops have made a fixed number of “flips” to the graph. Each flip applies to a subset of vertices (possibly overlapping with other flips), removes edges from its adjacent vertices, and adds edges connecting its non-adjacent vertices. A turn consists of three steps: the cops announce what they will flip next, the robber moves along a path of length at most \(s\), and then the cops undo their current flips and perform the new flips that they announced. The goal of the cops is to leave the robber stuck on an isolated vertex, while the goal of the robber is to escape forever. If a class of graphs has a function \(f(s)\) such that \(f(s)\) cops can win against a robber of speed \(s\), then it has bounded flip-width. If there is a speed \(s\) for which arbitrarily many cops may be needed to catch a speed-\(s\) robber, then the class has unbounded flip-width.</p>

<p>Beyond a more careful attention to detail and rigor, new developments are:</p>

<ul>
  <li>
    <p>Three-dimensional Delaunay triangulations have unbounded flip-width. This uses a construction from another recent blog post on <a href="/blog/2023/02/25/isohedral-delaunay-complexes.html">isohedral Delaunay complexes</a>.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Beta_skeleton">Beta-skeletons</a> have unbounded flip-width. There are actually two different kinds of beta-skeleton but the interesting case is for parameter values \(\beta&lt;1\), for which the two definitions agree.</p>
  </li>
  <li>
    <p>Another type of geometric graph for which we can prove unbounded flip-width, not mentioned in the previous post, is the rectangle of influence graphs. These connect pairs of points in their plane when their axis-aligned bounding box is empty of other points. We find a recursive construction for rectangle of influence graphs containing hypercube induced subgraphs of arbitrarily large dimension, which in turn have unbounded flip-width. As with the beta-skeletons, definitions for rectangle of influence graphs disagree (about what to do with points on the boundary of the bounding box) but our hypercube construction doesn’t need that ambiguity.</p>

    <p style="text-align:center"><img src="/blog/assets/2023/empty-rectangle.svg" alt="Hypercube in a rectangle of influence graph" /></p>
  </li>
  <li>
    <p>For most of the families of geometric graphs that we study, the robber can escape by stepping across only one edge per turn (\(s=1\)). This is a big improvement over the \(s=4\) from the blog post, and a much more natural speed limitation. The exception is for three-dimensional Delaunay triangulations; for these \(s=2\) works but we don’t know about \(s=1\). An \(s=2\) escape strategy for all of these graphs is very simple: move to a “lane” (one of two special types of vertex in the “interchange” graphs constructed in the previous blog post) that has two-edge paths to many other lanes. The \(s=1\) strategy is different, and for some of the geometric graphs is based on hypercube subgraphs rather than interchanges.</p>
  </li>
  <li>
    <p>An appendix extends the results on all of these graph classes (even 3d Delaunay triangulations) from unbounded flip-width to monadic independence, meaning that it is possible to use <em>transductions</em>, a certain kind of translation system defined using logical formulas, to get arbitrary graphs from graphs in these classes. The main ideas (from Szymon Toruńczyk) are to use Ramsey theory to simplify the interchange subgraphs into two cases, “sparse” and “dense”, to use the structure of these graphs to find a logical translation from the dense case to the sparse case, and to find a subdivision of any given graph as an induced subgraph of a sparse interchange.</p>
  </li>
</ul>

<p>(<a href="https://mathstodon.xyz/@11011110/110602049050780927">Discuss on Mastodon</a>)</p><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-24T18:07:00Z">Saturday, June 24 2023, 18:07</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://blog.computationalcomplexity.org/2023/06/can-you-put-n-pennies-on-n-x-n.html'>Can you put n pennies on an n x n chessboard so that all of the distances are distinct/how to look that up?</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>&nbsp;In Jan 2023 I went to the Joint Math Meeting of the AMS and the MAA and took notes on things to look up later. In one of the talks they discussed a problem and indicated that the answer was known, but did not give a reference or a proof. I emailed the authors and got no response. I tried to search the web but could not find it. SO I use this blog post to see if someone either knows the reference or can solve it outright, and either leave the answer in the comments, point to a paper that has the answer in the comments, or email me personally.&nbsp;</p><p>--------------------------------------------------------------------</p><p>A chessboard has squares that are 1 by 1.&nbsp;</p><p>Pennies have diameter 1.</p><p>QUESTION:&nbsp;</p><p>For which n is there a way to place n pennies on squares of the n x n chessboard so that all of the distances between centers of the pennies are DIFFERENT?</p><p>-----------------------------------------------------------</p><p>I have figured out that you CAN do this for n=3,4,5. I THINK the talk said&nbsp; it cannot be done for n=6. If&nbsp; you know or find a proof or disproof then please tell me. I am looking for human-readable proofs, not computer proofs.&nbsp; Similar for higher n.</p><p>I have a writeup of the n=3,4,5 cases&nbsp;here&nbsp;(ADDED LATER- I will edit this later in light of the very interesting comments made on this blog entry.)&nbsp;</p><p>----------------------------------------------------------------------</p><p>With technology and search engines it SHOULD be easier to find out answers to questions then it was in a prior era. And I think it is. But there are times when you are still better off asking&nbsp; someone, or in my case blog about it, to find the answer. Here is hoping it works!</p><p>ADDED LATER: Within 30 minutes of posting this one of my readers wrote a program and found tha tyou CAN do it for n=6 and gives the answer. Another commenter pointed to a website with the related quetion of putting as many pawns as you can on an 8x8 board.</p><p>ADDED LATER: There are now comments on the blog pointing to the FULL SOLUTION to the problem, which one can find&nbsp;here. In summary:&nbsp;</p><p>for n=3,...,7&nbsp; there IS a way to put n pennies on a chessboard such that all distances are distinct.</p><p>for n=8,...,14 a computer search shows that there is no such way.</p><p>for n=15 there is an INTERESTING PROOF that there is no such way (good thing - the computer program had not halted yet. I do not know if it every did.)&nbsp;</p><p>for n\ge 16 there is a NICE proof that there IS such way.&nbsp;</p><p>I am ECSTATIC!- I wanted to know the answer and now I do and its easy to understand!</p><p><br></p><p>By gasarch</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>&nbsp;In Jan 2023 I went to the Joint Math Meeting of the AMS and the MAA and took notes on things to look up later. In one of the talks they discussed a problem and indicated that the answer was known, but did not give a reference or a proof. I emailed the authors and got no response. I tried to search the web but could not find it. SO I use this blog post to see if someone either knows the reference or can solve it outright, and either leave the answer in the comments, point to a paper that has the answer in the comments, or email me personally.&nbsp;</p><p>--------------------------------------------------------------------</p><p>A chessboard has squares that are 1 by 1.&nbsp;</p><p>Pennies have diameter 1.</p><p>QUESTION:&nbsp;</p><p>For which n is there a way to place n pennies on squares of the n x n chessboard so that all of the distances between centers of the pennies are DIFFERENT?</p><p>-----------------------------------------------------------</p><p>I have figured out that you CAN do this for n=3,4,5. I THINK the talk said&nbsp; it cannot be done for n=6. If&nbsp; you know or find a proof or disproof then please tell me. I am looking for human-readable proofs, not computer proofs.&nbsp; Similar for higher n.</p><p>I have a writeup of the n=3,4,5 cases&nbsp;<a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/pennychess.pdf">here</a>&nbsp;(ADDED LATER- I will edit this later in light of the very interesting comments made on this blog entry.)&nbsp;</p><p>----------------------------------------------------------------------</p><p>With technology and search engines it SHOULD be easier to find out answers to questions then it was in a prior era. And I think it is. But there are times when you are still better off asking&nbsp; someone, or in my case blog about it, to find the answer. Here is hoping it works!</p><p>ADDED LATER: Within 30 minutes of posting this one of my readers wrote a program and found tha tyou CAN do it for n=6 and gives the answer. Another commenter pointed to a website with the related quetion of putting as many pawns as you can on an 8x8 board.</p><p>ADDED LATER: There are now comments on the blog pointing to the FULL SOLUTION to the problem, which one can find&nbsp;<a href="https://oscarcunningham.com/670/unique-distancing-problem/">here</a>. In summary:&nbsp;</p><p>for n=3,...,7&nbsp; there IS a way to put n pennies on a chessboard such that all distances are distinct.</p><p>for n=8,...,14 a computer search shows that there is no such way.</p><p>for n=15 there is an INTERESTING PROOF that there is no such way (good thing - the computer program had not halted yet. I do not know if it every did.)&nbsp;</p><p>for n\ge 16 there is a NICE proof that there IS such way.&nbsp;</p><p>I am ECSTATIC!- I wanted to know the answer and now I do and its easy to understand!</p><p><br /></p><p class="authors">By gasarch</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-24T14:35:00Z">Saturday, June 24 2023, 14:35</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Friday, June 23
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12440'>Sleptsov Nets are Turing-complete</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Bernard Berthomieu, Dmitry A. Zaitsev</p><p>The present paper proves that a Sleptsov net (SN) is Turing-complete, that
considerably improves, with a brief construct, the previous result that a
strong SN is Turing-complete. Remind that, unlike Petri nets, an SN always
fires enabled transitions at their maximal firing multiplicity, as a single
step, leaving for a nondeterministic choice of which fireable transitions to
fire. A strong SN restricts nondeterministic choice to firing only the
transitions having the highest firing multiplicity.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Berthomieu_B/0/1/0/all/0/1">Bernard Berthomieu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaitsev_D/0/1/0/all/0/1">Dmitry A. Zaitsev</a></p><p>The present paper proves that a Sleptsov net (SN) is Turing-complete, that
considerably improves, with a brief construct, the previous result that a
strong SN is Turing-complete. Remind that, unlike Petri nets, an SN always
fires enabled transitions at their maximal firing multiplicity, as a single
step, leaving for a nondeterministic choice of which fireable transitions to
fire. A strong SN restricts nondeterministic choice to firing only the
transitions having the highest firing multiplicity.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13073'>Unitary Complexity and the Uhlmann Transformation Problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: John Bostanci, Yuval Efron, Tony Metger, Alexander Poremba, Luowen Qian, Henry Yuen</p><p>State transformation problems such as compressing quantum information or
breaking quantum commitments are fundamental quantum tasks. However, their
computational difficulty cannot easily be characterized using traditional
complexity theory, which focuses on tasks with classical inputs and outputs.
</p>
<p>To study the complexity of such state transformation tasks, we introduce a
framework for unitary synthesis problems, including notions of reductions and
unitary complexity classes. We use this framework to study the complexity of
transforming one entangled state into another via local operations. We
formalize this as the Uhlmann Transformation Problem, an algorithmic version of
Uhlmann's theorem. Then, we prove structural results relating the complexity of
the Uhlmann Transformation Problem, polynomial space quantum computation, and
zero knowledge protocols.
</p>
<p>The Uhlmann Transformation Problem allows us to characterize the complexity
of a variety of tasks in quantum information processing, including decoding
noisy quantum channels, breaking falsifiable quantum cryptographic assumptions,
implementing optimal prover strategies in quantum interactive proofs, and
decoding the Hawking radiation of black holes. Our framework for unitary
complexity thus provides new avenues for studying the computational complexity
of many natural quantum information processing tasks.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Bostanci_J/0/1/0/all/0/1">John Bostanci</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Efron_Y/0/1/0/all/0/1">Yuval Efron</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Metger_T/0/1/0/all/0/1">Tony Metger</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Poremba_A/0/1/0/all/0/1">Alexander Poremba</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Qian_L/0/1/0/all/0/1">Luowen Qian</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Yuen_H/0/1/0/all/0/1">Henry Yuen</a></p><p>State transformation problems such as compressing quantum information or
breaking quantum commitments are fundamental quantum tasks. However, their
computational difficulty cannot easily be characterized using traditional
complexity theory, which focuses on tasks with classical inputs and outputs.
</p>
<p>To study the complexity of such state transformation tasks, we introduce a
framework for unitary synthesis problems, including notions of reductions and
unitary complexity classes. We use this framework to study the complexity of
transforming one entangled state into another via local operations. We
formalize this as the Uhlmann Transformation Problem, an algorithmic version of
Uhlmann's theorem. Then, we prove structural results relating the complexity of
the Uhlmann Transformation Problem, polynomial space quantum computation, and
zero knowledge protocols.
</p>
<p>The Uhlmann Transformation Problem allows us to characterize the complexity
of a variety of tasks in quantum information processing, including decoding
noisy quantum channels, breaking falsifiable quantum cryptographic assumptions,
implementing optimal prover strategies in quantum interactive proofs, and
decoding the Hawking radiation of black holes. Our framework for unitary
complexity thus provides new avenues for studying the computational complexity
of many natural quantum information processing tasks.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12611'>Geometric Graphs with Unbounded Flip-Width</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: David Eppstein, Rose McCarty</p><p>We consider the flip-width of geometric graphs, a notion of graph width
recently introduced by Toru\'nczyk. We prove that many different types of
geometric graphs have unbounded flip-width. These include interval graphs,
permutation graphs, circle graphs, intersection graphs of axis-aligned line
segments or axis-aligned unit squares, unit distance graphs, unit disk graphs,
visibility graphs of simple polygons, $\beta$-skeletons, 4-polytopes, rectangle
of influence graphs, and 3d Delaunay triangulations.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Eppstein_D/0/1/0/all/0/1">David Eppstein</a>, <a href="http://arxiv.org/find/cs/1/au:+McCarty_R/0/1/0/all/0/1">Rose McCarty</a></p><p>We consider the flip-width of geometric graphs, a notion of graph width
recently introduced by Toru\'nczyk. We prove that many different types of
geometric graphs have unbounded flip-width. These include interval graphs,
permutation graphs, circle graphs, intersection graphs of axis-aligned line
segments or axis-aligned unit squares, unit distance graphs, unit disk graphs,
visibility graphs of simple polygons, $\beta$-skeletons, 4-polytopes, rectangle
of influence graphs, and 3d Delaunay triangulations.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12508'>Polynomial Logical Zonotopes: A Set Representation for Reachability Analysis of Logical Systems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Amr Alanwar, Frank J. Jiang, Karl H. Johansson</p><p>In this paper, we introduce a set representation called polynomial logical
zonotopes for performing exact and computationally efficient reachability
analysis on logical systems. Polynomial logical zonotopes are a generalization
of logical zonotopes, which are able to represent up to 2^n binary vectors
using only n generators. Due to their construction, logical zonotopes are only
able to support exact computations of some logical operations (XOR, NOT, XNOR),
while other operations (AND, NAND, OR, NOR) result in over-approximations. In
order to perform all fundamental logical operations exactly, we formulate a
generalization of logical zonotopes that is constructed by additional dependent
generators and exponent matrices. We prove that through this polynomial-like
construction, we are able to perform all of the fundamental logical operations
(XOR, NOT, XNOR, AND, NAND, OR, NOR) exactly. While we are able to perform all
of the logical operations exactly, this comes with a slight increase in
computational complexity compared to logical zonotopes. We show that we can use
polynomial logical zonotopes to perform exact reachability analysis while
retaining a low computational complexity. To illustrate and showcase the
computational benefits of polynomial logical zonotopes, we present the results
of performing reachability analysis on two use cases: (1) safety verification
of an intersection crossing protocol, (2) and reachability analysis on a
high-dimensional Boolean function. Moreover, to highlight the extensibility of
logical zonotopes, we include an additional use case where we perform a
computationally tractable exhaustive search for the key of a linear-feedback
shift register.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Alanwar_A/0/1/0/all/0/1">Amr Alanwar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1">Frank J. Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Johansson_K/0/1/0/all/0/1">Karl H. Johansson</a></p><p>In this paper, we introduce a set representation called polynomial logical
zonotopes for performing exact and computationally efficient reachability
analysis on logical systems. Polynomial logical zonotopes are a generalization
of logical zonotopes, which are able to represent up to 2^n binary vectors
using only n generators. Due to their construction, logical zonotopes are only
able to support exact computations of some logical operations (XOR, NOT, XNOR),
while other operations (AND, NAND, OR, NOR) result in over-approximations. In
order to perform all fundamental logical operations exactly, we formulate a
generalization of logical zonotopes that is constructed by additional dependent
generators and exponent matrices. We prove that through this polynomial-like
construction, we are able to perform all of the fundamental logical operations
(XOR, NOT, XNOR, AND, NAND, OR, NOR) exactly. While we are able to perform all
of the logical operations exactly, this comes with a slight increase in
computational complexity compared to logical zonotopes. We show that we can use
polynomial logical zonotopes to perform exact reachability analysis while
retaining a low computational complexity. To illustrate and showcase the
computational benefits of polynomial logical zonotopes, we present the results
of performing reachability analysis on two use cases: (1) safety verification
of an intersection crossing protocol, (2) and reachability analysis on a
high-dimensional Boolean function. Moreover, to highlight the extensibility of
logical zonotopes, we include an additional use case where we perform a
computationally tractable exhaustive search for the key of a linear-feedback
shift register.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12655'>Preprocessing Complexity for Some Graph Problems Parameterized by Structural Parameters</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Manuel Lafond, Weidong Luo</p><p>Structural graph parameters play an important role in parameterized
complexity, including in kernelization. Notably, vertex cover, neighborhood
diversity, twin-cover, and modular-width have been studied extensively in the
last few years. However, there are many fundamental problems whose
preprocessing complexity is not fully understood under these parameters.
Indeed, the existence of polynomial kernels or polynomial Turing kernels for
famous problems such as Clique, Chromatic Number, and Steiner Tree has only
been established for a subset of structural parameters. In this work, we use
several techniques to obtain a complete preprocessing complexity landscape for
over a dozen of fundamental algorithmic problems.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lafond_M/0/1/0/all/0/1">Manuel Lafond</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1">Weidong Luo</a></p><p>Structural graph parameters play an important role in parameterized
complexity, including in kernelization. Notably, vertex cover, neighborhood
diversity, twin-cover, and modular-width have been studied extensively in the
last few years. However, there are many fundamental problems whose
preprocessing complexity is not fully understood under these parameters.
Indeed, the existence of polynomial kernels or polynomial Turing kernels for
famous problems such as Clique, Chromatic Number, and Steiner Tree has only
been established for a subset of structural parameters. In this work, we use
several techniques to obtain a complete preprocessing complexity landscape for
over a dozen of fundamental algorithmic problems.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12534'>Memory-Query Tradeoffs for Randomized Convex Optimization</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xi Chen, Binghui Peng</p><p>We show that any randomized first-order algorithm which minimizes a
$d$-dimensional, $1$-Lipschitz convex function over the unit ball must either
use $\Omega(d^{2-\delta})$ bits of memory or make $\Omega(d^{1+\delta/6-o(1)})$
queries, for any constant $\delta\in (0,1)$ and when the precision $\epsilon$
is quasipolynomially small in $d$. Our result implies that cutting plane
methods, which use $\tilde{O}(d^2)$ bits of memory and $\tilde{O}(d)$ queries,
are Pareto-optimal among randomized first-order algorithms, and quadratic
memory is required to achieve optimal query complexity for convex optimization.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Binghui Peng</a></p><p>We show that any randomized first-order algorithm which minimizes a
$d$-dimensional, $1$-Lipschitz convex function over the unit ball must either
use $\Omega(d^{2-\delta})$ bits of memory or make $\Omega(d^{1+\delta/6-o(1)})$
queries, for any constant $\delta\in (0,1)$ and when the precision $\epsilon$
is quasipolynomially small in $d$. Our result implies that cutting plane
methods, which use $\tilde{O}(d^2)$ bits of memory and $\tilde{O}(d)$ queries,
are Pareto-optimal among randomized first-order algorithms, and quadratic
memory is required to achieve optimal query complexity for convex optimization.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12549'>On Differentially Private Sampling from Gaussian and Product Distributions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Badih Ghazi, Xiao Hu, Ravi Kumar, Pasin Manurangsi</p><p>Given a dataset of $n$ i.i.d. samples from an unknown distribution $P$, we
consider the problem of generating a sample from a distribution that is close
to $P$ in total variation distance, under the constraint of differential
privacy (DP). We study the problem when $P$ is a multi-dimensional Gaussian
distribution, under different assumptions on the information available to the
DP mechanism: known covariance, unknown bounded covariance, and unknown
unbounded covariance. We present new DP sampling algorithms, and show that they
achieve near-optimal sample complexity in the first two settings. Moreover,
when $P$ is a product distribution on the binary hypercube, we obtain a pure-DP
algorithm whereas only an approximate-DP algorithm (with slightly worse sample
complexity) was previously known.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ghazi_B/0/1/0/all/0/1">Badih Ghazi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1">Ravi Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Manurangsi_P/0/1/0/all/0/1">Pasin Manurangsi</a></p><p>Given a dataset of $n$ i.i.d. samples from an unknown distribution $P$, we
consider the problem of generating a sample from a distribution that is close
to $P$ in total variation distance, under the constraint of differential
privacy (DP). We study the problem when $P$ is a multi-dimensional Gaussian
distribution, under different assumptions on the information available to the
DP mechanism: known covariance, unknown bounded covariance, and unknown
unbounded covariance. We present new DP sampling algorithms, and show that they
achieve near-optimal sample complexity in the first two settings. Moreover,
when $P$ is a product distribution on the binary hypercube, we obtain a pure-DP
algorithm whereas only an approximate-DP algorithm (with slightly worse sample
complexity) was previously known.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12667'>The Power of Menus in Contract Design</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Guru Guruganesh, Jon Schneider, Joshua Wang, Junyao Zhao</p><p>We study the power of menus of contracts in principal-agent problems with
adverse selection (agents can be one of several types) and moral hazard (we
cannot observe agent actions directly). For principal-agent problems with $T$
types and $n$ actions, we show that the best menu of contracts can obtain a
factor $\Omega(\max(n, \log T))$ more utility for the principal than the best
individual contract, partially resolving an open question of Guruganesh et al.
(2021). We then turn our attention to randomized menus of linear contracts,
where we likewise show that randomized linear menus can be $\Omega(T)$ better
than the best single linear contract. As a corollary, we show this implies an
analogous gap between deterministic menus of (general) contracts and randomized
menus of contracts (as introduced by Castiglioni et al. (2022)).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Guruganesh_G/0/1/0/all/0/1">Guru Guruganesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1">Jon Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Joshua Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Junyao Zhao</a></p><p>We study the power of menus of contracts in principal-agent problems with
adverse selection (agents can be one of several types) and moral hazard (we
cannot observe agent actions directly). For principal-agent problems with $T$
types and $n$ actions, we show that the best menu of contracts can obtain a
factor $\Omega(\max(n, \log T))$ more utility for the principal than the best
individual contract, partially resolving an open question of Guruganesh et al.
(2021). We then turn our attention to randomized menus of linear contracts,
where we likewise show that randomized linear menus can be $\Omega(T)$ better
than the best single linear contract. As a corollary, we show this implies an
analogous gap between deterministic menus of (general) contracts and randomized
menus of contracts (as introduced by Castiglioni et al. (2022)).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12682'>Counting occurrences of patterns in permutations</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Andrew R Conway, Anthony J Guttmann</p><p>We develop a new, powerful method for counting elements in a {\em multiset.}
As a first application, we use this algorithm to study the number of
occurrences of patterns in a permutation. For patterns of length 3 there are
two Wilf classes, and the general behaviour of these is reasonably well-known.
We slightly extend some of the known results in that case, and exhaustively
study the case of patterns of length 4, about which there is little previous
knowledge. For such patterns, there are seven Wilf classes, and based on
extensive enumerations and careful series analysis, we have conjectured the
asymptotic behaviour for all classes.
</p>
<p>Finally, we investigate a proposal of Blitvi\'c and Steingr\'imsson as to the
range of a parameter for which a particular generating function formed from the
occurrence sequences is itself a Stieltjes moment sequence.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Conway_A/0/1/0/all/0/1">Andrew R Conway</a>, <a href="http://arxiv.org/find/math/1/au:+Guttmann_A/0/1/0/all/0/1">Anthony J Guttmann</a></p><p>We develop a new, powerful method for counting elements in a {\em multiset.}
As a first application, we use this algorithm to study the number of
occurrences of patterns in a permutation. For patterns of length 3 there are
two Wilf classes, and the general behaviour of these is reasonably well-known.
We slightly extend some of the known results in that case, and exhaustively
study the case of patterns of length 4, about which there is little previous
knowledge. For such patterns, there are seven Wilf classes, and based on
extensive enumerations and careful series analysis, we have conjectured the
asymptotic behaviour for all classes.
</p>
<p>Finally, we investigate a proposal of Blitvi\'c and Steingr\'imsson as to the
range of a parameter for which a particular generating function formed from the
occurrence sequences is itself a Stieltjes moment sequence.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12771'>Faster Compression of Deterministic Finite Automata</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Philip Bille, Inge Li G&#xf8;rtz, Max Rish&#xf8;j Pedersen</p><p>Deterministic finite automata (DFA) are a classic tool for high throughput
matching of regular expressions, both in theory and practice.
</p>
<p>Due to their high space consumption, extensive research has been devoted to
compressed representations of DFAs that still support efficient pattern
matching queries.
</p>
<p>Kumar~et~al.~[SIGCOMM 2006] introduced the \emph{delayed deterministic finite
automaton} (\ddfa{}) which exploits the large redundancy between inter-state
transitions in the automaton.
</p>
<p>They showed it to obtain up to two orders of magnitude compression of
real-world DFAs, and their work formed the basis of numerous subsequent
results.
</p>
<p>Their algorithm, as well as later algorithms based on their idea, have an
inherent quadratic-time bottleneck, as they consider every pair of states to
compute the optimal compression.
</p>
<p>In this work we present a simple, general framework based on
locality-sensitive hashing for speeding up these algorithms to achieve
sub-quadratic construction times for \ddfa{}s.
</p>
<p>We apply the framework to speed up several algorithms to near-linear time,
and experimentally evaluate their performance on real-world regular expression
sets extracted from modern intrusion detection systems.
</p>
<p>We find an order of magnitude improvement in compression times, with either
little or no loss of compression, or even significantly better compression in
some cases.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bille_P/0/1/0/all/0/1">Philip Bille</a>, <a href="http://arxiv.org/find/cs/1/au:+Gortz_I/0/1/0/all/0/1">Inge Li G&#xf8;rtz</a>, <a href="http://arxiv.org/find/cs/1/au:+Pedersen_M/0/1/0/all/0/1">Max Rish&#xf8;j Pedersen</a></p><p>Deterministic finite automata (DFA) are a classic tool for high throughput
matching of regular expressions, both in theory and practice.
</p>
<p>Due to their high space consumption, extensive research has been devoted to
compressed representations of DFAs that still support efficient pattern
matching queries.
</p>
<p>Kumar~et~al.~[SIGCOMM 2006] introduced the \emph{delayed deterministic finite
automaton} (\ddfa{}) which exploits the large redundancy between inter-state
transitions in the automaton.
</p>
<p>They showed it to obtain up to two orders of magnitude compression of
real-world DFAs, and their work formed the basis of numerous subsequent
results.
</p>
<p>Their algorithm, as well as later algorithms based on their idea, have an
inherent quadratic-time bottleneck, as they consider every pair of states to
compute the optimal compression.
</p>
<p>In this work we present a simple, general framework based on
locality-sensitive hashing for speeding up these algorithms to achieve
sub-quadratic construction times for \ddfa{}s.
</p>
<p>We apply the framework to speed up several algorithms to near-linear time,
and experimentally evaluate their performance on real-world regular expression
sets extracted from modern intrusion detection systems.
</p>
<p>We find an order of magnitude improvement in compression times, with either
little or no loss of compression, or even significantly better compression in
some cases.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12934'>On boundedness of zeros of the independence polynomial of tor</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: David de Boer, Pjotr Buys, Han Peters, Guus Regts</p><p>We study boundedness of zeros of the independence polynomial of tori for
sequences of tori converging to the integer lattice. We prove that zeros are
bounded for sequences of balanced tori, but unbounded for sequences of highly
unbalanced tori. Here balanced means that the size of the torus is at most
exponential in the shortest side length, while highly unbalanced means that the
longest side length of the torus is super exponential in the product over the
other side lengths cubed. We discuss implications of our results to the
existence of efficient algorithms for approximating the independence polynomial
on tori.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Boer_D/0/1/0/all/0/1">David de Boer</a>, <a href="http://arxiv.org/find/math/1/au:+Buys_P/0/1/0/all/0/1">Pjotr Buys</a>, <a href="http://arxiv.org/find/math/1/au:+Peters_H/0/1/0/all/0/1">Han Peters</a>, <a href="http://arxiv.org/find/math/1/au:+Regts_G/0/1/0/all/0/1">Guus Regts</a></p><p>We study boundedness of zeros of the independence polynomial of tori for
sequences of tori converging to the integer lattice. We prove that zeros are
bounded for sequences of balanced tori, but unbounded for sequences of highly
unbalanced tori. Here balanced means that the size of the torus is at most
exponential in the shortest side length, while highly unbalanced means that the
longest side length of the torus is super exponential in the product over the
other side lengths cubed. We discuss implications of our results to the
existence of efficient algorithms for approximating the independence polynomial
on tori.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.13057'>SQ Lower Bounds for Learning Bounded Covariance GMMs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, Nikos Zarifis</p><p>We study the complexity of learning mixtures of separated Gaussians with
common unknown bounded covariance matrix. Specifically, we focus on learning
Gaussian mixture models (GMMs) on $\mathbb{R}^d$ of the form $P= \sum_{i=1}^k
w_i \mathcal{N}(\boldsymbol \mu_i,\mathbf \Sigma_i)$, where $\mathbf \Sigma_i =
\mathbf \Sigma \preceq \mathbf I$ and $\min_{i \neq j} \| \boldsymbol \mu_i -
\boldsymbol \mu_j\|_2 \geq k^\epsilon$ for some $\epsilon&gt;0$. Known learning
algorithms for this family of GMMs have complexity $(dk)^{O(1/\epsilon)}$. In
this work, we prove that any Statistical Query (SQ) algorithm for this problem
requires complexity at least $d^{\Omega(1/\epsilon)}$. In the special case
where the separation is on the order of $k^{1/2}$, we additionally obtain
fine-grained SQ lower bounds with the correct exponent. Our SQ lower bounds
imply similar lower bounds for low-degree polynomial tests. Conceptually, our
results provide evidence that known algorithms for this problem are nearly best
possible.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1">Ilias Diakonikolas</a>, <a href="http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1">Daniel M. Kane</a>, <a href="http://arxiv.org/find/cs/1/au:+Pittas_T/0/1/0/all/0/1">Thanasis Pittas</a>, <a href="http://arxiv.org/find/cs/1/au:+Zarifis_N/0/1/0/all/0/1">Nikos Zarifis</a></p><p>We study the complexity of learning mixtures of separated Gaussians with
common unknown bounded covariance matrix. Specifically, we focus on learning
Gaussian mixture models (GMMs) on $\mathbb{R}^d$ of the form $P= \sum_{i=1}^k
w_i \mathcal{N}(\boldsymbol \mu_i,\mathbf \Sigma_i)$, where $\mathbf \Sigma_i =
\mathbf \Sigma \preceq \mathbf I$ and $\min_{i \neq j} \| \boldsymbol \mu_i -
\boldsymbol \mu_j\|_2 \geq k^\epsilon$ for some $\epsilon&gt;0$. Known learning
algorithms for this family of GMMs have complexity $(dk)^{O(1/\epsilon)}$. In
this work, we prove that any Statistical Query (SQ) algorithm for this problem
requires complexity at least $d^{\Omega(1/\epsilon)}$. In the special case
where the separation is on the order of $k^{1/2}$, we additionally obtain
fine-grained SQ lower bounds with the correct exponent. Our SQ lower bounds
imply similar lower bounds for low-degree polynomial tests. Conceptually, our
results provide evidence that known algorithms for this problem are nearly best
possible.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-23T00:30:00Z">Friday, June 23 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Thursday, June 22
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://blog.computationalcomplexity.org/2023/06/dont-negotiate-with-logic.html'>Don't Negotiate with Logic</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Computer science and mathematicians often try to use logic to negotiate whether it be at a university or life in general. I've tried it myself and it doesn't usually work. Even if you have that (rare) perfect argument, remember Upton Sinclair's words:&nbsp;It is difficult to get a man to understand something, when his salary depends on his not understanding it.</p><p>So make sure their salary depends on them understanding it. Or more to the point, in a world with limited resources, why it makes sense for them to help you.&nbsp;</p><ol><li>Ideally go for the win-win. Why a certain decision helps the department/college/university as well as yourself. Asking for a small investment as a seed towards a large grant for example.</li><li>How would the decision make you or your students more successful? The success of a department is measured by the success of the faculty and students. On the other hand, why would a different decision hold you and your students back.</li></ol>Even outside the university, make your objectives in line with the objectives of the person you are negotiating with to lead to a better outcome.<br>Of course sometimes you are haggling over a price or a salary when it really is a zero-sum game. There it's good to know the BATNA, Best Alternative To a Negotiated Agreement, for yourself and the other entity. In other words, if they aren't selling to you what other options do you and they have?<br>There are whole books written about negotiating strategies. Mostly it comes down to making it work for both parties. That's what matters, not the logic.<p></p><p>By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Computer science and mathematicians often try to use logic to negotiate whether it be at a university or life in general. I've tried it myself and it doesn't usually work. Even if you have that (rare) perfect argument, remember <a href="https://www.goodreads.com/quotes/21810-it-is-difficult-to-get-a-man-to-understand-something">Upton Sinclair's words</a>:&nbsp;<i>It is difficult to get a man to understand something, when his salary depends on his not understanding it</i>.</p><p>So make sure their salary depends on them understanding it. Or more to the point, in a world with limited resources, why it makes sense for them to help you.&nbsp;</p><ol style="text-align: left;"><li>Ideally go for the win-win. Why a certain decision helps the department/college/university as well as yourself. Asking for a small investment as a seed towards a large grant for example.</li><li>How would the decision make you or your students more successful? The success of a department is measured by the success of the faculty and students. On the other hand, why would a different decision hold you and your students back.</li></ol><div>Even outside the university, make your objectives in line with the objectives of the person you are negotiating with to lead to a better outcome.</div><div><br /></div><div>Of course sometimes you are haggling over a price or a salary when it really is a zero-sum game. There it's good to know the <a href="https://www.investopedia.com/terms/b/best-alternative-to-a-negotiated-agreement-batna.asp">BATNA</a>, Best Alternative To a Negotiated Agreement, for yourself and the other entity. In other words, if they aren't selling to you what other options do you and they have?</div><div><br /></div><div>There are whole books written about negotiating strategies. Mostly it comes down to making it work for both parties. That's what matters, not the logic.</div><p></p><p class="authors">By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T17:15:00Z">Thursday, June 22 2023, 17:15</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12103'>Quantum and classical query complexities for determining connectedness of matroids</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xiaowei Huang, Shiguang Feng, Lvzhou Li</p><p>Connectivity is a fundamental structural property of matroids, and has been
studied algorithmically over 50 years. In 1974, Cunningham proposed a
deterministic algorithm consuming $O(n^{2})$ queries to the independence oracle
to determine whether a matroid is connected. Since then, no algorithm, not even
a random one, has worked better. To the best of our knowledge, the classical
query complexity lower bound and the quantum complexity for this problem have
not been considered. Thus, in this paper we are devoted to addressing these
issues, and our contributions are threefold as follows: (i) First, we prove
that the randomized query complexity of determining whether a matroid is
connected is $\Omega(n^2)$ and thus the algorithm proposed by Cunningham is
optimal in classical computing. (ii) Second, we present a quantum algorithm
with $O(n^{3/2})$ queries, which exhibits provable quantum speedups over
classical ones. (iii) Third, we prove that any quantum algorithm requires
$\Omega(n)$ queries, which indicates that quantum algorithms can achieve at
most a quadratic speedup over classical ones. Therefore, we have a relatively
comprehensive understanding of the potential of quantum computing in
determining the connectedness of matroids.\
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Huang_X/0/1/0/all/0/1">Xiaowei Huang</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Feng_S/0/1/0/all/0/1">Shiguang Feng</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Li_L/0/1/0/all/0/1">Lvzhou Li</a></p><p>Connectivity is a fundamental structural property of matroids, and has been
studied algorithmically over 50 years. In 1974, Cunningham proposed a
deterministic algorithm consuming $O(n^{2})$ queries to the independence oracle
to determine whether a matroid is connected. Since then, no algorithm, not even
a random one, has worked better. To the best of our knowledge, the classical
query complexity lower bound and the quantum complexity for this problem have
not been considered. Thus, in this paper we are devoted to addressing these
issues, and our contributions are threefold as follows: (i) First, we prove
that the randomized query complexity of determining whether a matroid is
connected is $\Omega(n^2)$ and thus the algorithm proposed by Cunningham is
optimal in classical computing. (ii) Second, we present a quantum algorithm
with $O(n^{3/2})$ queries, which exhibits provable quantum speedups over
classical ones. (iii) Third, we prove that any quantum algorithm requires
$\Omega(n)$ queries, which indicates that quantum algorithms can achieve at
most a quadratic speedup over classical ones. Therefore, we have a relatively
comprehensive understanding of the potential of quantum computing in
determining the connectedness of matroids.\
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12377'>Geometric Algorithms for $k$-NN Poisoning</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Diego Ihara Centurion, Karine Chubarian, Bohan Fan, Francesco Sgherzi, Thiruvenkadam S Radhakrishnan, Anastasios Sidiropoulos, Angelo Straight</p><p>We propose a label poisoning attack on geometric data sets against
$k$-nearest neighbor classification. We provide an algorithm that can compute
an $\varepsilon n$-additive approximation of the optimal poisoning in $n\cdot
2^{2^{O(d+k/\varepsilon)}}$ time for a given data set $X \in \mathbb{R}^d$,
where $|X| = n$. Our algorithm achieves its objectives through the application
of multi-scale random partitions.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Centurion_D/0/1/0/all/0/1">Diego Ihara Centurion</a>, <a href="http://arxiv.org/find/cs/1/au:+Chubarian_K/0/1/0/all/0/1">Karine Chubarian</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1">Bohan Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sgherzi_F/0/1/0/all/0/1">Francesco Sgherzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_T/0/1/0/all/0/1">Thiruvenkadam S Radhakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sidiropoulos_A/0/1/0/all/0/1">Anastasios Sidiropoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Straight_A/0/1/0/all/0/1">Angelo Straight</a></p><p>We propose a label poisoning attack on geometric data sets against
$k$-nearest neighbor classification. We provide an algorithm that can compute
an $\varepsilon n$-additive approximation of the optimal poisoning in $n\cdot
2^{2^{O(d+k/\varepsilon)}}$ time for a given data set $X \in \mathbb{R}^d$,
where $|X| = n$. Our algorithm achieves its objectives through the application
of multi-scale random partitions.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.11802'>Fast quantum algorithm for differential equations</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mohsen Bagherimehrab, Kouhei Nakaji, Nathan Wiebe, Al&#xe1;n Aspuru-Guzik</p><p>Partial differential equations (PDEs) are ubiquitous in science and
engineering. Prior quantum algorithms for solving the system of linear
algebraic equations obtained from discretizing a PDE have a computational
complexity that scales at least linearly with the condition number $\kappa$ of
the matrices involved in the computation. For many practical applications,
$\kappa$ scales polynomially with the size $N$ of the matrices, rendering a
polynomial-in-$N$ complexity for these algorithms. Here we present a quantum
algorithm with a complexity that is polylogarithmic in $N$ but is independent
of $\kappa$ for a large class of PDEs. Our algorithm generates a quantum state
that enables extracting features of the solution. Central to our methodology is
using a wavelet basis as an auxiliary system of coordinates in which the
condition number of associated matrices is independent of $N$ by a simple
diagonal preconditioner. We present numerical simulations showing the effect of
the wavelet preconditioner for several differential equations. Our work could
provide a practical way to boost the performance of quantum-simulation
algorithms where standard methods are used for discretization.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Bagherimehrab_M/0/1/0/all/0/1">Mohsen Bagherimehrab</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Nakaji_K/0/1/0/all/0/1">Kouhei Nakaji</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Wiebe_N/0/1/0/all/0/1">Nathan Wiebe</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Aspuru_Guzik_A/0/1/0/all/0/1">Al&#xe1;n Aspuru-Guzik</a></p><p>Partial differential equations (PDEs) are ubiquitous in science and
engineering. Prior quantum algorithms for solving the system of linear
algebraic equations obtained from discretizing a PDE have a computational
complexity that scales at least linearly with the condition number $\kappa$ of
the matrices involved in the computation. For many practical applications,
$\kappa$ scales polynomially with the size $N$ of the matrices, rendering a
polynomial-in-$N$ complexity for these algorithms. Here we present a quantum
algorithm with a complexity that is polylogarithmic in $N$ but is independent
of $\kappa$ for a large class of PDEs. Our algorithm generates a quantum state
that enables extracting features of the solution. Central to our methodology is
using a wavelet basis as an auxiliary system of coordinates in which the
condition number of associated matrices is independent of $N$ by a simple
diagonal preconditioner. We present numerical simulations showing the effect of
the wavelet preconditioner for several differential equations. Our work could
provide a practical way to boost the performance of quantum-simulation
algorithms where standard methods are used for discretization.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.11939'>A Parameterized Algorithm for Flat Folding</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: David Eppstein</p><p>We prove that testing the flat foldability of an origami crease pattern
(either labeled with mountain and valley folds, or unlabeled) is
fixed-parameter tractable when parameterized by the ply of the flat-folded
state and by the treewidth of an associated planar graph, the cell adjacency
graph of an arrangement of polygons formed by the flat-folded state. For flat
foldings of bounded ply, our algorithm is single-exponential in the treewidth;
this dependence on treewidth is necessary under the exponential time
hypothesis.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Eppstein_D/0/1/0/all/0/1">David Eppstein</a></p><p>We prove that testing the flat foldability of an origami crease pattern
(either labeled with mountain and valley folds, or unlabeled) is
fixed-parameter tractable when parameterized by the ply of the flat-folded
state and by the treewidth of an associated planar graph, the cell adjacency
graph of an arrangement of polygons formed by the flat-folded state. For flat
foldings of bounded ply, our algorithm is single-exponential in the treewidth;
this dependence on treewidth is necessary under the exponential time
hypothesis.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.11828'>Near-Optimal Dynamic Rounding of Fractional Matchings in Bipartite Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sayan Bhattacharya, Peter Kiss, Aaron Sidford, David Wajc</p><p>We study dynamic $(1-\epsilon)$-approximate rounding of fractional matchings
-- a key ingredient in numerous breakthroughs in the dynamic graph algorithms
literature. Our first contribution is a surprisingly simple deterministic
rounding algorithm in bipartite graphs with amortized update time
$O(\epsilon^{-1} \log^2 (\epsilon^{-1} \cdot n))$, matching an (unconditional)
recourse lower bound of $\Omega(\epsilon^{-1})$ up to logarithmic factors.
Moreover, this algorithm's update time improves provided the minimum (non-zero)
weight in the fractional matching is lower bounded throughout. Combining this
algorithm with novel dynamic \emph{partial rounding} algorithms to increase
this minimum weight, we obtain several algorithms that improve this dependence
on $n$. For example, we give a high-probability randomized algorithm with
$\tilde{O}(\epsilon^{-1}\cdot (\log\log n)^2)$-update time against adaptive
adversaries. (We use Soft-Oh notation, $\tilde{O}$, to suppress polylogarithmic
factors in the argument, i.e., $\tilde{O}(f)=O(f\cdot \mathrm{poly}(\log f))$.)
Using our rounding algorithms, we also round known $(1-\epsilon)$-decremental
fractional bipartite matching algorithms with no asymptotic overhead, thus
improving on state-of-the-art algorithms for the decremental bipartite matching
problem. Further, we provide extensions of our results to general graphs and to
maintaining almost-maximal matchings.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1">Sayan Bhattacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Kiss_P/0/1/0/all/0/1">Peter Kiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Sidford_A/0/1/0/all/0/1">Aaron Sidford</a>, <a href="http://arxiv.org/find/cs/1/au:+Wajc_D/0/1/0/all/0/1">David Wajc</a></p><p>We study dynamic $(1-\epsilon)$-approximate rounding of fractional matchings
-- a key ingredient in numerous breakthroughs in the dynamic graph algorithms
literature. Our first contribution is a surprisingly simple deterministic
rounding algorithm in bipartite graphs with amortized update time
$O(\epsilon^{-1} \log^2 (\epsilon^{-1} \cdot n))$, matching an (unconditional)
recourse lower bound of $\Omega(\epsilon^{-1})$ up to logarithmic factors.
Moreover, this algorithm's update time improves provided the minimum (non-zero)
weight in the fractional matching is lower bounded throughout. Combining this
algorithm with novel dynamic \emph{partial rounding} algorithms to increase
this minimum weight, we obtain several algorithms that improve this dependence
on $n$. For example, we give a high-probability randomized algorithm with
$\tilde{O}(\epsilon^{-1}\cdot (\log\log n)^2)$-update time against adaptive
adversaries. (We use Soft-Oh notation, $\tilde{O}$, to suppress polylogarithmic
factors in the argument, i.e., $\tilde{O}(f)=O(f\cdot \mathrm{poly}(\log f))$.)
Using our rounding algorithms, we also round known $(1-\epsilon)$-decremental
fractional bipartite matching algorithms with no asymptotic overhead, thus
improving on state-of-the-art algorithms for the decremental bipartite matching
problem. Further, we provide extensions of our results to general graphs and to
maintaining almost-maximal matchings.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.11951'>On the Optimal Bounds for Noisy Computing</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Banghua Zhu, Ziao Wang, Nadim Ghaddar, Jiantao Jiao, Lele Wang</p><p>We revisit the problem of computing with noisy information considered in
Feige et al. 1994, which includes computing the OR function from noisy queries,
and computing the MAX, SEARCH and SORT functions from noisy pairwise
comparisons. For $K$ given elements, the goal is to correctly recover the
desired function with probability at least $1-\delta$ when the outcome of each
query is flipped with probability $p$. We consider both the adaptive sampling
setting where each query can be adaptively designed based on past outcomes, and
the non-adaptive sampling setting where the query cannot depend on past
outcomes. The prior work provides tight bounds on the worst-case query
complexity in terms of the dependence on $K$. However, the upper and lower
bounds do not match in terms of the dependence on $\delta$ and $p$. We improve
the lower bounds for all the four functions under both adaptive and
non-adaptive query models. Most of our lower bounds match the upper bounds up
to constant factors when either $p$ or $\delta$ is bounded away from $0$, while
the ratio between the best prior upper and lower bounds goes to infinity when
$p\rightarrow 0$ or $p\rightarrow 1/2$. On the other hand, we also provide
matching upper and lower bounds for the number of queries in expectation,
improving both the upper and lower bounds for the variable-length query model.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Banghua Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghaddar_N/0/1/0/all/0/1">Nadim Ghaddar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1">Jiantao Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lele Wang</a></p><p>We revisit the problem of computing with noisy information considered in
Feige et al. 1994, which includes computing the OR function from noisy queries,
and computing the MAX, SEARCH and SORT functions from noisy pairwise
comparisons. For $K$ given elements, the goal is to correctly recover the
desired function with probability at least $1-\delta$ when the outcome of each
query is flipped with probability $p$. We consider both the adaptive sampling
setting where each query can be adaptively designed based on past outcomes, and
the non-adaptive sampling setting where the query cannot depend on past
outcomes. The prior work provides tight bounds on the worst-case query
complexity in terms of the dependence on $K$. However, the upper and lower
bounds do not match in terms of the dependence on $\delta$ and $p$. We improve
the lower bounds for all the four functions under both adaptive and
non-adaptive query models. Most of our lower bounds match the upper bounds up
to constant factors when either $p$ or $\delta$ is bounded away from $0$, while
the ratio between the best prior upper and lower bounds goes to infinity when
$p\rightarrow 0$ or $p\rightarrow 1/2$. On the other hand, we also provide
matching upper and lower bounds for the number of queries in expectation,
improving both the upper and lower bounds for the variable-length query model.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.11964'>Sampling Individually-Fair Rankings that are Always Group Fair</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sruthi Gorantla, Anay Mehrotra, Amit Deshpande, Anand Louis</p><p>Rankings on online platforms help their end-users find the relevant
information -- people, news, media, and products -- quickly. Fair ranking
tasks, which ask to rank a set of items to maximize utility subject to
satisfying group-fairness constraints, have gained significant interest in the
Algorithmic Fairness, Information Retrieval, and Machine Learning literature.
Recent works, however, identify uncertainty in the utilities of items as a
primary cause of unfairness and propose introducing randomness in the output.
This randomness is carefully chosen to guarantee an adequate representation of
each item (while accounting for the uncertainty). However, due to this
randomness, the output rankings may violate group fairness constraints. We give
an efficient algorithm that samples rankings from an individually-fair
distribution while ensuring that every output ranking is group fair. The
expected utility of the output ranking is at least $\alpha$ times the utility
of the optimal fair solution. Here, $\alpha$ depends on the utilities,
position-discounts, and constraints -- it approaches 1 as the range of
utilities or the position-discounts shrinks, or when utilities satisfy
distributional assumptions. Empirically, we observe that our algorithm achieves
individual and group fairness and that Pareto dominates the state-of-the-art
baselines.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gorantla_S/0/1/0/all/0/1">Sruthi Gorantla</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehrotra_A/0/1/0/all/0/1">Anay Mehrotra</a>, <a href="http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1">Amit Deshpande</a>, <a href="http://arxiv.org/find/cs/1/au:+Louis_A/0/1/0/all/0/1">Anand Louis</a></p><p>Rankings on online platforms help their end-users find the relevant
information -- people, news, media, and products -- quickly. Fair ranking
tasks, which ask to rank a set of items to maximize utility subject to
satisfying group-fairness constraints, have gained significant interest in the
Algorithmic Fairness, Information Retrieval, and Machine Learning literature.
Recent works, however, identify uncertainty in the utilities of items as a
primary cause of unfairness and propose introducing randomness in the output.
This randomness is carefully chosen to guarantee an adequate representation of
each item (while accounting for the uncertainty). However, due to this
randomness, the output rankings may violate group fairness constraints. We give
an efficient algorithm that samples rankings from an individually-fair
distribution while ensuring that every output ranking is group fair. The
expected utility of the output ranking is at least $\alpha$ times the utility
of the optimal fair solution. Here, $\alpha$ depends on the utilities,
position-discounts, and constraints -- it approaches 1 as the range of
utilities or the position-discounts shrinks, or when utilities satisfy
distributional assumptions. Empirically, we observe that our algorithm achieves
individual and group fairness and that Pareto dominates the state-of-the-art
baselines.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12042'>Block-Wise Index Modulation and Receiver Design for High-Mobility OTFS Communications</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mi Qian, Fei Ji, Yao Ge, Miaowen Wen, Xiang Cheng, H. Vincent Poor</p><p>As a promising technique for high-mobility wireless communications,
orthogonal time frequency space (OTFS) has been proved to enjoy excellent
advantages with respect to traditional orthogonal frequency division
multiplexing (OFDM). Although multiple studies have considered index modulation
(IM) based OTFS (IM-OTFS) schemes to further improve system performance, a
challenging and open problem is the development of effective IM schemes and
efficient receivers for practical OTFS systems that must operate in the
presence of channel delays and Doppler shifts. In this paper, we propose two
novel block-wise IM schemes for OTFS systems, named delay-IM with OTFS
(DeIM-OTFS) and Doppler-IM with OTFS (DoIM-OTFS), where a block of
delay/Doppler resource bins are activated simultaneously. Based on a maximum
likelihood (ML) detector, we analyze upper bounds on the average bit error
rates for the proposed DeIM-OTFS and DoIM-OTFS schemes, and verify their
performance advantages over the existing IM-OTFS systems. We also develop a
multi-layer joint symbol and activation pattern detection (MLJSAPD) algorithm
and a customized message passing detection (CMPD) algorithm for our proposed
DeIMOTFS and DoIM-OTFS systems with low complexity. Simulation results
demonstrate that our proposed MLJSAPD and CMPD algorithms can achieve desired
performance with robustness to the imperfect channel state information (CSI).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Qian_M/0/1/0/all/0/1">Mi Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_F/0/1/0/all/0/1">Fei Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1">Miaowen Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xiang Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1">H. Vincent Poor</a></p><p>As a promising technique for high-mobility wireless communications,
orthogonal time frequency space (OTFS) has been proved to enjoy excellent
advantages with respect to traditional orthogonal frequency division
multiplexing (OFDM). Although multiple studies have considered index modulation
(IM) based OTFS (IM-OTFS) schemes to further improve system performance, a
challenging and open problem is the development of effective IM schemes and
efficient receivers for practical OTFS systems that must operate in the
presence of channel delays and Doppler shifts. In this paper, we propose two
novel block-wise IM schemes for OTFS systems, named delay-IM with OTFS
(DeIM-OTFS) and Doppler-IM with OTFS (DoIM-OTFS), where a block of
delay/Doppler resource bins are activated simultaneously. Based on a maximum
likelihood (ML) detector, we analyze upper bounds on the average bit error
rates for the proposed DeIM-OTFS and DoIM-OTFS schemes, and verify their
performance advantages over the existing IM-OTFS systems. We also develop a
multi-layer joint symbol and activation pattern detection (MLJSAPD) algorithm
and a customized message passing detection (CMPD) algorithm for our proposed
DeIMOTFS and DoIM-OTFS systems with low complexity. Simulation results
demonstrate that our proposed MLJSAPD and CMPD algorithms can achieve desired
performance with robustness to the imperfect channel state information (CSI).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12071'>Optimal (degree+1)-Coloring in Congested Clique</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sam Coy, Artur Czumaj, Peter Davies, Gopinath Mishra</p><p>We consider the distributed complexity of the (degree+1)-list coloring
problem, in which each node $u$ of degree $d(u)$ is assigned a palette of
$d(u)+1$ colors, and the goal is to find a proper coloring using these color
palettes. The (degree+1)-list coloring problem is a natural generalization of
the classical $(\Delta+1)$-coloring and $(\Delta+1)$-list coloring problems,
both being benchmark problems extensively studied in distributed and parallel
computing. In this paper we settle the complexity of the (degree+1)-list
coloring problem in the Congested Clique model by showing that it can be solved
deterministically in a constant number of rounds.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Coy_S/0/1/0/all/0/1">Sam Coy</a>, <a href="http://arxiv.org/find/cs/1/au:+Czumaj_A/0/1/0/all/0/1">Artur Czumaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Davies_P/0/1/0/all/0/1">Peter Davies</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1">Gopinath Mishra</a></p><p>We consider the distributed complexity of the (degree+1)-list coloring
problem, in which each node $u$ of degree $d(u)$ is assigned a palette of
$d(u)+1$ colors, and the goal is to find a proper coloring using these color
palettes. The (degree+1)-list coloring problem is a natural generalization of
the classical $(\Delta+1)$-coloring and $(\Delta+1)$-list coloring problems,
both being benchmark problems extensively studied in distributed and parallel
computing. In this paper we settle the complexity of the (degree+1)-list
coloring problem in the Congested Clique model by showing that it can be solved
deterministically in a constant number of rounds.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12282'>Online Resource Allocation with Convex-set Machine-Learned Advice</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Negin Golrezaei, Patrick Jaillet, Zijie Zhou</p><p>Decision-makers often have access to a machine-learned prediction about
demand, referred to as advice, which can potentially be utilized in online
decision-making processes for resource allocation. However, exploiting such
advice poses challenges due to its potential inaccuracy. To address this issue,
we propose a framework that enhances online resource allocation decisions with
potentially unreliable machine-learned (ML) advice. We assume here that this
advice is represented by a general convex uncertainty set for the demand
vector.
</p>
<p>We introduce a parameterized class of Pareto optimal online resource
allocation algorithms that strike a balance between consistent and robust
ratios. The consistent ratio measures the algorithm's performance (compared to
the optimal hindsight solution) when the ML advice is accurate, while the
robust ratio captures performance under an adversarial demand process when the
advice is inaccurate. Specifically, in a C-Pareto optimal setting, we maximize
the robust ratio while ensuring that the consistent ratio is at least C. Our
proposed C-Pareto optimal algorithm is an adaptive protection level algorithm,
which extends the classical fixed protection level algorithm introduced in
Littlewood (2005) and Ball and Queyranne (2009). Solving a complex non-convex
continuous optimization problem characterizes the adaptive protection level
algorithm. To complement our algorithms, we present a simple method for
computing the maximum achievable consistent ratio, which serves as an estimate
for the maximum value of the ML advice. Additionally, we present numerical
studies to evaluate the performance of our algorithm in comparison to benchmark
algorithms. The results demonstrate that by adjusting the parameter C, our
algorithms effectively strike a balance between worst-case and average
performance, outperforming the benchmark algorithms.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Golrezaei_N/0/1/0/all/0/1">Negin Golrezaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaillet_P/0/1/0/all/0/1">Patrick Jaillet</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zijie Zhou</a></p><p>Decision-makers often have access to a machine-learned prediction about
demand, referred to as advice, which can potentially be utilized in online
decision-making processes for resource allocation. However, exploiting such
advice poses challenges due to its potential inaccuracy. To address this issue,
we propose a framework that enhances online resource allocation decisions with
potentially unreliable machine-learned (ML) advice. We assume here that this
advice is represented by a general convex uncertainty set for the demand
vector.
</p>
<p>We introduce a parameterized class of Pareto optimal online resource
allocation algorithms that strike a balance between consistent and robust
ratios. The consistent ratio measures the algorithm's performance (compared to
the optimal hindsight solution) when the ML advice is accurate, while the
robust ratio captures performance under an adversarial demand process when the
advice is inaccurate. Specifically, in a C-Pareto optimal setting, we maximize
the robust ratio while ensuring that the consistent ratio is at least C. Our
proposed C-Pareto optimal algorithm is an adaptive protection level algorithm,
which extends the classical fixed protection level algorithm introduced in
Littlewood (2005) and Ball and Queyranne (2009). Solving a complex non-convex
continuous optimization problem characterizes the adaptive protection level
algorithm. To complement our algorithms, we present a simple method for
computing the maximum achievable consistent ratio, which serves as an estimate
for the maximum value of the ML advice. Additionally, we present numerical
studies to evaluate the performance of our algorithm in comparison to benchmark
algorithms. The results demonstrate that by adjusting the parameter C, our
algorithms effectively strike a balance between worst-case and average
performance, outperforming the benchmark algorithms.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.12344'>An efficient, provably exact algorithm for the 0-1 loss linear classification problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xi He, Max A. Little</p><p>Algorithms for solving the linear classification problem have a long history,
dating back at least to 1936 with linear discriminant analysis. For linearly
separable data, many algorithms can obtain the exact solution to the
corresponding 0-1 loss classification problem efficiently, but for data which
is not linearly separable, it has been shown that this problem, in full
generality, is NP-hard. Alternative approaches all involve approximations of
some kind, including the use of surrogates for the 0-1 loss (for example, the
hinge or logistic loss) or approximate combinatorial search, none of which can
be guaranteed to solve the problem exactly. Finding efficient algorithms to
obtain an exact i.e. globally optimal solution for the 0-1 loss linear
classification problem with fixed dimension, remains an open problem. In
research we report here, we detail the construction of a new algorithm,
incremental cell enumeration (ICE), that can solve the 0-1 loss classification
problem exactly in polynomial time. To our knowledge, this is the first,
rigorously-proven polynomial time algorithm for this long-standing problem.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xi He</a>, <a href="http://arxiv.org/find/cs/1/au:+Little_M/0/1/0/all/0/1">Max A. Little</a></p><p>Algorithms for solving the linear classification problem have a long history,
dating back at least to 1936 with linear discriminant analysis. For linearly
separable data, many algorithms can obtain the exact solution to the
corresponding 0-1 loss classification problem efficiently, but for data which
is not linearly separable, it has been shown that this problem, in full
generality, is NP-hard. Alternative approaches all involve approximations of
some kind, including the use of surrogates for the 0-1 loss (for example, the
hinge or logistic loss) or approximate combinatorial search, none of which can
be guaranteed to solve the problem exactly. Finding efficient algorithms to
obtain an exact i.e. globally optimal solution for the 0-1 loss linear
classification problem with fixed dimension, remains an open problem. In
research we report here, we detail the construction of a new algorithm,
incremental cell enumeration (ICE), that can solve the 0-1 loss classification
problem exactly in polynomial time. To our knowledge, this is the first,
rigorously-proven polynomial time algorithm for this long-standing problem.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-22T00:30:00Z">Thursday, June 22 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Wednesday, June 21
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2023/06/21/flat-folding-map.html'>Flat folding and map folding</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Another day, another new arXiv preprint. Today’s is “A parameterized algorithm for flat folding” (arXiv:2306.11939, to appear at CCCG 2023), a paper I mentioned in the last slide of my recent talk on graph width parameters for parameterized geometric algorithms.
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Another day, another new arXiv preprint. Today’s is “A parameterized algorithm for flat folding” (<a href="https://arxiv.org/abs/2306.11939">arXiv:2306.11939</a>, to appear at CCCG 2023), a paper I mentioned in the last slide of my recent talk on <a href="https://www.ics.uci.edu/~eppstein/pubs/Epp-WPAGP-23.pdf">graph width parameters for parameterized geometric algorithms</a>.</p>

<p>It’s been known since the work of Marshall Bern and Barry Hayes in SODA 1996 that it’s NP-complete to test whether a given origami pattern (with or without an assignment of a mountain or valley fold to each crease) can be folded flat. Of course, many origami structures are not intended to be flat, but they still often start with a flat base, and testing flatness is a prerequisite for any more complicated foldability question you might have. On the other hand, if a pattern does fold flat, it’s very easy to determine where in plane each piece of paper folds to; the hard part is determining the above-below relation between different pieces of paper that fold to the same points.</p>

<p>The new paper defines a graph from any folding pattern, whose vertices represent connected regions of the plane that should be covered by the same pieces of paper in the folded state and whose edges represent adjacencies between those regions. An example is shown below.</p>

<p style="text-align:center"><img src="/blog/assets/2023/folding-graph.svg" alt="A folding pattern and its graph" /></p>

<p>The left side is the folding pattern (blue for valley folds and red for mountain folds): first, fold the right edge of the paper on top of the rest, along the vertical blue crease, and then second, fold the top right corner down, along the two diagonal creases. The right side is how it should map to a folded state (without an assignment of an above-below relation to the layers in that state) and the resulting graph. The goal of the algorithm is to figure out that the regions of the folding pattern should be layered (bottom to top) in the order: big pentagon, lower right trapezoid, upper right trapezoid, triangle. It determines whether a folded state like that exists, but not the sequence of moves you would need to make to get there. But some care is needed in defining the layer order: it has to be local, because there exist folding patterns where different regions are layered differently in different parts of the folded state.</p>

<p style="text-align:center"><img src="/blog/assets/2023/over-under.svg" alt="A folding pattern where two regions have different over-under relations at two different parts of the folded state" style="width:100%;max-width:720px" /></p>

<p>The paper shows that finding a valid above-below relation for each region can be done using a dynamic programming algorithm in time exponential in the treewidth of this graph, where the base of the exponential is factorial in the <em>ply</em> of the folding pattern, the maximum number of pieces of paper that all fold on top of each other at any single point. As it also shows, even for patterns of bounded ply, the exponential dependence on treewidth is necessary under standard complexity-theoretic assumptions.</p>

<p>But what about the dependence on ply? Is that necessary? Why should it be difficult to fold patterns that have many layers, but simple region adjacency graphs?</p>

<p>Unlike for treewidth, I don’t have a proof that high ply makes the problem hard. But there is a natural class of problems for which the ply is very high, the treewidth is tiny, and the complexity of finding a flat folding is a famous open problem. This is the <a href="https://en.wikipedia.org/wiki/Map_folding">map folding problem</a>, where the input is just a rectangular grid with labeled creases, and the desired output is a flat-folded state respecting the given labeling. Its region adjacency graph is just a single vertex (everything should fold onto a single square), but its ply can be big (the number of squares in the entire grid). Even for a very simple case, a \(2\times 2\) grid, Wikipedia provides the following illustration of the many vertical orderings among the squares of the grid that are possible. Each square is shown with a different color, visible on both of its sides:</p>

<p style="text-align:center"><img src="/blog/assets/2023/map-foldings.png" alt="Eight solutions to the 2x2 map folding problem" title="CC-BY-SA 3.0 image File:MapFoldings-2x2.png by Robert Dickau from Wikimedia commons" /></p>

<p>The \(1\times n\) map folding problem is easy: just fold over one square from the end of a strip of \(n\) squares, according to the label of its crease, and then solve the remaining \(1\times (n-1)\) problem recursively, treating the folded-over square and the next square that it is folded onto as a single unit. In a 2012 MIT master’s thesis supervised by Erik Demaine, Thomas Morgan <a href="http://dspace.mit.edu/handle/1721.1/77030">announced a solution to the \(2\times n\) map folding problem</a>, although I don’t know that it was ever published in any other form. The algorithm is complicated, with time bound \(O(n^9)\), and the writing is not always clear.</p>

<p>The techniques from Morgan’s thesis are very specific to the \(2\times n\) case. Basically, everything you need to know can be recovered from a one-dimensional flat folding of the central crease of the \(2\times n\) grid. At the end of the thesis there is a single paragraph on higher order grids, which (after removing unnecessary complications) boils down to testing all orderings of the squares that are consistent with the above-below relations forced by the crease labels of adjacent squares. The number of orderings can be very small (for instance, if you accordion-fold in one direction and then the other, there is only one consistent ordering) and when it is the algorithm is fast. Beyond that observation, the map folding problem for grids larger than \(2\times n\) appears to be wide open.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/110585322158385767">Discuss on Mastodon</a>)</p><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-21T18:34:00Z">Wednesday, June 21 2023, 18:34</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Tuesday, June 20
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2023/06/20/all-but-clique.html'>All-but-clique-universal graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          When he visited UCI last May, Noga Alon gave two talks: a technical seminar on universal graphs and a more general talk on some of his work in the theory of machine learning. The seminar talk was based in part on his paper “Asymptotically optimal induced universal graphs” (Geom. Funct. Anal. 2017) in which he proves that the smallest graphs containing all \(k\)-vertex graphs as induced subgraphs have size
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>When he visited UCI last May, Noga Alon gave two talks: a technical seminar on <a href="https://en.wikipedia.org/wiki/Universal_graph">universal graphs</a> and a more general talk on some of his work in the theory of machine learning. The seminar talk was based in part on his paper “Asymptotically optimal induced universal graphs” (<a href="https://doi.org/10.1007/s00039-017-0396-9"><em>Geom. Funct. Anal.</em> 2017</a>) in which he proves that the smallest graphs containing all \(k\)-vertex graphs as induced subgraphs have size</p>

\[2^{(k-1)/2}\bigl(1+o(1)\bigr),\]

<p>tight except possibly for the \(o(1)\) term. At the talk I asked him whether the same bound also worked for graphs that contain all <span style="white-space:nowrap">\(k\)-vertex</span> graphs except for the <span style="white-space:nowrap">\(k\)-clique,</span> but do not contain the <span style="white-space:nowrap">\(k\)-clique.</span> He came back a day later with the answer: Yes.</p>

<p>The reason for my question was that I needed these all-but-clique-universal graphs for a paper, now on arXiv: “Quasipolynomiality of the smallest missing induced subgraph”, with Andrea Lincoln and Virginia Vassilevska Williams, <a href="https://arxiv.org/abs/2306.11185">arXiv:2306.11185</a>, to appear in <em>J. Graph Alg. Appl.</em> This is a followup to <a href="/blog/2019/05/27/shattering-quasipolynomiality.html">an earlier blog post on quasipolynomiality</a> in which I observed that the smallest graph that is not an induced subgraph of some given graph could be found in quasipolynomial time. It necessarily has at most logarithmic size, so you can just do a brute force search of all subgraphs of that size to see which ones are missing. The new paper proves that, under the <a href="https://en.wikipedia.org/wiki/Exponential_time_hypothesis">exponential time hypothesis</a>, this sort of quasipolynomial time bound is necessary: you cannot solve the problem faster than the input size to a logarithmic power. The proof is an easy reduction from clique-finding in which one adds an all-but-clique-universal graph to the input to force the smallest missing graph to be a clique.</p>

<p>Another way of describing our paper is that it finds tight time bounds for determining the largest subgraph size for which a given graph is induced-universal. This adds to the small but growing repertoire of problems for which a quasipolynomial time bound is the correct time bound, under standard complexity theory assumptions. Others cited in the paper include finding approximate Nash equilibria, and finding maximum independent sets in hyperbolic unit disk graphs.</p>

<p>In our paper we use a recursive construction for all-but-clique-universal graphs that produces bigger graphs than Alon’s, of size <span style="white-space:nowrap">roughly \(2^k\),</span> for two reasons: first, because we need a deterministic construction (Alon’s is randomized), and second, because our paper was almost completely through the reviewing process when I found out about Alon’s construction. For proving quasipolynomiality, the bigger exponent in our construction isn’t important. But I thought this post would be a good place to describe Alon’s construction instead. Keep in mind that this is based on my faulty recollection of a conversation several weeks ago, so this will read more like half-baked research notes than like a completed result. I’ve probably introduced multiple mistakes into the construction: these are my fault, not Alon’s.</p>

<p>The main idea of Alon’s paper is to cover most induced subgraphs of size at most \(k\) by a big random graph, chosen with the specified number of vertices and with each edge present or absent independently with <span style="white-space:nowrap">probability \(\tfrac12\).</span> As he proves, this is (with high probability) a universal graph for all of the subgraphs that have few symmetries. But it may miss some highly symmetric subgraphs. To cover those, he shows that (regardless of its actual symmetries) every symmetric subgraph has three moderately-large ordered sets of vertices \(A\), \(B\), and \(C\) such that the <span style="white-space:nowrap">\(A\)–\(B\)</span> adjacencies have exactly the same pattern as the <span style="white-space:nowrap">\(A\)–\(C\)</span> adjacencies. Storing only one of these two sets of adjacencies saves enough information that he can use a less-efficient construction for a universal graph for the symmetric subgraphs. The result has size \(O(1/k)\) times the size of the big random part. Because it’s so much smaller, adding it to the big random graph only increases the \(o(1)\) term in their combined size bound.</p>

<p>Modifying this to produce an all-but-clique-universal graph requires only a few more ideas.</p>

<ul>
  <li>
    <p>First, the big random part can contain a <span style="white-space:nowrap">\(k\)-clique.</span> But the expected number of copies of any graph in this part is inversely proportional to its number of symmetries. The size of this part is specifically chosen by a formula (equation 1 of Alon’s paper) that makes the expected number of copies equal to one when the number of symmetries is moderate, <span style="white-space:nowrap">\(k^{16\sqrt{k\log k}}\).</span> In this way, there are many expected copies for graphs with few symmetries <span style="white-space:nowrap">(\(\le k^{8\sqrt{k\log k}}\)),</span> which with some care leads to a proof that all such graphs are covered with high probability. But because the number of symmetries of a clique <span style="white-space:nowrap">is \(k!\),</span> much larger than the moderate size bound above, the expected number of cliques is much smaller than one. So choosing a random graph conditioned to have no clique doesn’t really change the high probability of covering all the few-symmetry graphs.</p>
  </li>
  <li>
    <p>Second, the part \(S\) of Alon’s construction that covers the high-symmetry subgraphs will also contain a <span style="white-space:nowrap">\(k\)-clique,</span> but one can fix that by using the graph product \(S\times (K_{k}-e)\) with a clique minus an edge. Any <span style="white-space:nowrap">\(k\)-vertex</span> subgraph of this product either uses one copy of each clique vertex, and is missing the same edge, or it has two vertices coming from the same clique vertex, which cannot be adjacent in the product. Therefore, there are no <span style="white-space:nowrap">\(k\)-cliques</span> in the product, but all other <span style="white-space:nowrap">\(k\)-vertex</span> induced subgraphs remain present.</p>
  </li>
  <li>
    <p>Third, the product blows up the symmetric part by a factor <span style="white-space:nowrap">of \(k\),</span> big enough to increase the multiplier in the size of the overall graph from \(1+o(1)\) <span style="white-space:nowrap">to \(O(1)\).</span> To avoid this increase, we need to compensate by being more careful in the construction of \(S\) to make it even smaller. I didn’t hear about the details of this part.</p>
  </li>
</ul>

<p>Putting these ideas together should give an all-but-clique-universal graph of size</p>

\[2^{(k-1)/2}\bigl(1+o(1)\bigr),\]

<p>the same bound as for the induced-universal graphs. If that’s all too sketchy to be relied on, you could always just use the product of any induced-universal graph <span style="white-space:nowrap">with \(K_k-e\).</span> It’s bigger by a factor <span style="white-space:nowrap">of \(k\),</span> but it has the right exponent and doesn’t have any gaps in my recollection of its construction.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/110580932664089904">Discuss on Mastodon</a>)</p><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-20T23:49:00Z">Tuesday, June 20 2023, 23:49</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://rjlipton.wpcomstaging.com/2023/06/20/computer-science-marches-on/'>Computer Science Marches On</a></h3>
        <p class='tr-article-feed'>from <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          With a note on the death of someone who tried to stop it Arnold the Allosaurus is moving to new digs. All during my time at Princeton, he held sway in cavernous Guyot Hall, the home of several earth and environmental science departments. Thanks to a huge gift from alumnus Eric Schmidt and his wife [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>
<font color="#0044cc"><br />
<em>With a note on the death of someone who tried to stop it</em><br />
<font color="#000000"></p>
<p>
Arnold the Allosaurus is moving to new digs. All during my time at Princeton, he held sway in cavernous Guyot Hall, the home of several earth and environmental science departments. Thanks to a huge gift from alumnus Eric Schmidt and his wife Wendy, Guyot Hall will be <a href="https://engineering.princeton.edu/news/2019/05/29/gift-eric-and-wendy-schmidt-create-new-home-computer-science-princeton-university">rebuilt and expanded</a> as the new home for the Department of Computer Science and several affiliated centers.</p>
<p>
&nbsp;</p>
<p><P></p>
<table style="margin:auto;">
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2023/06/20/computer-science-marches-on/allosauruscropped/" rel="attachment wp-att-21792"><img data-attachment-id="21792" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/20/computer-science-marches-on/allosauruscropped/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/AllosaurusCropped.jpg?fit=736%2C625&amp;ssl=1" data-orig-size="736,625" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="AllosaurusCropped" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/AllosaurusCropped.jpg?fit=300%2C255&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/AllosaurusCropped.jpg?fit=600%2C510&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/AllosaurusCropped.jpg?resize=368%2C312&#038;ssl=1" alt="" width="368" height="312" class="aligncenter wp-image-21792" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/AllosaurusCropped.jpg?w=736&amp;ssl=1 736w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/AllosaurusCropped.jpg?resize=300%2C255&amp;ssl=1 300w" sizes="(max-width: 368px) 100vw, 368px" data-recalc-dims="1" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><FONT size="-2">Tongue-in-cheek <a href="https://www.dailyprincetonian.com/article/2023/05/princeton-guyot-hall-dinosaur-to-switch-major-to-cos">source</a></FONT>
</td>
</tr>
</table>
<p>
Arnold will travel to the new Environmental Studies and Schools of Engineering and Applied Sciences <a href="https://facilities.princeton.edu/projects/es-seas">complex</a> being built along Ivy Lane. The Guyot name will travel with him.</p>
<p>
<span id="more-21790"></span></p>
<p><H2> The Old New Building </H2></p>
<p><p>
I remember when the current Computer Science building was new. It was built in 1989 after I arrived. One striking fact in retrospect is that it isn&#8217;t named for anyone. Its name is the &#8220;Computer Science Building.&#8221; Another fact is that it has our favorite open problem encoded into its brickwork:</p>
<p>
&nbsp;</p>
<p><P></p>
<table style="margin:auto;">
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2023/06/20/computer-science-marches-on/cs_bricks/" rel="attachment wp-att-21793"><img data-attachment-id="21793" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/20/computer-science-marches-on/cs_bricks/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/CS_bricks.jpg?fit=706%2C179&amp;ssl=1" data-orig-size="706,179" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="CS_bricks" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/CS_bricks.jpg?fit=300%2C76&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/CS_bricks.jpg?fit=600%2C152&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/CS_bricks.jpg?resize=530%2C135&#038;ssl=1" alt="" width="530" height="135" class="aligncenter wp-image-21793" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/CS_bricks.jpg?w=706&amp;ssl=1 706w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/CS_bricks.jpg?resize=300%2C76&amp;ssl=1 300w" sizes="(max-width: 530px) 100vw, 530px" data-recalc-dims="1" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><FONT size="-2">Composite crop of <a href="https://www.cs.princeton.edu/news/article/princeton-cs-30-years-and-still-roll">src1</a>, <a href="https://www.cs.princeton.edu/general/bricks">src2</a></FONT>
</td>
</tr>
</table>
<p>
At the dedication, Bob Sedgewick <a href="https://www.cs.princeton.edu/news/article/princeton-cs-30-years-and-still-roll">called</a> this &#8220;a gargoyle for the 1990s.&#8221; A <a href="https://theprince.princeton.edu/princetonperiodicals/cgi-bin/princetonperiodicals?a=d&#038;d=WeeklyBulletin19891113-01.2.4&#038;srpos=2&#038;e=01-09-1989-01-01-1990--en-20--1--txt-txIN-computer+science+building------">story</a> at that time quoted that as a &#8220;20th century gargoyle.&#8221; Well, we are almost a quarter way into the 21st century and no resolution is in sight. Will the problem outlive the building?&#8212;it will almost certainly outlive CS occupancy of the building.</p>
<p>
Before 1985, we were part of the Electrical Engineering and Computer Science and occupied one floor of one wing of the Engineering Quadrangle. That&#8217;s all Ken remembers from his time in 1977&#8211;81 as an undergrad. He just went back for an off-year reunion and marveled at vast amount of construction in progress all over the campus. The 1989 building was outgrown by the field itself: right now CS-affiliated faculty and staff are housed in nine locations and 25&#37; of undergraduates choose the CS major.</p>
<p>
<p><H2> Other Gifts </H2></p>
<p><p>
Large gifts for computing have been made all around the country at an increasing pace. At Georgia Tech I was among the first occupants of the <a href="https://en.wikipedia.org/wiki/Klaus_Advanced_Computing_Building">Christopher W. Klaus Advanced Computing Building</a>. The building was announced in 2000 and finished in 2006. </p>
<p>
Some gifts are from people I know well. Mike Fischer and his wife Alice recently <a href="https://www.newhaven.edu/news/releases/2022/alice-fischer-gift.php">gave</a> &#36;2 million to the Computer Science Endowed Fund at the University of New Haven. Alice started Computer Science as a program at New Haven while Mike has been at Yale all the same time. Here they are looking like when I knew them from my own time at Yale:</p>
<p><P><br />
<a href="https://rjlipton.wpcomstaging.com/2023/06/20/computer-science-marches-on/mikeandalicefischer/" rel="attachment wp-att-21794"><img data-attachment-id="21794" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/20/computer-science-marches-on/mikeandalicefischer/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/MikeAndAliceFischer.png?fit=335%2C188&amp;ssl=1" data-orig-size="335,188" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="MikeAndAliceFischer" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/MikeAndAliceFischer.png?fit=300%2C168&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/MikeAndAliceFischer.png?fit=335%2C188&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/MikeAndAliceFischer.png?resize=335%2C188&#038;ssl=1" alt="" width="335" height="188" class="aligncenter size-full wp-image-21794" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/MikeAndAliceFischer.png?w=335&amp;ssl=1 335w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/MikeAndAliceFischer.png?resize=300%2C168&amp;ssl=1 300w" sizes="(max-width: 335px) 100vw, 335px" data-recalc-dims="1" /></a></p>
<p><P><br />
Mike&#8217;s late brother Patrick was at Penn State and then Vanderbilt when I knew him best. Vanderbilt in 1998 <a href="https://www.nytimes.com/1998/12/01/us/vanderbilt-u-receives-a-gift-of-300-million.html">received</a> perhaps the largest university gift ever in real dollar terms. It was from the family that owns Ingram Micro but was not specifically for computing. </p>
<p>
<p><H2> Coda </H2></p>
<p><p>
I say this because Patrick was a target of someone who tried to stop the progress of computing. His secretary opened the package addressed to him and was severely injured, needing hospitalization for three weeks. Patrick later speculated that he was targeted because he &#8220;went from pure math to theoretical computer science.&#8221; </p>
<p><P></p>
<table style="margin:auto;">
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2023/06/20/computer-science-marches-on/pf-2/" rel="attachment wp-att-21795"><img data-attachment-id="21795" data-permalink="https://rjlipton.wpcomstaging.com/2023/06/20/computer-science-marches-on/pf-2/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/pf.jpeg?fit=188%2C268&amp;ssl=1" data-orig-size="188,268" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pf" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/pf.jpeg?fit=188%2C268&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/pf.jpeg?fit=188%2C268&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/pf.jpeg?resize=188%2C268&#038;ssl=1" alt="" width="188" height="268" class="aligncenter size-full wp-image-21795" data-recalc-dims="1" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><FONT size="-2">Vanderbilt memorial <a href="https://news.vanderbilt.edu/2011/08/26/patrick-fischer-former-engineering-chair-dies/">source</a></FONT>
</td>
</tr>
</table>
<p><P><br />
Other targets were more and less fortunate. Three were killed. David Gelernter opened the package himself in the Yale Computer Science mail room and was left with permanent damage to his right eye and the loss of four fingers of his right hand. The sender of those bombs died by his own hand in prison ten days ago. We will not say any more about him.</p>
<p>
<p><H2> Open Problems </H2></p>
<p><p>
One of William Shakespeare&#8217;s best known lines is, &#8220;Not marble nor the gilded monuments of princes shall outlive this powerful rhyme.&#8221; Will the new buildings and endowments in computer science outlive &#8220;P=NP?&#8221;</p>
<p>
<p class="authors">By RJLipton+KWRegan</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-20T18:12:18Z">Tuesday, June 20 2023, 18:12</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Monday, June 19
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.09542'>Finite state verifiers with both private and public coins</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: M. Utkan Gezer, A. C. Cem Say</p><p>We consider the effects of allowing a finite state verifier in an interactive
proof system to use a bounded number of private coins, in addition to "public"
coins whose outcomes are visible to the prover. Although swapping between
private and public-coin machines does not change the class of verifiable
languages when the verifiers are given reasonably large time and space bounds,
this distinction has well known effects for the capabilities of constant space
verifiers. We show that a constant private-coin "budget" (independent of the
length of the input) increases the power of public-coin interactive proofs with
finite state verifiers considerably, and provide a new characterization of the
complexity class $\rm P$ as the set of languages that are verifiable by such
machines with arbitrarily small error in expected polynomial time.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gezer_M/0/1/0/all/0/1">M. Utkan Gezer</a>, <a href="http://arxiv.org/find/cs/1/au:+Say_A/0/1/0/all/0/1">A. C. Cem Say</a></p><p>We consider the effects of allowing a finite state verifier in an interactive
proof system to use a bounded number of private coins, in addition to "public"
coins whose outcomes are visible to the prover. Although swapping between
private and public-coin machines does not change the class of verifiable
languages when the verifiers are given reasonably large time and space bounds,
this distinction has well known effects for the capabilities of constant space
verifiers. We show that a constant private-coin "budget" (independent of the
length of the input) increases the power of public-coin interactive proofs with
finite state verifiers considerably, and provide a new characterization of the
complexity class $\rm P$ as the set of languages that are verifiable by such
machines with arbitrarily small error in expected polynomial time.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-19T00:30:00Z">Monday, June 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.09550'>Minimizing an Uncrossed Collection of Drawings</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Petr Hlin&#x11b;n&#xfd;, Tom&#xe1;&#x161; Masa&#x159;&#xed;k</p><p>In this paper, we introduce the following new concept in graph drawing. Our
task is to find a small collection of drawings such that they all together
satisfy some property that is useful for graph visualization. We propose
investigating a property where each edge is not crossed in at least one drawing
in the collection. We call such collection uncrossed. Such property is
motivated by a quintessential problem of the crossing number, where one asks
for a plane drawing where the number of edge crossings is minimum. Indeed, if
we are allowed to visualize only one drawing, then the one which minimizes the
number of crossings is probably the neatest for the first orientation. However,
a collection of drawings where each highlights a different aspect of a graph
without any crossings could shed even more light on the graph's structure.
</p>
<p>We propose two definitions. First, the uncrossed number, minimizes the number
of graph drawings in a collection, satisfying the uncrossed property. Second,
the uncrossed crossing number, minimizes the total number of crossings in the
collection that satisfy the uncrossed property. For both definitions, we
establish initial results. We prove that the uncrossed crossing number is
NP-hard, but there is an FPT algorithm parameterized by the solution size.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Hlineny_P/0/1/0/all/0/1">Petr Hlin&#x11b;n&#xfd;</a>, <a href="http://arxiv.org/find/cs/1/au:+Masarik_T/0/1/0/all/0/1">Tom&#xe1;&#x161; Masa&#x159;&#xed;k</a></p><p>In this paper, we introduce the following new concept in graph drawing. Our
task is to find a small collection of drawings such that they all together
satisfy some property that is useful for graph visualization. We propose
investigating a property where each edge is not crossed in at least one drawing
in the collection. We call such collection uncrossed. Such property is
motivated by a quintessential problem of the crossing number, where one asks
for a plane drawing where the number of edge crossings is minimum. Indeed, if
we are allowed to visualize only one drawing, then the one which minimizes the
number of crossings is probably the neatest for the first orientation. However,
a collection of drawings where each highlights a different aspect of a graph
without any crossings could shed even more light on the graph's structure.
</p>
<p>We propose two definitions. First, the uncrossed number, minimizes the number
of graph drawings in a collection, satisfying the uncrossed property. Second,
the uncrossed crossing number, minimizes the total number of crossings in the
collection that satisfy the uncrossed property. For both definitions, we
establish initial results. We prove that the uncrossed crossing number is
NP-hard, but there is an FPT algorithm parameterized by the solution size.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-19T00:30:00Z">Monday, June 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.09870'>An Efficient Algorithm for Power Dominating Set</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Thomas Bl&#xe4;sius, Max G&#xf6;ttlicher</p><p>The problem Power Dominating Set (PDS) is motivated by the placement of
phasor measurement units to monitor electrical networks. It asks for a minimum
set of vertices in a graph that observes all remaining vertices by exhaustively
applying two observation rules. Our contribution is twofold. First, we
determine the parameterized complexity of PDS by proving it is $W[P]$-complete
when parameterized with respect to the solution size. We note that it was only
known to be $W[2]$-hard before. Our second and main contribution is a new
algorithm for PDS that efficiently solves practical instances.
</p>
<p>Our algorithm consists of two complementary parts. The first is a set of
reduction rules for PDS that can also be used in conjunction with previously
existing algorithms. The second is an algorithm for solving the remaining
kernel based on the implicit hitting set approach. Our evaluation on a set of
power grid instances from the literature shows that our solver outperforms
previous state-of-the-art solvers for PDS by more than one order of magnitude
on average. Furthermore, our algorithm can solve previously unsolved instances
of continental scale within a few minutes.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Blasius_T/0/1/0/all/0/1">Thomas Bl&#xe4;sius</a>, <a href="http://arxiv.org/find/cs/1/au:+Gottlicher_M/0/1/0/all/0/1">Max G&#xf6;ttlicher</a></p><p>The problem Power Dominating Set (PDS) is motivated by the placement of
phasor measurement units to monitor electrical networks. It asks for a minimum
set of vertices in a graph that observes all remaining vertices by exhaustively
applying two observation rules. Our contribution is twofold. First, we
determine the parameterized complexity of PDS by proving it is $W[P]$-complete
when parameterized with respect to the solution size. We note that it was only
known to be $W[2]$-hard before. Our second and main contribution is a new
algorithm for PDS that efficiently solves practical instances.
</p>
<p>Our algorithm consists of two complementary parts. The first is a set of
reduction rules for PDS that can also be used in conjunction with previously
existing algorithms. The second is an algorithm for solving the remaining
kernel based on the implicit hitting set approach. Our evaluation on a set of
power grid instances from the literature shows that our solver outperforms
previous state-of-the-art solvers for PDS by more than one order of magnitude
on average. Furthermore, our algorithm can solve previously unsolved instances
of continental scale within a few minutes.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-19T00:30:00Z">Monday, June 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.09396'>Private Federated Frequency Estimation: Adapting to the Hardness of the Instance</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jingfeng Wu, Wennan Zhu, Peter Kairouz, Vladimir Braverman</p><p>In federated frequency estimation (FFE), multiple clients work together to
estimate the frequencies of their collective data by communicating with a
server that respects the privacy constraints of Secure Summation (SecSum), a
cryptographic multi-party computation protocol that ensures that the server can
only access the sum of client-held vectors. For single-round FFE, it is known
that count sketching is nearly information-theoretically optimal for achieving
the fundamental accuracy-communication trade-offs [Chen et al., 2022]. However,
we show that under the more practical multi-round FEE setting, simple
adaptations of count sketching are strictly sub-optimal, and we propose a novel
hybrid sketching algorithm that is provably more accurate. We also address the
following fundamental question: how should a practitioner set the sketch size
in a way that adapts to the hardness of the underlying problem? We propose a
two-phase approach that allows for the use of a smaller sketch size for simpler
problems (e.g. near-sparse or light-tailed distributions). We conclude our work
by showing how differential privacy can be added to our algorithm and verifying
its superior performance through extensive experiments conducted on large-scale
datasets.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jingfeng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wennan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1">Peter Kairouz</a>, <a href="http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1">Vladimir Braverman</a></p><p>In federated frequency estimation (FFE), multiple clients work together to
estimate the frequencies of their collective data by communicating with a
server that respects the privacy constraints of Secure Summation (SecSum), a
cryptographic multi-party computation protocol that ensures that the server can
only access the sum of client-held vectors. For single-round FFE, it is known
that count sketching is nearly information-theoretically optimal for achieving
the fundamental accuracy-communication trade-offs [Chen et al., 2022]. However,
we show that under the more practical multi-round FEE setting, simple
adaptations of count sketching are strictly sub-optimal, and we propose a novel
hybrid sketching algorithm that is provably more accurate. We also address the
following fundamental question: how should a practitioner set the sketch size
in a way that adapts to the hardness of the underlying problem? We propose a
two-phase approach that allows for the use of a smaller sketch size for simpler
problems (e.g. near-sparse or light-tailed distributions). We conclude our work
by showing how differential privacy can be added to our algorithm and verifying
its superior performance through extensive experiments conducted on large-scale
datasets.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-19T00:30:00Z">Monday, June 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.09666'>A Smooth Binary Mechanism for Efficient Private Continual Observation</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Joel Daniel Andersson, Rasmus Pagh</p><p>In privacy under continual observation we study how to release differentially
private estimates based on a dataset that evolves over time. The problem of
releasing private prefix sums of $x_1,x_2,x_3,\dots \in\{0,1\}$ (where the
value of each $x_i$ is to be private) is particularly well-studied, and a
generalized form is used in state-of-the-art methods for private stochastic
gradient descent (SGD). The seminal binary mechanism privately releases the
first $t$ prefix sums with noise of variance polylogarithmic in $t$. Recently,
Henzinger et al. and Denisov et al. showed that it is possible to improve on
the binary mechanism in two ways: The variance of the noise can be reduced by a
(large) constant factor, and also made more even across time steps. However,
their algorithms for generating the noise distribution are not as efficient as
one would like in terms of computation time and (in particular) space. We
address the efficiency problem by presenting a simple alternative to the binary
mechanism in which 1) generating the noise takes constant average time per
value, 2) the variance is reduced by a factor about 4 compared to the binary
mechanism, and 3) the noise distribution at each step is identical.
Empirically, a simple Python implementation of our approach outperforms the
running time of the approach of Henzinger et al., as well as an attempt to
improve their algorithm using high-performance algorithms for multiplication
with Toeplitz matrices.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Andersson_J/0/1/0/all/0/1">Joel Daniel Andersson</a>, <a href="http://arxiv.org/find/cs/1/au:+Pagh_R/0/1/0/all/0/1">Rasmus Pagh</a></p><p>In privacy under continual observation we study how to release differentially
private estimates based on a dataset that evolves over time. The problem of
releasing private prefix sums of $x_1,x_2,x_3,\dots \in\{0,1\}$ (where the
value of each $x_i$ is to be private) is particularly well-studied, and a
generalized form is used in state-of-the-art methods for private stochastic
gradient descent (SGD). The seminal binary mechanism privately releases the
first $t$ prefix sums with noise of variance polylogarithmic in $t$. Recently,
Henzinger et al. and Denisov et al. showed that it is possible to improve on
the binary mechanism in two ways: The variance of the noise can be reduced by a
(large) constant factor, and also made more even across time steps. However,
their algorithms for generating the noise distribution are not as efficient as
one would like in terms of computation time and (in particular) space. We
address the efficiency problem by presenting a simple alternative to the binary
mechanism in which 1) generating the noise takes constant average time per
value, 2) the variance is reduced by a factor about 4 compared to the binary
mechanism, and 3) the noise distribution at each step is identical.
Empirically, a simple Python implementation of our approach outperforms the
running time of the approach of Henzinger et al., as well as an attempt to
improve their algorithm using high-performance algorithms for multiplication
with Toeplitz matrices.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-19T00:30:00Z">Monday, June 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2306.09783'>MementoHash: A Stateful, Minimal Memory, Best Performing Consistent Hash Algorithm</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Massimo Coluzzi, Amos Brocco, Alessandro Antonucci, Tiziano Leidi</p><p>Consistent hashing is used in distributed systems and networking applications
to spread data evenly and efficiently across a cluster of nodes. In this paper,
we present MementoHash, a novel consistent hashing algorithm that eliminates
known limitations of state-of-the-art algorithms while keeping optimal
performance and minimal memory usage. We describe the algorithm in detail,
provide a pseudo-code implementation, and formally establish its solid
theoretical guarantees. To measure the efficacy of MementoHash, we compare its
performance, in terms of memory usage and lookup time, to that of
state-of-the-art algorithms, namely, AnchorHash, DxHash, and JumpHash. Unlike
JumpHash, MementoHash can handle random failures. Moreover, MementoHash does
not require fixing the overall capacity of the cluster (as AnchorHash and
DxHash do), allowing it to scale indefinitely. The number of removed nodes
affects the performance of all the considered algorithms. Therefore, we conduct
experiments considering three different scenarios: stable (no removed nodes),
one-shot removals (90% of the nodes removed at once), and incremental removals.
We report experimental results that averaged a varying number of nodes from ten
to one million. Results indicate that our algorithm shows optimal lookup
performance and minimal memory usage in its best-case scenario. It behaves
better than AnchorHash and DxHash in its average-case scenario and at least as
well as those two algorithms in its worst-case scenario. However, the
worst-case scenario for MementoHash occurs when more than 70% of the nodes
fail, which describes a unlikely scenario. Therefore, MementoHash shows the
best performance during the regular life cycle of a cluster.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Coluzzi_M/0/1/0/all/0/1">Massimo Coluzzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Brocco_A/0/1/0/all/0/1">Amos Brocco</a>, <a href="http://arxiv.org/find/cs/1/au:+Antonucci_A/0/1/0/all/0/1">Alessandro Antonucci</a>, <a href="http://arxiv.org/find/cs/1/au:+Leidi_T/0/1/0/all/0/1">Tiziano Leidi</a></p><p>Consistent hashing is used in distributed systems and networking applications
to spread data evenly and efficiently across a cluster of nodes. In this paper,
we present MementoHash, a novel consistent hashing algorithm that eliminates
known limitations of state-of-the-art algorithms while keeping optimal
performance and minimal memory usage. We describe the algorithm in detail,
provide a pseudo-code implementation, and formally establish its solid
theoretical guarantees. To measure the efficacy of MementoHash, we compare its
performance, in terms of memory usage and lookup time, to that of
state-of-the-art algorithms, namely, AnchorHash, DxHash, and JumpHash. Unlike
JumpHash, MementoHash can handle random failures. Moreover, MementoHash does
not require fixing the overall capacity of the cluster (as AnchorHash and
DxHash do), allowing it to scale indefinitely. The number of removed nodes
affects the performance of all the considered algorithms. Therefore, we conduct
experiments considering three different scenarios: stable (no removed nodes),
one-shot removals (90% of the nodes removed at once), and incremental removals.
We report experimental results that averaged a varying number of nodes from ten
to one million. Results indicate that our algorithm shows optimal lookup
performance and minimal memory usage in its best-case scenario. It behaves
better than AnchorHash and DxHash in its average-case scenario and at least as
well as those two algorithms in its worst-case scenario. However, the
worst-case scenario for MementoHash occurs when more than 70% of the nodes
fail, which describes a unlikely scenario. Therefore, MementoHash shows the
best performance during the regular life cycle of a cluster.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-06-19T00:30:00Z">Monday, June 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
