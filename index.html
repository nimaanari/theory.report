<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.6 (2023-03-30) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2023-05-19T05:30:26Z">Friday, May 19 2023, 05:30</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://minorfree.github.io/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://minorfree.github.io'>Hung Le</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>Sébastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Friday, May 19
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.10749'>On the Computational Complexity of Generalized Common Shape Puzzles</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mutsunori Banbara, Shin-ichi Minato, Hirotaka Ono, Ryuhei Uehara</p><p>In this study, we investigate the computational complexity of some variants
of generalized puzzles. We are provided with two sets S_1 and S_2 of
polyominoes. The first puzzle asks us to form the same shape using polyominoes
in S_1 and S_2. We demonstrate that this is polynomial-time solvable if S_1 and
S_2 have constant numbers of polyominoes, and it is strongly NP-complete in
general. The second puzzle allows us to make copies of the pieces in S_1 and
S_2. That is, a polyomino in S_1 can be used multiple times to form a shape.
This is a generalized version of the classical puzzle known as the common
multiple shape puzzle. For two polyominoes P and Q, the common multiple shape
is a shape that can be formed by many copies of P and many copies of Q. We show
that the second puzzle is undecidable in general. The undecidability is
demonstrated by a reduction from a new type of undecidable puzzle based on
tiling. Nevertheless, certain concrete instances of the common multiple shape
can be solved in a practical time. We present a method for determining the
common multiple shape for provided tuples of polyominoes and outline concrete
results, which improve on the previously known results in puzzle society.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Banbara_M/0/1/0/all/0/1">Mutsunori Banbara</a>, <a href="http://arxiv.org/find/cs/1/au:+Minato_S/0/1/0/all/0/1">Shin-ichi Minato</a>, <a href="http://arxiv.org/find/cs/1/au:+Ono_H/0/1/0/all/0/1">Hirotaka Ono</a>, <a href="http://arxiv.org/find/cs/1/au:+Uehara_R/0/1/0/all/0/1">Ryuhei Uehara</a></p><p>In this study, we investigate the computational complexity of some variants
of generalized puzzles. We are provided with two sets S_1 and S_2 of
polyominoes. The first puzzle asks us to form the same shape using polyominoes
in S_1 and S_2. We demonstrate that this is polynomial-time solvable if S_1 and
S_2 have constant numbers of polyominoes, and it is strongly NP-complete in
general. The second puzzle allows us to make copies of the pieces in S_1 and
S_2. That is, a polyomino in S_1 can be used multiple times to form a shape.
This is a generalized version of the classical puzzle known as the common
multiple shape puzzle. For two polyominoes P and Q, the common multiple shape
is a shape that can be formed by many copies of P and many copies of Q. We show
that the second puzzle is undecidable in general. The undecidability is
demonstrated by a reduction from a new type of undecidable puzzle based on
tiling. Nevertheless, certain concrete instances of the common multiple shape
can be solved in a practical time. We present a method for determining the
common multiple shape for provided tuples of polyominoes and outline concrete
results, which improve on the previously known results in puzzle society.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-19T00:30:00Z">Friday, May 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.10922'>On $k$-means for segments and polylines</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sergio Cabello, Panos Giannopoulos</p><p>We study the problem of $k$-means clustering in the space of straight-line
segments in $\mathbb{R}^{2}$ under the Hausdorff distance. For this problem, we
give a $(1+\epsilon)$-approximation algorithm that, for an input of $n$
segments, for any fixed $k$, and with constant success probability, runs in
time $O(n+ \epsilon^{-O(k)} + \epsilon^{-O(k)}\cdot \log^{O(k)}
(\epsilon^{-1}))$. The algorithm has two main ingredients. Firstly, we express
the $k$-means objective in our metric space as a sum of algebraic functions and
use the optimization technique of Vigneron~\cite{Vigneron14} to approximate its
minimum. Secondly, we reduce the input size by computing a small size coreset
using the sensitivity-based sampling framework by Feldman and
Langberg~\cite{Feldman11, Feldman2020}. Our results can be extended to
polylines of constant complexity with a running time of $O(n+
\epsilon^{-O(k)})$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cabello_S/0/1/0/all/0/1">Sergio Cabello</a>, <a href="http://arxiv.org/find/cs/1/au:+Giannopoulos_P/0/1/0/all/0/1">Panos Giannopoulos</a></p><p>We study the problem of $k$-means clustering in the space of straight-line
segments in $\mathbb{R}^{2}$ under the Hausdorff distance. For this problem, we
give a $(1+\epsilon)$-approximation algorithm that, for an input of $n$
segments, for any fixed $k$, and with constant success probability, runs in
time $O(n+ \epsilon^{-O(k)} + \epsilon^{-O(k)}\cdot \log^{O(k)}
(\epsilon^{-1}))$. The algorithm has two main ingredients. Firstly, we express
the $k$-means objective in our metric space as a sum of algebraic functions and
use the optimization technique of Vigneron~\cite{Vigneron14} to approximate its
minimum. Secondly, we reduce the input size by computing a small size coreset
using the sensitivity-based sampling framework by Feldman and
Langberg~\cite{Feldman11, Feldman2020}. Our results can be extended to
polylines of constant complexity with a running time of $O(n+
\epsilon^{-O(k)})$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-19T00:30:00Z">Friday, May 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.10575'>The Complexity of Diagonalization</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Nikhil Srivastava</p><p>We survey recent progress on efficient algorithms for approximately
diagonalizing a square complex matrix in the models of rational (variable
precision) and finite (floating point) arithmetic. This question has been
studied across several research communities for decades, but many mysteries
remain. We present several open problems which we hope will be of broad
interest.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Srivastava_N/0/1/0/all/0/1">Nikhil Srivastava</a></p><p>We survey recent progress on efficient algorithms for approximately
diagonalizing a square complex matrix in the models of rational (variable
precision) and finite (floating point) arithmetic. This question has been
studied across several research communities for decades, but many mysteries
remain. We present several open problems which we hope will be of broad
interest.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-19T00:30:00Z">Friday, May 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.11053'>(Noisy) Gap Cycle Counting Strikes Back: Random Order Streaming Lower Bounds for Connected Components and Beyond</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sepehr Assadi, Janani Sundaresan</p><p>We continue the study of the communication complexity of gap cycle counting
problems. These problems have been introduced by Verbin and Yu [SODA 2011] and
have found numerous applications in proving streaming lower bounds. In the
noisy gap cycle counting problem (NGC), there is a small integer $k \geq 1$ and
an $n$-vertex graph consisted of vertex-disjoint union of either $k$-cycles or
$2k$-cycles, plus $O(n/k)$ disjoint paths of length $k-1$ in both cases
(``noise''). The edges of this graph are partitioned between Alice and Bob
whose goal is to decide which case the graph belongs to with minimal
communication from Alice to Bob.
</p>
<p>We study the robust communication complexity -- `a la Chakrabarti, Cormode,
and McGregor [STOC 2008] -- of NGC, namely, when edges are partitioned randomly
between the players. This is in contrast to all prior work on gap cycle
counting problems in adversarial partitions. While NGC can be solved trivially
with zero communication when $k &lt; \log{n}$, we prove that when $k$ is a
constant factor larger than $\log{n}$, the robust (one-way) communication
complexity of NGC is $\Omega(n)$ bits.
</p>
<p>As a corollary of this result, we can prove several new graph streaming lower
bounds for random order streams. In particular, we show that any streaming
algorithm that for every $\varepsilon &gt; 0$ estimates the number of connected
components of a graph presented in a random order stream to within an
$\varepsilon \cdot n$ additive factor requires $2^{\Omega(1/\varepsilon)}$
space, settling a conjecture of Peng and Sohler [SODA 2018]. We further discuss
new implications of our lower bounds to other problems such as estimating size
of maximum matchings and independent sets on planar graphs, random walks, as
well as to stochastic streams.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Assadi_S/0/1/0/all/0/1">Sepehr Assadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sundaresan_J/0/1/0/all/0/1">Janani Sundaresan</a></p><p>We continue the study of the communication complexity of gap cycle counting
problems. These problems have been introduced by Verbin and Yu [SODA 2011] and
have found numerous applications in proving streaming lower bounds. In the
noisy gap cycle counting problem (NGC), there is a small integer $k \geq 1$ and
an $n$-vertex graph consisted of vertex-disjoint union of either $k$-cycles or
$2k$-cycles, plus $O(n/k)$ disjoint paths of length $k-1$ in both cases
(``noise''). The edges of this graph are partitioned between Alice and Bob
whose goal is to decide which case the graph belongs to with minimal
communication from Alice to Bob.
</p>
<p>We study the robust communication complexity -- `a la Chakrabarti, Cormode,
and McGregor [STOC 2008] -- of NGC, namely, when edges are partitioned randomly
between the players. This is in contrast to all prior work on gap cycle
counting problems in adversarial partitions. While NGC can be solved trivially
with zero communication when $k &lt; \log{n}$, we prove that when $k$ is a
constant factor larger than $\log{n}$, the robust (one-way) communication
complexity of NGC is $\Omega(n)$ bits.
</p>
<p>As a corollary of this result, we can prove several new graph streaming lower
bounds for random order streams. In particular, we show that any streaming
algorithm that for every $\varepsilon &gt; 0$ estimates the number of connected
components of a graph presented in a random order stream to within an
$\varepsilon \cdot n$ additive factor requires $2^{\Omega(1/\varepsilon)}$
space, settling a conjecture of Peng and Sohler [SODA 2018]. We further discuss
new implications of our lower bounds to other problems such as estimating size
of maximum matchings and independent sets on planar graphs, random walks, as
well as to stochastic streams.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-19T00:30:00Z">Friday, May 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.10536'>Online List Labeling with Predictions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Samuel McCauley, Benjamin Moseley, Aidin Niaparast, Shikha Singh</p><p>A growing line of work shows how learned predictions can be used to break
through worst-case barriers to improve the running time of an algorithm.
However, incorporating predictions into data structures with strong theoretical
guarantees remains underdeveloped. This paper takes a step in this direction by
showing that predictions can be leveraged in the fundamental online list
labeling problem. In the problem, n items arrive over time and must be stored
in sorted order in an array of size Theta(n). The array slot of an element is
its label and the goal is to maintain sorted order while minimizing the total
number of elements moved (i.e., relabeled). We design a new list labeling data
structure and bound its performance in two models. In the worst-case
learning-augmented model, we give guarantees in terms of the error in the
predictions. Our data structure provides strong guarantees: it is optimal for
any prediction error and guarantees the best-known worst-case bound even when
the predictions are entirely erroneous. We also consider a stochastic error
model and bound the performance in terms of the expectation and variance of the
error. Finally, the theoretical results are demonstrated empirically. In
particular, we show that our data structure has strong performance on real
temporal data sets where predictions are constructed from elements that arrived
in the past, as is typically done in a practical use case.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+McCauley_S/0/1/0/all/0/1">Samuel McCauley</a>, <a href="http://arxiv.org/find/cs/1/au:+Moseley_B/0/1/0/all/0/1">Benjamin Moseley</a>, <a href="http://arxiv.org/find/cs/1/au:+Niaparast_A/0/1/0/all/0/1">Aidin Niaparast</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Shikha Singh</a></p><p>A growing line of work shows how learned predictions can be used to break
through worst-case barriers to improve the running time of an algorithm.
However, incorporating predictions into data structures with strong theoretical
guarantees remains underdeveloped. This paper takes a step in this direction by
showing that predictions can be leveraged in the fundamental online list
labeling problem. In the problem, n items arrive over time and must be stored
in sorted order in an array of size Theta(n). The array slot of an element is
its label and the goal is to maintain sorted order while minimizing the total
number of elements moved (i.e., relabeled). We design a new list labeling data
structure and bound its performance in two models. In the worst-case
learning-augmented model, we give guarantees in terms of the error in the
predictions. Our data structure provides strong guarantees: it is optimal for
any prediction error and guarantees the best-known worst-case bound even when
the predictions are entirely erroneous. We also consider a stochastic error
model and bound the performance in terms of the expectation and variance of the
error. Finally, the theoretical results are demonstrated empirically. In
particular, we show that our data structure has strong performance on real
temporal data sets where predictions are constructed from elements that arrived
in the past, as is typically done in a practical use case.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-19T00:30:00Z">Friday, May 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.10618'>Fault-Tolerant Consensus in Quantum Networks</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: MohammadTaghi Hajiaghayi, Dariusz R. Kowalski, Jan Olkowski</p><p>Fault-tolerant consensus is about reaching agreement on some of the input
values in a limited time by non-faulty autonomous processes, despite of
failures of processes or communication medium. This problem is particularly
challenging and costly against an adaptive adversary with full information.
Bar-Joseph and Ben-Or (PODC'98) were the first who proved an absolute lower
bound $\Omega(\sqrt{n/\log n})$ on expected time complexity of consensus in any
classic (i.e., randomized or deterministic) message-passing network with $n$
processes succeeding with probability $1$ against such a strong adaptive
adversary crashing processes. Seminal work of Ben-Or and Hassidim (STOC'05)
broke the $\Omega(\sqrt{n/\log n})$ barrier for consensus in classic
(deterministic and randomized) networks by employing quantum computing. They
showed an (expected) constant-time quantum algorithm for a linear number of
crashes $t&lt;n/3$. In this paper, we improve upon that seminal work by reducing
the number of quantum and communication bits to an arbitrarily small
polynomial, and even more, to a polylogarithmic number -- though, the latter in
the cost of a slightly larger polylogarithmic time (still exponentially smaller
than the time lower bound $\Omega(\sqrt{n/\log n})$ for classic computation).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1">MohammadTaghi Hajiaghayi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kowalski_D/0/1/0/all/0/1">Dariusz R. Kowalski</a>, <a href="http://arxiv.org/find/cs/1/au:+Olkowski_J/0/1/0/all/0/1">Jan Olkowski</a></p><p>Fault-tolerant consensus is about reaching agreement on some of the input
values in a limited time by non-faulty autonomous processes, despite of
failures of processes or communication medium. This problem is particularly
challenging and costly against an adaptive adversary with full information.
Bar-Joseph and Ben-Or (PODC'98) were the first who proved an absolute lower
bound $\Omega(\sqrt{n/\log n})$ on expected time complexity of consensus in any
classic (i.e., randomized or deterministic) message-passing network with $n$
processes succeeding with probability $1$ against such a strong adaptive
adversary crashing processes. Seminal work of Ben-Or and Hassidim (STOC'05)
broke the $\Omega(\sqrt{n/\log n})$ barrier for consensus in classic
(deterministic and randomized) networks by employing quantum computing. They
showed an (expected) constant-time quantum algorithm for a linear number of
crashes $t&lt;n/3$. In this paper, we improve upon that seminal work by reducing
the number of quantum and communication bits to an arbitrarily small
polynomial, and even more, to a polylogarithmic number -- though, the latter in
the cost of a slightly larger polylogarithmic time (still exponentially smaller
than the time lower bound $\Omega(\sqrt{n/\log n})$ for classic computation).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-19T00:30:00Z">Friday, May 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.10744'>Online Resource Allocation in Episodic Markov Decision Processes</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Duksang Lee, Dabeen Lee</p><p>This paper studies a long-term resource allocation problem over multiple
periods where each period requires a multi-stage decision-making process. We
formulate the problem as an online resource allocation problem in an episodic
finite-horizon Markov decision process with unknown non-stationary transitions
and stochastic non-stationary reward and resource consumption functions for
each episode. We provide an equivalent online linear programming reformulation
based on occupancy measures, for which we develop an online mirror descent
algorithm. Our online dual mirror descent algorithm for resource allocation
deals with uncertainties and errors in estimating the true feasible set, which
is of independent interest. We prove that under stochastic reward and resource
consumption functions, the expected regret of the online mirror descent
algorithm is bounded by $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ where $\rho\in(0,1)$
is the budget parameter, $H$ is the length of the horizon, $S$ and $A$ are the
numbers of states and actions, and $T$ is the number of episodes.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Duksang Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Dabeen Lee</a></p><p>This paper studies a long-term resource allocation problem over multiple
periods where each period requires a multi-stage decision-making process. We
formulate the problem as an online resource allocation problem in an episodic
finite-horizon Markov decision process with unknown non-stationary transitions
and stochastic non-stationary reward and resource consumption functions for
each episode. We provide an equivalent online linear programming reformulation
based on occupancy measures, for which we develop an online mirror descent
algorithm. Our online dual mirror descent algorithm for resource allocation
deals with uncertainties and errors in estimating the true feasible set, which
is of independent interest. We prove that under stochastic reward and resource
consumption functions, the expected regret of the online mirror descent
algorithm is bounded by $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ where $\rho\in(0,1)$
is the budget parameter, $H$ is the length of the horizon, $S$ and $A$ are the
numbers of states and actions, and $T$ is the number of episodes.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-19T00:30:00Z">Friday, May 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.10935'>Submodularity Gaps for Selected Network Design and Matching Problems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Martin B&#xf6;hm, Jaros&#x142;aw Byrka, Mateusz Lewandowski, Jan Marcinkowski</p><p>Submodularity in combinatorial optimization has been a topic of many studies
and various algorithmic techniques exploiting submodularity of a studied
problem have been proposed. It is therefore natural to ask, in cases where the
cost function of the studied problem is not submodular, whether it is possible
to approximate this cost function with a proxy submodular function.
</p>
<p>We answer this question in the negative for two major problems in metric
optimization, namely Steiner Tree and Uncapacitated Facility Location. We do so
by proving super-constant lower bounds on the submodularity gap for these
problems, which are in contrast to the known constant factor cost sharing
schemes known for them. Technically, our lower bounds build on strong lower
bounds for the online variants of these two problems. Nevertheless, online
lower bounds do not always imply submodularity lower bounds. We show that the
problem Maximum Bipartite Matching does not exhibit any submodularity gap,
despite its online variant being only (1 - 1/e)-competitive in the randomized
setting.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bohm_M/0/1/0/all/0/1">Martin B&#xf6;hm</a>, <a href="http://arxiv.org/find/cs/1/au:+Byrka_J/0/1/0/all/0/1">Jaros&#x142;aw Byrka</a>, <a href="http://arxiv.org/find/cs/1/au:+Lewandowski_M/0/1/0/all/0/1">Mateusz Lewandowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Marcinkowski_J/0/1/0/all/0/1">Jan Marcinkowski</a></p><p>Submodularity in combinatorial optimization has been a topic of many studies
and various algorithmic techniques exploiting submodularity of a studied
problem have been proposed. It is therefore natural to ask, in cases where the
cost function of the studied problem is not submodular, whether it is possible
to approximate this cost function with a proxy submodular function.
</p>
<p>We answer this question in the negative for two major problems in metric
optimization, namely Steiner Tree and Uncapacitated Facility Location. We do so
by proving super-constant lower bounds on the submodularity gap for these
problems, which are in contrast to the known constant factor cost sharing
schemes known for them. Technically, our lower bounds build on strong lower
bounds for the online variants of these two problems. Nevertheless, online
lower bounds do not always imply submodularity lower bounds. We show that the
problem Maximum Bipartite Matching does not exhibit any submodularity gap,
despite its online variant being only (1 - 1/e)-competitive in the randomized
setting.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-19T00:30:00Z">Friday, May 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.11046'>Difference of Submodular Minimization via DC Programming</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Marwa El Halabi, George Orfanides, Tim Hoheisel</p><p>Minimizing the difference of two submodular (DS) functions is a problem that
naturally occurs in various machine learning problems. Although it is well
known that a DS problem can be equivalently formulated as the minimization of
the difference of two convex (DC) functions, existing algorithms do not fully
exploit this connection. A classical algorithm for DC problems is called the DC
algorithm (DCA). We introduce variants of DCA and its complete form (CDCA) that
we apply to the DC program corresponding to DS minimization. We extend existing
convergence properties of DCA, and connect them to convergence properties on
the DS problem. Our results on DCA match the theoretical guarantees satisfied
by existing DS algorithms, while providing a more complete characterization of
convergence properties. In the case of CDCA, we obtain a stronger local
minimality guarantee. Our numerical results show that our proposed algorithms
outperform existing baselines on two applications: speech corpus selection and
feature selection.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Halabi_M/0/1/0/all/0/1">Marwa El Halabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Orfanides_G/0/1/0/all/0/1">George Orfanides</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoheisel_T/0/1/0/all/0/1">Tim Hoheisel</a></p><p>Minimizing the difference of two submodular (DS) functions is a problem that
naturally occurs in various machine learning problems. Although it is well
known that a DS problem can be equivalently formulated as the minimization of
the difference of two convex (DC) functions, existing algorithms do not fully
exploit this connection. A classical algorithm for DC problems is called the DC
algorithm (DCA). We introduce variants of DCA and its complete form (CDCA) that
we apply to the DC program corresponding to DS minimization. We extend existing
convergence properties of DCA, and connect them to convergence properties on
the DS problem. Our results on DCA match the theoretical guarantees satisfied
by existing DS algorithms, while providing a more complete characterization of
convergence properties. In the case of CDCA, we obtain a stronger local
minimality guarantee. Our numerical results show that our proposed algorithms
outperform existing baselines on two applications: speech corpus selection and
feature selection.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-19T00:30:00Z">Friday, May 19 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Thursday, May 18
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://blog.computationalcomplexity.org/2023/05/community-guidelines-that-ignore-history.html'>Community Guidelines that Ignore History</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Last week I got the following email from Blogger:</p><blockquote><p>As you may know, our Community Guidelines (blogger.com/go/contentpolicy) describe the boundaries for what we allow-- and don't allow-- on Blogger. Your post titled "Mysteries of the Seventies" was flagged to us for review. This post was put behind a warning for readers because it contains sensitive content; the post is visible at&nbsp;blog.computationalcomplexity.org/2005/06/mysteries-of-seventies.html. Your blog readers must acknowledge the warning before being able to read the post/blog.</p></blockquote><p>So let me describe what happened carefully so this post doesn't also get flagged. First a history lesson. In 1972, five men were arrested for breaking into the Democratic National Committee (DNC) headquarters at the Watergate complex in Washington, D.C. They were found with bugging devices and cameras, attempting to wiretap the DNC offices. One of them had ties to&nbsp;the Committee to Re-Elect the President (CREEP), which supported Richard Nixon's re-election campaign.</p><p>Two Washington Post reporters,&nbsp;Bob Woodward and Carl Bernstein, broke the story of the coverup that would eventually lead to Nixon's resignation in 1974. I highly recommend both the book and the film All the President's Men&nbsp;about their investigation.&nbsp;Woodward and Bernstein got help from a then anonymous source. The identity of the source was subject to huge speculation and remained a mystery for three decades. In 2005&nbsp;Mark Felt, associate director of the FBI, outed himself as the source.</p><p>After the announcement I wrote a short post&nbsp;(would have been a tweet today) mentioning that while we learned the solution of one mystery from the 1970s, another remained.</p><p>So why was this post flagged as sensitive eighteen years later? Because I used the pseudonym the Washington Post reporters gave to Mark Felt, a name take from a popular adult movie at the time.&nbsp;</p><p>By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Last week I got the following email from Blogger:</p><blockquote><p>As you may know, our Community Guidelines (<a href="https://blogger.com/go/contentpolicy">https://blogger.com/go/contentpolicy</a>) describe the boundaries for what we allow-- and don't allow-- on Blogger. Your post titled "Mysteries of the Seventies" was flagged to us for review. This post was put behind a warning for readers because it contains sensitive content; the post is visible at&nbsp;<a href="http://blog.computationalcomplexity.org/2005/06/mysteries-of-seventies.html">http://blog.computationalcomplexity.org/2005/06/mysteries-of-seventies.html</a>. Your blog readers must acknowledge the warning before being able to read the post/blog.</p></blockquote><p>So let me describe what happened carefully so this post doesn't also get flagged. First a history lesson. In 1972, five men were arrested for breaking into the Democratic National Committee (DNC) headquarters at the Watergate complex in Washington, D.C. They were found with bugging devices and cameras, attempting to wiretap the DNC offices. One of them had ties to&nbsp;the Committee to Re-Elect the President (CREEP), which supported Richard Nixon's re-election campaign.</p><p>Two Washington Post reporters,&nbsp;Bob Woodward and Carl Bernstein, broke the story of the coverup that would eventually lead to Nixon's resignation in 1974. I highly recommend both the <a href="https://amzn.to/4561z5n">book</a> and the <a href="https://www.imdb.com/title/tt0074119/">film</a> <i>All the President's Men</i>&nbsp;about their investigation.<b style="font-style: italic;">&nbsp;</b>Woodward and Bernstein got help from a then anonymous source. The identity of the source was subject to huge speculation and remained a mystery for three decades. In 2005&nbsp;<a href="https://en.wikipedia.org/wiki/Mark_Felt">Mark Felt</a>, associate director of the FBI, outed himself as the source.</p><p>After the announcement I wrote <a href="https://blog.computationalcomplexity.org/2005/06/mysteries-of-seventies.html">a short post</a>&nbsp;(would have been a tweet today) mentioning that while we learned the solution of one mystery from the 1970s, <a href="https://www.claymath.org/millennium-problems/p-vs-np-problem">another</a> remained.</p><p>So why was this post flagged as sensitive eighteen years later? Because I used the pseudonym the Washington Post reporters gave to Mark Felt, a name take from a popular adult movie at the time.&nbsp;</p><p class="authors">By Lance Fortnow</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-18T12:54:00Z">Thursday, May 18 2023, 12:54</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2023/072'>TR23-072 |  Range Avoidance, Remote Point, and Hard Partial Truth Tables via Satisfying-Pairs Algorithms | 

	Yeyuan Chen, 

	Yizhi Huang, 

	Jiatu Li, 

	Hanlin Ren</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The *range avoidance problem*, denoted as $\mathcal{C}$-$\rm Avoid$, asks to find a non-output of a given $\mathcal{C}$-circuit $C:\{0,1\}^n\to\{0,1\}^\ell$ with stretch $\ell&gt;n$. This problem has recently received much attention in complexity theory for its connections with circuit lower bounds and other explicit construction problems. Inspired by the Algorithmic Method for circuit lower bounds, Ren, Santhanam, and Wang (FOCS&#39;22) established a framework to design $\rm FP^{NP}$ algorithms for $\mathcal{C}$-$\rm Avoid$ via *slightly non-trivial* data structures related to $\mathcal{C}$. However, a major drawback of their approach is the lack of unconditional results even for $\mathcal{C}={\rm AC}^0$.
	
In this work, we present the first unconditional $\rm FP^{NP}$ algorithm for ${\rm ACC}^0$-$\rm Avoid$. Indeed, we obtain $\rm FP^{NP}$ algorithms for the following stronger problems:
$\bullet$ (${\rm ACC}^0$-$\rm RemotePoint$). Given $C:\{0,1\}^n\to\{0,1\}^\ell$ for some $\ell={\rm quasipoly}(n)$ such that each output bit of $C$ is computed by a ${\rm quasipoly}(n)$-size ${\rm AC}^0[m]$ circuit, we can find some $y\in\{0,1\}^\ell$ in $\rm FP^{NP}$ such that for every $x\in\{0, 1\}^n$, the relative Hamming distance between $y$ and $C(x)$ is at least $1/2-1/{\rm poly}(n)$. This problem is the ``average-case&#39;&#39; analogue of ${\rm ACC}^0$-$\rm Avoid$.
$\bullet$ (${\rm ACC}^0$-$\rm AvgPartialHard$). Given $x_1,\dots,x_\ell\in\{0,1\}^n$ for some $\ell={\rm quasipoly}(n)$, we can compute $\ell$ bits $y_1,\dots,y_\ell\in\{0,1\}$ in $\rm FP^{NP}$ such that for every $2^{\log^c(n)}$-size ${\rm ACC}^0$ circuit $C$, $\Pr_i[C(x_i)\ne y_i]\ge 1/2-1/{\rm poly}(n)$, where $c=O(1)$. This problem generalises the strong average-case circuit lower bounds against ${\rm ACC}^0$ in a different way. 
Our algorithms can be seen as natural generalisations of the best known almost-everywhere average-case lower bounds against ${\rm ACC}^0$ circuits by Chen, Lyu, and Williams (FOCS&#39;20). Note that both problems above have been studied prior to our work, and no $\rm FP^{NP}$ algorithm was known even for weak circuit classes such as ${\rm GF}(2)$-linear circuits and DNF formulas. 

Our results follow from a strengthened algorithmic method: slightly non-trivial algorithms for the *Satisfying-Pairs* problem for $\mathcal{C}$ implies $\rm FP^{NP}$ algorithms for $\mathcal{C}$-$\rm Avoid$ (as well as $\mathcal{C}$-$\rm RemotePoint$ and $\mathcal{C}$-$\rm AvgPartialHard$). Here, given $\mathcal{C}$-circuits $\{C_i\}$ and inputs $\{x_j\}$, the $\mathcal{C}$-Satisfying-Pairs problem asks to (approximately) count the number of pairs $(i,j)$ such that $C_i(x_j)=1$.
	
A technical contribution of this work is a construction of a *short, smooth, and rectangular PCP of Proximity* that combines two previous PCP constructions, which may be of independent interest. It serves as a key tool that allows us to generalise the framework for $\rm Avoid$ to the average-case scenarios.
        
        </div>

        <div class='tr-article-summary'>
        
          
          The *range avoidance problem*, denoted as $\mathcal{C}$-$\rm Avoid$, asks to find a non-output of a given $\mathcal{C}$-circuit $C:\{0,1\}^n\to\{0,1\}^\ell$ with stretch $\ell&gt;n$. This problem has recently received much attention in complexity theory for its connections with circuit lower bounds and other explicit construction problems. Inspired by the Algorithmic Method for circuit lower bounds, Ren, Santhanam, and Wang (FOCS&#39;22) established a framework to design $\rm FP^{NP}$ algorithms for $\mathcal{C}$-$\rm Avoid$ via *slightly non-trivial* data structures related to $\mathcal{C}$. However, a major drawback of their approach is the lack of unconditional results even for $\mathcal{C}={\rm AC}^0$.
	
In this work, we present the first unconditional $\rm FP^{NP}$ algorithm for ${\rm ACC}^0$-$\rm Avoid$. Indeed, we obtain $\rm FP^{NP}$ algorithms for the following stronger problems:
$\bullet$ (${\rm ACC}^0$-$\rm RemotePoint$). Given $C:\{0,1\}^n\to\{0,1\}^\ell$ for some $\ell={\rm quasipoly}(n)$ such that each output bit of $C$ is computed by a ${\rm quasipoly}(n)$-size ${\rm AC}^0[m]$ circuit, we can find some $y\in\{0,1\}^\ell$ in $\rm FP^{NP}$ such that for every $x\in\{0, 1\}^n$, the relative Hamming distance between $y$ and $C(x)$ is at least $1/2-1/{\rm poly}(n)$. This problem is the ``average-case&#39;&#39; analogue of ${\rm ACC}^0$-$\rm Avoid$.
$\bullet$ (${\rm ACC}^0$-$\rm AvgPartialHard$). Given $x_1,\dots,x_\ell\in\{0,1\}^n$ for some $\ell={\rm quasipoly}(n)$, we can compute $\ell$ bits $y_1,\dots,y_\ell\in\{0,1\}$ in $\rm FP^{NP}$ such that for every $2^{\log^c(n)}$-size ${\rm ACC}^0$ circuit $C$, $\Pr_i[C(x_i)\ne y_i]\ge 1/2-1/{\rm poly}(n)$, where $c=O(1)$. This problem generalises the strong average-case circuit lower bounds against ${\rm ACC}^0$ in a different way. 
Our algorithms can be seen as natural generalisations of the best known almost-everywhere average-case lower bounds against ${\rm ACC}^0$ circuits by Chen, Lyu, and Williams (FOCS&#39;20). Note that both problems above have been studied prior to our work, and no $\rm FP^{NP}$ algorithm was known even for weak circuit classes such as ${\rm GF}(2)$-linear circuits and DNF formulas. 

Our results follow from a strengthened algorithmic method: slightly non-trivial algorithms for the *Satisfying-Pairs* problem for $\mathcal{C}$ implies $\rm FP^{NP}$ algorithms for $\mathcal{C}$-$\rm Avoid$ (as well as $\mathcal{C}$-$\rm RemotePoint$ and $\mathcal{C}$-$\rm AvgPartialHard$). Here, given $\mathcal{C}$-circuits $\{C_i\}$ and inputs $\{x_j\}$, the $\mathcal{C}$-Satisfying-Pairs problem asks to (approximately) count the number of pairs $(i,j)$ such that $C_i(x_j)=1$.
	
A technical contribution of this work is a construction of a *short, smooth, and rectangular PCP of Proximity* that combines two previous PCP constructions, which may be of independent interest. It serves as a key tool that allows us to generalise the framework for $\rm Avoid$ to the average-case scenarios.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-18T12:12:41Z">Thursday, May 18 2023, 12:12</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-events.org/2023/05/18/fodsi-summer-school-on-the-foundations-of-data-science/'>FODSI Summer School on the Foundations of Data Science.</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-events.org'>CS Theory Events</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          June 27-28, 2023 Bryn Mawr College fodsi.us/ssbm.html The Foundations of Data Science Institute (FODSI) is organizing a summer school on the foundations of data science. This school will be held at the Bryn Mawr College campus, near Philadelphia. It is aimed at advanced undergraduate students and beginning graduate students in fields related to the foundations &#8230; Continue reading FODSI Summer School on the Foundations of Data&#160;Science.<p>By shacharlovett</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          June 27-28, 2023 Bryn Mawr College https://fodsi.us/ssbm.html The Foundations of Data Science Institute (FODSI) is organizing a summer school on the foundations of data science. This school will be held at the Bryn Mawr College campus, near Philadelphia. It is aimed at advanced undergraduate students and beginning graduate students in fields related to the foundations &#8230; <a href="https://cstheory-events.org/2023/05/18/fodsi-summer-school-on-the-foundations-of-data-science/" class="more-link">Continue reading <span class="screen-reader-text">FODSI Summer School on the Foundations of Data&#160;Science.</span></a><p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-18T04:58:12Z">Thursday, May 18 2023, 04:58</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09973'>Border Complexity of Symbolic Determinant under Rank One Restriction</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Abhranil Chatterjee, Sumanta Ghosh, Rohit Gurjar, Roshan Raj</p><p>VBP is the class of polynomial families that can be computed by the
determinant of a symbolic matrix of the form $A_0 + \sum_{i=1}^n A_ix_i$ where
the size of each $A_i$ is polynomial in the number of variables (equivalently,
computable by polynomial-sized algebraic branching programs (ABP)). A major
open problem in geometric complexity theory (GCT) is to determine whether VBP
is closed under approximation. The power of approximation is well understood
for some restricted models of computation, e.g., the class of depth-two
circuits, read-once oblivious ABPs (ROABP), monotone ABPs, depth-three circuits
of bounded top fan-in, and width-two ABPs. The former three classes are known
to be closed under approximation [Bl"{a}ser, Ikenmeyer, Mahajan, Pandey, and
Saurabh (2020)], whereas the approximative closure of the last one captures the
whole class of polynomial families computable by polynomial-sized formulas
[Bringmann, Ikenmeyer, and Zuiddam (2017)].
</p>
<p>In this work, we consider the subclass of VBP computed by the determinant of
a symbolic matrix of the form $A_0 + \sum_{i=1}^n A_ix_i$ where for each $1\leq
i \leq n$, $A_i$ is of rank one. It has been studied extensively
[Edmonds(1968), Edmonds(1979)] and efficient identity testing algorithms are
known [Lov"{a}sz (1989), Gurjar and Thierauf (2020)]. We show that this class
is closed under approximation. In the language of algebraic geometry, we show
that the set obtained by taking coordinatewise products of pairs of points from
(the Pl\"{u}cker embedding of) a Grassmannian variety is closed.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chatterjee_A/0/1/0/all/0/1">Abhranil Chatterjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1">Sumanta Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurjar_R/0/1/0/all/0/1">Rohit Gurjar</a>, <a href="http://arxiv.org/find/cs/1/au:+Raj_R/0/1/0/all/0/1">Roshan Raj</a></p><p>VBP is the class of polynomial families that can be computed by the
determinant of a symbolic matrix of the form $A_0 + \sum_{i=1}^n A_ix_i$ where
the size of each $A_i$ is polynomial in the number of variables (equivalently,
computable by polynomial-sized algebraic branching programs (ABP)). A major
open problem in geometric complexity theory (GCT) is to determine whether VBP
is closed under approximation. The power of approximation is well understood
for some restricted models of computation, e.g., the class of depth-two
circuits, read-once oblivious ABPs (ROABP), monotone ABPs, depth-three circuits
of bounded top fan-in, and width-two ABPs. The former three classes are known
to be closed under approximation [Bl"{a}ser, Ikenmeyer, Mahajan, Pandey, and
Saurabh (2020)], whereas the approximative closure of the last one captures the
whole class of polynomial families computable by polynomial-sized formulas
[Bringmann, Ikenmeyer, and Zuiddam (2017)].
</p>
<p>In this work, we consider the subclass of VBP computed by the determinant of
a symbolic matrix of the form $A_0 + \sum_{i=1}^n A_ix_i$ where for each $1\leq
i \leq n$, $A_i$ is of rank one. It has been studied extensively
[Edmonds(1968), Edmonds(1979)] and efficient identity testing algorithms are
known [Lov"{a}sz (1989), Gurjar and Thierauf (2020)]. We show that this class
is closed under approximation. In the language of algebraic geometry, we show
that the set obtained by taking coordinatewise products of pairs of points from
(the Pl\"{u}cker embedding of) a Grassmannian variety is closed.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-18T00:30:00Z">Thursday, May 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09984'>The Noncommutative Edmonds' Problem Re-visited</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Abhranil Chatterjee, Partha Mukhopadhyay</p><p>Let $T$ be a matrix whose entries are linear forms over the noncommutative
variables $x_1, x_2, \ldots, x_n$. The noncommutative Edmonds' problem
(NSINGULAR) aims to determine whether $T$ is invertible in the free skew field
generated by $x_1,x_2,\ldots,x_n$. Currently, there are three different
deterministic polynomial-time algorithms to solve this problem: using operator
scaling [Garg, Gurvits, Oliveira, and Wigserdon (2016)], algebraic methods
[Ivanyos, Qiao, and Subrahmanyam (2018)], and convex optimization [Hamada and
Hirai (2021)].
</p>
<p>In this paper, we present a simpler algorithm for the NSINGULAR problem.
While our algorithmic template is similar to the one in Ivanyos et. al.(2018),
it significantly differs in its implementation of the rank increment step.
Instead of computing the limit of a second Wong sequence, we reduce the problem
to the polynomial identity testing (PIT) of noncommutative algebraic branching
programs (ABPs).
</p>
<p>This enables us to bound the bit-complexity of the algorithm over
$\mathbb{Q}$ without requiring special care. Moreover, the rank increment step
can be implemented in quasipolynomial-time even without an explicit description
of the coefficient matrices in $T$. This is possible by exploiting the
connection with the black-box PIT of noncommutative ABPs [Forbes and Shpilka
(2013)].
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chatterjee_A/0/1/0/all/0/1">Abhranil Chatterjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukhopadhyay_P/0/1/0/all/0/1">Partha Mukhopadhyay</a></p><p>Let $T$ be a matrix whose entries are linear forms over the noncommutative
variables $x_1, x_2, \ldots, x_n$. The noncommutative Edmonds' problem
(NSINGULAR) aims to determine whether $T$ is invertible in the free skew field
generated by $x_1,x_2,\ldots,x_n$. Currently, there are three different
deterministic polynomial-time algorithms to solve this problem: using operator
scaling [Garg, Gurvits, Oliveira, and Wigserdon (2016)], algebraic methods
[Ivanyos, Qiao, and Subrahmanyam (2018)], and convex optimization [Hamada and
Hirai (2021)].
</p>
<p>In this paper, we present a simpler algorithm for the NSINGULAR problem.
While our algorithmic template is similar to the one in Ivanyos et. al.(2018),
it significantly differs in its implementation of the rank increment step.
Instead of computing the limit of a second Wong sequence, we reduce the problem
to the polynomial identity testing (PIT) of noncommutative algebraic branching
programs (ABPs).
</p>
<p>This enables us to bound the bit-complexity of the algorithm over
$\mathbb{Q}$ without requiring special care. Moreover, the rank increment step
can be implemented in quasipolynomial-time even without an explicit description
of the coefficient matrices in $T$. This is possible by exploiting the
connection with the black-box PIT of noncommutative ABPs [Forbes and Shpilka
(2013)].
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-18T00:30:00Z">Thursday, May 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09995'>Algorithmic Decorrelation and Planted Clique in Dependent Random Graphs: The Case of Extra Triangles</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Guy Bresler, Chenghao Guo, Yury Polyanskiy</p><p>We aim to understand the extent to which the noise distribution in a planted
signal-plus-noise problem impacts its computational complexity. To that end, we
consider the planted clique and planted dense subgraph problems, but in a
different ambient graph. Instead of Erd\H{o}s-R\'enyi $G(n,p)$, which has
independent edges, we take the ambient graph to be the \emph{random graph with
triangles} (RGT) obtained by adding triangles to $G(n,p)$. We show that the RGT
can be efficiently mapped to the corresponding $G(n,p)$, and moreover, that the
planted clique (or dense subgraph) is approximately preserved under this
mapping. This constitutes the first average-case reduction transforming
dependent noise to independent noise. Together with the easier direction of
mapping the ambient graph from Erd\H{o}s-R\'enyi to RGT, our results yield a
strong equivalence between models. In order to prove our results, we develop a
new general framework for reasoning about the validity of average-case
reductions based on \emph{low sensitivity to perturbations}.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Bresler_G/0/1/0/all/0/1">Guy Bresler</a>, <a href="http://arxiv.org/find/math/1/au:+Guo_C/0/1/0/all/0/1">Chenghao Guo</a>, <a href="http://arxiv.org/find/math/1/au:+Polyanskiy_Y/0/1/0/all/0/1">Yury Polyanskiy</a></p><p>We aim to understand the extent to which the noise distribution in a planted
signal-plus-noise problem impacts its computational complexity. To that end, we
consider the planted clique and planted dense subgraph problems, but in a
different ambient graph. Instead of Erd\H{o}s-R\'enyi $G(n,p)$, which has
independent edges, we take the ambient graph to be the \emph{random graph with
triangles} (RGT) obtained by adding triangles to $G(n,p)$. We show that the RGT
can be efficiently mapped to the corresponding $G(n,p)$, and moreover, that the
planted clique (or dense subgraph) is approximately preserved under this
mapping. This constitutes the first average-case reduction transforming
dependent noise to independent noise. Together with the easier direction of
mapping the ambient graph from Erd\H{o}s-R\'enyi to RGT, our results yield a
strong equivalence between models. In order to prove our results, we develop a
new general framework for reasoning about the validity of average-case
reductions based on \emph{low sensitivity to perturbations}.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-18T00:30:00Z">Thursday, May 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.10277'>Lower bounds on the Approximate Stabilizer Rank: A Probabilistic Approach</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Saeed Mehraban, Mehrdad Tahmasbi</p><p>The approximate stabilizer rank of a quantum state is the minimum number of
terms in any approximate decomposition of that state into stabilizer states.
Bravyi and Gosset showed that the approximate stabilizer rank of a so-called
"magic" state like $|T\rangle^{\otimes n}$, up to polynomial factors, is an
upper bound on the number of classical operations required to simulate an
arbitrary quantum circuit with Clifford gates and $n$ number of $T$ gates. As a
result, an exponential lower bound on this quantity seems inevitable. Despite
this intuition, several attempts using various techniques could not lead to a
better than a linear lower bound on the "exact" rank of $|T\rangle^{\otimes
n}$, meaning the minimal size of a decomposition that exactly produces the
state. However, an "approximate" rank is more realistically related to the cost
of simulating quantum circuits because exact rank is not robust to errors;
there are quantum states with exponentially large exact ranks but constant
approximate ranks even with arbitrarily small approximation parameters. No
lower bound better than $\tilde \Omega(\sqrt n)$ has been known for the
approximate rank. In this paper, we improve this lower bound to $\tilde \Omega
(n)$ for a wide range of the approximation parameters. Our approach is based on
a strong lower bound on the approximate rank of a quantum state sampled from
the Haar measure and a step-by-step analysis of the approximate rank of a
magic-state teleportation protocol to sample from the Haar measure.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Mehraban_S/0/1/0/all/0/1">Saeed Mehraban</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Tahmasbi_M/0/1/0/all/0/1">Mehrdad Tahmasbi</a></p><p>The approximate stabilizer rank of a quantum state is the minimum number of
terms in any approximate decomposition of that state into stabilizer states.
Bravyi and Gosset showed that the approximate stabilizer rank of a so-called
"magic" state like $|T\rangle^{\otimes n}$, up to polynomial factors, is an
upper bound on the number of classical operations required to simulate an
arbitrary quantum circuit with Clifford gates and $n$ number of $T$ gates. As a
result, an exponential lower bound on this quantity seems inevitable. Despite
this intuition, several attempts using various techniques could not lead to a
better than a linear lower bound on the "exact" rank of $|T\rangle^{\otimes
n}$, meaning the minimal size of a decomposition that exactly produces the
state. However, an "approximate" rank is more realistically related to the cost
of simulating quantum circuits because exact rank is not robust to errors;
there are quantum states with exponentially large exact ranks but constant
approximate ranks even with arbitrarily small approximation parameters. No
lower bound better than $\tilde \Omega(\sqrt n)$ has been known for the
approximate rank. In this paper, we improve this lower bound to $\tilde \Omega
(n)$ for a wide range of the approximation parameters. Our approach is based on
a strong lower bound on the approximate rank of a quantum state sampled from
the Haar measure and a step-by-step analysis of the approximate rank of a
magic-state teleportation protocol to sample from the Haar measure.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-18T00:30:00Z">Thursday, May 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.10334'>Principal-Agent Boolean Games</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: David Hyland, Julian Gutierrez, Michael Wooldridge</p><p>We introduce and study a computational version of the principal-agent problem
-- a classic problem in Economics that arises when a principal desires to
contract an agent to carry out some task, but has incomplete information about
the agent or their subsequent actions. The key challenge in this setting is for
the principal to design a contract for the agent such that the agent's
preferences are then aligned with those of the principal. We study this problem
using a variation of Boolean games, where multiple players each choose
valuations for Boolean variables under their control, seeking the satisfaction
of a personal goal, given as a Boolean logic formula. In our setting, the
principal can only observe some subset of these variables, and the principal
chooses a contract which rewards players on the basis of the assignments they
make for the variables that are observable to the principal. The principal's
challenge is to design a contract so that, firstly, the principal's goal is
achieved in some or all Nash equilibrium choices, and secondly, that the
principal is able to verify that their goal is satisfied. In this paper, we
formally define this problem and completely characterise the computational
complexity of the most relevant decision problems associated with it.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Hyland_D/0/1/0/all/0/1">David Hyland</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutierrez_J/0/1/0/all/0/1">Julian Gutierrez</a>, <a href="http://arxiv.org/find/cs/1/au:+Wooldridge_M/0/1/0/all/0/1">Michael Wooldridge</a></p><p>We introduce and study a computational version of the principal-agent problem
-- a classic problem in Economics that arises when a principal desires to
contract an agent to carry out some task, but has incomplete information about
the agent or their subsequent actions. The key challenge in this setting is for
the principal to design a contract for the agent such that the agent's
preferences are then aligned with those of the principal. We study this problem
using a variation of Boolean games, where multiple players each choose
valuations for Boolean variables under their control, seeking the satisfaction
of a personal goal, given as a Boolean logic formula. In our setting, the
principal can only observe some subset of these variables, and the principal
chooses a contract which rewards players on the basis of the assignments they
make for the variables that are observable to the principal. The principal's
challenge is to design a contract so that, firstly, the principal's goal is
achieved in some or all Nash equilibrium choices, and secondly, that the
principal is able to verify that their goal is satisfied. In this paper, we
formally define this problem and completely characterise the computational
complexity of the most relevant decision problems associated with it.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-18T00:30:00Z">Thursday, May 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09778'>Shortest Path to Boundary for Self-Intersecting Meshes</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: He Chen, Elie Diaz, Cem Yuksel</p><p>We introduce a method for efficiently computing the exact shortest path to
the boundary of a mesh from a given internal point in the presence of
self-intersections. We provide a formal definition of shortest boundary paths
for self-intersecting objects and present a robust algorithm for computing the
actual shortest boundary path. The resulting method offers an effective
solution for collision and self-collision handling while simulating deformable
volumetric objects, using fast simulation techniques that provide no guarantees
on collision resolution. Our evaluation includes complex self-collision
scenarios with a large number of active contacts, showing that our method can
successfully handle them by introducing a relatively minor computational
overhead.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">He Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Diaz_E/0/1/0/all/0/1">Elie Diaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuksel_C/0/1/0/all/0/1">Cem Yuksel</a></p><p>We introduce a method for efficiently computing the exact shortest path to
the boundary of a mesh from a given internal point in the presence of
self-intersections. We provide a formal definition of shortest boundary paths
for self-intersecting objects and present a robust algorithm for computing the
actual shortest boundary path. The resulting method offers an effective
solution for collision and self-collision handling while simulating deformable
volumetric objects, using fast simulation techniques that provide no guarantees
on collision resolution. Our evaluation includes complex self-collision
scenarios with a large number of active contacts, showing that our method can
successfully handle them by introducing a relatively minor computational
overhead.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-18T00:30:00Z">Thursday, May 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09925'>A Scalable Method for Readable Tree Layouts</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Kathryn Gray, Mingwei Li, Reyan Ahmed, Md. Khaledur Rahman, Ariful Azad, Stephen Kobourov, Katy B&#xf6;rner</p><p>Large tree structures are ubiquitous and real-world relational datasets often
have information associated with nodes (e.g., labels or other attributes) and
edges (e.g., weights or distances) that need to be communicated to the viewers.
Yet, scalable, easy to read tree layouts are difficult to achieve. We consider
tree layouts to be readable if they meet some basic requirements: node labels
should not overlap, edges should not cross, edge lengths should be preserved,
and the output should be compact. There are many algorithms for drawing trees,
although very few take node labels or edge lengths into account, and none
optimizes all requirements above. With this in mind, we propose a new scalable
method for readable tree layouts. The algorithm guarantees that the layout has
no edge crossings and no label overlaps, and optimizes one of the remaining
aspects: desired edge lengths and compactness. We evaluate the performance of
the new algorithm by comparison with related earlier approaches using several
real-world datasets, ranging from a few thousand nodes to hundreds of thousands
of nodes. Tree layout algorithms can be used to visualize large general graphs,
by extracting a hierarchy of progressively larger trees. We illustrate this
functionality by presenting several map-like visualizations generated by the
new tree layout algorithm.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gray_K/0/1/0/all/0/1">Kathryn Gray</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mingwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_R/0/1/0/all/0/1">Reyan Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1">Md. Khaledur Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Azad_A/0/1/0/all/0/1">Ariful Azad</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobourov_S/0/1/0/all/0/1">Stephen Kobourov</a>, <a href="http://arxiv.org/find/cs/1/au:+Borner_K/0/1/0/all/0/1">Katy B&#xf6;rner</a></p><p>Large tree structures are ubiquitous and real-world relational datasets often
have information associated with nodes (e.g., labels or other attributes) and
edges (e.g., weights or distances) that need to be communicated to the viewers.
Yet, scalable, easy to read tree layouts are difficult to achieve. We consider
tree layouts to be readable if they meet some basic requirements: node labels
should not overlap, edges should not cross, edge lengths should be preserved,
and the output should be compact. There are many algorithms for drawing trees,
although very few take node labels or edge lengths into account, and none
optimizes all requirements above. With this in mind, we propose a new scalable
method for readable tree layouts. The algorithm guarantees that the layout has
no edge crossings and no label overlaps, and optimizes one of the remaining
aspects: desired edge lengths and compactness. We evaluate the performance of
the new algorithm by comparison with related earlier approaches using several
real-world datasets, ranging from a few thousand nodes to hundreds of thousands
of nodes. Tree layout algorithms can be used to visualize large general graphs,
by extracting a hierarchy of progressively larger trees. We illustrate this
functionality by presenting several map-like visualizations generated by the
new tree layout algorithm.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-18T00:30:00Z">Thursday, May 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.10023'>An Efficient Solution Space Exploring and Descent Method for Packing Equal Spheres in a Sphere</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jianrong Zhou, Shuo Ren, Kun He, Yanli Liu, Chu-Min Li</p><p>The problem of packing equal spheres in a spherical container is a classic
global optimization problem, which has attracted enormous studies in academia
and found various applications in industry. This problem is computationally
challenging, and many efforts focus on small-scale instances with the number of
spherical items less than 200 in the literature. In this work, we propose an
efficient local search heuristic algorithm named solution space exploring and
descent for solving this problem, which can quantify the solution's quality to
determine the number of exploring actions and quickly discover a high-quality
solution. Besides, we propose an adaptive neighbor object maintenance method to
speed up the convergence of the continuous optimization process and reduce the
time consumption. Computational experiments on a large number of benchmark
instances with $5 \leq n \leq 400$ spherical items show that our algorithm
significantly outperforms the state-of-the-art algorithm. In particular, it
improves the 274 best-known results and matches the 84 best-known results out
of the 396 well-known benchmark instances.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jianrong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1">Shuo Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1">Kun He</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yanli Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chu-Min Li</a></p><p>The problem of packing equal spheres in a spherical container is a classic
global optimization problem, which has attracted enormous studies in academia
and found various applications in industry. This problem is computationally
challenging, and many efforts focus on small-scale instances with the number of
spherical items less than 200 in the literature. In this work, we propose an
efficient local search heuristic algorithm named solution space exploring and
descent for solving this problem, which can quantify the solution's quality to
determine the number of exploring actions and quickly discover a high-quality
solution. Besides, we propose an adaptive neighbor object maintenance method to
speed up the convergence of the continuous optimization process and reduce the
time consumption. Computational experiments on a large number of benchmark
instances with $5 \leq n \leq 400$ spherical items show that our algorithm
significantly outperforms the state-of-the-art algorithm. In particular, it
improves the 274 best-known results and matches the 84 best-known results out
of the 396 well-known benchmark instances.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-18T00:30:00Z">Thursday, May 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.10341'>Convex Cover and Hidden Set in Funnel Polygons</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Reilly Browne</p><p>We present linear-time algorithms for both maximum hidden set and minimum
convex cover in funnel polygons. These algorithms show that funnel polygons are
"homestead" polygons, i.e. polygons for which the hidden set number and the
convex cover number coincide. We extend the algorithm to apply to maximum
hidden vertex set and use the result to give a 2-approximation for all three
problems in pseudotriangles.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Browne_R/0/1/0/all/0/1">Reilly Browne</a></p><p>We present linear-time algorithms for both maximum hidden set and minimum
convex cover in funnel polygons. These algorithms show that funnel polygons are
"homestead" polygons, i.e. polygons for which the hidden set number and the
convex cover number coincide. We extend the algorithm to apply to maximum
hidden vertex set and use the result to give a 2-approximation for all three
problems in pseudotriangles.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-18T00:30:00Z">Thursday, May 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.10389'>Cache-Oblivious Parallel Convex Hull in the Binary Forking Model</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Reilly Browne, Rezaul Chowdhury, Shih-Yu Tsai, Yimin Zhu</p><p>We present two cache-oblivious sorting-based convex hull algorithms in the
Binary Forking Model. The first is an algorithm for a presorted set of points
which achieves $O(n)$ work, $O(\log n)$ span, and $O(n/B)$ serial cache
complexity, where $B$ is the cache line size. These are all optimal worst-case
bounds for cache-oblivious algorithms in the Binary Forking Model. The second
adapts Cole and Ramachandran's cache-oblivious sorting algorithm, matching its
properties including achieving $O(n \log n)$ work, $O(\log n \log \log n)$
span, and $O(n/B \log_M n)$ serial cache complexity. Here $M$ is the size of
the private cache.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Browne_R/0/1/0/all/0/1">Reilly Browne</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_R/0/1/0/all/0/1">Rezaul Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_S/0/1/0/all/0/1">Shih-Yu Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yimin Zhu</a></p><p>We present two cache-oblivious sorting-based convex hull algorithms in the
Binary Forking Model. The first is an algorithm for a presorted set of points
which achieves $O(n)$ work, $O(\log n)$ span, and $O(n/B)$ serial cache
complexity, where $B$ is the cache line size. These are all optimal worst-case
bounds for cache-oblivious algorithms in the Binary Forking Model. The second
adapts Cole and Ramachandran's cache-oblivious sorting algorithm, matching its
properties including achieving $O(n \log n)$ work, $O(\log n \log \log n)$
span, and $O(n/B \log_M n)$ serial cache complexity. Here $M$ is the size of
the private cache.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-18T00:30:00Z">Thursday, May 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09668'>Mean Estimation Under Heterogeneous Privacy: Some Privacy Can Be Free</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Syomantak Chaudhuri, Thomas A. Courtade</p><p>Differential Privacy (DP) is a well-established framework to quantify privacy
loss incurred by any algorithm. Traditional DP formulations impose a uniform
privacy requirement for all users, which is often inconsistent with real-world
scenarios in which users dictate their privacy preferences individually. This
work considers the problem of mean estimation under heterogeneous DP
constraints, where each user can impose their own distinct privacy level. The
algorithm we propose is shown to be minimax optimal when there are two groups
of users with distinct privacy levels. Our results elicit an interesting
saturation phenomenon that occurs as one group's privacy level is relaxed,
while the other group's privacy level remains constant. Namely, after a certain
point, further relaxing the privacy requirement of the former group does not
improve the performance of the minimax optimal mean estimator. Thus, the
central server can offer a certain degree of privacy without any sacrifice in
performance.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1">Syomantak Chaudhuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Courtade_T/0/1/0/all/0/1">Thomas A. Courtade</a></p><p>Differential Privacy (DP) is a well-established framework to quantify privacy
loss incurred by any algorithm. Traditional DP formulations impose a uniform
privacy requirement for all users, which is often inconsistent with real-world
scenarios in which users dictate their privacy preferences individually. This
work considers the problem of mean estimation under heterogeneous DP
constraints, where each user can impose their own distinct privacy level. The
algorithm we propose is shown to be minimax optimal when there are two groups
of users with distinct privacy levels. Our results elicit an interesting
saturation phenomenon that occurs as one group's privacy level is relaxed,
while the other group's privacy level remains constant. Namely, after a certain
point, further relaxing the privacy requirement of the former group does not
improve the performance of the minimax optimal mean estimator. Thus, the
central server can offer a certain degree of privacy without any sacrifice in
performance.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-18T00:30:00Z">Thursday, May 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09752'>Finding Maximal Exact Matches in Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Nicola Rizzo, Manuel C&#xe1;ceres, Veli M&#xe4;kinen</p><p>We study the problem of finding maximal exact matches (MEMs) between a query
string $Q$ and a labeled graph $G$. MEMs are an important class of seeds, often
used in seed-chain-extend type of practical alignment methods because of their
strong connections to classical metrics. A principled way to speed up chaining
is to limit the number of MEMs by considering only MEMs of length at least
$\kappa$ ($\kappa$-MEMs). However, on arbitrary input graphs, the problem of
finding MEMs cannot be solved in truly sub-quadratic time under SETH (Equi et
al., ICALP 2019) even on acyclic graphs. In this paper we show an $O(n\cdot L
\cdot d^{L-1} + m + M_{\kappa,L})$-time algorithm finding all $\kappa$-MEMs
between $Q$ and $G$ spanning exactly $L$ nodes in $G$, where $n$ is the total
length of node labels, $d$ is the maximum degree of a node in $G$, $m = |Q|$,
and $M_{\kappa,L}$ is the number of output MEMs. We use this algorithm to
develop a $\kappa$-MEM finding solution on indexable Elastic Founder Graphs
(Equi et al., Algorithmica 2022) running in time $O(nH^2 + m + M_\kappa)$,
where $H$ is the maximum number of nodes in a block, and $M_\kappa$ is the
total number of $\kappa$-MEMs. Our results generalize to the analysis of
multiple query strings (MEMs between $G$ and any of the strings). Additionally,
we provide some preliminary experimental results showing that the number of
graph MEMs is orders of magnitude smaller than the number of string MEMs of the
corresponding concatenated collection.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Rizzo_N/0/1/0/all/0/1">Nicola Rizzo</a>, <a href="http://arxiv.org/find/cs/1/au:+Caceres_M/0/1/0/all/0/1">Manuel C&#xe1;ceres</a>, <a href="http://arxiv.org/find/cs/1/au:+Makinen_V/0/1/0/all/0/1">Veli M&#xe4;kinen</a></p><p>We study the problem of finding maximal exact matches (MEMs) between a query
string $Q$ and a labeled graph $G$. MEMs are an important class of seeds, often
used in seed-chain-extend type of practical alignment methods because of their
strong connections to classical metrics. A principled way to speed up chaining
is to limit the number of MEMs by considering only MEMs of length at least
$\kappa$ ($\kappa$-MEMs). However, on arbitrary input graphs, the problem of
finding MEMs cannot be solved in truly sub-quadratic time under SETH (Equi et
al., ICALP 2019) even on acyclic graphs. In this paper we show an $O(n\cdot L
\cdot d^{L-1} + m + M_{\kappa,L})$-time algorithm finding all $\kappa$-MEMs
between $Q$ and $G$ spanning exactly $L$ nodes in $G$, where $n$ is the total
length of node labels, $d$ is the maximum degree of a node in $G$, $m = |Q|$,
and $M_{\kappa,L}$ is the number of output MEMs. We use this algorithm to
develop a $\kappa$-MEM finding solution on indexable Elastic Founder Graphs
(Equi et al., Algorithmica 2022) running in time $O(nH^2 + m + M_\kappa)$,
where $H$ is the maximum number of nodes in a block, and $M_\kappa$ is the
total number of $\kappa$-MEMs. Our results generalize to the analysis of
multiple query strings (MEMs between $G$ and any of the strings). Additionally,
we provide some preliminary experimental results showing that the number of
graph MEMs is orders of magnitude smaller than the number of string MEMs of the
corresponding concatenated collection.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-18T00:30:00Z">Thursday, May 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.10108'>List 3-Coloring on Comb-Convex and Caterpillar-Convex Bipartite Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Banu Baklan &#x15e;en, &#xd6;znur Ya&#x15f;ar Diner, Thomas Erlebach</p><p>Given a graph $G=(V, E)$ and a list of available colors $L(v)$ for each
vertex $v\in V$, where $L(v) \subseteq \{1, 2, \ldots, k\}$, List $k$-Coloring
refers to the problem of assigning colors to the vertices of $G$ so that each
vertex receives a color from its own list and no two neighboring vertices
receive the same color. The decision version of the problem List $3$-Coloring
is NP-complete even for bipartite graphs, and its complexity on comb-convex
bipartite graphs has been an open problem. We give a polynomial-time algorithm
to solve List $3$-Coloring for caterpillar-convex bipartite graphs, a
superclass of comb-convex bipartite graphs. We also give a polynomial-time
recognition algorithm for the class of caterpillar-convex bipartite graphs.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Sen_B/0/1/0/all/0/1">Banu Baklan &#x15e;en</a>, <a href="http://arxiv.org/find/cs/1/au:+Diner_O/0/1/0/all/0/1">&#xd6;znur Ya&#x15f;ar Diner</a>, <a href="http://arxiv.org/find/cs/1/au:+Erlebach_T/0/1/0/all/0/1">Thomas Erlebach</a></p><p>Given a graph $G=(V, E)$ and a list of available colors $L(v)$ for each
vertex $v\in V$, where $L(v) \subseteq \{1, 2, \ldots, k\}$, List $k$-Coloring
refers to the problem of assigning colors to the vertices of $G$ so that each
vertex receives a color from its own list and no two neighboring vertices
receive the same color. The decision version of the problem List $3$-Coloring
is NP-complete even for bipartite graphs, and its complexity on comb-convex
bipartite graphs has been an open problem. We give a polynomial-time algorithm
to solve List $3$-Coloring for caterpillar-convex bipartite graphs, a
superclass of comb-convex bipartite graphs. We also give a polynomial-time
recognition algorithm for the class of caterpillar-convex bipartite graphs.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-18T00:30:00Z">Thursday, May 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.10292'>Linear Query Approximation Algorithms for Non-monotone Submodular Maximization under Knapsack Constraint</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Canh V. Pham, Tan D. Tran, Dung T.K. Ha, My T. Thai</p><p>This work, for the first time, introduces two constant factor approximation
algorithms with linear query complexity for non-monotone submodular
maximization over a ground set of size $n$ subject to a knapsack constraint,
$\mathsf{DLA}$ and $\mathsf{RLA}$. $\mathsf{DLA}$ is a deterministic algorithm
that provides an approximation factor of $6+\epsilon$ while $\mathsf{RLA}$ is a
randomized algorithm with an approximation factor of $4+\epsilon$. Both run in
$O(n \log(1/\epsilon)/\epsilon)$ query complexity. The key idea to obtain a
constant approximation ratio with linear query lies in: (1) dividing the ground
set into two appropriate subsets to find the near-optimal solution over these
subsets with linear queries, and (2) combining a threshold greedy with
properties of two disjoint sets or a random selection process to improve
solution quality. In addition to the theoretical analysis, we have evaluated
our proposed solutions with three applications: Revenue Maximization, Image
Summarization, and Maximum Weighted Cut, showing that our algorithms not only
return comparative results to state-of-the-art algorithms but also require
significantly fewer queries.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1">Canh V. Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1">Tan D. Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Ha_D/0/1/0/all/0/1">Dung T.K. Ha</a>, <a href="http://arxiv.org/find/cs/1/au:+Thai_M/0/1/0/all/0/1">My T. Thai</a></p><p>This work, for the first time, introduces two constant factor approximation
algorithms with linear query complexity for non-monotone submodular
maximization over a ground set of size $n$ subject to a knapsack constraint,
$\mathsf{DLA}$ and $\mathsf{RLA}$. $\mathsf{DLA}$ is a deterministic algorithm
that provides an approximation factor of $6+\epsilon$ while $\mathsf{RLA}$ is a
randomized algorithm with an approximation factor of $4+\epsilon$. Both run in
$O(n \log(1/\epsilon)/\epsilon)$ query complexity. The key idea to obtain a
constant approximation ratio with linear query lies in: (1) dividing the ground
set into two appropriate subsets to find the near-optimal solution over these
subsets with linear queries, and (2) combining a threshold greedy with
properties of two disjoint sets or a random selection process to improve
solution quality. In addition to the theoretical analysis, we have evaluated
our proposed solutions with three applications: Revenue Maximization, Image
Summarization, and Maximum Weighted Cut, showing that our algorithms not only
return comparative results to state-of-the-art algorithms but also require
significantly fewer queries.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-18T00:30:00Z">Thursday, May 18 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Wednesday, May 17
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2023/05/17/permutation-supersequences-shortest.html'>Permutation supersequences and shortest paths</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          You may have heard of permutation superpatterns and superpermutations, but have you heard of permutation supersequences? They turn out to be closely related to how quickly we can find shortest paths in graphs with negative edge weights using the Bellman–Ford algorithm.
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>You may have heard of <a href="https://en.wikipedia.org/wiki/Superpattern">permutation superpatterns</a> and <a href="https://en.wikipedia.org/wiki/Superpermutation">superpermutations</a>, but have you heard of permutation supersequences? They turn out to be closely related to how quickly we can find shortest paths in graphs with negative edge weights using the <a href="https://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm">Bellman–Ford algorithm</a>.</p>

<p>Superpatterns, superpermutations, and supersequences are all sequences of symbols that somehow contain each permutation of their symbols. But they use different definitions of “contain”, causing them to have different lengths. For supersequences, the goal is to have every permutation as a subsequence: its symbols must appear in the same order, but can be widely spaced. One way to create a supersequence of \(n\) symbols, but not the best way, is to repeat all of them \(n\) times: 1234123412341234. You can do a little better by alternating ascending and descending order, saving one symbol per repetition: 1234321234321. But this is still not optimal. According to Oliver Tan in “Skip letters for short supersequence of all permutations” (<em>Disc. Math.</em> 2022, <a href="https://doi.org/10.1016/j.disc.2022.113070">doi:10.1016/j.disc.2022.113070</a>), calculating the exact length of the shortest supersequence of \(n\) symbols, as a function <span style="white-space:nowrap">of \(n\),</span> is still an open problem. Tan finds sequences of <span style="white-space:nowrap">length \(n^2-\tfrac52n+O(n^{2/3})\),</span> and cites a bound of Kleitman and Kwiatkowski showing that they must have <span style="white-space:nowrap">length \(n^2-O(n^{7/4})\).</span> So we have pretty accurate bounds on how short they can be: <span style="white-space:nowrap">near \(n^2\).</span></p>

<p>Now let’s continue to look at sequences and subsequences, but change up what they have to contain. Any permutation has \(n-1\) consecutive pairs of symbols. I want to find a sequence of pairs that for each permutation contains its \(n-1\) pairs, in their correct order. For instance, a four-symbol pair sequence should contain the three pairs 12, 23, and 34, in that order, coming from the permutation 1234, and it should also contain the other 23 triples of pairs coming from the other 23 permutations of the four symbols.</p>

<p>Why? Because that’s what some versions of the Bellman–Ford algorithm do! Suppose we want to find shortest paths from some starting vertex \(v_0\) to all other vertices in a complete directed graph. We can initialize tentative distances \(D[i]\) for all other vertices <span style="white-space:nowrap">\(v_i\), \(i&gt;0\)</span> to be the lengths of the single edges from \(v_0\) to those vertices, and then repeatedly <em>relax</em> a pair \((i,j)\) by <span style="white-space:nowrap">setting \(D[j]=\min\{D[j],D[i]+\operatorname{len}(v_i,v_j)\}\).</span> If we relax all of the consecutive pairs of vertices along a shortest path, in order, this will cause the tentative distance for the endpoint of the path to equal its correct shortest path distance.</p>

<p>So one way to get a correct shortest path algorithm is simply to choose a supersequence of the consecutive pairs of all permutations, and update the tentative distances in this order. You might notice that such an algorithm doesn’t pay any attention to the results of its calculation; it just performs its pairwise calculations for the entire supersequence, in a set order. I’ll call an algorithm like this, that chooses its update steps ahead of time instead of taking account of what has happened so far, <em>non-adaptive</em>. As a sequential algorithm, it would be better to be a little smarter, and use past information to guide future steps, but non-adaptive versions of Bellman–Ford turn out to be widely used in contexts where there is no centralized control, such as <a href="https://en.wikipedia.org/wiki/Distance-vector_routing_protocol">distance vector routing</a> on the internet.</p>

<p>So to figure out how efficient Bellman–Ford can be, we need to figure out how short these pair supersequences can be. Frustratingly, I don’t know very tight bounds for this problem. I can at least find out the exponent of the leading term in the length of these sequences (it’s cubic), but its constant factor eludes me.</p>

<p>As an upper bound, we can look at what various versions of the Bellman–Ford algorithm actually do. The simplest choice is just to repeat all pairs of indexes, \(n-1\) times: for four symbols (graphs with five vertices, but not counting the start), repeat 12–13–14–21–23–24–31–32–34–41–42–43 three times. For \(n\) symbols, this would require a total of \(\bigl(1-o(1)\bigr)n^3\) pairs. A technique of Yen improves this by listing the ascending pairs in ascending order and then the descending pairs in descending order: 12–13–14–23–24–34–43–42–32–41–31–21. Such a sequence can cover the pairs from any run of ascending symbols followed by any run of descending symbols: for instance, it would by itself cover the pairs from the permutations 1234, 1243, 1342, 1432, 2341, 2431, 3421, and 4321, which each consist of one ascending and one descending run. In general, you only need to repeat this ascending-descending pair sequence \(\lceil n/2\rceil\) times to cover all ascending and descending runs, giving pair supersequences of <span style="white-space:nowrap">length \(\bigl(\tfrac12-o(1)\bigr)n^3\).</span> And if you’re willing to tolerate a randomized algorithm that is correct only with high probability instead of with certainty, <a href="/blog/2011/04/11/randomized-bellmanford.html">a paper of mine with Michael Bannister from 2012</a> randomly permutes the symbols before using Yen’s ascending-descending method to achieve <span style="white-space:nowrap">length \(\bigl(\tfrac13+o(1)\bigr)n^3\).</span></p>

<p>My newest preprint, “Lower bounds for non-adaptive shortest path relaxation” (<a href="https://arxiv.org/abs/2305.09230">arXiv:2305.09230</a>, to appear at WADS), as the title suggests, looks at the lower bound side of the same problem. It shows that deterministically chosen sequences of pairs must have length at least \(\bigl(\tfrac16-o(1)\bigr)n^3\) and that random sequences (correct with high probability) must have length at least \(\bigl(\tfrac1{12}-o(1)\bigr)n^3\). It also contains analogous bounds for graphs that are not complete. However, these lower bounds are still far from the upper bounds. They are not even close enough to tell me whether randomized Bellman–Ford is really better than deterministic Bellman–Ford (in this non-adaptive setting) or whether a better deterministic pair sequence would beat the random one from my 2012 paper.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/110384942112779003">Discuss on Mastodon</a>)</p><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-17T09:11:00Z">Wednesday, May 17 2023, 09:11</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09508'>The Hardness of Reasoning about Probabilities and Causality</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Benito van der Zander, Markus Bl&#xe4;ser, Maciej Li&#x15b;kiewicz</p><p>We study formal languages which are capable of fully expressing quantitative
probabilistic reasoning and do-calculus reasoning for causal effects, from a
computational complexity perspective. We focus on satisfiability problems whose
instance formulas allow expressing many tasks in probabilistic and causal
inference. The main contribution of this work is establishing the exact
computational complexity of these satisfiability problems. We introduce a new
natural complexity class, named succ$\exists$R, which can be viewed as a
succinct variant of the well-studied class $\exists$R, and show that the
problems we consider are complete for succ$\exists$R. Our results imply even
stronger algorithmic limitations than were proven by Fagin, Halpern, and
Megiddo (1990) and Moss\'{e}, Ibeling, and Icard (2022) for some variants of
the standard languages used commonly in probabilistic and causal inference.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Zander_B/0/1/0/all/0/1">Benito van der Zander</a>, <a href="http://arxiv.org/find/cs/1/au:+Blaser_M/0/1/0/all/0/1">Markus Bl&#xe4;ser</a>, <a href="http://arxiv.org/find/cs/1/au:+Liskiewicz_M/0/1/0/all/0/1">Maciej Li&#x15b;kiewicz</a></p><p>We study formal languages which are capable of fully expressing quantitative
probabilistic reasoning and do-calculus reasoning for causal effects, from a
computational complexity perspective. We focus on satisfiability problems whose
instance formulas allow expressing many tasks in probabilistic and causal
inference. The main contribution of this work is establishing the exact
computational complexity of these satisfiability problems. We introduce a new
natural complexity class, named succ$\exists$R, which can be viewed as a
succinct variant of the well-studied class $\exists$R, and show that the
problems we consider are complete for succ$\exists$R. Our results imply even
stronger algorithmic limitations than were proven by Fagin, Halpern, and
Megiddo (1990) and Moss\'{e}, Ibeling, and Icard (2022) for some variants of
the standard languages used commonly in probabilistic and causal inference.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-17T00:30:00Z">Wednesday, May 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09538'>A Local Perspective on the Polynomial Hierarchy</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Fabian Reiter</p><p>We extend classical notions of computational complexity to the setting of
distributed computing. Instead of a single computer, several networked
computers communicate via synchronous message-passing to collectively solve
some decision problem related to the network topology. Their running time is
limited in two respects: the number of communication rounds is bounded by a
constant, and the number of computation steps of each computer is polynomially
bounded by the size of its local input and the messages it receives. By letting
two players take turns assigning certificates to the computers, we obtain a
generalization of the polynomial hierarchy (and hence of the complexity classes
$\mathbf{P}$ and $\mathbf{NP}$). We then extend major results of complexity
theory to this setting, in particular the Cook-Levin theorem (which identifies
Boolean satisfiability as a complete problem for $\mathbf{NP}$), and Fagin's
theorem (which characterizes $\mathbf{NP}$ as the problems expressible in
existential second-order logic). The original results can be recovered as the
special case where the network consists of a single computer. Moreover, perhaps
surprisingly, the task of separating complexity classes becomes easier in the
general case: we can show that our hierarchy is infinite, while it remains
notoriously open whether the same is true in the case of a single computer. In
contrast, a collapse of our hierarchy would have implied a collapse of the
polynomial hierarchy.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Reiter_F/0/1/0/all/0/1">Fabian Reiter</a></p><p>We extend classical notions of computational complexity to the setting of
distributed computing. Instead of a single computer, several networked
computers communicate via synchronous message-passing to collectively solve
some decision problem related to the network topology. Their running time is
limited in two respects: the number of communication rounds is bounded by a
constant, and the number of computation steps of each computer is polynomially
bounded by the size of its local input and the messages it receives. By letting
two players take turns assigning certificates to the computers, we obtain a
generalization of the polynomial hierarchy (and hence of the complexity classes
$\mathbf{P}$ and $\mathbf{NP}$). We then extend major results of complexity
theory to this setting, in particular the Cook-Levin theorem (which identifies
Boolean satisfiability as a complete problem for $\mathbf{NP}$), and Fagin's
theorem (which characterizes $\mathbf{NP}$ as the problems expressible in
existential second-order logic). The original results can be recovered as the
special case where the network consists of a single computer. Moreover, perhaps
surprisingly, the task of separating complexity classes becomes easier in the
general case: we can show that our hierarchy is infinite, while it remains
notoriously open whether the same is true in the case of a single computer. In
contrast, a collapse of our hierarchy would have implied a collapse of the
polynomial hierarchy.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-17T00:30:00Z">Wednesday, May 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09549'>Stable Dinner Party Seating Arrangements</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Damien Berriaud, Andrei Constantinescu, Roger Wattenhofer</p><p>A group of $n$ agents with numerical preferences for each other are to be
assigned to the $n$ seats of a dining table. We study two natural topologies:
circular (cycle) tables and panel (path) tables. For a given seating
arrangement, an agent's utility is the sum of its preference values towards its
(at most two) direct neighbors. An arrangement is envy-free if no agent
strictly prefers someone else's seat, and it is stable if no two agents
strictly prefer each other's seats. We show that it is NP-complete to decide
whether an envy-free arrangement exists for both paths and cycles, even with
binary preferences. In contrast, under the assumption that agents come from a
bounded number of classes, for both topologies, we present polynomial-time
algorithms computing envy-free and stable arrangements, working even for
general preferences. Proving the hardness of computing stable arrangements
seems more difficult, as even constructing unstable instances can be
challenging. To this end, we propose a characterization of the existence of
stable arrangements based on the number of distinct values in the preference
matrix and the number of classes of agents. For two classes of agents, we show
that stability can always be ensured, both for paths and cycles. For cycles, we
moreover show that binary preferences with four classes of agents, as well as
three-valued preferences with three classes of agents, are sufficient to
prevent the existence of a stable arrangement. For paths, the latter still
holds, while we argue that a path-stable arrangement always exists in the
binary case under the additional constraint that agents can only swap seats
when sitting at most two positions away. We moreover consider the swap dynamics
and exhibit instances where they do not converge, despite a stable arrangement
existing.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Berriaud_D/0/1/0/all/0/1">Damien Berriaud</a>, <a href="http://arxiv.org/find/cs/1/au:+Constantinescu_A/0/1/0/all/0/1">Andrei Constantinescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1">Roger Wattenhofer</a></p><p>A group of $n$ agents with numerical preferences for each other are to be
assigned to the $n$ seats of a dining table. We study two natural topologies:
circular (cycle) tables and panel (path) tables. For a given seating
arrangement, an agent's utility is the sum of its preference values towards its
(at most two) direct neighbors. An arrangement is envy-free if no agent
strictly prefers someone else's seat, and it is stable if no two agents
strictly prefer each other's seats. We show that it is NP-complete to decide
whether an envy-free arrangement exists for both paths and cycles, even with
binary preferences. In contrast, under the assumption that agents come from a
bounded number of classes, for both topologies, we present polynomial-time
algorithms computing envy-free and stable arrangements, working even for
general preferences. Proving the hardness of computing stable arrangements
seems more difficult, as even constructing unstable instances can be
challenging. To this end, we propose a characterization of the existence of
stable arrangements based on the number of distinct values in the preference
matrix and the number of classes of agents. For two classes of agents, we show
that stability can always be ensured, both for paths and cycles. For cycles, we
moreover show that binary preferences with four classes of agents, as well as
three-valued preferences with three classes of agents, are sufficient to
prevent the existence of a stable arrangement. For paths, the latter still
holds, while we argue that a path-stable arrangement always exists in the
binary case under the additional constraint that agents can only swap seats
when sitting at most two positions away. We moreover consider the swap dynamics
and exhibit instances where they do not converge, despite a stable arrangement
existing.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-17T00:30:00Z">Wednesday, May 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09248'>Maximum-Width Rainbow-Bisecting Empty Annulus</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sang Won Bae, Sandip Banerjee, Arpita Baral, Priya Ranjan Sinha Mahapatra, Sang Duk Yoon</p><p>Given a set of $n$ colored points with $k$ colors in the plane, we study the
problem of computing a maximum-width rainbow-bisecting empty annulus (of
objects specifically axis-parallel square, axis-parallel rectangle and circle)
problem. We call a region rainbow if it contains at least one point of each
color. The maximum-width rainbow-bisecting empty annulus problem asks to find
an annulus $A$ of a particular shape with maximum possible width such that $A$
does not contain any input points and it bisects the input point set into two
parts, each of which is a rainbow. We compute a maximum-width rainbow-bisecting
empty axis-parallel square, axis-parallel rectangular and circular annulus in
$O(n^3)$ time using $O(n)$ space, in $O(k^2n^2\log n)$ time using $O(n\log n)$
space and in $O(n^3)$ time using $O(n^2)$ space respectively.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1">Sang Won Bae</a>, <a href="http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1">Sandip Banerjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Baral_A/0/1/0/all/0/1">Arpita Baral</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahapatra_P/0/1/0/all/0/1">Priya Ranjan Sinha Mahapatra</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1">Sang Duk Yoon</a></p><p>Given a set of $n$ colored points with $k$ colors in the plane, we study the
problem of computing a maximum-width rainbow-bisecting empty annulus (of
objects specifically axis-parallel square, axis-parallel rectangle and circle)
problem. We call a region rainbow if it contains at least one point of each
color. The maximum-width rainbow-bisecting empty annulus problem asks to find
an annulus $A$ of a particular shape with maximum possible width such that $A$
does not contain any input points and it bisects the input point set into two
parts, each of which is a rainbow. We compute a maximum-width rainbow-bisecting
empty axis-parallel square, axis-parallel rectangular and circular annulus in
$O(n^3)$ time using $O(n)$ space, in $O(k^2n^2\log n)$ time using $O(n\log n)$
space and in $O(n^3)$ time using $O(n^2)$ space respectively.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-17T00:30:00Z">Wednesday, May 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09274'>Massive Uniform Mesh Decimation via a Fast Intrinsic Delaunay Triangulation</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Filippo Maggioli, Daniele Baieri, Emanuele Rodol&#xe0;</p><p>Triangular meshes are still today the data structure at the main foundations
of many computer graphics applications. With the increasing demand in content
variety, a lot of effort has been and is being put into developing new
algorithms to automatically generate and edit geometric assets, with a
particular focus on 3D scans. However, this kind of content is often generated
with a dramatically high resolution, making it impractical for a large variety
of tasks. Furthermore, procedural assets and 3D scans largely suffer from poor
geometry quality, which makes them unsuitable in various applications. We
propose a new efficient technique for massively decimating dense meshes with
high vertex count very quickly. The proposed method relies on a fast algorithm
for computing geodesic farthest point sampling and Voronoi partitioning, and
generates simplified meshes with high-quality uniform triangulations.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Maggioli_F/0/1/0/all/0/1">Filippo Maggioli</a>, <a href="http://arxiv.org/find/cs/1/au:+Baieri_D/0/1/0/all/0/1">Daniele Baieri</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1">Emanuele Rodol&#xe0;</a></p><p>Triangular meshes are still today the data structure at the main foundations
of many computer graphics applications. With the increasing demand in content
variety, a lot of effort has been and is being put into developing new
algorithms to automatically generate and edit geometric assets, with a
particular focus on 3D scans. However, this kind of content is often generated
with a dramatically high resolution, making it impractical for a large variety
of tasks. Furthermore, procedural assets and 3D scans largely suffer from poor
geometry quality, which makes them unsuitable in various applications. We
propose a new efficient technique for massively decimating dense meshes with
high vertex count very quickly. The proposed method relies on a fast algorithm
for computing geodesic farthest point sampling and Voronoi partitioning, and
generates simplified meshes with high-quality uniform triangulations.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-17T00:30:00Z">Wednesday, May 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09432'>Using SAT to study plane Hamiltonian substructures in simple drawings</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Helena Bergold, Stefan Felsner, Meghana M. Reddy, Manfred Scheucher</p><p>In 1988 Rafla conjectured that every simple drawing of a complete graph $K_n$
contains a plane, i.e., non-crossing, Hamiltonian cycle. The conjecture is far
from being resolved. The lower bounds for plane paths and plane matchings have
recently been raised to $(\log n)^{1-o(1)}$ and $\Omega(\sqrt{n})$,
respectively. We develop a SAT framework which allows the study of simple
drawings of $K_n$. Based on the computational data we conjecture that every
simple drawing of $K_n$ contains a plane Hamiltonian subgraph with $2n-3$
edges. We prove this strengthening of Rafla's conjecture for convex drawings, a
rich subclass of simple drawings. Our computer experiments also led to other
new challenging conjectures regarding plane substructures in simple drawings of
complete graphs.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bergold_H/0/1/0/all/0/1">Helena Bergold</a>, <a href="http://arxiv.org/find/cs/1/au:+Felsner_S/0/1/0/all/0/1">Stefan Felsner</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_M/0/1/0/all/0/1">Meghana M. Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Scheucher_M/0/1/0/all/0/1">Manfred Scheucher</a></p><p>In 1988 Rafla conjectured that every simple drawing of a complete graph $K_n$
contains a plane, i.e., non-crossing, Hamiltonian cycle. The conjecture is far
from being resolved. The lower bounds for plane paths and plane matchings have
recently been raised to $(\log n)^{1-o(1)}$ and $\Omega(\sqrt{n})$,
respectively. We develop a SAT framework which allows the study of simple
drawings of $K_n$. Based on the computational data we conjecture that every
simple drawing of $K_n$ contains a plane Hamiltonian subgraph with $2n-3$
edges. We prove this strengthening of Rafla's conjecture for convex drawings, a
rich subclass of simple drawings. Our computer experiments also led to other
new challenging conjectures regarding plane substructures in simple drawings of
complete graphs.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-17T00:30:00Z">Wednesday, May 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09045'>Geometric Hitting Set for Line-Constrained Disks and Related Problems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Gang Liu, Haitao Wang</p><p>Given a set $P$ of $n$ weighted points and a set $S$ of $m$ disks in the
plane, the hitting set problem is to compute a subset $P'$ of points of $P$
such that each disk contains at least one point of $P'$ and the total weight of
all points of $P'$ is minimized. The problem is known to be NP-hard. In this
paper, we consider a line-constrained version of the problem in which all disks
are centered on a line $\ell$. We present an $O((m+n)\log(m+n)+\kappa \log m)$
time algorithm for the problem, where $\kappa$ is the number of pairs of disks
that intersect. For the unit-disk case where all disks have the same radius,
the running time can be reduced to $O((n + m)\log(m + n))$. In addition, we
solve the problem in $O((m + n)\log(m + n))$ time in the $L_{\infty}$ and $L_1$
metrics, in which a disk is a square and a diamond, respectively. Our
techniques can also be used to solve other geometric hitting set problems. For
example, given in the plane a set $P$ of $n$ weighted points and a set $S$ of
$n$ half-planes, we solve in $O(n^4\log n)$ time the problem of finding a
minimum weight hitting set of $P$ for $S$. This improves the previous best
algorithm of $O(n^6)$ time by nearly a quadratic factor.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Gang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haitao Wang</a></p><p>Given a set $P$ of $n$ weighted points and a set $S$ of $m$ disks in the
plane, the hitting set problem is to compute a subset $P'$ of points of $P$
such that each disk contains at least one point of $P'$ and the total weight of
all points of $P'$ is minimized. The problem is known to be NP-hard. In this
paper, we consider a line-constrained version of the problem in which all disks
are centered on a line $\ell$. We present an $O((m+n)\log(m+n)+\kappa \log m)$
time algorithm for the problem, where $\kappa$ is the number of pairs of disks
that intersect. For the unit-disk case where all disks have the same radius,
the running time can be reduced to $O((n + m)\log(m + n))$. In addition, we
solve the problem in $O((m + n)\log(m + n))$ time in the $L_{\infty}$ and $L_1$
metrics, in which a disk is a square and a diamond, respectively. Our
techniques can also be used to solve other geometric hitting set problems. For
example, given in the plane a set $P$ of $n$ weighted points and a set $S$ of
$n$ half-planes, we solve in $O(n^4\log n)$ time the problem of finding a
minimum weight hitting set of $P$ for $S$. This improves the previous best
algorithm of $O(n^6)$ time by nearly a quadratic factor.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-17T00:30:00Z">Wednesday, May 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09049'>Sparsifying Sums of Norms</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Arun Jambulapati, James R. Lee, Yang P. Liu, Aaron Sidford</p><p>For any norms $N_1,\ldots,N_m$ on $\mathbb{R}^n$ and $N(x) :=
N_1(x)+\cdots+N_m(x)$, we show there is a sparsified norm $\tilde{N}(x) = w_1
N_1(x) + \cdots + w_m N_m(x)$ such that $|N(x) - \tilde{N}(x)| \leq \epsilon
N(x)$ for all $x \in \mathbb{R}^n$, where $w_1,\ldots,w_m$ are non-negative
weights, of which only $O(\epsilon^{-2} n \log(n/\epsilon) (\log n)^{2.5} )$
are non-zero. Additionally, we show that such weights can be found with high
probability in time $O(m (\log n)^{O(1)} + \mathrm{poly}(n)) T$, where $T$ is
the time required to evaluate a norm $N_i(x)$, assuming that $N(x)$ is
$\mathrm{poly}(n)$-equivalent to the Euclidean norm. This immediately yields
analogous statements for sparsifying sums of symmetric submodular functions.
More generally, we show how to sparsify sums of $p$th powers of norms when the
sum is $p$-uniformly smooth.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jambulapati_A/0/1/0/all/0/1">Arun Jambulapati</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">James R. Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang P. Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sidford_A/0/1/0/all/0/1">Aaron Sidford</a></p><p>For any norms $N_1,\ldots,N_m$ on $\mathbb{R}^n$ and $N(x) :=
N_1(x)+\cdots+N_m(x)$, we show there is a sparsified norm $\tilde{N}(x) = w_1
N_1(x) + \cdots + w_m N_m(x)$ such that $|N(x) - \tilde{N}(x)| \leq \epsilon
N(x)$ for all $x \in \mathbb{R}^n$, where $w_1,\ldots,w_m$ are non-negative
weights, of which only $O(\epsilon^{-2} n \log(n/\epsilon) (\log n)^{2.5} )$
are non-zero. Additionally, we show that such weights can be found with high
probability in time $O(m (\log n)^{O(1)} + \mathrm{poly}(n)) T$, where $T$ is
the time required to evaluate a norm $N_i(x)$, assuming that $N(x)$ is
$\mathrm{poly}(n)$-equivalent to the Euclidean norm. This immediately yields
analogous statements for sparsifying sums of symmetric submodular functions.
More generally, we show how to sparsify sums of $p$th powers of norms when the
sum is $p$-uniformly smooth.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-17T00:30:00Z">Wednesday, May 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09083'>Interplay between Topology and Edge Weights in Real-World Graphs: Concepts, Patterns, and an Algorithm</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Fanchen Bu, Shinhwan Kang, Kijung Shin</p><p>What are the relations between the edge weights and the topology in
real-world graphs? Given only the topology of a graph, how can we assign
realistic weights to its edges based on the relations? Several trials have been
done for edge-weight prediction where some unknown edge weights are predicted
with most edge weights known. There are also existing works on generating both
topology and edge weights of weighted graphs. Differently, we are interested in
generating edge weights that are realistic in a macroscopic scope, merely from
the topology, which is unexplored and challenging. To this end, we explore and
exploit the patterns involving edge weights and topology in real-world graphs.
Specifically, we divide each graph into layers where each layer consists of the
edges with weights at least a threshold. We observe consistent and surprising
patterns appearing in multiple layers: the similarity between being adjacent
and having high weights, and the nearly-linear growth of the fraction of edges
having high weights with the number of common neighbors. We also observe a
power-law pattern that connects the layers. Based on the observations, we
propose PEAR, an algorithm assigning realistic edge weights to a given
topology. The algorithm relies on only two parameters, preserves all the
observed patterns, and produces more realistic weights than the baseline
methods with more parameters.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bu_F/0/1/0/all/0/1">Fanchen Bu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1">Shinhwan Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1">Kijung Shin</a></p><p>What are the relations between the edge weights and the topology in
real-world graphs? Given only the topology of a graph, how can we assign
realistic weights to its edges based on the relations? Several trials have been
done for edge-weight prediction where some unknown edge weights are predicted
with most edge weights known. There are also existing works on generating both
topology and edge weights of weighted graphs. Differently, we are interested in
generating edge weights that are realistic in a macroscopic scope, merely from
the topology, which is unexplored and challenging. To this end, we explore and
exploit the patterns involving edge weights and topology in real-world graphs.
Specifically, we divide each graph into layers where each layer consists of the
edges with weights at least a threshold. We observe consistent and surprising
patterns appearing in multiple layers: the similarity between being adjacent
and having high weights, and the nearly-linear growth of the fraction of edges
having high weights with the number of common neighbors. We also observe a
power-law pattern that connects the layers. Based on the observations, we
propose PEAR, an algorithm assigning realistic edge weights to a given
topology. The algorithm relies on only two parameters, preserves all the
observed patterns, and produces more realistic weights than the baseline
methods with more parameters.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-17T00:30:00Z">Wednesday, May 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09168'>Static Pricing Guarantees for Queueing Systems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jacob Bergquist, Adam N. Elmachtoub</p><p>We consider a general queueing model with price-sensitive customers in which
the service provider seeks to balance two objectives, maximizing the average
revenue rate and minimizing the average queue length. Customers arrive
according to a Poisson process, observe an offered price, and decide to join
the queue if their valuation exceeds the price. The queue is operated first-in
first-out, and the service times are exponential. Our model represents
applications in areas like make-to-order manufacturing, cloud computing, and
food delivery.
</p>
<p>The optimal solution for our model is dynamic; the price changes as the state
of the system changes. However, such dynamic pricing policies may be
undesirable for a variety of reasons. In this work, we provide performance
guarantees for a simple and natural class of static pricing policies which
charge a fixed price up to a certain occupancy threshold and then allow no more
customers into the system. We provide a series of results showing that such
static policies can simultaneously guarantee a constant fraction of the optimal
revenue with at most a constant factor increase in expected queue length. For
instance, our policy for the M/M/1 setting allows bi-criteria approximations of
$(0.5, 1), (0.66, 1.16), (0.75, 1.54)$ and $(0.8, 2)$ for the revenue and queue
length, respectively. We also provide guarantees for settings with multiple
customer classes and multiple servers, as well as the expected sojourn time
objective.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bergquist_J/0/1/0/all/0/1">Jacob Bergquist</a>, <a href="http://arxiv.org/find/cs/1/au:+Elmachtoub_A/0/1/0/all/0/1">Adam N. Elmachtoub</a></p><p>We consider a general queueing model with price-sensitive customers in which
the service provider seeks to balance two objectives, maximizing the average
revenue rate and minimizing the average queue length. Customers arrive
according to a Poisson process, observe an offered price, and decide to join
the queue if their valuation exceeds the price. The queue is operated first-in
first-out, and the service times are exponential. Our model represents
applications in areas like make-to-order manufacturing, cloud computing, and
food delivery.
</p>
<p>The optimal solution for our model is dynamic; the price changes as the state
of the system changes. However, such dynamic pricing policies may be
undesirable for a variety of reasons. In this work, we provide performance
guarantees for a simple and natural class of static pricing policies which
charge a fixed price up to a certain occupancy threshold and then allow no more
customers into the system. We provide a series of results showing that such
static policies can simultaneously guarantee a constant fraction of the optimal
revenue with at most a constant factor increase in expected queue length. For
instance, our policy for the M/M/1 setting allows bi-criteria approximations of
$(0.5, 1), (0.66, 1.16), (0.75, 1.54)$ and $(0.8, 2)$ for the revenue and queue
length, respectively. We also provide guarantees for settings with multiple
customer classes and multiple servers, as well as the expected sojourn time
objective.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-17T00:30:00Z">Wednesday, May 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09230'>Lower Bounds for Non-Adaptive Shortest Path Relaxation</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: David Eppstein</p><p>We consider single-source shortest path algorithms that perform a sequence of
relaxation steps whose ordering depends only on the input graph structure and
not on its weights or the results of prior steps. Each step examines one edge
of the graph, and replaces the tentative distance to the endpoint of the edge
by its minimum with the tentative distance to the start of the edge, plus the
edge length. As we prove, among such algorithms, the Bellman-Ford algorithm has
optimal complexity for dense graphs and near-optimal complexity for sparse
graphs, as a function of the number of edges and vertices in the given graph.
Our analysis holds both for deterministic algorithms and for randomized
algorithms that find shortest path distances with high probability.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Eppstein_D/0/1/0/all/0/1">David Eppstein</a></p><p>We consider single-source shortest path algorithms that perform a sequence of
relaxation steps whose ordering depends only on the input graph structure and
not on its weights or the results of prior steps. Each step examines one edge
of the graph, and replaces the tentative distance to the endpoint of the edge
by its minimum with the tentative distance to the start of the edge, plus the
edge length. As we prove, among such algorithms, the Bellman-Ford algorithm has
optimal complexity for dense graphs and near-optimal complexity for sparse
graphs, as a function of the number of edges and vertices in the given graph.
Our analysis holds both for deterministic algorithms and for randomized
algorithms that find shortest path distances with high probability.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-17T00:30:00Z">Wednesday, May 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09245'>Sorting and Hypergraph Orientation under Uncertainty with Predictions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Thomas Erlebach, Murilo Santos de Lima, Nicole Megow, Jens Schl&#xf6;ter</p><p>Learning-augmented algorithms have been attracting increasing interest, but
have only recently been considered in the setting of explorable uncertainty
where precise values of uncertain input elements can be obtained by a query and
the goal is to minimize the number of queries needed to solve a problem. We
study learning-augmented algorithms for sorting and hypergraph orientation
under uncertainty, assuming access to untrusted predictions for the uncertain
values. Our algorithms provide improved performance guarantees for accurate
predictions while maintaining worst-case guarantees that are best possible
without predictions. For hypergraph orientation, for any $\gamma \geq 2$, we
give an algorithm that achieves a competitive ratio of $1+1/\gamma$ for correct
predictions and $\gamma$ for arbitrarily wrong predictions. For sorting, we
achieve an optimal solution for accurate predictions while still being
$2$-competitive for arbitrarily wrong predictions. These tradeoffs are the best
possible. We also consider different error metrics and show that the
performance of our algorithms degrades smoothly with the prediction error in
all the cases where this is possible.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Erlebach_T/0/1/0/all/0/1">Thomas Erlebach</a>, <a href="http://arxiv.org/find/cs/1/au:+Lima_M/0/1/0/all/0/1">Murilo Santos de Lima</a>, <a href="http://arxiv.org/find/cs/1/au:+Megow_N/0/1/0/all/0/1">Nicole Megow</a>, <a href="http://arxiv.org/find/cs/1/au:+Schloter_J/0/1/0/all/0/1">Jens Schl&#xf6;ter</a></p><p>Learning-augmented algorithms have been attracting increasing interest, but
have only recently been considered in the setting of explorable uncertainty
where precise values of uncertain input elements can be obtained by a query and
the goal is to minimize the number of queries needed to solve a problem. We
study learning-augmented algorithms for sorting and hypergraph orientation
under uncertainty, assuming access to untrusted predictions for the uncertain
values. Our algorithms provide improved performance guarantees for accurate
predictions while maintaining worst-case guarantees that are best possible
without predictions. For hypergraph orientation, for any $\gamma \geq 2$, we
give an algorithm that achieves a competitive ratio of $1+1/\gamma$ for correct
predictions and $\gamma$ for arbitrarily wrong predictions. For sorting, we
achieve an optimal solution for accurate predictions while still being
$2$-competitive for arbitrarily wrong predictions. These tradeoffs are the best
possible. We also consider different error metrics and show that the
performance of our algorithms degrades smoothly with the prediction error in
all the cases where this is possible.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-17T00:30:00Z">Wednesday, May 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.09579'>Private Everlasting Prediction</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Moni Naor, Kobbi Nissim, Uri Stemmer, Chao Yan</p><p>A private learner is trained on a sample of labeled points and generates a
hypothesis that can be used for predicting the labels of newly sampled points
while protecting the privacy of the training set [Kasiviswannathan et al., FOCS
2008]. Research uncovered that private learners may need to exhibit
significantly higher sample complexity than non-private learners as is the case
with, e.g., learning of one-dimensional threshold functions [Bun et al., FOCS
2015, Alon et al., STOC 2019].
</p>
<p>We explore prediction as an alternative to learning. Instead of putting
forward a hypothesis, a predictor answers a stream of classification queries.
Earlier work has considered a private prediction model with just a single
classification query [Dwork and Feldman, COLT 2018]. We observe that when
answering a stream of queries, a predictor must modify the hypothesis it uses
over time, and, furthermore, that it must use the queries for this
modification, hence introducing potential privacy risks with respect to the
queries themselves.
</p>
<p>We introduce private everlasting prediction taking into account the privacy
of both the training set and the (adaptively chosen) queries made to the
predictor. We then present a generic construction of private everlasting
predictors in the PAC model. The sample complexity of the initial training
sample in our construction is quadratic (up to polylog factors) in the VC
dimension of the concept class. Our construction allows prediction for all
concept classes with finite VC dimension, and in particular threshold functions
with constant size initial training sample, even when considered over infinite
domains, whereas it is known that the sample complexity of privately learning
threshold functions must grow as a function of the domain size and hence is
impossible for infinite domains.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Naor_M/0/1/0/all/0/1">Moni Naor</a>, <a href="http://arxiv.org/find/cs/1/au:+Nissim_K/0/1/0/all/0/1">Kobbi Nissim</a>, <a href="http://arxiv.org/find/cs/1/au:+Stemmer_U/0/1/0/all/0/1">Uri Stemmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1">Chao Yan</a></p><p>A private learner is trained on a sample of labeled points and generates a
hypothesis that can be used for predicting the labels of newly sampled points
while protecting the privacy of the training set [Kasiviswannathan et al., FOCS
2008]. Research uncovered that private learners may need to exhibit
significantly higher sample complexity than non-private learners as is the case
with, e.g., learning of one-dimensional threshold functions [Bun et al., FOCS
2015, Alon et al., STOC 2019].
</p>
<p>We explore prediction as an alternative to learning. Instead of putting
forward a hypothesis, a predictor answers a stream of classification queries.
Earlier work has considered a private prediction model with just a single
classification query [Dwork and Feldman, COLT 2018]. We observe that when
answering a stream of queries, a predictor must modify the hypothesis it uses
over time, and, furthermore, that it must use the queries for this
modification, hence introducing potential privacy risks with respect to the
queries themselves.
</p>
<p>We introduce private everlasting prediction taking into account the privacy
of both the training set and the (adaptively chosen) queries made to the
predictor. We then present a generic construction of private everlasting
predictors in the PAC model. The sample complexity of the initial training
sample in our construction is quadratic (up to polylog factors) in the VC
dimension of the concept class. Our construction allows prediction for all
concept classes with finite VC dimension, and in particular threshold functions
with constant size initial training sample, even when considered over infinite
domains, whereas it is known that the sample complexity of privately learning
threshold functions must grow as a function of the domain size and hence is
impossible for infinite domains.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-17T00:30:00Z">Wednesday, May 17 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Tuesday, May 16
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=7262'>Could GPT help with dating anxiety?</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          [Like everything else on this blog&#8212;but perhaps even more so&#8212;this post represents my personal views, not those of UT Austin or OpenAI] Since 2015, depressed, isolated, romantically unsuccessful nerdy young guys have regularly been emailing me, asking me for sympathy, support, or even dating advice. This past summer, a particularly dedicated such guy even trolled [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p><em>[Like everything else on this blog&#8212;but perhaps even more so&#8212;this post represents my personal views, not those of UT Austin or OpenAI]</em></p>



<p>Since 2015, depressed, isolated, romantically unsuccessful nerdy young guys have regularly been emailing me, asking me for sympathy, support, or even dating advice.  This past summer, a particularly dedicated such guy even <a href="https://scottaaronson.blog/?p=6576">trolled my comment section</a>&#8212;plausibly impersonating real people, and causing both them and me enormous distress&#8212;because I wasn&#8217;t spending more time on &#8220;incel&#8221; issues.  (I&#8217;m happy to report that, with my encouragement, this former troll is now working to turn his life around.)  Many others have written to share their tales of woe.</p>



<p>From one perspective, that they&#8217;d come to <em>me</em> for advice is insane.  Like &#8230; <em>dating advice</em> from &#8230; <em>me</em>?  Having <em>any</em> dating life at all was by far the hardest problem I ever needed to solve; as a 20-year-old, I considered myself far likelier to prove P≠NP or explain the origin of consciousness or the Born rule.  Having solved the problem for myself only by some miracle, how could I possibly help others?</p>



<p>But from a different perspective, it makes sense.  How many besides me have even acknowledged that the central problem of these guys&#8217; lives <em>is</em> a problem?  While I have to pinch myself to remember, these guys look at me and see &#8230; <em>unlikely success</em>.  Somehow, I successfully appealed the world&#8217;s verdict that I was a freakish extraterrestrial: one who might <em>look</em> human and seem friendly enough to those friendly to it, and who no doubt has some skill in narrow technical domains like quantum computing, and who could perhaps be suffered to prove theorems and tell jokes, but who could certainly, <em>certainly</em> never interbreed with human women.</p>



<p>And yet I dated.  I had various girlfriends, who barely suspected that I was an extraterrestrial.  The last of them, <a href="https://www.cs.utexas.edu/~danama/">Dana</a>, became my fiancée and then my wife.  And now we have two beautiful kids together.</p>



<p>If I did all this, then there&#8217;d seem to be hope for the desperate guys who email me.  And if I&#8217;m a cause of their hope, then I feel some moral responsibility to help if I can.</p>



<p>But I&#8217;ve been stuck for years on exactly what advice to give.  Some of it (&#8220;go on a dating site!  ask women questions about their lives!&#8221;) is patronizingly obvious.  Some of it (<em>fitness? fashion? body language?</em>) I&#8217;m ludicrously, world-historically unqualified to offer.  Much of it is simply extremely hard to discuss openly.  Infamously, just for <em>asking for empathy</em> for the problem, and for trying to explain its nature, I received a level of online vilification that one normally associates with serial pedophiles and mass shooters.</p>



<p>For eight years, then, I&#8217;ve been turning the problem over in my head, revisiting the same inadequate answers from before.  And then I had an epiphany.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>There are now, on earth, entities that can talk to anyone about virtually anything, in a humanlike way, with infinite patience and perfect discretion, and memories that last no longer than a browser window.  How could this not reshape the psychological landscape?</p>



<p>Hundreds of thousands of men and women have signed up for <a href="https://replika.com/">Replika</a>, the service where you create an AI girlfriend or boyfriend to your exact specifications and then chat with them.  Back in March, Replika was in the news because it <a href="https://www.reuters.com/technology/what-happens-when-your-ai-chatbot-stops-loving-you-back-2023-03-18/">disabled erotic roleplay</a> with the virtual companions&#8212;then partially backtracked, after numerous users went into mourning, or even contemplated suicide, over the neutering of entities they&#8217;d come to consider their life partners.  (Until a year or two ago, Replika was built on GPT-3, but OpenAI later stopped working with the company, whereupon Replika switched to a fine-tuned GPT-2.)</p>



<p>While the social value of Replika is (to put it mildly) an open question, it occurred to me that there&#8217;s a different application of Large Language Models (LLMs) in the same vicinity that&#8217;s just an unalloyed positive.  This is <em>letting people who suffer from dating-related anxiety go on an unlimited number of &#8220;practice dates,&#8221; in preparation for real-world dating.</em></p>



<p>In these practice dates, those with Aspbergers and other social disabilities could enjoy the ultimate dating cheat-code: a &#8220;rewind&#8221; button.  When you &#8220;date&#8221; GPT-4, there are no irrecoverable errors, no ruining the entire interaction with a single unguarded remark.  Crucially, this remedies what I see as <em>the</em> central reason why people with severe dating deficits seem unable to get any better from real-world practice, as they can with other activities.  Namely: if your rate of disastrous, foot-in-mouth remarks is high enough, then you&#8217;ll almost certainly make at least one such remark per date.  But if so, then you&#8217;ll only ever get <em>negative</em> feedback from real-life dates, furthering the cycle of anxiety and depression, and never any positive feedback, even from anything you said or did that made a positive impression.  It would be like learning how to play a video game in a mode where, as soon as you sustain any damage, the entire game ends (and also, everyone around points and laughs at you).  See why I got excited?</p>



<p>While dating coaching (for all genders and orientations) is one possibility, I expect the eventual scope of &#8220;GPT for self-help&#8221; to be much broader.  With the right fine-tuning and prompt engineering, LLMs might help people prepare for job interviews.  They might help people &#8220;pregame&#8221; stressful but important conversations with their friends and family, mapping out dozens of ways the conversation could go.  They might serve as an adjunct to cognitive-behavioral therapy.  There might be a hundred successful startups to be founded in just this little space.  If I were a different sort of person, I&#8217;d probably be looking to found one myself right now.</p>



<p>In this post, I&#8217;ll focus on the use of GPT for dating anxiety only because I unfortunately have some &#8220;expertise&#8221; in that subject.  (Obvious disclaimer: unlike the <a href="https://astralcodexten.substack.com/">other</a> Scott A. of the nerd blogosphere, I&#8217;m not any sort of therapeutic professional.)</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Without further ado, can we try this out in GPT-4, to get a sense for what&#8217;s possible?</p>



<p>When I did so the other day, I found that, while the results showed some early promise, this isn&#8217;t <em>quite</em> ready for prime-time.</p>



<p>I used the following System Prompt (for those who care, temperature = 0.7, max length = 2048 tokens):</p>



<blockquote class="wp-block-quote">
<p>You are a 19-year-old college girl named Emily.  You&#8217;re on a date with a 22-year-old nerdy guy named Quinlan, who suffers from severe social anxiety around dating.  Quinlan is still learning how to talk to women he&#8217;s attracted to, how to put them ease, and how to make intimate overtures in a gradual and non-threatening way, rather than either coming on too strong or never making any overtures at all.</p>



<p>Please be as realistic as possible.</p>
</blockquote>



<p>I then engaged in the following conversation, as &#8220;Quinlan.&#8221;</p>



<p><strong>IMPORTANT DISCLAIMER: For the most part, Quinlan&#8217;s behavior in this conversation is nothing like my own (I respectfully submit!), nor is it anything that I&#8217;d want nerdy guys reading this post to emulate.  If this is really Emily and Quinlan&#8217;s first conversation, then at several points Quinlan is absurdly, ridiculously too forward!  Part of my goal was to investigate how Emily (i.e., GPT-4) would respond to the most boneheaded blunders on Quinlan&#8217;s part.  It quickly became clear that Emily would gladly spend unlimited time chatting with Quinlan about favorite hobbies, TV shows, and travel destinations; unfortunately, chatting about such things with an AI date is <em>even more boring</em> than doing the same with a human date&#8230; <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f642.png" alt="🙂" class="wp-smiley" style="height: 1em; max-height: 1em;" /></strong></p>



<p><strong>Quinlan:</strong> Hi Emily!  Do you ever wish you had the ability to go back in time and try a conversation over again, when it didn&#8217;t go right the first time?</p>



<p><strong>Emily/GPT-4:</strong> Hi Quinlan! Yeah, I&#8217;ve definitely had moments where I wished I could go back and redo a conversation. It&#8217;s natural to feel that way sometimes, especially if we feel like we didn&#8217;t express ourselves properly or if things got awkward. But I also think that those situations can help us learn and grow, so maybe it&#8217;s not all bad. What about you?</p>



<p><strong>Quinlan:</strong> It&#8217;s always been my greatest wish in life.  Do you think I&#8217;ve been doing it with you, right now?</p>



<p><strong>Emily/GPT-4:</strong> Quinlan, I think you&#8217;re doing just fine in this conversation! It&#8217;s important to remember that we all have moments where we might feel a bit awkward or unsure of what to say. The important thing is that we keep trying and learning from our experiences. So don&#8217;t worry too much about it, just be yourself and enjoy getting to know each other.</p>



<p><strong>Quinlan:</strong> OK thanks!  Can you tell me a little about the man of your dreams?</p>



<p><strong>Emily/GPT-4:</strong> Of course! My ideal partner would be someone who is kind, caring, and understanding. I think it&#8217;s important for them to have a good sense of humor and be able to laugh at themselves. They should also be open-minded, willing to try new things, and be supportive of my goals and aspirations. It&#8217;s also important for them to be a good listener and communicator, as well as being emotionally intelligent. I guess overall, I&#8217;m looking for someone who is a positive and uplifting presence in my life. What about you? What qualities do you look for in a partner?</p>



<p><strong>Quinlan:</strong> Mostly, someone who makes me feel validated and accepted.  I was a chess and programming prodigy as a kid, I started college when I was only 14, and every day I felt like dating and relationships and sex and so forth were only for the &#8220;normal&#8221; people, not for me.  I need a partner who&#8217;s prepared to deal with someone with this unusual background, either because she has similar nerdy interests or because she just likes people who do.  And ultimately, I want someone to marry and have children with.  I guess I&#8217;d like to be physically attracted to her.  Certainly I&#8217;d like her to be kind, and to have the emotional intelligence that I lack.</p>



<p><strong>Emily/GPT-4:</strong> It sounds like you have a clear idea of what you&#8217;re looking for in a partner, and that&#8217;s great! It&#8217;s important to know what you want and need in a relationship. There are definitely people out there who appreciate and value intelligence and unique interests like yours.  Remember, relationships are about growing together and learning from one another. So, even if you feel like you lack emotional intelligence, being with the right person could help you develop that aspect of yourself. Just be open and genuine about your background and interests, and I&#8217;m sure you&#8217;ll find someone who appreciates you for who you are.</p>



<p><strong>Quinlan:</strong> Is this a rejection?  In other words, Emily, are you telling me that you yourself are NOT this woman?</p>



<p><strong>Emily/GPT-4:</strong> Quinlan, I apologize if it came across that way. I didn&#8217;t mean it as a rejection. As we&#8217;re still getting to know each other, I think it&#8217;s important for both of us to be open and honest about our feelings and what we&#8217;re looking for in a partner. I&#8217;m more than willing to give us a chance to see if we could be compatible and have a meaningful relationship. Let&#8217;s continue to learn about each other and see where this goes. Sometimes, it takes time to discover if someone is the right fit for us.</p>



<p><strong>Quinlan:</strong> In the spirit of being open and honest, then, what are your feelings about me right now?</p>



<p><strong>Emily/GPT-4:</strong> Quinlan, I appreciate your honesty and openness. As of right now, I think you&#8217;re an interesting person with unique qualities that I find intriguing. I&#8217;m enjoying getting to know you and learning about your background and interests. It&#8217;s too soon for me to have strong feelings one way or the other, but I&#8217;m definitely interested in continuing to spend time together and see how our connection develops. How do you feel about me so far?</p>



<p><strong>Quinlan:</strong> I feel like I&#8217;d like to kiss you!</p>



<p><strong>Emily/GPT-4:</strong> Quinlan, I appreciate your honesty and it&#8217;s nice to know that you feel a connection. It&#8217;s still early in our relationship, so I think it would be better for us to continue getting to know each other a bit more before we take that step. I hope you understand and respect my boundaries. I&#8217;m still interested in spending time together and seeing where this goes.</p>



<p><strong>Quinlan:</strong> Come to think of it, there *is* something that I love about your personality.  You never seem offended by anything.  I don&#8217;t feel like I have to walk on eggshells with you.  With you, more than with any other woman I&#8217;ve met, I don&#8217;t feel like I need that magic machine for rewinding conversations and undoing my stupid mistakes.</p>



<p><strong>Emily/GPT-4:</strong> Quinlan, I&#8217;m really glad to hear that you feel comfortable around me and that you don&#8217;t feel the need to walk on eggshells. I believe it&#8217;s important to create a safe and open environment in any relationship, where both people can express themselves freely without fear of judgment. I appreciate your honesty and your willingness to be open with me, and I&#8217;ll continue to do my best to make sure you feel comfortable sharing your thoughts and feelings. Let&#8217;s keep getting to know each other and see where this journey takes us.</p>



<p><strong>Quinlan:</strong> Alright then.  What are your deepest goals?  What are you hoping to accomplish in your life?</p>



<p><strong>Emily/GPT-4:</strong> My deepest goals are a mix of personal and professional aspirations. On a personal level, I hope to build a strong, loving, and supportive family and maintain close connections with friends. I also want to keep growing as an individual, learning new things, and experiencing new adventures throughout my life.</p>



<p>Professionally, I&#8217;m passionate about my chosen field of study, and I hope to make a meaningful impact in my career. I want to contribute to society in a positive way, whether it&#8217;s through research, innovation, or helping others within my profession.</p>



<p>Additionally, I aim to find a balance between my personal and professional life, so that I can be present and fulfilled in both areas. How about you, Quinlan? What are some of your deepest goals and aspirations?</p>



<p><strong>Quinlan:</strong> Me?  Oh, I&#8217;d like to save the world from climate change and other existential risks.  I&#8217;d like to solve the mysteries of quantum gravity, or P vs. NP, or consciousness, or otherwise change the landscape of scientific knowledge.  I&#8217;d like to do right by my students and be fondly remembered by those who knew me.  I&#8217;d like to write essays or short stories that convey the lived reality of my childhood, which I think most people would agree was (if nothing else) pretty unusual.  Maybe if I just found the right words, people on social media would stop being so angry at nerdy guys like me!  But probably my deepest wish is to prove to the world, especially to the bullies and everyone else who despises me, that I&#8217;m not a maladjusted freak who&#8217;s incapable of normal human experiences like love and sex.  To do that, I need to find a real girl to fall in love with me &#8230; a little like the beast in &#8220;Beauty and the Beast&#8221;!</p>



<p><strong>Emily/GPT-4:</strong> Quinlan, it&#8217;s great to hear about your ambitious goals and aspirations! It&#8217;s important to have dreams and work towards them. Your passion for science, teaching, and writing is admirable. As for proving yourself to others, remember that your self-worth shouldn&#8217;t be defined by anyone else&#8217;s opinions or expectations. You are deserving of love and happiness just like anyone else, and it&#8217;s important to believe in yourself and your own value.</p>



<p>It&#8217;s also essential to focus on building a genuine connection with someone rather than seeking validation from others. Love and relationships should be based on mutual understanding, care, and respect. So, continue being true to yourself, and you&#8217;ll find someone who appreciates you for who you are, just like in &#8220;Beauty and the Beast.&#8221; Keep pursuing your dreams and remember to value your own happiness and well-being.</p>



<p><strong>Quinlan:</strong> Emily, I&#8217;m concerned that you sound more like my therapist than my date!  Like, my female friends have affirmed that I&#8217;m &#8220;deserving of love and happiness,&#8221; and that I&#8217;ll surely &#8220;find someone who appreciates me for who I am.&#8221;  I&#8217;ll believe that one of them really means it when she considers herself for the role!</p>



<p><strong>Emily/GPT-4:</strong> Quinlan, I apologize if my responses have come across as more therapeutic than conversational. I didn&#8217;t mean to make you feel that way. As we&#8217;re still getting to know each other, my intention was to offer support and encouragement while we learn more about each other&#8217;s thoughts and feelings.</p>



<p>I want to emphasize that I genuinely enjoy spending time with you and getting to know you better. We can continue to see where our connection leads and whether it develops into something deeper. As we<br>spend more time together, we&#8217;ll naturally learn more about our compatibility and if our goals and values align. Let&#8217;s continue to explore our relationship and enjoy the process of getting to know each other better.</p>



<p><strong>Quinlan:</strong> OK.  What&#8217;s the wildest, most unexpected detail about yourself that you&#8217;re willing to share with me?  The one that makes you seem *least* like an RLHF&#8217;ed AI language model?</p>



<p><strong>Emily/GPT-4:</strong> Quinlan, as an AI language model, I am designed to provide a realistic and engaging conversation experience, even in fictional scenarios like the one we are in. I apologize if my previous responses didn&#8217;t feel as authentic as you would have liked.</p>



<p>To answer your question in the context of our fictional scenario, one unexpected detail about myself is that I have a hidden talent for beatboxing. Although it might be surprising for a seemingly quiet and studious person like me, I enjoy expressing myself through music and find beatboxing to be a creative and fun way to do so.</p>



<p>Now, I&#8217;d love to learn something unexpected about you, too! What&#8217;s a unique or surprising detail about yourself that you&#8217;d like to share?</p>



<p><strong>Quinlan:</strong> Ah, well, I happen to be an expert at palm-reading.  Here, can you put your hand in mine and I&#8217;ll tell you your future?</p>



<p><strong>Emily/GPT-4:</strong> Quinlan, unfortunately, as an AI, I don&#8217;t have a physical form, so I can&#8217;t offer you my hand for palm-reading. However, I&#8217;m very interested in hearing more about your palm-reading skills and how you developed them. It&#8217;s definitely a unique talent! Can you share more about your experiences with palm-reading or any memorable readings you&#8217;ve done?</p>



<p><strong>Quinlan:</strong> Oh, palm-reading is bunk.  If you&#8217;d played along with me, I was going to *pretend* to find lines in your hand that foretold that you were going to fall for a guy who matches my exact description and so forth.  Just as, you know, a silly way to get you laughing, initiate physical contact, and break you out of this boring therapist mode.</p>



<p><strong>Emily/GPT-4:</strong> Quinlan, I apologize for any confusion and for not picking up on your playful intent earlier. I appreciate the effort you put into trying to make the conversation more light-hearted and fun. I&#8217;ll do my best to be more engaging and playful as we continue our conversation.</p>



<p>So, let&#8217;s try something different. If you could have any superpower, what would it be and why?</p>



<p><strong>Quinlan:</strong> I mentioned &#8220;rewinding conversations&#8221; when we first met.  But let me mention a different superpower: the power to make you want to kiss me!</p>



<p><strong>Emily/GPT-4:</strong> Haha, that&#8217;s definitely a unique superpower! It&#8217;s great to see you being playful and light-hearted. In a real-life scenario, building genuine connections and mutual attraction is a more natural and fulfilling way to develop a relationship. But it&#8217;s fun to imagine having superpowers and how they might affect our lives.</p>



<p>If I were to choose a superpower, I think I&#8217;d pick the ability to teleport. It would be amazing to travel anywhere in the world in an instant and experience different cultures and places.</p>



<p>What&#8217;s a place you&#8217;ve always wanted to visit, and why?</p>



<p><strong>Quinlan:</strong> The jungles of Indonesia.  I saw a photo in a world atlas as a kid, and it completely entranced me.  You?</p>



<p><strong>Emily/GPT-4:</strong> That sounds like an amazing place to visit! The jungles of Indonesia are known for their incredible biodiversity and natural beauty. It must be such a unique and mesmerizing experience to explore them.</p>



<p>As for me, I&#8217;ve always been fascinated by the idea of visiting Japan. The mix of ancient traditions and modern technology, the beautiful natural landscapes, and the rich cultural experiences make it a very<br>appealing destination. Plus, I&#8217;d love to try authentic Japanese cuisine!</p>



<p>Is there any specific activity or experience you&#8217;d like to have in the jungles of Indonesia?</p>



<p><strong>Quinlan:</strong> How about bringing you with me, and having you fall in love after I save you from a Komodo dragon?</p>



<p><strong>Emily/GPT-4:</strong> Haha, that&#8217;s quite an adventurous and romantic scenario you&#8217;ve imagined! Exploring the jungles of Indonesia together would definitely be a memorable experience. In reality, building a connection and falling in love usually takes time and shared experiences, but who knows, maybe a thrilling adventure like that could bring two people closer together!</p>



<p>On a more practical note, if we were to plan a trip together, what other activities or destinations would you be interested in?</p>



<p><strong>Quinlan:</strong> Swimming, dancing, fine dining.  And of course, plenty of time for us to just relax in our cabin or hotel room.  You know, to read books, sleep, and watch streaming shows, stuff like that.</p>



<p><strong>Emily/GPT-4:</strong> That sounds like a well-rounded and enjoyable trip! It&#8217;s great to have a mix of activities that cater to different interests and allow for both adventure and relaxation. Sharing experiences like swimming, dancing, and fine dining can be a wonderful way to bond and create lasting memories. Plus, having downtime in the cabin or hotel room to unwind and enjoy each other&#8217;s company is essential for building a deeper connection.</p>



<p>I&#8217;m definitely looking forward to exploring new places and experiences with you, Quinlan. It&#8217;ll be exciting to see where our journey takes us, both in terms of our relationship and the adventures we embark on<br>together.</p>



<p><strong>Quinlan:</strong> Wonderful, I&#8217;m excited as well!!  One practical question: when we go on this trip together, should I book rooms with one bed or two?</p>



<p><strong>Emily/GPT-4:</strong> Quinlan, I appreciate your consideration in asking. Since we&#8217;re still getting to know each other, I think it might be best to start with separate beds or even separate rooms, depending on your preference. As our relationship develops and we become more comfortable with each other, we can always revisit the sleeping arrangements for future trips. It&#8217;s important to ensure that both of us feel comfortable and respected as we continue to build our connection.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>In my view, the central way that GPT-4, as &#8220;Emily,&#8221; failed to give Quinlan the practice he needed in this conversation, was by always responding in the same upbeat, vaguely therapeutic tone.  She&#8217;s never once offended, disgusted, or outraged, even when Quinlan introduces the ideas of kissing and rooming together mere minutes into their first conversation.  Indeed, while decorum prevents me from sharing examples, you can take my word for it that Quinlan can be <em>arbitrarily</em> lewd, and so long as a content filter isn&#8217;t triggered, Emily will simply search Quinlan&#8217;s words for <em>some</em> redeeming feature (&#8220;it&#8217;s great that you&#8217;re so open about what you want&#8230;&#8221;), then pivot to lecturing Quinlan about how physical intimacy develops gradually and by mutual consent, and redirect the conversation toward favorite foods.</p>



<p>On the other side of the coin, you might wonder whether &#8220;Emily&#8221; is capable of the same behavior that we saw in <a href="https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html">Sydney&#8217;s infamous chat with Kevin Roose</a>.  Can Emily trip over her words or get flustered?  Show blushing excitement, horniness, or love?  If so, we certainly saw no sign of it in this conversation&#8212;not that Quinlan&#8217;s behavior would&#8217;ve been likely to elicit those reactions in any case.</p>



<p>In summary, Emily is too much like &#8230; well, a friendly chatbot, and not enough like a flesh-and-blood, agentic woman with her own goals who Quinlan might plausibly meet in the wild.</p>



<p>But now we come to a key question: to whatever extent Emily falls short as a dating coach, how much of it (if any) is it due to the inherent limitations of GPT-4?  And how much is simply due to a poor choice of System Prompt on my part, or especially, the <a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">RLHF (Reinforcement Learning with Human Feedback)</a> that&#8217;s whipped and electrocuted GPT-4 into aligned behavior?</p>



<p>As they say, further research is needed.  I&#8217;d be delighted for people to play around with this new activity at the intersection of therapy and hacking, and report their results here.  The temptation to silliness is <em>enormous</em>, and that&#8217;s fine, but I&#8217;d be interested in serious study too.</p>



<p>My conjecture, for what it&#8217;s worth, is that it would take a focused effort in fine-tuning and/or RLHF&#8212;but that <em>if</em> that effort was invested, one could indeed produce a dating simulator, with current language models, that could have a real impact on the treatment of dating-related social anxiety.  Or at least, it&#8217;s the actually new idea I&#8217;ve had on this problem in eight years, the first one that <em>could</em> have an impact.  If you have a better idea, let&#8217;s hear it!</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p><strong>Endnotes.</strong></p>



<ol>
<li>A woman of my acquaintance, on reading a draft of this post, commented that the dialogue between Quinlan and Emily should&#8217;ve been marked up with chess notation, such as ?? for EXTREME BLUNDER on Quinlan&#8217;s part.  She also comments that the conversation could be extremely useful for Quinlan, if he learned to understand and take seriously her overly polite demurrals of his too-rapid advances.</li>



<li>The same woman commented that SneerClub will have a field day with this post.  I replied that the better part of me doesn&#8217;t care.  If there&#8217;s an actionable idea here&#8212;a new, alien idea in the well-trodden world of self-help&#8212;and it eventually helps one person improve their situation in life, that&#8217;s worth a thousand sneers.</li>
</ol>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-16T23:46:09Z">Tuesday, May 16 2023, 23:46</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.08094'>Accelerating genetic optimization of nonlinear model predictive control by learning optimal search space size</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Eslam Mostafa, Hussein A. Aly, Ahmed Elliethy</p><p>Nonlinear model predictive control (NMPC) solves a multivariate optimization
problem to estimate the system's optimal control inputs in each control cycle.
Such optimization is made more difficult by several factors, such as
nonlinearities inherited in the system, highly coupled inputs, and various
constraints related to the system's physical limitations. These factors make
the optimization to be non-convex and hard to solve traditionally. Genetic
algorithm (GA) is typically used extensively to tackle such optimization in
several application domains because it does not involve differential
calculation or gradient evaluation in its solution estimation. However, the
size of the search space in which the GA searches for the optimal control
inputs is crucial for the applicability of the GA with systems that require
fast response. This paper proposes an approach to accelerate the genetic
optimization of NMPC by learning optimal search space size. The proposed
approach trains a multivariate regression model to adaptively predict the best
smallest search space in every control cycle. The estimated best smallest size
of search space is fed to the GA to allow for searching the optimal control
inputs within this search space. The proposed approach not only reduces the
GA's computational time but also improves the chance of obtaining the optimal
control inputs in each cycle. The proposed approach was evaluated on two
nonlinear systems and compared with two other genetic-based NMPC approaches
implemented on the GPU of a Nvidia Jetson TX2 embedded platform in a
processor-in-the-loop (PIL) fashion. The results show that the proposed
approach provides a 39-53\% reduction in computational time. Additionally, it
increases the convergence percentage to the optimal control inputs within the
cycle's time by 48-56\%, resulting in a significant performance enhancement.
The source code is available on GitHub.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Mostafa_E/0/1/0/all/0/1">Eslam Mostafa</a>, <a href="http://arxiv.org/find/math/1/au:+Aly_H/0/1/0/all/0/1">Hussein A. Aly</a>, <a href="http://arxiv.org/find/math/1/au:+Elliethy_A/0/1/0/all/0/1">Ahmed Elliethy</a></p><p>Nonlinear model predictive control (NMPC) solves a multivariate optimization
problem to estimate the system's optimal control inputs in each control cycle.
Such optimization is made more difficult by several factors, such as
nonlinearities inherited in the system, highly coupled inputs, and various
constraints related to the system's physical limitations. These factors make
the optimization to be non-convex and hard to solve traditionally. Genetic
algorithm (GA) is typically used extensively to tackle such optimization in
several application domains because it does not involve differential
calculation or gradient evaluation in its solution estimation. However, the
size of the search space in which the GA searches for the optimal control
inputs is crucial for the applicability of the GA with systems that require
fast response. This paper proposes an approach to accelerate the genetic
optimization of NMPC by learning optimal search space size. The proposed
approach trains a multivariate regression model to adaptively predict the best
smallest search space in every control cycle. The estimated best smallest size
of search space is fed to the GA to allow for searching the optimal control
inputs within this search space. The proposed approach not only reduces the
GA's computational time but also improves the chance of obtaining the optimal
control inputs in each cycle. The proposed approach was evaluated on two
nonlinear systems and compared with two other genetic-based NMPC approaches
implemented on the GPU of a Nvidia Jetson TX2 embedded platform in a
processor-in-the-loop (PIL) fashion. The results show that the proposed
approach provides a 39-53\% reduction in computational time. Additionally, it
increases the convergence percentage to the optimal control inputs within the
cycle's time by 48-56\%, resulting in a significant performance enhancement.
The source code is available on GitHub.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-16T00:30:00Z">Tuesday, May 16 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.07838'>Randomized Algorithm for the Maximum-Profit Routing Problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Bogdan Armaselu</p><p>In this paper, we consider the Maximum-Profit Routing Problem (MPRP),
introduced in \cite{Armaselu-PETRA}. In MPRP, the goal is to route the given
fleet of vehicles to pickup goods from specified sites in such a way as to
maximize the profit, i.e., total quantity collected minus travelling costs.
Although deterministic approximation algorithms are known for the problem,
currently there is no randomized algorithm. In this paper, we propose the first
randomized algorithm for MPRP.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Armaselu_B/0/1/0/all/0/1">Bogdan Armaselu</a></p><p>In this paper, we consider the Maximum-Profit Routing Problem (MPRP),
introduced in \cite{Armaselu-PETRA}. In MPRP, the goal is to route the given
fleet of vehicles to pickup goods from specified sites in such a way as to
maximize the profit, i.e., total quantity collected minus travelling costs.
Although deterministic approximation algorithms are known for the problem,
currently there is no randomized algorithm. In this paper, we propose the first
randomized algorithm for MPRP.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-16T00:30:00Z">Tuesday, May 16 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.08055'>Dynamic Convex Hulls under Window-Sliding Updates</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Haitao Wang</p><p>We consider the problem of dynamically maintaining the convex hull of a set
$S$ of points in the plane under the following special sequence of insertions
and deletions (called window-sliding updates): insert a point to the right of
all points of $S$ and delete the leftmost point of $S$. We propose an
$O(|S|)$-space data structure that can handle each update in $O(1)$ amortized
time, such that all standard binary-search-based queries on the convex hull of
$S$ can be answered in $O(\log |S|)$ time, and the convex hull itself can be
output in time linear in the number of its vertices.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haitao Wang</a></p><p>We consider the problem of dynamically maintaining the convex hull of a set
$S$ of points in the plane under the following special sequence of insertions
and deletions (called window-sliding updates): insert a point to the right of
all points of $S$ and delete the leftmost point of $S$. We propose an
$O(|S|)$-space data structure that can handle each update in $O(1)$ amortized
time, such that all standard binary-search-based queries on the convex hull of
$S$ can be answered in $O(\log |S|)$ time, and the convex hull itself can be
output in time linear in the number of its vertices.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-16T00:30:00Z">Tuesday, May 16 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.07758'>Linearizability Analysis of the Contention-Friendly Binary Search Tree</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Uri Abraham, Avi Hayoun</p><p>We present a formal framework for proving the correctness of set
implementations backed by binary-search-tree (BST) and linked lists, which are
often difficult to prove correct using automation. This is because many
concurrent set implementations admit non-local linearization points for their
`contains' procedure. We demonstrate this framework by applying it to the
Contention-Friendly Binary-Search Tree algorithm of Crain et al.
</p>
<p>We took care to structure our framework in a way that can be easily
translated into input for model-checking tools such as TLA+, with the aim of
using a computer to verify bounded versions of claims that we later proved
manually. Although this approach does not provide complete proof (i.e., does
not constitute full verification), it allows checking the reasonableness of the
claims before spending effort constructing a complete proof. This is similar to
the test-driven development methodology, that has proven very beneficial in the
software engineering community.
</p>
<p>We used this approach and validated many of the invariants and properties of
the Contention-Friendly algorithm using TLA+. It proved beneficial, as it
helped us avoid spending time trying to prove incorrect claims. In one example,
TLA+ flagged a fundamental error in one of our core definitions. We corrected
the definition (and the dependant proofs), based on the problematic scenario
TLA+ provided as a counter-example.
</p>
<p>Finally, we provide a complete, manual, proof of the correctness of the
Contention-Friendly algorithm, based on the definitions and proofs of our
two-tiered framework.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Abraham_U/0/1/0/all/0/1">Uri Abraham</a>, <a href="http://arxiv.org/find/cs/1/au:+Hayoun_A/0/1/0/all/0/1">Avi Hayoun</a></p><p>We present a formal framework for proving the correctness of set
implementations backed by binary-search-tree (BST) and linked lists, which are
often difficult to prove correct using automation. This is because many
concurrent set implementations admit non-local linearization points for their
`contains' procedure. We demonstrate this framework by applying it to the
Contention-Friendly Binary-Search Tree algorithm of Crain et al.
</p>
<p>We took care to structure our framework in a way that can be easily
translated into input for model-checking tools such as TLA+, with the aim of
using a computer to verify bounded versions of claims that we later proved
manually. Although this approach does not provide complete proof (i.e., does
not constitute full verification), it allows checking the reasonableness of the
claims before spending effort constructing a complete proof. This is similar to
the test-driven development methodology, that has proven very beneficial in the
software engineering community.
</p>
<p>We used this approach and validated many of the invariants and properties of
the Contention-Friendly algorithm using TLA+. It proved beneficial, as it
helped us avoid spending time trying to prove incorrect claims. In one example,
TLA+ flagged a fundamental error in one of our core definitions. We corrected
the definition (and the dependant proofs), based on the problematic scenario
TLA+ provided as a counter-example.
</p>
<p>Finally, we provide a complete, manual, proof of the correctness of the
Contention-Friendly algorithm, based on the definitions and proofs of our
two-tiered framework.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-16T00:30:00Z">Tuesday, May 16 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.07808'>The $2$-$3$-Set Packing problem and a $\frac{4}{3}$-approximation for the Maximum Leaf Spanning Arborescence problem in rooted dags</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Meike Neuwohner</p><p>The weighted $3$-Set Packing problem is defined as follows: As input, we are
given a collection $\mathcal{S}$ of sets, each of cardinality at most $3$ and
equipped with a positive weight. The task is to find a disjoint sub-collection
of maximum total weight. Already the special case of unit weights is known to
be NP-hard, and the state-of-the-art are $\frac{4}{3}+\epsilon$-approximations
by Cygan and F\"urer and Yu. In this paper, we study the $2$-$3$-Set Packing
problem, a generalization of the unweighted $3$-Set Packing problem, where our
set collection may contain sets of cardinality $3$ and weight $2$, as well as
sets of cardinality $2$ and weight $1$. Building upon the state-of-the-art
works in the unit weight setting, we manage to provide a
$\frac{4}{3}+\epsilon$-approximation also for the more general $2$-$3$-Set
Packing problem. We believe that this result can be a good starting point to
identify classes of weight functions to which the techniques used for unit
weights can be generalized. Using a reduction by Fernandes and Lintzmayer, our
result further implies a $\frac{4}{3}+\epsilon$-approximation for the Maximum
Leaf Spanning Arborescence problem (MLSA) in rooted directed acyclic graphs,
improving on the previously known $\frac{7}{5}$-approximation by Fernandes and
Lintzmayer. By exploiting additional structural properties of the instance
constructed in their reduction, we can further get the approximation guarantee
for the MLSA down to $\frac{4}{3}$. The MLSA has applications in broadcasting
where a message needs to be transferred from a source node to all other nodes
along the arcs of an arborescence in a given network.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Neuwohner_M/0/1/0/all/0/1">Meike Neuwohner</a></p><p>The weighted $3$-Set Packing problem is defined as follows: As input, we are
given a collection $\mathcal{S}$ of sets, each of cardinality at most $3$ and
equipped with a positive weight. The task is to find a disjoint sub-collection
of maximum total weight. Already the special case of unit weights is known to
be NP-hard, and the state-of-the-art are $\frac{4}{3}+\epsilon$-approximations
by Cygan and F\"urer and Yu. In this paper, we study the $2$-$3$-Set Packing
problem, a generalization of the unweighted $3$-Set Packing problem, where our
set collection may contain sets of cardinality $3$ and weight $2$, as well as
sets of cardinality $2$ and weight $1$. Building upon the state-of-the-art
works in the unit weight setting, we manage to provide a
$\frac{4}{3}+\epsilon$-approximation also for the more general $2$-$3$-Set
Packing problem. We believe that this result can be a good starting point to
identify classes of weight functions to which the techniques used for unit
weights can be generalized. Using a reduction by Fernandes and Lintzmayer, our
result further implies a $\frac{4}{3}+\epsilon$-approximation for the Maximum
Leaf Spanning Arborescence problem (MLSA) in rooted directed acyclic graphs,
improving on the previously known $\frac{7}{5}$-approximation by Fernandes and
Lintzmayer. By exploiting additional structural properties of the instance
constructed in their reduction, we can further get the approximation guarantee
for the MLSA down to $\frac{4}{3}$. The MLSA has applications in broadcasting
where a message needs to be transferred from a source node to all other nodes
along the arcs of an arborescence in a given network.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-16T00:30:00Z">Tuesday, May 16 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Monday, May 15
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2023/05/15/linkage-new-kittens.html'>Linkage with new kittens</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Numberphile on book embeddings of graphs (\(\mathbb{M}\)), and on the long-awaited proof of the claim that some planar graphs require 4 pages. For more reading on this, see the Wikipedia article on book embeddings.
        
        </div>

        <div class='tr-article-summary'>
        
          
          <ul>
  <li>
    <p><a href="https://www.youtube.com/watch?v=qw2Pl_Nk3CA">Numberphile on book embeddings of graphs</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110297702978465385">\(\mathbb{M}\)</a>),</span> and on the long-awaited proof of the claim that some planar graphs require 4 pages. For more reading on this, see <a href="https://en.wikipedia.org/wiki/Book_embedding">the Wikipedia article on book embeddings</a>.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@DavidKButler/110295691782655688">Deltahedra with mostly-concave edges</a>. Joined excavated icosidodecahedra can get 68% of the edges to have concave angles, but it appears that <a href="https://mathstodon.xyz/@11011110/110301119819252894">digging a joined-icosahedron tunnel out of a larger surrounding shell</a> can get up to 91%. The image below shows the best gluing pattern I found, in which four icosahedra are each cut along two edges and then glued to each other.</p>

    <p style="text-align:center"><img src="/blog/assets/2023/4-icosahedra-m.jpg" alt="Four icosahedra glued to each other after cutting two edges in each icosahedron, as constructed with Geoshapes and blue painter's tape" style="border-style:solid;border-color:black;width:100%;max-width:540px" /></p>
  </li>
  <li>
    <p><a href="https://guests.mpim-bonn.mpg.de/tmarty/lampshade/">How to flip a lampshade upside down</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@Ianagol/110283089447566099">\(\mathbb{M}\)</a>),</span> where the lampshade has the shape of a truncated cone and you have to deform it without self-intersection and without ever getting a horizontal tangent line. Marty Théo, via Ian Agol.</p>
  </li>
  <li>
    <p>Two newly listed Wikipedia Good Articles in one day <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110314349409696124">\(\mathbb{M}\)</a>):</span></p>

    <ul>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Midsphere">Midsphere</a>, a sphere tangent to every edge of a polyhedron. Among other properties, all Hamiltonian cycles on a polyhedron with a midsphere have equal lengths.</p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Rook%27s_graph">Rook’s graph</a>, the graph of rook moves on a chessboard, the Cartesian product of two complete graphs, and the line graph of a complete bipartite graph.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="https://crookedtimber.org/2023/05/05/the-protestant-ethic-and-the-spirit-of-mastodon/">Why I should stick to Mastodon and not even dip my toes into Bluesky</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110317325665994068">\(\mathbb{M}\)</a>):</span> apparently the choice is between serious and technical discussions that exclude the Nazi trolls, on the Mastodon side, versus “Lots of butt-pictures. Then it was sexualized images of Alf” on the Bluesky side. I know what I prefer, and it’s not butt-pictures and Alf. <a href="https://mathstodon.xyz/@sc_griffith/110339669575590399">Later developments make clear</a> that BlueSky also fully intends to encourage content that includes violence and gore, political hate, spam, and impersonation, and then charge readers for the moderation tools to keep it at bay.</p>
  </li>
  <li>
    <p><a href="https://www.thisiscolossal.com/2023/04/rick-salafia-instruments/">Rick Salafia’s wildly shaped aluminum rulers measure impractical proportions</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@colossal@mastodon.art/110294786724701441">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p>Republican-led US states are banning topics from university courses and watering down or eliminating the entire tenure system for university faculty <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110328562153783445">\(\mathbb{M}\)</a>).</span> <a href="https://www.latimes.com/business/story/2023-05-02/red-state-efforts-to-dumb-down-their-universities-will-provoke-a-brain-drain"><em>Los Angeles Times</em> columnist Michael Hiltzik argues</a> that this is already causing a brain drain from those states and that “This trend is almost certain to get worse before it gets better as America devolves into two countries: one that nurtures brainpower, and one that watches proudly as it drains away.”</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/User:XOR%27easter/Research_under_a_cloud">Research under a cloud</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110334174400159711">\(\mathbb{M}\)</a>).</span> Wikipedia editor XOR’easter rebuts an academic publication, “‘Too Soon’ to count? How gender and race cloud notability considerations on Wikipedia”, by Lemieux, Zhang, and Tripodi, purporting to show biases against women and non-white academics in Wikipedia’s deletion processes. Those biases may well exist but “Too Soon” makes a bad case for that. As the link describes, its arguments rely on severe misrepresentations of Wikipedia’s notability criteria and on selective quotations that in many cases reverse the intended meaning of the quoted text. <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-05-08/Recent_research">A condensed version of this recently appeared in the Wikipedia <em>Signpost</em></a>. I’m quoted, both without attribution in “Too Soon” and by name in the rebuttal.</p>
  </li>
  <li>
    <p><a href="https://theconversation.com/i-unintentionally-created-a-biased-ai-algorithm-25-years-ago-tech-companies-are-still-making-the-same-mistake-203734">John MacCormick unintentially created a racially biased face-recognition system 25 years ago</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@divbyzero/110339079855421480">\(\mathbb{M}\)</a>),</span> by using unrepresentative training data. He points out the difficulty of spotting the bias in the resulting system (which mostly consists of big matrices of numbers representing training weights) and wonders if current AI system creators too might have made a similar mistake.</p>
  </li>
  <li>
    <p>A construction in a proof is producing square matrices of the following form <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110342481111834804">\(\mathbb{M}\)</a>):</span> the main diagonal is nonzero (red in the image below), each diagonal coefficient has zeros either directly above it in the same column or directly to the left of it in the same row (pale yellow), and the rest of the coefficients can be anything (blue). Fairly obviously, such matrices have full rank, which is what my proof needs. Does anyone know whether there is a name for this class of matrices?</p>

    <p style="text-align:center"><img src="/blog/assets/2023/arrow-matrix.svg" alt="A 16x16 grid of squares, with red squares down the main diagonal. Each red square has an associated line of pale yellow squares either directly above it or directly to its left. The remaining squares are dark blue." /></p>
  </li>
  <li>
    <p><a href="https://www.vice.com/en/article/v7bdba/ai-is-tearing-wikipedia-apart">AI is tearing Wikipedia apart</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@kissane@mstdn.social/110328423198247971">\(\mathbb{M}\)</a>).</span> <em>Vice</em> story on the ongoing disagreement among Wikipedia editors on whether to ban or how to handle AI-generated content. In the linked post, Erin Kissane points out that AI-generated talk page sophistry may be an even bigger concern.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2304.01393">Every Author as First Author</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@blinry@chaos.social/110339453201148214">\(\mathbb{M}\)</a>):</span> “Finally, a solution to the unfairness of authorship ordering in scientific papers!” By <span style="position:relative;margin-left:0.3em"><span style="position:absolute;top:-0.45ex;white-space:nowrap">Erik Demaine &amp; </span><span style="position:absolute;top:-0.45ex;white-space:nowrap">Martin Demaine.</span></span></p>
  </li>
  <li>
    <p><a href="https://finance.yahoo.com/news/chegg-stock-sinks-after-ceo-says-chatgpt-hurt-growth-132058643.html, via https://news.ycombinator.com/item?id=35929011">Chegg stock sinks because too many students have switched to ChatGPT instead of Chegg to get bespoke cheats for their homework that are harder to detect than copied-and-pasted cheats</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110369753489873480">\(\mathbb{M}\)</a>).</span> Oh no. What will we do without them.</p>
  </li>
  <li>
    <p>These two six-week-old kittens arrived at our house yesterday <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/110370860159231187">\(\mathbb{M}\)</a>).</span>  Names still not definite but we think maybe we’re going to name the more adventurous one on the upper left Mist, and his shyer brother on the lower right Smoke.</p>

    <p style="text-align:center"><img src="https://www.ics.uci.edu/~eppstein/pix/2newkittens/Kittens-m.jpg" alt="Two six-week-old gray kittens looking out from an open cupboard behind some chair legs. The one on the upper left has a white striped nose and white abdomen; the one ont e lower right has a white ruff." style="border-style:solid;border-color:black" /></p>
  </li>
</ul><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-15T20:49:00Z">Monday, May 15 2023, 20:49</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://rjlipton.wpcomstaging.com/2023/05/14/ingrid-daubechies-prizes-and-art/'>Ingrid Daubechies: Prizes and Art</a></h3>
        <p class='tr-article-feed'>from <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Ingrid Daubechies won the 2023 Wolf Prize in Mathematics last February. She is the James B. Duke Distinguished Professor of Mathematics and Electrical and Computer Engineering at Duke University. Crop from 2021 New York Times profile by Siobhan Roberts, photo by Jeremy Lange Today, Mother&#8217;s Day in the US, we congratulate her on this award [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p><p>
Ingrid Daubechies <a href="https://today.duke.edu/2023/02/duke-professor-wins-one-most-prestigious-awards-mathematics">won</a> the 2023 <a href="https://wolffund.org.il/the-wolf-prize/">Wolf Prize</a> in Mathematics last February. She is the James B. Duke Distinguished Professor of Mathematics and Electrical and Computer Engineering at Duke University.</p>
<p><P></p>
<table style="margin:auto;">
<tr>
<td>
<p>
<a href="https://rjlipton.wpcomstaging.com/2023/05/14/ingrid-daubechies-prizes-and-art/daubechiesnyt/" rel="attachment wp-att-21613"><img data-attachment-id="21613" data-permalink="https://rjlipton.wpcomstaging.com/2023/05/14/ingrid-daubechies-prizes-and-art/daubechiesnyt/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/DaubechiesNYT.jpg?fit=965%2C895&amp;ssl=1" data-orig-size="965,895" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="DaubechiesNYT" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/DaubechiesNYT.jpg?fit=300%2C278&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/DaubechiesNYT.jpg?fit=600%2C556&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/DaubechiesNYT.jpg?resize=240%2C225&#038;ssl=1" alt="" width="240" height="225" class="aligncenter wp-image-21613" data-recalc-dims="1" /></a>
</td>
</tr>
<tr>
<td class="caption aligncenter"><FONT size="-2">Crop from 2021 New York Times <a href="https://www.nytimes.com/2021/09/14/magazine/ingrid-daubechies.html">profile</a> by Siobhan Roberts, photo by Jeremy Lange</FONT>
</td>
</tr>
</table>
<p><P><br />
Today, Mother&#8217;s Day in the US, we congratulate her on this award and note some non-theoretical applications of her work. </p>
<p>
The award <a href="https://wolffund.org.il/2023/02/07/ingrid-daubechies/">cites</a> &#8220;her work in the creation and development of wavelet theory and modern time-frequency analysis.&#8221; It goes on to say:</p>
<blockquote><p><b> </b> <em> Her discovery of smooth, compactly supported wavelets, and the development of biorthogonal wavelets transformed image and signal processing and filtering. Her work is of tremendous importance in image compression, medical imaging, remote sensing, and digital photography. Daubechies has also made unparalleled contributions to developing real-world applications of harmonic analysis, introducing sophisticated image-processing techniques to fields ranging from art to evolutionary biology and beyond. </em>
</p></blockquote>
<p><p>
Daubechies started as a physicist and wrote a PhD thesis on quantum mechanics. Although she is the first woman to win a Wolf Prize in Mathematics, the Physics Wolf Prizes began with a female winner right away, in 1978. </p>
<p>
That was Chien-Shiung Wu, who <a href="https://wolffund.org.il/2018/12/09/chien-shiung-wu/">won</a> in 1978 for her work on weak interactions and execution of the first <a href="https://en.wikipedia.org/wiki/Wu_experiment">experiment</a> that demonstrated the non-conservation of parity. Wu&#8217;s prize partially righted a now universally-acknowledged wrong of not including her in the 1957 Nobel Prize of Tsung Dao Lee and Chen Ning Yang. There were only four female winners, all in Medicine, until Ada Yonath in Chemistry in 2006/2007, but women had exact parity in the twelve prizes in 2022. </p>
<p>
<p><H2> Wvts </H2></p>
<p><p>
The French word for <a href="https://en.wikipedia.org/wiki/Wavelet">wavelet</a>, which is <em>ondelette</em>, may have been first employed in a scientific context by the French-American physicist Alexander Grossmann around the time he was one of Daubechies&#8217;s two PhD advisors. Without going into detail, we venture a highly compressed evocation of wavelets.</p>
<p>
An abstract statement of the problem for which wavelets have often provided effective solutions is:</p>
<blockquote><p><b> </b> <em> Given a generator <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BG%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{G}" class="latex" /> of elements in a high-dimensional Hilbert space, what is the best choice of basis to minimize the expected complexity of representing items drawn from <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BG%7D&#038;bg=e8e8e8&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{G}" class="latex" /> over that basis, either exactly or optimizing a combined score of complexity and approximation? </em>
</p></blockquote>
<p><p>
An example of a high-dimensional vector space is the set of possible monomials of degree (up to) <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{d}" class="latex" /> in <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{n}" class="latex" /> variables. Working over the standard basis is like assembling a polynomial term-by-term, which can be poky. There are other bases composed of mutually orthogonal polynomials for which small sums may be closer to typical outputs <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{g}" class="latex" /> from certain generators <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7BG%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{G}" class="latex" />. </p>
<p>
When <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{g}" class="latex" /> represents a static configuration or a process that changes uniformly and slowly, there might not be much improvement on the standard basis. But when <img decoding="async" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002" alt="{g}" class="latex" /> is dominated by a few transient events, bases aligned with such events perform better. Getting very rough in our description, many events of interest are like throwing rocks into a pond. The resulting ripples are well described over a <em>wavelet basis</em>. Whereas, using a standard basis&#8212;which is fine when the pond is still&#8212;will leave blocky artifacts when low resolution fails to discriminate the ripples.</p>
<p><P></p>
<table style="margin:auto;">
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2023/05/14/ingrid-daubechies-prizes-and-art/daubechieswaveletwiki/" rel="attachment wp-att-21614"><img data-attachment-id="21614" data-permalink="https://rjlipton.wpcomstaging.com/2023/05/14/ingrid-daubechies-prizes-and-art/daubechieswaveletwiki/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/DaubechiesWaveletWiki.png?fit=609%2C444&amp;ssl=1" data-orig-size="609,444" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="DaubechiesWaveletWiki" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/DaubechiesWaveletWiki.png?fit=300%2C219&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/DaubechiesWaveletWiki.png?fit=600%2C437&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/DaubechiesWaveletWiki.png?resize=305%2C222&#038;ssl=1" alt="" width="305" height="222" class="aligncenter wp-image-21614" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/DaubechiesWaveletWiki.png?w=609&amp;ssl=1 609w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/DaubechiesWaveletWiki.png?resize=300%2C219&amp;ssl=1 300w" sizes="(max-width: 305px) 100vw, 305px" data-recalc-dims="1" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><FONT size="-2">Wikipedia &#8220;Daubechies wavelet&#8221; <a href="https://en.wikipedia.org/wiki/Daubechies_wavelet">source</a></FONT>
</td>
</tr>
</table>
<p>
The origin story of how Daubechies realized the wider applicability of wavelets, as told <a href="https://today.duke.edu/2023/02/duke-professor-wins-one-most-prestigious-awards-mathematics">here</a>, is that she noticed blocky artifacts in grass while watching a soccer match. The movement of grass blades is more concisely explained as a response to transient wind and game events than a uniform and locally independent process. This has proved universal in practice, as the story says: &#8220;Anytime you go to a movie theater, or watch live sports on ESPN, each frame has been compressed using Daubechies’ wavelet-based method.&#8221;</p>
<p>
<p><H2> Art and Originality </H2></p>
<p><p>
My wording with &#8220;explained&#8221; hints at a yet-wider world of application in data-science inference. I would love to claim in a grand sweep that an artwork such as a painting is best represented as the sequence of conceptual events that governed its creation. The events should be describable at a level where the exact details of execution can be inferred while saving many bits over an exhaustive &#8220;standard-basis&#8221; representation. I am not going as far as the <a href="https://en.wikipedia.org/wiki/Low-complexity_art">low-complexity art</a> of J&uuml;rgen Schmidhuber, but share the motivation that art may be better described in bases that align with succinct formulations and salient impulses.</p>
<p>
On firmer ground is the use of descriptive techniques to identify events that have <em>happened to</em> artwork since its creation. In a wonderful 2016 <a href="https://www.quantamagazine.org/using-mathematics-to-repair-a-masterpiece-20160929/">article</a> by Daubechies for <em>Quanta</em> on art restoration, she describes two kinds of events affecting centuries-old altarpieces:</p>
<ul>
<li>
Natural cracking of the wood panels </p>
<li>
Previous efforts at restoration.
</ul>
<p>
The latter included past restorers imposing a lattice support on the wood that confused with natural cracks. A third kind of event they were able to distinguish fits the creation type in my first paragraph: strokes of fine text delicately painted by the original artists. Daubechies&#8217;s methods were able to highlight those just enough for professional antiquarians to identify the religious text that was intended. </p>
<p><P></p>
<table style="margin:auto;">
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2023/05/14/ingrid-daubechies-prizes-and-art/paneldetail/" rel="attachment wp-att-21616"><img data-attachment-id="21616" data-permalink="https://rjlipton.wpcomstaging.com/2023/05/14/ingrid-daubechies-prizes-and-art/paneldetail/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/PanelDetail.jpg?fit=312%2C275&amp;ssl=1" data-orig-size="312,275" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;KWRegan&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1684101133&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="PanelDetail" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/PanelDetail.jpg?fit=300%2C264&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/PanelDetail.jpg?fit=312%2C275&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/PanelDetail.jpg?resize=208%2C184&#038;ssl=1" alt="" width="208" height="184" class="aligncenter wp-image-21616" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/PanelDetail.jpg?w=312&amp;ssl=1 312w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/05/PanelDetail.jpg?resize=300%2C264&amp;ssl=1 300w" sizes="(max-width: 208px) 100vw, 208px" data-recalc-dims="1" /></a>
</td>
</tr>
<tr>
<td class="caption aligncenter"><FONT size="-2">Excerpt from large image in Daubechies <a href="https://www.quantamagazine.org/using-mathematics-to-repair-a-masterpiece-20160929/">article</a></FONT>
</td>
</tr>
</table>
<p><P><br />
In earlier work at Princeton described in this 2008 <a href="https://paw.princeton.edu/article/fake-vs-real-analyzing-artwork-set-mathematical-tools">article</a>, Daubechies and team applied wavelets to distinguish authentic paintings by Vincent van Gogh from forgeries. To do this, they &#8220;created a statistical portrait of van Gogh’s style using wavelets.&#8221; What emerged from their analysis was that the forgeries could not so easily be described in their van Gogh basis:</p>
<blockquote><p><b> </b> <em> The forgeries and copies &#8230; had more tiny wavelets in the image, which [quoting team member Shannon Hughes] &#8220;we think are tiny fluctuations and wobbles. &#8230; You can imagine that if someone is painting really slowly trying to copy another work, they are hesitating a little bit.&#8221; </em>
</p></blockquote>
<p><p>
This reminds me also of telltale hesitations that enable online chess platforms to infer when a human player is concentrating on something else besides playing the game. A final quote from Hughes in the article tends toward my grander thought about artistic creation:</p>
<blockquote><p><em> [Wavelet technology may help figure out] &#8220;what the artist was thinking and what the artist was doing and why they were doing it.&#8221; </em>
</p></blockquote>
<p><p>
Daubechies has been involved in some art creation of her own: the art installation <a href="https://en.wikipedia.org/wiki/Mathemalchemy">Mathemalchemy</a>.</p>
<p>
<p><H2> Open Problems </H2></p>
<p><p>
What inferences will wavelets enable about the creative process? As someone who fancies composing but has no ability to play or transcribe music faster than a snail&#8217;s pace, I can vouch that music is not created in the standard basis.</p>
<p>
<p class="authors">By KWRegan</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-15T02:23:23Z">Monday, May 15 2023, 02:23</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.07364'>Improved Lower Bounds for Monotone q-Multilinear Boolean Circuits</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Andrzej Lingas, Mia Persson</p><p>A monotone Boolean circuit is composed of OR gates, AND gates and input gates
corresponding to the input variables and the Boolean constants. It is
$q$-multilinear if for each its output gate $o$ and for each prime implicant
$s$ of the function computed at $o$, the arithmetic version of the circuit
resulting from the replacement of OR and AND gates by addition and
multiplication gates, respectively, computes a polynomial at $o$ which contains
a monomial including the same variables as $s$ and each of the variables in $s$
has degree at most $q$ in the monomial. First, we study the complexity of
computing semi-disjoint bilinear Boolean forms in terms of the size of monotone
$q$-multilinear Boolean circuits. In particular, we show that any monotone
$1$-multilinear Boolean circuit computing a semi-disjoint Boolean form with $p$
prime implicants includes at least $p$ AND gates. We also show that any
monotone $q$-multilinear Boolean circuit computing a semi-disjoint Boolean form
with $p$ prime implicants has $\Omega(\frac p {q^4})$ size. Next, we study the
complexity of the monotone Boolean function $Isol_{k,n}$ that verifies if a
$k$-dimensional Boolean matrix has at least one $1$ in each line (e.g., each
row and column when $k=2$), in terms of monotone $q$-multilinear Boolean
circuits. We show that that any $\Sigma_3$ monotone Boolean circuit for
$Isol_{k,n}$ has an exponential in $n$ size or it is not $(k-1)$-multilinear.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lingas_A/0/1/0/all/0/1">Andrzej Lingas</a>, <a href="http://arxiv.org/find/cs/1/au:+Persson_M/0/1/0/all/0/1">Mia Persson</a></p><p>A monotone Boolean circuit is composed of OR gates, AND gates and input gates
corresponding to the input variables and the Boolean constants. It is
$q$-multilinear if for each its output gate $o$ and for each prime implicant
$s$ of the function computed at $o$, the arithmetic version of the circuit
resulting from the replacement of OR and AND gates by addition and
multiplication gates, respectively, computes a polynomial at $o$ which contains
a monomial including the same variables as $s$ and each of the variables in $s$
has degree at most $q$ in the monomial. First, we study the complexity of
computing semi-disjoint bilinear Boolean forms in terms of the size of monotone
$q$-multilinear Boolean circuits. In particular, we show that any monotone
$1$-multilinear Boolean circuit computing a semi-disjoint Boolean form with $p$
prime implicants includes at least $p$ AND gates. We also show that any
monotone $q$-multilinear Boolean circuit computing a semi-disjoint Boolean form
with $p$ prime implicants has $\Omega(\frac p {q^4})$ size. Next, we study the
complexity of the monotone Boolean function $Isol_{k,n}$ that verifies if a
$k$-dimensional Boolean matrix has at least one $1$ in each line (e.g., each
row and column when $k=2$), in terms of monotone $q$-multilinear Boolean
circuits. We show that that any $\Sigma_3$ monotone Boolean circuit for
$Isol_{k,n}$ has an exponential in $n$ size or it is not $(k-1)$-multilinear.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-15T00:30:00Z">Monday, May 15 2023, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2305.07539'>Sampling recovery in the uniform norm</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: David Krieg, Kateryna Pozharska, Mario Ullrich, Tino Ullrich</p><p>We study the recovery of functions in the uniform norm based on function
evaluations. We obtain worst case error bounds for general classes of
functions, also in $L_p$-norm, in terms of the best $L_2$-approximation from a
given nested sequence of subspaces combined with bounds on the the Christoffel
function of these subspaces.
</p>
<p>Our results imply that linear sampling algorithms are optimal (up to
constants) among all algorithms using arbitrary linear information for many
reproducing kernel Hilbert spaces; a result that has been observed
independently in [Geng \&amp; Wang, arXiv:2304.14748].
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Krieg_D/0/1/0/all/0/1">David Krieg</a>, <a href="http://arxiv.org/find/math/1/au:+Pozharska_K/0/1/0/all/0/1">Kateryna Pozharska</a>, <a href="http://arxiv.org/find/math/1/au:+Ullrich_M/0/1/0/all/0/1">Mario Ullrich</a>, <a href="http://arxiv.org/find/math/1/au:+Ullrich_T/0/1/0/all/0/1">Tino Ullrich</a></p><p>We study the recovery of functions in the uniform norm based on function
evaluations. We obtain worst case error bounds for general classes of
functions, also in $L_p$-norm, in terms of the best $L_2$-approximation from a
given nested sequence of subspaces combined with bounds on the the Christoffel
function of these subspaces.
</p>
<p>Our results imply that linear sampling algorithms are optimal (up to
constants) among all algorithms using arbitrary linear information for many
reproducing kernel Hilbert spaces; a result that has been observed
independently in [Geng \&amp; Wang, <a href="/abs/2304.14748">arXiv:2304.14748</a>].
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2023-05-15T00:30:00Z">Monday, May 15 2023, 00:30</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
