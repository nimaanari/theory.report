<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.4 (2022-04-12) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2022-10-30T18:43:35Z">Sunday, October 30 2022, 18:43</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>Sébastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Sunday, October 30
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://blog.computationalcomplexity.org/2022/10/does-physics-nobel-prize-winner.html'>Does the Physics Nobel Prize Winner understand their own work (guest post)</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>&nbsp;David Marcus was a Math major a year ahead of my at SUNY Stonybrook (he graduated in 1979,</p><p>I graduated in 1980). He then got a PhD from MIT in Math, and is a reader of this blog.&nbsp; Recently he emailed me that he thinks the current Nobel Prize Winner in Physics does not understand his own work. Is it true? Lets find out!</p><p>------------------------</p><p>(Guest blog from David Marcus)</p><p>2022 Nobel Prize in Physics Awarded for Experiments that Demonstrate Nonlocality</p><p>The 2022 Nobel Prize in Physics was recently awarded to experimenters who demonstrated that the world is nonlocal. The curious thing is that neither the writers of the Nobel Prize press release nor the recipients seem to understand that this is what they demonstrated.</p><p><br></p><p>For example, the press release (see here) says: "John Clauser developed John Bell's ideas, leading to a practical experiment. When he took the measurements, they supported quantum mechanics by clearly violating a Bell inequality. This means that quantum mechanics cannot be replaced by a theory that uses hidden variables." That is not what the experiments mean, and the statement is false.</p><p>The word "locality" means that doing something here cannot instantly change something other there.</p><p>The experimental setup is the following: You prepare two particles, A and B, and send them in opposite directions so that they are far apart. You and your colleague do experiments on each particle at the same time. If you and your colleague perform the same experiment, then, from your experiment on A, you can predict with certainty the result of your colleague's experiment on B (and vice versa).</p><p>In a paper in 1935, Einstein, Podolsky, and Rosen pointed out that, assuming locality, the experimental results at A and B must be determined by the source that prepared the particles. They didn't actually say, "assuming locality", but they implicitly assumed it. (If you disagree with them, please offer an alternative.)</p><p>In 1964, John Bell published his paper. In it, he considered three of the experiments that could be done on the particles A and B. Assuming the results are determined by the source (which follows from Einstein, Podolsky, and Rosen's argument), he derived an inequality on the correlations between the results of the three experiments on the two particles. The math is simple; for details, see&nbsp;here.</p><p>The Nobel Prize winners did experiments, and their results violated Bell's inequality (or similar inequalities). Hence, the world is nonlocal.</p><p>The simplest theory that agrees with experiment is Bohmian Mechanics. This is a deterministic theory of particles whose motion is governed by a wave (the wave function being the solution of the Schrödinger equation). Of course, Bohmian Mechanics is nonlocal, as is the world.</p><p>By gasarch</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>&nbsp;David Marcus was a Math major a year ahead of my at SUNY Stonybrook (he graduated in 1979,</p><p>I graduated in 1980). He then got a PhD from MIT in Math, and is a reader of this blog.&nbsp; Recently he emailed me that he thinks the current Nobel Prize Winner in Physics does not understand his own work. Is it true? Lets find out!</p><p>------------------------</p><p>(Guest blog from David Marcus)</p><p>2022 Nobel Prize in Physics Awarded for Experiments that Demonstrate Nonlocality</p><p>The 2022 Nobel Prize in Physics was recently awarded to experimenters who demonstrated that the world is nonlocal. The curious thing is that neither the writers of the Nobel Prize press release nor the recipients seem to understand that this is what they demonstrated.</p><p><br /></p><p>For example, the press release (see <a href="https://www.nobelprize.org/prizes/physics/2022/press-release/">here</a>) says: "John Clauser developed John Bell's ideas, leading to a practical experiment. When he took the measurements, they supported quantum mechanics by clearly violating a Bell inequality. This means that quantum mechanics cannot be replaced by a theory that uses hidden variables." That is not what the experiments mean, and the statement is false.</p><p>The word "locality" means that doing something here cannot instantly change something other there.</p><p>The experimental setup is the following: You prepare two particles, A and B, and send them in opposite directions so that they are far apart. You and your colleague do experiments on each particle at the same time. If you and your colleague perform the same experiment, then, from your experiment on A, you can predict with certainty the result of your colleague's experiment on B (and vice versa).</p><p>In a paper in 1935, Einstein, Podolsky, and Rosen pointed out that, assuming locality, the experimental results at A and B must be determined by the source that prepared the particles. They didn't actually say, "assuming locality", but they implicitly assumed it. (If you disagree with them, please offer an alternative.)</p><p>In 1964, John Bell published his paper. In it, he considered three of the experiments that could be done on the particles A and B. Assuming the results are determined by the source (which follows from Einstein, Podolsky, and Rosen's argument), he derived an inequality on the correlations between the results of the three experiments on the two particles. The math is simple; for details, see&nbsp;<a href="http://www.scholarpedia.org/article/Bell%27s_theorem">here</a>.</p><p>The Nobel Prize winners did experiments, and their results violated Bell's inequality (or similar inequalities). Hence, the world is nonlocal.</p><p>The simplest theory that agrees with experiment is Bohmian Mechanics. This is a deterministic theory of particles whose motion is governed by a wave (the wave function being the solution of the Schrödinger equation). Of course, Bohmian Mechanics is nonlocal, as is the world.</p><p class="authors">By gasarch</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-30T17:35:00Z">Sunday, October 30 2022, 17:35</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Saturday, October 29
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/10/29/postdoc-at-linkoping-university-apply-by-november-7-2022/'>Postdoc at Linköping University (apply by November 7, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Linköping University invites applications for postdoc positions. In particular, the Theoretical Computer Science Laboratory is looking for postdocs with a strong background in theoretical computer science. Examples of research topics include (1) fine-grained complexity, (2) algebraic methods for CSPs, (3) parameterized complexity, and (4) randomized and approximation algorithms. Website: liu.se/en/work-at-liu/vacancies?rmpage=job&#38;rmjob=20079&#38;rmlang=UK Email: peter.jonsson@liu.se
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Linköping University invites applications for postdoc positions. In particular, the Theoretical Computer Science Laboratory<br />
is looking for postdocs with a strong background in theoretical computer science. Examples of research topics include (1) fine-grained complexity, (2) algebraic methods for CSPs, (3) parameterized complexity, and (4) randomized and approximation algorithms.</p>
<p>Website: <a href="https://liu.se/en/work-at-liu/vacancies?rmpage=job&amp;rmjob=20079&amp;rmlang=UK">https://liu.se/en/work-at-liu/vacancies?rmpage=job&amp;rmjob=20079&amp;rmlang=UK</a><br />
Email: peter.jonsson@liu.se</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-29T05:59:45Z">Saturday, October 29 2022, 05:59</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Friday, October 28
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/10/28/assisstant-professor-at-university-of-british-columbia-apply-by-november-15-2022/'>assisstant professor at University of British Columbia (apply by November 15, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The UBC math department seeks candidates for a tenure-track assistant professor position, with expertise in the mathematics of Machine Learning and AI. Specific topics of interest include, but are not limited to, neural nets, statistical learning theory, inverse problems, optimization, mathematical data science, and mathematics of information, with a strong theoretical component. Website: www.mathjobs.org/jobs/list/20980 Email: [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The UBC math department seeks candidates for a tenure-track assistant professor position, with expertise in the mathematics of Machine Learning and AI. Specific topics of interest include, but are not limited to, neural nets, statistical learning theory, inverse problems, optimization, mathematical data science, and mathematics of information, with a strong theoretical component.</p>
<p>Website: <a href="https://www.mathjobs.org/jobs/list/20980">https://www.mathjobs.org/jobs/list/20980</a><br />
Email: exec-coord@math.ubc.ca</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T19:41:58Z">Friday, October 28 2022, 19:41</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=6778'>On Bryan Caplan and his new book</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Yesterday I attended a lecture by George Mason University economist Bryan Caplan, who&#8217;s currently visiting UT Austin, about his new book entitled Don’t Be a Feminist. (See also here for previous back-and-forth between me and Bryan about his book.) A few remarks: (1) Maybe surprisingly, there were no protesters storming the lectern, no security detail, [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Yesterday I attended a lecture by George Mason University economist <a href="https://betonit.substack.com/">Bryan Caplan</a>, who&#8217;s currently visiting UT Austin, about his new book entitled <em><a href="https://www.amazon.com/Dont-Be-Feminist-Genuine-Justice/dp/B0BD3DFMMH/ref=asc_df_B0BD3DFMMH/">Don’t Be a Feminist</a></em>.  (<a href="https://betonit.substack.com/p/aaronson-on-feminism-my-reply">See also here</a> for previous back-and-forth between me and Bryan about his book.)  A few remarks:</p>



<p>(1) Maybe surprisingly, there were no protesters storming the lectern, no security detail, not even a single rotten vegetable thrown. About 30 people showed up, majority men but women too. They listened politely and asked polite questions afterward. One feminist civilly challenged Bryan during the Q&amp;A about his gender pay gap statistics.</p>



<p>(2) How is it that I got denounced <a></a>by half the planet for saying once, in a blog comment, that I agreed with 97% of feminism but had concerns with one particular way it was operationalized, whereas Bryan seems to be … not denounced in the slightest for publishing a book and going on a lecture tour about how he rejects feminism in its entirety as angry and self-pitying in addition to factually false? Who can explain this to me?</p>



<p>(3) For purposes of his argument, Bryan defines feminism as &#8220;the view that women are generally treated less fairly than men,&#8221; rather than (say) &#8220;the view that men and women <em>ought</em> to be treated equally,&#8221; or &#8220;the radical belief that women are people,&#8221; or other formulations that Bryan considers too obvious to debate.  He then rebuts feminism as he&#8217;s defined it, by taking the audience on a horror tour of all the ways society treats men less fairly than women (expectations of doing dirty and dangerous work, divorce law, military drafts as in Ukraine right now, &#8230;), as well as potentially benign explanations for apparent unfairness toward women, to argue that it&#8217;s at least <em>debatable</em> which sex gets the rawer deal on average.</p>



<p>During the Q&amp;A, I raised what I thought was the central objection to Bryan&#8217;s relatively narrow definition of feminism. Namely that, by the standards of 150 years ago, Bryan is <em>obviously</em> a feminist, and so am I, and so is everyone in the room. (Whereupon a right-wing business school professor interjected: &#8220;please don’t make assumptions about me!&#8221;)</p>



<p>I explained that <em>this</em> is why I call myself a feminist, despite agreeing with many of Bryan&#8217;s substantive points: because I want no one to imagine for a nanosecond that, if I had the power, I&#8217;d take gender relations back to how they were generations ago.</p>



<p>Bryan replied that >60% of Americans call themselves non-feminists in surveys. So, he asked me rhetorically, do <em>all</em> those Americans secretly yearn to take us back to the 19th century? Such a position, he said, seemed so absurdly uncharitable as not to be worth responding to.</p>



<p>Reflecting about it on my walk home, I realized: actually, give or take the exact percentages, this is <em>precisely</em> the progressive thesis. I.e., that just like at least a solid minority of Germans turned out to be totally fine with Nazism, however much they might&#8217;ve denied it beforehand, so too at least a solid minority of Americans would be fine with&#8212;if not ecstatic about&#8212;<em>The Handmaid&#8217;s Tale</em> made real. Indeed, they&#8217;d add, it&#8217;s only vociferous progressive activism that stands between us and that dystopia.</p>



<p>And if anyone were tempted to doubt this, progressives might point to the election of Donald Trump, the failed insurrection to maintain his power, and the repeal of <em>Roe</em> as proof enough to last for a quadrillion years.</p>



<p>Bryan would probably reply: why even waste time engaging with such a hysterical position? To me, though, the hysterical position sadly has more than a grain of truth to it. I <em>wish</em> we lived in a world where there was no point in calling oneself a pro-democracy anti-racist feminist and a hundred other banal and obvious things. I just don&#8217;t think that we do.</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T16:54:52Z">Friday, October 28 2022, 16:54</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://thmatters.wordpress.com/2022/10/28/acm-survey-on-math-requirements-for-the-cs-major/'>ACM survey on math requirements for the CS major</a></h3>
        <p class='tr-article-feed'>from <a href='https://thmatters.wordpress.com'>Theory Matters</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The ACM/IEEE-CS/AAAI CS2023 Curricular Task Force is working on updating the undergraduate CS curriculum guidelines for the next decade. They have distributed a survey about the role of math in that curriculum, which is of direct interest to the TCS community. Please consider taking the survey so your opinion is heard! From the Task Force: &#8212;&#8212;&#8212;&#8211; [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The <strong>ACM/IEEE-CS/AAAI CS2023 Curricular Task Force </strong>is working on updating the undergraduate CS curriculum guidelines for the next decade. They have distributed a survey about the role of math in that curriculum, which is of direct interest to the TCS community. Please consider taking the survey so your opinion is heard!</p>



<p>From the Task Force:</p>



<p>&#8212;&#8212;&#8212;&#8211;</p>



<p>Dear educator,</p>



<p>What math should undergraduate Computer Science students know?</p>



<p>The CS2023 Task Force is collecting (and will share!) input from the community on this very important topic both as a useful “sense of the community” for everyone and, pertinent to our immediate goal, to shape our decennial curricular recommendations.</p>



<p>We invite you to fill out a survey: <a href="https://tinyurl.com/7zjbu7pr" target="_blank" rel="noreferrer noopener">https://tinyurl.com/7zjbu7pr</a></p>



<p>As you fill out this survey, we ask you to reflect on:</p>



<ul>
<li>Discrete mathematics: student preparedness, topics covered, what’s missing?</li>



<li>What should come beyond discrete mathematics, if anything?</li>



<li>What do the new high-growth areas (AI, ML, quantum computing, data science) need by way of mathematical preparation?</li>



<li>Do most CS jobs need much mathematics, and do current mathematical requirements pose a barrier to some populations of students?</li>
</ul>



<p>Thank you in advance for taking the time to fill out the survey!</p>



<p><em>If you believe that other colleagues in your department can contribute, please forward the survey link to them</em>.</p>



<p>Amruth Kumar and Rajendra Raj</p>



<p>On behalf of the of the ACM/IEEE-CS/AAAI CS2023 Curricular Task Force</p>



<p>NOTE: By participating, you agree that we may use your responses for this study; and that this data may be presented in aggregate form (with no personally identifying information) in articles or websites.</p>



<p>&#8212;&#8212;&#8211;</p>
<p class="authors">By shuchic</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T15:09:38Z">Friday, October 28 2022, 15:09</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/10/28/postdoc-at-university-of-michigan-apply-by-january-10-2023/'>Postdoc at University of Michigan (apply by January 10, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The Theory Group at the University of Michigan invites applications for postdoctoral position(s) beginning September 2023. The position will have an initial appointment for one year but may be extended depending on circumstances. Applicants should be recent PhDs with interests that align well with our ongoing research. Website: forms.gle/P9SAVGhP3tu9rayZA Email: thsa@umich.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The Theory Group at the University of Michigan invites applications for postdoctoral position(s) beginning September 2023. The position will have an initial appointment for one year but may be extended depending on circumstances.</p>
<p>Applicants should be recent PhDs with interests that align well with our ongoing research.</p>
<p>Website: <a href="https://forms.gle/P9SAVGhP3tu9rayZA">https://forms.gle/P9SAVGhP3tu9rayZA</a><br />
Email: thsa@umich.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T13:06:28Z">Friday, October 28 2022, 13:06</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15325'>Geodesic packing in graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Paul Manuel, Bostjan Bresar, Sandi Klavzar</p><p>Given a graph $G$, a geodesic packing in $G$ is a set of vertex-disjoint
maximal geodesics, and the geodesic packing number of $G$, ${\gpack}(G)$, is
the maximum cardinality of a geodesic packing in $G$. It is proved that the
decision version of the geodesic packing number is NP-complete. We also
consider the geodesic transversal number, ${\gt}(G)$, which is the minimum
cardinality of a set of vertices that hit all maximal geodesics in $G$. While
$\gt(G)\ge \gpack(G)$ in every graph $G$, the quotient ${\rm gt}(G)/{\rm
gpack}(G)$ is investigated. By using the rook's graph, it is proved that there
does not exist a constant $C &lt; 3$ such that $\frac{{\rm gt}(G)}{{\rm
gpack}(G)}\le C$ would hold for all graphs $G$. If $T$ is a tree, then it is
proved that ${\rm gpack}(T) = {\rm gt}(T)$, and a linear algorithm for
determining ${\rm gpack}(T)$ is derived. The geodesic packing number is also
determined for the strong product of paths.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Manuel_P/0/1/0/all/0/1">Paul Manuel</a>, <a href="http://arxiv.org/find/math/1/au:+Bresar_B/0/1/0/all/0/1">Bostjan Bresar</a>, <a href="http://arxiv.org/find/math/1/au:+Klavzar_S/0/1/0/all/0/1">Sandi Klavzar</a></p><p>Given a graph $G$, a geodesic packing in $G$ is a set of vertex-disjoint
maximal geodesics, and the geodesic packing number of $G$, ${\gpack}(G)$, is
the maximum cardinality of a geodesic packing in $G$. It is proved that the
decision version of the geodesic packing number is NP-complete. We also
consider the geodesic transversal number, ${\gt}(G)$, which is the minimum
cardinality of a set of vertices that hit all maximal geodesics in $G$. While
$\gt(G)\ge \gpack(G)$ in every graph $G$, the quotient ${\rm gt}(G)/{\rm
gpack}(G)$ is investigated. By using the rook's graph, it is proved that there
does not exist a constant $C &lt; 3$ such that $\frac{{\rm gt}(G)}{{\rm
gpack}(G)}\le C$ would hold for all graphs $G$. If $T$ is a tree, then it is
proved that ${\rm gpack}(T) = {\rm gt}(T)$, and a linear algorithm for
determining ${\rm gpack}(T)$ is derived. The geodesic packing number is also
determined for the strong product of paths.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15509'>On Tsirelson pairs of C*-algebras</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Isaac Goldbring, Bradd Hart</p><p>We introduce the notion of a Tsirelson pair of C*-algebras, which is a pair
of C*-algebras for which the space of quantum strategies obtained by using
states on the minimal tensor product of the pair and the space of quantum
strategies obtained by using states on the maximal tensor product of the pair
coincide. We exhibit a number of examples of such pairs that are ``nontrivial''
in the sense that the minimal tensor product and the maximal tensor product of
the pair are not isomorphic. For example, we prove that any pair containing a
C*-algebra with Kirchberg's QWEP property is a Tsirelson pair. We then
introduce the notion of a C*-algebra with the Tsirelson property (TP) and
establish a number of closure properties for this class. We also show that the
class of C*-algebras with the TP form an axiomatizable class (in the sense of
model theory), but that this class admits no ``effective'' axiomatization.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Goldbring_I/0/1/0/all/0/1">Isaac Goldbring</a>, <a href="http://arxiv.org/find/math/1/au:+Hart_B/0/1/0/all/0/1">Bradd Hart</a></p><p>We introduce the notion of a Tsirelson pair of C*-algebras, which is a pair
of C*-algebras for which the space of quantum strategies obtained by using
states on the minimal tensor product of the pair and the space of quantum
strategies obtained by using states on the maximal tensor product of the pair
coincide. We exhibit a number of examples of such pairs that are ``nontrivial''
in the sense that the minimal tensor product and the maximal tensor product of
the pair are not isomorphic. For example, we prove that any pair containing a
C*-algebra with Kirchberg's QWEP property is a Tsirelson pair. We then
introduce the notion of a C*-algebra with the Tsirelson property (TP) and
establish a number of closure properties for this class. We also show that the
class of C*-algebras with the TP form an axiomatizable class (in the sense of
model theory), but that this class admits no ``effective'' axiomatization.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15601'>Discrete Bulk Reconstruction</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Scott Aaronson, Jason Pollack</p><p>According to the AdS/CFT correspondence, the geometries of certain spacetimes
are fully determined by quantum states that live on their boundaries -- indeed,
by the von Neumann entropies of portions of those boundary states. This work
investigates to what extent the geometries can be reconstructed from the
entropies in polynomial time. Bouland, Fefferman, and Vazirani (2019) argued
that the AdS/CFT map can be exponentially complex if one wants to reconstruct
regions such as the interiors of black holes. Our main result provides a sort
of converse: we show that, in the special case of a single 1D boundary, if the
input data consists of a list of entropies of contiguous boundary regions, and
if the entropies satisfy a single inequality called Strong Subadditivity, then
we can construct a graph model for the bulk in linear time. Moreover, the bulk
graph is planar, it has $O(N^2)$ vertices (the information-theoretic minimum),
and it's ``universal,'' with only the edge weights depending on the specific
entropies in question. From a combinatorial perspective, our problem boils down
to an ``inverse'' of the famous min-cut problem: rather than being given a
graph and asked to find a min-cut, here we're given the values of min-cuts
separating various sets of vertices, and need to find a weighted undirected
graph consistent with those values. Our solution to this problem relies on the
notion of a ``bulkless'' graph, which might be of independent interest for
AdS/CFT. We also make initial progress on the case of multiple 1D boundaries --
where the boundaries could be connected via wormholes -- including an upper
bound of $O(N^4)$ vertices whenever a planar bulk graph exists (thus putting
the problem into the complexity class $\mathsf{NP}$).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Aaronson_S/0/1/0/all/0/1">Scott Aaronson</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Pollack_J/0/1/0/all/0/1">Jason Pollack</a></p><p>According to the AdS/CFT correspondence, the geometries of certain spacetimes
are fully determined by quantum states that live on their boundaries -- indeed,
by the von Neumann entropies of portions of those boundary states. This work
investigates to what extent the geometries can be reconstructed from the
entropies in polynomial time. Bouland, Fefferman, and Vazirani (2019) argued
that the AdS/CFT map can be exponentially complex if one wants to reconstruct
regions such as the interiors of black holes. Our main result provides a sort
of converse: we show that, in the special case of a single 1D boundary, if the
input data consists of a list of entropies of contiguous boundary regions, and
if the entropies satisfy a single inequality called Strong Subadditivity, then
we can construct a graph model for the bulk in linear time. Moreover, the bulk
graph is planar, it has $O(N^2)$ vertices (the information-theoretic minimum),
and it's ``universal,'' with only the edge weights depending on the specific
entropies in question. From a combinatorial perspective, our problem boils down
to an ``inverse'' of the famous min-cut problem: rather than being given a
graph and asked to find a min-cut, here we're given the values of min-cuts
separating various sets of vertices, and need to find a weighted undirected
graph consistent with those values. Our solution to this problem relies on the
notion of a ``bulkless'' graph, which might be of independent interest for
AdS/CFT. We also make initial progress on the case of multiple 1D boundaries --
where the boundaries could be connected via wormholes -- including an upper
bound of $O(N^4)$ vertices whenever a planar bulk graph exists (thus putting
the problem into the complexity class $\mathsf{NP}$).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.14982'>LinearCoFold and LinearCoPartition: Linear-Time Algorithms for Secondary Structure Prediction of Interacting RNA molecules</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: He Zhang, Sizhen Li, Liang Zhang, David H. Mathews, Liang Huang</p><p>Many ncRNAs function through RNA-RNA interactions. Fast and reliable RNA
structure prediction with consideration of RNA-RNA interaction is useful. Some
existing tools are less accurate due to omitting the competing of
intermolecular and intramolecular base pairs, or focus more on predicting the
binding region rather than predicting the complete secondary structure of two
interacting strands. Vienna RNAcofold, which reduces the problem into the
classical single sequence folding by concatenating two strands, scales in cubic
time against the combined sequence length, and is slow for long sequences. To
address these issues, we present LinearCoFold, which predicts the complete
minimum free energy structure of two strands in linear runtime, and
LinearCoPartition, which calculates the cofolding partition function and base
pairing probabilities in linear runtime. LinearCoFold and LinearCoPartition
follows the concatenation strategy of RNAcofold, but are orders of magnitude
faster than RNAcofold. For example, on a sequence pair with combined length of
26,190 nt, LinearCoFold is 86.8x faster than RNAcofold MFE mode (0.6 minutes
vs. 52.1 minutes), and LinearCoPartition is 642.3x faster than RNAcofold
partition function mode (1.8 minutes vs. 1156.2 minutes). Different from the
local algorithms, LinearCoFold and LinearCoPartition are global cofolding
algorithms without restriction on base pair length. Surprisingly, LinearCoFold
and LinearCoPartition's predictions have higher PPV and sensitivity of
intermolecular base pairs. Furthermore, we apply LinearCoFold to predict the
RNA-RNA interaction between SARS-CoV-2 gRNA and human U4 snRNA, which has been
experimentally studied, and observe that LinearCoFold's prediction correlates
better to the wet lab results.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_H/0/1/0/all/0/1">He Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_S/0/1/0/all/0/1">Sizhen Li</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_L/0/1/0/all/0/1">Liang Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Mathews_D/0/1/0/all/0/1">David H. Mathews</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Huang_L/0/1/0/all/0/1">Liang Huang</a></p><p>Many ncRNAs function through RNA-RNA interactions. Fast and reliable RNA
structure prediction with consideration of RNA-RNA interaction is useful. Some
existing tools are less accurate due to omitting the competing of
intermolecular and intramolecular base pairs, or focus more on predicting the
binding region rather than predicting the complete secondary structure of two
interacting strands. Vienna RNAcofold, which reduces the problem into the
classical single sequence folding by concatenating two strands, scales in cubic
time against the combined sequence length, and is slow for long sequences. To
address these issues, we present LinearCoFold, which predicts the complete
minimum free energy structure of two strands in linear runtime, and
LinearCoPartition, which calculates the cofolding partition function and base
pairing probabilities in linear runtime. LinearCoFold and LinearCoPartition
follows the concatenation strategy of RNAcofold, but are orders of magnitude
faster than RNAcofold. For example, on a sequence pair with combined length of
26,190 nt, LinearCoFold is 86.8x faster than RNAcofold MFE mode (0.6 minutes
vs. 52.1 minutes), and LinearCoPartition is 642.3x faster than RNAcofold
partition function mode (1.8 minutes vs. 1156.2 minutes). Different from the
local algorithms, LinearCoFold and LinearCoPartition are global cofolding
algorithms without restriction on base pair length. Surprisingly, LinearCoFold
and LinearCoPartition's predictions have higher PPV and sensitivity of
intermolecular base pairs. Furthermore, we apply LinearCoFold to predict the
RNA-RNA interaction between SARS-CoV-2 gRNA and human U4 snRNA, which has been
experimentally studied, and observe that LinearCoFold's prediction correlates
better to the wet lab results.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15014'>Counting Perfect Matchings in Dense Graphs Is Hard</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Nicolas El Maalouly, Yanheng Wang</p><p>We show that the problem of counting perfect matchings remains #P-complete
even if we restrict the input to very dense graphs, proving the conjecture in
[5]. Here "dense graphs" refer to bipartite graphs of bipartite independence
number $\leq 2$, or general graphs of independence number $\leq 2$. Our proof
is by reduction from counting perfect matchings in bipartite graphs, via
elementary linear algebra tricks and graph constructions.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Maalouly_N/0/1/0/all/0/1">Nicolas El Maalouly</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanheng Wang</a></p><p>We show that the problem of counting perfect matchings remains #P-complete
even if we restrict the input to very dense graphs, proving the conjecture in
[5]. Here "dense graphs" refer to bipartite graphs of bipartite independence
number $\leq 2$, or general graphs of independence number $\leq 2$. Our proof
is by reduction from counting perfect matchings in bipartite graphs, via
elementary linear algebra tricks and graph constructions.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15114'>Faster Linear Algebra for Distance Matrices</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Piotr Indyk, Sandeep Silwal</p><p>The distance matrix of a dataset $X$ of $n$ points with respect to a distance
function $f$ represents all pairwise distances between points in $X$ induced by
$f$. Due to their wide applicability, distance matrices and related families of
matrices have been the focus of many recent algorithmic works. We continue this
line of research and take a broad view of algorithm design for distance
matrices with the goal of designing fast algorithms, which are specifically
tailored for distance matrices, for fundamental linear algebraic primitives.
Our results include efficient algorithms for computing matrix-vector products
for a wide class of distance matrices, such as the $\ell_1$ metric for which we
get a linear runtime, as well as an $\Omega(n^2)$ lower bound for any algorithm
which computes a matrix-vector product for the $\ell_{\infty}$ case, showing a
separation between the $\ell_1$ and the $\ell_{\infty}$ metrics. Our upper
bound results, in conjunction with recent works on the matrix-vector query
model, have many further downstream applications, including the fastest
algorithm for computing a relative error low-rank approximation for the
distance matrix induced by $\ell_1$ and $\ell_2^2$ functions and the fastest
algorithm for computing an additive error low-rank approximation for the
$\ell_2$ metric, in addition to applications for fast matrix multiplication
among others. We also give algorithms for constructing distance matrices and
show that one can construct an approximate $\ell_2$ distance matrix in time
faster than the bound implied by the Johnson-Lindenstrauss lemma.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Indyk_P/0/1/0/all/0/1">Piotr Indyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Silwal_S/0/1/0/all/0/1">Sandeep Silwal</a></p><p>The distance matrix of a dataset $X$ of $n$ points with respect to a distance
function $f$ represents all pairwise distances between points in $X$ induced by
$f$. Due to their wide applicability, distance matrices and related families of
matrices have been the focus of many recent algorithmic works. We continue this
line of research and take a broad view of algorithm design for distance
matrices with the goal of designing fast algorithms, which are specifically
tailored for distance matrices, for fundamental linear algebraic primitives.
Our results include efficient algorithms for computing matrix-vector products
for a wide class of distance matrices, such as the $\ell_1$ metric for which we
get a linear runtime, as well as an $\Omega(n^2)$ lower bound for any algorithm
which computes a matrix-vector product for the $\ell_{\infty}$ case, showing a
separation between the $\ell_1$ and the $\ell_{\infty}$ metrics. Our upper
bound results, in conjunction with recent works on the matrix-vector query
model, have many further downstream applications, including the fastest
algorithm for computing a relative error low-rank approximation for the
distance matrix induced by $\ell_1$ and $\ell_2^2$ functions and the fastest
algorithm for computing an additive error low-rank approximation for the
$\ell_2$ metric, in addition to applications for fast matrix multiplication
among others. We also give algorithms for constructing distance matrices and
show that one can construct an approximate $\ell_2$ distance matrix in time
faster than the bound implied by the Johnson-Lindenstrauss lemma.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15178'>Anonymized Histograms in Intermediate Privacy Models</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi</p><p>We study the problem of privately computing the anonymized histogram (a.k.a.
unattributed histogram), which is defined as the histogram without item labels.
Previous works have provided algorithms with $\ell_1$- and $\ell_2^2$-errors of
$O_\varepsilon(\sqrt{n})$ in the central model of differential privacy (DP).
</p>
<p>In this work, we provide an algorithm with a nearly matching error guarantee
of $\tilde{O}_\varepsilon(\sqrt{n})$ in the shuffle DP and pan-private models.
Our algorithm is very simple: it just post-processes the discrete
Laplace-noised histogram! Using this algorithm as a subroutine, we show
applications in privately estimating symmetric properties of distributions such
as entropy, support coverage, and support size.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ghazi_B/0/1/0/all/0/1">Badih Ghazi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamath_P/0/1/0/all/0/1">Pritish Kamath</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1">Ravi Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Manurangsi_P/0/1/0/all/0/1">Pasin Manurangsi</a></p><p>We study the problem of privately computing the anonymized histogram (a.k.a.
unattributed histogram), which is defined as the histogram without item labels.
Previous works have provided algorithms with $\ell_1$- and $\ell_2^2$-errors of
$O_\varepsilon(\sqrt{n})$ in the central model of differential privacy (DP).
</p>
<p>In this work, we provide an algorithm with a nearly matching error guarantee
of $\tilde{O}_\varepsilon(\sqrt{n})$ in the shuffle DP and pan-private models.
Our algorithm is very simple: it just post-processes the discrete
Laplace-noised histogram! Using this algorithm as a subroutine, we show
applications in privately estimating symmetric properties of distributions such
as entropy, support coverage, and support size.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15193'>A framework of distributionally robust possibilistic optimization</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Romain Guillaume, Adam Kasperski, Pawel Zielinski</p><p>In this paper, an optimization problem with uncertain constraint coefficients
is considered. Possibility theory is used to model the uncertainty. Namely, a
joint possibility distribution in constraint coefficient realizations, called
scenarios, is specified. This possibility distribution induces a necessity
measure in scenario set, which in turn describes an ambiguity set of
probability distributions in scenario set. The distributionally robust approach
is then used to convert the imprecise constraints into deterministic
equivalents. Namely, the left-hand side of an imprecise constraint is evaluated
by using a risk measure with respect to the worst probability distribution that
can occur. In this paper, the Conditional Value at Risk is used as the risk
measure, which generalizes the strict robust and expected value approaches,
commonly used in literature. A general framework for solving such a class of
problems is described. Some cases which can be solved in polynomial time are
identified.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Guillaume_R/0/1/0/all/0/1">Romain Guillaume</a>, <a href="http://arxiv.org/find/math/1/au:+Kasperski_A/0/1/0/all/0/1">Adam Kasperski</a>, <a href="http://arxiv.org/find/math/1/au:+Zielinski_P/0/1/0/all/0/1">Pawel Zielinski</a></p><p>In this paper, an optimization problem with uncertain constraint coefficients
is considered. Possibility theory is used to model the uncertainty. Namely, a
joint possibility distribution in constraint coefficient realizations, called
scenarios, is specified. This possibility distribution induces a necessity
measure in scenario set, which in turn describes an ambiguity set of
probability distributions in scenario set. The distributionally robust approach
is then used to convert the imprecise constraints into deterministic
equivalents. Namely, the left-hand side of an imprecise constraint is evaluated
by using a risk measure with respect to the worst probability distribution that
can occur. In this paper, the Conditional Value at Risk is used as the risk
measure, which generalizes the strict robust and expected value approaches,
commonly used in literature. A general framework for solving such a class of
problems is described. Some cases which can be solved in polynomial time are
identified.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15421'>AnyDijkstra, an algorithm to compute shortest paths on images with anytime properties</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Diego Ulisse Pizzagalli, Rolf Krause</p><p>Images conveniently capture the result of physical processes, representing
rich source of information for data driven medicine, engineering, and science.
The modeling of an image as a graph allows the application of graph-based
algorithms for content analysis. Amongst these, one of the most used is the
Dijkstra Single Source Shortest Path algorithm (DSSSP), which computes the path
with minimal cost from one starting node to all the other nodes of the graph.
However, the results of DSSSP remains unknown for nodes until they are
explored. Moreover, DSSSP execution is associated to frequent jumps between
distant locations in the graph, which results in non-optimal memory access,
reduced parallelization, and finally increased execution time. Therefore, we
propose AnyDijkstra, an iterative implementation of the Dijkstra SSSP algorithm
optimized for images, that retains anytime properties while accessing memory
following a cache-friendly scheme and maximizing parallelization.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Pizzagalli_D/0/1/0/all/0/1">Diego Ulisse Pizzagalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Krause_R/0/1/0/all/0/1">Rolf Krause</a></p><p>Images conveniently capture the result of physical processes, representing
rich source of information for data driven medicine, engineering, and science.
The modeling of an image as a graph allows the application of graph-based
algorithms for content analysis. Amongst these, one of the most used is the
Dijkstra Single Source Shortest Path algorithm (DSSSP), which computes the path
with minimal cost from one starting node to all the other nodes of the graph.
However, the results of DSSSP remains unknown for nodes until they are
explored. Moreover, DSSSP execution is associated to frequent jumps between
distant locations in the graph, which results in non-optimal memory access,
reduced parallelization, and finally increased execution time. Therefore, we
propose AnyDijkstra, an iterative implementation of the Dijkstra SSSP algorithm
optimized for images, that retains anytime properties while accessing memory
following a cache-friendly scheme and maximizing parallelization.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15439'>Learning versus Refutation in Noninteractive Local Differential Privacy</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Alexander Edmonds, Aleksandar Nikolov, Toniann Pitassi</p><p>We study two basic statistical tasks in non-interactive local differential
privacy (LDP): learning and refutation. Learning requires finding a concept
that best fits an unknown target function (from labelled samples drawn from a
distribution), whereas refutation requires distinguishing between data
distributions that are well-correlated with some concept in the class, versus
distributions where the labels are random. Our main result is a complete
characterization of the sample complexity of agnostic PAC learning for
non-interactive LDP protocols. We show that the optimal sample complexity for
any concept class is captured by the approximate $\gamma_2$~norm of a natural
matrix associated with the class. Combined with previous work [Edmonds, Nikolov
and Ullman, 2019] this gives an equivalence between learning and refutation in
the agnostic setting.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/stat/1/au:+Edmonds_A/0/1/0/all/0/1">Alexander Edmonds</a>, <a href="http://arxiv.org/find/stat/1/au:+Nikolov_A/0/1/0/all/0/1">Aleksandar Nikolov</a>, <a href="http://arxiv.org/find/stat/1/au:+Pitassi_T/0/1/0/all/0/1">Toniann Pitassi</a></p><p>We study two basic statistical tasks in non-interactive local differential
privacy (LDP): learning and refutation. Learning requires finding a concept
that best fits an unknown target function (from labelled samples drawn from a
distribution), whereas refutation requires distinguishing between data
distributions that are well-correlated with some concept in the class, versus
distributions where the labels are random. Our main result is a complete
characterization of the sample complexity of agnostic PAC learning for
non-interactive LDP protocols. We show that the optimal sample complexity for
any concept class is captured by the approximate $\gamma_2$~norm of a natural
matrix associated with the class. Combined with previous work [Edmonds, Nikolov
and Ullman, 2019] this gives an equivalence between learning and refutation in
the agnostic setting.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15630'>In-stream Probabilistic Cardinality Estimation for Bloom Filters</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Remy Scholler, Jean-Francois Couchot, Oumaima Alaoui-Ismaili, Denis Renaud, Eric Ballot</p><p>The amount of data coming from different sources such as IoT-sensors, social
networks, cellular networks, has increased exponentially during the last few
years. Probabilistic Data Structures (PDS) are efficient alternatives to
deterministic data structures suitable for large data processing and streaming
applications. They are mainly used for approximate membership queries,
frequency count, cardinality estimation and similarity research. Finding the
number of distinct elements in a large dataset or in streaming data is an
active research area. In this work, we show that usual methods based on Bloom
filters for this kind of cardinality estimation are relatively accurate on
average but have a high variance. Therefore, reducing this variance is
interesting to obtain accurate statistics. We propose a probabilistic approach
to estimate more accurately the cardinality of a Bloom filter based on its
parameters, i.e., number of hash functions $k$, size $m$, and a counter $s$
which is incremented whenever an element is not in the filter (i.e., when the
result of the membership query for this element is negative). The value of the
counter can never be larger than the exact cardinality due to the Bloom
filter's nature, but hash collisions can cause it to underestimate it. This
creates a counting error that we estimate accurately, in-stream, along with its
standard deviation. We also discuss a way to optimize the parameters of a Bloom
filter based on its counting error. We evaluate our approach with synthetic
data created from an analysis of a real mobility dataset provided by a mobile
network operator in the form of displacement matrices computed from mobile
phone records. The approach proposed here performs at least as well on average
and has a much lower variance (about 6 to 7 times less) than state of the art
methods.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Scholler_R/0/1/0/all/0/1">Remy Scholler</a>, <a href="http://arxiv.org/find/cs/1/au:+Couchot_J/0/1/0/all/0/1">Jean-Francois Couchot</a>, <a href="http://arxiv.org/find/cs/1/au:+Alaoui_Ismaili_O/0/1/0/all/0/1">Oumaima Alaoui-Ismaili</a>, <a href="http://arxiv.org/find/cs/1/au:+Renaud_D/0/1/0/all/0/1">Denis Renaud</a>, <a href="http://arxiv.org/find/cs/1/au:+Ballot_E/0/1/0/all/0/1">Eric Ballot</a></p><p>The amount of data coming from different sources such as IoT-sensors, social
networks, cellular networks, has increased exponentially during the last few
years. Probabilistic Data Structures (PDS) are efficient alternatives to
deterministic data structures suitable for large data processing and streaming
applications. They are mainly used for approximate membership queries,
frequency count, cardinality estimation and similarity research. Finding the
number of distinct elements in a large dataset or in streaming data is an
active research area. In this work, we show that usual methods based on Bloom
filters for this kind of cardinality estimation are relatively accurate on
average but have a high variance. Therefore, reducing this variance is
interesting to obtain accurate statistics. We propose a probabilistic approach
to estimate more accurately the cardinality of a Bloom filter based on its
parameters, i.e., number of hash functions $k$, size $m$, and a counter $s$
which is incremented whenever an element is not in the filter (i.e., when the
result of the membership query for this element is negative). The value of the
counter can never be larger than the exact cardinality due to the Bloom
filter's nature, but hash collisions can cause it to underestimate it. This
creates a counting error that we estimate accurately, in-stream, along with its
standard deviation. We also discuss a way to optimize the parameters of a Bloom
filter based on its counting error. We evaluate our approach with synthetic
data created from an analysis of a real mobility dataset provided by a mobile
network operator in the form of displacement matrices computed from mobile
phone records. The approach proposed here performs at least as well on average
and has a much lower variance (about 6 to 7 times less) than state of the art
methods.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Thursday, October 27
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/10/27/postdoctoral-fellowship-the-institute-for-emerging-core-methods-in-data-science-encore-at-university-of-california-san-diego-apply-by-december-15-2022/'>Postdoctoral fellowship – The Institute for Emerging CORE Methods in Data Science (EnCORE) at University of California – San Diego (apply by December 15, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Multiple postdoctoral fellowship opportunities are available with The Institute for Emerging CORE Methods in Data Science (EnCORE), a TRIPODS Phase II institute funded by the National Science Foundation. The EnCORE Institute is a collaboration of researchers between UC San Diego, UCLA, UT Austin and Penn. Website: academicjobsonline.org/ajo/jobs/23469 Email: bsaha@eng.ucsd.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Multiple postdoctoral fellowship opportunities are available with The Institute for Emerging CORE Methods in Data Science (EnCORE), a TRIPODS Phase II institute funded by the National Science Foundation. The EnCORE Institute is a collaboration of researchers between UC San Diego, UCLA, UT Austin and Penn.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/23469">https://academicjobsonline.org/ajo/jobs/23469</a><br />
Email: bsaha@eng.ucsd.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T23:19:32Z">Thursday, October 27 2022, 23:19</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/10/27/postdoc-at-sandia-national-labs-apply-by-december-20-2022/'>postdoc at Sandia National Labs (apply by December 20, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Sandia Labs is seeking a postdoc to work on quantum or quantum-inspired classical approximation, sublinear, or streaming algorithms, as part of a DOE-funded collaboration among several national labs and universities. We encourage theoretical computer scientists interested in quantum information but without prior expertise to apply! Website: far-qc.sandia.gov/job-opportunities/ Email: odparek@sandia.gov
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Sandia Labs is seeking a postdoc to work on quantum or quantum-inspired classical approximation, sublinear, or streaming algorithms, as part of a DOE-funded collaboration among several national labs and universities. We encourage theoretical computer scientists interested in quantum information but without prior expertise to apply!</p>
<p>Website: <a href="https://far-qc.sandia.gov/job-opportunities/">https://far-qc.sandia.gov/job-opportunities/</a><br />
Email: odparek@sandia.gov</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T20:38:30Z">Thursday, October 27 2022, 20:38</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://blog.computationalcomplexity.org/2022/10/the-media-coverage-of-matrix-result-is.html'>The Media Coverage of the Matrix result is Terrible (though not worse than usual)</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>&nbsp;BILL: A computer program (or an AI or an ML or whatever) found a BETTER way to do matrix mult! Its in the same spirit as Strassen. I've always wondered if Strassen was practical&nbsp; since it is simple, and computers have come a long way since 1969, though I suspect not (I WAS WRONG ABOUT THAT). I'll blog about and ask if Strassen will ever be used/practical&nbsp; &nbsp;(I did that post&nbsp;here).</p><p>READERS: Uh, Bill,&nbsp; (1) Strassen IS used and practical and (2) the new algorithm only works in&nbsp; GF(2). (Lance did a post about the new algorithm where he makes this explicit&nbsp;here.) Some readers claimed it was GF(2^k) and some that it was fields if char 2. In any case NO it is not a general algorithm.</p><p>BILL: There is good news and what others might consider bad news but I do not.</p><p>GOOD NEWS: I learned that Strassen IS practical and used, which I did not know.&nbsp;</p><p>GOOD NEWS: I learned that I was WRONG about the new algorithm since I just assumed it worked in general, and updated the post so others would not be deceived.&nbsp;</p><p>BAD NEWS: Darling asked if I was embarrassed to be wrong. If I am embarrassed that easily I would have quit blogging in 2009.&nbsp;</p><p>DARLING: So Bill, how did you get it so wrong?</p><p>BILL: Well obviously my bad for not doing my due diligence. But that's not what's interesting. What's interesting is that if you read the articles about it for the popular press you would have NO IDEA that it only works for mod 2. Its like reading that quantum computing will solve world hunger.</p><p>DARLING: It won't?</p><p>BILL: No it won't.&nbsp;</p><p>DARLING: I was being sarcastic.&nbsp;</p><p>BILL: Anyway, the coverage pushed two points</p><p>a) IMPRESSIVE that a computer could FIND these things that humans could not. This is TRUE (gee, how do I know that? The Gell-Mann Effect,&nbsp; is that people disgusted when they read a newspaper article on something they know about and find the mistakes later assume that the other articles are fine. SHOUT OUT to Jim Hefferon who telling me the name Gell-Mann Effect and left a comment with a pointer. The original version of this post had a BLANK there.)&nbsp;</p><p>b) The algorithm is practical! They did not quite say that but it was implied. And certainly there was NO mention of it only working in GF(2). And I was fooled into thinking that it might be competitive with Strassen.&nbsp;</p><p>READERS (of this blog entry, I predict) Uh, Bill, the popular press getting science news wrong and saying its more practical than it is probably predates the Bible. I can imagine&nbsp; the Cairo Times in 2000BC writing&nbsp;`Scientists discover that in any right triangle with sides a,b,c&nbsp; a^2+b^2=c^2 and this will enable us to build food silos and cure Hunger. In reality they knew that the 3,4,5 triangle was a right triangle, were no where near a proof of a general theorem, and I doubt it would have helped cure hunger.&nbsp;</p><p>BILL: This time the news was REALLY CLOSE to what I do (if&nbsp; R(5) is found by a computer and the media claims its practical I'll either have a very angry blog or repost my April Fools' day article on Ramsey Theory's application to&nbsp; History) AND I posted incorrectly about it. So, to quote many a bad movie</p><p>THIS TIME ITS PERSONAL!</p><p>By gasarch</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>&nbsp;BILL: A computer program (or an AI or an ML or whatever) found a BETTER way to do matrix mult! Its in the same spirit as Strassen. I've always wondered if Strassen was practical&nbsp; since it is simple, and computers have come a long way since 1969, though I suspect not (I WAS WRONG ABOUT THAT). I'll blog about and ask if Strassen will ever be used/practical&nbsp; &nbsp;(I did that post&nbsp;<a href="https://blog.computationalcomplexity.org/2022/10/will-strassens-matrix-mult-alg-ever-be.html">here</a>).</p><p>READERS: Uh, Bill,&nbsp; (1) Strassen IS used and practical and (2) the new algorithm only works in&nbsp; GF(2). (Lance did a post about the new algorithm where he makes this explicit&nbsp;<a href="https://blog.computationalcomplexity.org/2022/10/alpha-tensor.html">here</a>.) Some readers claimed it was GF(2^k) and some that it was fields if char 2. In any case NO it is not a general algorithm.</p><p>BILL: There is good news and what others might consider bad news but I do not.</p><p>GOOD NEWS: I learned that Strassen IS practical and used, which I did not know.&nbsp;</p><p>GOOD NEWS: I learned that I was WRONG about the new algorithm since I just assumed it worked in general, and updated the post so others would not be deceived.&nbsp;</p><p>BAD NEWS: Darling asked if I was embarrassed to be wrong. If I am embarrassed that easily I would have quit blogging in 2009.&nbsp;</p><p>DARLING: So Bill, how did you get it so wrong?</p><p>BILL: Well obviously my bad for not doing my due diligence. But that's not what's interesting. What's interesting is that if you read the articles about it for the popular press you would have NO IDEA that it only works for mod 2. Its like reading that quantum computing will solve world hunger.</p><p>DARLING: It won't?</p><p>BILL: No it won't.&nbsp;</p><p>DARLING: I was being sarcastic.&nbsp;</p><p>BILL: Anyway, the coverage pushed two points</p><p>a) IMPRESSIVE that a computer could FIND these things that humans could not. This is TRUE (gee, how do I know that? <i>The Gell-Mann Effec</i>t,&nbsp; is that people disgusted when they read a newspaper article on something they know about and find the mistakes later assume that the other articles are fine. SHOUT OUT to Jim Hefferon who telling me the name <i>Gell-Mann Effect</i> and left a comment with a pointer. The original version of this post had a BLANK there.)&nbsp;</p><p>b) The algorithm is practical! They did not quite say that but it was implied. And certainly there was NO mention of it only working in GF(2). And I was fooled into thinking that it might be competitive with Strassen.&nbsp;</p><p>READERS (of this blog entry, I predict) Uh, Bill, the popular press getting science news wrong and saying its more practical than it is probably predates the Bible. I can imagine&nbsp; the Cairo Times in 2000BC writing&nbsp;<i>`Scientists discover that in any right triangle with sides a,b,c&nbsp; a^2+b^2=c^2 and this will</i> <i>enable us to build food silos and cure Hunger</i>. In reality they knew that the 3,4,5 triangle was a right triangle, were no where near a proof of a general theorem, and I doubt it would have helped cure hunger.&nbsp;</p><p>BILL: This time the news was REALLY CLOSE to what I do (if&nbsp; R(5) is found by a computer and the media claims its practical I'll either have a very angry blog or repost my April Fools' day article on Ramsey Theory's application to&nbsp; History) AND I posted incorrectly about it. So, to quote many a bad movie</p><p><b>THIS TIME ITS PERSONAL!</b></p><p class="authors">By gasarch</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T11:46:00Z">Thursday, October 27 2022, 11:46</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.14259'>Net Separation-Oriented Printed Circuit Board Placement via Margin Maximization</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Chung-Kuan Cheng, Chia-Tung Ho, Chester Holtz</p><p>Packaging has become a crucial process due to the paradigm shift of More than
Moore. Addressing manufacturing and yield issues is a significant challenge for
modern layout algorithms.
</p>
<p>We propose to use printed circuit board (PCB) placement as a benchmark for
the packaging problem. A maximum-margin formulation is devised to improve the
separation between nets. Our framework includes seed layout proposals, a
coordinate descent-based procedure to optimize routability, and a mixed-integer
linear programming method to legalize the layout. We perform an extensive study
with 14 PCB designs and an open-source router. We show that the placements
produced by NS-place improve routed wirelength by up to 25\%, reduce the number
of vias by up to 50\%, and reduce the number of DRVs by 79\% compared to manual
and wirelength-minimal placements.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1">Chung-Kuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1">Chia-Tung Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Holtz_C/0/1/0/all/0/1">Chester Holtz</a></p><p>Packaging has become a crucial process due to the paradigm shift of More than
Moore. Addressing manufacturing and yield issues is a significant challenge for
modern layout algorithms.
</p>
<p>We propose to use printed circuit board (PCB) placement as a benchmark for
the packaging problem. A maximum-margin formulation is devised to improve the
separation between nets. Our framework includes seed layout proposals, a
coordinate descent-based procedure to optimize routability, and a mixed-integer
linear programming method to legalize the layout. We perform an extensive study
with 14 PCB designs and an open-source router. We show that the placements
produced by NS-place improve routed wirelength by up to 25\%, reduce the number
of vias by up to 50\%, and reduce the number of DRVs by 79\% compared to manual
and wirelength-minimal placements.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T00:30:00Z">Thursday, October 27 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.14736'>An Optimal Lower Bound for Simplex Range Reporting</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Peyman Afshani, Pingan Cheng</p><p>We give a simplified and improved lower bound for the simplex range reporting
problem. We show that given a set $P$ of $n$ points in $\mathbb{R}^d$, any data
structure that uses $S(n)$ space to answer such queries must have
$Q(n)=\Omega((n^2/S(n))^{(d-1)/d}+k)$ query time, where $k$ is the output size.
For near-linear space data structures, i.e., $S(n)=O(n\log^{O(1)}n)$, this
improves the previous lower bounds by Chazelle and Rosenberg [CR96] and Afshani
[A12] but perhaps more importantly, it is the first ever tight lower bound for
any variant of simplex range searching for $d\ge 3$ dimensions.
</p>
<p>We obtain our lower bound by making a simple connection to well-studied
problems in incident geometry which allows us to use known constructions in the
area. We observe that a small modification of a simple already existing
construction can lead to our lower bound. We believe that our proof is
accessible to a much wider audience, at least compared to the previous
intricate probabilistic proofs based on measure arguments by Chazelle and
Rosenberg [CR96] and Afshani [A12].
</p>
<p>The lack of tight or almost-tight (up to polylogarithmic factor) lower bounds
for near-linear space data structures is a major bottleneck in making progress
on problems such as proving lower bounds for multilevel data structures. It is
our hope that this new line of attack based on incidence geometry can lead to
further progress in this area.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Afshani_P/0/1/0/all/0/1">Peyman Afshani</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1">Pingan Cheng</a></p><p>We give a simplified and improved lower bound for the simplex range reporting
problem. We show that given a set $P$ of $n$ points in $\mathbb{R}^d$, any data
structure that uses $S(n)$ space to answer such queries must have
$Q(n)=\Omega((n^2/S(n))^{(d-1)/d}+k)$ query time, where $k$ is the output size.
For near-linear space data structures, i.e., $S(n)=O(n\log^{O(1)}n)$, this
improves the previous lower bounds by Chazelle and Rosenberg [CR96] and Afshani
[A12] but perhaps more importantly, it is the first ever tight lower bound for
any variant of simplex range searching for $d\ge 3$ dimensions.
</p>
<p>We obtain our lower bound by making a simple connection to well-studied
problems in incident geometry which allows us to use known constructions in the
area. We observe that a small modification of a simple already existing
construction can lead to our lower bound. We believe that our proof is
accessible to a much wider audience, at least compared to the previous
intricate probabilistic proofs based on measure arguments by Chazelle and
Rosenberg [CR96] and Afshani [A12].
</p>
<p>The lack of tight or almost-tight (up to polylogarithmic factor) lower bounds
for near-linear space data structures is a major bottleneck in making progress
on problems such as proving lower bounds for multilevel data structures. It is
our hope that this new line of attack based on incidence geometry can lead to
further progress in this area.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T00:30:00Z">Thursday, October 27 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.14664'>Coresets for Vertical Federated Learning: Regularized Linear Regression and $K$-Means Clustering</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Lingxiao Huang, Zhize Li, Jialin Sun, Haoyu Zhao</p><p>Vertical federated learning (VFL), where data features are stored in multiple
parties distributively, is an important area in machine learning. However, the
communication complexity for VFL is typically very high. In this paper, we
propose a unified framework by constructing coresets in a distributed fashion
for communication-efficient VFL. We study two important learning tasks in the
VFL setting: regularized linear regression and $k$-means clustering, and apply
our coreset framework to both problems. We theoretically show that using
coresets can drastically alleviate the communication complexity, while nearly
maintain the solution quality. Numerical experiments are conducted to
corroborate our theoretical findings.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lingxiao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhize Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jialin Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Haoyu Zhao</a></p><p>Vertical federated learning (VFL), where data features are stored in multiple
parties distributively, is an important area in machine learning. However, the
communication complexity for VFL is typically very high. In this paper, we
propose a unified framework by constructing coresets in a distributed fashion
for communication-efficient VFL. We study two important learning tasks in the
VFL setting: regularized linear regression and $k$-means clustering, and apply
our coreset framework to both problems. We theoretically show that using
coresets can drastically alleviate the communication complexity, while nearly
maintain the solution quality. Numerical experiments are conducted to
corroborate our theoretical findings.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T00:30:00Z">Thursday, October 27 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.14315'>Streaming Submodular Maximization with Differential Privacy</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Anamay Chaturvedi, Huy L&#xea; Nguyen, Thy Nguyen</p><p>In this work, we study the problem of privately maximizing a submodular
function in the streaming setting. Extensive work has been done on privately
maximizing submodular functions in the general case when the function depends
upon the private data of individuals. However, when the size of the data stream
drawn from the domain of the objective function is large or arrives very fast,
one must privately optimize the objective within the constraints of the
streaming setting. We establish fundamental differentially private baselines
for this problem and then derive better trade-offs between privacy and utility
for the special case of decomposable submodular functions. A submodular
function is decomposable when it can be written as a sum of submodular
functions; this structure arises naturally when each summand function models
the utility of an individual and the goal is to study the total utility of the
whole population as in the well-known Combinatorial Public Projects Problem.
Finally, we complement our theoretical analysis with experimental
corroboration.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chaturvedi_A/0/1/0/all/0/1">Anamay Chaturvedi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Huy L&#xea; Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thy Nguyen</a></p><p>In this work, we study the problem of privately maximizing a submodular
function in the streaming setting. Extensive work has been done on privately
maximizing submodular functions in the general case when the function depends
upon the private data of individuals. However, when the size of the data stream
drawn from the domain of the objective function is large or arrives very fast,
one must privately optimize the objective within the constraints of the
streaming setting. We establish fundamental differentially private baselines
for this problem and then derive better trade-offs between privacy and utility
for the special case of decomposable submodular functions. A submodular
function is decomposable when it can be written as a sum of submodular
functions; this structure arises naturally when each summand function models
the utility of an individual and the goal is to study the total utility of the
whole population as in the well-known Combinatorial Public Projects Problem.
Finally, we complement our theoretical analysis with experimental
corroboration.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T00:30:00Z">Thursday, October 27 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.14582'>WebCrack: Dynamic Dictionary Adjustment for Web Weak Password Detection based on Blasting Response Event Discrimination</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xiang Long, Yan Huang, Zhendong Liu, Lansheng Han, Haili Sun, Jingyuan He</p><p>The feature diversity of different web systems in page elements, submission
contents and return information makes it difficult to detect weak password
automatically. To solve this problem, multi-factor correlation detection method
as integrated in the DBKER algorithm is proposed to achieve automatic detection
of web weak passwords and universal passwords. It generates password
dictionaries based on PCFG algorithm, proposes to judge blasting result via 4
steps with traditional static keyword features and dynamic page feature
information. Then the blasting failure events are discriminated and the
usernames are blasted based on response time. Thereafter the weak password
dictionary is dynamically adjusted according to the hints provided by the
response failure page. Based on the algorithm, this paper implements a
detection system named WebCrack. Experimental results of two blasting tests on
DedeCMS and Discuz! systems as well as a random backend test show that the
proposed method can detect weak passwords and universal passwords of various
web systems with an average accuracy rate of about 93.75%, providing security
advisories for users' password settings with strong practicability.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1">Xiang Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhendong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1">Lansheng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Haili Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jingyuan He</a></p><p>The feature diversity of different web systems in page elements, submission
contents and return information makes it difficult to detect weak password
automatically. To solve this problem, multi-factor correlation detection method
as integrated in the DBKER algorithm is proposed to achieve automatic detection
of web weak passwords and universal passwords. It generates password
dictionaries based on PCFG algorithm, proposes to judge blasting result via 4
steps with traditional static keyword features and dynamic page feature
information. Then the blasting failure events are discriminated and the
usernames are blasted based on response time. Thereafter the weak password
dictionary is dynamically adjusted according to the hints provided by the
response failure page. Based on the algorithm, this paper implements a
detection system named WebCrack. Experimental results of two blasting tests on
DedeCMS and Discuz! systems as well as a random backend test show that the
proposed method can detect weak passwords and universal passwords of various
web systems with an average accuracy rate of about 93.75%, providing security
advisories for users' password settings with strong practicability.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T00:30:00Z">Thursday, October 27 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.14608'>Inapproximability of shortest paths on perfect matching polytopes</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jean Cardinal, Raphael Steiner</p><p>We consider the computational problem of finding short paths in the skeleton
of the perfect matching polytope of a bipartite graph. We prove that unless
$P=NP$, there is no polynomial-time algorithm that computes a path of constant
length between two vertices at distance two of the perfect matching polytope of
a bipartite graph. Conditioned on $P\neq NP$, this disproves a conjecture by
Ito, Kakimura, Kamiyama, Kobayashi and Okamoto [SIAM Journal on Discrete
Mathematics, 36(2), pp. 1102-1123 (2022)]. Assuming the Exponential Time
Hypothesis we prove the stronger result that there exists no polynomial-time
algorithm computing a path of length at most
$\left(\frac{1}{4}-o(1)\right)\frac{\log N}{\log \log N}$ between two vertices
at distance two of the perfect matching polytope of an $N$-vertex bipartite
graph. These results remain true if the bipartite graph is restricted to be of
maximum degree three. The above has the following interesting implication for
the performance of pivot rules for the simplex algorithm on simply-structured
combinatorial polytopes: If $P\neq NP$, then for every simplex pivot rule
executable in polynomial time and every constant $k \in \mathbb{N}$ there
exists a linear program on a perfect matching polytope and a starting vertex of
the polytope such that the optimal solution can be reached in two monotone
steps from the starting vertex, yet the pivot rule will require at least $k$
steps to reach the optimal solution. This result remains true in the more
general setting of pivot rules for so-called circuit-augmentation algorithms.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Cardinal_J/0/1/0/all/0/1">Jean Cardinal</a>, <a href="http://arxiv.org/find/math/1/au:+Steiner_R/0/1/0/all/0/1">Raphael Steiner</a></p><p>We consider the computational problem of finding short paths in the skeleton
of the perfect matching polytope of a bipartite graph. We prove that unless
$P=NP$, there is no polynomial-time algorithm that computes a path of constant
length between two vertices at distance two of the perfect matching polytope of
a bipartite graph. Conditioned on $P\neq NP$, this disproves a conjecture by
Ito, Kakimura, Kamiyama, Kobayashi and Okamoto [SIAM Journal on Discrete
Mathematics, 36(2), pp. 1102-1123 (2022)]. Assuming the Exponential Time
Hypothesis we prove the stronger result that there exists no polynomial-time
algorithm computing a path of length at most
$\left(\frac{1}{4}-o(1)\right)\frac{\log N}{\log \log N}$ between two vertices
at distance two of the perfect matching polytope of an $N$-vertex bipartite
graph. These results remain true if the bipartite graph is restricted to be of
maximum degree three. The above has the following interesting implication for
the performance of pivot rules for the simplex algorithm on simply-structured
combinatorial polytopes: If $P\neq NP$, then for every simplex pivot rule
executable in polynomial time and every constant $k \in \mathbb{N}$ there
exists a linear program on a perfect matching polytope and a starting vertex of
the polytope such that the optimal solution can be reached in two monotone
steps from the starting vertex, yet the pivot rule will require at least $k$
steps to reach the optimal solution. This result remains true in the more
general setting of pivot rules for so-called circuit-augmentation algorithms.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T00:30:00Z">Thursday, October 27 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.14629'>Highly unbreakable graph with a fixed excluded minor are almost rigid</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Daniel Lokshtanov, Marcin Pilipczuk, Micha&#x142; Pilipczuk, Saket Saurabh</p><p>A set $X \subseteq V(G)$ in a graph $G$ is $(q,k)$-unbreakable if every
separation $(A,B)$ of order at most $k$ in $G$ satisfies $|A \cap X| \leq q$ or
$|B \cap X| \leq q$. In this paper, we prove the following result: If a graph
$G$ excludes a fixed complete graph $K_h$ as a minor and satisfies certain
unbreakability guarantees, then $G$ is almost rigid in the following sense: the
vertices of $G$ can be partitioned in an isomorphism-invariant way into a part
inducing a graph of bounded treewidth and a part that admits a small
isomorphism-invariant family of labelings. This result is the key ingredient in
the fixed-parameter algorithm for Graph Isomorphism parameterized by the
Hadwiger number of the graph, which is presented in a companion paper.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Lokshtanov_D/0/1/0/all/0/1">Daniel Lokshtanov</a>, <a href="http://arxiv.org/find/math/1/au:+Pilipczuk_M/0/1/0/all/0/1">Marcin Pilipczuk</a>, <a href="http://arxiv.org/find/math/1/au:+Pilipczuk_M/0/1/0/all/0/1">Micha&#x142; Pilipczuk</a>, <a href="http://arxiv.org/find/math/1/au:+Saurabh_S/0/1/0/all/0/1">Saket Saurabh</a></p><p>A set $X \subseteq V(G)$ in a graph $G$ is $(q,k)$-unbreakable if every
separation $(A,B)$ of order at most $k$ in $G$ satisfies $|A \cap X| \leq q$ or
$|B \cap X| \leq q$. In this paper, we prove the following result: If a graph
$G$ excludes a fixed complete graph $K_h$ as a minor and satisfies certain
unbreakability guarantees, then $G$ is almost rigid in the following sense: the
vertices of $G$ can be partitioned in an isomorphism-invariant way into a part
inducing a graph of bounded treewidth and a part that admits a small
isomorphism-invariant family of labelings. This result is the key ingredient in
the fixed-parameter algorithm for Graph Isomorphism parameterized by the
Hadwiger number of the graph, which is presented in a companion paper.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T00:30:00Z">Thursday, October 27 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.14638'>Fixed-parameter tractability of Graph Isomorphism in graphs with an excluded minor</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Daniel Lokshtanov, Marcin Pilipczuk, Micha&#x142; Pilipczuk, Saket Saurabh</p><p>We prove that Graph Isomorphism and Canonization in graphs excluding a fixed
graph $H$ as a minor can be solved by an algorithm working in time $f(H)\cdot
n^{O(1)}$, where $f$ is some function. In other words, we show that these
problems are fixed-parameter tractable when parameterized by the size of the
excluded minor, with the caveat that the bound on the running time is not
necessarily computable. The underlying approach is based on decomposing the
graph in a canonical way into unbreakable (intuitively, well-connected) parts,
which essentially provides a reduction to the case where the given
$H$-minor-free graph is unbreakable itself. This is complemented by an analysis
of unbreakable $H$-minor-free graphs, performed in a second subordinate
manuscript, which reveals that every such graph can be canonically decomposed
into a part that admits few automorphisms and a part that has bounded
treewidth.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lokshtanov_D/0/1/0/all/0/1">Daniel Lokshtanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Pilipczuk_M/0/1/0/all/0/1">Marcin Pilipczuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Pilipczuk_M/0/1/0/all/0/1">Micha&#x142; Pilipczuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Saurabh_S/0/1/0/all/0/1">Saket Saurabh</a></p><p>We prove that Graph Isomorphism and Canonization in graphs excluding a fixed
graph $H$ as a minor can be solved by an algorithm working in time $f(H)\cdot
n^{O(1)}$, where $f$ is some function. In other words, we show that these
problems are fixed-parameter tractable when parameterized by the size of the
excluded minor, with the caveat that the bound on the running time is not
necessarily computable. The underlying approach is based on decomposing the
graph in a canonical way into unbreakable (intuitively, well-connected) parts,
which essentially provides a reduction to the case where the given
$H$-minor-free graph is unbreakable itself. This is complemented by an analysis
of unbreakable $H$-minor-free graphs, performed in a second subordinate
manuscript, which reveals that every such graph can be canonically decomposed
into a part that admits few automorphisms and a part that has bounded
treewidth.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T00:30:00Z">Thursday, October 27 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.14722'>Online TSP with Known Locations</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Evripidis Bampis, Bruno Escoffier, Niklas Hahn, Michalis Xefteris</p><p>In this paper, we consider the Online Traveling Salesperson Problem (OLTSP)
where the locations of the requests are known in advance, but not their arrival
times. We study both the open variant, in which the algorithm is not required
to return to the origin when all the requests are served, as well as the closed
variant, in which the algorithm has to return to the origin after serving all
the requests. Our aim is to measure the impact of the extra knowledge of the
locations on the competitiveness of the problem. We present an online
3/2-competitive algorithm for the general case and a matching lower bound for
both the open and the closed variant. Then, we focus on some interesting metric
spaces (ring, star, semi-line), providing both lower bounds and polynomial time
online algorithms for the problem.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bampis_E/0/1/0/all/0/1">Evripidis Bampis</a>, <a href="http://arxiv.org/find/cs/1/au:+Escoffier_B/0/1/0/all/0/1">Bruno Escoffier</a>, <a href="http://arxiv.org/find/cs/1/au:+Hahn_N/0/1/0/all/0/1">Niklas Hahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Xefteris_M/0/1/0/all/0/1">Michalis Xefteris</a></p><p>In this paper, we consider the Online Traveling Salesperson Problem (OLTSP)
where the locations of the requests are known in advance, but not their arrival
times. We study both the open variant, in which the algorithm is not required
to return to the origin when all the requests are served, as well as the closed
variant, in which the algorithm has to return to the origin after serving all
the requests. Our aim is to measure the impact of the extra knowledge of the
locations on the competitiveness of the problem. We present an online
3/2-competitive algorithm for the general case and a matching lower bound for
both the open and the closed variant. Then, we focus on some interesting metric
spaces (ring, star, semi-line), providing both lower bounds and polynomial time
online algorithms for the problem.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T00:30:00Z">Thursday, October 27 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.14869'>An Efficient Dynamic Multi-Sources To Single-Destination (DMS-SD) Algorithm In Smart City Navigation Using Adjacent Matrix</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ziren Xiao, Ruxin Xiao, Chang Liu, Honghao Gao, Xiaolong Xu, Shan Luo, Xinheng Wang</p><p>Dijkstra's algorithm is one of the most popular classic path planning
algorithms, achieving optimal solutions across a wide range of challenging
tasks. However, it only calculates the shortest distance from one vertex to
another, which is hard to directly apply to the Dynamic Multi-Sources to
Single-Destination (DMS-SD) problem. This paper proposes a modified Dijkstra
algorithm to address the DMS-SD problem, where the destination can be
dynamically changed. Our method deploys the concept of Adjacent Matrix from
Floyd's algorithm and achieves the goal with mathematical calculations. We
formally show that all-pairs shortest distance information in Floyd's algorithm
is not required in our algorithm. Extensive experiments verify the scalability
and optimality of the proposed method.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1">Ziren Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1">Ruxin Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Honghao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaolong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1">Shan Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinheng Wang</a></p><p>Dijkstra's algorithm is one of the most popular classic path planning
algorithms, achieving optimal solutions across a wide range of challenging
tasks. However, it only calculates the shortest distance from one vertex to
another, which is hard to directly apply to the Dynamic Multi-Sources to
Single-Destination (DMS-SD) problem. This paper proposes a modified Dijkstra
algorithm to address the DMS-SD problem, where the destination can be
dynamically changed. Our method deploys the concept of Adjacent Matrix from
Floyd's algorithm and achieves the goal with mathematical calculations. We
formally show that all-pairs shortest distance information in Floyd's algorithm
is not required in our algorithm. Extensive experiments verify the scalability
and optimality of the proposed method.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T00:30:00Z">Thursday, October 27 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.14894'>Learning to predict arbitrary quantum processes</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Hsin-Yuan Huang, Sitan Chen, John Preskill</p><p>We present an efficient machine learning (ML) algorithm for predicting any
unknown quantum process $\mathcal{E}$ over $n$ qubits. For a wide range of
distributions $\mathcal{D}$ on arbitrary $n$-qubit states, we show that this ML
algorithm can learn to predict any local property of the output from the
unknown process $\mathcal{E}$, with a small average error over input states
drawn from $\mathcal{D}$. The ML algorithm is computationally efficient even
when the unknown process is a quantum circuit with exponentially many gates.
Our algorithm combines efficient procedures for learning properties of an
unknown state and for learning a low-degree approximation to an unknown
observable. The analysis hinges on proving new norm inequalities, including a
quantum analogue of the classical Bohnenblust-Hille inequality, which we derive
by giving an improved algorithm for optimizing local Hamiltonians. Overall, our
results highlight the potential for ML models to predict the output of complex
quantum dynamics much faster than the time needed to run the process itself.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Huang_H/0/1/0/all/0/1">Hsin-Yuan Huang</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Chen_S/0/1/0/all/0/1">Sitan Chen</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Preskill_J/0/1/0/all/0/1">John Preskill</a></p><p>We present an efficient machine learning (ML) algorithm for predicting any
unknown quantum process $\mathcal{E}$ over $n$ qubits. For a wide range of
distributions $\mathcal{D}$ on arbitrary $n$-qubit states, we show that this ML
algorithm can learn to predict any local property of the output from the
unknown process $\mathcal{E}$, with a small average error over input states
drawn from $\mathcal{D}$. The ML algorithm is computationally efficient even
when the unknown process is a quantum circuit with exponentially many gates.
Our algorithm combines efficient procedures for learning properties of an
unknown state and for learning a low-degree approximation to an unknown
observable. The analysis hinges on proving new norm inequalities, including a
quantum analogue of the classical Bohnenblust-Hille inequality, which we derive
by giving an improved algorithm for optimizing local Hamiltonians. Overall, our
results highlight the potential for ML models to predict the output of complex
quantum dynamics much faster than the time needed to run the process itself.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T00:30:00Z">Thursday, October 27 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Wednesday, October 26
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/10/26/faculty-at-johns-hopkins-university-apply-by-january-6-2023/'>Faculty at Johns Hopkins University (apply by January 6, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The Johns Hopkins University’s Department of Computer Science invites applications for tenure-track faculty positions at all levels and across all areas of computer science. We are particularly interested in applicants in computer vision, networked systems, theoretical computer science, and machine learning. Website: cra.org/job/johns-hopkins-university-tenure-track-faculty-department-of-computer-science-4/ Email: fsearch2022@cs.jhu.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The Johns Hopkins University’s Department of Computer Science invites applications for tenure-track faculty positions at all levels and across all areas of computer science. We are particularly interested in applicants in computer vision, networked systems, theoretical computer science, and machine learning.</p>
<p>Website: <a href="https://cra.org/job/johns-hopkins-university-tenure-track-faculty-department-of-computer-science-4/">https://cra.org/job/johns-hopkins-university-tenure-track-faculty-department-of-computer-science-4/</a><br />
Email: fsearch2022@cs.jhu.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-26T22:56:02Z">Wednesday, October 26 2022, 22:56</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/10/26/tenured-tenure-track-positions-in-computer-science-at-nyu-shanghai-apply-by-february-1-2023/'>Tenured/Tenure-track Positions in Computer Science  at NYU Shanghai (apply by February 1, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          NYU Shanghai is currently inviting applications for Tenured or Tenure-Track positions in Computer Science. The search is not restricted to any rank and outstanding candidates at all levels are encouraged to apply. We seek candidates who have completed a Ph.D. in Computer Science, or a closely related discipline. We seek candidates in all sub-fields of [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>NYU Shanghai is currently inviting applications for Tenured or Tenure-Track positions in Computer Science. The search is not restricted to any rank and outstanding candidates at all levels are encouraged to apply. We seek candidates who have completed a Ph.D. in Computer Science, or a closely related discipline. We seek candidates in all sub-fields of Computer Science.</p>
<p>Website: <a href="https://apply.interfolio.com/116511">https://apply.interfolio.com/116511</a><br />
Email: shanghai.faculty.recruitment@nyu.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-26T16:37:08Z">Wednesday, October 26 2022, 16:37</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.13741'>Deep Neural Networks as the Semi-classical Limit of Topological Quantum Neural Networks: The problem of generalisation</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Antonino Marciano, Deen Chen, Filippo Fabrocini, Chris Fields, Matteo Lulli, Emanuele Zappala</p><p>Deep Neural Networks miss a principled model of their operation. A novel
framework for supervised learning based on Topological Quantum Field Theory
that looks particularly well suited for implementation on quantum processors
has been recently explored. We propose the use of this framework for
understanding the problem of generalization in Deep Neural Networks. More
specifically, in this approach Deep Neural Networks are viewed as the
semi-classical limit of Topological Quantum Neural Networks. A framework of
this kind explains easily the overfitting behavior of Deep Neural Networks
during the training step and the corresponding generalization capabilities.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Marciano_A/0/1/0/all/0/1">Antonino Marciano</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Chen_D/0/1/0/all/0/1">Deen Chen</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Fabrocini_F/0/1/0/all/0/1">Filippo Fabrocini</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Fields_C/0/1/0/all/0/1">Chris Fields</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Lulli_M/0/1/0/all/0/1">Matteo Lulli</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Zappala_E/0/1/0/all/0/1">Emanuele Zappala</a></p><p>Deep Neural Networks miss a principled model of their operation. A novel
framework for supervised learning based on Topological Quantum Field Theory
that looks particularly well suited for implementation on quantum processors
has been recently explored. We propose the use of this framework for
understanding the problem of generalization in Deep Neural Networks. More
specifically, in this approach Deep Neural Networks are viewed as the
semi-classical limit of Topological Quantum Neural Networks. A framework of
this kind explains easily the overfitting behavior of Deep Neural Networks
during the training step and the corresponding generalization capabilities.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-26T00:30:00Z">Wednesday, October 26 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.13694'>Worst-Case Adaptive Submodular Cover</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jing Yuan, Shaojie Tang</p><p>In this paper, we study the adaptive submodular cover problem under the
worst-case setting. This problem generalizes many previously studied problems,
namely, the pool-based active learning and the stochastic submodular set cover.
The input of our problem is a set of items (e.g., medical tests) and each item
has a random state (e.g., the outcome of a medical test), whose realization is
initially unknown. One must select an item at a fixed cost in order to observe
its realization. There is an utility function which is defined over items and
their states. Our goal is to sequentially select a group of items to achieve a
``goal value'' while minimizing the maximum cost across realizations (a.k.a.
worst-case cost). To facilitate our study, we introduce a broad class of
stochastic functions, called \emph{worst-case submodular function}. Assume the
utility function is worst-case submodular, we develop a tight $(\log
(Q/\eta)+1)$-approximation policy, where $Q$ is the ``goal value'' and $\eta$
is the minimum gap between $Q$ and any attainable utility value $\hat{Q}&lt;Q$. We
also study a worst-case maximum-coverage problem, whose goal is to select a
group of items to maximize its worst-case utility subject to a budget
constraint. This is a flipped problem of the minimum-cost-cover problem, and to
solve this problem, we develop a tight $(1-1/e)$-approximation solution.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jing Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Shaojie Tang</a></p><p>In this paper, we study the adaptive submodular cover problem under the
worst-case setting. This problem generalizes many previously studied problems,
namely, the pool-based active learning and the stochastic submodular set cover.
The input of our problem is a set of items (e.g., medical tests) and each item
has a random state (e.g., the outcome of a medical test), whose realization is
initially unknown. One must select an item at a fixed cost in order to observe
its realization. There is an utility function which is defined over items and
their states. Our goal is to sequentially select a group of items to achieve a
``goal value'' while minimizing the maximum cost across realizations (a.k.a.
worst-case cost). To facilitate our study, we introduce a broad class of
stochastic functions, called \emph{worst-case submodular function}. Assume the
utility function is worst-case submodular, we develop a tight $(\log
(Q/\eta)+1)$-approximation policy, where $Q$ is the ``goal value'' and $\eta$
is the minimum gap between $Q$ and any attainable utility value $\hat{Q}&lt;Q$. We
also study a worst-case maximum-coverage problem, whose goal is to select a
group of items to maximize its worst-case utility subject to a budget
constraint. This is a flipped problem of the minimum-cost-cover problem, and to
solve this problem, we develop a tight $(1-1/e)$-approximation solution.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-26T00:30:00Z">Wednesday, October 26 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.13706'>Gaussian Mean Testing Made Simple</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ilias Diakonikolas, Daniel M. Kane, Ankit Pensia</p><p>We study the following fundamental hypothesis testing problem, which we term
Gaussian mean testing. Given i.i.d. samples from a distribution $p$ on
$\mathbb{R}^d$, the task is to distinguish, with high probability, between the
following cases: (i) $p$ is the standard Gaussian distribution,
$\mathcal{N}(0,I_d)$, and (ii) $p$ is a Gaussian $\mathcal{N}(\mu,\Sigma)$ for
some unknown covariance $\Sigma$ and mean $\mu \in \mathbb{R}^d$ satisfying
$\|\mu\|_2 \geq \epsilon$. Recent work gave an algorithm for this testing
problem with the optimal sample complexity of $\Theta(\sqrt{d}/\epsilon^2)$.
Both the previous algorithm and its analysis are quite complicated. Here we
give an extremely simple algorithm for Gaussian mean testing with a one-page
analysis. Our algorithm is sample optimal and runs in sample linear time.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Diakonikolas_I/0/1/0/all/0/1">Ilias Diakonikolas</a>, <a href="http://arxiv.org/find/math/1/au:+Kane_D/0/1/0/all/0/1">Daniel M. Kane</a>, <a href="http://arxiv.org/find/math/1/au:+Pensia_A/0/1/0/all/0/1">Ankit Pensia</a></p><p>We study the following fundamental hypothesis testing problem, which we term
Gaussian mean testing. Given i.i.d. samples from a distribution $p$ on
$\mathbb{R}^d$, the task is to distinguish, with high probability, between the
following cases: (i) $p$ is the standard Gaussian distribution,
$\mathcal{N}(0,I_d)$, and (ii) $p$ is a Gaussian $\mathcal{N}(\mu,\Sigma)$ for
some unknown covariance $\Sigma$ and mean $\mu \in \mathbb{R}^d$ satisfying
$\|\mu\|_2 \geq \epsilon$. Recent work gave an algorithm for this testing
problem with the optimal sample complexity of $\Theta(\sqrt{d}/\epsilon^2)$.
Both the previous algorithm and its analysis are quite complicated. Here we
give an extremely simple algorithm for Gaussian mean testing with a one-page
analysis. Our algorithm is sample optimal and runs in sample linear time.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-26T00:30:00Z">Wednesday, October 26 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.13739'>Deterministic Small Vertex Connectivity in Almost Linear Time</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Thatchaphol Saranurak, Sorrachai Yingchareonthawornchai</p><p>In the vertex connectivity problem, given an undirected $n$-vertex $m$-edge
graph $G$, we need to compute the minimum number of vertices that can
disconnect $G$ after removing them. This problem is one of the most
well-studied graph problems. From 2019, a new line of work [Nanongkai et
al.~STOC'19;SODA'20;STOC'21] has used randomized techniques to break the
quadratic-time barrier and, very recently, culminated in an almost-linear time
algorithm via the recently announced maxflow algorithm by Chen et al. In
contrast, all known deterministic algorithms are much slower. The fastest
algorithm [Gabow FOCS'00] takes $O(m(n+\min\{c^{5/2},cn^{3/4}\}))$ time where
$c$ is the vertex connectivity. It remains open whether there exists a
subquadratic-time deterministic algorithm for any constant $c&gt;3$.
</p>
<p>In this paper, we give the first deterministic almost-linear time vertex
connectivity algorithm for all constants $c$. Our running time is
$m^{1+o(1)}2^{O(c^{2})}$ time, which is almost-linear for all $c=o(\sqrt{\log
n})$. This is the first deterministic algorithm that breaks the $O(n^{2})$-time
bound on sparse graphs where $m=O(n)$, which is known for more than 50 years
ago [Kleitman'69]. Towards our result, we give a new reduction framework to
vertex expanders which in turn exploits our new almost-linear time construction
of mimicking network for vertex connectivity. The previous construction by
Kratsch and Wahlstr\"{o}m [FOCS'12] requires large polynomial time and is
randomized.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Saranurak_T/0/1/0/all/0/1">Thatchaphol Saranurak</a>, <a href="http://arxiv.org/find/cs/1/au:+Yingchareonthawornchai_S/0/1/0/all/0/1">Sorrachai Yingchareonthawornchai</a></p><p>In the vertex connectivity problem, given an undirected $n$-vertex $m$-edge
graph $G$, we need to compute the minimum number of vertices that can
disconnect $G$ after removing them. This problem is one of the most
well-studied graph problems. From 2019, a new line of work [Nanongkai et
al.~STOC'19;SODA'20;STOC'21] has used randomized techniques to break the
quadratic-time barrier and, very recently, culminated in an almost-linear time
algorithm via the recently announced maxflow algorithm by Chen et al. In
contrast, all known deterministic algorithms are much slower. The fastest
algorithm [Gabow FOCS'00] takes $O(m(n+\min\{c^{5/2},cn^{3/4}\}))$ time where
$c$ is the vertex connectivity. It remains open whether there exists a
subquadratic-time deterministic algorithm for any constant $c&gt;3$.
</p>
<p>In this paper, we give the first deterministic almost-linear time vertex
connectivity algorithm for all constants $c$. Our running time is
$m^{1+o(1)}2^{O(c^{2})}$ time, which is almost-linear for all $c=o(\sqrt{\log
n})$. This is the first deterministic algorithm that breaks the $O(n^{2})$-time
bound on sparse graphs where $m=O(n)$, which is known for more than 50 years
ago [Kleitman'69]. Towards our result, we give a new reduction framework to
vertex expanders which in turn exploits our new almost-linear time construction
of mimicking network for vertex connectivity. The previous construction by
Kratsch and Wahlstr\"{o}m [FOCS'12] requires large polynomial time and is
randomized.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-26T00:30:00Z">Wednesday, October 26 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.13755'>Online and Bandit Algorithms Beyond $\ell_p$ Norms</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Thomas Kesselheim, Marco Molinaro, Sahil Singla</p><p>Vector norms play a fundamental role in computer science and optimization, so
there is an ongoing effort to generalize existing algorithms to settings beyond
$\ell_\infty$ and $\ell_p$ norms. We show that many online and bandit
applications for general norms admit good algorithms as long as the norm can be
approximated by a function that is ``gradient-stable'', a notion that we
introduce. Roughly it says that the gradient of the function should not
drastically decrease (multiplicatively) in any component as we increase the
input vector. We prove that several families of norms, including all monotone
symmetric norms, admit a gradient-stable approximation, giving us the first
online and bandit algorithms for these norm families.
</p>
<p>In particular, our notion of gradient-stability gives $O\big(\log^2
(\text{dimension})\big)$-competitive algorithms for the symmetric norm
generalizations of Online Generalized Load Balancing and Bandits with
Knapsacks. Our techniques extend to applications beyond symmetric norms as
well, e.g., to Online Vector Scheduling and to Online Generalized Assignment
with Convex Costs. Some key properties underlying our applications that are
implied by gradient-stable approximations are a ``smooth game inequality'' and
an approximate converse to Jensen's inequality.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Kesselheim_T/0/1/0/all/0/1">Thomas Kesselheim</a>, <a href="http://arxiv.org/find/cs/1/au:+Molinaro_M/0/1/0/all/0/1">Marco Molinaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1">Sahil Singla</a></p><p>Vector norms play a fundamental role in computer science and optimization, so
there is an ongoing effort to generalize existing algorithms to settings beyond
$\ell_\infty$ and $\ell_p$ norms. We show that many online and bandit
applications for general norms admit good algorithms as long as the norm can be
approximated by a function that is ``gradient-stable'', a notion that we
introduce. Roughly it says that the gradient of the function should not
drastically decrease (multiplicatively) in any component as we increase the
input vector. We prove that several families of norms, including all monotone
symmetric norms, admit a gradient-stable approximation, giving us the first
online and bandit algorithms for these norm families.
</p>
<p>In particular, our notion of gradient-stability gives $O\big(\log^2
(\text{dimension})\big)$-competitive algorithms for the symmetric norm
generalizations of Online Generalized Load Balancing and Bandits with
Knapsacks. Our techniques extend to applications beyond symmetric norms as
well, e.g., to Online Vector Scheduling and to Online Generalized Assignment
with Convex Costs. Some key properties underlying our applications that are
implied by gradient-stable approximations are a ``smooth game inequality'' and
an approximate converse to Jensen's inequality.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-26T00:30:00Z">Wednesday, October 26 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.13850'>Tight analysis of lazy: an improved algorithm for open online dial-a-ride</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Julia Baligacs, Yann Disser, David Weckbecker</p><p>In the open online dial-a-ride problem, a single server has to carry
transportation requests appearing over time in some metric space, subject to
minimizing the completion time. We improve on the best known upper bounds on
the competitive ratio on general metric spaces and on the half-line, in both,
the preemptive and non-preemptive version of the problem. We achieve this by
revisiting the algorithm Lazy recently suggested in [WAOA, 2022] and giving an
improved and tight analysis. More precisely, we show that it is
$(\frac{3}{2}+\sqrt{11/12}\thickapprox 2.457)$-competitive on general metric
spaces and $(1+\frac{1}{2}(1+\sqrt{3})\approx 2.366)$-competitive on the
half-line.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Baligacs_J/0/1/0/all/0/1">Julia Baligacs</a>, <a href="http://arxiv.org/find/cs/1/au:+Disser_Y/0/1/0/all/0/1">Yann Disser</a>, <a href="http://arxiv.org/find/cs/1/au:+Weckbecker_D/0/1/0/all/0/1">David Weckbecker</a></p><p>In the open online dial-a-ride problem, a single server has to carry
transportation requests appearing over time in some metric space, subject to
minimizing the completion time. We improve on the best known upper bounds on
the competitive ratio on general metric spaces and on the half-line, in both,
the preemptive and non-preemptive version of the problem. We achieve this by
revisiting the algorithm Lazy recently suggested in [WAOA, 2022] and giving an
improved and tight analysis. More precisely, we show that it is
$(\frac{3}{2}+\sqrt{11/12}\thickapprox 2.457)$-competitive on general metric
spaces and $(1+\frac{1}{2}(1+\sqrt{3})\approx 2.366)$-competitive on the
half-line.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-26T00:30:00Z">Wednesday, October 26 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.13854'>An Improved Algorithm for Open Online Dial-a-Ride</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Julia Baligacs, Yann Disser, Nils Mosis, David Weckbecker</p><p>We consider the open online dial-a-ride problem, where transportation
requests appear online in a metric space and need to be served by a single
server. The objective is to minimize the completion time until all requests
have been served. We present a new, parameterized algorithm for this problem
and prove that it attains a competitive ratio of $1 + \varphi \approx 2.618$
for some choice of its parameter, where $\varphi$ is the golden ratio. This
improves the best known bounds for open online dial-a-ride both for general
metric spaces as well as for the real line. We also give a lower bound
of~$2.457$ for the competitive ratio of our algorithm for any parameter choice.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Baligacs_J/0/1/0/all/0/1">Julia Baligacs</a>, <a href="http://arxiv.org/find/cs/1/au:+Disser_Y/0/1/0/all/0/1">Yann Disser</a>, <a href="http://arxiv.org/find/cs/1/au:+Mosis_N/0/1/0/all/0/1">Nils Mosis</a>, <a href="http://arxiv.org/find/cs/1/au:+Weckbecker_D/0/1/0/all/0/1">David Weckbecker</a></p><p>We consider the open online dial-a-ride problem, where transportation
requests appear online in a metric space and need to be served by a single
server. The objective is to minimize the completion time until all requests
have been served. We present a new, parameterized algorithm for this problem
and prove that it attains a competitive ratio of $1 + \varphi \approx 2.618$
for some choice of its parameter, where $\varphi$ is the golden ratio. This
improves the best known bounds for open online dial-a-ride both for general
metric spaces as well as for the real line. We also give a lower bound
of~$2.457$ for the competitive ratio of our algorithm for any parameter choice.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-26T00:30:00Z">Wednesday, October 26 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.13880'>Efficient and Stable Fully Dynamic Facility Location</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sayan Bhattacharya, Silvio Lattanzi, Nikos Parotsidis</p><p>We consider the classic facility location problem in fully dynamic data
streams, where elements can be both inserted and deleted. In this problem, one
is interested in maintaining a stable and high quality solution throughout the
data stream while using only little time per update (insertion or deletion). We
study the problem and provide the first algorithm that at the same time
maintains a constant approximation and incurs polylogarithmic amortized
recourse per update. We complement our theoretical results with an experimental
analysis showing the practical efficiency of our method.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1">Sayan Bhattacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Lattanzi_S/0/1/0/all/0/1">Silvio Lattanzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Parotsidis_N/0/1/0/all/0/1">Nikos Parotsidis</a></p><p>We consider the classic facility location problem in fully dynamic data
streams, where elements can be both inserted and deleted. In this problem, one
is interested in maintaining a stable and high quality solution throughout the
data stream while using only little time per update (insertion or deletion). We
study the problem and provide the first algorithm that at the same time
maintains a constant approximation and incurs polylogarithmic amortized
recourse per update. We complement our theoretical results with an experimental
analysis showing the practical efficiency of our method.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-26T00:30:00Z">Wednesday, October 26 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://benjamin-recht.github.io/2022/10/26/ai-image-search/'>Does AI Suck at Art?</a></h3>
        <p class='tr-article-feed'>from <a href='http://benjamin-recht.github.io/'>Ben Recht</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>I’ve been researching machine learning for a little over 20 years. For the past five years or so, with the latest wave of AI overpromising, I think I’ve been mostly known as an AI skeptic. But I’ve been engaging with these new AI image generation tools, and they are delightful. They have a lot of promise, and I want to explain why and suggest a few ways to make them even better.</p>

<p>Though I don’t talk about it much, for the past 20 years I’ve also been playing in an ambient shoegazer band called “the fun years.” My bandmate, Isaac Sparks, has been in charge of our visual design from the get-go. Over the years, he’s progressively refined his style, and he has been dipping his toes into the weird world of prompt-to-art.</p>

<p>Last week, we re-released an old single from 2006, and Isaac used midjourney for the cover art. Isaac is a turntablist, and approaches his art similarly to how he approaches searching for records. He seeks out happy accidents that can take on some new life when repurposed in new contexts. The cover of our first CDR, now that’s what i call droning, volume 4, was a photograph Isaac had taken out of my apartment window on a foggy evening. Over time, his covers grew more abstract. The cover of baby it’s cold inside is a collage of close up images of an old paint can Isaac found in the basement of his apartment complex. Most recently, the cover of our latest album typos in your obituary is a photograph of a stark, black, sculpture Isaac made out of wood scraps (He made a similar, Big Lebowski inspired sculpture for the cover art of my book with Steve Wright Optimization for Data Analysis).</p>

<p>For our new single, Isaac tried to come up with a midjourney prompt that captured the appropriate aesthetic of a fun years cover. It only took a couple of iterations to get what he wanted:</p>

<blockquote>
  <p>A mess of scrap paper, dull color plastic caps, dust, paint smear, chip, cruft, drinking straw, rusty nails, wooden lath, mold fractal, layers, black and white, realistic</p>
</blockquote>

<p>♦</p>

<p>And we went with the image in the bottom left.</p>

<p>♦</p>

<p>The image didn’t have any obvious visual artifacts and looked like something Isaac might have found out in the world. Having read recent reporting on some rather suspect copyright infringement, Isaac and I wondered if midjourney had obviously just ripped off the image.</p>

<p>For a lot of the early DALL-E memes that I saw on social media, I could often find strikingly similar pictures by pasting the prompt into google image search. But for Isaac’s mess prompt, we came up pretty empty, with some rather hilariously bad results.</p>

<p>♦</p>

<p>♦</p>

<p>Vaishaal Shankar then reminded me that there was a much better image search engine. The original DALL-E model which started the prompt-to-image craze was based on a neural network model called CLIP that learned a model to compare images and text. The model was trained on a huge data set of images paired with captions. It produced two functions: one that mapped images to a code book and one that mapped text to a code book. When you compared codes for two snippets of text, this would tell you how similar the snippets were. When you compared codes of two images, this would tell you how similar the images were. But one of the more amazing things about CLIP is that you could compare the codes of text and images and find images which were similar to the text.</p>

<p>Romain Beaumont built an image search system, clip-retrieval that used CLIP and a new data set LAION-5B consisting of 5 billion images scraped by common crawl. 5 billion! (Insert Dr. Evil meme). Romain hosts a free version of this system, where you type some text, it computes the codebook, and it returns all of the images in LAION-5B which have similar codes to your text. We tried Isaac’s prompt here, and now found some strikingly similar images.</p>

<p>♦</p>

<p>There were hundreds of art pieces and stock photos that captured elements of the spirit of the prompt. Again, with 5 billion images, it’s hard to imagine what’s not in there. But after scrolling through pages of similar images, we couldn’t find any of the four renderings produced by midjourney.</p>

<p>We tried the reverse image search in both clip-retrieval and google, and though we got back some similar textures, we couldn’t find the image itself.</p>

<p>♦</p>

<p>Now, just because we didn’t find our image doesn’t mean it’s not in the corpus somewhere. We looked at around 100 images, but what if we had looked at 1000? Or 10000? Perhaps it’s buried in the corner of some thumbnail in the LAION set. The biggest flaw of these tools is making artist attribution impossible. There should be some simple way of tracing back to the training set. Without traceback and attribution, this is just going to lead to annoying copyright lawsuits like we saw in the early days of sampling in music. We’d be better off if we could avoid those legal battles before they happen.</p>

<p>All that said, it really seems like the midjourney neural nets are doing something more than re-displaying images from the training set. There are certainly billions of amazing textures out in the world, and CLIP-style models make them easier to bring to the surface. But midjourney adds something extra to fuse these textures into something new. It’s much more than a “copyright infringing blur filter” and that’s pretty cool!</p>

<p>I still maintain that AI was and is overhyped. We were promised self-driving cars and cures for cancer, and we ended up with splashy tools for image generation. I’m not sure we can justify the billions of dollars of investment. But the image processing tools are still super fun and I want more to play with, so I’ll be selfish and hope OpenAI raises more money. I hope future variants allow for clearer navigation and enable more intentional sampling and pastiche of the source materials. And I can’t wait until they figure out how to make a plugin for Ableton Live that scours huge audio libraries to produce melted sonic textures. At that point, you won’t hear from me as an AI skeptic anymore as I’ll be blissfully hiding in my music studio.</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>I’ve been researching machine learning for a little over 20 years. For the past five years or so, with the latest wave of AI overpromising, I think I’ve been mostly known as an AI skeptic. But I’ve been engaging with these new AI image generation tools, and they are delightful. They have a lot of promise, and I want to explain why and suggest a few ways to make them even better.</p>

<p>Though I don’t talk about it much, for the past 20 years I’ve also been playing in an ambient shoegazer band called <a href="https://thefunyears.bandcamp.com/">“the fun years.”</a> My bandmate, <a href="http://www.isaacsparks.com/">Isaac Sparks</a>, has been in charge of our visual design from the get-go. Over the years, he’s progressively refined his style, and he has been dipping his toes into the weird world of prompt-to-art.</p>

<p>Last week, <a href="https://thefunyears.bandcamp.com/track/electricity-is-a-scarce-commodity">we re-released an old single from 2006</a>, and Isaac used <a href="https://www.midjourney.com/home/">midjourney</a> for the cover art. Isaac is a turntablist, and approaches his art similarly to how he approaches searching for records. He seeks out happy accidents that can take on some new life when repurposed in new contexts. The cover of our first CDR, <a href="https://thefunyears.bandcamp.com/album/now-thats-what-i-call-droning-volume-4"><em>now that’s what i call droning, volume 4</em></a>, was a photograph Isaac had taken out of my apartment window on a foggy evening. Over time, his covers grew more abstract. The cover of <a href="https://thefunyears.bandcamp.com/album/baby-its-cold-inside"><em>baby it’s cold inside</em></a> is a collage of close up images of an old paint can Isaac found in the basement of his apartment complex. Most recently, the cover of our latest album <a href="https://thefunyears.bandcamp.com/album/typos-in-your-obituary"><em>typos in your obituary</em></a> is a photograph of a stark, black, sculpture Isaac made out of wood scraps (He made a similar, Big Lebowski inspired sculpture for the cover art of my book with Steve Wright <a href="https://www.cambridge.org/core/books/optimization-for-data-analysis/C02C3708905D236AA354D1CE1739A6A2"><em>Optimization for Data Analysis</em></a>).</p>

<p>For our new single, Isaac tried to come up with a midjourney prompt that captured the appropriate aesthetic of a fun years cover. It only took a couple of iterations to get what he wanted:</p>

<blockquote>
  <p>A mess of scrap paper, dull color plastic caps, dust, paint smear, chip, cruft, drinking straw, rusty nails, wooden lath, mold fractal, layers, black and white, realistic</p>
</blockquote>

<p class="center"><img src="/assets/ai-art/mid_query_return.jpg" alt="midjourney returns some pretty cool cover art." width="100%" /></p>

<p>And we went with the image in the bottom left.</p>

<p class="center"><img src="/assets/ai-art/EIASC.jpg" alt="cover art of electricity is a scarce commodity." width="100%" /></p>

<p>The image didn’t have any obvious visual artifacts and looked like something Isaac might have found out in the world. Having read <a href="https://www.technologyreview.com/2022/09/16/1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/">recent reporting</a> on some rather suspect copyright infringement, Isaac and I wondered if midjourney had obviously just ripped off the image.</p>

<p>For a lot of the early <a href="https://openai.com/blog/dall-e/">DALL-E</a> memes that I saw on social media, I could often find strikingly similar pictures by pasting the prompt into google image search. But for Isaac’s mess prompt, we came up pretty empty, with some rather hilariously bad results.</p>

<p class="center"><img src="/assets/ai-art/google_image_stinks.png" alt="google image search results for Isaac's query." width="100%" /></p>

<p class="center"><img src="/assets/ai-art/google_image_stinks2.png" alt="more google image search results." width="100%" /></p>

<p><a href="http://vaishaal.com/">Vaishaal Shankar</a> then reminded me that there was a much better image search engine. The original <a href="https://openai.com/blog/dall-e/">DALL-E</a> model which started the prompt-to-image craze was based on a neural network model called <a href="https://openai.com/blog/clip/">CLIP</a> that learned a model to compare images and text. The model was trained on a huge data set of images paired with captions. It produced two functions: one that mapped images to a code book and one that mapped text to a code book. When you compared codes for two snippets of text, this would tell you how similar the snippets were. When you compared codes of two images, this would tell you how similar the images were. But one of the more amazing things about CLIP is that you could compare the codes of text and images and find images which were similar to the text.</p>

<p><a href="https://github.com/rom1504">Romain Beaumont</a> built an image search system, <a href="https://rom1504.github.io/clip-retrieval">clip-retrieval</a> that used CLIP and a new data set <a href="https://laion.ai/blog/laion-5b/">LAION-5B</a> consisting of 5 billion images scraped by <a href="https://commoncrawl.org/">common crawl</a>. 5 billion! (Insert Dr. Evil meme). Romain hosts a <a href="https://rom1504.github.io/clip-retrieval">free version of this system</a>, where you type some text, it computes the codebook, and it returns all of the images in LAION-5B which have similar codes to your text. We tried Isaac’s prompt here, and now found some strikingly similar images.</p>

<p class="center"><img src="/assets/ai-art/clip-retrieval-works.png" alt="clip-retrieval image search results for Isaac's query." width="100%" /></p>

<p>There were hundreds of art pieces and stock photos that captured elements of the spirit of the prompt. Again, with 5 billion images, it’s hard to imagine what’s not in there. But after scrolling through pages of similar images, we couldn’t find any of the four renderings produced by midjourney.</p>

<p>We tried the reverse image search in both clip-retrieval and google, and though we got back some similar textures, we couldn’t find the image itself.</p>

<p class="center"><img src="/assets/ai-art/clip-retrieval-image-search.png" alt="clip-retrieval reverse image search results for the cover art image." width="100%" /></p>

<p>Now, just because we didn’t find our image doesn’t mean it’s not in the corpus somewhere. We looked at around 100 images, but what if we had looked at 1000? Or 10000? Perhaps it’s buried in the corner of some thumbnail in the LAION set. The biggest flaw of these tools is making artist attribution impossible. There should be some simple way of tracing back to the training set. Without traceback and attribution, this is just going to lead to annoying copyright lawsuits like we saw in the early days of sampling in music. We’d be better off if we could avoid those legal battles before they happen.</p>

<p>All that said, it really seems like the midjourney neural nets are doing something more than re-displaying images from the training set. There are certainly billions of amazing textures out in the world, and CLIP-style models make them easier to bring to the surface. But midjourney adds something extra to fuse these textures into something new. It’s much more than a “copyright infringing blur filter” and that’s pretty cool!</p>

<p>I still maintain that AI was and is overhyped. We were promised self-driving cars and cures for cancer, and we ended up with splashy tools for image generation. I’m not sure we can justify the billions of dollars of investment. But the image processing tools are still super fun and I want more to play with, so I’ll be selfish and hope OpenAI raises more money. I hope future variants allow for clearer navigation and enable more intentional sampling and pastiche of the source materials. And I can’t wait until they figure out how to make a plugin for Ableton Live that scours huge audio libraries to produce melted sonic textures. At that point, you won’t hear from me as an AI skeptic anymore as I’ll be blissfully hiding in my music studio.</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-26T00:00:00Z">Wednesday, October 26 2022, 00:00</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Tuesday, October 25
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/10/25/tenure-track-faculty-at-university-of-rochester-apply-by-december-1-2022/'>Tenure-track Faculty at University of Rochester (apply by December 1, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The department of Computer Science at the University of Rochester is looking to hire three tenure-track faculty. One position is targeting cryptography/security but exceptional theory candidates in all other areas are welcome. Other positions are targeting computer systems and machine learning (broadly construed). Website: www.cs.rochester.edu/about/recruit.html#Faculty Email: kristi@cs.rochester.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The department of Computer Science at the University of Rochester is looking to hire three tenure-track faculty. One position is targeting cryptography/security but exceptional theory candidates in all other areas are welcome. Other positions are targeting computer systems and machine learning (broadly construed).</p>
<p>Website: <a href="https://www.cs.rochester.edu/about/recruit.html#Faculty">https://www.cs.rochester.edu/about/recruit.html#Faculty</a><br />
Email: kristi@cs.rochester.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-25T17:06:44Z">Tuesday, October 25 2022, 17:06</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://gilkalai.wordpress.com/2022/10/25/remarkable-limitations-of-linear-cross-entropy-as-a-measure-for-quantum-advantage-by-xun-gao-marcin-kalinowski-chi-ning-chou-mikhail-d-lukin-boaz-barak-and-soonwon-choi/'>Remarkable: “Limitations of Linear Cross-Entropy as a Measure for Quantum Advantage,” by Xun Gao, Marcin Kalinowski, Chi-Ning Chou, Mikhail D. Lukin, Boaz Barak, and Soonwon Choi</a></h3>
        <p class='tr-article-feed'>from <a href='https://gilkalai.wordpress.com'>Gil Kalai</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          In this post I would like to report about an important paper (posted Dec. 2021) by Xun Gao, Marcin Kalinowski, Chi-Ning Chou, Mikhail D. Lukin, Boaz Barak, and Soonwon Choi. (I am thankful to Xun Gao and&#160; Boaz Barak for &#8230; Continue reading &#8594;
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>In this post I would like to report about an important paper (posted Dec. 2021) by Xun Gao, Marcin Kalinowski, Chi-Ning Chou, Mikhail D. Lukin, Boaz Barak, and Soonwon Choi. (I am thankful to Xun Gao and&nbsp; Boaz Barak for helpful discussions).</p>
<h3><a href="https://arxiv.org/abs/2112.01657">Limitations of Linear Cross-Entropy as a Measure for Quantum Advantage</a></h3>
<p>Here is the abstract:</p>
<blockquote><p><em> <span id="2112.01657v1-abstract-full" class="abstract-full has-text-grey-dark mathjax"> Demonstrating quantum advantage requires experimental implementation of a computational task that is hard to achieve using state-of-the-art classical systems. One approach is to perform sampling from a probability distribution associated with a class of highly entangled many-body wavefunctions. It has been suggested that this approach can be certified with the Linear Cross-Entropy Benchmark (XEB). We critically examine this notion. First, in a &#8220;benign&#8221; setting where an honest implementation of noisy quantum circuits is assumed, we characterize the conditions under which the XEB approximates the fidelity. Second, in an &#8220;adversarial&#8221; setting where all possible classical algorithms are considered for comparison, we show that achieving relatively high XEB values does not imply faithful simulation of quantum dynamics. We present an efficient classical algorithm that, with 1 GPU within 2s, yields high XEB values, namely 2-12% of those obtained in experiments. By identifying and exploiting several vulnerabilities of the XEB, we achieve high XEB values without full simulation of quantum circuits. Remarkably, our algorithm features better scaling with the system size than noisy quantum devices for commonly studied random circuit ensembles. To quantitatively explain the success of our algorithm and the limitations of the XEB, we use a theoretical framework in which the average XEB and fidelity are mapped to statistical models. We illustrate the relation between the XEB and the fidelity for quantum circuits in various architectures, with different gate choices, and in the presence of noise. Our results show that XEB&#8217;s utility as a proxy for fidelity hinges on several conditions, which must be checked in the benign setting but cannot be assumed in the adversarial setting. Thus, the XEB alone has limited utility as a benchmark for quantum advantage. We discuss ways to overcome these limitations.</span></em></p></blockquote>
<h2>I. Three parameters for noisy quantum circuits:</h2>
<ol>
<li><strong>F</strong> &#8211; The fidelity. If <img src="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;phi" class="latex" /> is the ideal state and <img src="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;psi" class="latex" /> is the noisy state, then the fidelity <strong>F</strong> is defined by <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cpsi+%5Cleft+%7C+%5Cphi+%5Cright+%7C+%5Cpsi+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cpsi+%5Cleft+%7C+%5Cphi+%5Cright+%7C+%5Cpsi+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clangle+%5Cpsi+%5Cleft+%7C+%5Cphi+%5Cright+%7C+%5Cpsi+%5Crangle&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;langle &#92;psi &#92;left | &#92;phi &#92;right | &#92;psi &#92;rangle" class="latex" />,</li>
<li><strong>XEB</strong> &#8211; the linear cross entropy estimator for the fidelity</li>
<li><strong>P(No err)</strong> &#8211; The probability of no errors (denoted in the paper by <img src="https://s0.wp.com/latex.php?latex=p_%7Bno%7Eerr%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=p_%7Bno%7Eerr%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=p_%7Bno%7Eerr%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="p_{no~err}" class="latex" />).</li>
</ol>
<h2>II. Some issues:</h2>
<p>a) The fidelity <strong>F</strong> cannot be read from the distribution of the samples produced by the quantum computer. Suppose we are given an unlimited number of samples (or a large number of samples for which the empirical distribution is a good approximation to the noisy distribution), what is the best way to estimate the fidelity?</p>
<p>b) If we have a polynomial number of samples in (1/F). What are the best ways to estimate the fidelity?</p>
<p>c) What in general are the relations between <strong>F</strong>, <strong>XEB</strong>, and <strong>P(No err)?</strong></p>
<h2>III. A basic observation:</h2>
<p>A basic observation of the paper is that when you apply depolarizing noise to the gates, the resulting distribution has a positive correlation to the ideal distribution. (Hence this leads to positive XEB value.)&nbsp; The basic idea (as I see it) is simple: let&#8217;s consider 1-gate which is a certain unitary operator U.<br />
The space of such operators is spanned by I, X, Y, and Z. Let us assume that applying to U unitary noise, say, Y, will lead to a new quantum circuit which gives uncorrelated samples and fidelity estimator 0. However, applying (I+X+Y+Z), which replace the qubit with a maximal entropy state is a very basic form of noise (depolarizing noise on the qubit on which the gate acts) and this form of noise is expected to slash the fidelity estimator by four and not send it to zero. (For 2-gates the story is similar but this time there are 16 basic possibilities for unitary noise so we can expect that a depolarizing noise will slash the linear cross entropy estimator by 1/16 (and not to zero).)</p>
<h2>IV. Additional observations</h2>
<p>The paper by Gao et al. describes various additional reasons for which the effect of gate errors will lead to positive correlations with the ideal distribution, and in general will lead to strict inequalities</p>
<p style="text-align:center;"><strong>(1) &nbsp; XEB &nbsp; &gt;&nbsp; P(No err) </strong></p>
<p>and</p>
<p style="text-align:center;"><strong>(2) &nbsp;</strong> <strong>XEB &gt; F &gt; P(No err) </strong></p>
<p>First, it turns out that even a single unitary gate error can contribute to the increase of <strong>XEB&nbsp; </strong>(and also to increase<strong> F)</strong>, and, moreover, the effect of two (or more) gate errors can also lead to an increased <strong>XEB (</strong>and also an increased <strong>F</strong>.)</p>
<p>In expectation we can actually expect.</p>
<p style="text-align:center;"><strong>(3) &nbsp;&nbsp; XEB &gt; F &gt; P(No err) </strong></p>
<p>This is demonstrated by Figure 1 of the paper.</p>
<h2><img loading="lazy" data-attachment-id="23473" data-permalink="https://gilkalai.wordpress.com/2022/10/25/remarkable-limitations-of-linear-cross-entropy-as-a-measure-for-quantum-advantage-by-xun-gao-marcin-kalinowski-chi-ning-chou-mikhail-d-lukin-boaz-barak-and-soonwon-choi/xungao/" data-orig-file="https://gilkalai.files.wordpress.com/2022/10/xungao.png" data-orig-size="337,578" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="XunGao" data-image-description="" data-image-caption="" data-medium-file="https://gilkalai.files.wordpress.com/2022/10/xungao.png?w=175" data-large-file="https://gilkalai.files.wordpress.com/2022/10/xungao.png?w=337" class="alignnone  wp-image-23473" src="https://gilkalai.files.wordpress.com/2022/10/xungao.png?w=310&#038;h=532" alt="XunGao" width="310" height="532" srcset="https://gilkalai.files.wordpress.com/2022/10/xungao.png?w=310&amp;h=532 310w, https://gilkalai.files.wordpress.com/2022/10/xungao.png?w=87&amp;h=150 87w, https://gilkalai.files.wordpress.com/2022/10/xungao.png?w=175&amp;h=300 175w, https://gilkalai.files.wordpress.com/2022/10/xungao.png 337w" sizes="(max-width: 310px) 100vw, 310px"></h2>
<h2>V. The idea behind the spoofing algorithm</h2>
<p>The way Gao, Kalinowski, Chou, Lukin, Barak, and Choi used this observation is by applying such depolarization noise on a set of 2-gates that would split the circuit into two parts. This will lead to a sort of &#8220;patched&#8221; circuit for which one can make the computation separately on every patch, which gives much quicker classical algorithms.</p>
<h2>VI The asymptotic behavior</h2>
<p>One interesting aspect of the paper is that (as far as I could understand) when the number of qubits grows the &#8220;quantum advantage&#8221; , namely the advantage of the quantum algorithms over the classical algorithms, diminishes. As Gao et al. write</p>
<blockquote><p><span style="color:#0000ff;"><em>&#8220;Remarkably, the <strong>XEB </strong></em><em>value of our algorithm generally improves for larger </em><em>quantum circuits, whereas that of noisy quantum </em><em>devices quickly deteriorates. Such scaling continues to hold when the number of qubits is increased </em><em>while the depth of the circuit and the error-per-gate </em><em>are fixed&#8230;&#8221;</em></span></p></blockquote>
<p>Remark: This conclusion assumes that you need enough samples to verify the fidelity. Philosophically one can claim that the quantum advantage may apply for producing *one* sample; After all, your argument is based anyway on extrapolation, and for supremacy experiments you cannot verify even the individual amplitudes. (I made this claim in a discussion with Daniel Lidar regarding the very nice paper by <span dir="ltr" role="presentation">Zlokapa, Boixo, and Lidar, <a href="https://arxiv.org/abs/2005.02464">Boundaries of quantum supremacy </a></span><span dir="ltr" role="presentation">via random circuit sampling,</span> but I couldn&#8217;t persuade Daniel.)</p>
<h2>VII Relevance to the statistical analysis and the concerns regarding the Google 2019 experiment</h2>
<p>The paper is relevant to two important aspects of my <a href="https://gilkalai.files.wordpress.com/2022/08/sts836.pdf">statistical science paper</a> with Yosi Rinott and Tomer Shoham and to our work which puts the Google 2019 experiment under scrutiny.</p>
<ol>
<li>The fact that <strong>XEB</strong> is systematically larger than <strong>P(No err)</strong> may support the concern that the precise agreement of the <strong>XEB</strong> estimator with the <strong>P(No err)</strong> computation (Formula (77)) is &#8220;too good to be true,&#8221; namely, it fits too well with the researcher&#8217;s expectations rather than with physical reality.</li>
<li>The basic observation (III) implies that the exponential decay for the Fourier coefficients with the degrees, that we attributed only to readout errors is also caused by gate errors. Subsequently, the Fourier description of the data that we regarded as providing confirmation to the Google claim (see, Figure 2 in <a href="https://arxiv.org/abs/2210.12753">our recent paper</a>) actually appears to show that the empirical data does not fit the theory.</li>
</ol>
<p>Apropos Fourier methods and Xun Gao: The first I heard of Xun Gao&#8217;s work was in connection with his excellent early work with Duan <a href="https://arxiv.org/abs/1810.03176">Efficient classical simulation of noisy quantum computation</a> that used Fourier methods to study quantum circuits.</p>
<p class="authors">By Gil Kalai</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-25T04:59:48Z">Tuesday, October 25 2022, 04:59</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://theorydish.blog/2022/10/24/9th-toca-sv-11-18/'>9th TOCA-SV – 11/18</a></h3>
        <p class='tr-article-feed'>from <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The 9th TOCA-SV day is Coming on Friday 11/18/22, in the Google campus in Mountain View. It is free but you need to register here, where you can also see an up-to-date list of talks and abstracts. Schedule (tentative): 0930-1000: Breakfast 1000-1015: Welcome 1015-1100: Gagan Aggarwal (Google) 1100-1145: Li-Yang Tan (Stanford) 1145-1245: Short talks I 1245-1400: Lunch (provided) and campus tour 1400-1445: Sandy Irani (Simons/UC Berkeley) 1445-1530: Kunal Talwar (Apple) 1530-1600: Coffee Break 1600-1730: Short talks II
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The 9th TOCA-SV day is Coming on Friday 11/18/22, in the Google campus in Mountain View. It is free but you need to register <a href="https://sites.google.com/view/9th-toca-sv-nov-18-2022/">here</a>, where you can also see an up-to-date list of talks and abstracts.</p>
<div class="CjVfdc">Schedule (tentative):</div>
<p class="CDt4Ke zfr3Q" dir="ltr"><strong>0930-1000: Breakfast</strong></p>
<p class="CDt4Ke zfr3Q" dir="ltr"><strong>1000-1015: Welcome</strong></p>
<p class="CDt4Ke zfr3Q" dir="ltr"><strong>1015-1100: Gagan Aggarwal (Google)</strong></p>
<p class="CDt4Ke zfr3Q" dir="ltr"><strong>1100-1145: Li-Yang Tan (Stanford)</strong></p>
<p class="CDt4Ke zfr3Q" dir="ltr"><strong>1145-1245: Short talks I</strong></p>
<p class="CDt4Ke zfr3Q" dir="ltr"><strong>1245-1400: Lunch</strong><strong> (provided) </strong><strong>and campus tour</strong></p>
<p class="CDt4Ke zfr3Q" dir="ltr"><strong>1400-1445: Sandy Irani (Simons/UC Berkeley)</strong></p>
<p class="CDt4Ke zfr3Q" dir="ltr"><strong>1445-1530: Kunal Talwar (Apple)</strong></p>
<p class="CDt4Ke zfr3Q" dir="ltr"><strong>1530-1600: Coffee Break</strong></p>
<p class="CDt4Ke zfr3Q" dir="ltr"><strong>1600-1730: Short talks II</strong></p>
<p class="authors">By Omer Reingold</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-25T04:34:04Z">Tuesday, October 25 2022, 04:34</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.12289'>Complexity and Ramsey Largeness of Sets of Oracles Separating Complexity Classes</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Alex Creiner, Stephen Jackson</p><p>We prove two sets of results concerning computational complexity classes. The
first concerns a variation of the random oracle hypothesis posed by Bennett and
Gill after they showed that relative to a randomly chosen oracle, P not equal
NP with probability 1. This hypothesis was quickly disproven in several ways,
most famously in 1992 with the result that IP equals PSPACE, in spite of the
classes being shown unequal with probability 1. Here we propose a variation of
what it means to be ``large'' using the Ellentuck topology. In this new
context, we demonstrate that the set of oracles separating NP and co-NP is not
small, and obtain similar results for the separation of PSPACE from PH along
with the separation of NP from BQP. We demonstrate that this version of the
hypothesis turns it into a sufficient condition for unrelativized
relationships, at least in the three cases considered here. Second, we example
the descriptive complexity of the classes of oracles providing the separations
for these various classes, and determine their exact placement in the Borel
hierarchy.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Creiner_A/0/1/0/all/0/1">Alex Creiner</a>, <a href="http://arxiv.org/find/math/1/au:+Jackson_S/0/1/0/all/0/1">Stephen Jackson</a></p><p>We prove two sets of results concerning computational complexity classes. The
first concerns a variation of the random oracle hypothesis posed by Bennett and
Gill after they showed that relative to a randomly chosen oracle, P not equal
NP with probability 1. This hypothesis was quickly disproven in several ways,
most famously in 1992 with the result that IP equals PSPACE, in spite of the
classes being shown unequal with probability 1. Here we propose a variation of
what it means to be ``large'' using the Ellentuck topology. In this new
context, we demonstrate that the set of oracles separating NP and co-NP is not
small, and obtain similar results for the separation of PSPACE from PH along
with the separation of NP from BQP. We demonstrate that this version of the
hypothesis turns it into a sufficient condition for unrelativized
relationships, at least in the three cases considered here. Second, we example
the descriptive complexity of the classes of oracles providing the separations
for these various classes, and determine their exact placement in the Borel
hierarchy.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-25T00:30:00Z">Tuesday, October 25 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.12438'>Algorithms with Prediction Portfolios</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, Sergei Vassilvitskii</p><p>The research area of algorithms with predictions has seen recent success
showing how to incorporate machine learning into algorithm design to improve
performance when the predictions are correct, while retaining worst-case
guarantees when they are not. Most previous work has assumed that the algorithm
has access to a single predictor. However, in practice, there are many machine
learning methods available, often with incomparable generalization guarantees,
making it hard to pick a best method a priori. In this work we consider
scenarios where multiple predictors are available to the algorithm and the
question is how to best utilize them.
</p>
<p>Ideally, we would like the algorithm's performance to depend on the quality
of the best predictor. However, utilizing more predictions comes with a cost,
since we now have to identify which prediction is the best. We study the use of
multiple predictors for a number of fundamental problems, including matching,
load balancing, and non-clairvoyant scheduling, which have been well-studied in
the single predictor setting. For each of these problems we introduce new
algorithms that take advantage of multiple predictors, and prove bounds on the
resulting performance.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dinitz_M/0/1/0/all/0/1">Michael Dinitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1">Sungjin Im</a>, <a href="http://arxiv.org/find/cs/1/au:+Lavastida_T/0/1/0/all/0/1">Thomas Lavastida</a>, <a href="http://arxiv.org/find/cs/1/au:+Moseley_B/0/1/0/all/0/1">Benjamin Moseley</a>, <a href="http://arxiv.org/find/cs/1/au:+Vassilvitskii_S/0/1/0/all/0/1">Sergei Vassilvitskii</a></p><p>The research area of algorithms with predictions has seen recent success
showing how to incorporate machine learning into algorithm design to improve
performance when the predictions are correct, while retaining worst-case
guarantees when they are not. Most previous work has assumed that the algorithm
has access to a single predictor. However, in practice, there are many machine
learning methods available, often with incomparable generalization guarantees,
making it hard to pick a best method a priori. In this work we consider
scenarios where multiple predictors are available to the algorithm and the
question is how to best utilize them.
</p>
<p>Ideally, we would like the algorithm's performance to depend on the quality
of the best predictor. However, utilizing more predictions comes with a cost,
since we now have to identify which prediction is the best. We study the use of
multiple predictors for a number of fundamental problems, including matching,
load balancing, and non-clairvoyant scheduling, which have been well-studied in
the single predictor setting. For each of these problems we introduce new
algorithms that take advantage of multiple predictors, and prove bounds on the
resulting performance.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-25T00:30:00Z">Tuesday, October 25 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.12468'>Discrepancy Minimization in Input-Sparsity Time</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yichuan Deng, Zhao Song, Omri Weinstein</p><p>A recent work of Larsen [Lar23] gave a faster combinatorial alternative to
Bansal's SDP algorithm for finding a coloring $x\in\{-1,1\}^n$ that
approximately minimizes the discrepancy $\mathrm{disc}(A,x) : = \| A x
\|_{\infty}$ of a general real-valued $m\times n$ matrix $A$. Larsen's
algorithm runs in $\widetilde{O}(mn^2)$ time compared to Bansal's
$\widetilde{O}(mn^{4.5})$-time algorithm, at the price of a slightly weaker
logarithmic approximation ratio in terms of the hereditary discrepancy of $A$
[Ban10].
</p>
<p>In this work we present a combinatorial $\widetilde{O}(\mathrm{nnz}(A) +
n^3)$ time algorithm with the same approximation guarantee as Larsen, which is
optimal for tall matrices $m=\mathrm{poly}(n)$. Using a more intricate analysis
and fast matrix-multiplication, we achieve $\widetilde{O}(\mathrm{nnz}(A) +
n^{2.53})$ time, which breaks cubic runtime for square matrices, and bypasses
the barrier of linear-programming approaches [ES14] for which input-sparsity
time is currently out of reach.
</p>
<p>Our algorithm relies on two main ideas: (i) A new sketching technique for
finding a projection matrix with short $\ell_2$-basis using implicit
leverage-score sampling; (ii) A data structure for faster implementation of the
iterative Edge-Walk partial-coloring algorithm of Lovett-Meka, using an
alternative analysis that enables ``lazy" batch-updates with low-rank
corrections. Our result nearly closes the computational gap between real-valued
and binary matrices (set-systems), for which input-sparsity time coloring was
very recently obtained [JSS23].
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yichuan Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zhao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinstein_O/0/1/0/all/0/1">Omri Weinstein</a></p><p>A recent work of Larsen [Lar23] gave a faster combinatorial alternative to
Bansal's SDP algorithm for finding a coloring $x\in\{-1,1\}^n$ that
approximately minimizes the discrepancy $\mathrm{disc}(A,x) : = \| A x
\|_{\infty}$ of a general real-valued $m\times n$ matrix $A$. Larsen's
algorithm runs in $\widetilde{O}(mn^2)$ time compared to Bansal's
$\widetilde{O}(mn^{4.5})$-time algorithm, at the price of a slightly weaker
logarithmic approximation ratio in terms of the hereditary discrepancy of $A$
[Ban10].
</p>
<p>In this work we present a combinatorial $\widetilde{O}(\mathrm{nnz}(A) +
n^3)$ time algorithm with the same approximation guarantee as Larsen, which is
optimal for tall matrices $m=\mathrm{poly}(n)$. Using a more intricate analysis
and fast matrix-multiplication, we achieve $\widetilde{O}(\mathrm{nnz}(A) +
n^{2.53})$ time, which breaks cubic runtime for square matrices, and bypasses
the barrier of linear-programming approaches [ES14] for which input-sparsity
time is currently out of reach.
</p>
<p>Our algorithm relies on two main ideas: (i) A new sketching technique for
finding a projection matrix with short $\ell_2$-basis using implicit
leverage-score sampling; (ii) A data structure for faster implementation of the
iterative Edge-Walk partial-coloring algorithm of Lovett-Meka, using an
alternative analysis that enables ``lazy" batch-updates with low-rank
corrections. Our result nearly closes the computational gap between real-valued
and binary matrices (set-systems), for which input-sparsity time coloring was
very recently obtained [JSS23].
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-25T00:30:00Z">Tuesday, October 25 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.12495'>Quartic Samples Suffice for Fourier Interpolation</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Zhao Song, Baocheng Sun, Omri Weinstein, Ruizhe Zhang</p><p>We study the classic problem of interpolating a Fourier-sparse signal in the
time duration $[0, T]$ from noisy samples in the same range, where the ground
truth signal can be any $k$-Fourier-sparse signal with band-limit $[-F, F]$.
Our main result is an efficient Fourier Interpolation algorithm that improves
the previous best algorithm by [Chen, Kane, Price and Song, FOCS 2016] in the
following three aspects:
</p>
<p>$\bullet$ The sample complexity is improved from $\widetilde{O}(k^{51})$ to
$\widetilde{O}(k^{4})$.
</p>
<p>$\bullet$ The time complexity is improved from $
\widetilde{O}(k^{10\omega+40})$ to $\widetilde{O}(k^{4 \omega})$.
</p>
<p>$\bullet$ The output sparsity is improved from $\widetilde{O}(k^{10})$ to
$\widetilde{O}(k^{4})$.
</p>
<p>Here, $\omega$ denotes the exponent of fast matrix multiplication. The
state-of-the-art sample complexity of this problem is $\widetilde{O}(k)$, but
can only be achieved by an *exponential-time* algorithm. Our algorithm uses
slightly more samples ($\sim k^4$) in exchange for small polynomial runtime,
laying the groundwork for a practical Fourier Interpolation algorithm.
</p>
<p>The centerpiece of our algorithm is a new sufficient condition for the
frequency estimation task -- a high signal-to-noise (SNR) band condition --
which allows for efficient and accurate signal reconstruction. Based on this
condition together with a new structural decomposition of Fourier signals
(Signal Equivalent Method), we design a cheap algorithm for estimating each
"significant" frequency within a narrow range, which is then combined with a
new high-accuracy signal estimation algorithm to reconstruct the ground-truth
signal.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zhao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1">Baocheng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinstein_O/0/1/0/all/0/1">Omri Weinstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruizhe Zhang</a></p><p>We study the classic problem of interpolating a Fourier-sparse signal in the
time duration $[0, T]$ from noisy samples in the same range, where the ground
truth signal can be any $k$-Fourier-sparse signal with band-limit $[-F, F]$.
Our main result is an efficient Fourier Interpolation algorithm that improves
the previous best algorithm by [Chen, Kane, Price and Song, FOCS 2016] in the
following three aspects:
</p>
<p>$\bullet$ The sample complexity is improved from $\widetilde{O}(k^{51})$ to
$\widetilde{O}(k^{4})$.
</p>
<p>$\bullet$ The time complexity is improved from $
\widetilde{O}(k^{10\omega+40})$ to $\widetilde{O}(k^{4 \omega})$.
</p>
<p>$\bullet$ The output sparsity is improved from $\widetilde{O}(k^{10})$ to
$\widetilde{O}(k^{4})$.
</p>
<p>Here, $\omega$ denotes the exponent of fast matrix multiplication. The
state-of-the-art sample complexity of this problem is $\widetilde{O}(k)$, but
can only be achieved by an *exponential-time* algorithm. Our algorithm uses
slightly more samples ($\sim k^4$) in exchange for small polynomial runtime,
laying the groundwork for a practical Fourier Interpolation algorithm.
</p>
<p>The centerpiece of our algorithm is a new sufficient condition for the
frequency estimation task -- a high signal-to-noise (SNR) band condition --
which allows for efficient and accurate signal reconstruction. Based on this
condition together with a new structural decomposition of Fourier signals
(Signal Equivalent Method), we design a cheap algorithm for estimating each
"significant" frequency within a narrow range, which is then combined with a
new high-accuracy signal estimation algorithm to reconstruct the ground-truth
signal.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-25T00:30:00Z">Tuesday, October 25 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.12543'>Edge-weighted Online Stochastic Matching: Beating $1-\frac1e$</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Shuyi Yan</p><p>We study the edge-weighted online stochastic matching problem. Since Feldman,
Mehta, Mirrokni, and Muthukrishnan proposed the $(1-\frac1e)$-competitive
Suggested Matching algorithm, there has been no improvement for the general
edge-weighted online stochastic matching problem. In this paper, we introduce
the first algorithm beating the $1-\frac1e$ bound in this setting, achieving a
competitive ratio of $0.645$. Under the LP proposed by Jaillet and Lu, we
design an algorithmic preprocessing, dividing all edges into two classes. Then
based on the Suggested Matching algorithm, we adjust the matching strategy to
improve the performance on one class in the early stage and on another class in
the late stage, while keeping the matching events of different edges highly
independent. By balancing them, we guarantee the matching probability of every
single edge.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Shuyi Yan</a></p><p>We study the edge-weighted online stochastic matching problem. Since Feldman,
Mehta, Mirrokni, and Muthukrishnan proposed the $(1-\frac1e)$-competitive
Suggested Matching algorithm, there has been no improvement for the general
edge-weighted online stochastic matching problem. In this paper, we introduce
the first algorithm beating the $1-\frac1e$ bound in this setting, achieving a
competitive ratio of $0.645$. Under the LP proposed by Jaillet and Lu, we
design an algorithmic preprocessing, dividing all edges into two classes. Then
based on the Suggested Matching algorithm, we adjust the matching strategy to
improve the performance on one class in the early stage and on another class in
the late stage, while keeping the matching events of different edges highly
independent. By balancing them, we guarantee the matching probability of every
single edge.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-25T00:30:00Z">Tuesday, October 25 2022, 00:30</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
