<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.4 (2022-04-12) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2022-11-02T20:36:29Z">Wednesday, November 02 2022, 20:36</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>Sébastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Wednesday, November 02
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://tcsplus.wordpress.com/2022/11/02/tcs-talk-wednesday-november-9-yaonan-jin-columbia-university/'>TCS+ talk: Wednesday, November 9 — Yaonan Jin, Columbia University</a></h3>
        <p class='tr-article-feed'>from <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The next TCS+ talk will take place this coming Wednesday, November 9th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). Yaonan Jin from Columbia University will speak about &#8220;First Price Auction is 1-1/e² Efficient&#8221; (abstract below). You can reserve a spot as an individual or a group to [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The next TCS+ talk will take place this coming Wednesday, November 9th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Yaonan Jin</strong> from Columbia University will speak about &#8220;<em>First Price Auction is 1-1/e² Efficient</em>&#8221; (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: We prove that, for the first-price auction, the tight Price of Anarchy (PoA) and the tight Price of Stability (PoS) are both <img src="https://s0.wp.com/latex.php?latex=1-1%2Fe%5E2+%5Capprox+0.8647&#038;bg=fff&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1-1%2Fe%5E2+%5Capprox+0.8647&#038;bg=fff&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1-1%2Fe%5E2+%5Capprox+0.8647&#038;bg=fff&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1-1/e^2 &#92;approx 0.8647" class="latex" />, closing the gap between the best known bounds [0.7430, 0.8689].</p>
<p>Based on joint works with Pinyan Lu.<br />
<a href="https://arxiv.org/abs/2207.01761">https://arxiv.org/abs/2207.01761</a><br />
<a href="https://arxiv.org/abs/2207.04455">https://arxiv.org/abs/2207.04455</a></p></blockquote>
<p class="authors">By plustcs</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-02T15:47:26Z">Wednesday, November 02 2022, 15:47</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.00289'>Composable Coresets for Constrained Determinant Maximization and Beyond</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sepideh Mahabadi, Thuy-Duong Vuong</p><p>We study the task of determinant maximization under partition constraint, in
the context of large data sets. Given a point set $V\subset \mathbb{R}^d$ that
is partitioned into $s$ groups $V_1,..., V_s$, and integers $k_1,...,k_s$ where
$k=\sum_i k_i$, the goal is to pick $k_i$ points from group $i$ such that the
overall determinant of the picked $k$ points is maximized. Determinant
Maximization and its constrained variants have gained a lot of interest for
modeling diversityand have found applications in the context of fairness and
data summarization.
</p>
<p>We study the design of composable coresets for the constrained determinant
maximization problem. Composable coresets are small subsets of the data that
(approximately) preserve optimal solutions to optimization tasks and enable
efficient solutions in several other large data models including the
distributed and the streaming settings. In this work, we consider two regimes.
For the case of $k&gt;d$, we show a peeling algorithm that gives us a composable
coreset of size $kd$ with an approximation factor of $d^{O(d)}$. We complement
our results by showing that this approximation factor is tight. For the case of
$k\leq d$, we show that a simple modification of the previous algorithms
results in an optimal coreset verified by our lower bounds. Our results apply
to all strongly Rayleigh distribution and several other experimental design
problems. In addition, we show coreset construction algorithms under the more
general laminar matroid constraints.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Mahabadi_S/0/1/0/all/0/1">Sepideh Mahabadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1">Thuy-Duong Vuong</a></p><p>We study the task of determinant maximization under partition constraint, in
the context of large data sets. Given a point set $V\subset \mathbb{R}^d$ that
is partitioned into $s$ groups $V_1,..., V_s$, and integers $k_1,...,k_s$ where
$k=\sum_i k_i$, the goal is to pick $k_i$ points from group $i$ such that the
overall determinant of the picked $k$ points is maximized. Determinant
Maximization and its constrained variants have gained a lot of interest for
modeling diversityand have found applications in the context of fairness and
data summarization.
</p>
<p>We study the design of composable coresets for the constrained determinant
maximization problem. Composable coresets are small subsets of the data that
(approximately) preserve optimal solutions to optimization tasks and enable
efficient solutions in several other large data models including the
distributed and the streaming settings. In this work, we consider two regimes.
For the case of $k&gt;d$, we show a peeling algorithm that gives us a composable
coreset of size $kd$ with an approximation factor of $d^{O(d)}$. We complement
our results by showing that this approximation factor is tight. For the case of
$k\leq d$, we show that a simple modification of the previous algorithms
results in an optimal coreset verified by our lower bounds. Our results apply
to all strongly Rayleigh distribution and several other experimental design
problems. In addition, we show coreset construction algorithms under the more
general laminar matroid constraints.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-02T00:30:00Z">Wednesday, November 02 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.00120'>A GPU-friendly, Parallel, and (Almost-)In-Place Algorithm for Building Left-Balanced kd-Trees</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ingo Wald</p><p>We present an algorithm that allows for building left-balanced and complete
k-d trees over k-dimensional points in a trivially parallel and GPU friendly
way. Our algorithm requires exactly one int per data point as temporary
storage, and uses O(logN ) iterations, each of which performs one parallel
sort, and one trivially parallel CUDA per-node update kernel.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Wald_I/0/1/0/all/0/1">Ingo Wald</a></p><p>We present an algorithm that allows for building left-balanced and complete
k-d trees over k-dimensional points in a trivially parallel and GPU friendly
way. Our algorithm requires exactly one int per data point as temporary
storage, and uses O(logN ) iterations, each of which performs one parallel
sort, and one trivially parallel CUDA per-node update kernel.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-02T00:30:00Z">Wednesday, November 02 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.00332'>Computational Power of A Single Oblivious Mobile Agent in Two-Edge-Connected Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Taichi Inoue, Naoki Kitamura, Taisuke Izumi, Toshimitsu Masuzawa</p><p>We investigate the computational power of a single mobile agent in an
$n$-node graph with storage (i.e., node memory). It has been shown that the
system with one-bit agent memory and $O(1)$-bit storage is as powerful as the
one with $O(n)$-bit agent memory and $O(1)$-bit storage, and thus we focus on
the difference between one-bit memory agents and oblivious (i.e. zero-bit
memory) agents. While it has been also shown that their computational powers
are not equivalent, all the known results exhibiting such a difference rely on
the fact that oblivious agents cannot transfer any information from one side to
the other side across the bridge edge. Then our main question is stated as
follows: Are the computational powers of one-bit memory agents and oblivious
agents equivalent in 2-edge-connected graphs or not? The main contribution of
this paper is to answer this question positively under the relaxed assumption
that each node has $O(\log\Delta)$-bit storage ($\Delta$ is the maximum degree
of the graph). We present an algorithm of simulating any algorithm for a single
one-bit memory agent by one oblivious agent with $O(n^2)$-time overhead per
round. Our result implies that the topological structure of graphs
differentiating the computational powers of oblivious and non-oblivious agents
is completely characterized by the existence of bridge edges.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Inoue_T/0/1/0/all/0/1">Taichi Inoue</a>, <a href="http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1">Naoki Kitamura</a>, <a href="http://arxiv.org/find/cs/1/au:+Izumi_T/0/1/0/all/0/1">Taisuke Izumi</a>, <a href="http://arxiv.org/find/cs/1/au:+Masuzawa_T/0/1/0/all/0/1">Toshimitsu Masuzawa</a></p><p>We investigate the computational power of a single mobile agent in an
$n$-node graph with storage (i.e., node memory). It has been shown that the
system with one-bit agent memory and $O(1)$-bit storage is as powerful as the
one with $O(n)$-bit agent memory and $O(1)$-bit storage, and thus we focus on
the difference between one-bit memory agents and oblivious (i.e. zero-bit
memory) agents. While it has been also shown that their computational powers
are not equivalent, all the known results exhibiting such a difference rely on
the fact that oblivious agents cannot transfer any information from one side to
the other side across the bridge edge. Then our main question is stated as
follows: Are the computational powers of one-bit memory agents and oblivious
agents equivalent in 2-edge-connected graphs or not? The main contribution of
this paper is to answer this question positively under the relaxed assumption
that each node has $O(\log\Delta)$-bit storage ($\Delta$ is the maximum degree
of the graph). We present an algorithm of simulating any algorithm for a single
one-bit memory agent by one oblivious agent with $O(n^2)$-time overhead per
round. Our result implies that the topological structure of graphs
differentiating the computational powers of oblivious and non-oblivious agents
is completely characterized by the existence of bridge edges.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-02T00:30:00Z">Wednesday, November 02 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.00378'>A Near-Linear Kernel for Two-Parsimony Distance</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Elise Deen, Leo van Iersel, Remie Janssen, Mark Jones, Yuki Murakami, Norbert Zeh</p><p>The maximum parsimony distance $d_{\textrm{MP}}(T_1,T_2)$ and the
bounded-state maximum parsimony distance $d_{\textrm{MP}}^t(T_1,T_2)$ measure
the difference between two phylogenetic trees $T_1,T_2$ in terms of the maximum
difference between their parsimony scores for any character (with $t$ a bound
on the number of states in the character, in the case of
$d_{\textrm{MP}}^t(T_1,T_2)$). While computing $d_{\textrm{MP}}(T_1, T_2)$ was
previously shown to be fixed-parameter tractable with a linear kernel, no such
result was known for $d_{\textrm{MP}}^t(T_1,T_2)$. In this paper, we prove that
computing $d_{\textrm{MP}}^t(T_1, T_2)$ is fixed-parameter tractable for
all~$t$. Specifically, we prove that this problem has a kernel of size $O(k \lg
k)$, where $k = d_{\textrm{MP}}^t(T_1, T_2)$. As the primary analysis tool, we
introduce the concept of leg-disjoint incompatible quartets, which may be of
independent interest.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Deen_E/0/1/0/all/0/1">Elise Deen</a>, <a href="http://arxiv.org/find/cs/1/au:+Iersel_L/0/1/0/all/0/1">Leo van Iersel</a>, <a href="http://arxiv.org/find/cs/1/au:+Janssen_R/0/1/0/all/0/1">Remie Janssen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1">Mark Jones</a>, <a href="http://arxiv.org/find/cs/1/au:+Murakami_Y/0/1/0/all/0/1">Yuki Murakami</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeh_N/0/1/0/all/0/1">Norbert Zeh</a></p><p>The maximum parsimony distance $d_{\textrm{MP}}(T_1,T_2)$ and the
bounded-state maximum parsimony distance $d_{\textrm{MP}}^t(T_1,T_2)$ measure
the difference between two phylogenetic trees $T_1,T_2$ in terms of the maximum
difference between their parsimony scores for any character (with $t$ a bound
on the number of states in the character, in the case of
$d_{\textrm{MP}}^t(T_1,T_2)$). While computing $d_{\textrm{MP}}(T_1, T_2)$ was
previously shown to be fixed-parameter tractable with a linear kernel, no such
result was known for $d_{\textrm{MP}}^t(T_1,T_2)$. In this paper, we prove that
computing $d_{\textrm{MP}}^t(T_1, T_2)$ is fixed-parameter tractable for
all~$t$. Specifically, we prove that this problem has a kernel of size $O(k \lg
k)$, where $k = d_{\textrm{MP}}^t(T_1, T_2)$. As the primary analysis tool, we
introduce the concept of leg-disjoint incompatible quartets, which may be of
independent interest.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-02T00:30:00Z">Wednesday, November 02 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.00464'>On the zeroes of hypergraph independence polynomials</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: David Galvin, Gwen McKinley, Will Perkins, Michail Sarantis, Prasad Tetali</p><p>We study the locations of complex zeroes of independence polynomials of
bounded degree hypergraphs. For graphs, this is a long-studied subject with
applications to statistical physics, algorithms, and combinatorics. Results on
zero-free regions for bounded-degree graphs include Shearer's result on the
optimal zero-free disk, along with several recent results on other zero-free
regions. Much less is known for hypergraphs. We make some steps towards an
understanding of zero-free regions for bounded-degree hypergaphs by proving
that all hypergraphs of maximum degree $\Delta$ have a zero-free disk almost as
large as the optimal disk for graphs of maximum degree $\Delta$ established by
Shearer (of radius $\sim 1/(e \Delta)$). Up to logarithmic factors in $\Delta$
this is optimal, even for hypergraphs with all edge-sizes strictly greater than
$2$. We conjecture that for $k\ge 3$, $k$-uniform linear hypergraphs have a
much larger zero-free disk of radius $\Omega(\Delta^{- \frac{1}{k-1}} )$. We
establish this in the case of linear hypertrees.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Galvin_D/0/1/0/all/0/1">David Galvin</a>, <a href="http://arxiv.org/find/math/1/au:+McKinley_G/0/1/0/all/0/1">Gwen McKinley</a>, <a href="http://arxiv.org/find/math/1/au:+Perkins_W/0/1/0/all/0/1">Will Perkins</a>, <a href="http://arxiv.org/find/math/1/au:+Sarantis_M/0/1/0/all/0/1">Michail Sarantis</a>, <a href="http://arxiv.org/find/math/1/au:+Tetali_P/0/1/0/all/0/1">Prasad Tetali</a></p><p>We study the locations of complex zeroes of independence polynomials of
bounded degree hypergraphs. For graphs, this is a long-studied subject with
applications to statistical physics, algorithms, and combinatorics. Results on
zero-free regions for bounded-degree graphs include Shearer's result on the
optimal zero-free disk, along with several recent results on other zero-free
regions. Much less is known for hypergraphs. We make some steps towards an
understanding of zero-free regions for bounded-degree hypergaphs by proving
that all hypergraphs of maximum degree $\Delta$ have a zero-free disk almost as
large as the optimal disk for graphs of maximum degree $\Delta$ established by
Shearer (of radius $\sim 1/(e \Delta)$). Up to logarithmic factors in $\Delta$
this is optimal, even for hypergraphs with all edge-sizes strictly greater than
$2$. We conjecture that for $k\ge 3$, $k$-uniform linear hypergraphs have a
much larger zero-free disk of radius $\Omega(\Delta^{- \frac{1}{k-1}} )$. We
establish this in the case of linear hypertrees.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-02T00:30:00Z">Wednesday, November 02 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Tuesday, November 01
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/01/postdocs-at-harvard-at-harvard-university-apply-by-november-21-2022/'>Postdocs at Harvard at Harvard University (apply by November 21, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Multiple postdoc positions at Harvard University in theoretical CS, machine learning foundations, quantum computing, CS and society and more Website: windowsontheory.org/2022/11/01/postdocs-at-harvard/ Email: achoat@seas.harvard.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Multiple postdoc positions at Harvard University in theoretical CS, machine learning foundations, quantum computing, CS and society and more</p>
<p>Website: <a href="https://windowsontheory.org/2022/11/01/postdocs-at-harvard/">https://windowsontheory.org/2022/11/01/postdocs-at-harvard/</a><br />
Email: achoat@seas.harvard.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-01T17:19:24Z">Tuesday, November 01 2022, 17:19</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://windowsontheory.org/2022/11/01/postdocs-at-harvard/'>Postdocs at Harvard!</a></h3>
        <p class='tr-article-feed'>from <a href='https://windowsontheory.org'>Windows on Theory</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The ML Foundations and theory groups at Harvard are looking for postdocs for the coming academic year. There are also several other positions at Harvard, including at the Harvard Data Science Initiative (HDSI), Center of Mathematical Sciences and Applications (CMSA), Swartz fellows at the Center for Brain Sciences, the George Carrier fellowship in applied mathematics, &#8230; Continue reading Postdocs at Harvard!
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The <a href="https://mlfoundations.org/#opportunities">ML Foundations </a>and <a href="https://toc.seas.harvard.edu/positions">theory</a> groups at Harvard are looking for postdocs for the coming academic year. </p>



<p>There are also several other positions at Harvard, including at the <a href="https://datascience.harvard.edu/data-science-postdoctoral-fellows">Harvard Data Science Initiative (HDSI)</a>, <a href="https://cmsa.fas.harvard.edu/about-us/jobs/"> Center of Mathematical Sciences and Applications (CMSA),</a> <a href="https://cbs.fas.harvard.edu/research/theory/#swartz">Swartz fellows</a> at the Center for Brain Sciences,  the <a href="https://academicpositions.harvard.edu/postings/11637">George Carrier fellowship in applied mathematics,</a>  the <a href="https://crcs.seas.harvard.edu/apply">Center for Research on Computation and Society (CRCS)</a>,   <a href="https://quantum.harvard.edu/external-candidates">Harvard Quantum Initiative (HQI)</a>. I hope that the newly announced <a href="https://www.harvard.edu/kempner-institute/">Kempner Institute</a> will also be able to offer positions for the next academic year. </p>



<p>All these positions have different foci, conditions, and the searches are run by different institutions, even if the set of potential mentors might be overlapping. Hence I strongly recommend that people apply to all positions that they are interested in. (</p>



<p>These positions and others are posted on the <a href="https://mlfoundations.org/#opportunities">opportunities</a> section of the ML foundations home page ( <a href="https://mlfoundations.org/#opportunities" rel="nofollow">https://mlfoundations.org/#opportunities</a> ). When I hear of new opportunities, I may update there and/or on <a href="https://twitter.com/boazbaraktcs">Twitter</a>.</p>
<p class="authors">By Boaz Barak</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-01T17:16:59Z">Tuesday, November 01 2022, 17:16</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/01/postdoc-at-harvard-university-apply-by-december-1-2022/'>Postdoc at Harvard University (apply by December 1, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The Harvard CS Theory Group invites applications for a variety of postdoctoral fellowships, including the Michael Rabin Postdoctoral Fellowship, postdocs in the Privacy Tools Project, Fairness in Prediction Algorithms, and Foundations in Machine Learning, and postdocs with individual faculty members. Website: academicpositions.harvard.edu/postings/11762 Email: achoat@seas.harvard.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The Harvard CS Theory Group invites applications for a variety of postdoctoral fellowships, including the Michael Rabin Postdoctoral Fellowship, postdocs in the Privacy Tools Project, Fairness in Prediction Algorithms, and Foundations in Machine Learning, and postdocs with individual faculty members.</p>
<p>Website: <a href="https://academicpositions.harvard.edu/postings/11762">https://academicpositions.harvard.edu/postings/11762</a><br />
Email: achoat@seas.harvard.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-01T16:20:19Z">Tuesday, November 01 2022, 16:20</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://differentialprivacy.org/composition-basics/'>Composition Basics</a></h3>
        <p class='tr-article-feed'>from <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Our data is subject to many different uses. Many entities will have access to our data and those entities will perform many different analyses that involve our data. The greatest risk to privacy is that an attacker will combine multiple pieces of information from the same or different sources and that the combination of these will reveal sensitive details about us.
Thus we cannot study privacy leakage in a vacuum; it is important that we can reason about the accumulated privacy leakage over multiple independent analyses, which is known as composition. We have previously discussed why composition is so important for differential privacy.</p>

<p>This is the first in a series of posts on composition in which we will explain in more detail how compositoin analyses work.</p>

<p>Composition is quantitative. The differential privacy guarantee of the overall system will depend on the number of analyses and the privacy parameters that they each satisfy. The exact relationship between these quantities can be complex. There are various composition theorems that give bounds on the overall parameters in terms of the parameters of the parts of the system.</p>

<p>The simplest composition theorem is what is known as basic composition, which applies to pure \(\varepsilon\)-DP (although it can be extended to approximate \((\varepsilon,\delta)\)-DP):</p>

<blockquote>
  <p>Theorem (Basic Composition)
Let \(M_1, M_2, \cdots, M_k : \mathcal{X}^n \to \mathcal{Y}\) be randomized algorithms. Suppose \(M_j\) is \(\varepsilon_j\)-DP for each \(j \in [k]\).
Define \(M : \mathcal{X}^n \to \mathcal{Y}^k\) by \(M(x)=(M_1(x),M_2(x),\cdots,M_k(x))\), where each algorithm is run independently. Then \(M\) is \(\varepsilon\)-DP for \(\varepsilon = \sum_{j=1}^k \varepsilon_j\).</p>
</blockquote>

<p>Proof.
Fix an arbitrary pair of neighbouring datasets \(x,x’ \in \mathcal{X}^n\) and output \(y \in \mathcal{Y}^k\).
To establish that \(M\) is \(\varepsilon\)-DP, we must show that \(e^{-\varepsilon} \le \frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} \le e^\varepsilon\). By independence, we have \[\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} = \frac{\prod_{j=1}^k\mathbb{P}[M_j(x)=y_j]}{\prod_{j=1}^k\mathbb{P}[M_j(x’)=y_j]} =  \prod_{j=1}^k \frac{\mathbb{P}[M_j(x)=y_j]}{\mathbb{P}[M_j(x’)=y_j]} \le \prod_{j=1}^k e^{\varepsilon_j} = e^{\sum_{j=1}^k \varepsilon_j} = e^\varepsilon,\] where the inequality follows from the fact that each \(M_j\) is \(\varepsilon_j\)-DP and, hence, \(e^{-\varepsilon_j} \le \frac{\mathbb{P}[M_j(x)=y_j]}{\mathbb{P}[M_j(x’)=y_j]} \le e^{\varepsilon_j}\). Similarly, \(\prod_{j=1}^k \frac{\mathbb{P}[M_j(x)=y_j]}{\mathbb{P}[M_j(x’)=y_j]} \ge \prod_{j=1}^k e^{-\varepsilon_j}\), which completes the proof. ∎</p>

<p>Basic composition is already a powerful result, despite its simple proof; it establishes the versatility of differential privacy and allows us to begin reasoning about complex systems in terms of their building blocks. For example, suppose we have \(k\) functions \(f_1, \cdots, f_k : \mathcal{X}^n \to \mathbb{R}\) each of sensitivity \(1\). For each \(j \in [k]\), we know that adding \(\mathsf{Laplace}(1/\varepsilon)\) noise to the value of \(f_j(x)\) satisfies \(\varepsilon\)-DP. Thus, if we add independent \(\mathsf{Laplace}(1/\varepsilon)\) noise to each value \(f_j(x)\) for all \(j \in [k]\), then basic composition tells us that releasing this vector of \(k\) noisy values satisfies \(k\varepsilon\)-DP. If we want the overall system to be \(\varepsilon\)-DP, then we should add independent \(\mathsf{Laplace}(k/\varepsilon)\) noise to each value \(f_j(x)\).</p>

Is Basic Composition Optimal?

<p>If we want to release \(k\) values each of sensitivity \(1\) (as above) and have the overall release be \(\varepsilon\)-DP, then, using basic composition, we can add \(\mathsf{Laplace}(k/\varepsilon)\) noise to each value. The variance of the noise for each value is \(2k^2/\varepsilon^2\), so the standard deviation is \(\sqrt{2} k /\varepsilon\). In other words, the scale of the noise must grow linearly with the number of values \(k\) if the overall privacy and each value’s sensitivity is fixed. It is natural to wonder whether the scale of the Laplace noise can be reduced by improving the basic composition result. We now show that this is not possible.</p>

<p>For each \(j \in [k]\), let \(M_j : \mathcal{X}^n \to \mathbb{R}\) be the algorithm that releases \(f_j(x)\) with \(\mathsf{Laplace}(k/\varepsilon)\) noise added. Let \(M : \mathcal{X}^n \to \mathbb{R}^k\) be the composition of these \(k\) algorithms. Then \(M_j\) is \(\varepsilon/k\)-DP for each \(j \in [k]\) and basic composition tells us that \(M\) is \(\varepsilon\)-DP. The question is whether \(M\) satisfies a better DP guarantee than this – i.e., does \(M\) satisfy \(\varepsilon_*\)-DP for some \(\varepsilon_*&lt;\varepsilon\)?
Suppose we have neighbouring datasets \(x,x’\in\mathcal{X}^n\) such that \(f_j(x) = f_j(x’)+1\) for each \(j \in [k]\). Let \(y=(a,a,\cdots,a) \in \mathbb{R}^k\) for some \(a \ge \max_{j=1}^k f_j(x)\).
Then 
\[
        \frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} = \frac{\prod_{j=1}^k \mathbb{P}[f_j(x)+\mathsf{Laplace}(k/\varepsilon)=y_j]}{\prod_{j=1}^k \mathbb{P}[f_j(x’)+\mathsf{Laplace}(k/\varepsilon)=y_j]} 
\]
\[
         = \prod_{j=1}^k \frac{\frac{\varepsilon}{2k}\exp\left(-\frac{\varepsilon}{k} |y_j-f_j(x)| \right)}{\frac{\varepsilon}{2k}\exp\left(-\frac{\varepsilon}{k} |y_j-f_j(x’)| \right)} 
         = \prod_{j=1}^k \frac{\exp\left(-\frac{\varepsilon}{k} (y_j-f_j(x)) \right)}{\exp\left(-\frac{\varepsilon}{k} (y_j-f_j(x’)) \right)} 
\]
\[
         = \prod_{j=1}^k \exp\left(\frac{\varepsilon}{k}\left(f_j(x)-f_j(x’)\right)\right)
         = \exp\left( \frac{\varepsilon}{k} \sum_{j=1}^k \left(f_j(x)-f_j(x’)\right)\right)= e^\varepsilon,
\]
where the third equality removes the absolute values because \(y_j \ge f_j(x)\) and \(y_j \ge f_j(x’)\).
This shows that basic composition is optimal. For this example, we cannot prove a better guarantee than what is given by basic composition.</p>

<p>Is there some other way to improve upon basic composition that circumvents this example? Note that we assumed that there are neighbouring datasets \(x,x’\in\mathcal{X}^n\) such that \(f_j(x) = f_j(x’)+1\) for each \(j \in [k]\). In some settings, no such worst case datasets exist. In that case, instead of scaling the noise linearly with \(k\), we can scale the Laplace noise according to the \(\ell_1\) sensitivity \(\Delta_1 := \sup_{x,x’ \in \mathcal{X}^n \atop \text{neighbouring}} \sum_{j=1}^k |f_j(x)-f_j(x’)|\).</p>

<p>Instead of adding assumptions to the problem, we will look more closely at the example above.
We showed that there exists some output \(y \in \mathbb{R}^d\) such that \(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} = e^\varepsilon\).
However, such outputs \(y\) are very rare, as we require \(y_j \ge \max\{f_j(x),f_j(x’)\}\) for each \(j \in [k]\) where \(y_j = f_j(x) + \mathsf{Laplace}(k/\varepsilon)\). Thus, in order to observe an output \(y\) such that the likelihood ratio is maximal, all of the \(k\) Laplace noise samples must be positive, which happens with probability \(2^{-k}\). 
The fact that outputs \(y\) with maximal likelihood ratio are exceedingly rare turns out to be a general phenomenon and not specific to the example above.</p>

<p>Can we improve on basic composition if we only ask for a high probability bound? That is, instead of demanding \(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} \le e^{\varepsilon_*}\) for all \(y \in \mathcal{Y}\), we demand \(\mathbb{P}_{Y \gets M(x)}\left[\frac{\mathbb{P}[M(x)=Y]}{\mathbb{P}[M(x’)=Y]} \le e^{\varepsilon_*}\right] \ge 1-\delta\) for some \(0 &lt; \delta \ll 1\). Can we prove a better bound \(\varepsilon_* &lt; \varepsilon\) in this relaxed setting? The answer turns out to be yes.</p>

<p>The limitation of pure \(\varepsilon\)-DP is that events with tiny probability – which are negligible in real-world applications – can dominate the privacy analysis. This motivates us to move to relaxed notions of differential privacy, such as approximate \((\varepsilon,\delta)\)-DP and concentrated DP, which are less sensitive to low probability events.</p>

Preview: Advanced Composition

<p>By moving to approximate \((\varepsilon,\delta)\)-DP with \(\delta&gt;0\), we can prove an asymptotically better composition theorem, which is known as the advanced composition theorem [DRV10].</p>

<blockquote>
  <p>Theorem (Advanced Composition Starting from Pure DP1)
Let \(M_1, M_2, \cdots, M_k : \mathcal{X}^n \to \mathcal{Y}\) be randomized algorithms. Suppose \(M_j\) is \(\varepsilon_j\)-DP for each \(j \in [k]\).
Define \(M : \mathcal{X}^n \to \mathcal{Y}^k\) by \(M(x)=(M_1(x),M_2(x),\cdots,M_k(x))\), where each algorithm is run independently. Then \(M\) is \((\varepsilon,\delta)\)-DP for any \(\delta&gt;0\) with \[\varepsilon = \frac12 \sum_{j=1}^k \varepsilon_j^2 + \sqrt{2\log(1/\delta) \sum_{j=1}^k \varepsilon_j^2}.\]</p>
</blockquote>

<p>Recall that basic composition gives \(\delta=0\) and \(\varepsilon = \sum_{j=1}^k \varepsilon_j\). That is, basic composition scales with the 1-norm of the vector \((\varepsilon_1, \varepsilon_2, \cdots, \varepsilon_k)\), whereas advanced composition scales with the 2-norm of this vector (and the squared 2-norm).
Neither bound strictly dominates the other. However, asymptotically (in a sense we will make precise in the next paragraph) advanced composition dominates basic composition.</p>

<p>Suppose we have a fixed \((\varepsilon,\delta)\)-DP guarantee for the entire system and we must answer \(k\) queries of sensitivity \(1\).
Using basic composition, we can answer each query by adding \(\mathsf{Laplace}(k/\varepsilon)\) noise to each answer.
However, using advanced composition, we can answer each query by adding \(\mathsf{Laplace}(\sqrt{k/2\rho})\) noise to each answer, where2
\[\rho = \frac{\varepsilon^2}{4\log(1/\delta)+4\varepsilon}.\]
If the privacy parameters \(\varepsilon,\delta&gt;0\) are fixed (which implies \(\rho\) is fixed) and \(k \to \infty\), we can see that asymptotically advanced composition gives noise per query scaling as \(\Theta(\sqrt{k})\), while basic composition results in noise scaling as \(\Theta(k)\).</p>

<p> </p>

<p>In the next few posts we will explain how advanced composition works. We hope this conveys an intuitive understanding of composition and, in particular, how this \(\sqrt{k}\) asymptotic behaviour arises. If you want to read ahead, these posts are extracts from this book chapter.</p>




  <ol>
    <li>
      <p>This result generalizes to approximate DP. If instead we assume \(M_j\) is \((\varepsilon_j,\delta_j)\)-DP for each \(j \in [k]\), then the final composition is \((\varepsilon,\delta+\sum_{j=1}^k \delta_j)\)-DP with \(\varepsilon\) as before. &#8617;</p>
    </li>
    <li>
      <p>Adding \(\mathsf{Laplace}(\sqrt{k/2\rho})\) noise to a sensitivity-1 query ensures \(\varepsilon_j\)-DP for \(\varepsilon_j = \sqrt{2\rho/k}\). Hence \(\sum_{j=1}^k \varepsilon_j^2 = 2\rho\). Setting \(\rho = \frac{\varepsilon^2}{4\log(1/\delta)+4\varepsilon}\) ensures that \(\frac12 \sum_{j=1}^k \varepsilon_j^2 + \sqrt{2\log(1/\delta) \sum_{j=1}^k \varepsilon_j^2} = \rho + \sqrt{4\rho\log(1/\delta)} \le \varepsilon\). &#8617;</p>
    </li>
  </ol>
<p>By </p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Our data is subject to many different uses. Many entities will have access to our data and those entities will perform many different analyses that involve our data. The greatest risk to privacy is that an attacker will combine multiple pieces of information from the same or different sources and that the combination of these will reveal sensitive details about us.
Thus we cannot study privacy leakage in a vacuum; it is important that we can reason about the accumulated privacy leakage over multiple independent analyses, which is known as <em>composition</em>. We have <a href="/privacy-composition/">previously discussed</a> why composition is so important for differential privacy.</p>

<p>This is the first in a series of posts on <em>composition</em> in which we will explain in more detail how compositoin analyses work.</p>

<p>Composition is quantitative. The differential privacy guarantee of the overall system will depend on the number of analyses and the privacy parameters that they each satisfy. The exact relationship between these quantities can be complex. There are various composition theorems that give bounds on the overall parameters in terms of the parameters of the parts of the system.</p>

<p>The simplest composition theorem is what is known as basic composition, which applies to pure \(\varepsilon\)-DP (although it can be extended to approximate \((\varepsilon,\delta)\)-DP):</p>

<blockquote>
  <p><strong>Theorem</strong> (Basic Composition)
Let \(M_1, M_2, \cdots, M_k : \mathcal{X}^n \to \mathcal{Y}\) be randomized algorithms. Suppose \(M_j\) is \(\varepsilon_j\)-DP for each \(j \in [k]\).
Define \(M : \mathcal{X}^n \to \mathcal{Y}^k\) by \(M(x)=(M_1(x),M_2(x),\cdots,M_k(x))\), where each algorithm is run independently. Then \(M\) is \(\varepsilon\)-DP for \(\varepsilon = \sum_{j=1}^k \varepsilon_j\).</p>
</blockquote>

<p><em>Proof.</em>
Fix an arbitrary pair of neighbouring datasets \(x,x’ \in \mathcal{X}^n\) and output \(y \in \mathcal{Y}^k\).
To establish that \(M\) is \(\varepsilon\)-DP, we must show that \(e^{-\varepsilon} \le \frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} \le e^\varepsilon\). By independence, we have \[\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} = \frac{\prod_{j=1}^k\mathbb{P}[M_j(x)=y_j]}{\prod_{j=1}^k\mathbb{P}[M_j(x’)=y_j]} =  \prod_{j=1}^k \frac{\mathbb{P}[M_j(x)=y_j]}{\mathbb{P}[M_j(x’)=y_j]} \le \prod_{j=1}^k e^{\varepsilon_j} = e^{\sum_{j=1}^k \varepsilon_j} = e^\varepsilon,\] where the inequality follows from the fact that each \(M_j\) is \(\varepsilon_j\)-DP and, hence, \(e^{-\varepsilon_j} \le \frac{\mathbb{P}[M_j(x)=y_j]}{\mathbb{P}[M_j(x’)=y_j]} \le e^{\varepsilon_j}\). Similarly, \(\prod_{j=1}^k \frac{\mathbb{P}[M_j(x)=y_j]}{\mathbb{P}[M_j(x’)=y_j]} \ge \prod_{j=1}^k e^{-\varepsilon_j}\), which completes the proof. ∎</p>

<p>Basic composition is already a powerful result, despite its simple proof; it establishes the versatility of differential privacy and allows us to begin reasoning about complex systems in terms of their building blocks. For example, suppose we have \(k\) functions \(f_1, \cdots, f_k : \mathcal{X}^n \to \mathbb{R}\) each of sensitivity \(1\). For each \(j \in [k]\), we know that adding \(\mathsf{Laplace}(1/\varepsilon)\) noise to the value of \(f_j(x)\) satisfies \(\varepsilon\)-DP. Thus, if we add independent \(\mathsf{Laplace}(1/\varepsilon)\) noise to each value \(f_j(x)\) for all \(j \in [k]\), then basic composition tells us that releasing this vector of \(k\) noisy values satisfies \(k\varepsilon\)-DP. If we want the overall system to be \(\varepsilon\)-DP, then we should add independent \(\mathsf{Laplace}(k/\varepsilon)\) noise to each value \(f_j(x)\).</p>

<h2 id="is-basic-composition-optimal">Is Basic Composition Optimal?</h2>

<p>If we want to release \(k\) values each of sensitivity \(1\) (as above) and have the overall release be \(\varepsilon\)-DP, then, using basic composition, we can add \(\mathsf{Laplace}(k/\varepsilon)\) noise to each value. The variance of the noise for each value is \(2k^2/\varepsilon^2\), so the standard deviation is \(\sqrt{2} k /\varepsilon\). In other words, the scale of the noise must grow linearly with the number of values \(k\) if the overall privacy and each value’s sensitivity is fixed. It is natural to wonder whether the scale of the Laplace noise can be reduced by improving the basic composition result. We now show that this is not possible.</p>

<p>For each \(j \in [k]\), let \(M_j : \mathcal{X}^n \to \mathbb{R}\) be the algorithm that releases \(f_j(x)\) with \(\mathsf{Laplace}(k/\varepsilon)\) noise added. Let \(M : \mathcal{X}^n \to \mathbb{R}^k\) be the composition of these \(k\) algorithms. Then \(M_j\) is \(\varepsilon/k\)-DP for each \(j \in [k]\) and basic composition tells us that \(M\) is \(\varepsilon\)-DP. The question is whether \(M\) satisfies a better DP guarantee than this – i.e., does \(M\) satisfy \(\varepsilon_*\)-DP for some \(\varepsilon_*&lt;\varepsilon\)?
Suppose we have neighbouring datasets \(x,x’\in\mathcal{X}^n\) such that \(f_j(x) = f_j(x’)+1\) for each \(j \in [k]\). Let \(y=(a,a,\cdots,a) \in \mathbb{R}^k\) for some \(a \ge \max_{j=1}^k f_j(x)\).
Then 
\[
        \frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} = \frac{\prod_{j=1}^k \mathbb{P}[f_j(x)+\mathsf{Laplace}(k/\varepsilon)=y_j]}{\prod_{j=1}^k \mathbb{P}[f_j(x’)+\mathsf{Laplace}(k/\varepsilon)=y_j]} 
\]
\[
         = \prod_{j=1}^k \frac{\frac{\varepsilon}{2k}\exp\left(-\frac{\varepsilon}{k} |y_j-f_j(x)| \right)}{\frac{\varepsilon}{2k}\exp\left(-\frac{\varepsilon}{k} |y_j-f_j(x’)| \right)} 
         = \prod_{j=1}^k \frac{\exp\left(-\frac{\varepsilon}{k} (y_j-f_j(x)) \right)}{\exp\left(-\frac{\varepsilon}{k} (y_j-f_j(x’)) \right)} 
\]
\[
         = \prod_{j=1}^k \exp\left(\frac{\varepsilon}{k}\left(f_j(x)-f_j(x’)\right)\right)
         = \exp\left( \frac{\varepsilon}{k} \sum_{j=1}^k \left(f_j(x)-f_j(x’)\right)\right)= e^\varepsilon,
\]
where the third equality removes the absolute values because \(y_j \ge f_j(x)\) and \(y_j \ge f_j(x’)\).
This shows that basic composition is optimal. For this example, we cannot prove a better guarantee than what is given by basic composition.</p>

<p>Is there some other way to improve upon basic composition that circumvents this example? Note that we assumed that there are neighbouring datasets \(x,x’\in\mathcal{X}^n\) such that \(f_j(x) = f_j(x’)+1\) for each \(j \in [k]\). In some settings, no such worst case datasets exist. In that case, instead of scaling the noise linearly with \(k\), we can scale the Laplace noise according to the \(\ell_1\) sensitivity \(\Delta_1 := \sup_{x,x’ \in \mathcal{X}^n \atop \text{neighbouring}} \sum_{j=1}^k |f_j(x)-f_j(x’)|\).</p>

<p>Instead of adding assumptions to the problem, we will look more closely at the example above.
We showed that there exists some output \(y \in \mathbb{R}^d\) such that \(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} = e^\varepsilon\).
However, such outputs \(y\) are very rare, as we require \(y_j \ge \max\{f_j(x),f_j(x’)\}\) for each \(j \in [k]\) where \(y_j = f_j(x) + \mathsf{Laplace}(k/\varepsilon)\). Thus, in order to observe an output \(y\) such that the likelihood ratio is maximal, all of the \(k\) Laplace noise samples must be positive, which happens with probability \(2^{-k}\). 
The fact that outputs \(y\) with maximal likelihood ratio are exceedingly rare turns out to be a general phenomenon and not specific to the example above.</p>

<p>Can we improve on basic composition if we only ask for a high probability bound? That is, instead of demanding \(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} \le e^{\varepsilon_*}\) for all \(y \in \mathcal{Y}\), we demand \(\mathbb{P}_{Y \gets M(x)}\left[\frac{\mathbb{P}[M(x)=Y]}{\mathbb{P}[M(x’)=Y]} \le e^{\varepsilon_*}\right] \ge 1-\delta\) for some \(0 &lt; \delta \ll 1\). Can we prove a better bound \(\varepsilon_* &lt; \varepsilon\) in this relaxed setting? The answer turns out to be yes.</p>

<p>The limitation of pure \(\varepsilon\)-DP is that events with tiny probability – which are negligible in real-world applications – can dominate the privacy analysis. This motivates us to move to relaxed notions of differential privacy, such as approximate \((\varepsilon,\delta)\)-DP and concentrated DP, which are less sensitive to low probability events.</p>

<h2 id="preview-advanced-composition">Preview: Advanced Composition</h2>

<p>By moving to approximate \((\varepsilon,\delta)\)-DP with \(\delta&gt;0\), we can prove an asymptotically better composition theorem, which is known as <em>the advanced composition theorem</em> <strong><a href="https://ieeexplore.ieee.org/document/5670947" title="Cynthia Dwork, Guy Rothblum, Salil Vadhan. Boosting and Differential Privacy. FOCS 2010.">[DRV10]</a></strong>.</p>

<blockquote>
  <p><strong>Theorem</strong> (Advanced Composition Starting from Pure DP<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>)
Let \(M_1, M_2, \cdots, M_k : \mathcal{X}^n \to \mathcal{Y}\) be randomized algorithms. Suppose \(M_j\) is \(\varepsilon_j\)-DP for each \(j \in [k]\).
Define \(M : \mathcal{X}^n \to \mathcal{Y}^k\) by \(M(x)=(M_1(x),M_2(x),\cdots,M_k(x))\), where each algorithm is run independently. Then \(M\) is \((\varepsilon,\delta)\)-DP for any \(\delta&gt;0\) with \[\varepsilon = \frac12 \sum_{j=1}^k \varepsilon_j^2 + \sqrt{2\log(1/\delta) \sum_{j=1}^k \varepsilon_j^2}.\]</p>
</blockquote>

<p>Recall that basic composition gives \(\delta=0\) and \(\varepsilon = \sum_{j=1}^k \varepsilon_j\). That is, basic composition scales with the 1-norm of the vector \((\varepsilon_1, \varepsilon_2, \cdots, \varepsilon_k)\), whereas advanced composition scales with the 2-norm of this vector (and the squared 2-norm).
Neither bound strictly dominates the other. However, asymptotically (in a sense we will make precise in the next paragraph) advanced composition dominates basic composition.</p>

<p>Suppose we have a fixed \((\varepsilon,\delta)\)-DP guarantee for the entire system and we must answer \(k\) queries of sensitivity \(1\).
Using basic composition, we can answer each query by adding \(\mathsf{Laplace}(k/\varepsilon)\) noise to each answer.
However, using advanced composition, we can answer each query by adding \(\mathsf{Laplace}(\sqrt{k/2\rho})\) noise to each answer, where<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>
\[\rho = \frac{\varepsilon^2}{4\log(1/\delta)+4\varepsilon}.\]
If the privacy parameters \(\varepsilon,\delta&gt;0\) are fixed (which implies \(\rho\) is fixed) and \(k \to \infty\), we can see that asymptotically advanced composition gives noise per query scaling as \(\Theta(\sqrt{k})\), while basic composition results in noise scaling as \(\Theta(k)\).</p>

<p> </p>

<p>In the next few posts we will explain how advanced composition works. We hope this conveys an intuitive understanding of composition and, in particular, how this \(\sqrt{k}\) asymptotic behaviour arises. If you want to read ahead, these posts are extracts from <a href="https://arxiv.org/abs/2210.00597">this book chapter</a>.</p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>This result generalizes to approximate DP. If instead we assume \(M_j\) is \((\varepsilon_j,\delta_j)\)-DP for each \(j \in [k]\), then the final composition is \((\varepsilon,\delta+\sum_{j=1}^k \delta_j)\)-DP with \(\varepsilon\) as before. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Adding \(\mathsf{Laplace}(\sqrt{k/2\rho})\) noise to a sensitivity-1 query ensures \(\varepsilon_j\)-DP for \(\varepsilon_j = \sqrt{2\rho/k}\). Hence \(\sum_{j=1}^k \varepsilon_j^2 = 2\rho\). Setting \(\rho = \frac{\varepsilon^2}{4\log(1/\delta)+4\varepsilon}\) ensures that \(\frac12 \sum_{j=1}^k \varepsilon_j^2 + \sqrt{2\log(1/\delta) \sum_{j=1}^k \varepsilon_j^2} = \rho + \sqrt{4\rho\log(1/\delta)} \le \varepsilon\). <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div><p class="authors">By </p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-01T15:45:00Z">Tuesday, November 01 2022, 15:45</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.16565'>The isotropy group of the matrix multiplication tensor</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: V.P.Burichenko</p><p>By an {\em isotropy group} of a tensor $t\in V_1 \otimes V_2\otimes
V_3=\widetilde V$ we mean the group of all invertible linear transformations of
$\widetilde V$ that leave $t$ invariant and are compatible (in an obvious
sense) with the structure of tensor product on~$\widetilde V$. We consider the
case where $t$ is the structure tensor of multiplication map of rectangular
matrices. The isotropy group of this tensor was studied in 1970s by de Groote,
Strassen, and Brockett-Dobkin. In the present work we enlarge, make more
precise, expose in the language of group actions on tensor spaces, and endow
with proofs the results previously known. This is necessary for studying the
algorithms of fast matrix multiplication admitting symmetries. The latter seems
to be a promising new way for constructing fast algorithms.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Burichenko_V/0/1/0/all/0/1">V.P.Burichenko</a></p><p>By an {\em isotropy group} of a tensor $t\in V_1 \otimes V_2\otimes
V_3=\widetilde V$ we mean the group of all invertible linear transformations of
$\widetilde V$ that leave $t$ invariant and are compatible (in an obvious
sense) with the structure of tensor product on~$\widetilde V$. We consider the
case where $t$ is the structure tensor of multiplication map of rectangular
matrices. The isotropy group of this tensor was studied in 1970s by de Groote,
Strassen, and Brockett-Dobkin. In the present work we enlarge, make more
precise, expose in the language of group actions on tensor spaces, and endow
with proofs the results previously known. This is necessary for studying the
algorithms of fast matrix multiplication admitting symmetries. The latter seems
to be a promising new way for constructing fast algorithms.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-01T00:30:00Z">Tuesday, November 01 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.16351'>Parallel Breadth-First Search and Exact Shortest Paths and Stronger Notions for Approximate Distances</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: V&#xe1;clav Rozho&#x148;, Bernhard Haeupler, Anders Martinsson, Christoph Grunau, Goran Zuzic</p><p>We introduce stronger notions for approximate single-source shortest-path
distances, show how to efficiently compute them from weaker standard notions,
and demonstrate the algorithmic power of these new notions and transformations.
One application is the first work-efficient parallel algorithm for computing
exact single-source shortest paths graphs -- resolving a major open problem in
parallel computing.
</p>
<p>Given a source vertex in a directed graph with polynomially-bounded
nonnegative integer lengths, the algorithm computes an exact shortest path tree
in $m \log^{O(1)} n$ work and $n^{1/2+o(1)}$ depth. Previously, no parallel
algorithm improving the trivial linear depths of Dijkstra's algorithm without
significantly increasing the work was known, even for the case of undirected
and unweighted graphs (i.e., for computing a BFS-tree).
</p>
<p>Our main result is a black-box transformation that uses $\log^{O(1)} n$
standard approximate distance computations to produce approximate distances
which also satisfy the subtractive triangle inequality (up to a
$(1+\varepsilon)$ factor) and even induce an exact shortest path tree in a
graph with only slightly perturbed edge lengths. These strengthened
approximations are algorithmically significantly more powerful and overcome
well-known and often encountered barriers for using approximate distances. In
directed graphs they can even be boosted to exact distances. This results in a
black-box transformation of any (parallel or distributed) algorithm for
approximate shortest paths in directed graphs into an algorithm computing exact
distances at essentially no cost. Applying this to the recent breakthroughs of
Fineman et al. for compute approximate SSSP-distances via approximate hopsets
gives new parallel and distributed algorithm for exact shortest paths.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Rozhon_V/0/1/0/all/0/1">V&#xe1;clav Rozho&#x148;</a>, <a href="http://arxiv.org/find/cs/1/au:+Haeupler_B/0/1/0/all/0/1">Bernhard Haeupler</a>, <a href="http://arxiv.org/find/cs/1/au:+Martinsson_A/0/1/0/all/0/1">Anders Martinsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Grunau_C/0/1/0/all/0/1">Christoph Grunau</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuzic_G/0/1/0/all/0/1">Goran Zuzic</a></p><p>We introduce stronger notions for approximate single-source shortest-path
distances, show how to efficiently compute them from weaker standard notions,
and demonstrate the algorithmic power of these new notions and transformations.
One application is the first work-efficient parallel algorithm for computing
exact single-source shortest paths graphs -- resolving a major open problem in
parallel computing.
</p>
<p>Given a source vertex in a directed graph with polynomially-bounded
nonnegative integer lengths, the algorithm computes an exact shortest path tree
in $m \log^{O(1)} n$ work and $n^{1/2+o(1)}$ depth. Previously, no parallel
algorithm improving the trivial linear depths of Dijkstra's algorithm without
significantly increasing the work was known, even for the case of undirected
and unweighted graphs (i.e., for computing a BFS-tree).
</p>
<p>Our main result is a black-box transformation that uses $\log^{O(1)} n$
standard approximate distance computations to produce approximate distances
which also satisfy the subtractive triangle inequality (up to a
$(1+\varepsilon)$ factor) and even induce an exact shortest path tree in a
graph with only slightly perturbed edge lengths. These strengthened
approximations are algorithmically significantly more powerful and overcome
well-known and often encountered barriers for using approximate distances. In
directed graphs they can even be boosted to exact distances. This results in a
black-box transformation of any (parallel or distributed) algorithm for
approximate shortest paths in directed graphs into an algorithm computing exact
distances at essentially no cost. Applying this to the recent breakthroughs of
Fineman et al. for compute approximate SSSP-distances via approximate hopsets
gives new parallel and distributed algorithm for exact shortest paths.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-01T00:30:00Z">Tuesday, November 01 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.16386'>Dynamic Bandits with an Auto-Regressive Temporal Structure</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Qinyi Chen, Negin Golrezaei, Djallel Bouneffouf</p><p>Multi-armed bandit (MAB) problems are mainly studied under two extreme
settings known as stochastic and adversarial. These two settings, however, do
not capture realistic environments such as search engines and marketing and
advertising, in which rewards stochastically change in time. Motivated by that,
we introduce and study a dynamic MAB problem with stochastic temporal
structure, where the expected reward of each arm is governed by an
auto-regressive (AR) model. Due to the dynamic nature of the rewards, simple
"explore and commit" policies fail, as all arms have to be explored
continuously over time. We formalize this by characterizing a per-round regret
lower bound, where the regret is measured against a strong (dynamic) benchmark.
We then present an algorithm whose per-round regret almost matches our regret
lower bound. Our algorithm relies on two mechanisms: (i) alternating between
recently pulled arms and unpulled arms with potential, and (ii) restarting.
These mechanisms enable the algorithm to dynamically adapt to changes and
discard irrelevant past information at a suitable rate. In numerical studies,
we further demonstrate the strength of our algorithm under different types of
non-stationary settings.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qinyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Golrezaei_N/0/1/0/all/0/1">Negin Golrezaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouneffouf_D/0/1/0/all/0/1">Djallel Bouneffouf</a></p><p>Multi-armed bandit (MAB) problems are mainly studied under two extreme
settings known as stochastic and adversarial. These two settings, however, do
not capture realistic environments such as search engines and marketing and
advertising, in which rewards stochastically change in time. Motivated by that,
we introduce and study a dynamic MAB problem with stochastic temporal
structure, where the expected reward of each arm is governed by an
auto-regressive (AR) model. Due to the dynamic nature of the rewards, simple
"explore and commit" policies fail, as all arms have to be explored
continuously over time. We formalize this by characterizing a per-round regret
lower bound, where the regret is measured against a strong (dynamic) benchmark.
We then present an algorithm whose per-round regret almost matches our regret
lower bound. Our algorithm relies on two mechanisms: (i) alternating between
recently pulled arms and unpulled arms with potential, and (ii) restarting.
These mechanisms enable the algorithm to dynamically adapt to changes and
discard irrelevant past information at a suitable rate. In numerical studies,
we further demonstrate the strength of our algorithm under different types of
non-stationary settings.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-01T00:30:00Z">Tuesday, November 01 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.16456'>Flows, Scaling, and Entropy Revisited: a Unified Perspective via Optimizing Joint Distributions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jason M. Altschuler</p><p>In this short expository note, we describe a unified algorithmic perspective
on several classical problems which have traditionally been studied in
different communities. This perspective views the main characters -- the
problems of Optimal Transport, Minimum Mean Cycle, Matrix Scaling, and Matrix
Balancing -- through the same lens of optimization problems over joint
probability distributions P(x,y) with constrained marginals. While this is how
Optimal Transport is typically introduced, this lens is markedly less
conventional for the other three problems. This perspective leads to a simple
and unified framework spanning problem formulation, algorithm development, and
runtime analysis.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Altschuler_J/0/1/0/all/0/1">Jason M. Altschuler</a></p><p>In this short expository note, we describe a unified algorithmic perspective
on several classical problems which have traditionally been studied in
different communities. This perspective views the main characters -- the
problems of Optimal Transport, Minimum Mean Cycle, Matrix Scaling, and Matrix
Balancing -- through the same lens of optimization problems over joint
probability distributions P(x,y) with constrained marginals. While this is how
Optimal Transport is typically introduced, this lens is markedly less
conventional for the other three problems. This perspective leads to a simple
and unified framework spanning problem formulation, algorithm development, and
runtime analysis.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-01T00:30:00Z">Tuesday, November 01 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.16460'>The Vector Balancing Constant for Zonotopes</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Laurel Heck, Victor Reis, Thomas Rothvoss</p><p>The vector balancing constant $\mathrm{vb}(K,Q)$ of two symmetric convex
bodies $K,Q$ is the minimum $r \geq 0$ so that any number of vectors from $K$
can be balanced into an $r$-scaling of $Q$. A question raised by Schechtman is
whether for any zonotope $K \subseteq \mathbb{R}^d$ one has $\mathrm{vb}(K,K)
\lesssim \sqrt{d}$. Intuitively, this asks whether a natural geometric
generalization of Spencer's Theorem (for which $K = B^d_\infty$) holds. We
prove that for any zonotope $K \subseteq \mathbb{R}^d$ one has
$\mathrm{vb}(K,K) \lesssim \sqrt{d} \log \log \log d$. Our main technical
contribution is a tight lower bound on the Gaussian measure of any section of a
normalized zonotope, generalizing Vaaler's Theorem for cubes. We also prove
that for two different normalized zonotopes $K$ and $Q$ one has
$\mathrm{vb}(K,Q) \lesssim \sqrt{d \log d}$. All the bounds are constructive
and the corresponding colorings can be computed in polynomial time.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Heck_L/0/1/0/all/0/1">Laurel Heck</a>, <a href="http://arxiv.org/find/math/1/au:+Reis_V/0/1/0/all/0/1">Victor Reis</a>, <a href="http://arxiv.org/find/math/1/au:+Rothvoss_T/0/1/0/all/0/1">Thomas Rothvoss</a></p><p>The vector balancing constant $\mathrm{vb}(K,Q)$ of two symmetric convex
bodies $K,Q$ is the minimum $r \geq 0$ so that any number of vectors from $K$
can be balanced into an $r$-scaling of $Q$. A question raised by Schechtman is
whether for any zonotope $K \subseteq \mathbb{R}^d$ one has $\mathrm{vb}(K,K)
\lesssim \sqrt{d}$. Intuitively, this asks whether a natural geometric
generalization of Spencer's Theorem (for which $K = B^d_\infty$) holds. We
prove that for any zonotope $K \subseteq \mathbb{R}^d$ one has
$\mathrm{vb}(K,K) \lesssim \sqrt{d} \log \log \log d$. Our main technical
contribution is a tight lower bound on the Gaussian measure of any section of a
normalized zonotope, generalizing Vaaler's Theorem for cubes. We also prove
that for two different normalized zonotopes $K$ and $Q$ one has
$\mathrm{vb}(K,Q) \lesssim \sqrt{d \log d}$. All the bounds are constructive
and the corresponding colorings can be computed in polynomial time.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-01T00:30:00Z">Tuesday, November 01 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.16534'>Improved Approximation Algorithms for Capacitated Vehicle Routing with Fixed Capacity</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Jingyang Zhao, Mingyu Xiao</p><p>The Capacitated Vehicle Routing Problem (CVRP) is one of the most extensively
studied problems in combinatorial optimization. According to the property of
the demand of customers, we distinguish three variants of CVRP: unit-demand,
splittable and unsplittable. We consider $k$-CVRP in general metrics and
general graphs, where $k$ is the capacity of the vehicle and all the three
versions are APX-hard for each fixed $k\geq 3$.
</p>
<p>In this paper, we give a $(5/2-\Theta(\sqrt{1/k}))$-approximation algorithm
for splittable and unit-demand $k$-CVRP and a
$(5/2+\ln2-\Theta(\sqrt{1/k}))$-approximation algorithm for unsplittable
$k$-CVRP (assume the approximation ratio for metric TSP is $\alpha=3/2$). Thus,
our approximation ratio is better than previous results for sufficient large
$k$, say $k\leq 1.7\times 10^7$.
</p>
<p>For small $k$, we design independent algorithms by using more techniques to
get further improvements. For splittable and unit-demand cases, we improve the
ratio from $1.934$ to $1.500$ for $k=3$, and from $1.750$ to $1.667$ for $k=4$.
For the unsplittable case, we improve the ratio from $2.693$ to $1.500$ for
$k=3$, from $2.443$ to $1.750$ for $k=4$, and from $2.893$ to $2.157$ for
$k=5$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jingyang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1">Mingyu Xiao</a></p><p>The Capacitated Vehicle Routing Problem (CVRP) is one of the most extensively
studied problems in combinatorial optimization. According to the property of
the demand of customers, we distinguish three variants of CVRP: unit-demand,
splittable and unsplittable. We consider $k$-CVRP in general metrics and
general graphs, where $k$ is the capacity of the vehicle and all the three
versions are APX-hard for each fixed $k\geq 3$.
</p>
<p>In this paper, we give a $(5/2-\Theta(\sqrt{1/k}))$-approximation algorithm
for splittable and unit-demand $k$-CVRP and a
$(5/2+\ln2-\Theta(\sqrt{1/k}))$-approximation algorithm for unsplittable
$k$-CVRP (assume the approximation ratio for metric TSP is $\alpha=3/2$). Thus,
our approximation ratio is better than previous results for sufficient large
$k$, say $k\leq 1.7\times 10^7$.
</p>
<p>For small $k$, we design independent algorithms by using more techniques to
get further improvements. For splittable and unit-demand cases, we improve the
ratio from $1.934$ to $1.500$ for $k=3$, and from $1.750$ to $1.667$ for $k=4$.
For the unsplittable case, we improve the ratio from $2.693$ to $1.500$ for
$k=3$, from $2.443$ to $1.750$ for $k=4$, and from $2.893$ to $2.157$ for
$k=5$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-01T00:30:00Z">Tuesday, November 01 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.16790'>One Gradient Frank-Wolfe for Decentralized Online Convex and Submodular Optimization</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Tuan-Anh Nguyen, Nguyen Kim Thang, Denis Trystram</p><p>Decentralized learning has been studied intensively in recent years motivated
by its wide applications in the context of federated learning. The majority of
previous research focuses on the offline setting in which the objective
function is static. However, the offline setting becomes unrealistic in
numerous machine learning applications that witness the change of massive data.
In this paper, we propose \emph{decentralized online} algorithm for convex and
continuous DR-submodular optimization, two classes of functions that are
present in a variety of machine learning problems. Our algorithms achieve
performance guarantees comparable to those in the centralized offline setting.
Moreover, on average, each participant performs only a \emph{single} gradient
computation per time step. Subsequently, we extend our algorithms to the bandit
setting. Finally, we illustrate the competitive performance of our algorithms
in real-world experiments.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tuan-Anh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Thang_N/0/1/0/all/0/1">Nguyen Kim Thang</a>, <a href="http://arxiv.org/find/cs/1/au:+Trystram_D/0/1/0/all/0/1">Denis Trystram</a></p><p>Decentralized learning has been studied intensively in recent years motivated
by its wide applications in the context of federated learning. The majority of
previous research focuses on the offline setting in which the objective
function is static. However, the offline setting becomes unrealistic in
numerous machine learning applications that witness the change of massive data.
In this paper, we propose \emph{decentralized online} algorithm for convex and
continuous DR-submodular optimization, two classes of functions that are
present in a variety of machine learning problems. Our algorithms achieve
performance guarantees comparable to those in the centralized offline setting.
Moreover, on average, each participant performs only a \emph{single} gradient
computation per time step. Subsequently, we extend our algorithms to the bandit
setting. Finally, we illustrate the competitive performance of our algorithms
in real-world experiments.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-01T00:30:00Z">Tuesday, November 01 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Monday, October 31
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2022/10/31/halloween-linkage.html'>Halloween linkage</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Perspective drawing often consists of mapping a curved world onto a flat screen. But what if you mapped a polyhedral world onto a curved screen (\(\mathbb{M}\))? Artwork by Szegedi Csaba, 1986. His more recent work is different enough that it’s not obvious they’re the same person.
        
        </div>

        <div class='tr-article-summary'>
        
          
          <ul>
  <li>
    <p>Perspective drawing often consists of mapping a curved world onto a flat screen. But <a href="http://www.szegedicsaba.com/curvedscreen.htm">what if you mapped a polyhedral world onto a curved screen</a> <span style="white-space:nowrap">(<a href="https://mastodon.social/@curved_ruler/109174245124286751">\(\mathbb{M}\)</a>)?</span> Artwork by Szegedi Csaba, 1986. <a href="https://www.szegedicsaba.hu/en">His more recent work</a> is different enough that it’s not obvious they’re the same person.</p>
  </li>
  <li>
    <p><a href="https://www.wired.com/story/wikipedia-state-sponsored-disinformation/">The hunt for Wikipedia’s disinformation moles</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109187943967850219">\(\mathbb{M}\)</a>).</span> <em>Wired</em> on coordinated long-term state-level disinformation campaigns on Wikipedia. I couldn’t find their link to the research they report on, but it appears to be “<a href="https://www.isdglobal.org/isd-publications/information-warfare-and-wikipedia/">Information Warfare and Wikipedia</a>“(by Carl Miller, Melanie Smith, Oliver Marsh, Kata Balint, Chris Inskip, and Francesca Visser of the Institute for Strategic Dialogue), which focuses on Russian disinformation in their war on Ukraine.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematicians-surprised-by-hidden-fibonacci-numbers-20221017/">Fibonacci numbers and fractals hiding in geometric optimization problems in symplectic geometry</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109193514333145471">\(\mathbb{M}\)</a>).</span>  I don’t understand symplectic geometry at all but this article kind of makes me want to try. Based on <a href="https://annals.math.princeton.edu/2012/175-3/p05">a 2012 paper by Dusa McDuff and Felix Schlenk</a>, and <a href="https://doi.org/10.1007/978-3-030-80979-9_2">a 2021 paper by 
Maria Bertozzi, Tara S. Holm, Emily Maw, Dusa McDuff, Grace T. Mwakyoma, Ana Rita Pires, and Morgan Weiler</a> (<a href="https://arxiv.org/abs/2010.08567">arXiv:2010.08567</a>).</p>
  </li>
  <li>
    <p>My colleague and coauthor <a href="https://meetings.informs.org/wordpress/indianapolis2022/awards-hall/">Vijay Vazirani wins the INFORMS John von Neumann Theory Prize</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109199025441051589">\(\mathbb{M}\)</a>)</span>  “for his fundamental and sustained contributions to the design of algorithms, including approximation algorithms, computational complexity theory, and algorithmic game theory, central to operations research and the management sciences”.</p>
  </li>
  <li>
    <p>Lost documents from ancient Greek science are still being discovered in palimpsests, parchments that have been scraped clean and rewritten with something else, via modern multispectral imaging techniques <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109204919184604129">\(\mathbb{M}\)</a>).</span> The latest: <a href="https://arstechnica.com/science/2022/10/part-of-lost-star-catalog-of-hipparchus-found-lurking-under-medieval-codex/">a constellation from a lost star catalog by Hipparchus</a> on the <a href="https://en.wikipedia.org/wiki/Codex_Climaci_Rescriptus">Codex Climaci rescriptus</a> from Saint Catherine’s Monastery on the Sinai Peninsula.</p>
  </li>
  <li>
    <p><a href="https://xorshammer.com/2008/09/04/a-geometrically-natural-uncomputable-function/">A geometrically natural uncomputable function</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109210523463519897">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=33268451">via</a>; from 2008), reporting on Alexander Nebutovsky’s “<a href="https://doi.org/10.1002/cpa.3160480402">Non-recursive functions, knots ‘with thick ropes’, and self-clenching ‘thick’ hyperspheres</a>”. If you embed an <span style="white-space:nowrap">\(n\)-dimensional</span> sphere into an <span style="white-space:nowrap">\((n+1)\)-dimensional</span> one, you can always continuously move the embedding to the equator, but you may have to move the embedding closer to itself first. How much closer? It’s uncomputable!</p>
  </li>
  <li>
    <p><a href="https://origami.kosmulski.org/blog/2022-10-23-fujimoto-books-public-domain">Five geometric origami books by Shuzo Fujimoto now public-domain</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109218557060750300">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=33307845">via</a>). They were originally published in the 1970s and 1980s and focus on polyhedra, tessellations, modular origami, and the like. In Japanese, but with lots of diagrams.</p>
  </li>
  <li>
    <p>Chris Purcell asks: <a href="https://mathstodon.xyz/@ccppurcell/109223240056862107">what do you call those fractal cracking patterns in paint, mud, etc.</a>?</p>
  </li>
  <li>
    <p><a href="https://arstechnica.com/science/2022/10/the-feds-new-open-access-policy-whos-gonna-pay-for-it/">Who will pay the publication fees under new grant-agency rules that all publications must be open access</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109232886231027636">\(\mathbb{M}\)</a>)?</span> “We face a growing risk that the ability to pay APCs – rather than the merits of the research – will determine what and who gets published.” … “I fear that forcing pay-to-play for every paper will end up amplifying existing inequities.”</p>
  </li>
  <li>
    <p>Claims to have proven <a href="https://doi.org/10.1007/s11225-022-10017-2">the twin prime conjecture</a> and <a href="https://doi.org/10.1007/s11225-022-10015-4">the existence of infinitely many Mersenne primes</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109237034145768039">\(\mathbb{M}\)</a>)</span> were published by Janusz Czelakowski in a peer-reviewed Polish logic journal. Beyond the obvious mistake in theorem 1.1 of the 1st link (the word “prime” is missing at an important point), other editors in a discussion on Wikipedia (where I found this) were skeptical that model theory and forcing are the right way to prove results like this. After <a href="https://mathoverflow.net/questions/433278/czelakowskis-claimed-proof-of-the-twin-prime-conjecture">the MathOverflow discussion</a> turned up more serious errors, the editor-in-chief promised a retraction.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Kite_(geometry)">New Wikipedia Good Article on kites</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109243040526023165">\(\mathbb{M}\)</a>),</span> the quadrilaterals, not the toys on strings and not the birds. The article briefly mentions that every non-rhombus kite has sides that are bitangents to two unequal circles. I was unable to source and did not include (although a figure makes it obvious) that this can be reversed. Every two unequal circles have four bitangents forming sides of exactly three quadrilaterals: a convex kite, a concave kite, and an antiparallelogram.</p>
  </li>
  <li>
    <p>The warm welcome given to edgelords on Twitter by the site’s new owner has led to another mass migration to Mastodon. One of the familiar names making the move is Vi Hart, who posted among other things about <a href="https://mastodon.social/@vihart/109244906964274293">60 whisks tangled together in icosahedral symmetry</a>.</p>
  </li>
  <li>
    <p>The usual version of the <a href="https://en.wikipedia.org/wiki/Chessboard_paradox">chessboard paradox</a> features seeming dissections of two different-area rectangles into the same set of triangles and trapezoids. Bruce35dc finds a variant with <a href="https://mathstodon.xyz/@Bruce35dc/109246400468582681">three dissections of the same rectangle</a>, into seven pieces, six out of the seven, and five of the six!</p>
  </li>
  <li>
    <p>It is a truth universally acknowledged that unless you write your papers with a catchy start, the referees will get bored and stop reading before getting to the important parts. <a href="https://igorpak.wordpress.com/2022/10/26/how-to-start-a-paper/">Igor Pak has some advice</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109261439776959800">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p>A few years ago Google search was good enough, and Wikipedia’s bad enough, that I would regularly search Google for “something site:en.wikipedia.org”. Now the tables have turned: <a href="https://www.theverge.com/23416056/wikipedia-app-vs-google-mobile-search">James Vincent suggests switching to the Wikipedia mobile app as the default for searches</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109265698873986719">\(\mathbb{M}\)</a>,</span> <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2022-10-31/In_the_media">via</a>) since much of the time what you are trying to find is there or linked from there. Of course, many of my searches seek sources for Wikipedia content, so that wouldn’t help me…</p>
  </li>
</ul><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-31T17:24:00Z">Monday, October 31 2022, 17:24</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=6784'>Oh right, quantum computing</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          These days, I often need to remind myself that, as an undergrad, grad student, postdoc, or professor, I&#8217;ve now been doing quantum computing research for a quarter-century&#8212;i.e., well over half of the subject&#8217;s existence. As a direct result, when I feel completely jaded about a new development in QC, it might actually be exciting. When [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>These days, I often need to remind myself that, as an undergrad, grad student, postdoc, or professor, I&#8217;ve now been doing quantum computing research for a quarter-century&#8212;i.e., well over half of the subject&#8217;s existence.  As a direct result, when I feel completely jaded about a new development in QC, it might actually be exciting.  When I feel moderately excited, it might actually be the most exciting thing for years.</p>



<p>With that in mind:</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>(1) Last week National Public Radio&#8217;s Marketplace <a href="https://www.marketplace.org/2022/10/27/china-and-the-us-vie-for-quantum-computing-supremacy/amp/">interviewed</a> me, John Martinis, and others about the current state of quantum computing.  While the piece wasn&#8217;t entirely hype-free, I&#8217;m pleased to report that my own views were represented accurately!  To wit:</p>



<blockquote class="wp-block-quote"><p>“There is a tsunami of hype about what quantum computers are going to revolutionize,” said Scott Aaronson, a professor of computer science at the University of Texas at Austin. “Quantum computing has turned into a word that venture capitalists or people seeking government funding will sprinkle on anything because it sounds good.”</p><p>Aaronson warned we can’t be certain that these computers will in fact revolutionize machine learning and finance and optimization problems.  “We can’t prove that there’s not a quantum algorithm that solves all these problems super fast, but we can’t even prove there’s not an algorithm for a conventional computer that does it,” he said.  [In the recorded version, they replaced this by a simpler but also accurate thought: namely, that we can&#8217;t prove one way or the other whether there&#8217;s a useful quantum advantage for these tasks.]</p><p><span style="font-size: revert; color: initial;"></span></p></blockquote>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>(2) I don&#8217;t like to use this blog to toot my own research horn, but on Thursday my postdoc Jason Pollack and I released a paper, entitled <a href="https://arxiv.org/pdf/2210.15601.pdf">Discrete Bulk Reconstruction</a>. And to be honest, I&#8217;m pretty damned excited about it.  It represents about 8 months of Jason&#8212;a cosmologist and string theorist who studied under Sean Carroll&#8212;helping me understand <a href="https://en.wikipedia.org/wiki/AdS/CFT_correspondence">AdS/CFT</a> in the language of the undergraduate CS curriculum, like min-cuts on undirected graphs, so that we could then look for polynomial-time algorithms to implement the holographic mapping from boundary quantum states to the spatial geometry in the bulk.  We drew heavily on previous work in the same direction, especially the already-seminal 2015 <a href="https://arxiv.org/abs/1505.07839">holographic entropy cone</a> paper by Ning Bao et al.  But I&#8217;d like to think that, among other things, our work represents a new frontier in just how accessible AdS/CFT itself can be made to CS and discrete math types.  Anyway, here&#8217;s the abstract if you&#8217;re interested:</p>



<blockquote class="wp-block-quote"><p>According to the <i>AdS/CFT correspondence</i>, the geometries of certain spacetimes are fully determined by quantum states that live on their boundaries &#8212; indeed, by the von Neumann entropies of portions of those boundary states. This work investigates to what extent the geometries can be reconstructed from the entropies <i>in polynomial time</i>. Bouland, Fefferman, and Vazirani (2019) argued that the AdS/CFT map can be exponentially complex if one wants to reconstruct regions such as the interiors of black holes. Our main result provides a sort of converse: we show that, in the special case of a single 1D boundary, if the input data consists of a list of entropies of <i>contiguous</i> boundary regions, and if the entropies satisfy a single inequality called Strong Subadditivity, then we can construct a graph model for the bulk in linear time. Moreover, the bulk graph is planar, it has O(N<sup>2</sup>) vertices (the information-theoretic minimum), and it&#8217;s &#8220;universal,&#8221; with only the edge weights depending on the specific entropies in question. From a combinatorial perspective, our problem boils down to an &#8220;inverse&#8221; of the famous min-cut problem: rather than being given a graph and asked to find a min-cut, here we&#8217;re given the values of min-cuts separating various sets of vertices, and need to find a weighted undirected graph consistent with those values. Our solution to this problem relies on the notion of a &#8220;bulkless&#8221; graph, which might be of independent interest for AdS/CFT. We also make initial progress on the case of multiple 1D boundaries &#8212; where the boundaries could be connected via wormholes &#8212; including an upper bound of O(N<sup>4</sup>) vertices whenever a planar bulk graph exists (thus putting the problem into the complexity class NP).</p></blockquote>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>(3) Anand Natarajan and Chinmay Nirkhe posted a preprint entitled <a href="https://arxiv.org/pdf/2210.15380.pdf">A classical oracle separation between QMA and QCMA</a>, which makes progress on a problem that&#8217;s been raised on this blog all the way back to its inception.  A bit of context: <a href="https://en.wikipedia.org/wiki/QMA">QMA</a>, Quantum Merlin-Arthur, captures what can be proven using a quantum state with poly(n) qubits as the proof, and a polynomial-time quantum algorithm as the verifier.  QCMA, or Quantum Classical Merlin-Arthur, is the same as QMA except that now the proof has to be classical.  A fundamental problem of quantum complexity theory, first raised by <a href="https://arxiv.org/abs/quant-ph/0210077">Aharonov and Naveh</a> in 2002, is whether QMA=QCMA.  In 2007, <a href="https://arxiv.org/abs/quant-ph/0604056">Greg Kuperberg and I</a> introduced the concept of quantum oracle separation&#8212;that is, a unitary that can be applied in a black-box manner&#8212;in order to show that there&#8217;s a quantum oracle relative to which QCMA≠QMA.  In 2015, <a href="https://arxiv.org/abs/1510.06750">Fefferman and Kimmel</a> improved this, to show that there&#8217;s a &#8220;randomized in-place&#8221; oracle relative to which QCMA≠QMA.  Natarajan and Nirkhe now remove the &#8220;in-place&#8221; part, meaning the only thing still &#8220;wrong&#8221; with their oracle is that it&#8217;s randomized.  Derandomizing their construction would finally settle this 20-year-old open problem (except, of course, for the minor detail of whether QMA=QCMA in the &#8220;real,&#8221; unrelativized world!).</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>(4) Oh right, the Google group reports the use of their superconducting processor to <a href="https://arxiv.org/pdf/2210.10255.pdf">simulate non-abelian anyons</a>.  Cool.</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-31T05:26:35Z">Monday, October 31 2022, 05:26</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15714'>List Agreement Expansion from Coboundary Expansion</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Roy Gotlib, Tali Kaufman</p><p>One of the key components in PCP constructions are agreement tests. In
agreement test the tester is given access to subsets of fixed size of some set,
each equipped with an assignment. The tester is then tasked with testing
whether these local assignments agree with some global assignment over the
entire set. One natural generalization of this concept is the case where,
instead of a single assignment to each local view, the tester is given access
to $l$ different assignments for every subset. The tester is then tasked with
testing whether there exist $l$ global functions that agree with all of the
assignments of all of the local views.
</p>
<p>In this work we present sufficient condition for a set system to exhibit this
generalized definition of list agreement expansion. This is, to our knowledge,
the first work to consider this natural generalization of agreement testing.
Despite initially appearing very similar to agreement expansion, list agreement
expansion seem to require a different set of techniques. This is due to the
fact that the natural extension of agreement testing does not suffice when
testing for list agreement, as list agreement crucially relies on a global
structure. It follows that if a local assignments satisfy list agreement they
must not only agree locally but also exhibit some additional structure. In
order to test for the existence of this additional structure we use a
connection between covering spaces of a high dimensional complex and its
coboundaries. We use this connection as a form of ``decoupling''.
</p>
<p>Moreover, we show that any set system that exhibits list agreement expansion
also supports direct sum testing. This is the first scheme for direct sum
testing that works regardless of the parity of the sizes of the local sets.
Prior to our work the schemes for direct sum testing were based on the parity
of the sizes of the local tests.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gotlib_R/0/1/0/all/0/1">Roy Gotlib</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaufman_T/0/1/0/all/0/1">Tali Kaufman</a></p><p>One of the key components in PCP constructions are agreement tests. In
agreement test the tester is given access to subsets of fixed size of some set,
each equipped with an assignment. The tester is then tasked with testing
whether these local assignments agree with some global assignment over the
entire set. One natural generalization of this concept is the case where,
instead of a single assignment to each local view, the tester is given access
to $l$ different assignments for every subset. The tester is then tasked with
testing whether there exist $l$ global functions that agree with all of the
assignments of all of the local views.
</p>
<p>In this work we present sufficient condition for a set system to exhibit this
generalized definition of list agreement expansion. This is, to our knowledge,
the first work to consider this natural generalization of agreement testing.
Despite initially appearing very similar to agreement expansion, list agreement
expansion seem to require a different set of techniques. This is due to the
fact that the natural extension of agreement testing does not suffice when
testing for list agreement, as list agreement crucially relies on a global
structure. It follows that if a local assignments satisfy list agreement they
must not only agree locally but also exhibit some additional structure. In
order to test for the existence of this additional structure we use a
connection between covering spaces of a high dimensional complex and its
coboundaries. We use this connection as a form of ``decoupling''.
</p>
<p>Moreover, we show that any set system that exhibits list agreement expansion
also supports direct sum testing. This is the first scheme for direct sum
testing that works regardless of the parity of the sizes of the local sets.
Prior to our work the schemes for direct sum testing were based on the parity
of the sizes of the local tests.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-31T00:30:00Z">Monday, October 31 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15748'>DESSERT: An Efficient Algorithm for Vector Set Search with Vector Set Queries</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Joshua Engels, Benjamin Coleman, Vihan Lakshman, Anshumali Shrivastava</p><p>We study the problem of \emph{vector set search} with \emph{vector set
queries}. This task is analogous to traditional near-neighbor search, with the
exception that both the query and each element in the collection are
\textit{sets} of vectors. We identify this problem as a core subroutine for
many web applications and find that existing solutions are unacceptably slow.
Towards this end, we present a new approximate search algorithm, DESSERT ({\bf
D}ESSERT {\bf E}ffeciently {\bf S}earches {\bf S}ets of {\bf E}mbeddings via
{\bf R}etrieval {\bf T}ables). DESSERT is a general tool with strong
theoretical guarantees and excellent empirical performance. When we integrate
DESSERT into ColBERT, a highly optimized state-of-the-art semantic search
method, we find a 2-5x speedup on the MSMarco passage ranking task with minimal
loss in recall, underscoring the effectiveness and practical applicability of
our proposal.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Engels_J/0/1/0/all/0/1">Joshua Engels</a>, <a href="http://arxiv.org/find/cs/1/au:+Coleman_B/0/1/0/all/0/1">Benjamin Coleman</a>, <a href="http://arxiv.org/find/cs/1/au:+Lakshman_V/0/1/0/all/0/1">Vihan Lakshman</a>, <a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1">Anshumali Shrivastava</a></p><p>We study the problem of \emph{vector set search} with \emph{vector set
queries}. This task is analogous to traditional near-neighbor search, with the
exception that both the query and each element in the collection are
\textit{sets} of vectors. We identify this problem as a core subroutine for
many web applications and find that existing solutions are unacceptably slow.
Towards this end, we present a new approximate search algorithm, DESSERT ({\bf
D}ESSERT {\bf E}ffeciently {\bf S}earches {\bf S}ets of {\bf E}mbeddings via
{\bf R}etrieval {\bf T}ables). DESSERT is a general tool with strong
theoretical guarantees and excellent empirical performance. When we integrate
DESSERT into ColBERT, a highly optimized state-of-the-art semantic search
method, we find a 2-5x speedup on the MSMarco passage ranking task with minimal
loss in recall, underscoring the effectiveness and practical applicability of
our proposal.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-31T00:30:00Z">Monday, October 31 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15962'>Parallel Self-Avoiding Walks for a Low-Autocorrelation Binary Sequences Problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Borko Bo&#x161;kovi&#x107;, Jana Herzog, Janez Brest</p><p>A low-autocorrelation binary sequences problem with a high figure of merit
factor represents a formidable computational challenge. An efficient parallel
computing algorithm is required to reach the new best-known solutions for this
problem. Therefore, we developed the $\mathit{sokol}_{\mathit{skew}}$ solver
for the skew-symmetric search space. The developed solver takes the advantage
of parallel computing on graphics processing units. The solver organized the
search process as a sequence of parallel and contiguous self-avoiding walks and
achieved a speedup factor of 387 compared with $\mathit{lssOrel}$, its
predecessor. The $\mathit{sokol}_{\mathit{skew}}$ solver belongs to stochastic
solvers and can not guarantee the optimality of solutions. To mitigate this
problem, we established the predictive model of stopping conditions according
to the small instances for which the optimal skew-symmetric solutions are
known. With its help and 99% probability, the $\mathit{sokol}_{\mathit{skew}}$
solver found all the known and seven new best-known skew-symmetric sequences
for odd instances from $L=121$ to $L=223$. For larger instances, the solver can
not reach 99% probability within our limitations, but it still found several
new best-known binary sequences. We also analyzed the trend of the best merit
factor values, and it shows that as sequence size increases, the value of the
merit factor also increases, and this trend is flatter for larger instances.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Boskovic_B/0/1/0/all/0/1">Borko Bo&#x161;kovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Herzog_J/0/1/0/all/0/1">Jana Herzog</a>, <a href="http://arxiv.org/find/cs/1/au:+Brest_J/0/1/0/all/0/1">Janez Brest</a></p><p>A low-autocorrelation binary sequences problem with a high figure of merit
factor represents a formidable computational challenge. An efficient parallel
computing algorithm is required to reach the new best-known solutions for this
problem. Therefore, we developed the $\mathit{sokol}_{\mathit{skew}}$ solver
for the skew-symmetric search space. The developed solver takes the advantage
of parallel computing on graphics processing units. The solver organized the
search process as a sequence of parallel and contiguous self-avoiding walks and
achieved a speedup factor of 387 compared with $\mathit{lssOrel}$, its
predecessor. The $\mathit{sokol}_{\mathit{skew}}$ solver belongs to stochastic
solvers and can not guarantee the optimality of solutions. To mitigate this
problem, we established the predictive model of stopping conditions according
to the small instances for which the optimal skew-symmetric solutions are
known. With its help and 99% probability, the $\mathit{sokol}_{\mathit{skew}}$
solver found all the known and seven new best-known skew-symmetric sequences
for odd instances from $L=121$ to $L=223$. For larger instances, the solver can
not reach 99% probability within our limitations, but it still found several
new best-known binary sequences. We also analyzed the trend of the best merit
factor values, and it shows that as sequence size increases, the value of the
merit factor also increases, and this trend is flatter for larger instances.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-31T00:30:00Z">Monday, October 31 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Sunday, October 30
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/10/30/lecturer-teaching-faculty-at-princeton-university-apply-by-february-1-2023/'>Lecturer (Teaching Faculty) at Princeton University (apply by February 1, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The Princeton University CS Department invites applications to join our teaching faculty. The teaching load for teaching faculty is typically one course per semester plus independent work advising, leaving time to pursue other activities, such as engaging in research. Review of applications will begin November 2022 on a rolling basis for Fall 2023 appointments. Website: [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The Princeton University CS Department invites applications to join our teaching faculty. The teaching load for teaching faculty is typically one course per semester plus independent work advising, leaving time to pursue other activities, such as engaging in research.</p>
<p>Review of applications will begin November 2022 on a rolling basis for Fall 2023 appointments.</p>
<p>Website: <a href="https://lift.cs.princeton.edu/hiring.html">https://lift.cs.princeton.edu/hiring.html</a><br />
Email: pparedes@cs.princeton.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-30T21:11:58Z">Sunday, October 30 2022, 21:11</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/10/30/assistant-professor-of-teaching-at-university-at-buffalo-apply-by-november-15-2022/'>Assistant Professor of Teaching at University at Buffalo (apply by November 15, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We have an opening for a teaching position for teaching courses in algorithms and computer security. Successful candidates will help support the establishment of a new course-based MS program. The deadline is a soft deadline as we will consider candidates until the position is filled but applying by Nov 15 would ensure timely consideration of [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>We have an opening for a teaching position for teaching courses in algorithms and computer security. Successful candidates will help support the establishment of a new course-based MS program. The deadline is a soft deadline as we will consider candidates until the position is filled but applying by Nov 15 would ensure timely consideration of your application!</p>
<p>Website: <a href="https://www.ubjobs.buffalo.edu/postings/36683">https://www.ubjobs.buffalo.edu/postings/36683</a><br />
Email: atri@buffalo.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-30T20:46:24Z">Sunday, October 30 2022, 20:46</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/10/30/tenure-track-faculty-position-at-university-at-buffalo-apply-by-december-30-2022/'>Tenure track faculty position at University at Buffalo (apply by December 30, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We are hiring tenure track/tenured professor in theory (among four areas in total). We also have positions open for machine learning and security/privacy that would also be theory friendly. Website: www.ubjobs.buffalo.edu/postings/37335 Email: atri@buffalo.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>We are hiring tenure track/tenured professor in theory (among four areas in total). We also have positions open for machine learning and security/privacy that would also be theory friendly.</p>
<p>Website: <a href="https://www.ubjobs.buffalo.edu/postings/37335">https://www.ubjobs.buffalo.edu/postings/37335</a><br />
Email: atri@buffalo.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-30T20:43:18Z">Sunday, October 30 2022, 20:43</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://blog.computationalcomplexity.org/2022/10/does-physics-nobel-prize-winner.html'>What was the recent  Nobel Prize in Physics really about?(Guest Post)</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>&nbsp;David Marcus was a Math major a year ahead of me at SUNY Stony brook (he graduated in 1979,</p><p>I graduated in 1980). He then got a PhD from MIT in Math, and is a reader of this blog.&nbsp; Recently he emailed me that he thinks the current Nobel Prize Winners in Physics do not understand their own work. Is it true? Let's find out!</p><p>------------------------</p><p>(Guest blog from David Marcus)</p><p>2022 Nobel Prize in Physics Awarded for Experiments that Demonstrate Nonlocality</p><p>The 2022 Nobel Prize in Physics was recently awarded to experimenters who demonstrated that the world is nonlocal. The curious thing is that neither the writers of the Nobel Prize press release nor the recipients seem to understand that this is what they demonstrated.</p><p><br></p><p>For example, the press release (see here) says: "John Clauser developed John Bell's ideas, leading to a practical experiment. When he took the measurements, they supported quantum mechanics by clearly violating a Bell inequality. This means that quantum mechanics cannot be replaced by a theory that uses hidden variables." That is not what the experiments mean, and the statement is false.</p><p>The word "locality" means that doing something here cannot instantly change something other there.</p><p>The experimental setup is the following: You prepare two particles, A and B, and send them in opposite directions so that they are far apart. You and your colleague do experiments on each particle at the same time. If you and your colleague perform the same experiment, then, from your experiment on A, you can predict with certainty the result of your colleague's experiment on B (and vice versa).</p><p>In a paper in 1935, Einstein, Podolsky, and Rosen pointed out that, assuming locality, the experimental results at A and B must be determined by the source that prepared the particles. They didn't actually say, "assuming locality", but they implicitly assumed it. (If you disagree with them, please offer an alternative.)</p><p>In 1964, John Bell published his paper. In it, he considered three of the experiments that could be done on the particles A and B. Assuming the results are determined by the source (which follows from Einstein, Podolsky, and Rosen's argument), he derived an inequality on the correlations between the results of the three experiments on the two particles. The math is simple; for details, see&nbsp;here.</p><p>The Nobel Prize winners did experiments, and their results violated Bell's inequality (or similar inequalities). Hence, the world is nonlocal.</p><p>The simplest theory that agrees with experiment is Bohmian Mechanics. This is a deterministic theory of particles whose motion is governed by a wave (the wave function being the solution of the Schrödinger equation). Of course, Bohmian Mechanics is nonlocal, as is the world.</p><p>By gasarch</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>&nbsp;David Marcus was a Math major a year ahead of me at SUNY Stony brook (he graduated in 1979,</p><p>I graduated in 1980). He then got a PhD from MIT in Math, and is a reader of this blog.&nbsp; Recently he emailed me that he thinks the current Nobel Prize Winners in Physics do not understand their own work. Is it true? Let's find out!</p><p>------------------------</p><p>(Guest blog from David Marcus)</p><p>2022 Nobel Prize in Physics Awarded for Experiments that Demonstrate Nonlocality</p><p>The 2022 Nobel Prize in Physics was recently awarded to experimenters who demonstrated that the world is nonlocal. The curious thing is that neither the writers of the Nobel Prize press release nor the recipients seem to understand that this is what they demonstrated.</p><p><br /></p><p>For example, the press release (see <a href="https://www.nobelprize.org/prizes/physics/2022/press-release/">here</a>) says: "John Clauser developed John Bell's ideas, leading to a practical experiment. When he took the measurements, they supported quantum mechanics by clearly violating a Bell inequality. This means that quantum mechanics cannot be replaced by a theory that uses hidden variables." That is not what the experiments mean, and the statement is false.</p><p>The word "locality" means that doing something here cannot instantly change something other there.</p><p>The experimental setup is the following: You prepare two particles, A and B, and send them in opposite directions so that they are far apart. You and your colleague do experiments on each particle at the same time. If you and your colleague perform the same experiment, then, from your experiment on A, you can predict with certainty the result of your colleague's experiment on B (and vice versa).</p><p>In a paper in 1935, Einstein, Podolsky, and Rosen pointed out that, assuming locality, the experimental results at A and B must be determined by the source that prepared the particles. They didn't actually say, "assuming locality", but they implicitly assumed it. (If you disagree with them, please offer an alternative.)</p><p>In 1964, John Bell published his paper. In it, he considered three of the experiments that could be done on the particles A and B. Assuming the results are determined by the source (which follows from Einstein, Podolsky, and Rosen's argument), he derived an inequality on the correlations between the results of the three experiments on the two particles. The math is simple; for details, see&nbsp;<a href="http://www.scholarpedia.org/article/Bell%27s_theorem">here</a>.</p><p>The Nobel Prize winners did experiments, and their results violated Bell's inequality (or similar inequalities). Hence, the world is nonlocal.</p><p>The simplest theory that agrees with experiment is Bohmian Mechanics. This is a deterministic theory of particles whose motion is governed by a wave (the wave function being the solution of the Schrödinger equation). Of course, Bohmian Mechanics is nonlocal, as is the world.</p><p class="authors">By gasarch</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-30T17:35:00Z">Sunday, October 30 2022, 17:35</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Saturday, October 29
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/10/29/postdoc-at-linkoping-university-apply-by-november-7-2022/'>Postdoc at Linköping University (apply by November 7, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Linköping University invites applications for postdoc positions. In particular, the Theoretical Computer Science Laboratory is looking for postdocs with a strong background in theoretical computer science. Examples of research topics include (1) fine-grained complexity, (2) algebraic methods for CSPs, (3) parameterized complexity, and (4) randomized and approximation algorithms. Website: liu.se/en/work-at-liu/vacancies?rmpage=job&#38;rmjob=20079&#38;rmlang=UK Email: peter.jonsson@liu.se
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Linköping University invites applications for postdoc positions. In particular, the Theoretical Computer Science Laboratory<br />
is looking for postdocs with a strong background in theoretical computer science. Examples of research topics include (1) fine-grained complexity, (2) algebraic methods for CSPs, (3) parameterized complexity, and (4) randomized and approximation algorithms.</p>
<p>Website: <a href="https://liu.se/en/work-at-liu/vacancies?rmpage=job&amp;rmjob=20079&amp;rmlang=UK">https://liu.se/en/work-at-liu/vacancies?rmpage=job&amp;rmjob=20079&amp;rmlang=UK</a><br />
Email: peter.jonsson@liu.se</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-29T05:59:45Z">Saturday, October 29 2022, 05:59</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Friday, October 28
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/10/28/assisstant-professor-at-university-of-british-columbia-apply-by-november-15-2022/'>assisstant professor at University of British Columbia (apply by November 15, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The UBC math department seeks candidates for a tenure-track assistant professor position, with expertise in the mathematics of Machine Learning and AI. Specific topics of interest include, but are not limited to, neural nets, statistical learning theory, inverse problems, optimization, mathematical data science, and mathematics of information, with a strong theoretical component. Website: www.mathjobs.org/jobs/list/20980 Email: [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The UBC math department seeks candidates for a tenure-track assistant professor position, with expertise in the mathematics of Machine Learning and AI. Specific topics of interest include, but are not limited to, neural nets, statistical learning theory, inverse problems, optimization, mathematical data science, and mathematics of information, with a strong theoretical component.</p>
<p>Website: <a href="https://www.mathjobs.org/jobs/list/20980">https://www.mathjobs.org/jobs/list/20980</a><br />
Email: exec-coord@math.ubc.ca</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T19:41:58Z">Friday, October 28 2022, 19:41</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=6778'>On Bryan Caplan and his new book</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Yesterday I attended a lecture by George Mason University economist Bryan Caplan, who&#8217;s currently visiting UT Austin, about his new book entitled Don’t Be a Feminist. (See also here for previous back-and-forth between me and Bryan about his book.) A few remarks: (1) Maybe surprisingly, there were no protesters storming the lectern, no security detail, [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Yesterday I attended a lecture by George Mason University economist <a href="https://betonit.substack.com/">Bryan Caplan</a>, who&#8217;s currently visiting UT Austin, about his new book entitled <em><a href="https://www.amazon.com/Dont-Be-Feminist-Genuine-Justice/dp/B0BD3DFMMH/ref=asc_df_B0BD3DFMMH/">Don’t Be a Feminist</a></em>.  (<a href="https://betonit.substack.com/p/aaronson-on-feminism-my-reply">See also here</a> for previous back-and-forth between me and Bryan about his book.)  A few remarks:</p>



<p>(1) Maybe surprisingly, there were no protesters storming the lectern, no security detail, not even a single rotten vegetable thrown. About 30 people showed up, majority men but women too. They listened politely and asked polite questions afterward. One feminist civilly challenged Bryan during the Q&amp;A about his gender pay gap statistics.</p>



<p>(2) How is it that I got denounced <a></a>by half the planet for saying once, in a blog comment, that I agreed with 97% of feminism but had concerns with one particular way it was operationalized, whereas Bryan seems to be … not denounced in the slightest for publishing a book and going on a lecture tour about how he rejects feminism in its entirety as angry and self-pitying in addition to factually false? Who can explain this to me?</p>



<p>(3) For purposes of his argument, Bryan defines feminism as &#8220;the view that women are generally treated less fairly than men,&#8221; rather than (say) &#8220;the view that men and women <em>ought</em> to be treated equally,&#8221; or &#8220;the radical belief that women are people,&#8221; or other formulations that Bryan considers too obvious to debate.  He then rebuts feminism as he&#8217;s defined it, by taking the audience on a horror tour of all the ways society treats men less fairly than women (expectations of doing dirty and dangerous work, divorce law, military drafts as in Ukraine right now, &#8230;), as well as potentially benign explanations for apparent unfairness toward women, to argue that it&#8217;s at least <em>debatable</em> which sex gets the rawer deal on average.</p>



<p>During the Q&amp;A, I raised what I thought was the central objection to Bryan&#8217;s relatively narrow definition of feminism. Namely that, by the standards of 150 years ago, Bryan is <em>obviously</em> a feminist, and so am I, and so is everyone in the room. (Whereupon a right-wing business school professor interjected: &#8220;please don’t make assumptions about me!&#8221;)</p>



<p>I explained that <em>this</em> is why I call myself a feminist, despite agreeing with many of Bryan&#8217;s substantive points: because I want no one to imagine for a nanosecond that, if I had the power, I&#8217;d take gender relations back to how they were generations ago.</p>



<p>Bryan replied that >60% of Americans call themselves non-feminists in surveys. So, he asked me rhetorically, do <em>all</em> those Americans secretly yearn to take us back to the 19th century? Such a position, he said, seemed so absurdly uncharitable as not to be worth responding to.</p>



<p>Reflecting about it on my walk home, I realized: actually, give or take the exact percentages, this is <em>precisely</em> the progressive thesis. I.e., that just like at least a solid minority of Germans turned out to be totally fine with Nazism, however much they might&#8217;ve denied it beforehand, so too at least a solid minority of Americans would be fine with&#8212;if not ecstatic about&#8212;<em>The Handmaid&#8217;s Tale</em> made real. Indeed, they&#8217;d add, it&#8217;s only vociferous progressive activism that stands between us and that dystopia.</p>



<p>And if anyone were tempted to doubt this, progressives might point to the election of Donald Trump, the failed insurrection to maintain his power, and the repeal of <em>Roe</em> as proof enough to last for a quadrillion years.</p>



<p>Bryan would probably reply: why even waste time engaging with such a hysterical position? To me, though, the hysterical position sadly has more than a grain of truth to it. I <em>wish</em> we lived in a world where there was no point in calling oneself a pro-democracy anti-racist feminist and a hundred other banal and obvious things. I just don&#8217;t think that we do.</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T16:54:52Z">Friday, October 28 2022, 16:54</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://thmatters.wordpress.com/2022/10/28/acm-survey-on-math-requirements-for-the-cs-major/'>ACM survey on math requirements for the CS major</a></h3>
        <p class='tr-article-feed'>from <a href='https://thmatters.wordpress.com'>Theory Matters</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The ACM/IEEE-CS/AAAI CS2023 Curricular Task Force is working on updating the undergraduate CS curriculum guidelines for the next decade. They have distributed a survey about the role of math in that curriculum, which is of direct interest to the TCS community. Please consider taking the survey so your opinion is heard! From the Task Force: &#8212;&#8212;&#8212;&#8211; [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The <strong>ACM/IEEE-CS/AAAI CS2023 Curricular Task Force </strong>is working on updating the undergraduate CS curriculum guidelines for the next decade. They have distributed a survey about the role of math in that curriculum, which is of direct interest to the TCS community. Please consider taking the survey so your opinion is heard!</p>



<p>From the Task Force:</p>



<p>&#8212;&#8212;&#8212;&#8211;</p>



<p>Dear educator,</p>



<p>What math should undergraduate Computer Science students know?</p>



<p>The CS2023 Task Force is collecting (and will share!) input from the community on this very important topic both as a useful “sense of the community” for everyone and, pertinent to our immediate goal, to shape our decennial curricular recommendations.</p>



<p>We invite you to fill out a survey: <a href="https://tinyurl.com/7zjbu7pr" target="_blank" rel="noreferrer noopener">https://tinyurl.com/7zjbu7pr</a></p>



<p>As you fill out this survey, we ask you to reflect on:</p>



<ul>
<li>Discrete mathematics: student preparedness, topics covered, what’s missing?</li>



<li>What should come beyond discrete mathematics, if anything?</li>



<li>What do the new high-growth areas (AI, ML, quantum computing, data science) need by way of mathematical preparation?</li>



<li>Do most CS jobs need much mathematics, and do current mathematical requirements pose a barrier to some populations of students?</li>
</ul>



<p>Thank you in advance for taking the time to fill out the survey!</p>



<p><em>If you believe that other colleagues in your department can contribute, please forward the survey link to them</em>.</p>



<p>Amruth Kumar and Rajendra Raj</p>



<p>On behalf of the of the ACM/IEEE-CS/AAAI CS2023 Curricular Task Force</p>



<p>NOTE: By participating, you agree that we may use your responses for this study; and that this data may be presented in aggregate form (with no personally identifying information) in articles or websites.</p>



<p>&#8212;&#8212;&#8211;</p>
<p class="authors">By shuchic</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T15:09:38Z">Friday, October 28 2022, 15:09</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/10/28/postdoc-at-university-of-michigan-apply-by-january-10-2023/'>Postdoc at University of Michigan (apply by January 10, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The Theory Group at the University of Michigan invites applications for postdoctoral position(s) beginning September 2023. The position will have an initial appointment for one year but may be extended depending on circumstances. Applicants should be recent PhDs with interests that align well with our ongoing research. Website: forms.gle/P9SAVGhP3tu9rayZA Email: thsa@umich.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The Theory Group at the University of Michigan invites applications for postdoctoral position(s) beginning September 2023. The position will have an initial appointment for one year but may be extended depending on circumstances.</p>
<p>Applicants should be recent PhDs with interests that align well with our ongoing research.</p>
<p>Website: <a href="https://forms.gle/P9SAVGhP3tu9rayZA">https://forms.gle/P9SAVGhP3tu9rayZA</a><br />
Email: thsa@umich.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T13:06:28Z">Friday, October 28 2022, 13:06</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15325'>Geodesic packing in graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Paul Manuel, Bostjan Bresar, Sandi Klavzar</p><p>Given a graph $G$, a geodesic packing in $G$ is a set of vertex-disjoint
maximal geodesics, and the geodesic packing number of $G$, ${\gpack}(G)$, is
the maximum cardinality of a geodesic packing in $G$. It is proved that the
decision version of the geodesic packing number is NP-complete. We also
consider the geodesic transversal number, ${\gt}(G)$, which is the minimum
cardinality of a set of vertices that hit all maximal geodesics in $G$. While
$\gt(G)\ge \gpack(G)$ in every graph $G$, the quotient ${\rm gt}(G)/{\rm
gpack}(G)$ is investigated. By using the rook's graph, it is proved that there
does not exist a constant $C &lt; 3$ such that $\frac{{\rm gt}(G)}{{\rm
gpack}(G)}\le C$ would hold for all graphs $G$. If $T$ is a tree, then it is
proved that ${\rm gpack}(T) = {\rm gt}(T)$, and a linear algorithm for
determining ${\rm gpack}(T)$ is derived. The geodesic packing number is also
determined for the strong product of paths.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Manuel_P/0/1/0/all/0/1">Paul Manuel</a>, <a href="http://arxiv.org/find/math/1/au:+Bresar_B/0/1/0/all/0/1">Bostjan Bresar</a>, <a href="http://arxiv.org/find/math/1/au:+Klavzar_S/0/1/0/all/0/1">Sandi Klavzar</a></p><p>Given a graph $G$, a geodesic packing in $G$ is a set of vertex-disjoint
maximal geodesics, and the geodesic packing number of $G$, ${\gpack}(G)$, is
the maximum cardinality of a geodesic packing in $G$. It is proved that the
decision version of the geodesic packing number is NP-complete. We also
consider the geodesic transversal number, ${\gt}(G)$, which is the minimum
cardinality of a set of vertices that hit all maximal geodesics in $G$. While
$\gt(G)\ge \gpack(G)$ in every graph $G$, the quotient ${\rm gt}(G)/{\rm
gpack}(G)$ is investigated. By using the rook's graph, it is proved that there
does not exist a constant $C &lt; 3$ such that $\frac{{\rm gt}(G)}{{\rm
gpack}(G)}\le C$ would hold for all graphs $G$. If $T$ is a tree, then it is
proved that ${\rm gpack}(T) = {\rm gt}(T)$, and a linear algorithm for
determining ${\rm gpack}(T)$ is derived. The geodesic packing number is also
determined for the strong product of paths.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15509'>On Tsirelson pairs of C*-algebras</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Isaac Goldbring, Bradd Hart</p><p>We introduce the notion of a Tsirelson pair of C*-algebras, which is a pair
of C*-algebras for which the space of quantum strategies obtained by using
states on the minimal tensor product of the pair and the space of quantum
strategies obtained by using states on the maximal tensor product of the pair
coincide. We exhibit a number of examples of such pairs that are ``nontrivial''
in the sense that the minimal tensor product and the maximal tensor product of
the pair are not isomorphic. For example, we prove that any pair containing a
C*-algebra with Kirchberg's QWEP property is a Tsirelson pair. We then
introduce the notion of a C*-algebra with the Tsirelson property (TP) and
establish a number of closure properties for this class. We also show that the
class of C*-algebras with the TP form an axiomatizable class (in the sense of
model theory), but that this class admits no ``effective'' axiomatization.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Goldbring_I/0/1/0/all/0/1">Isaac Goldbring</a>, <a href="http://arxiv.org/find/math/1/au:+Hart_B/0/1/0/all/0/1">Bradd Hart</a></p><p>We introduce the notion of a Tsirelson pair of C*-algebras, which is a pair
of C*-algebras for which the space of quantum strategies obtained by using
states on the minimal tensor product of the pair and the space of quantum
strategies obtained by using states on the maximal tensor product of the pair
coincide. We exhibit a number of examples of such pairs that are ``nontrivial''
in the sense that the minimal tensor product and the maximal tensor product of
the pair are not isomorphic. For example, we prove that any pair containing a
C*-algebra with Kirchberg's QWEP property is a Tsirelson pair. We then
introduce the notion of a C*-algebra with the Tsirelson property (TP) and
establish a number of closure properties for this class. We also show that the
class of C*-algebras with the TP form an axiomatizable class (in the sense of
model theory), but that this class admits no ``effective'' axiomatization.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15601'>Discrete Bulk Reconstruction</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Scott Aaronson, Jason Pollack</p><p>According to the AdS/CFT correspondence, the geometries of certain spacetimes
are fully determined by quantum states that live on their boundaries -- indeed,
by the von Neumann entropies of portions of those boundary states. This work
investigates to what extent the geometries can be reconstructed from the
entropies in polynomial time. Bouland, Fefferman, and Vazirani (2019) argued
that the AdS/CFT map can be exponentially complex if one wants to reconstruct
regions such as the interiors of black holes. Our main result provides a sort
of converse: we show that, in the special case of a single 1D boundary, if the
input data consists of a list of entropies of contiguous boundary regions, and
if the entropies satisfy a single inequality called Strong Subadditivity, then
we can construct a graph model for the bulk in linear time. Moreover, the bulk
graph is planar, it has $O(N^2)$ vertices (the information-theoretic minimum),
and it's ``universal,'' with only the edge weights depending on the specific
entropies in question. From a combinatorial perspective, our problem boils down
to an ``inverse'' of the famous min-cut problem: rather than being given a
graph and asked to find a min-cut, here we're given the values of min-cuts
separating various sets of vertices, and need to find a weighted undirected
graph consistent with those values. Our solution to this problem relies on the
notion of a ``bulkless'' graph, which might be of independent interest for
AdS/CFT. We also make initial progress on the case of multiple 1D boundaries --
where the boundaries could be connected via wormholes -- including an upper
bound of $O(N^4)$ vertices whenever a planar bulk graph exists (thus putting
the problem into the complexity class $\mathsf{NP}$).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/quant-ph/1/au:+Aaronson_S/0/1/0/all/0/1">Scott Aaronson</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Pollack_J/0/1/0/all/0/1">Jason Pollack</a></p><p>According to the AdS/CFT correspondence, the geometries of certain spacetimes
are fully determined by quantum states that live on their boundaries -- indeed,
by the von Neumann entropies of portions of those boundary states. This work
investigates to what extent the geometries can be reconstructed from the
entropies in polynomial time. Bouland, Fefferman, and Vazirani (2019) argued
that the AdS/CFT map can be exponentially complex if one wants to reconstruct
regions such as the interiors of black holes. Our main result provides a sort
of converse: we show that, in the special case of a single 1D boundary, if the
input data consists of a list of entropies of contiguous boundary regions, and
if the entropies satisfy a single inequality called Strong Subadditivity, then
we can construct a graph model for the bulk in linear time. Moreover, the bulk
graph is planar, it has $O(N^2)$ vertices (the information-theoretic minimum),
and it's ``universal,'' with only the edge weights depending on the specific
entropies in question. From a combinatorial perspective, our problem boils down
to an ``inverse'' of the famous min-cut problem: rather than being given a
graph and asked to find a min-cut, here we're given the values of min-cuts
separating various sets of vertices, and need to find a weighted undirected
graph consistent with those values. Our solution to this problem relies on the
notion of a ``bulkless'' graph, which might be of independent interest for
AdS/CFT. We also make initial progress on the case of multiple 1D boundaries --
where the boundaries could be connected via wormholes -- including an upper
bound of $O(N^4)$ vertices whenever a planar bulk graph exists (thus putting
the problem into the complexity class $\mathsf{NP}$).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.14982'>LinearCoFold and LinearCoPartition: Linear-Time Algorithms for Secondary Structure Prediction of Interacting RNA molecules</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: He Zhang, Sizhen Li, Liang Zhang, David H. Mathews, Liang Huang</p><p>Many ncRNAs function through RNA-RNA interactions. Fast and reliable RNA
structure prediction with consideration of RNA-RNA interaction is useful. Some
existing tools are less accurate due to omitting the competing of
intermolecular and intramolecular base pairs, or focus more on predicting the
binding region rather than predicting the complete secondary structure of two
interacting strands. Vienna RNAcofold, which reduces the problem into the
classical single sequence folding by concatenating two strands, scales in cubic
time against the combined sequence length, and is slow for long sequences. To
address these issues, we present LinearCoFold, which predicts the complete
minimum free energy structure of two strands in linear runtime, and
LinearCoPartition, which calculates the cofolding partition function and base
pairing probabilities in linear runtime. LinearCoFold and LinearCoPartition
follows the concatenation strategy of RNAcofold, but are orders of magnitude
faster than RNAcofold. For example, on a sequence pair with combined length of
26,190 nt, LinearCoFold is 86.8x faster than RNAcofold MFE mode (0.6 minutes
vs. 52.1 minutes), and LinearCoPartition is 642.3x faster than RNAcofold
partition function mode (1.8 minutes vs. 1156.2 minutes). Different from the
local algorithms, LinearCoFold and LinearCoPartition are global cofolding
algorithms without restriction on base pair length. Surprisingly, LinearCoFold
and LinearCoPartition's predictions have higher PPV and sensitivity of
intermolecular base pairs. Furthermore, we apply LinearCoFold to predict the
RNA-RNA interaction between SARS-CoV-2 gRNA and human U4 snRNA, which has been
experimentally studied, and observe that LinearCoFold's prediction correlates
better to the wet lab results.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_H/0/1/0/all/0/1">He Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_S/0/1/0/all/0/1">Sizhen Li</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_L/0/1/0/all/0/1">Liang Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Mathews_D/0/1/0/all/0/1">David H. Mathews</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Huang_L/0/1/0/all/0/1">Liang Huang</a></p><p>Many ncRNAs function through RNA-RNA interactions. Fast and reliable RNA
structure prediction with consideration of RNA-RNA interaction is useful. Some
existing tools are less accurate due to omitting the competing of
intermolecular and intramolecular base pairs, or focus more on predicting the
binding region rather than predicting the complete secondary structure of two
interacting strands. Vienna RNAcofold, which reduces the problem into the
classical single sequence folding by concatenating two strands, scales in cubic
time against the combined sequence length, and is slow for long sequences. To
address these issues, we present LinearCoFold, which predicts the complete
minimum free energy structure of two strands in linear runtime, and
LinearCoPartition, which calculates the cofolding partition function and base
pairing probabilities in linear runtime. LinearCoFold and LinearCoPartition
follows the concatenation strategy of RNAcofold, but are orders of magnitude
faster than RNAcofold. For example, on a sequence pair with combined length of
26,190 nt, LinearCoFold is 86.8x faster than RNAcofold MFE mode (0.6 minutes
vs. 52.1 minutes), and LinearCoPartition is 642.3x faster than RNAcofold
partition function mode (1.8 minutes vs. 1156.2 minutes). Different from the
local algorithms, LinearCoFold and LinearCoPartition are global cofolding
algorithms without restriction on base pair length. Surprisingly, LinearCoFold
and LinearCoPartition's predictions have higher PPV and sensitivity of
intermolecular base pairs. Furthermore, we apply LinearCoFold to predict the
RNA-RNA interaction between SARS-CoV-2 gRNA and human U4 snRNA, which has been
experimentally studied, and observe that LinearCoFold's prediction correlates
better to the wet lab results.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15014'>Counting Perfect Matchings in Dense Graphs Is Hard</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Nicolas El Maalouly, Yanheng Wang</p><p>We show that the problem of counting perfect matchings remains #P-complete
even if we restrict the input to very dense graphs, proving the conjecture in
[5]. Here "dense graphs" refer to bipartite graphs of bipartite independence
number $\leq 2$, or general graphs of independence number $\leq 2$. Our proof
is by reduction from counting perfect matchings in bipartite graphs, via
elementary linear algebra tricks and graph constructions.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Maalouly_N/0/1/0/all/0/1">Nicolas El Maalouly</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanheng Wang</a></p><p>We show that the problem of counting perfect matchings remains #P-complete
even if we restrict the input to very dense graphs, proving the conjecture in
[5]. Here "dense graphs" refer to bipartite graphs of bipartite independence
number $\leq 2$, or general graphs of independence number $\leq 2$. Our proof
is by reduction from counting perfect matchings in bipartite graphs, via
elementary linear algebra tricks and graph constructions.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15114'>Faster Linear Algebra for Distance Matrices</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Piotr Indyk, Sandeep Silwal</p><p>The distance matrix of a dataset $X$ of $n$ points with respect to a distance
function $f$ represents all pairwise distances between points in $X$ induced by
$f$. Due to their wide applicability, distance matrices and related families of
matrices have been the focus of many recent algorithmic works. We continue this
line of research and take a broad view of algorithm design for distance
matrices with the goal of designing fast algorithms, which are specifically
tailored for distance matrices, for fundamental linear algebraic primitives.
Our results include efficient algorithms for computing matrix-vector products
for a wide class of distance matrices, such as the $\ell_1$ metric for which we
get a linear runtime, as well as an $\Omega(n^2)$ lower bound for any algorithm
which computes a matrix-vector product for the $\ell_{\infty}$ case, showing a
separation between the $\ell_1$ and the $\ell_{\infty}$ metrics. Our upper
bound results, in conjunction with recent works on the matrix-vector query
model, have many further downstream applications, including the fastest
algorithm for computing a relative error low-rank approximation for the
distance matrix induced by $\ell_1$ and $\ell_2^2$ functions and the fastest
algorithm for computing an additive error low-rank approximation for the
$\ell_2$ metric, in addition to applications for fast matrix multiplication
among others. We also give algorithms for constructing distance matrices and
show that one can construct an approximate $\ell_2$ distance matrix in time
faster than the bound implied by the Johnson-Lindenstrauss lemma.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Indyk_P/0/1/0/all/0/1">Piotr Indyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Silwal_S/0/1/0/all/0/1">Sandeep Silwal</a></p><p>The distance matrix of a dataset $X$ of $n$ points with respect to a distance
function $f$ represents all pairwise distances between points in $X$ induced by
$f$. Due to their wide applicability, distance matrices and related families of
matrices have been the focus of many recent algorithmic works. We continue this
line of research and take a broad view of algorithm design for distance
matrices with the goal of designing fast algorithms, which are specifically
tailored for distance matrices, for fundamental linear algebraic primitives.
Our results include efficient algorithms for computing matrix-vector products
for a wide class of distance matrices, such as the $\ell_1$ metric for which we
get a linear runtime, as well as an $\Omega(n^2)$ lower bound for any algorithm
which computes a matrix-vector product for the $\ell_{\infty}$ case, showing a
separation between the $\ell_1$ and the $\ell_{\infty}$ metrics. Our upper
bound results, in conjunction with recent works on the matrix-vector query
model, have many further downstream applications, including the fastest
algorithm for computing a relative error low-rank approximation for the
distance matrix induced by $\ell_1$ and $\ell_2^2$ functions and the fastest
algorithm for computing an additive error low-rank approximation for the
$\ell_2$ metric, in addition to applications for fast matrix multiplication
among others. We also give algorithms for constructing distance matrices and
show that one can construct an approximate $\ell_2$ distance matrix in time
faster than the bound implied by the Johnson-Lindenstrauss lemma.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15178'>Anonymized Histograms in Intermediate Privacy Models</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi</p><p>We study the problem of privately computing the anonymized histogram (a.k.a.
unattributed histogram), which is defined as the histogram without item labels.
Previous works have provided algorithms with $\ell_1$- and $\ell_2^2$-errors of
$O_\varepsilon(\sqrt{n})$ in the central model of differential privacy (DP).
</p>
<p>In this work, we provide an algorithm with a nearly matching error guarantee
of $\tilde{O}_\varepsilon(\sqrt{n})$ in the shuffle DP and pan-private models.
Our algorithm is very simple: it just post-processes the discrete
Laplace-noised histogram! Using this algorithm as a subroutine, we show
applications in privately estimating symmetric properties of distributions such
as entropy, support coverage, and support size.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ghazi_B/0/1/0/all/0/1">Badih Ghazi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamath_P/0/1/0/all/0/1">Pritish Kamath</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1">Ravi Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Manurangsi_P/0/1/0/all/0/1">Pasin Manurangsi</a></p><p>We study the problem of privately computing the anonymized histogram (a.k.a.
unattributed histogram), which is defined as the histogram without item labels.
Previous works have provided algorithms with $\ell_1$- and $\ell_2^2$-errors of
$O_\varepsilon(\sqrt{n})$ in the central model of differential privacy (DP).
</p>
<p>In this work, we provide an algorithm with a nearly matching error guarantee
of $\tilde{O}_\varepsilon(\sqrt{n})$ in the shuffle DP and pan-private models.
Our algorithm is very simple: it just post-processes the discrete
Laplace-noised histogram! Using this algorithm as a subroutine, we show
applications in privately estimating symmetric properties of distributions such
as entropy, support coverage, and support size.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15193'>A framework of distributionally robust possibilistic optimization</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Romain Guillaume, Adam Kasperski, Pawel Zielinski</p><p>In this paper, an optimization problem with uncertain constraint coefficients
is considered. Possibility theory is used to model the uncertainty. Namely, a
joint possibility distribution in constraint coefficient realizations, called
scenarios, is specified. This possibility distribution induces a necessity
measure in scenario set, which in turn describes an ambiguity set of
probability distributions in scenario set. The distributionally robust approach
is then used to convert the imprecise constraints into deterministic
equivalents. Namely, the left-hand side of an imprecise constraint is evaluated
by using a risk measure with respect to the worst probability distribution that
can occur. In this paper, the Conditional Value at Risk is used as the risk
measure, which generalizes the strict robust and expected value approaches,
commonly used in literature. A general framework for solving such a class of
problems is described. Some cases which can be solved in polynomial time are
identified.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Guillaume_R/0/1/0/all/0/1">Romain Guillaume</a>, <a href="http://arxiv.org/find/math/1/au:+Kasperski_A/0/1/0/all/0/1">Adam Kasperski</a>, <a href="http://arxiv.org/find/math/1/au:+Zielinski_P/0/1/0/all/0/1">Pawel Zielinski</a></p><p>In this paper, an optimization problem with uncertain constraint coefficients
is considered. Possibility theory is used to model the uncertainty. Namely, a
joint possibility distribution in constraint coefficient realizations, called
scenarios, is specified. This possibility distribution induces a necessity
measure in scenario set, which in turn describes an ambiguity set of
probability distributions in scenario set. The distributionally robust approach
is then used to convert the imprecise constraints into deterministic
equivalents. Namely, the left-hand side of an imprecise constraint is evaluated
by using a risk measure with respect to the worst probability distribution that
can occur. In this paper, the Conditional Value at Risk is used as the risk
measure, which generalizes the strict robust and expected value approaches,
commonly used in literature. A general framework for solving such a class of
problems is described. Some cases which can be solved in polynomial time are
identified.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15421'>AnyDijkstra, an algorithm to compute shortest paths on images with anytime properties</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Diego Ulisse Pizzagalli, Rolf Krause</p><p>Images conveniently capture the result of physical processes, representing
rich source of information for data driven medicine, engineering, and science.
The modeling of an image as a graph allows the application of graph-based
algorithms for content analysis. Amongst these, one of the most used is the
Dijkstra Single Source Shortest Path algorithm (DSSSP), which computes the path
with minimal cost from one starting node to all the other nodes of the graph.
However, the results of DSSSP remains unknown for nodes until they are
explored. Moreover, DSSSP execution is associated to frequent jumps between
distant locations in the graph, which results in non-optimal memory access,
reduced parallelization, and finally increased execution time. Therefore, we
propose AnyDijkstra, an iterative implementation of the Dijkstra SSSP algorithm
optimized for images, that retains anytime properties while accessing memory
following a cache-friendly scheme and maximizing parallelization.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Pizzagalli_D/0/1/0/all/0/1">Diego Ulisse Pizzagalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Krause_R/0/1/0/all/0/1">Rolf Krause</a></p><p>Images conveniently capture the result of physical processes, representing
rich source of information for data driven medicine, engineering, and science.
The modeling of an image as a graph allows the application of graph-based
algorithms for content analysis. Amongst these, one of the most used is the
Dijkstra Single Source Shortest Path algorithm (DSSSP), which computes the path
with minimal cost from one starting node to all the other nodes of the graph.
However, the results of DSSSP remains unknown for nodes until they are
explored. Moreover, DSSSP execution is associated to frequent jumps between
distant locations in the graph, which results in non-optimal memory access,
reduced parallelization, and finally increased execution time. Therefore, we
propose AnyDijkstra, an iterative implementation of the Dijkstra SSSP algorithm
optimized for images, that retains anytime properties while accessing memory
following a cache-friendly scheme and maximizing parallelization.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15439'>Learning versus Refutation in Noninteractive Local Differential Privacy</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Alexander Edmonds, Aleksandar Nikolov, Toniann Pitassi</p><p>We study two basic statistical tasks in non-interactive local differential
privacy (LDP): learning and refutation. Learning requires finding a concept
that best fits an unknown target function (from labelled samples drawn from a
distribution), whereas refutation requires distinguishing between data
distributions that are well-correlated with some concept in the class, versus
distributions where the labels are random. Our main result is a complete
characterization of the sample complexity of agnostic PAC learning for
non-interactive LDP protocols. We show that the optimal sample complexity for
any concept class is captured by the approximate $\gamma_2$~norm of a natural
matrix associated with the class. Combined with previous work [Edmonds, Nikolov
and Ullman, 2019] this gives an equivalence between learning and refutation in
the agnostic setting.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/stat/1/au:+Edmonds_A/0/1/0/all/0/1">Alexander Edmonds</a>, <a href="http://arxiv.org/find/stat/1/au:+Nikolov_A/0/1/0/all/0/1">Aleksandar Nikolov</a>, <a href="http://arxiv.org/find/stat/1/au:+Pitassi_T/0/1/0/all/0/1">Toniann Pitassi</a></p><p>We study two basic statistical tasks in non-interactive local differential
privacy (LDP): learning and refutation. Learning requires finding a concept
that best fits an unknown target function (from labelled samples drawn from a
distribution), whereas refutation requires distinguishing between data
distributions that are well-correlated with some concept in the class, versus
distributions where the labels are random. Our main result is a complete
characterization of the sample complexity of agnostic PAC learning for
non-interactive LDP protocols. We show that the optimal sample complexity for
any concept class is captured by the approximate $\gamma_2$~norm of a natural
matrix associated with the class. Combined with previous work [Edmonds, Nikolov
and Ullman, 2019] this gives an equivalence between learning and refutation in
the agnostic setting.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.15630'>In-stream Probabilistic Cardinality Estimation for Bloom Filters</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Remy Scholler, Jean-Francois Couchot, Oumaima Alaoui-Ismaili, Denis Renaud, Eric Ballot</p><p>The amount of data coming from different sources such as IoT-sensors, social
networks, cellular networks, has increased exponentially during the last few
years. Probabilistic Data Structures (PDS) are efficient alternatives to
deterministic data structures suitable for large data processing and streaming
applications. They are mainly used for approximate membership queries,
frequency count, cardinality estimation and similarity research. Finding the
number of distinct elements in a large dataset or in streaming data is an
active research area. In this work, we show that usual methods based on Bloom
filters for this kind of cardinality estimation are relatively accurate on
average but have a high variance. Therefore, reducing this variance is
interesting to obtain accurate statistics. We propose a probabilistic approach
to estimate more accurately the cardinality of a Bloom filter based on its
parameters, i.e., number of hash functions $k$, size $m$, and a counter $s$
which is incremented whenever an element is not in the filter (i.e., when the
result of the membership query for this element is negative). The value of the
counter can never be larger than the exact cardinality due to the Bloom
filter's nature, but hash collisions can cause it to underestimate it. This
creates a counting error that we estimate accurately, in-stream, along with its
standard deviation. We also discuss a way to optimize the parameters of a Bloom
filter based on its counting error. We evaluate our approach with synthetic
data created from an analysis of a real mobility dataset provided by a mobile
network operator in the form of displacement matrices computed from mobile
phone records. The approach proposed here performs at least as well on average
and has a much lower variance (about 6 to 7 times less) than state of the art
methods.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Scholler_R/0/1/0/all/0/1">Remy Scholler</a>, <a href="http://arxiv.org/find/cs/1/au:+Couchot_J/0/1/0/all/0/1">Jean-Francois Couchot</a>, <a href="http://arxiv.org/find/cs/1/au:+Alaoui_Ismaili_O/0/1/0/all/0/1">Oumaima Alaoui-Ismaili</a>, <a href="http://arxiv.org/find/cs/1/au:+Renaud_D/0/1/0/all/0/1">Denis Renaud</a>, <a href="http://arxiv.org/find/cs/1/au:+Ballot_E/0/1/0/all/0/1">Eric Ballot</a></p><p>The amount of data coming from different sources such as IoT-sensors, social
networks, cellular networks, has increased exponentially during the last few
years. Probabilistic Data Structures (PDS) are efficient alternatives to
deterministic data structures suitable for large data processing and streaming
applications. They are mainly used for approximate membership queries,
frequency count, cardinality estimation and similarity research. Finding the
number of distinct elements in a large dataset or in streaming data is an
active research area. In this work, we show that usual methods based on Bloom
filters for this kind of cardinality estimation are relatively accurate on
average but have a high variance. Therefore, reducing this variance is
interesting to obtain accurate statistics. We propose a probabilistic approach
to estimate more accurately the cardinality of a Bloom filter based on its
parameters, i.e., number of hash functions $k$, size $m$, and a counter $s$
which is incremented whenever an element is not in the filter (i.e., when the
result of the membership query for this element is negative). The value of the
counter can never be larger than the exact cardinality due to the Bloom
filter's nature, but hash collisions can cause it to underestimate it. This
creates a counting error that we estimate accurately, in-stream, along with its
standard deviation. We also discuss a way to optimize the parameters of a Bloom
filter based on its counting error. We evaluate our approach with synthetic
data created from an analysis of a real mobility dataset provided by a mobile
network operator in the form of displacement matrices computed from mobile
phone records. The approach proposed here performs at least as well on average
and has a much lower variance (about 6 to 7 times less) than state of the art
methods.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-28T00:30:00Z">Friday, October 28 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Thursday, October 27
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/10/27/postdoctoral-fellowship-the-institute-for-emerging-core-methods-in-data-science-encore-at-university-of-california-san-diego-apply-by-december-15-2022/'>Postdoctoral fellowship – The Institute for Emerging CORE Methods in Data Science (EnCORE) at University of California – San Diego (apply by December 15, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Multiple postdoctoral fellowship opportunities are available with The Institute for Emerging CORE Methods in Data Science (EnCORE), a TRIPODS Phase II institute funded by the National Science Foundation. The EnCORE Institute is a collaboration of researchers between UC San Diego, UCLA, UT Austin and Penn. Website: academicjobsonline.org/ajo/jobs/23469 Email: bsaha@eng.ucsd.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Multiple postdoctoral fellowship opportunities are available with The Institute for Emerging CORE Methods in Data Science (EnCORE), a TRIPODS Phase II institute funded by the National Science Foundation. The EnCORE Institute is a collaboration of researchers between UC San Diego, UCLA, UT Austin and Penn.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/23469">https://academicjobsonline.org/ajo/jobs/23469</a><br />
Email: bsaha@eng.ucsd.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T23:19:32Z">Thursday, October 27 2022, 23:19</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/10/27/postdoc-at-sandia-national-labs-apply-by-december-20-2022/'>postdoc at Sandia National Labs (apply by December 20, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Sandia Labs is seeking a postdoc to work on quantum or quantum-inspired classical approximation, sublinear, or streaming algorithms, as part of a DOE-funded collaboration among several national labs and universities. We encourage theoretical computer scientists interested in quantum information but without prior expertise to apply! Website: far-qc.sandia.gov/job-opportunities/ Email: odparek@sandia.gov
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Sandia Labs is seeking a postdoc to work on quantum or quantum-inspired classical approximation, sublinear, or streaming algorithms, as part of a DOE-funded collaboration among several national labs and universities. We encourage theoretical computer scientists interested in quantum information but without prior expertise to apply!</p>
<p>Website: <a href="https://far-qc.sandia.gov/job-opportunities/">https://far-qc.sandia.gov/job-opportunities/</a><br />
Email: odparek@sandia.gov</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T20:38:30Z">Thursday, October 27 2022, 20:38</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://blog.computationalcomplexity.org/2022/10/the-media-coverage-of-matrix-result-is.html'>The Media Coverage of the Matrix result is Terrible (though not worse than usual)</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>&nbsp;BILL: A computer program (or an AI or an ML or whatever) found a BETTER way to do matrix mult! Its in the same spirit as Strassen. I've always wondered if Strassen was practical&nbsp; since it is simple, and computers have come a long way since 1969, though I suspect not (I WAS WRONG ABOUT THAT). I'll blog about and ask if Strassen will ever be used/practical&nbsp; &nbsp;(I did that post&nbsp;here).</p><p>READERS: Uh, Bill,&nbsp; (1) Strassen IS used and practical and (2) the new algorithm only works in&nbsp; GF(2). (Lance did a post about the new algorithm where he makes this explicit&nbsp;here.) Some readers claimed it was GF(2^k) and some that it was fields if char 2. In any case NO it is not a general algorithm.</p><p>BILL: There is good news and what others might consider bad news but I do not.</p><p>GOOD NEWS: I learned that Strassen IS practical and used, which I did not know.&nbsp;</p><p>GOOD NEWS: I learned that I was WRONG about the new algorithm since I just assumed it worked in general, and updated the post so others would not be deceived.&nbsp;</p><p>BAD NEWS: Darling asked if I was embarrassed to be wrong. If I am embarrassed that easily I would have quit blogging in 2009.&nbsp;</p><p>DARLING: So Bill, how did you get it so wrong?</p><p>BILL: Well obviously my bad for not doing my due diligence. But that's not what's interesting. What's interesting is that if you read the articles about it for the popular press you would have NO IDEA that it only works for mod 2. Its like reading that quantum computing will solve world hunger.</p><p>DARLING: It won't?</p><p>BILL: No it won't.&nbsp;</p><p>DARLING: I was being sarcastic.&nbsp;</p><p>BILL: Anyway, the coverage pushed two points</p><p>a) IMPRESSIVE that a computer could FIND these things that humans could not. This is TRUE (gee, how do I know that? The Gell-Mann Effect,&nbsp; is that people disgusted when they read a newspaper article on something they know about and find the mistakes later assume that the other articles are fine. SHOUT OUT to Jim Hefferon who telling me the name Gell-Mann Effect and left a comment with a pointer. The original version of this post had a BLANK there.)&nbsp;</p><p>b) The algorithm is practical! They did not quite say that but it was implied. And certainly there was NO mention of it only working in GF(2). And I was fooled into thinking that it might be competitive with Strassen.&nbsp;</p><p>READERS (of this blog entry, I predict) Uh, Bill, the popular press getting science news wrong and saying its more practical than it is probably predates the Bible. I can imagine&nbsp; the Cairo Times in 2000BC writing&nbsp;`Scientists discover that in any right triangle with sides a,b,c&nbsp; a^2+b^2=c^2 and this will enable us to build food silos and cure Hunger. In reality they knew that the 3,4,5 triangle was a right triangle, were no where near a proof of a general theorem, and I doubt it would have helped cure hunger.&nbsp;</p><p>BILL: This time the news was REALLY CLOSE to what I do (if&nbsp; R(5) is found by a computer and the media claims its practical I'll either have a very angry blog or repost my April Fools' day article on Ramsey Theory's application to&nbsp; History) AND I posted incorrectly about it. So, to quote many a bad movie</p><p>THIS TIME ITS PERSONAL!</p><p>By gasarch</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>&nbsp;BILL: A computer program (or an AI or an ML or whatever) found a BETTER way to do matrix mult! Its in the same spirit as Strassen. I've always wondered if Strassen was practical&nbsp; since it is simple, and computers have come a long way since 1969, though I suspect not (I WAS WRONG ABOUT THAT). I'll blog about and ask if Strassen will ever be used/practical&nbsp; &nbsp;(I did that post&nbsp;<a href="https://blog.computationalcomplexity.org/2022/10/will-strassens-matrix-mult-alg-ever-be.html">here</a>).</p><p>READERS: Uh, Bill,&nbsp; (1) Strassen IS used and practical and (2) the new algorithm only works in&nbsp; GF(2). (Lance did a post about the new algorithm where he makes this explicit&nbsp;<a href="https://blog.computationalcomplexity.org/2022/10/alpha-tensor.html">here</a>.) Some readers claimed it was GF(2^k) and some that it was fields if char 2. In any case NO it is not a general algorithm.</p><p>BILL: There is good news and what others might consider bad news but I do not.</p><p>GOOD NEWS: I learned that Strassen IS practical and used, which I did not know.&nbsp;</p><p>GOOD NEWS: I learned that I was WRONG about the new algorithm since I just assumed it worked in general, and updated the post so others would not be deceived.&nbsp;</p><p>BAD NEWS: Darling asked if I was embarrassed to be wrong. If I am embarrassed that easily I would have quit blogging in 2009.&nbsp;</p><p>DARLING: So Bill, how did you get it so wrong?</p><p>BILL: Well obviously my bad for not doing my due diligence. But that's not what's interesting. What's interesting is that if you read the articles about it for the popular press you would have NO IDEA that it only works for mod 2. Its like reading that quantum computing will solve world hunger.</p><p>DARLING: It won't?</p><p>BILL: No it won't.&nbsp;</p><p>DARLING: I was being sarcastic.&nbsp;</p><p>BILL: Anyway, the coverage pushed two points</p><p>a) IMPRESSIVE that a computer could FIND these things that humans could not. This is TRUE (gee, how do I know that? <i>The Gell-Mann Effec</i>t,&nbsp; is that people disgusted when they read a newspaper article on something they know about and find the mistakes later assume that the other articles are fine. SHOUT OUT to Jim Hefferon who telling me the name <i>Gell-Mann Effect</i> and left a comment with a pointer. The original version of this post had a BLANK there.)&nbsp;</p><p>b) The algorithm is practical! They did not quite say that but it was implied. And certainly there was NO mention of it only working in GF(2). And I was fooled into thinking that it might be competitive with Strassen.&nbsp;</p><p>READERS (of this blog entry, I predict) Uh, Bill, the popular press getting science news wrong and saying its more practical than it is probably predates the Bible. I can imagine&nbsp; the Cairo Times in 2000BC writing&nbsp;<i>`Scientists discover that in any right triangle with sides a,b,c&nbsp; a^2+b^2=c^2 and this will</i> <i>enable us to build food silos and cure Hunger</i>. In reality they knew that the 3,4,5 triangle was a right triangle, were no where near a proof of a general theorem, and I doubt it would have helped cure hunger.&nbsp;</p><p>BILL: This time the news was REALLY CLOSE to what I do (if&nbsp; R(5) is found by a computer and the media claims its practical I'll either have a very angry blog or repost my April Fools' day article on Ramsey Theory's application to&nbsp; History) AND I posted incorrectly about it. So, to quote many a bad movie</p><p><b>THIS TIME ITS PERSONAL!</b></p><p class="authors">By gasarch</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T11:46:00Z">Thursday, October 27 2022, 11:46</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.14259'>Net Separation-Oriented Printed Circuit Board Placement via Margin Maximization</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Chung-Kuan Cheng, Chia-Tung Ho, Chester Holtz</p><p>Packaging has become a crucial process due to the paradigm shift of More than
Moore. Addressing manufacturing and yield issues is a significant challenge for
modern layout algorithms.
</p>
<p>We propose to use printed circuit board (PCB) placement as a benchmark for
the packaging problem. A maximum-margin formulation is devised to improve the
separation between nets. Our framework includes seed layout proposals, a
coordinate descent-based procedure to optimize routability, and a mixed-integer
linear programming method to legalize the layout. We perform an extensive study
with 14 PCB designs and an open-source router. We show that the placements
produced by NS-place improve routed wirelength by up to 25\%, reduce the number
of vias by up to 50\%, and reduce the number of DRVs by 79\% compared to manual
and wirelength-minimal placements.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1">Chung-Kuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1">Chia-Tung Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Holtz_C/0/1/0/all/0/1">Chester Holtz</a></p><p>Packaging has become a crucial process due to the paradigm shift of More than
Moore. Addressing manufacturing and yield issues is a significant challenge for
modern layout algorithms.
</p>
<p>We propose to use printed circuit board (PCB) placement as a benchmark for
the packaging problem. A maximum-margin formulation is devised to improve the
separation between nets. Our framework includes seed layout proposals, a
coordinate descent-based procedure to optimize routability, and a mixed-integer
linear programming method to legalize the layout. We perform an extensive study
with 14 PCB designs and an open-source router. We show that the placements
produced by NS-place improve routed wirelength by up to 25\%, reduce the number
of vias by up to 50\%, and reduce the number of DRVs by 79\% compared to manual
and wirelength-minimal placements.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T00:30:00Z">Thursday, October 27 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.14736'>An Optimal Lower Bound for Simplex Range Reporting</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Peyman Afshani, Pingan Cheng</p><p>We give a simplified and improved lower bound for the simplex range reporting
problem. We show that given a set $P$ of $n$ points in $\mathbb{R}^d$, any data
structure that uses $S(n)$ space to answer such queries must have
$Q(n)=\Omega((n^2/S(n))^{(d-1)/d}+k)$ query time, where $k$ is the output size.
For near-linear space data structures, i.e., $S(n)=O(n\log^{O(1)}n)$, this
improves the previous lower bounds by Chazelle and Rosenberg [CR96] and Afshani
[A12] but perhaps more importantly, it is the first ever tight lower bound for
any variant of simplex range searching for $d\ge 3$ dimensions.
</p>
<p>We obtain our lower bound by making a simple connection to well-studied
problems in incident geometry which allows us to use known constructions in the
area. We observe that a small modification of a simple already existing
construction can lead to our lower bound. We believe that our proof is
accessible to a much wider audience, at least compared to the previous
intricate probabilistic proofs based on measure arguments by Chazelle and
Rosenberg [CR96] and Afshani [A12].
</p>
<p>The lack of tight or almost-tight (up to polylogarithmic factor) lower bounds
for near-linear space data structures is a major bottleneck in making progress
on problems such as proving lower bounds for multilevel data structures. It is
our hope that this new line of attack based on incidence geometry can lead to
further progress in this area.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Afshani_P/0/1/0/all/0/1">Peyman Afshani</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1">Pingan Cheng</a></p><p>We give a simplified and improved lower bound for the simplex range reporting
problem. We show that given a set $P$ of $n$ points in $\mathbb{R}^d$, any data
structure that uses $S(n)$ space to answer such queries must have
$Q(n)=\Omega((n^2/S(n))^{(d-1)/d}+k)$ query time, where $k$ is the output size.
For near-linear space data structures, i.e., $S(n)=O(n\log^{O(1)}n)$, this
improves the previous lower bounds by Chazelle and Rosenberg [CR96] and Afshani
[A12] but perhaps more importantly, it is the first ever tight lower bound for
any variant of simplex range searching for $d\ge 3$ dimensions.
</p>
<p>We obtain our lower bound by making a simple connection to well-studied
problems in incident geometry which allows us to use known constructions in the
area. We observe that a small modification of a simple already existing
construction can lead to our lower bound. We believe that our proof is
accessible to a much wider audience, at least compared to the previous
intricate probabilistic proofs based on measure arguments by Chazelle and
Rosenberg [CR96] and Afshani [A12].
</p>
<p>The lack of tight or almost-tight (up to polylogarithmic factor) lower bounds
for near-linear space data structures is a major bottleneck in making progress
on problems such as proving lower bounds for multilevel data structures. It is
our hope that this new line of attack based on incidence geometry can lead to
further progress in this area.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T00:30:00Z">Thursday, October 27 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.14664'>Coresets for Vertical Federated Learning: Regularized Linear Regression and $K$-Means Clustering</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Lingxiao Huang, Zhize Li, Jialin Sun, Haoyu Zhao</p><p>Vertical federated learning (VFL), where data features are stored in multiple
parties distributively, is an important area in machine learning. However, the
communication complexity for VFL is typically very high. In this paper, we
propose a unified framework by constructing coresets in a distributed fashion
for communication-efficient VFL. We study two important learning tasks in the
VFL setting: regularized linear regression and $k$-means clustering, and apply
our coreset framework to both problems. We theoretically show that using
coresets can drastically alleviate the communication complexity, while nearly
maintain the solution quality. Numerical experiments are conducted to
corroborate our theoretical findings.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lingxiao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhize Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jialin Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Haoyu Zhao</a></p><p>Vertical federated learning (VFL), where data features are stored in multiple
parties distributively, is an important area in machine learning. However, the
communication complexity for VFL is typically very high. In this paper, we
propose a unified framework by constructing coresets in a distributed fashion
for communication-efficient VFL. We study two important learning tasks in the
VFL setting: regularized linear regression and $k$-means clustering, and apply
our coreset framework to both problems. We theoretically show that using
coresets can drastically alleviate the communication complexity, while nearly
maintain the solution quality. Numerical experiments are conducted to
corroborate our theoretical findings.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T00:30:00Z">Thursday, October 27 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.14315'>Streaming Submodular Maximization with Differential Privacy</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Anamay Chaturvedi, Huy L&#xea; Nguyen, Thy Nguyen</p><p>In this work, we study the problem of privately maximizing a submodular
function in the streaming setting. Extensive work has been done on privately
maximizing submodular functions in the general case when the function depends
upon the private data of individuals. However, when the size of the data stream
drawn from the domain of the objective function is large or arrives very fast,
one must privately optimize the objective within the constraints of the
streaming setting. We establish fundamental differentially private baselines
for this problem and then derive better trade-offs between privacy and utility
for the special case of decomposable submodular functions. A submodular
function is decomposable when it can be written as a sum of submodular
functions; this structure arises naturally when each summand function models
the utility of an individual and the goal is to study the total utility of the
whole population as in the well-known Combinatorial Public Projects Problem.
Finally, we complement our theoretical analysis with experimental
corroboration.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chaturvedi_A/0/1/0/all/0/1">Anamay Chaturvedi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Huy L&#xea; Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thy Nguyen</a></p><p>In this work, we study the problem of privately maximizing a submodular
function in the streaming setting. Extensive work has been done on privately
maximizing submodular functions in the general case when the function depends
upon the private data of individuals. However, when the size of the data stream
drawn from the domain of the objective function is large or arrives very fast,
one must privately optimize the objective within the constraints of the
streaming setting. We establish fundamental differentially private baselines
for this problem and then derive better trade-offs between privacy and utility
for the special case of decomposable submodular functions. A submodular
function is decomposable when it can be written as a sum of submodular
functions; this structure arises naturally when each summand function models
the utility of an individual and the goal is to study the total utility of the
whole population as in the well-known Combinatorial Public Projects Problem.
Finally, we complement our theoretical analysis with experimental
corroboration.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T00:30:00Z">Thursday, October 27 2022, 00:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2210.14582'>WebCrack: Dynamic Dictionary Adjustment for Web Weak Password Detection based on Blasting Response Event Discrimination</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xiang Long, Yan Huang, Zhendong Liu, Lansheng Han, Haili Sun, Jingyuan He</p><p>The feature diversity of different web systems in page elements, submission
contents and return information makes it difficult to detect weak password
automatically. To solve this problem, multi-factor correlation detection method
as integrated in the DBKER algorithm is proposed to achieve automatic detection
of web weak passwords and universal passwords. It generates password
dictionaries based on PCFG algorithm, proposes to judge blasting result via 4
steps with traditional static keyword features and dynamic page feature
information. Then the blasting failure events are discriminated and the
usernames are blasted based on response time. Thereafter the weak password
dictionary is dynamically adjusted according to the hints provided by the
response failure page. Based on the algorithm, this paper implements a
detection system named WebCrack. Experimental results of two blasting tests on
DedeCMS and Discuz! systems as well as a random backend test show that the
proposed method can detect weak passwords and universal passwords of various
web systems with an average accuracy rate of about 93.75%, providing security
advisories for users' password settings with strong practicability.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1">Xiang Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhendong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1">Lansheng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Haili Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jingyuan He</a></p><p>The feature diversity of different web systems in page elements, submission
contents and return information makes it difficult to detect weak password
automatically. To solve this problem, multi-factor correlation detection method
as integrated in the DBKER algorithm is proposed to achieve automatic detection
of web weak passwords and universal passwords. It generates password
dictionaries based on PCFG algorithm, proposes to judge blasting result via 4
steps with traditional static keyword features and dynamic page feature
information. Then the blasting failure events are discriminated and the
usernames are blasted based on response time. Thereafter the weak password
dictionary is dynamically adjusted according to the hints provided by the
response failure page. Based on the algorithm, this paper implements a
detection system named WebCrack. Experimental results of two blasting tests on
DedeCMS and Discuz! systems as well as a random backend test show that the
proposed method can detect weak passwords and universal passwords of various
web systems with an average accuracy rate of about 93.75%, providing security
advisories for users' password settings with strong practicability.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-10-27T00:30:00Z">Thursday, October 27 2022, 00:30</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
