<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0RQ5M78VX5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0RQ5M78VX5');
  </script>

  <meta charset='utf-8'>
  <meta name='generator' content='Pluto 1.6.2 on Ruby 3.0.4 (2022-04-12) [x86_64-linux]'>

  <title>Theory of Computing Report</title>

  <link rel="alternate" type="application/rss+xml" title="Posts (RSS)" href="rss20.xml" />
  <link rel="alternate" type="application/atom+xml" title="Posts (Atom)" href="atom.xml" />
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/solid.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/fontawesome.min.css">
  <link rel='stylesheet' type='text/css' href='css/theory.css'>
</head>
<body>
  <details class="tr-panel" open>
    <summary>
      <span>Last Update</span>
      <div class="tr-small">
        
          <time class='timeago' datetime="2022-11-17T14:33:59Z">Thursday, November 17 2022, 14:33</time>
        
      </div>
      <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
    </summary>
    <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

    <ul class='tr-subscriptions tr-small' >
    
      <li>
        <a href='http://arxiv.org/rss/cs.CC'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.CG'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a>
      </li>
    
      <li>
        <a href='http://arxiv.org/rss/cs.DS'><img src='icon/feed.png'></a>
        <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a>
      </li>
    
      <li>
        <a href='http://aaronsadventures.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://aaronsadventures.blogspot.com/'>Aaron Roth</a>
      </li>
    
      <li>
        <a href='https://adamsheffer.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamsheffer.wordpress.com'>Adam Sheffer</a>
      </li>
    
      <li>
        <a href='https://adamdsmith.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://adamdsmith.wordpress.com'>Adam Smith</a>
      </li>
    
      <li>
        <a href='https://polylogblog.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://polylogblog.wordpress.com'>Andrew McGregor</a>
      </li>
    
      <li>
        <a href='https://corner.mimuw.edu.pl/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://corner.mimuw.edu.pl'>Banach's Algorithmic Corner</a>
      </li>
    
      <li>
        <a href='http://www.argmin.net/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://benjamin-recht.github.io/'>Ben Recht</a>
      </li>
    
      <li>
        <a href='http://bit-player.org/feed/atom/'><img src='icon/feed.png'></a>
        <a href='http://bit-player.org'>bit-player</a>
      </li>
    
      <li>
        <a href='https://cstheory-jobs.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-jobs.org'>CCI: jobs</a>
      </li>
    
      <li>
        <a href='https://cstheory-events.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://cstheory-events.org'>CS Theory Events</a>
      </li>
    
      <li>
        <a href='http://blog.computationalcomplexity.org/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a>
      </li>
    
      <li>
        <a href='https://11011110.github.io/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://11011110.github.io/blog/'>David Eppstein</a>
      </li>
    
      <li>
        <a href='https://daveagp.wordpress.com/category/toc/feed/'><img src='icon/feed.png'></a>
        <a href='https://daveagp.wordpress.com'>David Pritchard</a>
      </li>
    
      <li>
        <a href='https://decentdescent.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://decentdescent.org/'>Decent Descent</a>
      </li>
    
      <li>
        <a href='https://decentralizedthoughts.github.io/feed'><img src='icon/feed.png'></a>
        <a href='https://decentralizedthoughts.github.io'>Decentralized Thoughts</a>
      </li>
    
      <li>
        <a href='https://differentialprivacy.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://differentialprivacy.org'>DifferentialPrivacy.org</a>
      </li>
    
      <li>
        <a href='https://eccc.weizmann.ac.il//feeds/reports/'><img src='icon/feed.png'></a>
        <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a>
      </li>
    
      <li>
        <a href='https://emanueleviola.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://emanueleviola.wordpress.com'>Emanuele Viola</a>
      </li>
    
      <li>
        <a href='https://3dpancakes.typepad.com/ernie/atom.xml'><img src='icon/feed.png'></a>
        <a href='https://3dpancakes.typepad.com/ernie/'>Ernie's 3D Pancakes</a>
      </li>
    
      <li>
        <a href='https://dstheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://dstheory.wordpress.com'>Foundation of Data Science - Virtual Talk Series</a>
      </li>
    
      <li>
        <a href='https://francisbach.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://francisbach.com'>Francis Bach</a>
      </li>
    
      <li>
        <a href='https://gilkalai.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://gilkalai.wordpress.com'>Gil Kalai</a>
      </li>
    
      <li>
        <a href='https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.oregonstate.edu/glencora'>Glencora Borradaile</a>
      </li>
    
      <li>
        <a href='https://research.googleblog.com/feeds/posts/default/-/Algorithms'><img src='icon/feed.png'></a>
        <a href='https://research.googleblog.com/search/label/Algorithms'>Google Research Blog: Algorithms</a>
      </li>
    
      <li>
        <a href='https://gradientscience.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='https://gradientscience.org/'>Gradient Science</a>
      </li>
    
      <li>
        <a href='http://grigory.us/blog/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://grigory.github.io/blog'>Grigory Yaroslavtsev</a>
      </li>
    
      <li>
        <a href='https://tcsmath.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsmath.wordpress.com'>James R. Lee</a>
      </li>
    
      <li>
        <a href='https://kamathematics.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://kamathematics.wordpress.com'>Kamathematics</a>
      </li>
    
      <li>
        <a href='http://processalgebra.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://processalgebra.blogspot.com/'>Luca Aceto</a>
      </li>
    
      <li>
        <a href='https://lucatrevisan.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://lucatrevisan.wordpress.com'>Luca Trevisan</a>
      </li>
    
      <li>
        <a href='https://mittheory.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mittheory.wordpress.com'>MIT CSAIL Student Blog</a>
      </li>
    
      <li>
        <a href='http://mybiasedcoin.blogspot.com/feeds/posts/default'><img src='icon/feed.png'></a>
        <a href='http://mybiasedcoin.blogspot.com/'>Michael Mitzenmacher</a>
      </li>
    
      <li>
        <a href='http://blog.mrtz.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://blog.mrtz.org/'>Moritz Hardt</a>
      </li>
    
      <li>
        <a href='http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://mysliceofpizza.blogspot.com/search/label/aggregator'>Muthu Muthukrishnan</a>
      </li>
    
      <li>
        <a href='https://nisheethvishnoi.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://nisheethvishnoi.wordpress.com'>Nisheeth Vishnoi</a>
      </li>
    
      <li>
        <a href='http://www.solipsistslog.com/feed/'><img src='icon/feed.png'></a>
        <a href='http://www.solipsistslog.com'>Noah Stephens-Davidowitz</a>
      </li>
    
      <li>
        <a href='http://www.offconvex.org/feed.xml'><img src='icon/feed.png'></a>
        <a href='http://offconvex.github.io/'>Off the Convex Path</a>
      </li>
    
      <li>
        <a href='http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator'><img src='icon/feed.png'></a>
        <a href='http://paulwgoldberg.blogspot.com/search/label/aggregator'>Paul Goldberg</a>
      </li>
    
      <li>
        <a href='https://ptreview.sublinear.info/?feed=rss2'><img src='icon/feed.png'></a>
        <a href='https://ptreview.sublinear.info'>Property Testing Review</a>
      </li>
    
      <li>
        <a href='https://rjlipton.wpcomstaging.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a>
      </li>
    
      <li>
        <a href='https://blogs.princeton.edu/imabandit/feed/'><img src='icon/feed.png'></a>
        <a href='https://blogs.princeton.edu/imabandit'>Sébastien Bubeck</a>
      </li>
    
      <li>
        <a href='https://scottaaronson.blog/?feed=atom'><img src='icon/feed.png'></a>
        <a href='https://scottaaronson.blog'>Scott Aaronson</a>
      </li>
    
      <li>
        <a href='https://blog.simons.berkeley.edu/feed/'><img src='icon/feed.png'></a>
        <a href='https://blog.simons.berkeley.edu'>Simons Institute Blog</a>
      </li>
    
      <li>
        <a href='https://tcsplus.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://tcsplus.wordpress.com'>TCS+ Seminar Series</a>
      </li>
    
      <li>
        <a href='https://toc4fairness.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://toc4fairness.org'>TOC for Fairness</a>
      </li>
    
      <li>
        <a href='http://www.blogger.com/feeds/6555947/posts/default?alt=atom'><img src='icon/feed.png'></a>
        <a href='http://blog.geomblog.org/'>The Geomblog</a>
      </li>
    
      <li>
        <a href='https://www.let-all.com/blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://www.let-all.com/blog'>The Learning Theory Alliance Blog</a>
      </li>
    
      <li>
        <a href='https://theorydish.blog/feed/'><img src='icon/feed.png'></a>
        <a href='https://theorydish.blog'>Theory Dish: Stanford Blog</a>
      </li>
    
      <li>
        <a href='https://thmatters.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://thmatters.wordpress.com'>Theory Matters</a>
      </li>
    
      <li>
        <a href='https://mycqstate.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://mycqstate.wordpress.com'>Thomas Vidick</a>
      </li>
    
      <li>
        <a href='https://agtb.wordpress.com/feed/'><img src='icon/feed.png'></a>
        <a href='https://agtb.wordpress.com'>Turing's Invisible Hand</a>
      </li>
    
      <li>
        <a href='https://windowsontheory.org/feed/'><img src='icon/feed.png'></a>
        <a href='https://windowsontheory.org'>Windows on Theory</a>
      </li>
    
    </ul>

    <p class='tr-small'><a href="opml.xml">OPML feed</a> of all feeds.</p>
    <p class='tr-small'>Subscribe to the <a href="atom.xml">Atom feed</a>, <a href="rss20.xml">RSS feed</a>, or follow on <a href="https://twitter.com/cstheory">Twitter</a>, to stay up to date.</p>
    <p class='tr-small'>Source on <a href="https://github.com/nimaanari/theory.report">GitHub</a>.</p>
    <p class='tr-small'>Maintained by Nima Anari, Arnab Bhattacharyya, Gautam Kamath.</p>
    <p class='tr-small'>Powered by <a href='https://github.com/feedreader'>Pluto</a>.</p>
  </details>

  <div class="tr-opts">
    <i id='tr-show-headlines' class="fa-solid fa-fw fa-window-minimize tr-button" title='Show Headlines Only'></i>
    <i id='tr-show-snippets' class="fa-solid fa-fw fa-compress tr-button" title='Show Snippets'></i>
    <i id='tr-show-fulltext' class="fa-solid fa-fw fa-expand tr-button" title='Show Full Text'></i>
  </div>

  <h1>Theory of Computing Report</h1>

  <div class="tr-articles tr-shrink">
    
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Thursday, November 17
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/17/senior-faculty-position-at-williams-college-apply-by-december-1-2022/'>Senior Faculty Position at Williams College (apply by December 1, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The Department of Computer Science at Williams College invites applications for a tenured faculty position at the associate or full professor level beginning July 1, 2023. We welcome candidates from all areas of computer science who can contribute to the vibrancy of our academic community through their research, teaching, and service. Website: apply.interfolio.com/111662 Email: cshiring@williams.edu
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The Department of Computer Science at Williams College invites applications for a tenured faculty position at the associate or full professor level beginning July 1, 2023. We welcome candidates from all areas of computer science who can contribute to the vibrancy of our academic community through their research, teaching, and service.</p>
<p>Website: <a href="https://apply.interfolio.com/111662">https://apply.interfolio.com/111662</a><br />
Email: cshiring@williams.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T14:11:06Z">Thursday, November 17 2022, 14:11</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/17/postdocs-at-max-planck-institute-for-informatics-apply-by-december-31-2022/'>Postdocs at Max Planck Institute for Informatics (apply by December 31, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We are looking for applicants from all areas of algorithms and complexity, including related areas like mathematical optimization, distributed computing, and algorithms engineering. Postdoctoral fellowships are available at the algorithms and complexity department for two years through the Guest Program of our institute. Website: www.mpi-inf.mpg.de/d1postdoc Email: d1office@mpi-inf.mpg.de
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>We are looking for applicants from all areas of algorithms and complexity, including related areas like mathematical optimization, distributed computing, and algorithms engineering. Postdoctoral fellowships are available at the algorithms and complexity department for two years through the Guest Program of our institute.</p>
<p>Website: <a href="http://www.mpi-inf.mpg.de/d1postdoc">http://www.mpi-inf.mpg.de/d1postdoc</a><br />
Email: d1office@mpi-inf.mpg.de</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T13:29:17Z">Thursday, November 17 2022, 13:29</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/17/postdoc-position-in-algorithms-at-university-of-warwick-apply-by-december-6-2022/'>Postdoc position in algorithms at University of Warwick (apply by December 6, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          In connection with a research grant of Dr. Ramanujan Sridharan and Prof. Graham Cormode at University of Warwick, UK, we are seeking excellent candidates for a postdoctoral fellow position in the area of design and analysis of parameterized and approximation algorithms. The position is for 18 months and start date can be negotiated (preferably by [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>In connection with a research grant of Dr. Ramanujan Sridharan and Prof. Graham Cormode at University of Warwick, UK, we are seeking excellent candidates for a postdoctoral fellow position in the area of design and analysis of parameterized and approximation algorithms.<br />
The position is for 18 months and start date can be negotiated (preferably by March 2023).</p>
<p>Website: <a href="https://tinyurl.com/ksz8rjfa">https://tinyurl.com/ksz8rjfa</a><br />
Email: r.maadapuzhi-sridharan@warwick.ac.uk</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T08:54:03Z">Thursday, November 17 2022, 08:54</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://scottaaronson.blog/?p=6813'>Sneerers</a></h3>
        <p class='tr-article-feed'>from <a href='https://scottaaronson.blog'>Scott Aaronson</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          In the past few weeks, I&#8217;ve learned two ways to think about online sneerers that have been helping me tremendously, and that I wanted to share in case they&#8217;re helpful to others: First, they&#8217;re like a train in a movie that&#8217;s barreling directly towards the camera. If you haven&#8217;t yet internalized how the medium works, [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>In the past few weeks, I&#8217;ve learned two ways to think about online sneerers that have been helping me tremendously, and that I wanted to share in case they&#8217;re helpful to others:</p>



<p>First, they&#8217;re like a train in a movie that&#8217;s barreling directly towards the camera. If you haven&#8217;t yet internalized how the medium works, absolutely terrifying! Run from the theater! If you <em>have</em> internalized it, though, you can sit and watch without even flinching.</p>



<p>Second, the sneerers are like alligators&#8212;and about as likely to be moved by your appeals to reason and empathy. But if, like me, you&#8217;re lucky enough to have a loving family, friends, colleagues, and a nigh-uncancellable career, then it&#8217;s as though you&#8217;re standing on a bridge high above, looking down at the gators as they snap their jaws at you uselessly. There&#8217;s <em>really</em> no moral or intellectual obligation to go down to the swamp to wrestle them.  If they mean to attack you, let them at least come up to the bridge.</p>
<p class="authors">By Scott</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T01:48:08Z">Thursday, November 17 2022, 01:48</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08524'>Complexity Results for Implication Bases of Convex Geometries</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Todd Bichoupan</p><p>A convex geometry is finite zero-closed closure system that satisfies the
anti-exchange property. Complexity results are given for two open problems
related to representations of convex geometries using implication bases. In
particular, the problem of optimizing an implication basis for a convex
geometry is shown to be NP-hard by establishing a reduction from the minimum
cardinality generator problem for general closure systems. Furthermore, even
the problem of deciding whether an implication basis defines a convex geometry
is shown to be co-NP-complete by a reduction from the Boolean tautology
problem.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bichoupan_T/0/1/0/all/0/1">Todd Bichoupan</a></p><p>A convex geometry is finite zero-closed closure system that satisfies the
anti-exchange property. Complexity results are given for two open problems
related to representations of convex geometries using implication bases. In
particular, the problem of optimizing an implication basis for a convex
geometry is shown to be NP-hard by establishing a reduction from the minimum
cardinality generator problem for general closure systems. Furthermore, even
the problem of deciding whether an implication basis defines a convex geometry
is shown to be co-NP-complete by a reduction from the Boolean tautology
problem.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T01:30:00Z">Thursday, November 17 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08563'>The wrong direction of Jensen's inequality is algorithmically right</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Or Zamir</p><p>Let $\mathcal{A}$ be an algorithm with expected running time $e^X$,
conditioned on the value of some random variable $X$. We construct an algorithm
$\mathcal{A'}$ with expected running time $O(e^{E[X]})$, that fully executes
$\mathcal{A}$. In particular, an algorithm whose running time is a random
variable $T$ can be converted to one with expected running time $O(e^{E[\ln
T]})$, which is never worse than $O(E[T])$. No information about the
distribution of $X$ is required for the construction of $\mathcal{A}'$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Zamir_O/0/1/0/all/0/1">Or Zamir</a></p><p>Let $\mathcal{A}$ be an algorithm with expected running time $e^X$,
conditioned on the value of some random variable $X$. We construct an algorithm
$\mathcal{A'}$ with expected running time $O(e^{E[X]})$, that fully executes
$\mathcal{A}$. In particular, an algorithm whose running time is a random
variable $T$ can be converted to one with expected running time $O(e^{E[\ln
T]})$, which is never worse than $O(E[T])$. No information about the
distribution of $X$ is required for the construction of $\mathcal{A}'$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T01:30:00Z">Thursday, November 17 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09106'>The Exact Bipartite Matching Polytope Has Exponential Extension Complexity</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Xinrui Jia, Ola Svensson, Weiqiang Yuan</p><p>Given a graph with edges colored red or blue and an integer $k$, the exact
perfect matching problem asks if there exists a perfect matching with exactly
$k$ red edges. There exists a randomized polylogarithmic-time parallel
algorithm to solve this problem, dating back to the eighties, but no
deterministic polynomial-time algorithm is known, even for bipartite graphs. In
this paper we show that there is no sub-exponential sized linear program that
can describe the convex hull of exact matchings in bipartite graphs. In fact,
we prove something stronger, that there is no sub-exponential sized linear
program to describe the convex hull of perfect matchings with an odd number of
red edges.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1">Xinrui Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Svensson_O/0/1/0/all/0/1">Ola Svensson</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1">Weiqiang Yuan</a></p><p>Given a graph with edges colored red or blue and an integer $k$, the exact
perfect matching problem asks if there exists a perfect matching with exactly
$k$ red edges. There exists a randomized polylogarithmic-time parallel
algorithm to solve this problem, dating back to the eighties, but no
deterministic polynomial-time algorithm is known, even for bipartite graphs. In
this paper we show that there is no sub-exponential sized linear program that
can describe the convex hull of exact matchings in bipartite graphs. In fact,
we prove something stronger, that there is no sub-exponential sized linear
program to describe the convex hull of perfect matchings with an odd number of
red edges.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T01:30:00Z">Thursday, November 17 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09075'>Keeping it sparse: Computing Persistent Homology revised</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Ulrich Bauer, Talha Bin Masood, Barbara Giunti, Guillaume Houry, Michael Kerber, Abhishek Rathod</p><p>In this work, we study several variants of matrix reduction via Gaussian
elimination that try to keep the reduced matrix sparse. The motivation comes
from the growing field of topological data analysis where matrix reduction is
the major subroutine to compute barcodes. We propose two novel variants of the
standard algorithm, called swap and retrospective reductions, which improve
upon state-of-the-art techniques on several examples in practice. We also
present novel output-sensitive bounds for the retrospective variant which
better explain the discrepancy between the cubic worst-case complexity bound
and the almost linear practical behavior of matrix reduction. Finally, we
provide several constructions on which one of the variants performs strictly
better than the others.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bauer_U/0/1/0/all/0/1">Ulrich Bauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Masood_T/0/1/0/all/0/1">Talha Bin Masood</a>, <a href="http://arxiv.org/find/cs/1/au:+Giunti_B/0/1/0/all/0/1">Barbara Giunti</a>, <a href="http://arxiv.org/find/cs/1/au:+Houry_G/0/1/0/all/0/1">Guillaume Houry</a>, <a href="http://arxiv.org/find/cs/1/au:+Kerber_M/0/1/0/all/0/1">Michael Kerber</a>, <a href="http://arxiv.org/find/cs/1/au:+Rathod_A/0/1/0/all/0/1">Abhishek Rathod</a></p><p>In this work, we study several variants of matrix reduction via Gaussian
elimination that try to keep the reduced matrix sparse. The motivation comes
from the growing field of topological data analysis where matrix reduction is
the major subroutine to compute barcodes. We propose two novel variants of the
standard algorithm, called swap and retrospective reductions, which improve
upon state-of-the-art techniques on several examples in practice. We also
present novel output-sensitive bounds for the retrospective variant which
better explain the discrepancy between the cubic worst-case complexity bound
and the almost linear practical behavior of matrix reduction. Finally, we
provide several constructions on which one of the variants performs strictly
better than the others.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T01:30:00Z">Thursday, November 17 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.09101'>Comparative Learning: A Sample Complexity Theory for Two Hypothesis Classes</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Lunjia Hu, Charlotte Peale</p><p>In many learning theory problems, a central role is played by a hypothesis
class: we might assume that the data is labeled according to a hypothesis in
the class (usually referred to as the realizable setting), or we might evaluate
the learned model by comparing it with the best hypothesis in the class (the
agnostic setting).
</p>
<p>Taking a step beyond these classic setups that involve only a single
hypothesis class, we introduce comparative learning as a combination of the
realizable and agnostic settings in PAC learning: given two binary hypothesis
classes $S$ and $B$, we assume that the data is labeled according to a
hypothesis in the source class $S$ and require the learned model to achieve an
accuracy comparable to the best hypothesis in the benchmark class $B$. Even
when both $S$ and $B$ have infinite VC dimensions, comparative learning can
still have a small sample complexity. We show that the sample complexity of
comparative learning is characterized by the mutual VC dimension
$\mathsf{VC}(S,B)$ which we define to be the maximum size of a subset shattered
by both $S$ and $B$. We also show a similar result in the online setting, where
we give a regret characterization in terms of the mutual Littlestone dimension
$\mathsf{Ldim}(S,B)$. These results also hold for partial hypotheses.
</p>
<p>We additionally show that the insights necessary to characterize the sample
complexity of comparative learning can be applied to characterize the sample
complexity of realizable multiaccuracy and multicalibration using the mutual
fat-shattering dimension, an analogue of the mutual VC dimension for
real-valued hypotheses. This not only solves an open problem proposed by Hu,
Peale, Reingold (2022), but also leads to independently interesting results
extending classic ones about regression, boosting, and covering number to our
two-hypothesis-class setting.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Lunjia Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peale_C/0/1/0/all/0/1">Charlotte Peale</a></p><p>In many learning theory problems, a central role is played by a hypothesis
class: we might assume that the data is labeled according to a hypothesis in
the class (usually referred to as the realizable setting), or we might evaluate
the learned model by comparing it with the best hypothesis in the class (the
agnostic setting).
</p>
<p>Taking a step beyond these classic setups that involve only a single
hypothesis class, we introduce comparative learning as a combination of the
realizable and agnostic settings in PAC learning: given two binary hypothesis
classes $S$ and $B$, we assume that the data is labeled according to a
hypothesis in the source class $S$ and require the learned model to achieve an
accuracy comparable to the best hypothesis in the benchmark class $B$. Even
when both $S$ and $B$ have infinite VC dimensions, comparative learning can
still have a small sample complexity. We show that the sample complexity of
comparative learning is characterized by the mutual VC dimension
$\mathsf{VC}(S,B)$ which we define to be the maximum size of a subset shattered
by both $S$ and $B$. We also show a similar result in the online setting, where
we give a regret characterization in terms of the mutual Littlestone dimension
$\mathsf{Ldim}(S,B)$. These results also hold for partial hypotheses.
</p>
<p>We additionally show that the insights necessary to characterize the sample
complexity of comparative learning can be applied to characterize the sample
complexity of realizable multiaccuracy and multicalibration using the mutual
fat-shattering dimension, an analogue of the mutual VC dimension for
real-valued hypotheses. This not only solves an open problem proposed by Hu,
Peale, Reingold (2022), but also leads to independently interesting results
extending classic ones about regression, boosting, and covering number to our
two-hypothesis-class setting.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T01:30:00Z">Thursday, November 17 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08586'>Bandit Algorithms for Prophet Inequality and Pandora's Box</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Khashayar Gatmiry, Thomas Kesselheim, Sahil Singla, Yifan Wang</p><p>The Prophet Inequality and Pandora's Box problems are fundamental stochastic
problem with applications in Mechanism Design, Online Algorithms, Stochastic
Optimization, Optimal Stopping, and Operations Research. A usual assumption in
these works is that the probability distributions of the $n$ underlying random
variables are given as input to the algorithm. Since in practice these
distributions need to be learned, we initiate the study of such stochastic
problems in the Multi-Armed Bandits model.
</p>
<p>In the Multi-Armed Bandits model we interact with $n$ unknown distributions
over $T$ rounds: in round $t$ we play a policy $x^{(t)}$ and receive a partial
(bandit) feedback on the performance of $x^{(t)}$. The goal is to minimize the
regret, which is the difference over $T$ rounds in the total value of the
optimal algorithm that knows the distributions vs. the total value of our
algorithm that learns the distributions from the partial feedback. Our main
results give near-optimal $\tilde{O}(\mathsf{poly}(n)\sqrt{T})$ total regret
algorithms for both Prophet Inequality and Pandora's Box.
</p>
<p>Our proofs proceed by maintaining confidence intervals on the unknown indices
of the optimal policy. The exploration-exploitation tradeoff prevents us from
directly refining these confidence intervals, so the main technique is to
design a regret upper bound that is learnable while playing low-regret Bandit
policies.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gatmiry_K/0/1/0/all/0/1">Khashayar Gatmiry</a>, <a href="http://arxiv.org/find/cs/1/au:+Kesselheim_T/0/1/0/all/0/1">Thomas Kesselheim</a>, <a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1">Sahil Singla</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yifan Wang</a></p><p>The Prophet Inequality and Pandora's Box problems are fundamental stochastic
problem with applications in Mechanism Design, Online Algorithms, Stochastic
Optimization, Optimal Stopping, and Operations Research. A usual assumption in
these works is that the probability distributions of the $n$ underlying random
variables are given as input to the algorithm. Since in practice these
distributions need to be learned, we initiate the study of such stochastic
problems in the Multi-Armed Bandits model.
</p>
<p>In the Multi-Armed Bandits model we interact with $n$ unknown distributions
over $T$ rounds: in round $t$ we play a policy $x^{(t)}$ and receive a partial
(bandit) feedback on the performance of $x^{(t)}$. The goal is to minimize the
regret, which is the difference over $T$ rounds in the total value of the
optimal algorithm that knows the distributions vs. the total value of our
algorithm that learns the distributions from the partial feedback. Our main
results give near-optimal $\tilde{O}(\mathsf{poly}(n)\sqrt{T})$ total regret
algorithms for both Prophet Inequality and Pandora's Box.
</p>
<p>Our proofs proceed by maintaining confidence intervals on the unknown indices
of the optimal policy. The exploration-exploitation tradeoff prevents us from
directly refining these confidence intervals, so the main technique is to
design a regret upper bound that is learnable while playing low-regret Bandit
policies.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T01:30:00Z">Thursday, November 17 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08605'>A Dichotomy Theorem for Linear Time Homomorphism Orbit Counting in Bounded Degeneracy Graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Daniel Paul-Pena, C. Seshadhri</p><p>Counting the number of homomorphisms of a pattern graph H in a large input
graph G is a fundamental problem in computer science. There are myriad
applications of this problem in databases, graph algorithms, and network
science. Often, we need more than just the total count. Especially in large
network analysis, we wish to compute, for each vertex v of G, the number of
H-homomorphisms that v participates in. This problem is referred to as
homomorphism orbit counting, as it relates to the orbits of vertices of H under
its automorphisms.
</p>
<p>Given the need for fast algorithms for this problem, we study when
near-linear time algorithms are possible. A natural restriction is to assume
that the input graph G has bounded degeneracy, a commonly observed property in
modern massive networks. Can we characterize the patterns H for which
homomorphism orbit counting can be done in linear time?
</p>
<p>We discover a dichotomy theorem that resolves this problem. For pattern H,
let l be the length of the longest induced path between any two vertices of the
same orbit (under the automorphisms of H). If l &lt;= 5, then H-homomorphism orbit
counting can be done in linear time for bounded degeneracy graphs. If l &gt; 5,
then (assuming fine-grained complexity conjectures) there is no near-linear
time algorithm for this problem. We build on existing work on dichotomy
theorems for counting the total H-homomorphism count. Somewhat surprisingly,
there exist (and we characterize) patterns H for which the total homomorphism
count can be computed in linear time, but the corresponding orbit counting
problem cannot be done in near-linear time.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Paul_Pena_D/0/1/0/all/0/1">Daniel Paul-Pena</a>, <a href="http://arxiv.org/find/cs/1/au:+Seshadhri_C/0/1/0/all/0/1">C. Seshadhri</a></p><p>Counting the number of homomorphisms of a pattern graph H in a large input
graph G is a fundamental problem in computer science. There are myriad
applications of this problem in databases, graph algorithms, and network
science. Often, we need more than just the total count. Especially in large
network analysis, we wish to compute, for each vertex v of G, the number of
H-homomorphisms that v participates in. This problem is referred to as
homomorphism orbit counting, as it relates to the orbits of vertices of H under
its automorphisms.
</p>
<p>Given the need for fast algorithms for this problem, we study when
near-linear time algorithms are possible. A natural restriction is to assume
that the input graph G has bounded degeneracy, a commonly observed property in
modern massive networks. Can we characterize the patterns H for which
homomorphism orbit counting can be done in linear time?
</p>
<p>We discover a dichotomy theorem that resolves this problem. For pattern H,
let l be the length of the longest induced path between any two vertices of the
same orbit (under the automorphisms of H). If l &lt;= 5, then H-homomorphism orbit
counting can be done in linear time for bounded degeneracy graphs. If l &gt; 5,
then (assuming fine-grained complexity conjectures) there is no near-linear
time algorithm for this problem. We build on existing work on dichotomy
theorems for counting the total H-homomorphism count. Somewhat surprisingly,
there exist (and we characterize) patterns H for which the total homomorphism
count can be computed in linear time, but the corresponding orbit counting
problem cannot be done in near-linear time.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T01:30:00Z">Thursday, November 17 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08711'>Beyond Worst-Case Budget-Feasible Mechanism Design</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Aviad Rubinstein, Junyao Zhao</p><p>Motivated by large-market applications such as crowdsourcing, we revisit the
problem of budget-feasible mechanism design under a "small-bidder assumption".
Anari, Goel, and Nikzad (2018) gave a mechanism that has optimal competitive
ratio $1-1/e$ on worst-case instances. However, we observe that on many
realistic instances, their mechanism is significantly outperformed by a simpler
open clock auction by Ensthaler and Giebe (2014), although the open clock
auction only achieves competitive ratio $1/2$ in the worst case. Is there a
mechanism that gets the best of both worlds, i.e., a mechanism that is
worst-case optimal and performs favorably on realistic instances?
</p>
<p>Our first main result is the design and the analysis of a natural mechanism
that gives an affirmative answer to our question above: (i) We prove that on
every instance, our mechanism performs at least as good as all uniform
mechanisms, including Anari, Goel, and Nikzad's and Ensthaler and Giebe's
mechanisms. (ii) Moreover, we empirically evaluate our mechanism on various
realistic instances and observe that it beats the worst-case $1-1/e$
competitive ratio by a large margin and compares favorably to both mechanisms
mentioned above.
</p>
<p>Our second main result is more interesting in theory: We show that in the
semi-adversarial model of budget-smoothed analysis, where the adversary designs
a single worst-case market for a distribution of budgets, our mechanism is
optimal among all (including non-uniform) mechanisms; furthermore our mechanism
guarantees a strictly better-than-$(1-1/e)$ expected competitive ratio for any
non-trivial budget distribution regardless of the market. We complement the
positive result with a characterization of the worst-case markets for any given
budget distribution and prove a fairly robust hardness result that holds
against any budget distribution and any mechanism.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Rubinstein_A/0/1/0/all/0/1">Aviad Rubinstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Junyao Zhao</a></p><p>Motivated by large-market applications such as crowdsourcing, we revisit the
problem of budget-feasible mechanism design under a "small-bidder assumption".
Anari, Goel, and Nikzad (2018) gave a mechanism that has optimal competitive
ratio $1-1/e$ on worst-case instances. However, we observe that on many
realistic instances, their mechanism is significantly outperformed by a simpler
open clock auction by Ensthaler and Giebe (2014), although the open clock
auction only achieves competitive ratio $1/2$ in the worst case. Is there a
mechanism that gets the best of both worlds, i.e., a mechanism that is
worst-case optimal and performs favorably on realistic instances?
</p>
<p>Our first main result is the design and the analysis of a natural mechanism
that gives an affirmative answer to our question above: (i) We prove that on
every instance, our mechanism performs at least as good as all uniform
mechanisms, including Anari, Goel, and Nikzad's and Ensthaler and Giebe's
mechanisms. (ii) Moreover, we empirically evaluate our mechanism on various
realistic instances and observe that it beats the worst-case $1-1/e$
competitive ratio by a large margin and compares favorably to both mechanisms
mentioned above.
</p>
<p>Our second main result is more interesting in theory: We show that in the
semi-adversarial model of budget-smoothed analysis, where the adversary designs
a single worst-case market for a distribution of budgets, our mechanism is
optimal among all (including non-uniform) mechanisms; furthermore our mechanism
guarantees a strictly better-than-$(1-1/e)$ expected competitive ratio for any
non-trivial budget distribution regardless of the market. We complement the
positive result with a characterization of the worst-case markets for any given
budget distribution and prove a fairly robust hardness result that holds
against any budget distribution and any mechanism.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-17T01:30:00Z">Thursday, November 17 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Wednesday, November 16
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/16/postdoc-at-university-of-oxford-apply-by-january-16-2023/'>postdoc at University of Oxford (apply by January 16, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          A postdoctoral position is available to work with James Worrell on a UKRI Frontier Research project (ERC replacement grant) on computational problems on dynamical systems. Website: www.cs.ox.ac.uk/news/2107-full.html Email: jbw@cs.ox.ac.uk
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>A postdoctoral position is available to work with James Worrell on a UKRI Frontier Research project (ERC replacement grant) on computational problems on dynamical systems.</p>
<p>Website: <a href="https://www.cs.ox.ac.uk/news/2107-full.html">https://www.cs.ox.ac.uk/news/2107-full.html</a><br />
Email: jbw@cs.ox.ac.uk</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T16:57:43Z">Wednesday, November 16 2022, 16:57</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/'>The Gerrymanders Have It</a></h3>
        <p class='tr-article-feed'>from <a href='https://rjlipton.wpcomstaging.com'>Richard Lipton</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The real winner of the 2022 midterms in the House David Wasserman is an elections analyst for the Cook Political Report. He is known for forecasting the results of elections after people have voted. His words &#8220;I&#8217;ve seen enough&#8221; to declare an outcome are taken as seriously as any network election call. This week, with [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>
<font color="#0044cc"><br />
<em>The real winner of the 2022 midterms in the House</em><br />
<font color="#000000"></p>
<p><a href="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/wassermancook/" rel="attachment wp-att-20492"><img data-attachment-id="20492" data-permalink="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/wassermancook/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/WassermanCook.jpg?fit=170%2C170&amp;ssl=1" data-orig-size="170,170" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="WassermanCook" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/WassermanCook.jpg?fit=170%2C170&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/WassermanCook.jpg?fit=170%2C170&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/WassermanCook.jpg?resize=140%2C140&#038;ssl=1" alt="" width="140" height="140" class="alignright wp-image-20492" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/WassermanCook.jpg?w=170&amp;ssl=1 170w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/WassermanCook.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 140px) 100vw, 140px" data-recalc-dims="1" /></a></p>
<p>
David Wasserman is an <a href="https://www.cookpolitical.com/about/staff/david-wasserman">elections analyst</a> for the <a href="https://en.wikipedia.org/wiki/The_Cook_Political_Report_with_Amy_Walter">Cook Political Report</a>. He is known for forecasting the results of elections <em>after</em> people have voted. His words &#8220;I&#8217;ve seen enough&#8221; to declare an outcome are taken as seriously as any network election call. </p>
<p>
This week, with nine US House races uncalled and control of the chamber still unknown, he is working overtime.</p>
<p>
So have been your humble blog staff, in various life-necessary ways besides this blog. For me (Ken) it is not just being referenced six times in a <a href="https://www.espn.com/espn/story/_/id/34840275/hans-niemann-files-100-million-lawsuit-magnus-carlsen">&#36;100M lawsuit</a>&#8212;much else has been going on. Right now I am preparing a full formal report to the International Chess Federation (FIDE) for their own investigation.</p>
<p>
The election has also diverted our time. Insofar as both of us have involvements in <em>predictive analytics</em>, it behooves us to examine how well election models have been faring and where they may have systematic failings. The Washington Post shows its <a href="https://www.washingtonpost.com/election-results/2022/house/?itid=sn_elections_4/">models</a> of several of the uncalled House races in California plus one in Oregon. The New York Times showed its &#8220;needle&#8221; on Election Night but stopped its <a href="https://www.nytimes.com/interactive/2022/11/08/us/elections/results-house.html">prognostications</a> once the long-count stage began. As we write, the Republicans are on the cusp of the House majority threshold of 218 and will likely exceed it by two or three seats, but they are not yet declared the winner. The winners we can declare, however, are the <em>gerrymanders</em></p>
<p>
<p><H2> Redistricting </H2></p>
<p><p>
Wasserman goes by the handle <a href="https://twitter.com/Redistrict">@Redistrict</a> on Twitter. Redistricting is a neutral name for the re-drawing of boundaries of a region in which an election occurs. This is not confined to the US, but has special status because the US Constitution requires updating the number of Representatives for each state after each decennial US Census, and districts can be redrawn to reflect population shifts within a state even if the state has not gained or lost a member. The political science of drawing these maps is Wasserman&#8217;s specialty. </p>
<p>
To illustrate how the choice of boundaries can affect election outcomes, say we have a &#8220;state&#8221; of just nine people to divide equally into three districts:</p>
<ul>
<li>
<a href="https://fivethirtyeight.com/features/the-case-for-a-democratic-surprise-on-election-night/">Nathaniel Bleu</a>, Carrie Cyan, Alberto Az&uacute;l, Sandy Sapphire. </p>
<li>
<a href="https://fivethirtyeight.com/features/the-case-for-a-republican-sweep-on-election-night/">Nathan Redd</a>, Corrie Crimson, Philip Roth, Sally Scarlet, Ruby Rover.
</ul>
<p>
The &#8220;red voters&#8221; have an overall 5-4 majority. But if the districts are drawn like so, then the state will elect more blue than red representatives:</p>
<ol>
<li>
Bleu, Cyan, Redd. </p>
<li>
Az&uacute;l, Sapphire, Crimson. </p>
<li>
Roth, Scarlet, Ruby.
</ol>
<p>
What happened is that the red votes in district 3 were overkill. This is shown at left in the picture below. Two other natural ways of drawing the boundaries, however, result in two majority red districts.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/gerrymandernogrid/" rel="attachment wp-att-20495"><img data-attachment-id="20495" data-permalink="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/gerrymandernogrid/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/GerrymanderNoGrid.jpg?fit=824%2C259&amp;ssl=1" data-orig-size="824,259" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;regan&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1668526288&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="GerrymanderNoGrid" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/GerrymanderNoGrid.jpg?fit=300%2C94&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/GerrymanderNoGrid.jpg?fit=600%2C189&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/GerrymanderNoGrid.jpg?resize=550%2C173&#038;ssl=1" alt="" width="550" height="173" class="aligncenter wp-image-20495" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/GerrymanderNoGrid.jpg?w=824&amp;ssl=1 824w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/GerrymanderNoGrid.jpg?resize=300%2C94&amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/GerrymanderNoGrid.jpg?resize=768%2C241&amp;ssl=1 768w" sizes="(max-width: 550px) 100vw, 550px" data-recalc-dims="1" /></a></p>
<p>
The third map at right favors Red more robustly in the following sense: If Ruby Rover is any of the bottom three red dots and flips to blue, Red will still win two seats. Whereas, in the second map, any of four flips costs Red a seat. </p>
<p>
The second map, however, gives Red a chance of a clean sweep if either of the two blue voters at left flips to red. Whereas, the third map gives no such chance. </p>
<p>
Redistricting becomes <em>gerrymandering</em> when one side has control to draw a map yielding outcomes out of proportion to the other side&#8217;s voters. The <a href="https://constitutioncenter.org/the-constitution/articles/article-i/clauses/750">Elections Clause</a> of the US Constitution empowers state legislatures to <em>prescribe the manner</em> of state elections, subject to regulation and revision <em>by the US Congress</em>. Some states&#8217; legislatures have vested non-partisan commissions with districting power, while others&#8217; legislatures assume this power to benefit the side currently controlling them.</p>
<p>
<p><H2> Mathematics of Redistricting </H2></p>
<p><p>
Dick wrote a 2019 <a href="https://rjlipton.wpcomstaging.com/2019/07/03/mathematics-of-gerrymandering/">post</a> on the mathematics of gerrymanders. It includes a richer graphic on how they work. Here we will take a view from 20,000 feet and begin with some airy generalities.</p>
<ul>
<li>
<em>Random assignment amplifies the majority</em>. One might think that a completely random assignment of voters to districts would be fairest. But doing so amplifies a distinct majority party into total command of the state&#8217;s races. If we multiplied our 9 people into 900 while keeping proportions, and then chose three groups of 300 at random, it is overwhelmingly likely that majority vote in each group would go red. </p>
<li>
<em>Gerrymanders can favor or disfavor the minority</em>. This is exemplified by both our graphic above and the richer one in Dick&#8217;s post. They are, however, <em>all</em> fairer to the minority than random assignment. </p>
<li>
<em>Proportional representation</em> is practiced in several foreign countries, notably in Europe. This is generally most <em>fair</em>, but runs counter to the notion of geographical community as especially enshrined in US traditions. </p>
<li>
In any map, the higher one side&#8217;s percentage of voters in any one district, the lower the <em>efficiency</em> of each of those voters. Broadly speaking, one side&#8217;s objective in any gerrymander is to minimize the efficiency of the other side&#8217;s voters. There are <a href="https://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=13749&#038;context=journal_articles">various</a> <a href="https://gerrymander.princeton.edu/">metrics</a> for quantifying this.
</ul>
<p>
The US has an organic tendency toward gerrymanders through its rural-suburban-urban spectrum. The rural and urban sides have become more partisan during our lifetimes. When a city has population near the share of one Representative, it is natural to make it into one district. If the blue voters are, say, 80&#37; in that district, then they are individually highly inefficient. Meanwhile, a higher number of red voters&#8212;those besides the 20&#37; inside the city district&#8212;are freed to be efficient elsewhere. </p>
<p>
The logic of clustering a blue city can, however, turn on a dime if the surrounding areas are red and populous enough. Then the city can be divided into pizza slices, each joined with enough red to overpower it. This recently <a href="https://www.cnn.com/interactive/2022/politics/us-redistricting/tennessee-redistricting-map/">happened</a> with Nashville in Tennessee:</p>
<p><a href="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/nashvillemap-2/" rel="attachment wp-att-20498"><img data-attachment-id="20498" data-permalink="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/nashvillemap-2/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NashvilleMap.png?fit=685%2C508&amp;ssl=1" data-orig-size="685,508" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="NashvilleMap" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NashvilleMap.png?fit=300%2C222&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NashvilleMap.png?fit=600%2C445&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NashvilleMap.png?resize=342%2C254&#038;ssl=1" alt="" width="342" height="254" class="aligncenter wp-image-20498" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NashvilleMap.png?w=685&amp;ssl=1 685w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NashvilleMap.png?resize=300%2C222&amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NashvilleMap.png?resize=200%2C150&amp;ssl=1 200w" sizes="(max-width: 342px) 100vw, 342px" data-recalc-dims="1" /></a></p>
<p>
This change strikes us as increasing the efficiency of the Nashville voters&#8212;and the surrounding rural voters too. Thus efficiency is not the only metrizable notion that is relevant to fairness. </p>
<p>
<p><H2> Difference Makers </H2></p>
<p><p>
A key episode in this year&#8217;s redistricting was the <a href="https://fivethirtyeight.com/features/new-york-just-cost-democrats-their-big-redistricting-advantage/">rejection</a> by the New York Court of Appeals of the district map drawn up by the Democratic-controlled state legislature. The <a href="https://projects.fivethirtyeight.com/redistricting-2022-maps/new-york/amended_democratic_proposal/">map at left</a> below, was replaced by the <a href="https://projects.fivethirtyeight.com/redistricting-2022-maps/new-york/">map at right</a>.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/nychange3/" rel="attachment wp-att-20500"><img data-attachment-id="20500" data-permalink="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/nychange3/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NYChange3.png?fit=1216%2C477&amp;ssl=1" data-orig-size="1216,477" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="NYChange3" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NYChange3.png?fit=300%2C118&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NYChange3.png?fit=600%2C236&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NYChange3.png?resize=460%2C180&#038;ssl=1" alt="" width="460" height="180" class="aligncenter wp-image-20500" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NYChange3.png?resize=1024%2C402&amp;ssl=1 1024w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NYChange3.png?resize=300%2C118&amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NYChange3.png?resize=768%2C301&amp;ssl=1 768w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NYChange3.png?resize=1200%2C471&amp;ssl=1 1200w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/NYChange3.png?w=1216&amp;ssl=1 1216w" sizes="(max-width: 460px) 100vw, 460px" data-recalc-dims="1" /></a></p>
<p>
Among several of the first map&#8217;s sins was lack of geometric contiguity as codified in law in one district that hopped over the Long Island Sound. Three consequential changes were Syracuse losing its reach down to Ithaca while absorbing red areas northeast, Long Island&#8217;s red area being divided between two districts, and Staten Island being joined to red rather than blue areas of Brooklyn. <a href="https://en.wikipedia.org/wiki/The_City_(website)">The City</a> published an <a href="https://www.thecity.nyc/2022/11/9/23450433/max-rose-malliotakis-staten-island-brooklyn-redistricting-election">analysis</a> from last week&#8217;s voting records that the district with Staten Island would have gone blue with the original map. All close districts went Republican, and this alone may make the difference in tbe majority. </p>
<p><p>
Even while Illinois lost a seat from population shifts, their Democrats conjured a new blue seat snaking through Springfield. Again the maps are mashups of ones <a href="https://projects.fivethirtyeight.com/redistricting-2022-maps/illinois/">created</a> by FiveThirtyEight, not by Bart Simpson.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/illinoischange/" rel="attachment wp-att-20501"><img data-attachment-id="20501" data-permalink="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/illinoischange/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?fit=807%2C609&amp;ssl=1" data-orig-size="807,609" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;KWRegan&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1668559241&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="IllinoisChange" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?fit=300%2C226&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?fit=600%2C453&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?resize=410%2C309&#038;ssl=1" alt="" width="410" height="309" class="aligncenter wp-image-20501" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?w=807&amp;ssl=1 807w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?resize=300%2C226&amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?resize=768%2C580&amp;ssl=1 768w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?resize=800%2C600&amp;ssl=1 800w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?resize=400%2C300&amp;ssl=1 400w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/IllinoisChange.jpg?resize=200%2C150&amp;ssl=1 200w" sizes="(max-width: 410px) 100vw, 410px" data-recalc-dims="1" /></a></p>
<p><P><br />
Meanwhile, Florida not only gained a seat, but their Republicans <a href="https://projects.fivethirtyeight.com/redistricting-2022-maps/florida/">created</a> three more strong ones for themselves even before considering their increased Election Day margins on the whole. </p>
<p><P><br />
<a href="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/floridachange/" rel="attachment wp-att-20502"><img data-attachment-id="20502" data-permalink="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/floridachange/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/FloridaChange.jpg?fit=802%2C582&amp;ssl=1" data-orig-size="802,582" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;KWRegan&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1668559706&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="FloridaChange" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/FloridaChange.jpg?fit=300%2C218&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/FloridaChange.jpg?fit=600%2C435&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/FloridaChange.jpg?resize=410%2C309&#038;ssl=1" alt="" width="410" height="309" class="aligncenter wp-image-20502" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/FloridaChange.jpg?resize=400%2C300&amp;ssl=1 400w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/FloridaChange.jpg?resize=200%2C150&amp;ssl=1 200w" sizes="(max-width: 410px) 100vw, 410px" data-recalc-dims="1" /></a></p>
<p><H2> Variability </H2></p>
<p><p>
We have tried to be balanced in our choice of examples. Our main point is not whether the changes are signed blue or red, but rather their absolute value. The variance alone is likely to dwarf the margin of the final House majority. </p>
<p>
Thus, instead of trying to define districts according to some criterion of <em>fairness</em>, can we instead postulate that revisions adhere to metrics for minimizing <em>variability</em>? This requires maintaining the sequence of past maps and population distributions as a reference, rather than treating each new map <em>ab ovo</em>.</p>
<p>
To be sure, it is possible for maps to conserve variability while defying any notions of geometric regularity. Here are the 2000 and 2002 maps for one Chicago area district:</p>
<p><P><br />
<a href="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/obamachange2/" rel="attachment wp-att-20523"><img data-attachment-id="20523" data-permalink="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/obamachange2/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/ObamaChange2.jpg?fit=592%2C481&amp;ssl=1" data-orig-size="592,481" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;KWRegan&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1668562083&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="ObamaChange2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/ObamaChange2.jpg?fit=300%2C244&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/ObamaChange2.jpg?fit=592%2C481&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/ObamaChange2.jpg?resize=410%2C355&#038;ssl=1" alt="" width="410" height="355" class="aligncenter size-full wp-image-20523" data-recalc-dims="1" /></a></p>
<p><P><br />
As recounted <a href="https://redistricting.lls.edu/redistricting-101/why-should-we-care/">here</a>, only one element of variability mattered most to the incumbent about the right-hand map. That was to exclude the home marked by the blue pin at upper right. It was the residence of a potential challenger: Barack Obama.</p>
<p>
<p><H2> Open Questions </H2></p>
<p><p>
Have we shed any more light on mathematical criteria that might curtail the variability and arbitrariness of redistricting?</p>
<p>
Here is a second question, along lines of my saying above that how election models fare can matter to my chess work. FiveThirtyEight are catching heat for their modeling of Washington&#8217;s Third Congressional District, where Marie Gluesenkamp Perez upset the Republican Joe Kent. They had Perez at only a 2&#37; chance to win:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/kentperezrace538/" rel="attachment wp-att-20505"><img data-attachment-id="20505" data-permalink="https://rjlipton.wpcomstaging.com/2022/11/16/the-gerrymanders-have-it/kentperezrace538/" data-orig-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/KentPerezRace538.jpg?fit=668%2C363&amp;ssl=1" data-orig-size="668,363" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;KWRegan&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1668592000&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="KentPerezRace538" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/KentPerezRace538.jpg?fit=300%2C163&amp;ssl=1" data-large-file="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/KentPerezRace538.jpg?fit=600%2C326&amp;ssl=1" decoding="async" loading="lazy" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/KentPerezRace538.jpg?resize=460%2C250&#038;ssl=1" alt="" width="460" height="250" class="aligncenter size-full wp-image-20505" srcset="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/KentPerezRace538.jpg?w=668&amp;ssl=1 668w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/11/KentPerezRace538.jpg?resize=300%2C163&amp;ssl=1 300w" sizes="(max-width: 460px) 100vw, 460px" data-recalc-dims="1" /></a></p>
<p>
Our question is, given that over a hundred races were under the 99&#37;-lock level, and allowing for covariance over all races, shouldn&#8217;t one expect to have one such case? If &#8220;a 2&#37; chance to win&#8221; really means what it says in your model, not just a hedge for modeling uncertainty, then it should have 2&#37; expectation, no?  This can be argued back-and-forth a few more rounds based on how FiveThirtyEight&#8217;s simulations work, but my point will remain&#8212;and it is important in both my top-level need to gauge unlikelihoods longer than 2&#37; <em>and</em> my model&#8217;s internal need for precision and accuracy on estimating low-probability moves, especially <em>blunders</em>.</p>
<p><P><br />
[fixed Obama pin, added to end question, some small word changes]</p>
<p class="authors">By KWRegan</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T15:42:52Z">Wednesday, November 16 2022, 15:42</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/157'>TR22-157 |  Border complexity via elementary symmetric polynomials | 

	Pranjal Dutta, 

	Fulvio Gesmundo, 

	Christian Ikenmeyer, 

	Gorav Jindal, 

	Vladimir Lysikov</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          In (ToCT’20) Kumar surprisingly proved that every polynomial can be approximated as a sum of a constant and a product of linear polynomials. In this work, we prove the converse of Kumar&#39;s result which ramifies in a surprising new formulation of Waring rank and border Waring rank. From this conclusion, we branch out into two different directions, and implement the geometric complexity theory (GCT) approach in two different settings.

In the first direction, we study the orbit closure of the product-plus-power polynomial, determine its stabilizer, and determine the properties of its boundary points. We also connect its fundamental invariant to the Alon-Tarsi conjecture on Latin squares, and prove several exponential separations between related polynomials contained in the affine closure of product-plus-product polynomials. We fully implement the GCT approach and obtain several equations for the product-plus-power polynomial from its symmetries via representation theoretic multiplicity obstructions.

In the second direction, we demonstrate that the non-commutative variant of Kumar&#39;s result is intimately connected to the constructions of Ben-Or and Cleve (SICOMP&#39;92), and Bringmann, Ikenmeyer, Zuiddam (JACM&#39;18), which describe algebraic formulas in terms of iterated matrix multiplication. From this we obtain that a variant of the elementary symmetric polynomial is complete for V3F, a large subclass of VF, under homogeneous border projections. In the regime of quasipolynomial complexity, our polynomial has the same power as the determinant or as arbitrary circuits, i.e., VQP. This is the first completeness result under homogeneous projections for a subclass of VBP. Such results are required to set up the GCT approach in a way that avoids the no-go theorems of B\&quot;urgisser, Ikenmeyer, Panova (JAMS&#39;19).

Finally, using general geometric considerations, we significantly improve the relationship between the Waring rank and the border Waring rank of polynomials. In particular, if the border Waring rank of a homogeneous polynomial $f$ is $k$, then, the Waring rank of  $f$ can be at most $\exp(k) \cdot d$, while previously it was known to be $O(d^k)$.
        
        </div>

        <div class='tr-article-summary'>
        
          
          In (ToCT’20) Kumar surprisingly proved that every polynomial can be approximated as a sum of a constant and a product of linear polynomials. In this work, we prove the converse of Kumar&#39;s result which ramifies in a surprising new formulation of Waring rank and border Waring rank. From this conclusion, we branch out into two different directions, and implement the geometric complexity theory (GCT) approach in two different settings.

In the first direction, we study the orbit closure of the product-plus-power polynomial, determine its stabilizer, and determine the properties of its boundary points. We also connect its fundamental invariant to the Alon-Tarsi conjecture on Latin squares, and prove several exponential separations between related polynomials contained in the affine closure of product-plus-product polynomials. We fully implement the GCT approach and obtain several equations for the product-plus-power polynomial from its symmetries via representation theoretic multiplicity obstructions.

In the second direction, we demonstrate that the non-commutative variant of Kumar&#39;s result is intimately connected to the constructions of Ben-Or and Cleve (SICOMP&#39;92), and Bringmann, Ikenmeyer, Zuiddam (JACM&#39;18), which describe algebraic formulas in terms of iterated matrix multiplication. From this we obtain that a variant of the elementary symmetric polynomial is complete for V3F, a large subclass of VF, under homogeneous border projections. In the regime of quasipolynomial complexity, our polynomial has the same power as the determinant or as arbitrary circuits, i.e., VQP. This is the first completeness result under homogeneous projections for a subclass of VBP. Such results are required to set up the GCT approach in a way that avoids the no-go theorems of B\&quot;urgisser, Ikenmeyer, Panova (JAMS&#39;19).

Finally, using general geometric considerations, we significantly improve the relationship between the Waring rank and the border Waring rank of polynomials. In particular, if the border Waring rank of a homogeneous polynomial $f$ is $k$, then, the Waring rank of  $f$ can be at most $\exp(k) \cdot d$, while previously it was known to be $O(d^k)$.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T15:03:39Z">Wednesday, November 16 2022, 15:03</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://gilkalai.wordpress.com/2022/11/16/barnabas-janzer-rotation-inside-convex-kakeya-sets/'>Barnabás Janzer: Rotation inside convex Kakeya sets</a></h3>
        <p class='tr-article-feed'>from <a href='https://gilkalai.wordpress.com'>Gil Kalai</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Barnabás Janzer studied the following question: Suppose we have convex body in that contains a copy of a convex body in every orientation. Is it always possible to move any one copy of to another copy of , keeping inside &#8230; Continue reading &#8594;
        
        </div>

        <div class='tr-article-summary'>
        
          
          <blockquote><p>Barnabás Janzer studied the following question:</p>
<p><span style="color:#993366;"><em>Suppose we have convex body <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="K" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathbb R^n" class="latex" /> that contains a copy of a convex body <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" /> in every orientation. Is it always possible to move any one copy of <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" /> to another copy of <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" />, keeping inside <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="K" class="latex" />?</em></span></p></blockquote>
<p>I will let you test your intuition about what the answer should be. First, some background.</p>
<p>A Kakeya set is a set that contains unit unterval in every direction. A famous open problem is the conjecture that every Kakeya set in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathbb R^d" class="latex" /> has Housdorff dimension <img src="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=d&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="d" class="latex" />. in 2008, Zeev Dvir <a href="https://terrytao.wordpress.com/2008/03/24/dvirs-proof-of-the-finite-field-kakeya-conjecture/">found a simple remarkable proof</a> for a finite field analog of the conjecture. Finding possible connections between the finite field problem and the Euclidean problem is an exciting problem. Can we use the finite field result to prove the Euclidean result? Can we use or refine the finite field methods for the Euclidean problem? Here is a <a href="https://www.quantamagazine.org/new-number-systems-point-geometry-problem-toward-a-real-solution-20220726/">recent Quanta Magazine article</a> about exciting &#8220;intermediate results&#8221;.</p>
<p>The question about the connection between finite fields analogs and questions over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+Z&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathbb Z" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb+R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathbb R" class="latex" /> can be asked about other problems. One example is the Roth problem (relevant posts <a href="https://gilkalai.wordpress.com/2020/07/08/to-cheer-you-up-in-difficult-times-7-bloom-and-sisask-just-broke-the-logarithm-barrier-for-roths-theorem/">I</a>,<a href="https://gilkalai.wordpress.com/2010/11/24/roths-theorem-sanders-reaches-the-logarithmic-barrier/">II</a>, <a href="https://gilkalai.wordpress.com/2009/03/25/an-open-discussion-and-polls-around-roths-theorem/">III</a>) vs. the cup set problems (relevant posts<a href="https://gilkalai.wordpress.com/2016/05/15/mind-boggling-following-the-work-of-croot-lev-and-pach-jordan-ellenberg-settled-the-cap-set-problem/"> I</a>,<a href="https://gilkalai.wordpress.com/2009/03/25/an-open-discussion-and-polls-around-roths-theorem/">II</a>,<a href="https://gilkalai.wordpress.com/2009/02/07/frankl-rodls-theorem-and-variations-on-the-cap-set-problem-a-recent-research-project-with-roy-meshulam-a/">III</a>).</p>
<p><span id="more-23515"></span></p>
<p>Going back to our problem (posed by H. T. Croft) about convex Kakeya sets:</p>
<blockquote><p>Suppose we have convex body <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="K" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathbb R^n" class="latex" /> that contains a copy of a convex body <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" /> in every orientation. Is it always possible to move any one copy of <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" /> to another copy of <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" />, keeping inside <img src="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="K" class="latex" />?</p></blockquote>
<p>Barnabás Janzer proved:</p>
<h3><span style="color:#000080;">The answer is yes in 2 dimensions:</span></h3>
<h3><span style="color:#000080;">The answer is also yes in <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="n" class="latex" /> dimensions if <img src="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="S" class="latex" /> is one-dimensional (i.e. an interval).</span></h3>
<h3><span style="color:#ff0000;"><strong>But, amazingly: the answer is no in general in 4 dimensions!</strong></span></h3>
<p>Here is the link to the paper:</p>
<h3><a href="https://arxiv.org/abs/2209.09728">Rotation inside convex Kakeya sets</a></h3>
<p><!--more--></p>
<p class="authors">By Gil Kalai</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T13:05:59Z">Wednesday, November 16 2022, 13:05</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://francisbach.com/sums-of-squares-for-dummies/'>Sums-of-squares for dummies: a view from the Fourier domain</a></h3>
        <p class='tr-article-feed'>from <a href='https://francisbach.com'>Francis Bach</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          In these last two years, I have been studying intensively sum-of-squares relaxations for optimization, learning a lot from many great research papers [1, 2], review papers [3], books [4, 5, 6, 7, 8], and even websites. Much of the literature focuses on polynomials as the de facto starting point. While this leads to deep connections...
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="justify-text">In these last two years, I have been studying intensively sum-of-squares relaxations for optimization, learning a lot from many great research papers [<a href="https://epubs.siam.org/doi/pdf/10.1137/S1052623400366802">1</a>, <a href="http://www.mit.edu/~parrilo/pubs/files/SDPrelaxations.pdf">2</a>], review papers [<a href="https://homepages.cwi.nl/~monique/files/laurent-ima.pdf">3</a>], books [4, 5, 6, 7, 8], and even <a href="https://www.sumofsquares.org/">websites</a>. </p>



<p class="justify-text">Much of the literature focuses on polynomials as the de facto starting point. While this leads to deep connections between many fields within mathematics, and many applications in various areas (optimal control, data science, etc.), the need for arguably non-natural hierarchies (at least for beginners) sometimes makes the exposition hard to follow at first, and notations a tiny bit cumbersome.</p>



<p class="justify-text">I think that one reason is the aim for generality of the approaches. If you want to minimize potentially arbitrary continuous functions on a set defined by arbitrary equality or inequality constraints, there is not much choice if you want a unique generic mathematical framework that can approximate all of these problems. But if you are willing to optimize an arbitrary continuous function only on a &#8220;simple&#8221; set, then things considerably simplify: most results have explicit reasonably simple proofs, and you can even prove the exponential convergence of the SOS hierarchy! The main goal of this post is to present such old and recent results.</p>



<p class="justify-text">One other aim is to highlight the classical dual view of optimization problems through the characterization of probability measures by their moments, which has many applications beyond optimization.</p>



<p class="justify-text">By simple sets, I mean the interval \([0,\! 1]\) as a toy example, but by taking tensor products, this extends to the hypercube \([0,\! 1]^n\), and with a bit more work, you can add the Boolean hypercube \(\{-1,1\}^n\) and Euclidean spheres (see below). The key property that we will need is the availability of a nice orthonormal basis of square integrable functions.</p>



<h2>Minimization of quadratic forms</h2>



<p class="justify-text">In order to cover all cases, I will consider a generic minimization problem, $$\min_{x \in \mathcal{X}} \ f(x),$$ where \(\mathcal{X}\) is a compact set, e.g., \([0,\! 1]^n\) or \(\{-1,+1\}^n\) (I will use \([0,\! 1]\) as a running example). I will also first assume that the function \(f: \mathcal{X} \to \mathbb{R}\) can be represented as a quadratic form in some complex vector-valued feature \(\varphi: \mathcal{X} \to \mathbb{C}^d\), that is, there exists a Hermitian matrix \(F \in \mathbb{C}^{d \times d}\) (i.e., such that \(F^\ast = F\), where \(F^\ast\) is the &#8220;<a href="https://en.wikipedia.org/wiki/Conjugate_transpose">conjugate-transpose</a>&#8221; of the matrix \(F\)), such that $$\forall x \in \mathcal{X}, \ f(x) = \varphi(x)^\ast F \varphi(x) = \sum_{j,k=1}^d \varphi_j(x) \varphi_k(x)^\ast F_{jk}.$$ I will also assume that features are normalized, that is, \(\| \varphi(x)\| = 1\) for all \(x \in \mathcal{X}\). Thus, the constant function equal to one is represented by the identity matrix.</p>



<p class="justify-text">For our running example, with \(\mathcal{X} = [0,\! 1]\), we can consider the Fourier basis $$\varphi_\omega(x) = \frac{1}{\sqrt{2r+1}} e^{2i\pi \omega x}, \ \ \omega \in \{-r,-r+1,\dots,r-1,r\}, $$ and then the assumption on \(f\) is that it is a real-valued linear combination of complex exponentials \(x \mapsto e^{2i\pi \omega x}\), for \(\omega \in \{-2r,-2r+1,\dots,2r-1,2r\}\), and thus a trigonometric polynomial of degree \(2r\) (that is a real bivariate polynomial of degree \(2r\) in \(\cos(2\pi x)\) and \(\sin(2\pi x)\)). </p>



<p class="justify-text">We see immediately in this example, that the representation of \(f\) through \(F\) is not unique. This is due to the fact that in the real vector space of Hermitian matrices of dimension \(d\), the rank-one matrices \( \varphi(x) \varphi(x)^\ast\) may only occupy a specific linear subspace, which we denote \(\mathcal{V}\). For the specific case of univariate trigonometric polynomials, we have $$(\varphi(x) \varphi(x)^\ast)_{\omega\omega&#8217;} =  \frac{1}{{2r+1}} e^{2i\pi (\omega\, -\, \omega&#8217;) x},$$ so that the set \(\mathcal{V}\) is exactly the set of Hermitian <a href="https://en.wikipedia.org/wiki/Toeplitz_matrix">Toeplitz matrices</a> (which are constant along all diagonals), and thus a strict subset of all Hermitian matrices.</p>



<p class="justify-text">We will often use the trace trick to represent a quadratic form as a linear form in \(\varphi(x) \varphi(x)^\ast\), that is, $$ \varphi(x)^\ast F \varphi(x)  = {\rm tr} \big[ F \varphi(x) \varphi(x)^\ast \big].$$  The set of matrices \(F\) that represent the function \(f\) exactly is an affine subspace orthogonal to \(\mathcal{V}\), that is, of the form \(F + \mathcal{V}^\perp\), where \(\mathcal{V}^\perp\) is the orthogonal of \(\mathcal{V}\) in the set of Hermitian matrices equipped with the dot-product \((M,N) \mapsto {\rm tr}(MN)\). For our running example, \(\mathcal{V}\) has dimension \(4r+1\), and hence its orthogonal has dimension \((2r+1)^2-4r-1 = 4r^2\).</p>



<h2>Two dual convex views</h2>



<p class="justify-text">The minimization of \(f\) has two dual convex optimization formulations, which are both natural, and which both provide insights into the problems.</p>



<p class="justify-text"><strong>Maximizing lower bounds. </strong>The first view is simply maximizing a lower bound on \(f\), that is, $$\min_{x \in \mathcal{X}} f(x) = \max_{c \in \mathbb{R}} \ c \ \mbox{ such that }\  \forall x \in \mathcal{X}, f(x)\, &#8211; c \geqslant 0.$$ The optimal \(c\) is then \(\min_{x \in \mathcal{X}} f(x)\). See an illustration below.</p>


<div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2022/09/f_above_c-1024x670.png" alt="" class="wp-image-8246" width="309" height="202" srcset="https://francisbach.com/wp-content/uploads/2022/09/f_above_c-1024x670.png 1024w, https://francisbach.com/wp-content/uploads/2022/09/f_above_c-300x196.png 300w, https://francisbach.com/wp-content/uploads/2022/09/f_above_c-768x503.png 768w, https://francisbach.com/wp-content/uploads/2022/09/f_above_c-1536x1006.png 1536w, https://francisbach.com/wp-content/uploads/2022/09/f_above_c-350x230.png 350w, https://francisbach.com/wp-content/uploads/2022/09/f_above_c-850x557.png 850w, https://francisbach.com/wp-content/uploads/2022/09/f_above_c.png 2007w" sizes="(max-width: 309px) 100vw, 309px" /></figure></div>


<p class="justify-text">As mentioned in an <a href="https://francisbach.com/finding-global-minima-with-kernel-approximations/">earlier post</a>, all optimization problems are thus convex! The hard computational task here is to represent efficiently non-negative functions to handle the (convex) constraint that \(f &#8211; c\) is non-negative.</p>



<p class="justify-text"><strong>Mininimizing expectations. </strong>The second view is looking for a probability measure \(\mu\) on \(\mathcal{X}\) with minimal expectation \(\int_\mathcal{X} \! f(x) d\mu(x)\). Denoting \(\mathcal{P}(\mathcal{X})\) the set of probability distributions on \(\mathcal{X}\), we have: $$\min_{x \in \mathcal{X}} f(x)  = \min_{ \mu \in \mathcal{P}(\mathcal{X})}  \int_\mathcal{X} f(x) d\mu(x),$$ and the minimizer is any probability measure supported on the minimizers of \(f\).</p>



<p class="justify-text">When \(f\) can be represented by a quadratic form, then the objective function becomes \(\displaystyle {\rm tr} \Big[ F\! \int_\mathcal{X} \varphi(x)\varphi(x)^\ast d\mu(x) \Big]\), which is linear in the moment matrix $$\Sigma = \int_\mathcal{X} \varphi(x)\varphi(x)^\ast d\mu(x).$$ The hard computational task is to represent efficiently the set of allowed moment matrices, which happens to be the closure of the convex hull \(\mathcal{K}\) of all \( \varphi(x) \varphi(x)^\ast\), \(x \in \mathcal{X}\) (remember that \(\mathcal{V}\) defined above is its linear span and thus contains \(\mathcal{K}\)). See an illustration below.</p>


<div class="wp-block-image justify-text">
<figure class="aligncenter size-large is-resized"><img loading="lazy" src="https://francisbach.com/wp-content/uploads/2022/09/KV-2-1024x279.png" alt="" class="wp-image-8257" width="588" height="160" srcset="https://francisbach.com/wp-content/uploads/2022/09/KV-2-1024x279.png 1024w, https://francisbach.com/wp-content/uploads/2022/09/KV-2-300x82.png 300w, https://francisbach.com/wp-content/uploads/2022/09/KV-2-768x209.png 768w, https://francisbach.com/wp-content/uploads/2022/09/KV-2-850x232.png 850w, https://francisbach.com/wp-content/uploads/2022/09/KV-2.png 1534w" sizes="(max-width: 588px) 100vw, 588px" /><figcaption>Convex hull \(\mathcal{K}\) and span \(\mathcal{V}\) of all \(\varphi(x) \varphi(x)^\ast\), \(x \in \mathcal{X}\), when \(\mathcal{X} = \{x_1,\dots,x_5\}\) has 5 elements.</figcaption></figure></div>


<p>A key computational task will be to find outer approximations of \(\mathcal{K}\) based in particular on the knowledge of \(\mathcal{V}\).</p>



<p class="justify-text"><strong>Equivalence by convex duality.</strong> The two views are equivalent as one can see \(\mu\) as the Lagrange multiplier for the constraint \(\forall x \in \mathcal{X}, f(x)\, &#8211; c \geqslant 0\), which has to be a finite positive measure [10]. The Lagrangian is $$\mathcal{L}(c,\mu) = c + \int_\mathcal{X} ( f(x) \, &#8211; c) d\mu(x),$$ and minimizing with respect to the primal variable \(c\) leads to the constraint \(\int_\mathcal{X} d\mu(x) = 1\), that is, \(\mu\) is a probability distribution. Note that the traditional complementary slackness condition imposes that the optimal measure \(\mu\) puts mass only on minimizers of \(f\).</p>



<h2>Convex relaxation: the sum-of-squares (SOS) view</h2>



<p class="justify-text">A natural idea to characterize non-negative functions represented as quadratic forms is to consider positive-semidefinite (PSD) Hermitian matrices, that is, Hermitian matrices with non-negative eigenvalues. Indeed, if \(g(x) = \varphi(x)^\ast G \varphi(x)\) with \(G \succcurlyeq 0\) (which is another notation for \(G\) PSD), then \(g(x) \geqslant 0\) for all \(x \in \mathcal{X}\). This leads to the relaxation: $$\tag{1} \max_{c \in \mathbb{R}, \ A \in \mathbb{C}^{d \times d}} \ c \  \mbox{ such that } \  \forall x \in \mathcal{X}, \ f(x) = c + \varphi(x)^\ast A \varphi(x), \ A \succcurlyeq 0,$$ which is always lower than the minimal value of \(f\) since we replace non-negativity of \(f-c\) by a stronger sufficient condition.  This is called an SOS relaxation because when \(G \succcurlyeq 0\), we can write \(g(x) = \varphi(x)^\ast G \varphi(x)\) as the sum of the squares of the functions \(x \mapsto \lambda_i^{1/2} u_i^\ast \varphi(x)\), where \(\lambda_i\) is the \(i\)-th (non-negative real) eigenvalue of \(G\), and \(u_i \in \mathbb{C}^d\) the corresponding eigenvector.</p>



<p class="justify-text">We can write the equality constraint compactly as (using that features have unit norm): $$ \forall x \in \mathcal{X}, \ \varphi(x)^\ast [ F &#8211; c I &#8211; A ] \varphi(x) = 0 \ \ \Leftrightarrow \ \ F &#8211; c I &#8211; A \in \mathcal{V}^\perp.$$ Using a variable \(-Y\) for an element of \(\mathcal{V}^\perp\) and eliminating \(A\), we get a constraint \(F   + Y \succcurlyeq cI \), leading to \(c\) being the lowest eigenvalue of \(F +Y\) at optimum. This leads to a particularly simple formula for the relaxation: $$\tag{2} \max_{Y \in \mathcal{V}^\perp} \  \lambda_{\min} ( F + Y ).$$</p>



<p class="justify-text">The relaxation is tight for all \(F\), if and only if all non-negative functions represented as quadratic forms are SOS. This is not the case in general, and the relaxation is thus not tight in general, except in a few cases like uni-dimensional trigonometric polynomials.</p>



<p class="justify-text">It is tempting to set \(Y = 0\), and consider \(\lambda_{\min}(F)\), which is directly a lower-bound on the minimal value of \(f\), because, since features have been assumed to have unit norm, $$\lambda_{\min}(F) \ =\  \min_{ \| z\|=1} z^\ast F z\  \leqslant\  \min_{x \in \mathcal{X}} \ \varphi(x)^\ast F \varphi(x).$$ This is the classical spectral relaxation, which is much simpler than the SOS relaxation and can already lead to interesting behaviors. Note that we can see problem \((2)\) as the spectral relaxation optimized over all matrices \(F + Y\) that define the same quadratic form in \(\varphi(x)\).</p>



<p class="justify-text">Before looking at the performance of such relaxations, let&#8217;s first look at the dual view.</p>



<h2>Convex relaxation: the moment view</h2>



<p class="justify-text">Given that the objective function is linear in \( \varphi(x) \varphi(x)^\ast\), a traditional approach is to find outer approximations of the convex set \(\mathcal{K}\). The least we can expect from an outer approximation is that the affine hull is preserved, that is, we want to make sure that a matrix \(\Sigma\) in our approximation satisfies \(\Sigma \in \mathcal{V}\) (that is, Toeplitz for the trigonometric polynomial case), and \({\rm tr }(\Sigma) = 1\) (coming from \({\rm tr}(\varphi(x)\varphi(x)^\ast) = \| \varphi(x)\|^2 = 1\)).</p>



<p class="justify-text">Another natural property of the matrices \( \varphi(x) \varphi(x)^\ast\) is that they are positive semi-definite (PSD). The moment relaxation will exactly be the combination of these two properties (affine hull + PSD), leading to the outer approximation $$\widehat{\mathcal{K}} = \big\{ \Sigma \in \mathcal{V}, \ {\rm tr}(\Sigma)=1, \ \Sigma \succcurlyeq 0 \big\},$$ and to the relaxation $$\tag{3} \min_{ \Sigma \in \mathbb{C}^{d \times d}} \ {\rm tr} [ F \Sigma ] \ \mbox{ such that } \ \Sigma \succcurlyeq 0, \ {\rm tr}( \Sigma) = 1, \ \Sigma \in \mathcal{V}.$$</p>



<p class="justify-text">Removing the constraint that \(\Sigma \in \mathcal{V}\), we obtain again the (weaker) spectral relaxation.</p>



<p class="justify-text">Note that the outer approximation of \(\mathcal{K}\) by \(\widehat{\mathcal{K}}\) has many applications beyond optimization (see, e.g., [4, 7]). We get a tight relaxation for all \(F\), if and only if \(\mathcal{K}=  \widehat{\mathcal{K}}\), which happens only in a few cases.</p>



<h2>Equivalence by convex duality</h2>



<p class="justify-text">By introducing the same Lagrange multiplier \(\mu\) which is now a <em>signed</em> measure (since we have an equality constraint), we obtain the Lagrangian $$\mathcal{L}(c,A, \mu) = c + \int_\mathcal{X} \big( f(x) \, &#8211; c \, &#8211; \,  \varphi(x)^\ast A \varphi(x)\big) d\mu(x),$$ and minimizing with respect to \(c\) leads to the constraint \(\int_\mathcal{X} d\mu(x) = 1\), while optimizing with respect to \(A\) leads to the constraint \(  \int_\mathcal{X} \varphi(x)\varphi(x)^\ast d\mu(x) \succcurlyeq 0\), and we exactly get the moment relaxation by setting \(\Sigma = \int_\mathcal{X} \varphi(x)\varphi(x)^\ast d\mu(x)\). Another way of seeing it is that the two semidefinite programs \((2)\) and \((3)\) are dual to each other.</p>



<p class="justify-text">The two views are thus equivalent, and tightness of one is equivalent to the tightness of the other. We will see below that for our running example of univariate trigonometric polynomials, the relaxation is tight, before looking at the multivariate case, where the relaxation is not tight, but can be made as tight as desired. We will then allow infinite-dimensional feature maps by allowing infinitely many frequencies.</p>



<p class="justify-text">Let&#8217;s first mention a few &#8220;practicalities&#8221;: solving either \((2)\) or \((3)\) with efficient algorithms, and obtaining a candidate minimizer from a moment matrix \(\Sigma\).</p>



<p class="justify-text"><strong>Convex optimization algorithms.</strong> Standard interior point methods for semi-definite programming problems can be used for either \((2)\) or \((3)\). This will become slow when the dimension \(d\) of the feature map \(\varphi(x)\) gets large, as each iteration will be of complexity \(O(d^3)\). Moreover, encoding the vector space \(\mathcal{V}\) can be rather painful when \(\mathcal{X}\) has many dimensions. For efficient descriptions of \(\mathcal{V}\) based on sampling, see the kernel section below. For efficient algorithms based on smoothing the minimal eigenvalue, see the bottom of the post.</p>



<p class="justify-text"><strong>Recovering a minimizer. </strong>This is often referred to as &#8220;extraction&#8221; in the SOS literature [<a href="https://homepages.laas.fr/henrion/papers/extract.pdf">29</a>], and a variety of techniques exist. Note also that randomized techniques which are classical in combinatorial optimization can also be considered [<a href="https://dl.acm.org/doi/pdf/10.1145/227683.227684">30</a>].</p>



<h2>Univariate trigonometric polynomials</h2>



<p class="justify-text">We first start with the SOS view, by showing that all non-negative trigonometric polynomials are sums-of-squares, and here a single square.</p>



<p class="justify-text"><strong>Fejér-Riesz theorem</strong> [11, 12] (if you can read German). We consider a trigonometric polynomial of degree \(r\), that is, \(g(x) = \sum_{\omega = -r}^r c_\omega e^{2i\pi \omega x}\) with real non-negative values. This imposes that \(c_{-\omega} = c_\omega^\ast\). Then it is non-negative if and only if it can be written as the square modulus of a complex-valued trigonometric polynomial. The elementary proof based on roots of polynomials is shown at the end of the post.</p>



<p class="justify-text">We here get a single square, with an elementary proof. Note that this is a stronger result than being a sum of several squares. We can now look at the dual intepretation.</p>



<p class="justify-text"><strong>Positivity of Hermitian Toeplitz matrices.</strong> As mentioned earlier, our approximation set \(\widehat{\mathcal{K}}\) for \(\mathcal{K}\) is the set of PSD Toeplitz matrices of unit trace. We have a tight relaxation if we can show that for any such matrix \(\Sigma\), there exists a probability measure \(\mu\) on \([0,\! 1]\) such that $$ \Sigma_{\omega\omega&#8217;}= \int_{0}^1 e^{2 i \pi (\omega \, -\,  \omega&#8217;)x} d\mu(x)$$ for all \(\omega,\omega&#8217;\). This is a classical result in Toeplitz matrix theory, from [13] (if you read German), well summarized in [14].</p>



<p class="justify-text"><strong>Comparison with spectral relaxation.</strong> Assume we are given a trigonometric polynomial of degree \(2r\), that is, \(f(x) = \sum_{\omega = -2r}^{2r} c_\omega e^{2i\pi \omega x}\). As mentioned earlier, there are many ways of representing it as a quadratic form \(\varphi(x)^\ast F \varphi(x)\) with \(\varphi(x)_\omega = e^{2i\pi \omega x}/\sqrt{2r+1}\) for \(\omega \in \{-r,\dots,r\}\). This will not change the SOS relaxation but will make a difference for the spectral relaxation. One standard one is to choose \(F\) to be a Toeplitz matrix. Since \(F_{\omega\omega&#8217;}\) should depend only on \(\omega-\omega&#8217;\), by counting the number of elements in each diagonal, we must have $$\tag{4} F_{\omega \omega&#8217;} = c_{\omega\, -\, \omega&#8217;} \frac{2r+1}{2r+1-|\omega\, -\, \omega&#8217;|}= c_{\omega\, -\, \omega&#8217;} \frac{1}{1-|\omega\, -\, \omega&#8217;|/(2r+1)}.$$</p>



<p class="justify-text">The spectral relaxation amounts to compute the lowest eigenvalue of a Toeplitz matrix, which is a well studied problem [<a href="https://ee.stanford.edu/~gray/toeplitz.pdf">9</a>], in particular when the dimension of the matrix grows. In our context, this corresponds to considering a feature \(\varphi\) of size \(s\) larger than \(r\) (like we will do for multivariate polynomials below), representing the function \(f\) in this basis through a Toeplitz matrix \(F\) of size \((2s+1)\times(2s+1)\) with the same formula as \((4)\), and seeing how the spectral relaxation tends to the optimal value when \(s\) goes to infinity.</p>



<p class="justify-text">We first consider the Toeplitz matrices with values \(c_{\omega-\omega&#8217;}1_{|\omega-\omega&#8217;|\leqslant 2r}\) for \(|\omega|, |\omega&#8217;| \leqslant s\), which is, a band-diagonal Toeplitz matrix. it is shown in [31, Section 5.4] that the lowest eigenvalue of this Toeplitz matrix has the following asymptotic expansion \(f(x_\ast) +  \frac{f^{\prime\prime}(x_\ast)}{32 s^2}\) where \(x_\ast \in [0,\! 1]\) is the minimizer of \(f\) (assumed to be unique). We thus get an upper approximation of \(O(1/s^2)\). By taking into account the multiplicative factor \(\frac{1}{1-|\omega-\omega&#8217;|/(2s+1)}\) from Eq. \((4)\), we get an overall factor of \(O(1/s)\), which is what we observe in practice (see simple experiment below).</p>



<p>The spectral relaxation is convergent, but much slower than the SOS relaxation (which is tight for \(r\geqslant s\)).</p>


<div class="wp-block-image justify-text">
<figure class="aligncenter size-large is-resized"><img loading="lazy" src="https://francisbach.com/wp-content/uploads/2022/10/SOS_1d_blog-1024x401.png" alt="" class="wp-image-8339" width="633" height="247" srcset="https://francisbach.com/wp-content/uploads/2022/10/SOS_1d_blog-1024x401.png 1024w, https://francisbach.com/wp-content/uploads/2022/10/SOS_1d_blog-300x117.png 300w, https://francisbach.com/wp-content/uploads/2022/10/SOS_1d_blog-768x300.png 768w, https://francisbach.com/wp-content/uploads/2022/10/SOS_1d_blog-1536x601.png 1536w, https://francisbach.com/wp-content/uploads/2022/10/SOS_1d_blog-850x332.png 850w, https://francisbach.com/wp-content/uploads/2022/10/SOS_1d_blog.png 1846w" sizes="(max-width: 633px) 100vw, 633px" /><figcaption>Left: trigonometric polynomial \(f\) of degree 4. Right: performance of the spectral relaxation, which requires a large degree to be a close approximation (with observed rate \(1/s\)). Here the SOS relaxation for \(s = r\) is already tight.</figcaption></figure></div>


<p class="justify-text"><strong>Consequences for regular polynomials on \([-1,1]\).</strong> Non-negative polynomials \(P\) on \([-1,1]\) can also be characterized, by looking at the non-negativity of \(f(x) = P(\cos 2 \pi x)\) for \(x \in [0,\! 1]\), which is equivalent to the existence of a complex trigonometric polynomial whose square modulus is exactly \(f\). Playing around with <a href="https://en.wikipedia.org/wiki/Chebyshev_polynomials">Chebyshev polynomials of the two kinds</a>, one can show that \(P(y)\) then has to a sum of terms of the form \(Q(y)^2 + (1-y^2) R(y)^2\), where \(Q\) and \(R\) are polynomials, which is the classical SOS characterization of non-negative polynomials in \([-1,1]\) [<a href="https://www.ams.org/journals/tran/2000-352-10/S0002-9947-00-02595-2/S0002-9947-00-02595-2.pdf">15</a>]. See all details and the extension to higher dimensions in [<a href="https://arxiv.org/pdf/2211.04889.pdf">33</a>].</p>



<h2>Extension to multivariate trigonometric polynomials</h2>



<p class="justify-text">We now consider multivariate trigonometric polynomials in dimension \(n\), on \([0,\! 1]^n\), which are polynomials in \(\cos 2\pi x_1, \sin 2\pi x_1, \dots, \cos 2\pi x_n, \sin 2 \pi x_n\). We first start with a counter-example to the equivalence between non-negativity and being a sum of squares.</p>



<p class="justify-text"><strong>Negative result (counter-example). </strong>Non-negative polynomials may not be sums-of-squares, as shown by this counter example based on Motzkin&#8217;s example [22]: $$M(1-\cos 2 \pi x_1,1-\cos 2\pi x_2, 1 &#8211; \cos 2 \pi x_3)$$ $$\mbox{ with } M(y_1,y_2,y_3) = y_1^2 y_2  + y_1  y_2^2 + y_3^3 \, &#8211; 3 y_1  y_2  y_3.$$ It is non-negative as (up to a factor of \(1/3\)) the difference between the arithmetic and geometric mean of \(y_1^2 y_2\), \(y_1  y_2^2\), and \(y_3^3\), and not a sum of squares.  Note that to check that \(f\) is not a sum-of-squares of polynomials <em>of a given degree</em>, we can solve an SDP and look at the smallest \(c \geqslant 0\) such that \(f + c\) is a sum-of-squares, and check that the optimal \(c\) is strictly greater than zero (we here obtain \(c \approx 1.1 \times 10^{-4}\) for degree \(2\) polynomials). It is a bit more involved to show that there is no SOS polynomials of any degree (cancellation of terms are possible so the degree can be higher than the one of \(f\)), see [22].</p>



<p>So not all non-negative polynomials are sums-of-squares in dimension larger than 1. This is however not the end of the story, as we now describe.</p>



<p class="justify-text"><strong>Towards positive results. </strong>We consider a trigonometric polynomial, that is, a function \(f(x) = P(e^{2i\pi x_1},\dots,e^{2i\pi x_n})\) where \(P\) is a function of the form $$P(z) = \sum_{ \omega \in \mathbb{Z}^d} c_\omega z^\omega, $$ where only a finite number of \(c_\omega\)&#8217;s are not equal to zero (we use the notation \(z^\omega = \prod_{i=1}^n z_i^{\omega_i}\)). Like in the one-dimensional case, the function \(f\) is real-valued as soon as \(c_{-\omega} = c_\omega^\ast\) for all \(\omega \in \mathbb{Z}^n\). These are exactly multivariate (real-valued) trigonometric polynomials.</p>



<p class="justify-text">We now state a few results relating non-negativity and sums-of-squares, whose &#8220;elementary&#8221; proofs will either be sketched at the end of this post (below references) or available in [33].</p>



<p class="justify-text"><strong>Strictly positive polynomials are sums-of-squares.</strong> For any multivariate trigonometric polynomial \(f\) of the form above, if \(f\) is <em>strictly positive</em> on \([0,\! 1]^n\), then there exists a finite family of trigonometric polynomials \((g_i)_{i \in I}\)  such that $$ \forall x \in [0,\! 1]^n, \ f(x) = \sum_{i \in I} |g_i(x)|^2.$$ This result dates back to [28], and has a reasonably simple proof based on Bochner&#8217;s theorem (see at the bottom of the post the proof from [23]).</p>



<p class="justify-text">There are two key deviations from the univariate case: </p>



<ol class="justify-text"><li>The degrees of the squares \(|g_i|^2\) can be larger than the degree of \(f\) and are not bounded a priori, thus, we need to try sums of squares of degrees larger than the degree of \(f\), thus creating a <em>hierarchy</em> of optimization problems of higher dimensions, whose optimal value will converge to the minimum of \(f\).</li><li>Only <em>strictly positive</em> polynomials can always be decomposed, implying we may not have finite convergence of the hierarchy in general.</li></ol>



<p class="justify-text">It implies that SOS hierarchies will always converge to the true value, without giving any idea of the convergence rate. The next result provides such a result (but still quite pessimistic), without any assumption.</p>



<p class="justify-text"><strong>Adding an arbitrarily small constant to a non-negative trigonometric polynomial makes it an SOS of known degree.</strong> Given a multivariate trigonometric polynomial of degree less than \(2r\) with values in \([0,\! 1]\), \(f + c\) is a sum of squares of polynomials of degree less than \(s \geqslant r\) as soon as \(c \geqslant c^\ast(r,s)\) for some \(c^\ast(r,s)\). Moreover, when \(s \to +\infty\), we have recently shown in [33] that, asymptotically, \(c^\ast(r,s) \leqslant \frac{6 r^2 n}{s^2} \sum_{\omega \in \mathbb{Z}^n} | \hat{f}(\omega)|\). This immediately provides the convergence rate \(c^\ast(r,s)\) for the SOS relaxation of degree \(s\).</p>



<p class="justify-text">This results in an adaptation of<span style="font-size: revert; color: initial; font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen-Sans, Ubuntu, Cantarell, &quot;Helvetica Neue&quot;, sans-serif;"> [20] to \([0,\! 1]^n\) rather than the hypersphere, which makes the proof significantly faster (see [<a href="https://arxiv.org/pdf/2211.04889.pdf">33</a>] and the related results for regular polynomials on \([-1,1]^n\) in [<a href="https://link.springer.com/content/pdf/10.1007/s11590-022-01922-5.pdf">34</a>]). It provides a quantitative </span>guarantee (<span style="font-size: revert; color: initial; font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen-Sans, Ubuntu, Cantarell, &quot;Helvetica Neue&quot;, sans-serif;">even more explicit than for regular polynomials [26])</span>: the approximation guarantee scales as \(s^{-2}\), with a feature dimension \(d = (2s+1)^n \sim s^n\), and thus with a feature of dimension \(d\), we get an error proportional to \(d^{-2/n}\), thus with a scaling in the underlying dimension \(n\) subject to the curse of dimensionality.</p>



<p class="justify-text"><strong>Comparison with spectral relaxation.</strong> Using the same reasoning as in dimension \(n=1\), we get instead a convergence rate in \(O(1/s)\) for the spectral algorithm. So we get convergence, at rate which is only slightly worse. However, the behavior in \(O(1/s)\) is empirically observed in practice in all cases, while the rate in \(O(1/s^2)\) for the SOS relaxation is pessimistic, and the convergence rate is <em>much</em> better in practice. Can we obtain tighter convergence rates?</p>



<h2 class="justify-text">Exponential convergence</h2>



<p class="justify-text">In order to show exponential convergence when the degree \(s\) goes to infinity, we need to add an extra assumption regarding the behavior of the function \(f\) around its minimizer. Essentially, all global minimizers are isolated and the Hessian is invertible. This is often referred to as &#8220;strict-second-order minimizers&#8221;, and such assumptions have already been used to show finite convergence of the hierarchy, although with no bound on the required degree [<a href="https://link.springer.com/content/pdf/10.1007/s10107-013-0680-x.pdf">35</a>].</p>



<p class="justify-text">Leveraging our earlier work [21, 24], we recently showed in [<a href="https://arxiv.org/pdf/2211.04889.pdf">33</a>] with Alessandro Rudi that the convergence rate was exponential when the minimum is attained at isolated points with invertible Hessians. The proof is a bit involved, but rely on a crucial point. We deviate from previous work on polynomial hierarchies by a strong focus on <em>smoothness properties</em> of the optimization problems rather than its algebraic properties. More precisely, this allows us to use square roots and matrix square roots together with their differentiability properties (square roots typically lead to non-polynomial functions when taken on polynomials).</p>



<h2>Optimization beyond trigonometric polynomials</h2>



<p class="justify-text">When \(f\) is not a trigonometric polynomial, there are essentially two options: (1) approximate with a trigonometric polynomial and minimize the approximation with the SOS hierarchy, or (2) target directly the minimization of the original function through sampling. The second option is described in the next section (and was already the topic of an <a href="https://francisbach.com/finding-global-minima-with-kernel-approximations/">earlier blog post</a>). We now describe the first option.</p>



<p class="justify-text"><strong> Approximation by truncated Fourier series.</strong> Depending on how the function is accessed, through function values or through values of its Fourier series, the approximation of a regular function \(f\) through a degree \(r\) polynomial is a well-studied problem. For simplicity, I only consider truncating the Fourier series obtained by keeping frequencies \(\omega\) such that \(\| \omega\|_\infty \leqslant r\). The error which is made depends on the regularity of the original function, that is, if \(f\) has all of its \((\beta + n)\)-th order derivatives that are bounded, then the truncation error is of order \(1/r^{\beta}\). Thus, to reach precision \(\varepsilon\), we need \(d \sim r^n \sim \varepsilon^{-n/\beta}\) basis functions. See an illustration below in two dimensions, where aim to approximate the function below.</p>


<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img loading="lazy" src="https://francisbach.com/wp-content/uploads/2022/11/bike-1.gif" alt="" class="wp-image-8433" width="447" height="356"/></figure></div>


<p class="justify-text">The function is approximated by computing the Fourier series and keeping only frequencies \(\omega\) such that \(\| \omega \|_\infty \leqslant r\) for varying \(r\).</p>


<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img loading="lazy" src="https://francisbach.com/wp-content/uploads/2022/11/velo_continuous-1.gif" alt="" class="wp-image-8434" width="388" height="375"/></figure></div>


<p></p>



<h2>Extension to function value oracle</h2>



<p class="justify-text">Now that the traditional &#8220;décor&#8221; of sum-of-squares optimization based on finite-dimensional expansions has been set, we can look at recent infinite-dimensional extensions based on positive definite kernels.</p>



<p class="justify-text"><strong>From knowing \(\mathcal{V}\) to exact sampling.</strong> In the formulation above,&nbsp;instead of using an explicit representation of \(f(x)\) as \(\varphi(x)^\ast F \varphi(x)\), and determine precisely \(\mathcal{V}\), an alternative is to use sufficiently many distinct points \(x_1,\dots,x_m \in \mathcal{X}\), and use the constraint: $$ \forall i \in \{1,\dots,m\}, \ f(x_i) -c = \varphi(x_i)^\ast A \varphi(x_i). $$ If the matrices \(\varphi(x_i) \varphi(x_i)^\ast\), \(i \in \{1,\dots,m\}\), span \(\mathcal{V}\), this is equivalent to the original formulation [<a href="https://epubs.siam.org/doi/pdf/10.1137/15M1052548">25</a>], and leads to formulations that are less cumbersome to code (multivariate Toeplitz matrices are an example). An extra advantage is to access \(f\) only through a function-value oracle (often called a zero-th order oracle). Note that we often need to take \(m\) larger than the dimension \(2d\) (the real dimension of \(\mathbb{C}^d\)), but always less than \(d^2\) (the real dimension of the set of Hermitian matrices).</p>



<p><strong>Undersampling and regularization.</strong> A key insight from [<a href="https://www.di.ens.fr/~fbach/gloptikernel.pdf">21</a>] is to consider situations where the number of samples \( m\) is not large enough, and add a regularizer \( -\lambda {\rm tr}(A)\), that is,&nbsp;$$ \sup_{c \in \mathbb{R}, \ A \succcurlyeq 0} c\,  &#8211; \lambda {\rm tr}(A) \&nbsp; \mbox{ such that } \ \forall i \in \{1,\dots,m\}, \ f(x_i) -c = \varphi(x_i)^\ast A \varphi(x_i).$$ The dual is then: $$  \inf_{\alpha \in \mathbb{R}^m} \sum_{i=1}^m \alpha_i f(x_i) \mbox{ such that } \sum_{i=1}^m \alpha_i = 1, \ \sum_{i=1}^m \alpha_i \varphi(x_i)\varphi(x_i)^\ast + \lambda I \succcurlyeq 0. $$</p>



<p class="justify-text">When \( m\) is large enough and \( \lambda\) is sufficiently small, this leads to a controlled approximation. This is particularly interesting when \(\varphi(x)\) is infinite-dimensional and such that&nbsp;the relaxation is tight, which we now look at.</p>



<p class="justify-text"><strong>Infinite-dimensional feature map.</strong> In the trigonometric polynomial example, we can consider $$\varphi(x)_\omega = \hat{q}(\omega)^{1/2} e^{2i \pi \omega^\top x},$$ with an extra weight function \(\hat{q}: \mathbb{Z}^n \to \mathbb{R}_+\), such that \(\sum_{\omega \in \mathbb{Z}^n} \hat{q}(\omega) = 1\), so that \(\| \varphi(x)\| = 1\) (we essentially get a probability distribution on \(\mathbb{Z}^n\)). Finite-dimensional embeddings correspond to \(\hat{q}\) having finite support, but we can now to apply the SOS technique in infinite dimensions, since \(\hat{q}\) may have infinitely many non-zero values.  </p>



<p class="justify-text"><strong>Tightness.</strong> A key benefit of going infinite-dimensional is that the SOS relaxation is always tight as soon as \(\hat{q}(\omega) &gt;0\) for all \(\omega \in \mathbb{Z}^d\). Indeed, the relaxation is the following optimization problem: $$\inf_{ \mu \ {\rm finite \ measure} } \int_{\mathcal{X}} f(x) d\mu(x) \ \mbox{ such that } \ \int_{\mathcal{X}} \varphi(x)\varphi(x)^\ast d\mu(x) \succcurlyeq 0, \ \int_\mathcal{X} d\mu(x) = 1.$$ Looking at all finite sub-matrices of \(\int_{\mathcal{X}} \varphi(x)\varphi(x)^\ast d\mu(x)\), we see that the function \(\omega \mapsto \int_{[0, 1]^n} e^{2i\pi \omega^\top x} d\mu(x)\) is positive definite, and thus by <a href="https://en.wikipedia.org/wiki/Bochner%27s_theorem">Bochner&#8217;s theorem</a>, \(\mu\) has to be a probability measure.</p>



<p class="justify-text">We can now combine with &#8220;sampling + regularization&#8221; technique mentioned above, and only query the function at a finite number of points. As shown in [21, 24], with a proper choice of \(\lambda\), then we get a convergence rate of \(O(1/m^{-\beta/n})\) when \(f\) is \((\beta + 2+ n/2)\)-times differentiable. When \(f\) is infinitely-differentiable (like for functions with finite support), we can get exponential convergence in \(m\), but with a constant in from of the exponential that can be exponential in dimension. See more details in an <a href="https://francisbach.com/finding-global-minima-with-kernel-approximations/">earlier blog post</a> and in [33].</p>



<p class="justify-text"><strong style="font-size: revert; color: initial; font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen-Sans, Ubuntu, Cantarell, &quot;Helvetica Neue&quot;, sans-serif;">Representer theorem. </strong><span style="font-size: revert; color: initial; font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen-Sans, Ubuntu, Cantarell, &quot;Helvetica Neue&quot;, sans-serif;">Another consequence is a &#8220;representer theorem&#8221;, that is, we can reduce the search to \(A = \sum_{i,j=1}^m C_{ij} \varphi(x_i)\varphi(x_j)^\ast\) with \( C \in  \mathbb{R}^{m \times m}\) positive-semidefinite. </span>Like for traditional kernel methods, the representer theorem is particularly interesting when \(d\) is infinite like above.</p>



<p class="justify-text">But this is already useful with finite-dimensional embeddings where the span \(\mathcal{V}\) is difficult to characterize. In this case, only the kernel has to be simple to compute; for example, for the usual finite support case, we have $$k(x,y) = \frac{1}{2r+1} \frac{ \sin (2r+1) \pi x }{\sin \pi x},$$ in one dimension (this extends to all dimensions). We can also get a closed form for \(\hat{q}(\omega) = \rho^{\|\omega\|_1}\).</p>



<h2>Extension to Boolean hypercube</h2>



<p class="justify-text">When \(\mathcal{X} = \{-1,1\}^n \), we can use feature vectors composed of Boolean Fourier components of increasing orders [<a href="https://arxiv.org/pdf/2105.10386.pdf">16</a>]. This corresponds to features of the form \(\varphi_A(x) = \prod_{i \in A} x_i \in \{-1,1\}\), where \(A\) is a subset of \(\{1,\dots,n\}\). They are all normalized.</p>



<p class="justify-text">If we consider a set \( \mathcal{A}\) of subsets of \( \{1,\dots,n\}\), then, the element indexed \((A,B)\) of \(\varphi(x) \varphi(x)^\ast\) only depends on the symmetric difference \(A \Delta B = ( A \backslash B) \cup ( B \backslash A)\). The relaxation is not tight in general, but if we see our moment matrix as a submatrix obtained from a sufficiently larger set of subsets, then we obtain a tight formulation (here we know that the hierarchy has to be finite [<a href="https://link.springer.com/content/pdf/10.1007/3-540-45535-3.pdf">17</a>, <a href="https://pubsonline.informs.org/doi/pdf/10.1287/moor.28.3.470.16391">18</a>, <a href="https://arxiv.org/pdf/2011.04027.pdf">19</a>]). </p>



<p class="justify-text">At the first layer (with \( \mathcal{A}\) composed of singletons), we get the traditional semi-definite programming relaxation of quadratic optimization problems on \(\{-1,1\}^n\) [<a href="https://dl.acm.org/doi/pdf/10.1145/227683.227684">30</a>]. See this nice <a href="https://www.sumofsquares.org/">website</a> for the many extensions. An interesting aspect is that tightness is not necessary in many applications.</p>



<h2>Extension to hyperspheres</h2>



<p class="justify-text">It is common to see the set \([0,\! 1]\) with periodic functions as the one-dimensional <a href="https://en.wikipedia.org/wiki/Torus">torus</a>, that is, the quotient set \(\mathbb{R} / \mathbb{Z}\). Another classical interpretation is to see it as the unit circle in \(\mathbb{R}^2\), with the bijection \(x \mapsto (\cos 2\pi x, \sin 2 \pi x)\). The real Fourier basis functions \(x \mapsto \cos 2 \pi \omega x\) and \(x \mapsto \sin 2 \pi \omega x\) are then polynomials \(P(y_1,y_2)\) in \(y_1 = \cos 2\pi x\) and \(y_2 = \sin 2\pi x\). Not all polynomials are recovered, as one can check that we need to have \(\Delta P = \frac{\partial^2 P}{\partial y_1^2} +  \frac{\partial^2 P}{\partial y_2^2}=0\), that is the Laplacian of \(P\) is zero, which is often referred to as being <a href="https://en.wikipedia.org/wiki/Harmonic_polynomial">harmonic</a>. This extends to hyperspheres in any dimension (see [<a href="https://arxiv.org/pdf/1908.05155.pdf">20</a>] for details), and also to Euclidean balls by adding a dimension.</p>



<h2>Conclusion</h2>



<p class="justify-text">In this blog post, I have tried to describe the basics of sums-of-squares optimization, hopefully from a simple perspective. The key was to reduce the scope to the minimization over simple sets with no complex constraints. This allowed us to present significantly improved convergence results from [33].</p>



<p class="justify-text">There is of course many more interesting ideas to cover, such as constrained optimisation, applications within signal processing [8], and extensions to the computation of log-partition functions [32], as well as important practical scalability issues to alleviate. Moreover, the SOS framework goes beyond optimization, either through other infinite-dimensional optimization problems or moment-based formulations (see many examples in [4, 7]). Many topics for future posts!</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Alessandro Rudi for proofreading this blog post and making good clarifying suggestions.</p>



<p>&nbsp;</p>



<h2>References</h2>



<p class="justify-text">[1] Jean-Bernard Lasserre. <a href="https://epubs.siam.org/doi/pdf/10.1137/S1052623400366802">Global optimization with polynomials and the problem of moments</a>. <em>SIAM Journal on Optimization</em>, 11(3):796–817, 2001.<br>[2] Pablo A. Parrilo. <a href="http://www.mit.edu/~parrilo/pubs/files/SDPrelaxations.pdf">Semidefinite programming relaxations for semialgebraic problems</a>. <em>Mathematical Programming</em>, 96(2):293–320, 2003.<br>[3] Monique Laurent. <a href="https://homepages.cwi.nl/~monique/files/laurent-ima.pdf">Sums of squares, moment matrices and optimization over polynomials</a>. In <em>Emerging applications of algebraic geometry</em>. Springer, 157-270, 2009.<br>[4] Jean-Bernard Lasserre.&nbsp;<em><a href="https://www.worldscientific.com/worldscibooks/10.1142/p665#t=aboutBook">Moments, positive polynomials and their applications</a></em>. World Scientific, 2009.<br>[5] Grigoriy Blekherman, Pablo A. Parrilo, Rekha R. Thomas, editors.&nbsp;<em><a href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611972290.fm">Semidefinite optimization and convex algebraic geometry</a></em>. Society for Industrial and Applied Mathematics, 2012.<br>[6] Jean-Bernard Lasserre.&nbsp;<em><a href="https://www.cambridge.org/core/books/an-introduction-to-polynomial-and-semialgebraic-optimization/5A7C6F7E54E28CE72BA7A7F84D0858ED">An introduction to polynomial and semi-algebraic optimization</a></em>. Cambridge University Press, 2015.<br>[7] Didier Henrion, Milan Korda, and Jean-Bernard Lasserre.&nbsp;<em><a href="https://www.worldscientific.com/worldscibooks/10.1142/q0252#t=aboutBook">The Moment-SOS Hierarchy: Lectures In Probability, Statistics, Computational Geometry, Control And Nonlinear PDEs</a></em>. World Scientific, 2020.<br>[8] Bogdan Dumitrescu.&nbsp;<em><a href="https://link.springer.com/book/10.1007/978-3-319-53688-0">Positive trigonometric polynomials and signal processing applications</a></em>. Springer, 2007.<br>[9] Robert M. Gray. <a href="https://ee.stanford.edu/~gray/toeplitz.pdf">Toeplitz and circulant matrices: A review</a>. <em>Foundations and Trends in Communications and Information Theory</em>&nbsp;2(3): 155-239, 2006.<br>[10] Johannes Jahn. <em><a href="https://link.springer.com/book/10.1007/978-3-540-49379-2">Introduction to the Theory of Nonlinear Optimization</a></em>. Springer, 2020<br>[11] Leopold Fejér. <a href="https://www.degruyter.com/document/doi/10.1515/crll.1916.146.53/pdf">Über trigonometrische Polynome</a>. <em>Journal für die reine und angewandte Mathematik</em>. 146:55-82, 1916.<br>[12] Friedrich Riesz. <a href="https://www.degruyter.com/document/doi/10.1515/crll.1916.146.83/pdf">Über ein Problem des Herrn Carathéodory</a>. <em>Journal für die reine und angewandte Mathematik</em>. 146:83-87, 1916.<br>[13] C. Carathéodory and L. Fejér. <a href="https://ia800708.us.archive.org/view_archive.php?archive=/28/items/crossref-pre-1923-scholarly-works/10.1007%252Fbf02983491.zip&amp;file=10.1007%252Fbf03014796.pdf">Über den Zusammenhang der Extreme von harmonischen Funktionen mit ihren Koeffizienten und über den Picard-Landauschen Satz</a>, Rendiconti del Circolo Matematico di Palermo, 32:218–239, 1911.<br>[14] Mihály Bakonyi, Ekaterina V. Lopushanskaya. <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-0346-0180-1_4.pdf">Moment problems for real measures on the unit circle</a>.&nbsp;<em>Recent Advances in Operator Theory in Hilbert and Krein Spaces</em>. Birkhäuser Basel, 49-60, 2009.<br>[15] Victoria Powers and Bruce Reznick. <em><a href="https://www.ams.org/journals/tran/2000-352-10/S0002-9947-00-02595-2/S0002-9947-00-02595-2.pdf">Polynomials that are positive on an interval</a></em>. Transactions of the American Mathematical Society, 352(10):4677–4692, 2000.<br>[16] Ryan O’Donnell. <em><a href="https://arxiv.org/pdf/2105.10386.pdf">Analysis of Boolean functions</a></em>. Cambridge University Press, 2014.<br>[17] Jean-Bernard Lasserre. <a href="https://link.springer.com/content/pdf/10.1007/3-540-45535-3.pdf">An explicit exact SDP relaxation for nonlinear 0–1 programs</a>. In <em>International Conference on Integer Programming and Combinatorial Optimization</em>, pages 293–303. Springer, 2001.<br>[18] Monique Laurent. <a href="https://pubsonline.informs.org/doi/pdf/10.1287/moor.28.3.470.16391">A comparison of the Sherali-Adams, Lovász-Schrijver, and Lasserre relaxations for 0–1 programming</a>. <em>Mathematics of Operations Research</em>, 28(3):470–496, 2003.<br>[19] Lucas Slot and Monique Laurent. <a href="https://arxiv.org/pdf/2011.04027.pdf">Sum-of-squares hierarchies for binary polynomial optimization</a>. <em>Mathematical Programming</em>, pages 1–40, 2022.<br>[20] Kun Fang and Hamza Fawzi. <a href="https://arxiv.org/pdf/1908.05155.pdf">The sum-of-squares hierarchy on the sphere and applications in quantum information theory</a>. <em>Mathematical Programming</em>, 190(1):331–360, 2021.<br>[21] Alessandro Rudi, Ulysse Marteau-Ferey, Francis Bach. <a href="https://www.di.ens.fr/~fbach/gloptikernel.pdf">Finding Global Minima via Kernel </a><a href="https://arxiv.org/pdf/2012.11978">Approximations</a>. Technical report arXiv:2012.11978, 2020.<br>[22] Aron Naftalevich, and M. Schreiber. <a href="https://link.springer.com/content/pdf/10.1007/BFb0074598.pdf">Trigonometric polynomials and sums of squares</a>. In <em>Number Theory</em>. Springer, 225-238, 1985.<br>[23] Alexandre Megretski. <a href="http://www.mit.edu/~ameg/images/sos_cdc.ps">Positivity of trigonometric polynomials</a>.&nbsp;In <em>International Conference on Decision and Control</em>, 2003.<br>[24] Blake Woodworth, Francis Bach, Alessandro Rudi.&nbsp;<a href="https://proceedings.mlr.press/v178/woodworth22a/woodworth22a.pdf">Non-Convex Optimization with Certificates and Fast Rates Through Kernel Sums of Squares</a>. In <em>Proceedings of the Conference on Learning Theory</em>, 2022.<br>[25] Diego Cifuentes, Pablo A. Parrilo. <a href="https://epubs.siam.org/doi/pdf/10.1137/15M1052548">Sampling algebraic varieties for sum of squares programs</a>.&nbsp;<em>SIAM Journal on Optimization</em>,&nbsp;27(4): 2381-2404, 2017.<br>[26] Jean-Bernard Lasserre. <a href="https://epubs.siam.org/doi/pdf/10.1137/070693709">A sum of squares approximation of nonnegative polynomials</a>.&nbsp;<em>SIAM Review</em>,&nbsp;49(4):651-669, 2007.<br>[27] Yurii Nesterov. <a href="https://link.springer.com/content/pdf/10.1007/s10107-006-0001-8.pdf">Smoothing technique and its applications in semidefinite optimization</a>.&nbsp;<em>Mathematical Programming</em>&nbsp;110(2):245-259, 2007.<br>[28] Mihai Putinar. <a href="https://gallica.bnf.fr/ark:/12148/bpt6k58688425/f747.item">Sur la complexification du problème des moments</a>. <em>Comptes Rendus de l&#8217;Académie des Sciences, </em>Série 1, Mathématique&nbsp;314(10):743-745, 1992.<br>[29] Didier Henrion and Jean-Bernard Lasserre. <a href="https://homepages.laas.fr/henrion/papers/extract.pdf">Detecting global optimality and extracting solutions in GloptiPoly</a>. In&nbsp;<em>Positive polynomials in control</em>, pages 293-310, 2005.<br>[30] Michel X. Goemans and David P. Williamson. <a href="https://dl.acm.org/doi/pdf/10.1145/227683.227684">Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming</a>. <em>Journal of the ACM</em>, 42(6):1115–1145, 1995.<br>[31] Ulf Grenander, and Gabor Szegö.&nbsp;<em>Toeplitz forms and their applications</em>. Univ of California Press, 1958.<br>[32] Francis Bach.&nbsp;<a href="https://arxiv.org/pdf/2206.13285">Sum-of-Squares Relaxations for Information Theory and Variational Inference</a>. Technical report, arXiv:2206.13285, 2022.<br>[33] Francis Bach and Alessandro Rudi. <a href="https://arxiv.org/pdf/2211.04889.pdf">Exponential convergence of sum-of-squares hierarchies for trigonometric polynomials</a>. Technical report arXiv:2211.04889, 2022.<br>[34] Monique Laurent and Lucas Slot. <a href="https://link.springer.com/content/pdf/10.1007/s11590-022-01922-5.pdf">An effective version of Schmüdgen’s Positivstellensatz for the hypercube</a>. <em>Optimization Letters</em>, 2022.<br>[35] Jiawang Nie. <a href="https://link.springer.com/content/pdf/10.1007/s10107-013-0680-x.pdf">Optimality conditions and finite convergence of Lasserre’s hierarchy</a>. <em>Mathematical Programming</em>, 146(1):97–121, 2014</p>



<p class="justify-text"><br></p>



<h2>Proof of Fejér-Riesz theorem</h2>



<p class="justify-text">We consider a trigonometric polynomial of degree \(r\), that is, \(g(x) = \sum_{\omega = -r}^r c_\omega e^{2i\pi \omega x}\) with real non-negative values. This imposes that \(c_{-\omega} = c_\omega^\ast\). We assume that \(c_r \neq 0\). We consider the rational function \(R(z) = \sum_{\omega = -r}^r c_\omega z^\omega\), so that \(g(x) = R(e^{2i\pi x})\). We can write \(R\) as \(R(z) = z^{-r} Q(z)\) for a polynomial \(Q\) of degree \(2r\). We can now examine the roots of \(R\). The identity \(c_{-\omega} = c_\omega^\ast\) implies that if \(z\) is a root of \(Q\), so is \(1/z^\ast\). Thus all roots which do not have unit modulus come in pairs \((z_b,1/z_b^\ast)\), for \(| z_b| &lt; 1\), with \(b \in \{1,\dots,m\}\). We then have \(2r \, &#8211; 2m\) roots of unit modulus, denoted \(e^{2i\pi \rho_a}\), for \(a \in \{1,\dots,2r-2m\}\).</p>



<p class="justify-text">Thus, we can write the polynomial \(Q\) as: $$Q(z) = \kappa \prod_{a=1}^{2r-2m} ( z  \, &#8211; e^{2i\pi \rho_a})  \prod_{b = 1}^m ( z \, &#8211; z_b)( z \, &#8211; 1/z_b^\ast), $$ for a certain \(\kappa \in \mathbb{C}\), leading to $$R(z) = z^{-r} Q(z) = \kappa&#8217; z^{-r+m} \prod_{a=1}^{2r-2m} ( z \, &#8211; e^{2i\pi \rho_a}) \prod_{b =1}^m ( z \, &#8211; z_b)( z^{-1}  \, &#8211;  z_b^\ast),$$ for some \(\kappa&#8217;  \in \mathbb{C}\). For \(z = e^{2i\pi x}\), the second product is a positive number since \(z^{-1} = z^\ast\). Writing \( e^{2i\pi x}  \, &#8211; e^{2i\pi \rho_a} = 2i e^{i\pi(x+\rho_a)} \sin \pi (x-\rho_a)\), we can see that in order to avoid a change of sign of \(\sin \pi (x-\rho_a)\) around some \(x\), each \(\rho_a\) must appear with even multiplicity. Thus, for \(z = e^{2i\pi x}\), the term \(z^{-r+m} \prod_{a=1}^{2r-2m} ( z \, &#8211; e^{2i\pi \rho_a})\) is equal to \(\prod_{a=1}^{r-m} (-4) e^{2 i\pi \rho_a} \sin^2 \pi (x-\rho_a)\), and thus, \(\kappa^{\prime\prime} = \kappa&#8217; \prod_{a=1}^{r-m} (-4)  e^{2 i\pi \rho_a}\) has to be a non-negative real number. Thus, overall, $$g(x) = R(e^{2i\pi x}) = \Big| \sqrt{\kappa^{\prime\prime}}  \prod_{a=1}^r (  e^{2i\pi x}  &#8211; e^{2i\pi \rho_a}) \prod_{b =1}^m ( e^{2i\pi x}  &#8211; z_b) \Big|^2.$$ </p>



<p class="justify-text">We here get a single square, with an elementary proof. Note that this is a stronger result than being a sum of several squares. We can now look at the dual intepretation.</p>



<h2>First-order algorithms</h2>



<p class="justify-text">In order to solve the problem in Eq. \((1)\), we can use standard smoothing [27], that is, replace \(\lambda_{\min}(F+Y)\) by \(\varepsilon \log {\rm tr } \exp((F+Y) / \varepsilon)\), and apply accelerated projected gradient descent, which requires to orthogonally project on \(\mathcal{V}^\perp\), or equivalently on \(\mathcal{V}^\perp\).</p>



<h2>All strictly positive trigonometric polynomials are sums-of-squares</h2>



<p class="justify-text">In order to provide an &#8220;elementary&#8221; proof of this result that dates back to [28], we follow [23] and proceed by contradiction, and assume that there is a strictly positive trigonometric polynomial \(R\) which is not a sum-of-squares. Since the set of SOS polynomials is a convex cone, by the <a href="https://en.wikipedia.org/wiki/Hahn%E2%80%93Banach_theorem">Hahn-Banach theorem</a>, there must exist a non identically zero linear form \(L\) which is non-negative on all SOS polynomials, and such that \(L(R) \leqslant 0\). In order to conclude, we will need to show that linear forms on trigonometric polynomials that are non-negative on SOS polynomials, can be written as  $$\tag{4} L(P)  = \int_{[0, 1]^n} P(e^{ 2i\pi x_1} ,\dots, e^{2i\pi x_n}) d\mu(x)$$ for a non-negative measure \(\mu\) on \([0,\! 1]^n\) (which is not uniformly equal to zero). This then leads to a contradiction, since it implies $$0 \geqslant L(R) \geqslant \min_{x \in [0, 1]^n} R(e^{ 2i\pi x_1} ,\dots, e^{2i\pi x_n})  \times \mu([0,\! 1]^n).$$</p>



<p class="justify-text">We now need to show that all bounded linear forms on trigonometric polynomials that are non-negative on SOS polynomials can be represented as in Eq. \((4)\).</p>



<p class="justify-text"><strong>Using <a href="https://en.wikipedia.org/wiki/Bochner%27s_theorem">Bochner&#8217;s theorem</a>.</strong> We define a function \(g: \mathbb{Z}^n \to \mathbb{C}\) as \(g(\omega) = L( z^\omega)\). This function is <a href="https://en.wikipedia.org/wiki/Positive-definite_function">positive definite</a>, as, for any \(\omega_1,\dots,\omega_m \in \mathbb{Z}^n\) and any complex numbers \(\alpha_1,\dots,\alpha_m\), $$ \sum_{i,j=1}^m \alpha_i \alpha_j^\ast g(\omega_i &#8211; \omega_j) = L \Big( \Big| \sum_{i=1}^m \alpha_i z^{\omega_i} \Big|^2 \Big).$$ Thus by Bochner&#8217;s theorem, there exists a positive measure \(\mu\) on \([0,\! 1]^n\) such that $$g(\omega) = \int_{[0, 1]^n} e^{2i\pi \omega^\top x} d\mu(x),$$ which exactly leads to the desired result.</p>
<p class="authors">By Francis Bach</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T12:53:49Z">Wednesday, November 16 2022, 12:53</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07691'>Low-depth arithmetic circuit lower bounds via shifted partials</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Prashanth Amireddy, Ankit Garg, Neeraj Kayal, Chandan Saha, Bhargav Thankey</p><p>We prove super-polynomial lower bounds for low-depth arithmetic circuits
using the shifted partials measure [Gupta-Kamath-Kayal-Saptharishi, CCC 2013],
[Kayal, ECCC 2012] and the affine projections of partials measure
[Garg-Kayal-Saha, FOCS 2020], [Kayal-Nair-Saha, STACS 2016]. The recent
breakthrough work of Limaye, Srinivasan and Tavenas [FOCS 2021] proved these
lower bounds by proving lower bounds for low-depth set-multilinear circuits. An
interesting aspect of our proof is that it does not require conversion of a
circuit to a set-multilinear circuit, nor does it involve a random restriction.
We are able to upper bound the measures for homogeneous formulas directly,
without going via set-multilinearity. Our lower bounds hold for the iterated
matrix multiplication as well as the Nisan-Wigderson design polynomials. We
also define a subclass of homogeneous formulas which we call unique parse tree
(UPT) formulas, and prove superpolynomial lower bounds for these. This
generalizes the superpolynomial lower bounds for regular formulas in
[Kayal-Saha-Saptharishi, STOC 2014], [Fournier-Limaye-Malod-Srinivasan, STOC
2014].
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Amireddy_P/0/1/0/all/0/1">Prashanth Amireddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1">Ankit Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Kayal_N/0/1/0/all/0/1">Neeraj Kayal</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_C/0/1/0/all/0/1">Chandan Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Thankey_B/0/1/0/all/0/1">Bhargav Thankey</a></p><p>We prove super-polynomial lower bounds for low-depth arithmetic circuits
using the shifted partials measure [Gupta-Kamath-Kayal-Saptharishi, CCC 2013],
[Kayal, ECCC 2012] and the affine projections of partials measure
[Garg-Kayal-Saha, FOCS 2020], [Kayal-Nair-Saha, STACS 2016]. The recent
breakthrough work of Limaye, Srinivasan and Tavenas [FOCS 2021] proved these
lower bounds by proving lower bounds for low-depth set-multilinear circuits. An
interesting aspect of our proof is that it does not require conversion of a
circuit to a set-multilinear circuit, nor does it involve a random restriction.
We are able to upper bound the measures for homogeneous formulas directly,
without going via set-multilinearity. Our lower bounds hold for the iterated
matrix multiplication as well as the Nisan-Wigderson design polynomials. We
also define a subclass of homogeneous formulas which we call unique parse tree
(UPT) formulas, and prove superpolynomial lower bounds for these. This
generalizes the superpolynomial lower bounds for regular formulas in
[Kayal-Saha-Saptharishi, STOC 2014], [Fournier-Limaye-Malod-Srinivasan, STOC
2014].
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07900'>Parameterized Inapproximability of the Minimum Distance Problem over all Fields and the Shortest Vector Problem in all $\ell_p$ Norms</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Huck Bennett, Mahdi Cheraghchi, Venkatesan Guruswami, Jo&#xe3;o Ribeiro</p><p>We prove that the Minimum Distance Problem (MDP) on linear codes over any
fixed finite field and parameterized by the input distance bound is W[1]-hard
to approximate within any constant factor. We also prove analogous results for
the parameterized Shortest Vector Problem (SVP) on integer lattices.
Specifically, we prove that SVP in the $\ell_p$ norm is W[1]-hard to
approximate within any constant factor for any fixed $p &gt;1$ and W[1]-hard to
approximate within a factor approaching $2$ for $p=1$. (We show hardness under
randomized reductions in each case.)
</p>
<p>These results answer the main questions left open (and explicitly posed) by
Bhattacharyya, Bonnet, Egri, Ghoshal, Karthik C. S., Lin, Manurangsi, and Marx
(Journal of the ACM, 2021) on the complexity of parameterized MDP and SVP. For
MDP, they established similar hardness for binary linear codes and left the
case of general fields open. For SVP in $\ell_p$ norms with $p &gt; 1$, they
showed inapproximability within some constant factor (depending on $p$) and
left open showing such hardness for arbitrary constant factors. They also left
open showing W[1]-hardness even of exact SVP in the $\ell_1$ norm.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bennett_H/0/1/0/all/0/1">Huck Bennett</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheraghchi_M/0/1/0/all/0/1">Mahdi Cheraghchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Guruswami_V/0/1/0/all/0/1">Venkatesan Guruswami</a>, <a href="http://arxiv.org/find/cs/1/au:+Ribeiro_J/0/1/0/all/0/1">Jo&#xe3;o Ribeiro</a></p><p>We prove that the Minimum Distance Problem (MDP) on linear codes over any
fixed finite field and parameterized by the input distance bound is W[1]-hard
to approximate within any constant factor. We also prove analogous results for
the parameterized Shortest Vector Problem (SVP) on integer lattices.
Specifically, we prove that SVP in the $\ell_p$ norm is W[1]-hard to
approximate within any constant factor for any fixed $p &gt;1$ and W[1]-hard to
approximate within a factor approaching $2$ for $p=1$. (We show hardness under
randomized reductions in each case.)
</p>
<p>These results answer the main questions left open (and explicitly posed) by
Bhattacharyya, Bonnet, Egri, Ghoshal, Karthik C. S., Lin, Manurangsi, and Marx
(Journal of the ACM, 2021) on the complexity of parameterized MDP and SVP. For
MDP, they established similar hardness for binary linear codes and left the
case of general fields open. For SVP in $\ell_p$ norms with $p &gt; 1$, they
showed inapproximability within some constant factor (depending on $p$) and
left open showing such hardness for arbitrary constant factors. They also left
open showing W[1]-hardness even of exact SVP in the $\ell_1$ norm.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07923'>A Theory for Discrete-time Boolean Finite Dynamical Systems with Uncertainty</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mitsunori Ogihara, Kei Uchizawa</p><p>Dynamical Systems is a field that studies the collective behavior of objects
that update their states according to some rules. Discrete-time Boolean Finite
Dynamical System (DT-BFDS) is a subfield where the systems have some finite
number of objects whose states are Boolean values, and the state updates occur
in discrete time. In the subfield of DT-BFDS, researchers aim to (i) design
models for capturing real-world phenomena and using the models to make
predictions and (ii) develop simulation techniques for acquiring insights about
the systems' behavior. Useful for both aims is understanding the system
dynamics mathematically before executing the systems. Obtaining a mathematical
understanding of BFDS is quite challenging, even for simple systems, because
the state space of a system grows exponentially in the number of objects.
Researchers have used computational complexity to circumvent the challenge. The
complexity theoretic research in DT-BFDS has successfully produced complete
characterizations for many dynamical problems.
</p>
<p>The DT-BFDS studies have mainly dealt with deterministic models, where the
update at each time step is deterministic, so the system dynamics are
completely determinable from the initial setting. However, natural systems have
uncertainty. Models having uncertainty may lead to far-better understandings of
nature. Although a few attempts have explored DT-BFDS with uncertainty,
including stochastic initialization and tie-breaking, they have scratched only
a tiny surface of models with uncertainty. The introduction of uncertainty can
be through two schemes. One is the introduction of alternate update functions.
The other is the introduction of alternate update schedules. 37This paper
establishes a theory of models with uncertainty and proves some fundamental
results.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ogihara_M/0/1/0/all/0/1">Mitsunori Ogihara</a>, <a href="http://arxiv.org/find/cs/1/au:+Uchizawa_K/0/1/0/all/0/1">Kei Uchizawa</a></p><p>Dynamical Systems is a field that studies the collective behavior of objects
that update their states according to some rules. Discrete-time Boolean Finite
Dynamical System (DT-BFDS) is a subfield where the systems have some finite
number of objects whose states are Boolean values, and the state updates occur
in discrete time. In the subfield of DT-BFDS, researchers aim to (i) design
models for capturing real-world phenomena and using the models to make
predictions and (ii) develop simulation techniques for acquiring insights about
the systems' behavior. Useful for both aims is understanding the system
dynamics mathematically before executing the systems. Obtaining a mathematical
understanding of BFDS is quite challenging, even for simple systems, because
the state space of a system grows exponentially in the number of objects.
Researchers have used computational complexity to circumvent the challenge. The
complexity theoretic research in DT-BFDS has successfully produced complete
characterizations for many dynamical problems.
</p>
<p>The DT-BFDS studies have mainly dealt with deterministic models, where the
update at each time step is deterministic, so the system dynamics are
completely determinable from the initial setting. However, natural systems have
uncertainty. Models having uncertainty may lead to far-better understandings of
nature. Although a few attempts have explored DT-BFDS with uncertainty,
including stochastic initialization and tie-breaking, they have scratched only
a tiny surface of models with uncertainty. The introduction of uncertainty can
be through two schemes. One is the introduction of alternate update functions.
The other is the introduction of alternate update schedules. 37This paper
establishes a theory of models with uncertainty and proves some fundamental
results.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07798'>A Uniform Sampling Procedure for Abstract Triangulations of Surfaces</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Rajan Shankar, Jonathan Spreer</p><p>We present a procedure to sample uniformly from the set of combinatorial
isomorphism types of balanced triangulations of surfaces - also known as
graph-encoded surfaces. For a given number $n$, the sample is a weighted set of
graph-encoded surfaces with $2n$ triangles.
</p>
<p>The sampling procedure relies on connections between graph-encoded surfaces
and permutations, and basic properties of the symmetric group.
</p>
<p>We implement our method and present a number of experimental findings based
on the analysis of $138$ million runs of our sampling procedure, producing
graph-encoded surfaces with up to $280$ triangles.
</p>
<p>Namely, we determine that, for $n$ fixed, the empirical mean genus
$\bar{g}(n)$ of our sample is very close to $\bar{g}(n) = \frac{n-1}{2} -
(16.98n -110.61)^{1/4}$. Moreover, we present experimental evidence that the
associated genus distribution more and more concentrates on a vanishing portion
of all possible genera as $n$ tends to infinity. Finally, we observe from our
data that the mean number of non-trivial symmetries of a uniformly chosen graph
encoding of a surface decays to zero at a rate super-exponential in $n$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Shankar_R/0/1/0/all/0/1">Rajan Shankar</a>, <a href="http://arxiv.org/find/math/1/au:+Spreer_J/0/1/0/all/0/1">Jonathan Spreer</a></p><p>We present a procedure to sample uniformly from the set of combinatorial
isomorphism types of balanced triangulations of surfaces - also known as
graph-encoded surfaces. For a given number $n$, the sample is a weighted set of
graph-encoded surfaces with $2n$ triangles.
</p>
<p>The sampling procedure relies on connections between graph-encoded surfaces
and permutations, and basic properties of the symmetric group.
</p>
<p>We implement our method and present a number of experimental findings based
on the analysis of $138$ million runs of our sampling procedure, producing
graph-encoded surfaces with up to $280$ triangles.
</p>
<p>Namely, we determine that, for $n$ fixed, the empirical mean genus
$\bar{g}(n)$ of our sample is very close to $\bar{g}(n) = \frac{n-1}{2} -
(16.98n -110.61)^{1/4}$. Moreover, we present experimental evidence that the
associated genus distribution more and more concentrates on a vanishing portion
of all possible genera as $n$ tends to infinity. Finally, we observe from our
data that the mean number of non-trivial symmetries of a uniformly chosen graph
encoding of a surface decays to zero at a rate super-exponential in $n$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07978'>Shellability is hard even for balls</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Pavel Pat&#xe1;k, Martin Tancer</p><p>The main goal of this paper is to show that shellability is NP-hard for
triangulated d-balls (this also gives hardness for triangulated
d-manifolds/d-pseudomanifolds with boundary) as soon as d is at least 3. This
extends our earlier work with Goaoc, Pat\'akov\'a and Wagner on hardness of
shellability of 2-complexes and answers some questions implicitly raised by
Danaraj and Klee in 1978 and explicitly mentioned by Santamar\'ia-Galvis and
Woodroofe. Together with the main goal, we also prove that collapsibility is
NP-hard for 3-complexes embeddable in the 3-space, extending an earlier work of
the second author and answering an open question mentioned by Cohen, Fasy,
Miller, Nayyeri, Peng and Walkington; and that shellability is NP-hard for
2-complexes embeddable in the 3-space, answering another question of
Santamar\'ia-Galvis and Woodroofe (in a slightly stronger form than what is
given by the main result).
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Patak_P/0/1/0/all/0/1">Pavel Pat&#xe1;k</a>, <a href="http://arxiv.org/find/cs/1/au:+Tancer_M/0/1/0/all/0/1">Martin Tancer</a></p><p>The main goal of this paper is to show that shellability is NP-hard for
triangulated d-balls (this also gives hardness for triangulated
d-manifolds/d-pseudomanifolds with boundary) as soon as d is at least 3. This
extends our earlier work with Goaoc, Pat\'akov\'a and Wagner on hardness of
shellability of 2-complexes and answers some questions implicitly raised by
Danaraj and Klee in 1978 and explicitly mentioned by Santamar\'ia-Galvis and
Woodroofe. Together with the main goal, we also prove that collapsibility is
NP-hard for 3-complexes embeddable in the 3-space, extending an earlier work of
the second author and answering an open question mentioned by Cohen, Fasy,
Miller, Nayyeri, Peng and Walkington; and that shellability is NP-hard for
2-complexes embeddable in the 3-space, answering another question of
Santamar\'ia-Galvis and Woodroofe (in a slightly stronger form than what is
given by the main result).
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08091'>About the Reconstruction of Convex Lattice Sets from One or Two X-rays</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Yan Gerard</p><p>We consider a class of problems of Discrete Tomography which has been deeply
investigated in the past: the reconstruction of convex lattice sets from their
horizontal and/or vertical X-rays, i.e. from the number of points in a sequence
of consecutive horizontal and vertical lines. The reconstruction of the
HV-convex polyominoes works usually in two steps, first the filling step
consisting in filling operations, second the convex aggregation of the
switching components. We prove three results about the convex aggregation step:
(1) The convex aggregation step used for the reconstruction of HV-convex
polyominoes does not always provide a solution. The example yielding to this
result is called \textit{the bad guy} and disproves a conjecture of the domain.
(2) The reconstruction of a digital convex lattice set from only one X-ray can
be performed in polynomial time. We prove it by encoding the convex aggregation
problem in a Directed Acyclic Graph. (3) With the same strategy, we prove that
the reconstruction of fat digital convex sets from their horizontal and
vertical X-rays can be solved in polynomial time. Fatness is a property of the
digital convex sets regarding the relative position of the left, right, top and
bottom points of the set. The complexity of the reconstruction of the lattice
sets which are not fat remains an open question.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Gerard_Y/0/1/0/all/0/1">Yan Gerard</a></p><p>We consider a class of problems of Discrete Tomography which has been deeply
investigated in the past: the reconstruction of convex lattice sets from their
horizontal and/or vertical X-rays, i.e. from the number of points in a sequence
of consecutive horizontal and vertical lines. The reconstruction of the
HV-convex polyominoes works usually in two steps, first the filling step
consisting in filling operations, second the convex aggregation of the
switching components. We prove three results about the convex aggregation step:
(1) The convex aggregation step used for the reconstruction of HV-convex
polyominoes does not always provide a solution. The example yielding to this
result is called \textit{the bad guy} and disproves a conjecture of the domain.
(2) The reconstruction of a digital convex lattice set from only one X-ray can
be performed in polynomial time. We prove it by encoding the convex aggregation
problem in a Directed Acyclic Graph. (3) With the same strategy, we prove that
the reconstruction of fat digital convex sets from their horizontal and
vertical X-rays can be solved in polynomial time. Fatness is a property of the
digital convex sets regarding the relative position of the left, right, top and
bottom points of the set. The complexity of the reconstruction of the lattice
sets which are not fat remains an open question.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08184'>Improved Coresets for Euclidean $k$-Means</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Vincent Cohen-Addad, Kasper Green Larsen, David Saulpic, Chris Schwiegelshohn, Omar Ali Sheikh-Omar</p><p>Given a set of $n$ points in $d$ dimensions, the Euclidean $k$-means problem
(resp. the Euclidean $k$-median problem) consists of finding $k$ centers such
that the sum of squared distances (resp. sum of distances) from every point to
its closest center is minimized. The arguably most popular way of dealing with
this problem in the big data setting is to first compress the data by computing
a weighted subset known as a coreset and then run any algorithm on this subset.
The guarantee of the coreset is that for any candidate solution, the ratio
between coreset cost and the cost of the original instance is less than a
$(1\pm \varepsilon)$ factor. The current state of the art coreset size is
$\tilde O(\min(k^{2} \cdot \varepsilon^{-2},k\cdot \varepsilon^{-4}))$ for
Euclidean $k$-means and $\tilde O(\min(k^{2} \cdot \varepsilon^{-2},k\cdot
\varepsilon^{-3}))$ for Euclidean $k$-median. The best known lower bound for
both problems is $\Omega(k \varepsilon^{-2})$. In this paper, we improve the
upper bounds $\tilde O(\min(k^{3/2} \cdot \varepsilon^{-2},k\cdot
\varepsilon^{-4}))$ for $k$-means and $\tilde O(\min(k^{4/3} \cdot
\varepsilon^{-2},k\cdot \varepsilon^{-3}))$ for $k$-median. In particular, ours
is the first provable bound that breaks through the $k^2$ barrier while
retaining an optimal dependency on $\varepsilon$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Cohen_Addad_V/0/1/0/all/0/1">Vincent Cohen-Addad</a>, <a href="http://arxiv.org/find/cs/1/au:+Larsen_K/0/1/0/all/0/1">Kasper Green Larsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Saulpic_D/0/1/0/all/0/1">David Saulpic</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwiegelshohn_C/0/1/0/all/0/1">Chris Schwiegelshohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheikh_Omar_O/0/1/0/all/0/1">Omar Ali Sheikh-Omar</a></p><p>Given a set of $n$ points in $d$ dimensions, the Euclidean $k$-means problem
(resp. the Euclidean $k$-median problem) consists of finding $k$ centers such
that the sum of squared distances (resp. sum of distances) from every point to
its closest center is minimized. The arguably most popular way of dealing with
this problem in the big data setting is to first compress the data by computing
a weighted subset known as a coreset and then run any algorithm on this subset.
The guarantee of the coreset is that for any candidate solution, the ratio
between coreset cost and the cost of the original instance is less than a
$(1\pm \varepsilon)$ factor. The current state of the art coreset size is
$\tilde O(\min(k^{2} \cdot \varepsilon^{-2},k\cdot \varepsilon^{-4}))$ for
Euclidean $k$-means and $\tilde O(\min(k^{2} \cdot \varepsilon^{-2},k\cdot
\varepsilon^{-3}))$ for Euclidean $k$-median. The best known lower bound for
both problems is $\Omega(k \varepsilon^{-2})$. In this paper, we improve the
upper bounds $\tilde O(\min(k^{3/2} \cdot \varepsilon^{-2},k\cdot
\varepsilon^{-4}))$ for $k$-means and $\tilde O(\min(k^{4/3} \cdot
\varepsilon^{-2},k\cdot \varepsilon^{-3}))$ for $k$-median. In particular, ours
is the first provable bound that breaks through the $k^2$ barrier while
retaining an optimal dependency on $\varepsilon$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08333'>Deformation Spaces and Static Animations</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Gabriel Dorfsman-Hopkins</p><p>We study applications of 3D printing to the broad goal of understanding how
mathematical objects vary continuously in families. To do so, we model the
varying parameter as the vertical axis of a 3D print, introducing the notion of
a static animation: a 3D printed object each of whose layers is a member of the
continuously deforming family. We survey examples and draw connections to
algebraic geometry, complex dynamics, chaos theory, and more. We also include a
detailed tutorial (with accompanying code and files) so that the reader can
create static animations of their own.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Dorfsman_Hopkins_G/0/1/0/all/0/1">Gabriel Dorfsman-Hopkins</a></p><p>We study applications of 3D printing to the broad goal of understanding how
mathematical objects vary continuously in families. To do so, we model the
varying parameter as the vertical axis of a 3D print, introducing the notion of
a static animation: a 3D printed object each of whose layers is a member of the
continuously deforming family. We survey examples and draw connections to
algebraic geometry, complex dynamics, chaos theory, and more. We also include a
detailed tutorial (with accompanying code and files) so that the reader can
create static animations of their own.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07644'>Bounds and Estimates on the Average Edit Distance</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Gianfranco Bilardi, Michele Schimd</p><p>The edit distance is a metric of dissimilarity between strings, widely
applied in computational biology, speech recognition, and machine learning. Let
$e_k(n)$ denote the average edit distance between random, independent strings
of $n$ characters from an alphabet of size $k$. For $k \geq 2$, it is an open
problem how to efficiently compute the exact value of $\alpha_{k}(n) =
e_k(n)/n$ as well as of $\alpha_{k} = \lim_{n \to \infty} \alpha_{k}(n)$, a
limit known to exist.
</p>
<p>This paper shows that $\alpha_k(n)-Q(n) \leq \alpha_k \leq \alpha_k(n)$, for
a specific $Q(n)=\Theta(\sqrt{\log n / n})$, a result which implies that
$\alpha_k$ is computable. The exact computation of $\alpha_k(n)$ is explored,
leading to an algorithm running in time $T=\mathcal{O}(n^2k\min(3^n,k^n))$, a
complexity that makes it of limited practical use.
</p>
<p>An analysis of statistical estimates is proposed, based on McDiarmid's
inequality, showing how $\alpha_k(n)$ can be evaluated with good accuracy, high
confidence level, and reasonable computation time, for values of $n$ say up to
a quarter million. Correspondingly, 99.9\% confidence intervals of width
approximately $10^{-2}$ are obtained for $\alpha_k$.
</p>
<p>Combinatorial arguments on edit scripts are exploited to analytically
characterize an efficiently computable lower bound $\beta_k^*$ to $\alpha_k$,
such that $ \lim_{k \to \infty} \beta_k^*=1$. In general, $\beta_k^* \leq
\alpha_k \leq 1-1/k$; for $k$ greater than a few dozens, computing $\beta_k^*$
is much faster than generating good statistical estimates with confidence
intervals of width $1-1/k-\beta_k^*$.
</p>
<p>The techniques developed in the paper yield improvements on most previously
published numerical values as well as results for alphabet sizes and string
lengths not reported before.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Bilardi_G/0/1/0/all/0/1">Gianfranco Bilardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Schimd_M/0/1/0/all/0/1">Michele Schimd</a></p><p>The edit distance is a metric of dissimilarity between strings, widely
applied in computational biology, speech recognition, and machine learning. Let
$e_k(n)$ denote the average edit distance between random, independent strings
of $n$ characters from an alphabet of size $k$. For $k \geq 2$, it is an open
problem how to efficiently compute the exact value of $\alpha_{k}(n) =
e_k(n)/n$ as well as of $\alpha_{k} = \lim_{n \to \infty} \alpha_{k}(n)$, a
limit known to exist.
</p>
<p>This paper shows that $\alpha_k(n)-Q(n) \leq \alpha_k \leq \alpha_k(n)$, for
a specific $Q(n)=\Theta(\sqrt{\log n / n})$, a result which implies that
$\alpha_k$ is computable. The exact computation of $\alpha_k(n)$ is explored,
leading to an algorithm running in time $T=\mathcal{O}(n^2k\min(3^n,k^n))$, a
complexity that makes it of limited practical use.
</p>
<p>An analysis of statistical estimates is proposed, based on McDiarmid's
inequality, showing how $\alpha_k(n)$ can be evaluated with good accuracy, high
confidence level, and reasonable computation time, for values of $n$ say up to
a quarter million. Correspondingly, 99.9\% confidence intervals of width
approximately $10^{-2}$ are obtained for $\alpha_k$.
</p>
<p>Combinatorial arguments on edit scripts are exploited to analytically
characterize an efficiently computable lower bound $\beta_k^*$ to $\alpha_k$,
such that $ \lim_{k \to \infty} \beta_k^*=1$. In general, $\beta_k^* \leq
\alpha_k \leq 1-1/k$; for $k$ greater than a few dozens, computing $\beta_k^*$
is much faster than generating good statistical estimates with confidence
intervals of width $1-1/k-\beta_k^*$.
</p>
<p>The techniques developed in the paper yield improvements on most previously
published numerical values as well as results for alphabet sizes and string
lengths not reported before.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07794'>Augmented Thresholds for MONI</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: C&#xe9;sar Mart&#xed;nez-Guardiola, Nathaniel K. Brown, Fernando Silva-Coira, Dominik K&#xf6;ppl, Travis Gagie, Susana Ladra</p><p>MONI (Rossi et al., 2022) can store a pangenomic dataset T in small space and
later, given a pattern P, quickly find the maximal exact matches (MEMs) of P
with respect to T. In this paper we consider its one-pass version (Boucher et
al., 2021), whose query times are dominated in our experiments by longest
common extension (LCE) queries. We show how a small modification lets us avoid
most of these queries and thus significantly speeds up MONI in practice while
only slightly increasing its size.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Martinez_Guardiola_C/0/1/0/all/0/1">C&#xe9;sar Mart&#xed;nez-Guardiola</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1">Nathaniel K. Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_Coira_F/0/1/0/all/0/1">Fernando Silva-Coira</a>, <a href="http://arxiv.org/find/cs/1/au:+Koppl_D/0/1/0/all/0/1">Dominik K&#xf6;ppl</a>, <a href="http://arxiv.org/find/cs/1/au:+Gagie_T/0/1/0/all/0/1">Travis Gagie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ladra_S/0/1/0/all/0/1">Susana Ladra</a></p><p>MONI (Rossi et al., 2022) can store a pangenomic dataset T in small space and
later, given a pattern P, quickly find the maximal exact matches (MEMs) of P
with respect to T. In this paper we consider its one-pass version (Boucher et
al., 2021), whose query times are dominated in our experiments by longest
common extension (LCE) queries. We show how a small modification lets us avoid
most of these queries and thus significantly speeds up MONI in practice while
only slightly increasing its size.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07796'>Massively Parallel Algorithms for $b$-Matching</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mohsen Ghaffari, Christoph Grunau, Slobodan Mitrovi&#x107;</p><p>This paper presents an $O(\log\log \bar{d})$ round massively parallel
algorithm for $1+\epsilon$ approximation of maximum weighted $b$-matchings,
using near-linear memory per machine. Here $\bar{d}$ denotes the average degree
in the graph and $\epsilon$ is an arbitrarily small positive constant. Recall
that $b$-matching is the natural and well-studied generalization of the
matching problem where different vertices are allowed to have multiple (and
differing number of) incident edges in the matching. Concretely, each vertex
$v$ is given a positive integer budget $b_v$ and it can have up to $b_v$
incident edges in the matching. Previously, there were known algorithms with
round complexity $O(\log\log n)$, or $O(\log\log \Delta)$ where $\Delta$
denotes maximum degree, for $1+\epsilon$ approximation of weighted matching and
for maximal matching [Czumaj et al., STOC'18, Ghaffari et al. PODC'18; Assadi
et al. SODA'19; Behnezhad et al. FOCS'19; Gamlath et al. PODC'19], but these
algorithms do not extend to the more general $b$-matching problem.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1">Mohsen Ghaffari</a>, <a href="http://arxiv.org/find/cs/1/au:+Grunau_C/0/1/0/all/0/1">Christoph Grunau</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitrovic_S/0/1/0/all/0/1">Slobodan Mitrovi&#x107;</a></p><p>This paper presents an $O(\log\log \bar{d})$ round massively parallel
algorithm for $1+\epsilon$ approximation of maximum weighted $b$-matchings,
using near-linear memory per machine. Here $\bar{d}$ denotes the average degree
in the graph and $\epsilon$ is an arbitrarily small positive constant. Recall
that $b$-matching is the natural and well-studied generalization of the
matching problem where different vertices are allowed to have multiple (and
differing number of) incident edges in the matching. Concretely, each vertex
$v$ is given a positive integer budget $b_v$ and it can have up to $b_v$
incident edges in the matching. Previously, there were known algorithms with
round complexity $O(\log\log n)$, or $O(\log\log \Delta)$ where $\Delta$
denotes maximum degree, for $1+\epsilon$ approximation of weighted matching and
for maximal matching [Czumaj et al., STOC'18, Ghaffari et al. PODC'18; Assadi
et al. SODA'19; Behnezhad et al. FOCS'19; Gamlath et al. PODC'19], but these
algorithms do not extend to the more general $b$-matching problem.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.07829'>On Sparsification of Stochastic Packing Problems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Shaddin Dughmi, Yusuf Hakan Kalayci, Neel Patel</p><p>Motivated by recent progress on stochastic matching with few queries, we
embark on a systematic study of the sparsification of stochastic packing
problems (SPP) more generally. Specifically, we consider SPPs where elements
are independently active with a probability p, and ask whether one can
(non-adaptively) compute a sparse set of elements guaranteed to contain an
approximately optimal solution to the realized (active) subproblem. We seek
structural and algorithmic results of broad applicability to such problems. Our
focus is on computing sparse sets containing on the order of d feasible
solutions to the packing problem, where d is linear or at most poly. in 1/p.
Crucially, we require d to be independent of the any parameter related to the
``size'' of the packing problem. We refer to d as the degree of the sparsifier,
as is consistent with graph theoretic degree in the special case of matching.
First, we exhibit a generic sparsifier of degree 1/p based on contention
resolution. This sparsifier's approximation ratio matches the best contention
resolution scheme (CRS) for any packing problem for additive objectives, and
approximately matches the best monotone CRS for submodular objectives. Second,
we embark on outperforming this generic sparsifier for matroids, their
intersections and weighted matching. These improved sparsifiers feature
different algorithmic and analytic approaches, and have degree linear in 1/p.
In the case of a single matroid, our sparsifier tends to the optimal solution.
For weighted matching, we combine our contention-resolution-based sparsifier
with technical approaches of prior work to improve the state of the art ratio
from 0.501 to 0.536. Third, we examine packing problems with submodular
objectives. We show that even the simplest such problems do not admit
sparsifiers approaching optimality.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dughmi_S/0/1/0/all/0/1">Shaddin Dughmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalayci_Y/0/1/0/all/0/1">Yusuf Hakan Kalayci</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1">Neel Patel</a></p><p>Motivated by recent progress on stochastic matching with few queries, we
embark on a systematic study of the sparsification of stochastic packing
problems (SPP) more generally. Specifically, we consider SPPs where elements
are independently active with a probability p, and ask whether one can
(non-adaptively) compute a sparse set of elements guaranteed to contain an
approximately optimal solution to the realized (active) subproblem. We seek
structural and algorithmic results of broad applicability to such problems. Our
focus is on computing sparse sets containing on the order of d feasible
solutions to the packing problem, where d is linear or at most poly. in 1/p.
Crucially, we require d to be independent of the any parameter related to the
``size'' of the packing problem. We refer to d as the degree of the sparsifier,
as is consistent with graph theoretic degree in the special case of matching.
First, we exhibit a generic sparsifier of degree 1/p based on contention
resolution. This sparsifier's approximation ratio matches the best contention
resolution scheme (CRS) for any packing problem for additive objectives, and
approximately matches the best monotone CRS for submodular objectives. Second,
we embark on outperforming this generic sparsifier for matroids, their
intersections and weighted matching. These improved sparsifiers feature
different algorithmic and analytic approaches, and have degree linear in 1/p.
In the case of a single matroid, our sparsifier tends to the optimal solution.
For weighted matching, we combine our contention-resolution-based sparsifier
with technical approaches of prior work to improve the state of the art ratio
from 0.501 to 0.536. Third, we examine packing problems with submodular
objectives. We show that even the simplest such problems do not admit
sparsifiers approaching optimality.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08157'>Taming Large-Scale Genomic Analyses via Sparsified Genomics</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Mohammed Alser, Julien Eudine, Onur Mutlu</p><p>Searching for similar genomic sequences is an essential and fundamental step
in biomedical research and an overwhelming majority of genomic analyses.
State-of-the-art computational methods performing such comparisons fail to cope
with the exponential growth of genomic sequencing data. We introduce the
concept of sparsified genomics where we systematically exclude a large number
of bases from genomic sequences and enable much faster and more
memory-efficient processing of the sparsified, shorter genomic sequences, while
providing similar or even higher accuracy compared to processing non-sparsified
sequences. Sparsified genomics provides significant benefits to many genomic
analyses and has broad applicability. We show that sparsifying genomic
sequences greatly accelerates the state-of-the-art read mapper (minimap2) by
1.54-8.8x using real Illumina, HiFi, and ONT reads, while providing a higher
number of mapped reads and more detected small and structural variations.
Sparsifying genomic sequences makes containment search through very large
genomes and very large databases 72.7-75.88x faster and 723.3x more
storage-efficient than searching through non-sparsified genomic sequences (with
CMash and KMC3). Sparsifying genomic sequences enables robust microbiome
discovery by providing 54.15-61.88x faster and 720x more storage-efficient
taxonomic profiling of metagenomic samples over the state-of-art tool
(Metalign). We design and open-source a framework called Genome-on-Diet as an
example tool for sparsified genomics, which can be freely downloaded from
github.com/CMU-SAFARI/Genome-on-Diet.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Alser_M/0/1/0/all/0/1">Mohammed Alser</a>, <a href="http://arxiv.org/find/cs/1/au:+Eudine_J/0/1/0/all/0/1">Julien Eudine</a>, <a href="http://arxiv.org/find/cs/1/au:+Mutlu_O/0/1/0/all/0/1">Onur Mutlu</a></p><p>Searching for similar genomic sequences is an essential and fundamental step
in biomedical research and an overwhelming majority of genomic analyses.
State-of-the-art computational methods performing such comparisons fail to cope
with the exponential growth of genomic sequencing data. We introduce the
concept of sparsified genomics where we systematically exclude a large number
of bases from genomic sequences and enable much faster and more
memory-efficient processing of the sparsified, shorter genomic sequences, while
providing similar or even higher accuracy compared to processing non-sparsified
sequences. Sparsified genomics provides significant benefits to many genomic
analyses and has broad applicability. We show that sparsifying genomic
sequences greatly accelerates the state-of-the-art read mapper (minimap2) by
1.54-8.8x using real Illumina, HiFi, and ONT reads, while providing a higher
number of mapped reads and more detected small and structural variations.
Sparsifying genomic sequences makes containment search through very large
genomes and very large databases 72.7-75.88x faster and 723.3x more
storage-efficient than searching through non-sparsified genomic sequences (with
CMash and KMC3). Sparsifying genomic sequences enables robust microbiome
discovery by providing 54.15-61.88x faster and 720x more storage-efficient
taxonomic profiling of metagenomic samples over the state-of-art tool
(Metalign). We design and open-source a framework called Genome-on-Diet as an
example tool for sparsified genomics, which can be freely downloaded from
https://github.com/CMU-SAFARI/Genome-on-Diet.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08283'>The RED-BLUE SEPARATION problem on graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Subhadeep Ranjan Dev, Sanjana Dey, Florent Foucaud, Ralf Klasing, Tuomo Lehtil&#xe4;</p><p>We introduce the Red-Blue Separation problem on graphs, where we are given a
graph $G=(V,E)$ whose vertices are colored either red or blue, and we want to
select a (small) subset $S \subseteq V$, called red-blue separating set, such
that for every red-blue pair of vertices, there is a vertex $s \in S$ whose
closed neighborhood contains exactly one of the two vertices of the pair. We
study the computational complexity of Red-Blue Separation, in which one asks
whether a given red-blue colored graph has a red-blue separating set of size at
most a given integer. We prove that the problem is NP-complete even for
restricted graph classes. We also show that it is always approximable in
polynomial time within a factor of $2\ln n$, where $n$ is the input graph's
order. In contrast, for triangle-free graphs and for graphs of bounded maximum
degree, we show that Red-Blue Separation is solvable in polynomial time when
the size of the smaller color class is bounded by a constant. However, on
general graphs, we show that the problem is $W[2]$-hard even when parameterized
by the solution size plus the size of the smaller color class. We also consider
the problem Max Red-Blue Separation where the coloring is not part of the
input. Here, given an input graph $G$, we want to determine the smallest
integer $k$ such that, for every possible red-blue coloring of $G$, there is a
red-blue separating set of size at most $k$. We derive tight bounds on the
cardinality of an optimal solution of Max Red-Blue Separation, showing that it
can range from logarithmic in the graph order, up to the order minus one. We
also give bounds with respect to related parameters. For trees however we prove
an upper bound of two-thirds the order. We then show that Max Red-Blue
Separation is NP-hard, even for graphs of bounded maximum degree, but can be
approximated in polynomial time within a factor of $O(\ln^2 n)$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1">Subhadeep Ranjan Dev</a>, <a href="http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1">Sanjana Dey</a>, <a href="http://arxiv.org/find/cs/1/au:+Foucaud_F/0/1/0/all/0/1">Florent Foucaud</a>, <a href="http://arxiv.org/find/cs/1/au:+Klasing_R/0/1/0/all/0/1">Ralf Klasing</a>, <a href="http://arxiv.org/find/cs/1/au:+Lehtila_T/0/1/0/all/0/1">Tuomo Lehtil&#xe4;</a></p><p>We introduce the Red-Blue Separation problem on graphs, where we are given a
graph $G=(V,E)$ whose vertices are colored either red or blue, and we want to
select a (small) subset $S \subseteq V$, called red-blue separating set, such
that for every red-blue pair of vertices, there is a vertex $s \in S$ whose
closed neighborhood contains exactly one of the two vertices of the pair. We
study the computational complexity of Red-Blue Separation, in which one asks
whether a given red-blue colored graph has a red-blue separating set of size at
most a given integer. We prove that the problem is NP-complete even for
restricted graph classes. We also show that it is always approximable in
polynomial time within a factor of $2\ln n$, where $n$ is the input graph's
order. In contrast, for triangle-free graphs and for graphs of bounded maximum
degree, we show that Red-Blue Separation is solvable in polynomial time when
the size of the smaller color class is bounded by a constant. However, on
general graphs, we show that the problem is $W[2]$-hard even when parameterized
by the solution size plus the size of the smaller color class. We also consider
the problem Max Red-Blue Separation where the coloring is not part of the
input. Here, given an input graph $G$, we want to determine the smallest
integer $k$ such that, for every possible red-blue coloring of $G$, there is a
red-blue separating set of size at most $k$. We derive tight bounds on the
cardinality of an optimal solution of Max Red-Blue Separation, showing that it
can range from logarithmic in the graph order, up to the order minus one. We
also give bounds with respect to related parameters. For trees however we prove
an upper bound of two-thirds the order. We then show that Max Red-Blue
Separation is NP-hard, even for graphs of bounded maximum degree, but can be
approximated in polynomial time within a factor of $O(\ln^2 n)$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08324'>Approximating Flexible Graph Connectivity via R\"acke Tree based Rounding</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Chandra Chekuri, Rhea Jain</p><p>Flexible graph connectivity is a new network design model introduced by
Adjiashvili. It has seen several recent algorithmic advances. Despite these,
the approximability even in the setting of a single-pair $(s,t)$ is poorly
understood. In our recent work, we raised the question of whether there is
poly-logarithmic approximation for the survivable network design version
(Flex-SNDP) when the connectivity requirements are fixed constants. In this
paper, we adapt a powerful framework for survivable network design recently
developed by Chen, Laekhanukit, Liao, and Zhang to give an affirmative answer
to the question. The framework of is based on R\"acke trees and group Steiner
tree rounding. The algorithm and analysis also establishes an upper bound on
the integrality gap of an LP relaxation for Flex-SNDP.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Chekuri_C/0/1/0/all/0/1">Chandra Chekuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1">Rhea Jain</a></p><p>Flexible graph connectivity is a new network design model introduced by
Adjiashvili. It has seen several recent algorithmic advances. Despite these,
the approximability even in the setting of a single-pair $(s,t)$ is poorly
understood. In our recent work, we raised the question of whether there is
poly-logarithmic approximation for the survivable network design version
(Flex-SNDP) when the connectivity requirements are fixed constants. In this
paper, we adapt a powerful framework for survivable network design recently
developed by Chen, Laekhanukit, Liao, and Zhang to give an affirmative answer
to the question. The framework of is based on R\"acke trees and group Steiner
tree rounding. The algorithm and analysis also establishes an upper bound on
the integrality gap of an LP relaxation for Flex-SNDP.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08373'>SDPs and Robust Satisfiability of Promise CSP</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Joshua Brakensiek, Venkatesan Guruswami, Sai Sandeep</p><p>For a constraint satisfaction problem (CSP), a robust satisfaction algorithm
is one that outputs an assignment satisfying most of the constraints on
instances that are near-satisfiable. It is known that the CSPs that admit
efficient robust satisfaction algorithms are precisely those of bounded width,
i.e., CSPs whose satisfiability can be checked by a simple local consistency
algorithm (eg., 2-SAT or Horn-SAT in the Boolean case). While the exact
satisfiability of a bounded width CSP can be checked by combinatorial
algorithms, the robust algorithm is based on rounding a canonical Semidefinite
programming(SDP) relaxation.
</p>
<p>In this work, we initiate the study of robust satisfaction algorithms for
promise CSPs, which are a vast generalization of CSPs that have received much
attention recently. The motivation is to extend the theory beyond CSPs, as well
as to better understand the power of SDPs. We present robust SDP rounding
algorithms under some general conditions, namely the existence of majority or
alternating threshold polymorphisms. On the hardness front, we prove that the
lack of such polymorphisms makes the PCSP hard for all pairs of symmetric
Boolean predicates. Our method involves a novel method to argue SDP gaps via
the absence of certain colorings of the sphere, with connections to sphere
Ramsey theory.
</p>
<p>We conjecture that PCSPs with robust satisfaction algorithms are precisely
those for which the feasibility of the canonical SDP implies (exact)
satisfiability. We also give a precise algebraic condition, known as a minion
characterization, of which PCSPs have the latter property.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Brakensiek_J/0/1/0/all/0/1">Joshua Brakensiek</a>, <a href="http://arxiv.org/find/cs/1/au:+Guruswami_V/0/1/0/all/0/1">Venkatesan Guruswami</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandeep_S/0/1/0/all/0/1">Sai Sandeep</a></p><p>For a constraint satisfaction problem (CSP), a robust satisfaction algorithm
is one that outputs an assignment satisfying most of the constraints on
instances that are near-satisfiable. It is known that the CSPs that admit
efficient robust satisfaction algorithms are precisely those of bounded width,
i.e., CSPs whose satisfiability can be checked by a simple local consistency
algorithm (eg., 2-SAT or Horn-SAT in the Boolean case). While the exact
satisfiability of a bounded width CSP can be checked by combinatorial
algorithms, the robust algorithm is based on rounding a canonical Semidefinite
programming(SDP) relaxation.
</p>
<p>In this work, we initiate the study of robust satisfaction algorithms for
promise CSPs, which are a vast generalization of CSPs that have received much
attention recently. The motivation is to extend the theory beyond CSPs, as well
as to better understand the power of SDPs. We present robust SDP rounding
algorithms under some general conditions, namely the existence of majority or
alternating threshold polymorphisms. On the hardness front, we prove that the
lack of such polymorphisms makes the PCSP hard for all pairs of symmetric
Boolean predicates. Our method involves a novel method to argue SDP gaps via
the absence of certain colorings of the sphere, with connections to sphere
Ramsey theory.
</p>
<p>We conjecture that PCSPs with robust satisfaction algorithms are precisely
those for which the feasibility of the canonical SDP implies (exact)
satisfiability. We also give a precise algebraic condition, known as a minion
characterization, of which PCSPs have the latter property.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.08381'>Optimizing Polymatroid Functions</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Sungjin Im, Benjamin Moseley, Hung Q. Ngo, Kirk Pruhs, Alireza Samadian</p><p>We consider a class of optimization problems that involve determining the
maximum value that a function in a particular class can attain subject to a
collection of difference constraints. We show that a particular linear
programming technique, based on duality and projections, can be used to
rederive some structural results that were previously established using more ad
hoc methods. We then show that this technique can be used to obtain a
polynomial-time algorithm for a certain type of simple difference constraints.
Finally we give lower bound results that show that certain possible extensions
of these results are probably not feasible.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1">Sungjin Im</a>, <a href="http://arxiv.org/find/cs/1/au:+Moseley_B/0/1/0/all/0/1">Benjamin Moseley</a>, <a href="http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1">Hung Q. Ngo</a>, <a href="http://arxiv.org/find/cs/1/au:+Pruhs_K/0/1/0/all/0/1">Kirk Pruhs</a>, <a href="http://arxiv.org/find/cs/1/au:+Samadian_A/0/1/0/all/0/1">Alireza Samadian</a></p><p>We consider a class of optimization problems that involve determining the
maximum value that a function in a particular class can attain subject to a
collection of difference constraints. We show that a particular linear
programming technique, based on duality and projections, can be used to
rederive some structural results that were previously established using more ad
hoc methods. We then show that this technique can be used to obtain a
polynomial-time algorithm for a certain type of simple difference constraints.
Finally we give lower bound results that show that certain possible extensions
of these results are probably not feasible.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-16T01:30:00Z">Wednesday, November 16 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    
    <h2 class='tr-new-date'>
      <i class='fa-regular fa-calendar'></i> Tuesday, November 15
    </h2>
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/156'>TR22-156 |  Parameterized Inapproximability of the Minimum Distance Problem over all Fields and the Shortest Vector Problem in all $\ell_p$ Norms | 

	Huck Bennett, 

	Mahdi Cheraghchi, 

	Venkatesan Guruswami, 

	Joao Ribeiro</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          We prove that the Minimum Distance Problem (MDP) on linear codes over any fixed finite field and parameterized by the input distance bound is W[1]-hard to approximate within any constant factor. We also prove analogous results for the parameterized Shortest Vector Problem (SVP) on integer lattices. Specifically, we prove that SVP in the $\ell_p$ norm is W[1]-hard to approximate within any constant factor for any fixed $p &gt;1$ and W[1]-hard to approximate within a factor approaching $2$ for $p=1$.(We show hardness under randomized reductions in each case.)

These results answer the main questions left open (and explicitly posed) by Bhattacharyya, Bonnet, Egri, Ghoshal, Karthik C. S., Lin, Manurangsi, and Marx (Journal of the ACM, 2021) on the complexity of parameterized MDP and SVP. For MDP, they established similar hardness for binary linear codes and left the case of general fields open. For SVP in $\ell_p$ norms with $p &gt; 1$, they showed inapproximability within some constant factor (depending on $p$) and left open showing such hardness for arbitrary constant factors. They also left open showing W[1]-hardness even of exact SVP in the $\ell_1$ norm.
        
        </div>

        <div class='tr-article-summary'>
        
          
          We prove that the Minimum Distance Problem (MDP) on linear codes over any fixed finite field and parameterized by the input distance bound is W[1]-hard to approximate within any constant factor. We also prove analogous results for the parameterized Shortest Vector Problem (SVP) on integer lattices. Specifically, we prove that SVP in the $\ell_p$ norm is W[1]-hard to approximate within any constant factor for any fixed $p &gt;1$ and W[1]-hard to approximate within a factor approaching $2$ for $p=1$.(We show hardness under randomized reductions in each case.)

These results answer the main questions left open (and explicitly posed) by Bhattacharyya, Bonnet, Egri, Ghoshal, Karthik C. S., Lin, Manurangsi, and Marx (Journal of the ACM, 2021) on the complexity of parameterized MDP and SVP. For MDP, they established similar hardness for binary linear codes and left the case of general fields open. For SVP in $\ell_p$ norms with $p &gt; 1$, they showed inapproximability within some constant factor (depending on $p$) and left open showing such hardness for arbitrary constant factors. They also left open showing W[1]-hardness even of exact SVP in the $\ell_1$ norm.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T19:01:27Z">Tuesday, November 15 2022, 19:01</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://eccc.weizmann.ac.il/report/2022/155'>TR22-155 |  Testing of Index-Invariant Properties in the Huge Object Model | 

	Sayantan Sen, 

	Sourav Chakraborty, 

	Eldar Fischer, 

	Arijit Ghosh, 

	Gopinath Mishra</a></h3>
        <p class='tr-article-feed'>from <a href='https://eccc.weizmann.ac.il/'>ECCC Papers</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The study of distribution testing has become ubiquitous in the area of property testing, both for its theoretical appeal, as well as for its applications in other fields of Computer Science, and in various real-life statistical tasks.

The original distribution testing model relies on samples drawn independently from the distribution to be tested. However, when testing distributions over the $n$-dimensional Hamming cube $\left\{0,1\right\}^{n}$ for a large $n$, even reading a few samples is infeasible. To address this, Goldreich and Ron [ITCS 2022] have defined a model called the huge object model, in which the samples may only be queried in a few places.

In this work, we initiate a study of a general class of properties in the huge object model, those that are invariant under a permutation of the indices of the vectors in $\left\{0,1\right\}^{n}$, while still not being necessarily fully symmetric as per the definition used in traditional distribution testing.

We  prove that every index-invariant property satisfying a bounded VC-dimension restriction admits a property tester with a number of queries independent of $n$. To complement this result, we argue that satisfying only index-invariance or only a VC-dimension bound is insufficient to guarantee a tester whose query complexity is independent of $n$. Moreover, we prove that the dependency of sample and query complexities of our tester on the VC-dimension is essentially tight. As a second part of this work, we address the question of the number of queries required for non-adaptive testing. We show that it can be at most quadratic in the number of queries required for an adaptive tester in the case of index-invariant properties. This is in contrast with the tight (easily provable) exponential gap between adaptive and non-adaptive testers for general non-index-invariant properties. Finally, we provide an index-invariant property for which the quadratic gap between adaptive and non-adaptive query complexities for testing is almost tight.
        
        </div>

        <div class='tr-article-summary'>
        
          
          The study of distribution testing has become ubiquitous in the area of property testing, both for its theoretical appeal, as well as for its applications in other fields of Computer Science, and in various real-life statistical tasks.

The original distribution testing model relies on samples drawn independently from the distribution to be tested. However, when testing distributions over the $n$-dimensional Hamming cube $\left\{0,1\right\}^{n}$ for a large $n$, even reading a few samples is infeasible. To address this, Goldreich and Ron [ITCS 2022] have defined a model called the huge object model, in which the samples may only be queried in a few places.

In this work, we initiate a study of a general class of properties in the huge object model, those that are invariant under a permutation of the indices of the vectors in $\left\{0,1\right\}^{n}$, while still not being necessarily fully symmetric as per the definition used in traditional distribution testing.

We  prove that every index-invariant property satisfying a bounded VC-dimension restriction admits a property tester with a number of queries independent of $n$. To complement this result, we argue that satisfying only index-invariance or only a VC-dimension bound is insufficient to guarantee a tester whose query complexity is independent of $n$. Moreover, we prove that the dependency of sample and query complexities of our tester on the VC-dimension is essentially tight. As a second part of this work, we address the question of the number of queries required for non-adaptive testing. We show that it can be at most quadratic in the number of queries required for an adaptive tester in the case of index-invariant properties. This is in contrast with the tight (easily provable) exponential gap between adaptive and non-adaptive testers for general non-index-invariant properties. Finally, we provide an index-invariant property for which the quadratic gap between adaptive and non-adaptive query complexities for testing is almost tight.
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T18:43:10Z">Tuesday, November 15 2022, 18:43</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://11011110.github.io/blog/2022/11/15/linkage.html'>Linkage</a></h3>
        <p class='tr-article-feed'>from <a href='https://11011110.github.io/blog/'>David Eppstein</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The massive influx of new users and new content to Mastodon has caused a greater number of these to be boosts of someone else’s post rather than posts of my own.
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The massive influx of new users and new content to Mastodon has caused a greater number of these to be boosts of someone else’s post rather than posts of my own.</p>

<ul>
  <li>
    <p><a href="https://www.quantamagazine.org/in-math-and-life-svetlana-jitomirskaya-stares-down-complexity-20221101/"><em>Quanta</em> has a nice profile of my colleague at UC Irvine, Ukrainian-born mathematician Svetlana Jitomirskaya</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109269250786290297">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://westy31.home.xs4all.nl/Penrose/Carboard_Impossible_Penrose_triangle.html">A cardboard model of the “impossible” Penrose triangle</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@GerardWestendorp/109259133843305538">\(\mathbb{M}\)</a>),</span> with construction instructions, a printable cutout, and a demo video link, by Gerard Westendorp.</p>
  </li>
  <li>
    <p><a href="https://ventrella.com/organic-algorithm/">Organic algorithm series</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109282060578920502">\(\mathbb{M}\)</a>),</span> artworks by Jeffrey Ventrella.</p>
  </li>
  <li>
    <p>Fatih Kızılkaya, a student of David Kempe at USC, gave an excellent talk in our theory seminar on <a href="https://arxiv.org/abs/2206.07098">“Plurality Veto” voting</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109288019438622582">\(\mathbb{M}\)</a>).</span> The idea is to count first-place votes per candidate, then let each voter cancel a single vote for their least-favorite remaining candidate (one voter at a time or simultaneously and fractionally) until only one candidate has votes left. With voters and candidates in a metric space and preferences by distance, this 3-approximates the min average distance to voters, best possible.</p>
  </li>
  <li>
    <p><a href="https://scholarlykitchen.sspnet.org/2022/11/01/guest-post-wikipedias-citations-are-influencing-scholars-and-publishers/">Getting an academic publication cited on Wikipedia tends to lead to more citations elsewhere, and open-access publications tend to be more frequently cited on Wikipedia</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109292417799090772">\(\mathbb{M}\)</a>,</span> <a href="https://retractionwatch.com/2022/11/05/weekend-reads-double-edition-sciences-nasty-photoshopping-problem-dr-ozs-publication-ban-image-manipulation-detection-software/">via</a>).</p>
  </li>
  <li>
    <p><a href="https://gilkalai.wordpress.com/2022/10/19/james-davies-every-finite-colouring-of-the-plane-contains-a-monochromatic-pair-of-points-at-an-odd-distance-from-each-other/">Every finite colouring of the plane contains a monochromatic pair of points at an odd distance from each other</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109299861480951154">\(\mathbb{M}\)</a>),</span> based on <a href="https://arxiv.org/abs/2209.15598">a new arXiv preprint by James Davies</a>. The paper is pretty heavy going and the linked blog post doesn’t give much detail, so providing a more generally understandable version of this looks like a challenge.</p>
  </li>
  <li>
    <p><a href="http://fredrik-j.blogspot.com/2009/02/how-not-to-compute-harmonic-numbers.html">How (not) to compute harmonic numbers</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@peterluschny/109280460006661224">\(\mathbb{M}\)</a>).</span> Like for factorials, exact computation using divide-and-conquer works much better than the obvious method of sequentially adding each unit fraction. But factorials have an even faster algorithm; it’s unclear whether the same idea can be made to work for the harmonic numbers.</p>
  </li>
  <li>
    <p><a href="http://courses.csail.mit.edu/6.849/fall12/lectures/C01.html">Erik Demaine’s geometric folding algorithms course lectures</a>
 <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@j2kun/109305200398869548">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://xkcd.com/2694/">xkcd on the bridges of Königsburg</a> <span style="white-space:nowrap">(<a href="https://mastodon.xyz/@xkcd/109288968362775008">\(\mathbb{M}\)</a>).</span> This one is definitely going into my lecture notes for the next time it comes around.</p>

    <p style="text-align:center"><img src="https://imgs.xkcd.com/comics/konigsberg.png" alt="xkcd on the bridges of Königsburg" /></p>
  </li>
  <li>
    <p>News I didn’t know about the university I work for, UC Irvine: apparently <a href="https://www.latimes.com/sports/story/2022-11-01/esports-uc-irvine-overwatch-league-of-legends-valorant">we’re an “esports powerhouse”</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109322880998262132">\(\mathbb{M}\)</a>,</span> <a href="https://web.archive.org/web/20221107065218/https://www.latimes.com/sports/story/2022-11-01/esports-uc-irvine-overwatch-league-of-legends-valorant">archive</a>). Definitely preferable to funneling all the alumni donations into football stadium construction! (Unlike many big US universities we don’t have a football team and I like it that way.)</p>
  </li>
  <li>
    <p>Zhao Liang asks: <a href="https://mathstodon.xyz/@neozhaoliang/109324352766569049">which polyhedra have the property that if you make them out of mirrors and stand inside, you will see a tessellation of space?</a></p>
  </li>
  <li>
    <p><a href="https://discrete-notes.github.io/mandatory-attendance">About mandatory attendance by presenters at CS research conferences</a> <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109331612952132380">\(\mathbb{M}\)</a>).</span> I <a href="/blog/2022/11/13/report-from-latin.html">just attended LATIN</a>, a hybrid in person/online conference that ended up heavily tilted to in-person participation. In-person clearly leads to significantly greater interaction rather than mere passive reception of talks. I am very attracted to the idea of reducing the carbon footprint of my travel but in my experience we have not found a successful online replacement for that aspect of conferencing.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@divbyzero/109314296701448645">Dave Richeson folds a regular octagon from a strip of paper</a>.</p>
  </li>
  <li>
    <p>Our graduate students and their union went on strike yesterday. The <a href="https://www.latimes.com/california/story/2022-11-14/photos-strike-by-48-000-university-of-california-academic-workers-causes-systemwide-disruptions"><em>LA Times</em> has a photo-essay</a>  <span style="white-space:nowrap">(<a href="https://mathstodon.xyz/@11011110/109345285468088888">\(\mathbb{M}\)</a>,</span> <a href="https://web.archive.org/web/20221115011509/https://www.latimes.com/california/story/2022-11-14/photos-strike-by-48-000-university-of-california-academic-workers-causes-systemwide-disruptions">archive</a>). For the union’s demands and university’s counteroffers see <a href="https://www.statnews.com/2022/11/14/university-of-california-academic-workers-go-out-on-strike-to-demand-higher-wages/">this story</a>.
You can also find recent <a href="https://www.ucop.edu/academic-personnel-programs/_files/2022/oct-2021-scales/t22.pdf">grad student</a> and <a href="https://www.ucop.edu/academic-personnel-programs/_files/2022-23/oct-2022-salary-scales/t1.pdf">faculty</a> salary scales online, but note that notionally at least the student salaries are for halftime work.</p>
  </li>
</ul><p class="authors">By David Eppstein</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T17:54:00Z">Tuesday, November 15 2022, 17:54</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/15/post-doc-at-university-of-cambridge-apply-by-december-19-2022/'>Post-doc at University of Cambridge (apply by December 19, 2022)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          A post-doctoral research position is available to work with Prof. Anuj Dawar on a UKRI-funded (ERC advanced grant replacement) project exploring the limits of symmetric computation. Website: www.jobs.cam.ac.uk/job/37949/ Email: anuj.dawar@cl.cam.ac.uk
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>A post-doctoral research position is available to work with Prof. Anuj Dawar on a UKRI-funded (ERC advanced grant replacement) project exploring the limits of symmetric computation.</p>
<p>Website: <a href="https://www.jobs.cam.ac.uk/job/37949/">https://www.jobs.cam.ac.uk/job/37949/</a><br />
Email: anuj.dawar@cl.cam.ac.uk</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T17:21:22Z">Tuesday, November 15 2022, 17:21</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/15/professors-associate-professors-assistant-professors-at-the-chinese-university-of-hong-kong-apply-by-may-31-2023/'>Professor(s) / Associate Professor(s) / Assistant Professor(s) at The Chinese University of Hong Kong (apply by May 31, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          Applications are invited for:- Department of Computer Science and Engineering Professor(s) / Associate Professor(s) / Assistant Professor(s) (Ref: 220002PK) (Closing date: May 31, 2023) The Department of Computer Science and Engineering (CSE) at the Chinese University of Hong Kong has multiple tenure-track faculty positions at all ranks to pursue new strategic research initiatives. Website: cuhk.taleo.net/careersection/cu_career_teach/jobdetail.ftl?job=220002PK&#38;tz=GMT%2B08%3A00&#38;tzname=Asia%2FHong_Kong [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>Applications are invited for:-</p>
<p>Department of Computer Science and Engineering<br />
Professor(s) / Associate Professor(s) / Assistant Professor(s) (Ref: 220002PK) (Closing date: May 31, 2023)</p>
<p>The Department of Computer Science and Engineering (CSE) at the Chinese University of Hong Kong has multiple tenure-track faculty positions at all ranks to pursue new strategic research initiatives.</p>
<p>Website: <a href="https://cuhk.taleo.net/careersection/cu_career_teach/jobdetail.ftl?job=220002PK&amp;tz=GMT%2B08%3A00&amp;tzname=Asia%2FHong_Kong">https://cuhk.taleo.net/careersection/cu_career_teach/jobdetail.ftl?job=220002PK&amp;tz=GMT%2B08%3A00&amp;tzname=Asia%2FHong_Kong</a><br />
Email: dept@cse.cuhk.edu.hk</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T08:43:36Z">Tuesday, November 15 2022, 08:43</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://blog.computationalcomplexity.org/2022/11/who-first-thought-of-notion-of.html'>Who first thought of the notion of Polynomial Time?</a></h3>
        <p class='tr-article-feed'>from <a href='http://blog.computationalcomplexity.org/'>Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>(Updated version of&nbsp; Computational Intractability: A Guide to Algorithmic Lower Bound by Demaine-Gasarch-Hajiaghayi is&nbsp;here)</p><p><br></p><p>Any question like who first though of X is often hard to answer. I blogged about who first came up with the Fib numbers&nbsp;here. I've heard rumors that IBM had search engines way before Google but could not figure out how to make money off of it. There are other examples.&nbsp;</p><p>I had learned that Cobham defined P in the paper The intrinsic computational difficulty of functions, in 1965. It was The conference on Logic, Methodology, and Philosophy of Science. The paper is&nbsp;here.&nbsp; Jack Edmonds had the notion of P in the paper Paths, Trees, and Flowers&nbsp;here&nbsp;in 1965.</p><p>While it is true that Cobham defined P in that paper, and he might have been the first one to do so, was the notion somehow around earlier. I first thought the answer was no. Why?&nbsp; Because if you look at Joe Kruskal's paper on MST&nbsp; (see here) you don't see anything resembling time complexity. No O(E+Vlog V) or whatnot. So I thought that if the notion of&nbsp;&nbsp;this algorithm runs in such-and-such time was not in the air, then certainly any notion of P could not have been.&nbsp;</p><p>Hence I was surprised when I accidentally (more on that later) came across the following:&nbsp;</p><p>In 1910 (really, 1910)&nbsp; H.C.Pocklington analyzed two algorithms for solving quadratic congruences and noticed that&nbsp;</p><p>one took time proportional to a power of the log of the modulus, where as</p><p>the other took time proportional to the modulus itself or its square root.&nbsp;</p><p>THAT is the distinction between P and NOT-P.&nbsp;</p><p>The paper is titled The determination of the exponent to which a number belongs, the practical solution of certain congruences, and the law of quadratic reciprocity. It appeared in 1910, in the Proceedings of the Cambridge Philosophical&nbsp; Society, Volume 16, pages 1-5. (I could not find it online. If you know of a place online for it, leave a comment.)&nbsp;</p><p>ADDED LATER: Here is the article in pieces:&nbsp;Page 1,&nbsp;Pages2,3,&nbsp;Pages4,5.</p><p>How did I come across this? And why had I NOT come across this in my roughly 40 years working in complexity theory?&nbsp;</p><p>I came across it while reading a blog of Scotts, The Kolmogorov Option, see&nbsp;here&nbsp;where Pocklington is mentioned in passing. I am surprised how arbitrary the set of things ones knows can be. I have put the Pocklington story in the Demaine-Gasarch-Hajiaghayi book&nbsp;Computational Intractability: A Guide to Algorithmic Lower Bounds&nbsp;so that this knowledge gets to be better known.</p><p>ADDED LATER: That Cobham and Edmonds are known for discovering or inventing P is an example of&nbsp; the well known&nbsp;</p><p>Columbus Principle: Things are named after the LAST person to discover them (note that Columbus was the last person to discover America.)</p><p>Bonus Question: Most principles where the author is not on it, the author might be unknown. NOT in this case. I KNOW who coined the term `Columbus Principle' Do you? (It was not me.)&nbsp;</p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p>By gasarch</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>(Updated version of&nbsp; <i>Computational Intractability: A Guide to Algorithmic Lower Bound</i> by Demaine-Gasarch-Hajiaghayi is&nbsp;<a href="https://hardness.mit.edu/">here</a>)</p><p><br /></p><p>Any question like <i>who first though of X </i>is often hard to answer. I blogged about who first came up with the Fib numbers&nbsp;<a href="https://blog.computationalcomplexity.org/search?q=Fib">here</a>. I've heard rumors that IBM had search engines way before Google but could not figure out how to make money off of it. There are other examples.&nbsp;</p><p>I had learned that Cobham defined P in the paper <i>The intrinsic computational difficulty of functions</i>, in 1965. It was The conference on Logic, Methodology, and Philosophy of Science. The paper is&nbsp;<a href="https://www.cs.toronto.edu/~sacook/homepage/cobham_intrinsic.pdf">here</a>.&nbsp; Jack Edmonds had the notion of P in the paper <i>Paths, Trees, and Flowers&nbsp;<a href="https://math.nist.gov/~JBernal/p_t_f.pdf">here</a>&nbsp;</i>in 1965.</p><p>While it is true that Cobham defined P in that paper, and he might have been the first one to do so, was the notion somehow around earlier. I first thought the answer was no. Why?&nbsp; Because if you look at Joe Kruskal's paper on MST&nbsp; (see <a href="https://www.ams.org/journals/proc/1956-007-01/S0002-9939-1956-0078686-7/S0002-9939-1956-0078686-7.pdf">here</a>) you don't see anything resembling time complexity. No O(E+Vlog V) or whatnot. So I thought that if the notion of&nbsp;&nbsp;<i>this algorithm runs in such-and-such time </i>was not in the air, then certainly any notion of P could not have been.&nbsp;</p><p>Hence I was surprised when I accidentally (more on that later) came across the following:&nbsp;</p><p>In 1910 (really, 1910)&nbsp; H.C.Pocklington analyzed two algorithms for solving quadratic congruences and noticed that&nbsp;</p><p><i>one took time proportional to a power of the log of the modulus, where as</i></p><p><i>the other took time proportional to the modulus itself or its square root.&nbsp;</i></p><p>THAT is the distinction between P and NOT-P.&nbsp;</p><p>The paper is titled <i>The determination of the exponent to which a number belongs, the practical solution of</i> <i>certain congruences, and the law of quadratic reciprocity. </i>It appeared in 1910, in the Proceedings of the Cambridge Philosophical&nbsp; Society, Volume 16, pages 1-5. (I could not find it online. If you know of a place online for it, leave a comment.)&nbsp;</p><p>ADDED LATER: Here is the article in pieces:&nbsp;<a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/pockpage1.png">Page 1</a>,&nbsp;<a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/pockpage2,3.png">Pages2,3</a>,&nbsp;<a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/pockpage4,5.png">Pages4,5</a>.</p><p>How did I come across this? And why had I NOT come across this in my roughly 40 years working in complexity theory?&nbsp;</p><p>I came across it while reading a blog of Scotts, <i>The Kolmogorov Option, </i>see&nbsp;<a href="https://scottaaronson.blog/?p=3376">here</a>&nbsp;where Pocklington is mentioned in passing. I am surprised how arbitrary the set of things ones knows can be. I have put the Pocklington story in the Demaine-Gasarch-Hajiaghayi book&nbsp;<a href="https://hardness.mit.edu/">Computational Intractability: A Guide to Algorithmic Lower Bounds</a>&nbsp;so that this knowledge gets to be better known.</p><p>ADDED LATER: That Cobham and Edmonds are known for discovering or inventing P is an example of&nbsp; the well known&nbsp;</p><p><b>Columbus Principle</b>: Things are named after the LAST person to discover them (note that Columbus was the last person to discover America.)</p><p>Bonus Question: Most principles where the author is not on it, the author might be unknown. NOT in this case. I KNOW who coined the term `Columbus Principle' Do you? (It was not me.)&nbsp;</p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p class="authors">By gasarch</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T04:18:00Z">Tuesday, November 15 2022, 04:18</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='https://cstheory-jobs.org/2022/11/15/faculty-at-university-of-toronto-apply-by-january-9-2023/'>Faculty at University of Toronto (apply by January 9, 2023)</a></h3>
        <p class='tr-article-feed'>from <a href='https://cstheory-jobs.org'>CCI: jobs</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          The Department of Computer Science at the University of Toronto invites applications for multiple positions. In addition to a number of positions open to all areas of computer science, both at the assistant and at the associate levels, we also have positions targeted to computer security and cryptography, as well as to machine learning. Website: [&#8230;]
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p>The Department of Computer Science at the University of Toronto invites applications for multiple positions. In addition to a number of positions open to all areas of computer science, both at the assistant and at the associate levels, we also have positions targeted to computer security and cryptography, as well as to machine learning.</p>
<p>Website: <a href="https://web.cs.toronto.edu/employment-opportunities">https://web.cs.toronto.edu/employment-opportunities</a><br />
Email: recruit@cs.toronto.edu</p>
<p class="authors">By shacharlovett</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T03:18:06Z">Tuesday, November 15 2022, 03:18</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06485'>On automorphism group of a possible short algorithm for multiplication of $3\times3$ matrices</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CC/recent'>arXiv: Computational Complexity</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Vladimir Burichenko</p><p>Studying algorithms admitting nontrivial symmetries is a prospective way of
constructing new short algorithms of matrix multiplication. The main result of
the article is that if there exists an algorithm of multiplicative length
$l\leq22$ for multuplication of $3\times3$ matrices then its automorphism group
is isomorphic to a subgroup of $S_l\times S_3$.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Burichenko_V/0/1/0/all/0/1">Vladimir Burichenko</a></p><p>Studying algorithms admitting nontrivial symmetries is a prospective way of
constructing new short algorithms of matrix multiplication. The main result of
the article is that if there exists an algorithm of multiplicative length
$l\leq22$ for multuplication of $3\times3$ matrices then its automorphism group
is isomorphic to a subgroup of $S_l\times S_3$.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06472'>The Stackelberg Game: responses to regular strategies</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.CG/recent'>arXiv: Computational Geometry</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Thomas Byrne</p><p>Following the solution to the One-Round Voronoi Game in arXiv:2011.13275, we
naturally may want to consider similar games based upon the competitive
locating of points and subsequent dividing of territories. In order to appease
the tears of White (the first player) after they have potentially been tricked
into going first in a game of point-placement, an alternative game (or rather,
an extension of the Voronoi game) is the Stackelberg game where all is not lost
if Black (the second player) gains over half of the contested area. It turns
out that plenty of results can be transferred from One-Round Voronoi Game and
what remains to be explored for the Stackelberg game is how best White can
mitigate the damage of Black's placements. Since significant weaknesses in
certain arrangements were outlined in arXiv:2011.13275, we shall first consider
arrangements that still satisfy these results (namely, White plays a certain
grid arrangement) and then explore how Black can best exploit these positions.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Byrne_T/0/1/0/all/0/1">Thomas Byrne</a></p><p>Following the solution to the One-Round Voronoi Game in <a href="/abs/2011.13275">arXiv:2011.13275</a>, we
naturally may want to consider similar games based upon the competitive
locating of points and subsequent dividing of territories. In order to appease
the tears of White (the first player) after they have potentially been tricked
into going first in a game of point-placement, an alternative game (or rather,
an extension of the Voronoi game) is the Stackelberg game where all is not lost
if Black (the second player) gains over half of the contested area. It turns
out that plenty of results can be transferred from One-Round Voronoi Game and
what remains to be explored for the Stackelberg game is how best White can
mitigate the damage of Black's placements. Since significant weaknesses in
certain arrangements were outlined in <a href="/abs/2011.13275">arXiv:2011.13275</a>, we shall first consider
arrangements that still satisfy these results (namely, White plays a certain
grid arrangement) and then explore how Black can best exploit these positions.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06459'>Faster Walsh-Hadamard and Discrete Fourier Transforms From Matrix Non-Rigidity</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Josh Alman, Kevin Rao</p><p>We give algorithms with lower arithmetic operation counts for both the
Walsh-Hadamard Transform (WHT) and the Discrete Fourier Transform (DFT) on
inputs of power-of-2 size $N$.
</p>
<p>For the WHT, our new algorithm has an operation count of $\frac{23}{24}N \log
N + O(N)$. To our knowledge, this gives the first improvement on the $N \log N$
operation count of the simple, folklore Fast Walsh-Hadamard Transform
algorithm.
</p>
<p>For the DFT, our new FFT algorithm uses $\frac{15}{4}N \log N + O(N)$ real
arithmetic operations. Our leading constant $\frac{15}{4} = 3.75$ is the first
improvement in over 15 years; it improves on the prior best leading constant
$\frac{34}{9} = 3.777\ldots$ by Van Buskirk from 2004, which in turn improved
on the leading constant of $4$ from the split-radix algorithm of Yavne from
1968 and the leading constant of $5$ from the Cooley-Tukey algorithm from 1965.
</p>
<p>Our new WHT algorithm takes advantage of a recent line of work on the
non-rigidity of the WHT: we decompose the WHT matrix as the sum of a low-rank
matrix and a sparse matrix, and then analyze the structures of these matrices
to achieve a lower operation count. Our new DFT algorithm comes from a novel
reduction, showing that parts of the previous best FFT algorithms can be
replaced by calls to an algorithm for the WHT. Replacing the folklore WHT
algorithm with our new improved algorithm leads to our improved FFT.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Alman_J/0/1/0/all/0/1">Josh Alman</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1">Kevin Rao</a></p><p>We give algorithms with lower arithmetic operation counts for both the
Walsh-Hadamard Transform (WHT) and the Discrete Fourier Transform (DFT) on
inputs of power-of-2 size $N$.
</p>
<p>For the WHT, our new algorithm has an operation count of $\frac{23}{24}N \log
N + O(N)$. To our knowledge, this gives the first improvement on the $N \log N$
operation count of the simple, folklore Fast Walsh-Hadamard Transform
algorithm.
</p>
<p>For the DFT, our new FFT algorithm uses $\frac{15}{4}N \log N + O(N)$ real
arithmetic operations. Our leading constant $\frac{15}{4} = 3.75$ is the first
improvement in over 15 years; it improves on the prior best leading constant
$\frac{34}{9} = 3.777\ldots$ by Van Buskirk from 2004, which in turn improved
on the leading constant of $4$ from the split-radix algorithm of Yavne from
1968 and the leading constant of $5$ from the Cooley-Tukey algorithm from 1965.
</p>
<p>Our new WHT algorithm takes advantage of a recent line of work on the
non-rigidity of the WHT: we decompose the WHT matrix as the sum of a low-rank
matrix and a sparse matrix, and then analyze the structures of these matrices
to achieve a lower operation count. Our new DFT algorithm comes from a novel
reduction, showing that parts of the previous best FFT algorithms can be
replaced by calls to an algorithm for the WHT. Replacing the folklore WHT
algorithm with our new improved algorithm leads to our improved FFT.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06521'>On maximal 3-edge-connected subgraphs of undirected graphs</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Loukas Georgiadis, Giuseppe F. Italiano, Evangelos Kosinas, Debasish Pattanayak</p><p>We show how to find and efficiently maintain maximal 3-edge-connected
subgraphs in undirected graphs. In particular, we provide the following
results. $(1)$ Two algorithms for the incremental maintenance of the maximal
$3$-edge-connected subgraphs. These algorithms allow for vertex and edge
insertions, interspersed with queries asking whether two vertices belong to the
same maximal $3$-edge-connected subgraph, and there is a trade-off between
their time- and space-complexity. Specifically, the first algorithm has
$O(m\alpha(m,n) + n^2\log^2 n)$ total running time and uses $O(n)$ space, where
$m$ is the number of edge insertions and queries, and $n$ is the total number
of vertices inserted starting from an empty graph. The second algorithm
performs the same operations in faster $O(m\alpha(m,n) + n^2\alpha(n,n))$ time
in total, using $O(n^2)$ space. $(2)$ A fully dynamic algorithm for maintaining
information about the maximal $k$-edge-connected subgraphs for fixed $k$. Our
update bounds are ${O}(n\sqrt{n}\,\log n)$ worst-case time for $k&gt;4$ and
${O}(n\sqrt{n}\,)$ worst-case time for $k\in\{3,4\}$. In both cases, we achieve
constant time for maximal $k$-edge-connected subgraph queries. $(3)$ A
deterministic algorithm for computing the maximal $k$-edge-connected subgraphs,
for any fixed $k&gt;2$, in $\widetilde{O}(m+n\sqrt{n}\,)$ time. This result
improves substantially on the previously best known deterministic bounds and
matches (modulo $\log$ factors) the expected time of the best randomized
algorithm for the same problem. $(4)$ A linear-time algorithm for computing a
spanning subgraph with $O(n\log n)$ edges that has the same maximal
$k$-edge-connected subgraphs as the original graph.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Georgiadis_L/0/1/0/all/0/1">Loukas Georgiadis</a>, <a href="http://arxiv.org/find/cs/1/au:+Italiano_G/0/1/0/all/0/1">Giuseppe F. Italiano</a>, <a href="http://arxiv.org/find/cs/1/au:+Kosinas_E/0/1/0/all/0/1">Evangelos Kosinas</a>, <a href="http://arxiv.org/find/cs/1/au:+Pattanayak_D/0/1/0/all/0/1">Debasish Pattanayak</a></p><p>We show how to find and efficiently maintain maximal 3-edge-connected
subgraphs in undirected graphs. In particular, we provide the following
results. $(1)$ Two algorithms for the incremental maintenance of the maximal
$3$-edge-connected subgraphs. These algorithms allow for vertex and edge
insertions, interspersed with queries asking whether two vertices belong to the
same maximal $3$-edge-connected subgraph, and there is a trade-off between
their time- and space-complexity. Specifically, the first algorithm has
$O(m\alpha(m,n) + n^2\log^2 n)$ total running time and uses $O(n)$ space, where
$m$ is the number of edge insertions and queries, and $n$ is the total number
of vertices inserted starting from an empty graph. The second algorithm
performs the same operations in faster $O(m\alpha(m,n) + n^2\alpha(n,n))$ time
in total, using $O(n^2)$ space. $(2)$ A fully dynamic algorithm for maintaining
information about the maximal $k$-edge-connected subgraphs for fixed $k$. Our
update bounds are ${O}(n\sqrt{n}\,\log n)$ worst-case time for $k&gt;4$ and
${O}(n\sqrt{n}\,)$ worst-case time for $k\in\{3,4\}$. In both cases, we achieve
constant time for maximal $k$-edge-connected subgraph queries. $(3)$ A
deterministic algorithm for computing the maximal $k$-edge-connected subgraphs,
for any fixed $k&gt;2$, in $\widetilde{O}(m+n\sqrt{n}\,)$ time. This result
improves substantially on the previously best known deterministic bounds and
matches (modulo $\log$ factors) the expected time of the best randomized
algorithm for the same problem. $(4)$ A linear-time algorithm for computing a
spanning subgraph with $O(n\log n)$ edges that has the same maximal
$k$-edge-connected subgraphs as the original graph.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06530'>Multi-Epoch Matrix Factorization Mechanisms for Private Machine Learning</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Christopher A. Choquette-Choo, H. Brendan McMahan, Keith Rush, Abhradeep Thakurta</p><p>We introduce new differentially private (DP) mechanisms for gradient-based
machine learning (ML) training involving multiple passes (epochs) of a dataset,
substantially improving the achievable privacy-utility-computation tradeoffs.
Our key contribution is an extension of the online matrix factorization DP
mechanism to multiple participations, substantially generalizing the approach
of DMRST2022. We first give conditions under which it is possible to reduce the
problem with per-iteration vector contributions to the simpler one of scalar
contributions. Using this, we formulate the construction of optimal (in total
squared error at each iterate) matrix mechanisms for SGD variants as a convex
program. We propose an efficient optimization algorithm via a closed form
solution to the dual function.
</p>
<p>While tractable, both solving the convex problem offline and computing the
necessary noise masks during training can become prohibitively expensive when
many training steps are necessary. To address this, we design a
Fourier-transform-based mechanism with significantly less computation and only
a minor utility decrease.
</p>
<p>Extensive empirical evaluation on two tasks: example-level DP for image
classification and user-level DP for language modeling, demonstrate substantial
improvements over the previous state-of-the-art. Though our primary application
is to ML, we note our main DP results are applicable to arbitrary linear
queries and hence may have much broader applicability.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Choquette_Choo_C/0/1/0/all/0/1">Christopher A. Choquette-Choo</a>, <a href="http://arxiv.org/find/cs/1/au:+McMahan_H/0/1/0/all/0/1">H. Brendan McMahan</a>, <a href="http://arxiv.org/find/cs/1/au:+Rush_K/0/1/0/all/0/1">Keith Rush</a>, <a href="http://arxiv.org/find/cs/1/au:+Thakurta_A/0/1/0/all/0/1">Abhradeep Thakurta</a></p><p>We introduce new differentially private (DP) mechanisms for gradient-based
machine learning (ML) training involving multiple passes (epochs) of a dataset,
substantially improving the achievable privacy-utility-computation tradeoffs.
Our key contribution is an extension of the online matrix factorization DP
mechanism to multiple participations, substantially generalizing the approach
of DMRST2022. We first give conditions under which it is possible to reduce the
problem with per-iteration vector contributions to the simpler one of scalar
contributions. Using this, we formulate the construction of optimal (in total
squared error at each iterate) matrix mechanisms for SGD variants as a convex
program. We propose an efficient optimization algorithm via a closed form
solution to the dual function.
</p>
<p>While tractable, both solving the convex problem offline and computing the
necessary noise masks during training can become prohibitively expensive when
many training steps are necessary. To address this, we design a
Fourier-transform-based mechanism with significantly less computation and only
a minor utility decrease.
</p>
<p>Extensive empirical evaluation on two tasks: example-level DP for image
classification and user-level DP for language modeling, demonstrate substantial
improvements over the previous state-of-the-art. Though our primary application
is to ML, we note our main DP results are applicable to arbitrary linear
queries and hence may have much broader applicability.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06549'>Hypercubes and Hamiltonian Cycles of Display Sets of Rooted Phylogenetic Networks</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Janosch D&#xf6;cker, Simone Linz, Charles Semple</p><p>In the context of reconstructing phylogenetic networks from a collection of
phylogenetic trees, several characterisations and subsequently algorithms have
been established to reconstruct a phylogenetic network that collectively embeds
all trees in the input in some minimum way. For many instances however, the
resulting network also embeds additional phylogenetic trees that are not part
of the input. However, little is known about these inferred trees. In this
paper, we explore the relationships among all phylogenetic trees that are
embedded in a given phylogenetic network. First, we investigate some
combinatorial properties of the collection P of all rooted binary phylogenetic
trees that are embedded in a rooted binary phylogenetic network N. To this end,
we associated a particular graph G, which we call rSPR graph, with the elements
in P and show that, if |P|=2^k, where k is the number of vertices with
in-degree two in N, then G has a Hamiltonian cycle. Second, by exploiting rSPR
graphs and properties of hypercubes, we turn to the well-studied class of
rooted binary level-1 networks and give necessary and sufficient conditions for
when a set of rooted binary phylogenetic trees can be embedded in a level-1
network without inferring any additional trees. Lastly, we show how these
conditions translate into a polynomial-time algorithm to reconstruct such a
network if it exists.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/math/1/au:+Docker_J/0/1/0/all/0/1">Janosch D&#xf6;cker</a>, <a href="http://arxiv.org/find/math/1/au:+Linz_S/0/1/0/all/0/1">Simone Linz</a>, <a href="http://arxiv.org/find/math/1/au:+Semple_C/0/1/0/all/0/1">Charles Semple</a></p><p>In the context of reconstructing phylogenetic networks from a collection of
phylogenetic trees, several characterisations and subsequently algorithms have
been established to reconstruct a phylogenetic network that collectively embeds
all trees in the input in some minimum way. For many instances however, the
resulting network also embeds additional phylogenetic trees that are not part
of the input. However, little is known about these inferred trees. In this
paper, we explore the relationships among all phylogenetic trees that are
embedded in a given phylogenetic network. First, we investigate some
combinatorial properties of the collection P of all rooted binary phylogenetic
trees that are embedded in a rooted binary phylogenetic network N. To this end,
we associated a particular graph G, which we call rSPR graph, with the elements
in P and show that, if |P|=2^k, where k is the number of vertices with
in-degree two in N, then G has a Hamiltonian cycle. Second, by exploiting rSPR
graphs and properties of hypercubes, we turn to the well-studied class of
rooted binary level-1 networks and give necessary and sufficient conditions for
when a set of rooted binary phylogenetic trees can be embedded in a level-1
network without inferring any additional trees. Lastly, we show how these
conditions translate into a polynomial-time algorithm to reconstruct such a
network if it exists.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06567'>Pareto-Optimal Learning-Augmented Algorithms for Online k-Search Problems</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Russell Lee, Bo Sun, John C.S. Lui, Mohammad Hajiesmaili</p><p>This paper leverages machine learned predictions to design online algorithms
for the k-max and k-min search problems. Our algorithms can achieve
performances competitive with the offline algorithm in hindsight when the
predictions are accurate (i.e., consistency) and also provide worst-case
guarantees when the predictions are arbitrarily wrong (i.e., robustness).
Further, we show that our algorithms have attained the Pareto-optimal trade-off
between consistency and robustness, where no other algorithms for k-max or
k-min search can improve on the consistency for a given robustness. To
demonstrate the performance of our algorithms, we evaluate them in experiments
of buying and selling Bitcoin.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1">Russell Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1">Bo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Lui_J/0/1/0/all/0/1">John C.S. Lui</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajiesmaili_M/0/1/0/all/0/1">Mohammad Hajiesmaili</a></p><p>This paper leverages machine learned predictions to design online algorithms
for the k-max and k-min search problems. Our algorithms can achieve
performances competitive with the offline algorithm in hindsight when the
predictions are accurate (i.e., consistency) and also provide worst-case
guarantees when the predictions are arbitrarily wrong (i.e., robustness).
Further, we show that our algorithms have attained the Pareto-optimal trade-off
between consistency and robustness, where no other algorithms for k-max or
k-min search can improve on the consistency for a given robustness. To
demonstrate the performance of our algorithms, we evaluate them in experiments
of buying and selling Bitcoin.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06636'>Approximation Algorithms for Drone Delivery Scheduling Problem</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Saswata Jana, Partha Sarathi Mandal</p><p>The coordination among drones and ground vehicles for last-mile delivery has
gained significant interest in recent years. In this paper, we study
\textit{multiple drone delivery scheduling problem(MDSP) \cite{Betti_ICDCN22}
for last-mile delivery, where we have a set of drones with an identical battery
budget and a set of delivery locations, along with reward or profit for
delivery, cost and delivery time intervals. The objective of the MDSP is to
find a collection of conflict-free schedules for each drone such that the total
profit for delivery is maximum subject to the battery constraint of the drones.
Here we propose a fully polynomial time approximation scheme (FPTAS) for the
single drone delivery scheduling problem (SDSP) and a
$\frac{1}{4}$-approximation algorithm for MDSP with a constraint on the number
of drones.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Jana_S/0/1/0/all/0/1">Saswata Jana</a>, <a href="http://arxiv.org/find/cs/1/au:+Mandal_P/0/1/0/all/0/1">Partha Sarathi Mandal</a></p><p>The coordination among drones and ground vehicles for last-mile delivery has
gained significant interest in recent years. In this paper, we study
\textit{multiple drone delivery scheduling problem(MDSP) \cite{Betti_ICDCN22}
for last-mile delivery, where we have a set of drones with an identical battery
budget and a set of delivery locations, along with reward or profit for
delivery, cost and delivery time intervals. The objective of the MDSP is to
find a collection of conflict-free schedules for each drone such that the total
profit for delivery is maximum subject to the battery constraint of the drones.
Here we propose a fully polynomial time approximation scheme (FPTAS) for the
single drone delivery scheduling problem (SDSP) and a
$\frac{1}{4}$-approximation algorithm for MDSP with a constraint on the number
of drones.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
    

    <details class='tr-article' open>
      <summary class='tr-article-header'>
        <h3 class='tr-article-title'><a href='http://arxiv.org/abs/2211.06732'>Distributed and secure linear algebra -- Master Thesis</a></h3>
        <p class='tr-article-feed'>from <a href='https://arxiv.org/list/cs.DS/recent'>arXiv: Data Structures and Algorithms</a></p>
        <i class="fa-solid fa-fw fa-chevron-down tr-chevron"></i>
      </summary>

      <i class="fa-solid fa-fw fa-chevron-up tr-chevron"></i>

      <div class='tr-article-body'>
        <div class='tr-article-snippet'>
        
          <p>Authors: Hugo Mirault</p><p>Cryptography is the discipline that allows securing of the exchange of
information. In this internship, we will focus on a certain branch of this
discipline, secure computation in a network. The main goal of this internship,
illustrated in this report, is to adapt a roster of protocols intended to do
linear algebra. We want to adapt them to do algebra for matrices with
polynomial coefficients. We then wish to make a complete analysis of the
different complexities of these protocols.
</p>
        
        </div>

        <div class='tr-article-summary'>
        
          
          <p class="arxiv-authors"><b>Authors:</b> <a href="http://arxiv.org/find/cs/1/au:+Mirault_H/0/1/0/all/0/1">Hugo Mirault</a></p><p>Cryptography is the discipline that allows securing of the exchange of
information. In this internship, we will focus on a certain branch of this
discipline, secure computation in a network. The main goal of this internship,
illustrated in this report, is to adapt a roster of protocols intended to do
linear algebra. We want to adapt them to do algebra for matrices with
polynomial coefficients. We then wish to make a complete analysis of the
different complexities of these protocols.
</p>
        
        </div>

        <div class='tr-article-footer'>
          <time class='timeago' datetime="2022-11-15T01:30:00Z">Tuesday, November 15 2022, 01:30</time>
        </div>
      </div>
    </details>
  
  </div>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js' type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.7/jquery.timeago.min.js" type="text/javascript"></script>
  <script src='js/theory.js'></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
