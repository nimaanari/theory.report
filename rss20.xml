<?xml version="1.0"?>
<rss version="2.0">

<channel>
  <title>Theory of Computing Report</title>
  <link></link>
  <language>en</language>
  <description></description>


<item>
  <title>Open problem - Better privacy guarantees for larger groups</title>
  <guid>https://differentialprivacy.org/open-problem-better-privacy-guarantees-for-larger-groups/</guid>
  <link>https://differentialprivacy.org/open-problem-better-privacy-guarantees-for-larger-groups/</link>
  <description>
    &lt;p&gt;Consider a simple query counting the number of people in various mutually exclusive groups.
In the differential privacy literature, it is typical to assume that each of these groups should be subject to the same privacy loss: the noise added to each count has the same magnitude, and everyone gets the same privacy guarantees.
However, in settings where these groups have vastly different population sizes, larger populations may be willing to accept more error in exchange for stronger privacy protections.
In particular, in many use cases, &lt;em&gt;relative&lt;/em&gt; error (the noisy count is within 5% of the true value) matters more than absolute error (the noisy count is at a distance of at most 100 of the true value).
This leads to a natural question: can we use this fact to develop a mechanism that improves the privacy guarantees of individuals in larger groups, subject to a constraint on relative error?&lt;/p&gt;

&lt;h3 id=&quot;problem-definition&quot;&gt;Problem definition&lt;/h3&gt;

&lt;p&gt;Our goal is to obtain a mechanism which minimizes the overall privacy loss for each group without exceeding a relative error threshold for each group.
To formalize this goal, we first define a notion of per-group privacy we call group-wise zero-concentrated differential privacy as follows.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; &lt;em&gt;Group-wise zero-concentrated differential privacy.&lt;/em&gt;
Assume possible datasets consist of records from domain \(U\), and \(U\) can be partitioned into \(k\) fixed, disjoint groups \(U_1\), …, \(U_k\). Let \(v : \mathcal{D} \rightarrow \mathbb{R}^k\) be a function associating a dataset to a vector of privacy budgets (one per group). We say a mechanism \(\mathcal{M}\) satisfies \(v\)-group-wise zero-concentrated differential privacy (zCDP) if for any two datasets \(D\), \(D’\) differing in the addition or removal of a record in \(U_i\), and for all \(\alpha&amp;gt;1\), we have:
\[
D_\alpha\left(\mathcal{M}(D||\mathcal{M}(D’)\right) \le \alpha \cdot {v(D)}_i
\]
\[
D_\alpha\left(\mathcal{M}(D’)||\mathcal{M}(D)\right) \le \alpha \cdot {v(D)}_i
\]
where \(D_\alpha\) is the Rényi divergence of order \(\alpha\).&lt;/p&gt;

&lt;p&gt;This definition is similar to &lt;em&gt;tailored DP&lt;/em&gt;, defined in [&lt;a href=&quot;https://eprint.iacr.org/2014/982.pdf&quot;&gt;LP15&lt;/a&gt;]: each individual gets a different privacy guarantee, depending on which group they belong to;
this guarantee also depends on how many people are in this group.
We use zCDP as our definition of privacy due to its compatibility with the Gaussian mechanism; the same idea could easily be applied to other definitions like with Rényi DP or pure DP.&lt;/p&gt;

&lt;p&gt;From there we can give a more formal definition of the problem as follows. The goal is to minimize the privacy loss for each individual group, while keeping the error under a given threshold.
For larger groups that can accept more noise, this means adding more noise to achieve the smallest possible privacy loss.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem.&lt;/strong&gt;
Let \(r \in (0,1]\) be an acceptable level of relative error, and \(k\) be the number of distinct, mutually-exclusive partitions of domain \(X\).
Given a dataset \(D\), let \(x(D)\) be a vector containing the count of records in each partition.
The objective is to find a mechanism \(\mathcal{M}\) which takes in \(r\), \(k\), and \(D\) and outputs \(\hat{x}(D)\) such that \(E\left[\left|{x(D)}_i-{\hat{x}(D)}_i\right|\right]&amp;lt;r\cdot {x(D)}_i\) for all \(i\), and satisfies \(v\)-group-wise zCDP where \(v(D)_i\) is as small as possible for all \(i\).
&lt;br /&gt;
To prevent pathological mechanisms that optimize for specific datasets, we add two constraints to the problem: the privacy guarantee \(v(D)_i\) should only depend on \(x(D)_i\), and should be nonincreasing with \(x(D)_i\).&lt;/p&gt;

&lt;p&gt;Since the relative error thresholds are proportional to the population size, each population can tolerate a different amount of noise.
This means that to minimize the privacy loss for each group, the mechanism must add noise of different scales to each group.
Of course, directly using \(x(D)_i\) to determine the scale of the noise for group \(i\) leads to a privacy loss which is data dependent, similarly to e.g. PATE [&lt;a href=&quot;https://openreview.net/forum?id=HkwoSDPg&quot;&gt;PAEGT17&lt;/a&gt;], and as such should be treated as a protected value.&lt;/p&gt;

&lt;h3 id=&quot;an-example-mechanism&quot;&gt;An example mechanism&lt;/h3&gt;

&lt;p&gt;An example mechanism that seems like it could address this problem is as follows.
First, perform the original counting query and add Gaussian noise to satisfy \(\rho\)-zCDP.
Then, add additional Gaussian noise to each count, with a variance that depends on the noisy count itself — adding more noise to larger groups.
This mechanism is outlined in Algorithm 1.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Algorithm 1.&lt;/strong&gt;
&lt;em&gt;Adding data-dependent noise as a post-processing step.&lt;/em&gt;
&lt;br /&gt;
Require: A dataset \(D\) where each data point belongs to one of \(k\) groups, a privacy parameter \(\rho\), and a relative error rate \(r\).&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Let \(\sigma^2 = 1/(2\rho)\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;For&lt;/strong&gt; \(i=1\) to \(k\) &lt;strong&gt;do&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;\(\qquad\) Let \(x_i\) be the number of people in \(D\) in group \(i\)&lt;/li&gt;
  &lt;li&gt;\(\qquad\) Sample \(X_i \sim \mathcal{N}(x_i, \sigma^2)\)&lt;/li&gt;
  &lt;li&gt;\(\qquad\) Sample \(Y_i \sim \mathcal{N}_{k}(X_i, (rX_i)^2)\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;end for&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;return&lt;/strong&gt; \(Y_1,\dots,Y_k\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Algorithm 1 achieves this goal of having approximately \(r\) error in each group: the total variance error of the mechanism is \(\sigma^2 + (rX)^2\), and \(X\) is a zCDP measure of \(f(D)\).
This mechanism satisfies at least \(\rho\)-zCDP: line 4 is an invocation of the Gaussian mechanism with privacy parameter \(\rho\), and line 5 is a post processing step and as such preserves the zCDP guarantee.
We would like to show that this algorithm also satisfies a stronger group-wise zCDP guarantee.&lt;/p&gt;

&lt;p&gt;This makes intuitive sense: line 5 adds additional Gaussian noise without using the private data directly.
Since the noise scale in line 5 is proportional to the total count in line 4, we expect the privacy guarantee to be significantly stronger for large groups with more noise.
Further, we can verify experimentally that when the data magnitude is large compared to the noise, the output distribution for each group is close to a Gaussian distribution.&lt;/p&gt;

&lt;p&gt;The below figure illustrates this finding.
We plot 1,000,000 sample outputs of Algorithm 1 (red) with parameters \(\sigma^2 = 100\) and \(r= 0.3\), and compare it to the best fit Gaussian distribution (black outline) with mean \(10,002.6\) and standard deviation of \(2995.1\).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/two-stage-noise-gaussian.png&quot; width=&quot;70%&quot; alt=&quot;A comparison between sample outputs of Algorithm 1 and the best-fit Gaussian distribution, showing that both match very closely.&quot; style=&quot;margin:auto;display: block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With parameters such as these, the output of the mechanism looks and behaves like a Gaussian distribution, which should be ideal to characterize the zCDP guarantee.
However, it is difficult to directly quantify this guarantee, due to the changing variance which is also a random variable.
Likewise, if the true count is close to zero or if the first instance of noise is large compared to the true count than the resulting distribution takes on a heavy skew and is no longer similar to a single Gaussian distribution.
Such distributions with randomized variances have not, to the best of our knowledge, been considered much in the literature, and we do not know whether the mechanism’s output distribution follows some well-studied distribution.&lt;/p&gt;

&lt;p&gt;The randomized variance also makes it difficult to bound the Rényi divergence of the distribution and characterize the zCDP guarantees directly.
Current privacy amplification techniques are insufficient, as those techniques consider adding additional noise where the noise parameters are independent of the data itself.&lt;/p&gt;

&lt;p&gt;Perhaps the most promising direction to understand more about such processes is the area of stochastic differential equations, where it is common to study noise with data-dependent variance.
The Bessel process [&lt;a href=&quot;http://www.stat.ucla.edu/~ywu/research/documents/StochasticDifferentialEquations.pdf&quot;&gt;Øks03&lt;/a&gt;] is an example of such a process, where the noise is dependent on the current value.
This process captures the noise added as post-processing (Line 5), but not the initial noise-addition step (Line 4).
Furthermore, to the best of our knowledge, the Bessel process and other value-dependent stochastic differential equations do not have closed-form solutions.&lt;/p&gt;

&lt;h3 id=&quot;goal&quot;&gt;Goal&lt;/h3&gt;

&lt;p&gt;We see two possible paths forward to address the original question. One path would be to obtain an analysis of Algorithm 1 which shows non-trivial improved privacy guarantees for larger groups.
We tried multiple approaches, but could not prove such a result.&lt;/p&gt;

&lt;p&gt;An alternative path would be to develop a different algorithm, which achieves better privacy guarantees for larger groups while maintaining the error below the relative error threshold for all groups.&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By &lt;/p&gt;
  </description>
  <pubDate>2023-06-27 01:00:00 UTC</pubDate>
  <author>DifferentialPrivacy.org</author>
</item>

<item>
  <title>On the local consequence of modal Product logic: standard completeness and decidability</title>
  <guid>http://arxiv.org/abs/2306.13903</guid>
  <link>http://arxiv.org/abs/2306.13903</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Vidal_A/0/1/0/all/0/1&quot;&gt;Amanda Vidal&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Modal extensions of Product fuzzy logic can be considered both over Kripke
models whose accessibility relation is valued, and over Kripke models with
classical accessibility relation. We study the local consequence of the
previous two modal Product logics. We prove that these logics are standard
complete, in the sense that the logic defined over Kripke models evaluated over
all product algebras coincides with that defined over Kripke models evaluated
over the standard product algebra (with universe [0,1]). This holds both for
the logic over classical Kripke frames, and for that over frames with a valued
accessibility relation. Second, we prove that the previous logics are
decidable.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-27 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>A Circuit Complexity Formulation of Algorithmic Information Theory</title>
  <guid>http://arxiv.org/abs/2306.14087</guid>
  <link>http://arxiv.org/abs/2306.14087</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wyeth_C/0/1/0/all/0/1&quot;&gt;Cole Wyeth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sturtivant_C/0/1/0/all/0/1&quot;&gt;Carl Sturtivant&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Inspired by Solomonoffs theory of inductive inference, we propose a prior
based on circuit complexity. There are several advantages to this approach.
First, it relies on a complexity measure that does not depend on the choice of
UTM. There is one universal definition for Boolean circuits involving an
universal operation such as nand with simple conversions to alternative
definitions such as and, or, and not. Second, there is no analogue of the
halting problem. The output value of a circuit can be calculated recursively by
computer in time proportional to the number of gates, while a short program may
run for a very long time. Our prior assumes that a Boolean function, or
equivalently, Boolean string of fixed length, is generated by some Bayesian
mixture of circuits. This model is appropriate for learning Boolean functions
from partial information, a problem often encountered within machine learning
as &quot;binary classification.&quot; We argue that an inductive bias towards simple
explanations as measured by circuit complexity is appropriate for this problem.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-27 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Using persistent homology to understand dimensionality reduction in resting-state fMRI</title>
  <guid>http://arxiv.org/abs/2306.13802</guid>
  <link>http://arxiv.org/abs/2306.13802</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Easley_T/0/1/0/all/0/1&quot;&gt;Ty Easley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freese_K/0/1/0/all/0/1&quot;&gt;Kevin Freese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munch_E/0/1/0/all/0/1&quot;&gt;Elizabeth Munch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bijsterbosch_J/0/1/0/all/0/1&quot;&gt;Janine Bijsterbosch&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Evaluating the success of a manifold learning method remains a challenging
problem, especially for methods adapted to a specific application domain. The
present work investigates shared geometric structure across different
dimensionality reduction (DR) algorithms within the scope of neuroimaging
applications. We examine reduced-dimension embeddings produced by a
representative assay of dimension reductions for brain data (&quot;brain
representations&quot;) through the lens of persistent homology, making statistical
claims about topological differences using a recent topological boostrap
method. We cluster these methods based on their induced topologies, finding
feature type and number -- rather than reduction algorithm -- as the main
drivers of observed topological differences.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-27 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Adaptive Privacy Composition for Accuracy-first Mechanisms</title>
  <guid>http://arxiv.org/abs/2306.13824</guid>
  <link>http://arxiv.org/abs/2306.13824</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rogers_R/0/1/0/all/0/1&quot;&gt;Ryan Rogers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samorodnitsky_G/0/1/0/all/0/1&quot;&gt;Gennady Samorodnitsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Steven Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramdas_A/0/1/0/all/0/1&quot;&gt;Aaditya Ramdas&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In many practical applications of differential privacy, practitioners seek to
provide the best privacy guarantees subject to a target level of accuracy. A
recent line of work by \cite{LigettNeRoWaWu17, WhitehouseWuRaRo22} has
developed such accuracy-first mechanisms by leveraging the idea of \emph{noise
reduction} that adds correlated noise to the sufficient statistic in a private
computation and produces a sequence of increasingly accurate answers. A major
advantage of noise reduction mechanisms is that the analysts only pay the
privacy cost of the least noisy or most accurate answer released. Despite this
appealing property in isolation, there has not been a systematic study on how
to use them in conjunction with other differentially private mechanisms. A
fundamental challenge is that the privacy guarantee for noise reduction
mechanisms is (necessarily) formulated as \emph{ex-post privacy} that bounds
the privacy loss as a function of the released outcome. Furthermore, there has
yet to be any study on how ex-post private mechanisms compose, which allows us
to track the accumulated privacy over several mechanisms. We develop privacy
filters \citep{RogersRoUlVa16, FeldmanZr21, WhitehouseRaRoWu22} that allow an
analyst to adaptively switch between differentially private and ex-post private
mechanisms subject to an overall privacy guarantee.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-27 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>A Dynamic Data Structure for Representing Timed Transitive Closures on Disk</title>
  <guid>http://arxiv.org/abs/2306.13937</guid>
  <link>http://arxiv.org/abs/2306.13937</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brito_L/0/1/0/all/0/1&quot;&gt;Luiz F. Afra Brito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albertini_M/0/1/0/all/0/1&quot;&gt;Marcelo Keese Albertini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Travencolo_B/0/1/0/all/0/1&quot;&gt;Bruno A. N. Traven&amp;#xe7;olo&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Temporal graphs represent interactions between entities over time. These
interactions may be direct, a contact between two vertices at some time
instant, or indirect, through sequences of contacts called journeys. Deciding
whether an entity can reach another through a journey is useful for various
applications in complex networks. In this paper, we present a disk-based data
structure that maintains temporal reachability information under the addition
of new contacts in a non-chronological order. It represents the \emph{timed
transitive closure} (TTC) by a set of \emph{expanded} R-tuples of the form $(u,
v, t^-, t^+)$, which encodes the existence of journeys from vertex $u$ to
vertex $v$ with departure at time $t^-$ and arrival at time $t^+$. Let $n$ be
the number of vertices and $\tau$ be the number of timestamps in the lifetime
of the temporal graph. Our data structure explicitly maintains this information
in linear arrays using $O(n^2\tau)$ space so that sequential accesses on disk
are prioritized. Furthermore, it adds a new unsorted contact $(u, v, t)$
accessing $O\left(\frac{n^2\tau}{B}\right)$ sequential pages in the worst-case,
where $B$ is the of pages on disk; it answers whether there is of a journey
from a vertex $u$ to a vertex $v$ within a time interval $[t_1, t_2]$ accessing
a single page; it answers whether all vertices can reach each other in $[t_1,
t_2]$; and it reconstructs a valid journey that validates the reachability from
a vertex $u$ to a vertex $v$ within $[t_1, t_1]$ accessing
$O\left(\frac{n\tau}{B}\right)$ pages. Our experiments show that our novel data
structure are better that the best known approach for the majority of cases
using synthetic and real world datasets.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-27 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>On Scalable Testing of Samplers</title>
  <guid>http://arxiv.org/abs/2306.13958</guid>
  <link>http://arxiv.org/abs/2306.13958</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pote_Y/0/1/0/all/0/1&quot;&gt;Yash Pote&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meel_K/0/1/0/all/0/1&quot;&gt;Kuldeep S. Meel&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this paper we study the problem of testing of constrained samplers over
high-dimensional distributions with $(\varepsilon,\eta,\delta)$ guarantees.
Samplers are increasingly used in a wide range of safety-critical ML
applications, and hence the testing problem has gained importance. For
$n$-dimensional distributions, the existing state-of-the-art algorithm,
$\mathsf{Barbarik2}$, has a worst case query complexity of exponential in $n$
and hence is not ideal for use in practice. Our primary contribution is an
exponentially faster algorithm that has a query complexity linear in $n$ and
hence can easily scale to larger instances. We demonstrate our claim by
implementing our algorithm and then comparing it against $\mathsf{Barbarik2}$.
Our experiments on the samplers $\mathsf{wUnigen3}$ and $\mathsf{wSTS}$, find
that $\mathsf{Barbarik3}$ requires $10\times$ fewer samples for
$\mathsf{wUnigen3}$ and $450\times$ fewer samples for $\mathsf{wSTS}$ as
compared to $\mathsf{Barbarik2}$.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-27 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Approximation Algorithm for Unrooted Prize-Collecting Forest with Multiple Components and Its Application on Prize-Collecting Sweep Coverage</title>
  <guid>http://arxiv.org/abs/2306.13996</guid>
  <link>http://arxiv.org/abs/2306.13996</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Wei Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhao Zhang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this paper, we present a polynomial time 2-approximation algorithm for the
{\em unrooted prize-collecting forest with $K$ components} (URPCF$_K$) problem,
the goal of which is to find a forest with exactly $K$ connected components to
minimize the weight of the forest plus the penalty incurred by the vertices not
spanned by the forest. For its rooted version RPCF$_K$, a 2-approximation
algorithm is known. For the unrooted version, transforming it into a rooted
version by guessing roots runs in time exponentially depending on $K$, which is
unacceptable when $K$ is not a constant. We conquer this challenge by designing
a rootless growing plus rootless pruning algorithm. As an application, we make
use of this algorithm to solve the {\em prize-collecting min-sensor sweep
cover} problem, improving previous approximation ratio 8 to 5.
&lt;/p&gt;
&lt;p&gt;Keywords: approximation algorithm, prize-collecting Steiner forest, sweep
cover.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-27 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>On finding 2-cuts and 3-edge-connected components in parallel</title>
  <guid>http://arxiv.org/abs/2306.14103</guid>
  <link>http://arxiv.org/abs/2306.14103</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsin_Y/0/1/0/all/0/1&quot;&gt;Yung H. Tsin&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Given a connected undirected multigraph G (a graph that may contain parallel
edges), the algorithm of [19] finds the 3-edge-connected components of $G$ in
linear time using an innovative graph contraction technique based on a
depth-first search. In [21], it was shown that the algorithm can be extended to
produce a Mader construction sequence for each 3-edge-connected component, a
cactus representation of the 2-cuts (cut-pairs) of each 2-edge-connected
component of $G$, and the 1-cuts (bridges) at the same time.
&lt;/p&gt;
&lt;p&gt;In this paper, we further extend the algorithm of [19] to generate the 2-cuts
and the 3-edge-connected components of $G$ simultaneously in linear time by
performing only one depth-first search over the input graph. Previously known
algorithms solve the two problems separately in multiple phases.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-27 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>AI Ends It All</title>
  <guid>https://rjlipton.wpcomstaging.com/?p=21826</guid>
  <link>https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/</link>
  <description>
    &lt;p&gt;
I was getting a lift with a friend&amp;#8212;Greg Skau&amp;#8212;just the other day. No not in his boat, but in his car. &lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/gs/&quot; rel=&quot;attachment wp-att-21828&quot;&gt;&lt;img data-attachment-id=&quot;21828&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/gs/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?fit=219%2C320&amp;amp;ssl=1&quot; data-orig-size=&quot;219,320&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;1&amp;quot;}&quot; data-image-title=&quot;gs&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?fit=205%2C300&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?fit=219%2C320&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?resize=219%2C320&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;219&quot; height=&quot;320&quot; class=&quot;aligncenter size-full wp-image-21828&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?w=219&amp;amp;ssl=1 219w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gs.jpeg?resize=205%2C300&amp;amp;ssl=1 205w&quot; sizes=&quot;(max-width: 219px) 100vw, 219px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;br /&gt;
Our conversation turned to the topic of: &amp;#8220;is AI a threat to all of us?&amp;#8221; Indeed. See &lt;a href=&quot;https://www.standard.co.uk/insider/ai-apocalypse-life-robots-take-over-elon-musk-chatgpt-b1078423.html&quot;&gt;this&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; The year is 2050. The location is London&amp;#8212; but not as we know it. GodBot, a robot so intelligent it can out-smart any human, is in charge of the United Kingdom &amp;#8212; the entire planet, in fact &amp;#8212; and just announced its latest plan to reverse global temperature rises: an international zero-child, zero-reproduction policy, which will see all human females systematically destroyed and replaced with carbon-neutral sex robots. &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
That my friend Greg would raise questions about AI seemed pretty neat. It seemed natural yet quite cool that a friend&amp;#8212;who was not an AI expert&amp;#8212;would raise these issues. I am also not an AI expert&amp;#8212;not even an advanced beginner. But it is on just about everyone&amp;#8217;s top list of questions. We had a fun conversation, but failed to resolve the issue. Of course.&lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; AI Limits &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
&lt;a href=&quot;https://openai.com&quot;&gt;OpenAI&lt;/a&gt; has just revealed that ChatGPT-4 has learned to lie, telling a human it was a blind person in order to get a task done. Somehow lies seem to set such AI systems apart from what we might have thought were the limits of AI.&lt;/p&gt;
&lt;p&gt;
Another thought on AI is: Is physical law an &lt;a href=&quot;https://getpocket.com/explore/item/is-physical-law-an-alien-intelligence?utm_source=pocket-newtab&quot;&gt;Alien Intelligence?&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; Arthur Clarke once pointed out that any sufficiently advanced technology is going to be indistinguishable from magic. If you dropped in on a bunch of Paleolithic farmers with your iPhone and a pair of sneakers, you&amp;#8217;d undoubtedly seem pretty magical. But the contrast is only middling: The farmers would still recognize you as basically like them, and before long they&amp;#8217;d be taking selfies. But what if life has moved so far on that it doesn&amp;#8217;t just appear magical, but appears like physics? &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
This is related to Clarke&amp;#8217;s &lt;a href=&quot;https://en.wikipedia.org/wiki/Clarke&amp;#37;27s_three_laws&quot;&gt;three laws&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong. &lt;/p&gt;
&lt;li&gt;
The only way of discovering the limits of the possible is to venture a little way past them into the impossible. &lt;/p&gt;
&lt;li&gt;
Any sufficiently advanced technology is indistinguishable from magic.
&lt;/ul&gt;
&lt;p&gt;
Or take a look at his T-shirt (referring to &lt;a href=&quot;https://web.mit.edu/m-i-t/science_fiction/jenkins/jenkins_4.html&quot;&gt;this&lt;/a&gt;): &lt;/p&gt;
&lt;p&gt;&lt;P&gt;&lt;br /&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/ac/&quot; rel=&quot;attachment wp-att-21829&quot;&gt;&lt;img data-attachment-id=&quot;21829&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/ac/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?fit=240%2C240&amp;amp;ssl=1&quot; data-orig-size=&quot;240,240&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;ac&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?fit=240%2C240&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?fit=240%2C240&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?resize=240%2C240&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;240&quot; height=&quot;240&quot; class=&quot;aligncenter size-full wp-image-21829&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?w=240&amp;amp;ssl=1 240w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?resize=150%2C150&amp;amp;ssl=1 150w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ac.jpeg?resize=200%2C200&amp;amp;ssl=1 200w&quot; sizes=&quot;(max-width: 240px) 100vw, 240px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; Open Problems &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
Alan Perlis&amp;#8212;the first Turing award winner&amp;#8212;is famous for his many &lt;a href=&quot;http://www.cs.yale.edu/homes/perlis-alan/quotes.html&quot;&gt;quotes&lt;/a&gt;. One was: &amp;#8220;A year spent in artificial intelligence is enough to make one believe in God.&amp;#8221;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/ai/&quot; rel=&quot;attachment wp-att-21830&quot;&gt;&lt;img data-attachment-id=&quot;21830&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/26/ai-ends-it-all/ai/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ai.jpeg?fit=300%2C240&amp;amp;ssl=1&quot; data-orig-size=&quot;300,240&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;ai&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ai.jpeg?fit=300%2C240&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ai.jpeg?fit=300%2C240&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ai.jpeg?resize=300%2C240&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;300&quot; height=&quot;240&quot; class=&quot;aligncenter size-full wp-image-21830&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
I enjoyed working for Alan at my first job at Yale University, many years ago. See &lt;a href=&quot;https://blog.computationalcomplexity.org/2021/06/i-went-to-debate-about-program-verif.html&quot;&gt;Fortnow&amp;#8217;s&lt;/a&gt; blog for comments on our joint work with Perlis and Rich DeMillo&amp;#8212;&lt;a href=&quot;https://www.cs.umd.edu/~gasarch/BLOGPAPERS/social.pdf&quot;&gt;Social Processes and Proofs of Theorems and Programs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;
Ken pipes in that the record seems to indicate that this late-1970s quote reflected frustration during the long &amp;#8220;AI winter&amp;#8221; when basic human capabilities stayed beyond reach. He also notes that the 2050 date for sex robots was &lt;a href=&quot;https://en.wikipedia.org/wiki/Love_and_Sex_with_Robots&quot;&gt;forecast&lt;/a&gt; by the British chess master who was previously best known for winning a famous computer bet in 1978.&lt;/p&gt;
&lt;p&gt;
&lt;p class=&quot;authors&quot;&gt;By rjlipton&lt;/p&gt;
  </description>
  <pubDate>2023-06-26 17:56:41 UTC</pubDate>
  <author>Richard Lipton</author>
</item>

<item>
  <title>$3f+1$ is needed in Partial Synchrony even against a Rollback adversary</title>
  <guid>https://decentralizedthoughts.github.io/2023-06-26-dls-meets-rollback/</guid>
  <link>https://decentralizedthoughts.github.io/2023-06-26-dls-meets-rollback/</link>
  <description>
    We covered the classic DLS88 split brain impossibility result against a Byzantine adversary in a previous post: DLS88: (Theorem 4.4) It is impossible to solve Agreement under partial synchrony against a Byzantine adversary if $f \geq n/3$. In a follow up, we discussed how CJKR12 strengthen this result by observing...
  </description>
  <pubDate>2023-06-26 11:00:00 UTC</pubDate>
  <author>Decentralized Thoughts</author>
</item>

<item>
  <title>Quantum Merlin-Arthur and proofs without relative phase</title>
  <guid>http://arxiv.org/abs/2306.13247</guid>
  <link>http://arxiv.org/abs/2306.13247</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Bassirian_R/0/1/0/all/0/1&quot;&gt;Roozbeh Bassirian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Fefferman_B/0/1/0/all/0/1&quot;&gt;Bill Fefferman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Marwaha_K/0/1/0/all/0/1&quot;&gt;Kunal Marwaha&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study a variant of QMA where quantum proofs have no relative phase (i.e.
non-negative amplitudes, up to a global phase). If only completeness is
modified, this class is equal to QMA [&lt;a href=&quot;/abs/1410.2882&quot;&gt;arXiv:1410.2882&lt;/a&gt;]; but if both
completeness and soundness are modified, the class (named QMA+ by Jeronimo and
Wu) can be much more powerful. We show that QMA+ with some constant gap is
equal to NEXP, yet QMA+ with some *other* constant gap is equal to QMA. One
interpretation is that Merlin&#39;s ability to &quot;deceive&quot; originates from relative
phase at least as much as from entanglement, since QMA(2) $\subseteq$ NEXP.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-26 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>A SAT Solver and Computer Algebra Attack on the Minimum Kochen-Specker Problem</title>
  <guid>http://arxiv.org/abs/2306.13319</guid>
  <link>http://arxiv.org/abs/2306.13319</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Bright_C/0/1/0/all/0/1&quot;&gt;Curtis Bright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ganesh_V/0/1/0/all/0/1&quot;&gt;Vijay Ganesh&lt;/a&gt;&lt;/p&gt;&lt;p&gt;One of the foundational results in quantum mechanics is the Kochen-Specker
(KS) theorem, which states that any theory whose predictions agree with quantum
mechanics must be contextual, i.e., a quantum observation cannot be understood
as revealing a pre-existing value. The theorem hinges on the existence of a
mathematical object called a KS vector system. While many KS vector systems are
known to exist, the problem of finding the minimum KS vector system has
remained stubbornly open for over 55 years, despite significant attempts by
leading scientists and mathematicians. In this paper, we present a new method
based on a combination of a SAT solver and a computer algebra system (CAS) to
address this problem. Our approach improves the lower bound on the minimum
number of vectors in a KS system from 22 to 24, and is about 35,000 times more
efficient compared to the previous best computational methods. The increase in
efficiency derives from the fact we are able to exploit the powerful
combinatorial search-with-learning capabilities of a SAT solver together with
the isomorph-free exhaustive generation methods of a CAS. The quest for the
minimum KS vector system is motivated by myriad applications such as
simplifying experimental tests of contextuality, zero-error classical
communication, dimension witnessing, and the security of certain quantum
cryptographic protocols. To the best of our knowledge, this is the first
application of a novel SAT+CAS system to a problem in the realm of quantum
foundations.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-26 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Decomposition of Geometric Graphs into Star Forests</title>
  <guid>http://arxiv.org/abs/2306.13201</guid>
  <link>http://arxiv.org/abs/2306.13201</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pach_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe1;nos Pach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Saghafian_M/0/1/0/all/0/1&quot;&gt;Morteza Saghafian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Schnider_P/0/1/0/all/0/1&quot;&gt;Patrick Schnider&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We solve a problem of Dujmovi\&#39;c and Wood (2007) by showing that a complete
convex geometric graph on $n$ vertices cannot be decomposed into fewer than
$n-1$ star-forests, each consisting of noncrossing edges. This bound is clearly
tight. We also discuss similar questions for abstract graphs.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-26 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Effective data reduction algorithm for topological data analysis</title>
  <guid>http://arxiv.org/abs/2306.13312</guid>
  <link>http://arxiv.org/abs/2306.13312</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Seonmi Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jinseok Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jeong Rye Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Seung Yeop Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_H/0/1/0/all/0/1&quot;&gt;Hongdae Yun&lt;/a&gt;&lt;/p&gt;&lt;p&gt;One of the most interesting tools that have recently entered the data science
toolbox is topological data analysis (TDA). With the explosion of available
data sizes and dimensions, identifying and extracting the underlying structure
of a given dataset is a fundamental challenge in data science, and TDA provides
a methodology for analyzing the shape of a dataset using tools and prospects
from algebraic topology. However, the computational complexity makes it quickly
infeasible to process large datasets, especially those with high dimensions.
Here, we introduce a preprocessing strategy called the Characteristic Lattice
Algorithm (CLA), which allows users to reduce the size of a given dataset as
desired while maintaining geometric and topological features in order to make
the computation of TDA feasible or to shorten its computation time. In
addition, we derive a stability theorem and an upper bound of the barcode
errors for CLA based on the bottleneck distance.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-26 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Adversarial Resilience in Sequential Prediction via Abstention</title>
  <guid>http://arxiv.org/abs/2306.13119</guid>
  <link>http://arxiv.org/abs/2306.13119</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1&quot;&gt;Surbhi Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1&quot;&gt;Steve Hanneke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1&quot;&gt;Shay Moran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shetty_A/0/1/0/all/0/1&quot;&gt;Abhishek Shetty&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the problem of sequential prediction in the stochastic setting with
an adversary that is allowed to inject clean-label adversarial (or
out-of-distribution) examples. Algorithms designed to handle purely stochastic
data tend to fail in the presence of such adversarial examples, often leading
to erroneous predictions. This is undesirable in many high-stakes applications
such as medical recommendations, where abstaining from predictions on
adversarial examples is preferable to misclassification. On the other hand,
assuming fully adversarial data leads to very pessimistic bounds that are often
vacuous in practice.
&lt;/p&gt;
&lt;p&gt;To capture this motivation, we propose a new model of sequential prediction
that sits between the purely stochastic and fully adversarial settings by
allowing the learner to abstain from making a prediction at no cost on
adversarial examples. Assuming access to the marginal distribution on the
non-adversarial examples, we design a learner whose error scales with the VC
dimension (mirroring the stochastic setting) of the hypothesis class, as
opposed to the Littlestone dimension which characterizes the fully adversarial
setting. Furthermore, we design a learner for VC dimension~1 classes, which
works even in the absence of access to the marginal distribution. Our key
technical contribution is a novel measure for quantifying uncertainty for
learning VC classes, which may be of independent interest.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-26 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Breaking the cubic barrier in the Solovay-Kitaev algorithm</title>
  <guid>http://arxiv.org/abs/2306.13158</guid>
  <link>http://arxiv.org/abs/2306.13158</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kuperberg_G/0/1/0/all/0/1&quot;&gt;Greg Kuperberg&lt;/a&gt; (UC Davis)&lt;/p&gt;&lt;p&gt;We improve the Solovay-Kitaev theorem and algorithm for a general finite,
inverse-closed generating set acting on a qudit. Prior versions of the
algorithm can efficiently find a word of length $O((\log
1/\epsilon)^{3+\delta})$ to approximate an arbitrary target gate to within
$\epsilon$. Using two new ideas, each of which reduces the exponent separately,
our new bound on the world length is $O((\log
1/\epsilon)^{1.44042\ldots+\delta})$. Our result holds more generally for any
finite set that densely generates any connected, semisimple real Lie group,
with an extra length term in the non-compact case to reach group elements far
away from the identity.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-26 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>A Fast Maximum $k$-Plex Algorithm Parameterized by the Degeneracy Gap</title>
  <guid>http://arxiv.org/abs/2306.13258</guid>
  <link>http://arxiv.org/abs/2306.13258</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhengren Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chunyu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1&quot;&gt;Mingyu Xiao&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Given a graph, the $k$-plex is a vertex set in which each vertex is not
adjacent to at most $k-1$ other vertices in the set. The maximum $k$-plex
problem, which asks for the largest $k$-plex from a given graph, is an
important but computationally challenging problem in applications like graph
search and community detection. So far, there is a number of empirical
algorithms without sufficient theoretical explanations on the efficiency. We
try to bridge this gap by defining a novel parameter of the input instance,
$g_k(G)$, the gap between the degeneracy bound and the size of maximum $k$-plex
in the given graph, and presenting an exact algorithm parameterized by
$g_k(G)$. In other words, we design an algorithm with running time polynomial
in the size of input graph and exponential in $g_k(G)$ where $k$ is a constant.
Usually, $g_k(G)$ is small and bounded by $O(\log{(|V|)})$ in real-world
graphs, indicating that the algorithm runs in polynomial time. We also carry
out massive experiments and show that the algorithm is competitive with the
state-of-the-art solvers. Additionally, for large $k$ values such as $15$ and
$20$, our algorithm has superior performance over existing algorithms.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-26 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>On minimum $t$-claw deletion in split graphs</title>
  <guid>http://arxiv.org/abs/2306.13306</guid>
  <link>http://arxiv.org/abs/2306.13306</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Sounaka Mishra&lt;/a&gt;&lt;/p&gt;&lt;p&gt;For $t\geq 3$, $K_{1, t}$ is called $t$-claw. In minimum $t$-claw deletion
problem (\texttt{Min-$t$-Claw-Del}), given a graph $G=(V, E)$, it is required
to find a vertex set $S$ of minimum size such that $G[V\setminus S]$ is
$t$-claw free. In a split graph, the vertex set is partitioned into two sets
such that one forms a clique and the other forms an independent set. Every
$t$-claw in a split graph has a center vertex in the clique partition. This
observation motivates us to consider the minimum one-sided bipartite $t$-claw
deletion problem (\texttt{Min-$t$-OSBCD}). Given a bipartite graph $G=(A \cup
B, E)$, in \texttt{Min-$t$-OSBCD} it is asked to find a vertex set $S$ of
minimum size such that $G[V \setminus S]$ has no $t$-claw with the center
vertex in $A$. A primal-dual algorithm approximates \texttt{Min-$t$-OSBCD}
within a factor of $t$. We prove that it is $\UGC$-hard to approximate with a
factor better than $t$. We also prove it is approximable within a factor of 2
for dense bipartite graphs. By using these results on \texttt{Min-$t$-OSBCD},
we prove that \texttt{Min-$t$-Claw-Del} is $\UGC$-hard to approximate within a
factor better than $t$, for split graphs. We also consider their complementary
maximization problems and prove that they are $\APX$-complete.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-26 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Fair integer programming under dichotomous preferences</title>
  <guid>http://arxiv.org/abs/2306.13383</guid>
  <link>http://arxiv.org/abs/2306.13383</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demeulemeester_T/0/1/0/all/0/1&quot;&gt;Tom Demeulemeester&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goossens_D/0/1/0/all/0/1&quot;&gt;Dries Goossens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hermans_B/0/1/0/all/0/1&quot;&gt;Ben Hermans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leus_R/0/1/0/all/0/1&quot;&gt;Roel Leus&lt;/a&gt;&lt;/p&gt;&lt;p&gt;One cannot make truly fair decisions using integer linear programs unless one
controls the selection probabilities of the (possibly many) optimal solutions.
For this purpose, we propose a unified framework when binary decision variables
represent agents with dichotomous preferences, who only care about whether they
are selected in the final solution. We develop several general-purpose
algorithms to fairly select optimal solutions, for example, by maximizing the
Nash product or the minimum selection probability, or by using a random
ordering of the agents as a selection criterion (Random Serial Dictatorship).
As such, we embed the black-box procedure of solving an integer linear program
into a framework that is explainable from start to finish. Moreover, we study
the axiomatic properties of the proposed methods by embedding our framework
into the rich literature of cooperative bargaining and probabilistic social
choice. Lastly, we evaluate the proposed methods on a specific application,
namely kidney exchange. We find that while the methods maximizing the Nash
product or the minimum selection probability outperform the other methods on
the evaluated welfare criteria, methods such as Random Serial Dictatorship
perform reasonably well in computation times that are similar to those of
finding a single optimal solution.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-26 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Improved Competitive Ratios for Online Bipartite Matching on Degree Bounded Graphs</title>
  <guid>http://arxiv.org/abs/2306.13387</guid>
  <link>http://arxiv.org/abs/2306.13387</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yilong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaowei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shengwei Zhou&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We consider the online bipartite matching problem on $(k,d)$-bounded graphs,
where each online vertex has at most $d$ neighbors, each offline vertex has at
least $k$ neighbors, and $k\geq d\geq 2$. The model of $(k,d)$-bounded graphs
is proposed by Naor and Wajc (EC 2015 and TEAC 2018) to model the online
advertising applications in which offline advertisers are interested in a large
number of ad slots, while each online ad slot is interesting to a small number
of advertisers. They proposed deterministic and randomized algorithms with a
competitive ratio of $1 - (1-1/d)^k$ for the problem, and show that the
competitive ratio is optimal for deterministic algorithms. They also raised the
open questions of whether strictly better competitive ratios can be achieved
using randomized algorithms, for both the adversarial and stochastic arrival
models. In this paper we answer both of their open problems affirmatively. For
the adversarial arrival model, we propose a randomized algorithm with
competitive ratio $1 - (1-1/d)^k + \Omega(d^{-4}\cdot e^{-\frac{k}{d}})$ for
all $k\geq d\geq 2$. We also consider the stochastic model and show that even
better competitive ratios can be achieved. We show that for all $k\geq d\geq
2$, the competitive ratio is always at least $0.8237$. We further consider the
$b$-matching problem when each offline vertex can be matched at most $b$ times,
and provide several competitive ratio lower bounds for the adversarial and
stochastic model.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-26 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>TR23-091 |  Succinct Computational Secret Sharing | 

	Benny Applebaum, 

	Amos Beimel, 

	Yuval Ishai, 

	Eyal Kushilevitz, 

	Tianren Liu, 

	Vinod Vaikuntanathan</title>
  <guid>https://eccc.weizmann.ac.il/report/2023/091</guid>
  <link>https://eccc.weizmann.ac.il/report/2023/091</link>
  <description>
    A secret-sharing scheme enables a dealer to share a secret $s$ among $n$ parties such that only authorized subsets of parties, specified by a monotone access structure $f:\{0,1\}^n\to\{0,1\}$, can reconstruct $s$ from their shares. Other subsets of parties learn nothing about $s$.

The question of minimizing the (largest) share size for a given $f$ has been the subject of a large body of work. However, in most existing constructions for general access structures $f$, the share size is not much smaller than the size of some natural computational representation of $f$, a fact that has often been referred to as the ``representation size barrier&amp;#39;&amp;#39; in secret sharing.

In this work, we initiate a systematic study of succinct computational  secret sharing (SCSS), where the secrecy requirement is computational and the goal is to substantially beat the representation size barrier. We obtain the following main results.

(1) SCSS via Projective PRGs. We introduce the notion of a *projective PRG*, a pseudorandom generator for which any subset of the output bits can be revealed while keeping the other output bits hidden, using a *short* projective seed. We construct projective PRGs with different levels of succinctness under a variety of computational assumptions, and apply them towards constructing SCSS for graph access structures, monotone CNF formulas, and (less succinctly) useful subclasses of monotone circuits and branching programs. Most notably, under the sub-exponential RSA assumption, we obtain a SCSS scheme that, given an arbitrary access structure $f$, represented by a truth table of size $N=2^n$, produces shares of size $\polylog(N)=\poly(n)$ in time $\tilde O(N)$. For comparison, the share size of the best known information-theoretic schemes is $O(N^{0.58})$.

(2) SCSS via One-way Functions. Under the (minimal) assumption that one-way functions exist, we obtain a near-quadratic separation between the total share size of computational and information-theoretic secret sharing. This is the strongest separation one can hope for, given the state of the art in secret sharing lower bounds.
We also construct SCSS schemes from one-way functions for useful classes of access structures, including forbidden graphs and monotone DNF formulas.  This leads to constructions of fully-decomposable conditional disclosure of secrets (also known as privacy-free garbled circuits) for general functions, represented by a truth table of size $N=2^n$, with share size $\polylog(N)$ and computation time $\tilde O(N)$, assuming sub-exponentially secure one-way functions.
  </description>
  <pubDate>2023-06-25 01:21:48 UTC</pubDate>
  <author>ECCC Papers</author>
</item>

<item>
  <title>A Hidden Heroine</title>
  <guid>https://rjlipton.wpcomstaging.com/?p=21806</guid>
  <link>https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/</link>
  <description>
    &lt;p&gt;William Friedman was famous as one who broke codes during both world wars. I knew about him from articles such as &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5419462/pdf/1.pdf&quot;&gt;this&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/wf/&quot; rel=&quot;attachment wp-att-21808&quot;&gt;&lt;img data-attachment-id=&quot;21808&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/wf/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/wf.jpeg?fit=299%2C168&amp;amp;ssl=1&quot; data-orig-size=&quot;299,168&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;wf&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/wf.jpeg?fit=299%2C168&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/wf.jpeg?fit=299%2C168&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/wf.jpeg?resize=299%2C168&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;299&quot; height=&quot;168&quot; class=&quot;aligncenter size-full wp-image-21808&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
But wait &lt;img decoding=&quot;async&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&amp;#038;c=20201002&quot; alt=&quot;{&amp;#92;dots}&quot; class=&quot;latex&quot; /&gt; His wife Elizebeth Smith Friedman is the star of a PBS TV &lt;a href=&quot;https://www.imdb.com/title/tt12599258/&quot;&gt;special&lt;/a&gt;. Together they were the first great cryptographers of modern times. They quickly shifted gear from working on the hypothesis of embedded cryptograms in William Shakespeare&amp;#8217;s plays in 1915&amp;#8211;16 to helping the US WW I effort from 1917 on. &lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/ef/&quot; rel=&quot;attachment wp-att-21809&quot;&gt;&lt;img data-attachment-id=&quot;21809&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/ef/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ef.jpeg?fit=271%2C186&amp;amp;ssl=1&quot; data-orig-size=&quot;271,186&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;ef&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ef.jpeg?fit=271%2C186&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ef.jpeg?fit=271%2C186&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/ef.jpeg?resize=271%2C186&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;271&quot; height=&quot;186&quot; class=&quot;aligncenter size-full wp-image-21809&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
What is so interesting is that I was unaware of her great contributions. I thought I knew the history of code breaking. But I was totally wrong. Elizebeth Friedman&amp;#8217;s work on decrypting coded radio messages helped tip the balances of WWI and WWII. She saved thousands of lives, but her work was hidden by the US government for 62 years. Her superiors&amp;#8212;all men&amp;#8212;took credit for her work. She initially got &lt;em&gt;none&lt;/em&gt;. Nothing at all. &lt;/p&gt;
&lt;p&gt;
This is&amp;#8212;at least it was&amp;#8212;one of the terrible injustices in the history of code breaking. &lt;/p&gt;
&lt;p&gt;
&lt;span id=&quot;more-21806&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;H2&gt; Century-Later Recognition &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
Her role was hidden until documents concerning it were declassified in 2008. She had taken an oath during her WW II work with the US Navy to keep that secret until her death, which came in 1980 with no fanfare. &lt;/p&gt;
&lt;p&gt;
Still, it took a decade more for true public awareness of her importance. Three recent biographies are: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
Gregg Stuart Smith, &lt;em&gt;A Life in Code: Pioneer Cryptanalyst Elizebeth Smith Friedman&lt;/em&gt;, &lt;a href=&quot;https://www.amazon.com/stores/G.-Stuart-Smith/author/B0077D2M0O?ref=ap_rdr&amp;#038;store_ref=ap_rdr&amp;#038;isDramIntegrated=true&amp;#038;shoppingPortalEnabled=true&quot;&gt;2017&lt;/a&gt;. &lt;/p&gt;
&lt;li&gt;
Jason Fagone, &lt;em&gt;The Woman Who Smashed Codes: A True Story of Love, Spies, and the Unlikely Heroine Who Outwitted America&amp;#8217;s Enemies&lt;/em&gt;, &lt;a href=&quot;https://www.amazon.com/Woman-Who-Smashed-Codes-Outwitted/dp/0062430513/ref=pd_lpo_sccl_2/146-5971751-8425411&quot;&gt;2018&lt;/a&gt;. &lt;/p&gt;
&lt;li&gt;
Amy Butler Greenfield, &lt;em&gt;The Woman All Spies Fear: Code Breaker Elizebeth Smith Friedman and Her Hidden Life&lt;/em&gt;, &lt;a href=&quot;https://www.goodreads.com/en/book/show/56364344-the-woman-all-spies-fear&quot;&gt;2021&lt;/a&gt;.
&lt;/ul&gt;
&lt;p&gt;
Although the last one is written for a young-adult audience, with large print and short chapters, it still has wonderful detail on her life and work. &lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/greenfieldbook/&quot; rel=&quot;attachment wp-att-21811&quot;&gt;&lt;img data-attachment-id=&quot;21811&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/greenfieldbook/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?fit=600%2C894&amp;amp;ssl=1&quot; data-orig-size=&quot;600,894&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;GreenfieldBook&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?fit=201%2C300&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?fit=600%2C894&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?resize=200%2C300&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;200&quot; height=&quot;300&quot; class=&quot;aligncenter wp-image-21811&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?resize=201%2C300&amp;amp;ssl=1 201w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?zoom=2&amp;amp;resize=200%2C300&amp;amp;ssl=1 400w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/GreenfieldBook.jpg?zoom=3&amp;amp;resize=200%2C300&amp;amp;ssl=1 600w&quot; sizes=&quot;(max-width: 200px) 100vw, 200px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
In the meantime, the NSA&amp;#8217;s own tribute, dated &lt;a href=&quot;https://media.defense.gov/2021/Jul/13/2002761955/-1/-1/0/FRIEDMAN-LEGACY-TRANSCRIPT.PDF&quot;&gt;2006&lt;/a&gt;, gives details on Elizebeth but is headlined only for William. Most of its 226 pages reprints six lectures by William that were originally circulated within the agency in 1963. It reprints a 1980 memorial to her at the end.&lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; How She Started &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
Her origin story is amazing as well. She was one of only two in her Midwest farming family of nine to attend college and obtained a degree in English after transferring to a school closer to home. Her first job opportunity, a brief stint as substitute principal at a public high school in Indiana, did not lead to other teaching positions, so she moved back with her family. She journeyed to Chicago to look for work, using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Newberry_Library&quot;&gt;Newberry Library&lt;/a&gt; as a hub.&lt;/p&gt;
&lt;p&gt;
In one of her last days there, a librarian tipped her that a visiting millionaire, George Fabyan, was looking for help on a project involving Shakespeare. She connected with him and the scholarly director of the project, Elizabeth Gallup, and became employed at Fabyan&amp;#8217;s private Riverbank Research Laboratory.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/gf/&quot; rel=&quot;attachment wp-att-21812&quot;&gt;&lt;img data-attachment-id=&quot;21812&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/gf/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gf.jpeg?fit=203%2C249&amp;amp;ssl=1&quot; data-orig-size=&quot;203,249&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;gf&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gf.jpeg?fit=203%2C249&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gf.jpeg?fit=203%2C249&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/gf.jpeg?resize=203%2C249&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;203&quot; height=&quot;249&quot; class=&quot;aligncenter size-full wp-image-21812&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
Riverbank had standard scientific projects as well. William Friedman was employed out of Cornell to work on plant genetics. One of several factors drawing him to the Shakespeare project was that his skill photographing plants was handy for images of original manuscripts kept in England. A second was developing techniques for statistical analysis. A third was Elizebeth. &lt;/p&gt;
&lt;p&gt;
By the time they married in 1917, they had worked out that the statistical randomness of defective type metal re-used by Elizabethan printers and bias in how the alleged codes in Shakespeare were identified effaced the claimed footprints of Francis Bacon&amp;#8217;s &lt;a href=&quot;https://en.wikipedia.org/wiki/Bacon&#39;s_cipher&quot;&gt;two-face cipher&lt;/a&gt;. Before they could even ascertain how to publish their eight draft papers of study, however, America&amp;#8217;s entry into World War I pressed them into other applications. Fabyan himself volunteered the services of his lab for top-secret work. &lt;/p&gt;
&lt;p&gt;
Their work on Shakespeare &lt;a href=&quot;https://www.amazon.com/Shakespearean-Ciphers-Examined-cryptographic-Shakespeare/dp/0521141397&quot;&gt;was published&lt;/a&gt; in 1957. It wasn&amp;#8217;t top secret and it bore both their names, as did its 1955 &lt;a href=&quot;https://books.google.com/books/about/The_Cryptologist_Looks_at_Shakespeare.html?id=BBdiuAAACAAJ&amp;#038;hl=en&amp;#038;output=html_text&quot;&gt;manuscript&lt;/a&gt; which won the Folger Library Shakespeare Prize. &lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; Open Problems &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
In 1999, the year of its creation, she was inducted to the NSA Hall of Honor&amp;#8212;see &lt;a href=&quot;https://www.pbs.org/wgbh/americanexperience/features/codebreaker-elizebeth-friedman-fought-nazi-spies/&quot;&gt;this&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/code-2/&quot; rel=&quot;attachment wp-att-21813&quot;&gt;&lt;img data-attachment-id=&quot;21813&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2023/06/24/a-hidden-heroine/code-2/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/code.jpeg?fit=320%2C208&amp;amp;ssl=1&quot; data-orig-size=&quot;320,208&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;code&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/code.jpeg?fit=300%2C195&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/code.jpeg?fit=320%2C208&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/code.jpeg?resize=320%2C208&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;320&quot; height=&quot;208&quot; class=&quot;aligncenter size-full wp-image-21813&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/code.jpeg?w=320&amp;amp;ssl=1 320w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2023/06/code.jpeg?resize=300%2C195&amp;amp;ssl=1 300w&quot; sizes=&quot;(max-width: 320px) 100vw, 320px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
I hope that in the future credit will be given equally to women as well as to men. This of course presupposes that the women had the opportunity to begin with. Let&amp;#8217;s hope so.&lt;/p&gt;
&lt;p&gt;
&lt;p class=&quot;authors&quot;&gt;By RJLipton+KWRegan&lt;/p&gt;
  </description>
  <pubDate>2023-06-24 21:53:47 UTC</pubDate>
  <author>Richard Lipton</author>
</item>

<item>
  <title>Geometric flip-width revisited</title>
  <guid>https://11011110.github.io/blog/2023/06/24/geometric-flip-width</guid>
  <link>https://11011110.github.io/blog/2023/06/24/geometric-flip-width.html</link>
  <description>
    &lt;p&gt;I recently posted here about the &lt;a href=&quot;/blog/2023/02/20/geometric-graphs-unbounded.html&quot;&gt;flip-width of geometric graphs&lt;/a&gt;, and to readers of that post, my new preprint “Geometric Graphs with Unbounded Flip-Width” (&lt;a href=&quot;http://arxiv.org/abs/2306.12611&quot;&gt;arXiv:2306.12611&lt;/a&gt;, with Rose McCarty, to appear in CCCG 2023) will look very familiar. It even has the same title! However, the process of turning it into a paper led to some improvements. Let me summarize them briefly here.&lt;/p&gt;

&lt;p&gt;First, a reminder of the main concept, flip-width. This is defined using a pursuit–evasion game in which a robber tries to escape cops by following paths in a graph. At each turn, the cops have made a fixed number of “flips” to the graph. Each flip applies to a subset of vertices (possibly overlapping with other flips), removes edges from its adjacent vertices, and adds edges connecting its non-adjacent vertices. A turn consists of three steps: the cops announce what they will flip next, the robber moves along a path of length at most \(s\), and then the cops undo their current flips and perform the new flips that they announced. The goal of the cops is to leave the robber stuck on an isolated vertex, while the goal of the robber is to escape forever. If a class of graphs has a function \(f(s)\) such that \(f(s)\) cops can win against a robber of speed \(s\), then it has bounded flip-width. If there is a speed \(s\) for which arbitrarily many cops may be needed to catch a speed-\(s\) robber, then the class has unbounded flip-width.&lt;/p&gt;

&lt;p&gt;Beyond a more careful attention to detail and rigor, new developments are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Three-dimensional Delaunay triangulations have unbounded flip-width. This uses a construction from another recent blog post on &lt;a href=&quot;/blog/2023/02/25/isohedral-delaunay-complexes.html&quot;&gt;isohedral Delaunay complexes&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Beta_skeleton&quot;&gt;Beta-skeletons&lt;/a&gt; have unbounded flip-width. There are actually two different kinds of beta-skeleton but the interesting case is for parameter values \(\beta&amp;lt;1\), for which the two definitions agree.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another type of geometric graph for which we can prove unbounded flip-width, not mentioned in the previous post, is the rectangle of influence graphs. These connect pairs of points in their plane when their axis-aligned bounding box is empty of other points. We find a recursive construction for rectangle of influence graphs containing hypercube induced subgraphs of arbitrarily large dimension, which in turn have unbounded flip-width. As with the beta-skeletons, definitions for rectangle of influence graphs disagree (about what to do with points on the boundary of the bounding box) but our hypercube construction doesn’t need that ambiguity.&lt;/p&gt;

    &lt;p style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/blog/assets/2023/empty-rectangle.svg&quot; alt=&quot;Hypercube in a rectangle of influence graph&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For most of the families of geometric graphs that we study, the robber can escape by stepping across only one edge per turn (\(s=1\)). This is a big improvement over the \(s=4\) from the blog post, and a much more natural speed limitation. The exception is for three-dimensional Delaunay triangulations; for these \(s=2\) works but we don’t know about \(s=1\). An \(s=2\) escape strategy for all of these graphs is very simple: move to a “lane” (one of two special types of vertex in the “interchange” graphs constructed in the previous blog post) that has two-edge paths to many other lanes. The \(s=1\) strategy is different, and for some of the geometric graphs is based on hypercube subgraphs rather than interchanges.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An appendix extends the results on all of these graph classes (even 3d Delaunay triangulations) from unbounded flip-width to monadic independence, meaning that it is possible to use &lt;em&gt;transductions&lt;/em&gt;, a certain kind of translation system defined using logical formulas, to get arbitrary graphs from graphs in these classes. The main ideas (from Szymon Toruńczyk) are to use Ramsey theory to simplify the interchange subgraphs into two cases, “sparse” and “dense”, to use the structure of these graphs to find a logical translation from the dense case to the sparse case, and to find a subdivision of any given graph as an induced subgraph of a sparse interchange.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/110602049050780927&quot;&gt;Discuss on Mastodon&lt;/a&gt;)&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By David Eppstein&lt;/p&gt;
  </description>
  <pubDate>2023-06-24 18:07:00 UTC</pubDate>
  <author>David Eppstein</author>
</item>

<item>
  <title>Can you put n pennies on an n x n chessboard so that all of the distances are distinct/how to look that up?</title>
  <guid>tag:blogger.com,1999:blog-3722233.post-9038733708392236603</guid>
  <link>https://blog.computationalcomplexity.org/2023/06/can-you-put-n-pennies-on-n-x-n.html</link>
  <description>
    &lt;p&gt;&amp;nbsp;In Jan 2023 I went to the Joint Math Meeting of the AMS and the MAA and took notes on things to look up later. In one of the talks they discussed a problem and indicated that the answer was known, but did not give a reference or a proof. I emailed the authors and got no response. I tried to search the web but could not find it. SO I use this blog post to see if someone either knows the reference or can solve it outright, and either leave the answer in the comments, point to a paper that has the answer in the comments, or email me personally.&amp;nbsp;&lt;/p&gt;&lt;p&gt;--------------------------------------------------------------------&lt;/p&gt;&lt;p&gt;A chessboard has squares that are 1 by 1.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Pennies have diameter 1.&lt;/p&gt;&lt;p&gt;QUESTION:&amp;nbsp;&lt;/p&gt;&lt;p&gt;For which n is there a way to place n pennies on squares of the n x n chessboard so that all of the distances between centers of the pennies are DIFFERENT?&lt;/p&gt;&lt;p&gt;-----------------------------------------------------------&lt;/p&gt;&lt;p&gt;I have figured out that you CAN do this for n=3,4,5. I THINK the talk said&amp;nbsp; it cannot be done for n=6. If&amp;nbsp; you know or find a proof or disproof then please tell me. I am looking for human-readable proofs, not computer proofs.&amp;nbsp; Similar for higher n.&lt;/p&gt;&lt;p&gt;I have a writeup of the n=3,4,5 cases&amp;nbsp;&lt;a href=&quot;https://www.cs.umd.edu/~gasarch/BLOGPAPERS/pennychess.pdf&quot;&gt;here&lt;/a&gt;&amp;nbsp;(ADDED LATER- I will edit this later in light of the very interesting comments made on this blog entry.)&amp;nbsp;&lt;/p&gt;&lt;p&gt;----------------------------------------------------------------------&lt;/p&gt;&lt;p&gt;With technology and search engines it SHOULD be easier to find out answers to questions then it was in a prior era. And I think it is. But there are times when you are still better off asking&amp;nbsp; someone, or in my case blog about it, to find the answer. Here is hoping it works!&lt;/p&gt;&lt;p&gt;ADDED LATER: Within 30 minutes of posting this one of my readers wrote a program and found tha tyou CAN do it for n=6 and gives the answer. Another commenter pointed to a website with the related quetion of putting as many pawns as you can on an 8x8 board.&lt;/p&gt;&lt;p&gt;ADDED LATER: There are now comments on the blog pointing to the FULL SOLUTION to the problem, which one can find&amp;nbsp;&lt;a href=&quot;https://oscarcunningham.com/670/unique-distancing-problem/&quot;&gt;here&lt;/a&gt;. In summary:&amp;nbsp;&lt;/p&gt;&lt;p&gt;for n=3,...,7&amp;nbsp; there IS a way to put n pennies on a chessboard such that all distances are distinct.&lt;/p&gt;&lt;p&gt;for n=8,...,14 a computer search shows that there is no such way.&lt;/p&gt;&lt;p&gt;for n=15 there is an INTERESTING PROOF that there is no such way (good thing - the computer program had not halted yet. I do not know if it every did.)&amp;nbsp;&lt;/p&gt;&lt;p&gt;for n\ge 16 there is a NICE proof that there IS such way.&amp;nbsp;&lt;/p&gt;&lt;p&gt;I am ECSTATIC!- I wanted to know the answer and now I do and its easy to understand!&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By gasarch&lt;/p&gt;
  </description>
  <pubDate>2023-06-24 14:35:00 UTC</pubDate>
  <author>Computational Complexity</author>
</item>

<item>
  <title>Sleptsov Nets are Turing-complete</title>
  <guid>http://arxiv.org/abs/2306.12440</guid>
  <link>http://arxiv.org/abs/2306.12440</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berthomieu_B/0/1/0/all/0/1&quot;&gt;Bernard Berthomieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaitsev_D/0/1/0/all/0/1&quot;&gt;Dmitry A. Zaitsev&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The present paper proves that a Sleptsov net (SN) is Turing-complete, that
considerably improves, with a brief construct, the previous result that a
strong SN is Turing-complete. Remind that, unlike Petri nets, an SN always
fires enabled transitions at their maximal firing multiplicity, as a single
step, leaving for a nondeterministic choice of which fireable transitions to
fire. A strong SN restricts nondeterministic choice to firing only the
transitions having the highest firing multiplicity.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-23 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Unitary Complexity and the Uhlmann Transformation Problem</title>
  <guid>http://arxiv.org/abs/2306.13073</guid>
  <link>http://arxiv.org/abs/2306.13073</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Bostanci_J/0/1/0/all/0/1&quot;&gt;John Bostanci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Efron_Y/0/1/0/all/0/1&quot;&gt;Yuval Efron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Metger_T/0/1/0/all/0/1&quot;&gt;Tony Metger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Poremba_A/0/1/0/all/0/1&quot;&gt;Alexander Poremba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Qian_L/0/1/0/all/0/1&quot;&gt;Luowen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Yuen_H/0/1/0/all/0/1&quot;&gt;Henry Yuen&lt;/a&gt;&lt;/p&gt;&lt;p&gt;State transformation problems such as compressing quantum information or
breaking quantum commitments are fundamental quantum tasks. However, their
computational difficulty cannot easily be characterized using traditional
complexity theory, which focuses on tasks with classical inputs and outputs.
&lt;/p&gt;
&lt;p&gt;To study the complexity of such state transformation tasks, we introduce a
framework for unitary synthesis problems, including notions of reductions and
unitary complexity classes. We use this framework to study the complexity of
transforming one entangled state into another via local operations. We
formalize this as the Uhlmann Transformation Problem, an algorithmic version of
Uhlmann&#39;s theorem. Then, we prove structural results relating the complexity of
the Uhlmann Transformation Problem, polynomial space quantum computation, and
zero knowledge protocols.
&lt;/p&gt;
&lt;p&gt;The Uhlmann Transformation Problem allows us to characterize the complexity
of a variety of tasks in quantum information processing, including decoding
noisy quantum channels, breaking falsifiable quantum cryptographic assumptions,
implementing optimal prover strategies in quantum interactive proofs, and
decoding the Hawking radiation of black holes. Our framework for unitary
complexity thus provides new avenues for studying the computational complexity
of many natural quantum information processing tasks.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-23 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Geometric Graphs with Unbounded Flip-Width</title>
  <guid>http://arxiv.org/abs/2306.12611</guid>
  <link>http://arxiv.org/abs/2306.12611</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eppstein_D/0/1/0/all/0/1&quot;&gt;David Eppstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCarty_R/0/1/0/all/0/1&quot;&gt;Rose McCarty&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We consider the flip-width of geometric graphs, a notion of graph width
recently introduced by Toru\&#39;nczyk. We prove that many different types of
geometric graphs have unbounded flip-width. These include interval graphs,
permutation graphs, circle graphs, intersection graphs of axis-aligned line
segments or axis-aligned unit squares, unit distance graphs, unit disk graphs,
visibility graphs of simple polygons, $\beta$-skeletons, 4-polytopes, rectangle
of influence graphs, and 3d Delaunay triangulations.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-23 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Polynomial Logical Zonotopes: A Set Representation for Reachability Analysis of Logical Systems</title>
  <guid>http://arxiv.org/abs/2306.12508</guid>
  <link>http://arxiv.org/abs/2306.12508</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alanwar_A/0/1/0/all/0/1&quot;&gt;Amr Alanwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1&quot;&gt;Frank J. Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johansson_K/0/1/0/all/0/1&quot;&gt;Karl H. Johansson&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this paper, we introduce a set representation called polynomial logical
zonotopes for performing exact and computationally efficient reachability
analysis on logical systems. Polynomial logical zonotopes are a generalization
of logical zonotopes, which are able to represent up to 2^n binary vectors
using only n generators. Due to their construction, logical zonotopes are only
able to support exact computations of some logical operations (XOR, NOT, XNOR),
while other operations (AND, NAND, OR, NOR) result in over-approximations. In
order to perform all fundamental logical operations exactly, we formulate a
generalization of logical zonotopes that is constructed by additional dependent
generators and exponent matrices. We prove that through this polynomial-like
construction, we are able to perform all of the fundamental logical operations
(XOR, NOT, XNOR, AND, NAND, OR, NOR) exactly. While we are able to perform all
of the logical operations exactly, this comes with a slight increase in
computational complexity compared to logical zonotopes. We show that we can use
polynomial logical zonotopes to perform exact reachability analysis while
retaining a low computational complexity. To illustrate and showcase the
computational benefits of polynomial logical zonotopes, we present the results
of performing reachability analysis on two use cases: (1) safety verification
of an intersection crossing protocol, (2) and reachability analysis on a
high-dimensional Boolean function. Moreover, to highlight the extensibility of
logical zonotopes, we include an additional use case where we perform a
computationally tractable exhaustive search for the key of a linear-feedback
shift register.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-23 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Preprocessing Complexity for Some Graph Problems Parameterized by Structural Parameters</title>
  <guid>http://arxiv.org/abs/2306.12655</guid>
  <link>http://arxiv.org/abs/2306.12655</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lafond_M/0/1/0/all/0/1&quot;&gt;Manuel Lafond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1&quot;&gt;Weidong Luo&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Structural graph parameters play an important role in parameterized
complexity, including in kernelization. Notably, vertex cover, neighborhood
diversity, twin-cover, and modular-width have been studied extensively in the
last few years. However, there are many fundamental problems whose
preprocessing complexity is not fully understood under these parameters.
Indeed, the existence of polynomial kernels or polynomial Turing kernels for
famous problems such as Clique, Chromatic Number, and Steiner Tree has only
been established for a subset of structural parameters. In this work, we use
several techniques to obtain a complete preprocessing complexity landscape for
over a dozen of fundamental algorithmic problems.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-23 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Memory-Query Tradeoffs for Randomized Convex Optimization</title>
  <guid>http://arxiv.org/abs/2306.12534</guid>
  <link>http://arxiv.org/abs/2306.12534</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1&quot;&gt;Binghui Peng&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We show that any randomized first-order algorithm which minimizes a
$d$-dimensional, $1$-Lipschitz convex function over the unit ball must either
use $\Omega(d^{2-\delta})$ bits of memory or make $\Omega(d^{1+\delta/6-o(1)})$
queries, for any constant $\delta\in (0,1)$ and when the precision $\epsilon$
is quasipolynomially small in $d$. Our result implies that cutting plane
methods, which use $\tilde{O}(d^2)$ bits of memory and $\tilde{O}(d)$ queries,
are Pareto-optimal among randomized first-order algorithms, and quadratic
memory is required to achieve optimal query complexity for convex optimization.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-23 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>On Differentially Private Sampling from Gaussian and Product Distributions</title>
  <guid>http://arxiv.org/abs/2306.12549</guid>
  <link>http://arxiv.org/abs/2306.12549</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghazi_B/0/1/0/all/0/1&quot;&gt;Badih Ghazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1&quot;&gt;Ravi Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manurangsi_P/0/1/0/all/0/1&quot;&gt;Pasin Manurangsi&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Given a dataset of $n$ i.i.d. samples from an unknown distribution $P$, we
consider the problem of generating a sample from a distribution that is close
to $P$ in total variation distance, under the constraint of differential
privacy (DP). We study the problem when $P$ is a multi-dimensional Gaussian
distribution, under different assumptions on the information available to the
DP mechanism: known covariance, unknown bounded covariance, and unknown
unbounded covariance. We present new DP sampling algorithms, and show that they
achieve near-optimal sample complexity in the first two settings. Moreover,
when $P$ is a product distribution on the binary hypercube, we obtain a pure-DP
algorithm whereas only an approximate-DP algorithm (with slightly worse sample
complexity) was previously known.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-23 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>The Power of Menus in Contract Design</title>
  <guid>http://arxiv.org/abs/2306.12667</guid>
  <link>http://arxiv.org/abs/2306.12667</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guruganesh_G/0/1/0/all/0/1&quot;&gt;Guru Guruganesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jon Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Joshua Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Junyao Zhao&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the power of menus of contracts in principal-agent problems with
adverse selection (agents can be one of several types) and moral hazard (we
cannot observe agent actions directly). For principal-agent problems with $T$
types and $n$ actions, we show that the best menu of contracts can obtain a
factor $\Omega(\max(n, \log T))$ more utility for the principal than the best
individual contract, partially resolving an open question of Guruganesh et al.
(2021). We then turn our attention to randomized menus of linear contracts,
where we likewise show that randomized linear menus can be $\Omega(T)$ better
than the best single linear contract. As a corollary, we show this implies an
analogous gap between deterministic menus of (general) contracts and randomized
menus of contracts (as introduced by Castiglioni et al. (2022)).
&lt;/p&gt;
  </description>
  <pubDate>2023-06-23 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Counting occurrences of patterns in permutations</title>
  <guid>http://arxiv.org/abs/2306.12682</guid>
  <link>http://arxiv.org/abs/2306.12682</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Conway_A/0/1/0/all/0/1&quot;&gt;Andrew R Conway&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Guttmann_A/0/1/0/all/0/1&quot;&gt;Anthony J Guttmann&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We develop a new, powerful method for counting elements in a {\em multiset.}
As a first application, we use this algorithm to study the number of
occurrences of patterns in a permutation. For patterns of length 3 there are
two Wilf classes, and the general behaviour of these is reasonably well-known.
We slightly extend some of the known results in that case, and exhaustively
study the case of patterns of length 4, about which there is little previous
knowledge. For such patterns, there are seven Wilf classes, and based on
extensive enumerations and careful series analysis, we have conjectured the
asymptotic behaviour for all classes.
&lt;/p&gt;
&lt;p&gt;Finally, we investigate a proposal of Blitvi\&#39;c and Steingr\&#39;imsson as to the
range of a parameter for which a particular generating function formed from the
occurrence sequences is itself a Stieltjes moment sequence.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-23 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Faster Compression of Deterministic Finite Automata</title>
  <guid>http://arxiv.org/abs/2306.12771</guid>
  <link>http://arxiv.org/abs/2306.12771</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bille_P/0/1/0/all/0/1&quot;&gt;Philip Bille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gortz_I/0/1/0/all/0/1&quot;&gt;Inge Li G&amp;#xf8;rtz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedersen_M/0/1/0/all/0/1&quot;&gt;Max Rish&amp;#xf8;j Pedersen&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Deterministic finite automata (DFA) are a classic tool for high throughput
matching of regular expressions, both in theory and practice.
&lt;/p&gt;
&lt;p&gt;Due to their high space consumption, extensive research has been devoted to
compressed representations of DFAs that still support efficient pattern
matching queries.
&lt;/p&gt;
&lt;p&gt;Kumar~et~al.~[SIGCOMM 2006] introduced the \emph{delayed deterministic finite
automaton} (\ddfa{}) which exploits the large redundancy between inter-state
transitions in the automaton.
&lt;/p&gt;
&lt;p&gt;They showed it to obtain up to two orders of magnitude compression of
real-world DFAs, and their work formed the basis of numerous subsequent
results.
&lt;/p&gt;
&lt;p&gt;Their algorithm, as well as later algorithms based on their idea, have an
inherent quadratic-time bottleneck, as they consider every pair of states to
compute the optimal compression.
&lt;/p&gt;
&lt;p&gt;In this work we present a simple, general framework based on
locality-sensitive hashing for speeding up these algorithms to achieve
sub-quadratic construction times for \ddfa{}s.
&lt;/p&gt;
&lt;p&gt;We apply the framework to speed up several algorithms to near-linear time,
and experimentally evaluate their performance on real-world regular expression
sets extracted from modern intrusion detection systems.
&lt;/p&gt;
&lt;p&gt;We find an order of magnitude improvement in compression times, with either
little or no loss of compression, or even significantly better compression in
some cases.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-23 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>On boundedness of zeros of the independence polynomial of tor</title>
  <guid>http://arxiv.org/abs/2306.12934</guid>
  <link>http://arxiv.org/abs/2306.12934</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Boer_D/0/1/0/all/0/1&quot;&gt;David de Boer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Buys_P/0/1/0/all/0/1&quot;&gt;Pjotr Buys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Peters_H/0/1/0/all/0/1&quot;&gt;Han Peters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Regts_G/0/1/0/all/0/1&quot;&gt;Guus Regts&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study boundedness of zeros of the independence polynomial of tori for
sequences of tori converging to the integer lattice. We prove that zeros are
bounded for sequences of balanced tori, but unbounded for sequences of highly
unbalanced tori. Here balanced means that the size of the torus is at most
exponential in the shortest side length, while highly unbalanced means that the
longest side length of the torus is super exponential in the product over the
other side lengths cubed. We discuss implications of our results to the
existence of efficient algorithms for approximating the independence polynomial
on tori.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-23 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>SQ Lower Bounds for Learning Bounded Covariance GMMs</title>
  <guid>http://arxiv.org/abs/2306.13057</guid>
  <link>http://arxiv.org/abs/2306.13057</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1&quot;&gt;Ilias Diakonikolas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1&quot;&gt;Daniel M. Kane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pittas_T/0/1/0/all/0/1&quot;&gt;Thanasis Pittas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zarifis_N/0/1/0/all/0/1&quot;&gt;Nikos Zarifis&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the complexity of learning mixtures of separated Gaussians with
common unknown bounded covariance matrix. Specifically, we focus on learning
Gaussian mixture models (GMMs) on $\mathbb{R}^d$ of the form $P= \sum_{i=1}^k
w_i \mathcal{N}(\boldsymbol \mu_i,\mathbf \Sigma_i)$, where $\mathbf \Sigma_i =
\mathbf \Sigma \preceq \mathbf I$ and $\min_{i \neq j} \| \boldsymbol \mu_i -
\boldsymbol \mu_j\|_2 \geq k^\epsilon$ for some $\epsilon&amp;gt;0$. Known learning
algorithms for this family of GMMs have complexity $(dk)^{O(1/\epsilon)}$. In
this work, we prove that any Statistical Query (SQ) algorithm for this problem
requires complexity at least $d^{\Omega(1/\epsilon)}$. In the special case
where the separation is on the order of $k^{1/2}$, we additionally obtain
fine-grained SQ lower bounds with the correct exponent. Our SQ lower bounds
imply similar lower bounds for low-degree polynomial tests. Conceptually, our
results provide evidence that known algorithms for this problem are nearly best
possible.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-23 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Don&#39;t Negotiate with Logic</title>
  <guid>tag:blogger.com,1999:blog-3722233.post-1971310189381809136</guid>
  <link>https://blog.computationalcomplexity.org/2023/06/dont-negotiate-with-logic.html</link>
  <description>
    &lt;p&gt;Computer science and mathematicians often try to use logic to negotiate whether it be at a university or life in general. I&#39;ve tried it myself and it doesn&#39;t usually work. Even if you have that (rare) perfect argument, remember &lt;a href=&quot;https://www.goodreads.com/quotes/21810-it-is-difficult-to-get-a-man-to-understand-something&quot;&gt;Upton Sinclair&#39;s words&lt;/a&gt;:&amp;nbsp;&lt;i&gt;It is difficult to get a man to understand something, when his salary depends on his not understanding it&lt;/i&gt;.&lt;/p&gt;&lt;p&gt;So make sure their salary depends on them understanding it. Or more to the point, in a world with limited resources, why it makes sense for them to help you.&amp;nbsp;&lt;/p&gt;&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Ideally go for the win-win. Why a certain decision helps the department/college/university as well as yourself. Asking for a small investment as a seed towards a large grant for example.&lt;/li&gt;&lt;li&gt;How would the decision make you or your students more successful? The success of a department is measured by the success of the faculty and students. On the other hand, why would a different decision hold you and your students back.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;Even outside the university, make your objectives in line with the objectives of the person you are negotiating with to lead to a better outcome.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Of course sometimes you are haggling over a price or a salary when it really is a zero-sum game. There it&#39;s good to know the &lt;a href=&quot;https://www.investopedia.com/terms/b/best-alternative-to-a-negotiated-agreement-batna.asp&quot;&gt;BATNA&lt;/a&gt;, Best Alternative To a Negotiated Agreement, for yourself and the other entity. In other words, if they aren&#39;t selling to you what other options do you and they have?&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;There are whole books written about negotiating strategies. Mostly it comes down to making it work for both parties. That&#39;s what matters, not the logic.&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By Lance Fortnow&lt;/p&gt;
  </description>
  <pubDate>2023-06-22 17:15:00 UTC</pubDate>
  <author>Computational Complexity</author>
</item>

<item>
  <title>Quantum and classical query complexities for determining connectedness of matroids</title>
  <guid>http://arxiv.org/abs/2306.12103</guid>
  <link>http://arxiv.org/abs/2306.12103</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shiguang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lvzhou Li&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Connectivity is a fundamental structural property of matroids, and has been
studied algorithmically over 50 years. In 1974, Cunningham proposed a
deterministic algorithm consuming $O(n^{2})$ queries to the independence oracle
to determine whether a matroid is connected. Since then, no algorithm, not even
a random one, has worked better. To the best of our knowledge, the classical
query complexity lower bound and the quantum complexity for this problem have
not been considered. Thus, in this paper we are devoted to addressing these
issues, and our contributions are threefold as follows: (i) First, we prove
that the randomized query complexity of determining whether a matroid is
connected is $\Omega(n^2)$ and thus the algorithm proposed by Cunningham is
optimal in classical computing. (ii) Second, we present a quantum algorithm
with $O(n^{3/2})$ queries, which exhibits provable quantum speedups over
classical ones. (iii) Third, we prove that any quantum algorithm requires
$\Omega(n)$ queries, which indicates that quantum algorithms can achieve at
most a quadratic speedup over classical ones. Therefore, we have a relatively
comprehensive understanding of the potential of quantum computing in
determining the connectedness of matroids.\
&lt;/p&gt;
  </description>
  <pubDate>2023-06-22 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Geometric Algorithms for $k$-NN Poisoning</title>
  <guid>http://arxiv.org/abs/2306.12377</guid>
  <link>http://arxiv.org/abs/2306.12377</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Centurion_D/0/1/0/all/0/1&quot;&gt;Diego Ihara Centurion&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chubarian_K/0/1/0/all/0/1&quot;&gt;Karine Chubarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1&quot;&gt;Bohan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sgherzi_F/0/1/0/all/0/1&quot;&gt;Francesco Sgherzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radhakrishnan_T/0/1/0/all/0/1&quot;&gt;Thiruvenkadam S Radhakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sidiropoulos_A/0/1/0/all/0/1&quot;&gt;Anastasios Sidiropoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Straight_A/0/1/0/all/0/1&quot;&gt;Angelo Straight&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We propose a label poisoning attack on geometric data sets against
$k$-nearest neighbor classification. We provide an algorithm that can compute
an $\varepsilon n$-additive approximation of the optimal poisoning in $n\cdot
2^{2^{O(d+k/\varepsilon)}}$ time for a given data set $X \in \mathbb{R}^d$,
where $|X| = n$. Our algorithm achieves its objectives through the application
of multi-scale random partitions.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-22 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Fast quantum algorithm for differential equations</title>
  <guid>http://arxiv.org/abs/2306.11802</guid>
  <link>http://arxiv.org/abs/2306.11802</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Bagherimehrab_M/0/1/0/all/0/1&quot;&gt;Mohsen Bagherimehrab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Nakaji_K/0/1/0/all/0/1&quot;&gt;Kouhei Nakaji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wiebe_N/0/1/0/all/0/1&quot;&gt;Nathan Wiebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Aspuru_Guzik_A/0/1/0/all/0/1&quot;&gt;Al&amp;#xe1;n Aspuru-Guzik&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Partial differential equations (PDEs) are ubiquitous in science and
engineering. Prior quantum algorithms for solving the system of linear
algebraic equations obtained from discretizing a PDE have a computational
complexity that scales at least linearly with the condition number $\kappa$ of
the matrices involved in the computation. For many practical applications,
$\kappa$ scales polynomially with the size $N$ of the matrices, rendering a
polynomial-in-$N$ complexity for these algorithms. Here we present a quantum
algorithm with a complexity that is polylogarithmic in $N$ but is independent
of $\kappa$ for a large class of PDEs. Our algorithm generates a quantum state
that enables extracting features of the solution. Central to our methodology is
using a wavelet basis as an auxiliary system of coordinates in which the
condition number of associated matrices is independent of $N$ by a simple
diagonal preconditioner. We present numerical simulations showing the effect of
the wavelet preconditioner for several differential equations. Our work could
provide a practical way to boost the performance of quantum-simulation
algorithms where standard methods are used for discretization.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-22 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>A Parameterized Algorithm for Flat Folding</title>
  <guid>http://arxiv.org/abs/2306.11939</guid>
  <link>http://arxiv.org/abs/2306.11939</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eppstein_D/0/1/0/all/0/1&quot;&gt;David Eppstein&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We prove that testing the flat foldability of an origami crease pattern
(either labeled with mountain and valley folds, or unlabeled) is
fixed-parameter tractable when parameterized by the ply of the flat-folded
state and by the treewidth of an associated planar graph, the cell adjacency
graph of an arrangement of polygons formed by the flat-folded state. For flat
foldings of bounded ply, our algorithm is single-exponential in the treewidth;
this dependence on treewidth is necessary under the exponential time
hypothesis.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-22 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Near-Optimal Dynamic Rounding of Fractional Matchings in Bipartite Graphs</title>
  <guid>http://arxiv.org/abs/2306.11828</guid>
  <link>http://arxiv.org/abs/2306.11828</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1&quot;&gt;Sayan Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiss_P/0/1/0/all/0/1&quot;&gt;Peter Kiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sidford_A/0/1/0/all/0/1&quot;&gt;Aaron Sidford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wajc_D/0/1/0/all/0/1&quot;&gt;David Wajc&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study dynamic $(1-\epsilon)$-approximate rounding of fractional matchings
-- a key ingredient in numerous breakthroughs in the dynamic graph algorithms
literature. Our first contribution is a surprisingly simple deterministic
rounding algorithm in bipartite graphs with amortized update time
$O(\epsilon^{-1} \log^2 (\epsilon^{-1} \cdot n))$, matching an (unconditional)
recourse lower bound of $\Omega(\epsilon^{-1})$ up to logarithmic factors.
Moreover, this algorithm&#39;s update time improves provided the minimum (non-zero)
weight in the fractional matching is lower bounded throughout. Combining this
algorithm with novel dynamic \emph{partial rounding} algorithms to increase
this minimum weight, we obtain several algorithms that improve this dependence
on $n$. For example, we give a high-probability randomized algorithm with
$\tilde{O}(\epsilon^{-1}\cdot (\log\log n)^2)$-update time against adaptive
adversaries. (We use Soft-Oh notation, $\tilde{O}$, to suppress polylogarithmic
factors in the argument, i.e., $\tilde{O}(f)=O(f\cdot \mathrm{poly}(\log f))$.)
Using our rounding algorithms, we also round known $(1-\epsilon)$-decremental
fractional bipartite matching algorithms with no asymptotic overhead, thus
improving on state-of-the-art algorithms for the decremental bipartite matching
problem. Further, we provide extensions of our results to general graphs and to
maintaining almost-maximal matchings.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-22 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>On the Optimal Bounds for Noisy Computing</title>
  <guid>http://arxiv.org/abs/2306.11951</guid>
  <link>http://arxiv.org/abs/2306.11951</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Banghua Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghaddar_N/0/1/0/all/0/1&quot;&gt;Nadim Ghaddar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jiantao Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lele Wang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We revisit the problem of computing with noisy information considered in
Feige et al. 1994, which includes computing the OR function from noisy queries,
and computing the MAX, SEARCH and SORT functions from noisy pairwise
comparisons. For $K$ given elements, the goal is to correctly recover the
desired function with probability at least $1-\delta$ when the outcome of each
query is flipped with probability $p$. We consider both the adaptive sampling
setting where each query can be adaptively designed based on past outcomes, and
the non-adaptive sampling setting where the query cannot depend on past
outcomes. The prior work provides tight bounds on the worst-case query
complexity in terms of the dependence on $K$. However, the upper and lower
bounds do not match in terms of the dependence on $\delta$ and $p$. We improve
the lower bounds for all the four functions under both adaptive and
non-adaptive query models. Most of our lower bounds match the upper bounds up
to constant factors when either $p$ or $\delta$ is bounded away from $0$, while
the ratio between the best prior upper and lower bounds goes to infinity when
$p\rightarrow 0$ or $p\rightarrow 1/2$. On the other hand, we also provide
matching upper and lower bounds for the number of queries in expectation,
improving both the upper and lower bounds for the variable-length query model.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-22 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Sampling Individually-Fair Rankings that are Always Group Fair</title>
  <guid>http://arxiv.org/abs/2306.11964</guid>
  <link>http://arxiv.org/abs/2306.11964</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorantla_S/0/1/0/all/0/1&quot;&gt;Sruthi Gorantla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehrotra_A/0/1/0/all/0/1&quot;&gt;Anay Mehrotra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1&quot;&gt;Amit Deshpande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Louis_A/0/1/0/all/0/1&quot;&gt;Anand Louis&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Rankings on online platforms help their end-users find the relevant
information -- people, news, media, and products -- quickly. Fair ranking
tasks, which ask to rank a set of items to maximize utility subject to
satisfying group-fairness constraints, have gained significant interest in the
Algorithmic Fairness, Information Retrieval, and Machine Learning literature.
Recent works, however, identify uncertainty in the utilities of items as a
primary cause of unfairness and propose introducing randomness in the output.
This randomness is carefully chosen to guarantee an adequate representation of
each item (while accounting for the uncertainty). However, due to this
randomness, the output rankings may violate group fairness constraints. We give
an efficient algorithm that samples rankings from an individually-fair
distribution while ensuring that every output ranking is group fair. The
expected utility of the output ranking is at least $\alpha$ times the utility
of the optimal fair solution. Here, $\alpha$ depends on the utilities,
position-discounts, and constraints -- it approaches 1 as the range of
utilities or the position-discounts shrinks, or when utilities satisfy
distributional assumptions. Empirically, we observe that our algorithm achieves
individual and group fairness and that Pareto dominates the state-of-the-art
baselines.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-22 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Block-Wise Index Modulation and Receiver Design for High-Mobility OTFS Communications</title>
  <guid>http://arxiv.org/abs/2306.12042</guid>
  <link>http://arxiv.org/abs/2306.12042</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_M/0/1/0/all/0/1&quot;&gt;Mi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_F/0/1/0/all/0/1&quot;&gt;Fei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1&quot;&gt;Miaowen Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xiang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1&quot;&gt;H. Vincent Poor&lt;/a&gt;&lt;/p&gt;&lt;p&gt;As a promising technique for high-mobility wireless communications,
orthogonal time frequency space (OTFS) has been proved to enjoy excellent
advantages with respect to traditional orthogonal frequency division
multiplexing (OFDM). Although multiple studies have considered index modulation
(IM) based OTFS (IM-OTFS) schemes to further improve system performance, a
challenging and open problem is the development of effective IM schemes and
efficient receivers for practical OTFS systems that must operate in the
presence of channel delays and Doppler shifts. In this paper, we propose two
novel block-wise IM schemes for OTFS systems, named delay-IM with OTFS
(DeIM-OTFS) and Doppler-IM with OTFS (DoIM-OTFS), where a block of
delay/Doppler resource bins are activated simultaneously. Based on a maximum
likelihood (ML) detector, we analyze upper bounds on the average bit error
rates for the proposed DeIM-OTFS and DoIM-OTFS schemes, and verify their
performance advantages over the existing IM-OTFS systems. We also develop a
multi-layer joint symbol and activation pattern detection (MLJSAPD) algorithm
and a customized message passing detection (CMPD) algorithm for our proposed
DeIMOTFS and DoIM-OTFS systems with low complexity. Simulation results
demonstrate that our proposed MLJSAPD and CMPD algorithms can achieve desired
performance with robustness to the imperfect channel state information (CSI).
&lt;/p&gt;
  </description>
  <pubDate>2023-06-22 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Optimal (degree+1)-Coloring in Congested Clique</title>
  <guid>http://arxiv.org/abs/2306.12071</guid>
  <link>http://arxiv.org/abs/2306.12071</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coy_S/0/1/0/all/0/1&quot;&gt;Sam Coy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Czumaj_A/0/1/0/all/0/1&quot;&gt;Artur Czumaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davies_P/0/1/0/all/0/1&quot;&gt;Peter Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1&quot;&gt;Gopinath Mishra&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We consider the distributed complexity of the (degree+1)-list coloring
problem, in which each node $u$ of degree $d(u)$ is assigned a palette of
$d(u)+1$ colors, and the goal is to find a proper coloring using these color
palettes. The (degree+1)-list coloring problem is a natural generalization of
the classical $(\Delta+1)$-coloring and $(\Delta+1)$-list coloring problems,
both being benchmark problems extensively studied in distributed and parallel
computing. In this paper we settle the complexity of the (degree+1)-list
coloring problem in the Congested Clique model by showing that it can be solved
deterministically in a constant number of rounds.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-22 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Online Resource Allocation with Convex-set Machine-Learned Advice</title>
  <guid>http://arxiv.org/abs/2306.12282</guid>
  <link>http://arxiv.org/abs/2306.12282</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golrezaei_N/0/1/0/all/0/1&quot;&gt;Negin Golrezaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaillet_P/0/1/0/all/0/1&quot;&gt;Patrick Jaillet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zijie Zhou&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Decision-makers often have access to a machine-learned prediction about
demand, referred to as advice, which can potentially be utilized in online
decision-making processes for resource allocation. However, exploiting such
advice poses challenges due to its potential inaccuracy. To address this issue,
we propose a framework that enhances online resource allocation decisions with
potentially unreliable machine-learned (ML) advice. We assume here that this
advice is represented by a general convex uncertainty set for the demand
vector.
&lt;/p&gt;
&lt;p&gt;We introduce a parameterized class of Pareto optimal online resource
allocation algorithms that strike a balance between consistent and robust
ratios. The consistent ratio measures the algorithm&#39;s performance (compared to
the optimal hindsight solution) when the ML advice is accurate, while the
robust ratio captures performance under an adversarial demand process when the
advice is inaccurate. Specifically, in a C-Pareto optimal setting, we maximize
the robust ratio while ensuring that the consistent ratio is at least C. Our
proposed C-Pareto optimal algorithm is an adaptive protection level algorithm,
which extends the classical fixed protection level algorithm introduced in
Littlewood (2005) and Ball and Queyranne (2009). Solving a complex non-convex
continuous optimization problem characterizes the adaptive protection level
algorithm. To complement our algorithms, we present a simple method for
computing the maximum achievable consistent ratio, which serves as an estimate
for the maximum value of the ML advice. Additionally, we present numerical
studies to evaluate the performance of our algorithm in comparison to benchmark
algorithms. The results demonstrate that by adjusting the parameter C, our
algorithms effectively strike a balance between worst-case and average
performance, outperforming the benchmark algorithms.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-22 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>An efficient, provably exact algorithm for the 0-1 loss linear classification problem</title>
  <guid>http://arxiv.org/abs/2306.12344</guid>
  <link>http://arxiv.org/abs/2306.12344</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Little_M/0/1/0/all/0/1&quot;&gt;Max A. Little&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Algorithms for solving the linear classification problem have a long history,
dating back at least to 1936 with linear discriminant analysis. For linearly
separable data, many algorithms can obtain the exact solution to the
corresponding 0-1 loss classification problem efficiently, but for data which
is not linearly separable, it has been shown that this problem, in full
generality, is NP-hard. Alternative approaches all involve approximations of
some kind, including the use of surrogates for the 0-1 loss (for example, the
hinge or logistic loss) or approximate combinatorial search, none of which can
be guaranteed to solve the problem exactly. Finding efficient algorithms to
obtain an exact i.e. globally optimal solution for the 0-1 loss linear
classification problem with fixed dimension, remains an open problem. In
research we report here, we detail the construction of a new algorithm,
incremental cell enumeration (ICE), that can solve the 0-1 loss classification
problem exactly in polynomial time. To our knowledge, this is the first,
rigorously-proven polynomial time algorithm for this long-standing problem.
&lt;/p&gt;
  </description>
  <pubDate>2023-06-22 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Flat folding and map folding</title>
  <guid>https://11011110.github.io/blog/2023/06/21/flat-folding-map</guid>
  <link>https://11011110.github.io/blog/2023/06/21/flat-folding-map.html</link>
  <description>
    &lt;p&gt;Another day, another new arXiv preprint. Today’s is “A parameterized algorithm for flat folding” (&lt;a href=&quot;https://arxiv.org/abs/2306.11939&quot;&gt;arXiv:2306.11939&lt;/a&gt;, to appear at CCCG 2023), a paper I mentioned in the last slide of my recent talk on &lt;a href=&quot;https://www.ics.uci.edu/~eppstein/pubs/Epp-WPAGP-23.pdf&quot;&gt;graph width parameters for parameterized geometric algorithms&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It’s been known since the work of Marshall Bern and Barry Hayes in SODA 1996 that it’s NP-complete to test whether a given origami pattern (with or without an assignment of a mountain or valley fold to each crease) can be folded flat. Of course, many origami structures are not intended to be flat, but they still often start with a flat base, and testing flatness is a prerequisite for any more complicated foldability question you might have. On the other hand, if a pattern does fold flat, it’s very easy to determine where in plane each piece of paper folds to; the hard part is determining the above-below relation between different pieces of paper that fold to the same points.&lt;/p&gt;

&lt;p&gt;The new paper defines a graph from any folding pattern, whose vertices represent connected regions of the plane that should be covered by the same pieces of paper in the folded state and whose edges represent adjacencies between those regions. An example is shown below.&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/blog/assets/2023/folding-graph.svg&quot; alt=&quot;A folding pattern and its graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The left side is the folding pattern (blue for valley folds and red for mountain folds): first, fold the right edge of the paper on top of the rest, along the vertical blue crease, and then second, fold the top right corner down, along the two diagonal creases. The right side is how it should map to a folded state (without an assignment of an above-below relation to the layers in that state) and the resulting graph. The goal of the algorithm is to figure out that the regions of the folding pattern should be layered (bottom to top) in the order: big pentagon, lower right trapezoid, upper right trapezoid, triangle. It determines whether a folded state like that exists, but not the sequence of moves you would need to make to get there. But some care is needed in defining the layer order: it has to be local, because there exist folding patterns where different regions are layered differently in different parts of the folded state.&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/blog/assets/2023/over-under.svg&quot; alt=&quot;A folding pattern where two regions have different over-under relations at two different parts of the folded state&quot; style=&quot;width:100%;max-width:720px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The paper shows that finding a valid above-below relation for each region can be done using a dynamic programming algorithm in time exponential in the treewidth of this graph, where the base of the exponential is factorial in the &lt;em&gt;ply&lt;/em&gt; of the folding pattern, the maximum number of pieces of paper that all fold on top of each other at any single point. As it also shows, even for patterns of bounded ply, the exponential dependence on treewidth is necessary under standard complexity-theoretic assumptions.&lt;/p&gt;

&lt;p&gt;But what about the dependence on ply? Is that necessary? Why should it be difficult to fold patterns that have many layers, but simple region adjacency graphs?&lt;/p&gt;

&lt;p&gt;Unlike for treewidth, I don’t have a proof that high ply makes the problem hard. But there is a natural class of problems for which the ply is very high, the treewidth is tiny, and the complexity of finding a flat folding is a famous open problem. This is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Map_folding&quot;&gt;map folding problem&lt;/a&gt;, where the input is just a rectangular grid with labeled creases, and the desired output is a flat-folded state respecting the given labeling. Its region adjacency graph is just a single vertex (everything should fold onto a single square), but its ply can be big (the number of squares in the entire grid). Even for a very simple case, a \(2\times 2\) grid, Wikipedia provides the following illustration of the many vertical orderings among the squares of the grid that are possible. Each square is shown with a different color, visible on both of its sides:&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/blog/assets/2023/map-foldings.png&quot; alt=&quot;Eight solutions to the 2x2 map folding problem&quot; title=&quot;CC-BY-SA 3.0 image File:MapFoldings-2x2.png by Robert Dickau from Wikimedia commons&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The \(1\times n\) map folding problem is easy: just fold over one square from the end of a strip of \(n\) squares, according to the label of its crease, and then solve the remaining \(1\times (n-1)\) problem recursively, treating the folded-over square and the next square that it is folded onto as a single unit. In a 2012 MIT master’s thesis supervised by Erik Demaine, Thomas Morgan &lt;a href=&quot;http://dspace.mit.edu/handle/1721.1/77030&quot;&gt;announced a solution to the \(2\times n\) map folding problem&lt;/a&gt;, although I don’t know that it was ever published in any other form. The algorithm is complicated, with time bound \(O(n^9)\), and the writing is not always clear.&lt;/p&gt;

&lt;p&gt;The techniques from Morgan’s thesis are very specific to the \(2\times n\) case. Basically, everything you need to know can be recovered from a one-dimensional flat folding of the central crease of the \(2\times n\) grid. At the end of the thesis there is a single paragraph on higher order grids, which (after removing unnecessary complications) boils down to testing all orderings of the squares that are consistent with the above-below relations forced by the crease labels of adjacent squares. The number of orderings can be very small (for instance, if you accordion-fold in one direction and then the other, there is only one consistent ordering) and when it is the algorithm is fast. Beyond that observation, the map folding problem for grids larger than \(2\times n\) appears to be wide open.&lt;/p&gt;

&lt;p&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/110585322158385767&quot;&gt;Discuss on Mastodon&lt;/a&gt;)&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By David Eppstein&lt;/p&gt;
  </description>
  <pubDate>2023-06-21 18:34:00 UTC</pubDate>
  <author>David Eppstein</author>
</item>

</channel>
</rss>
