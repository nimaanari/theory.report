<?xml version="1.0"?>
<rss version="2.0">

<channel>
  <title>Theory of Computing Report</title>
  <link></link>
  <language>en</language>
  <description></description>


<item>
  <title>TR23-005 |  Cumulative Memory Lower Bounds for Randomized and Quantum Computation | 

	Paul Beame, 

	Niels Kornerup</title>
  <guid>https://eccc.weizmann.ac.il/report/2023/005</guid>
  <link>https://eccc.weizmann.ac.il/report/2023/005</link>
  <description>
    Cumulative memory---the sum of space used over the steps of a computation---is a fine-grained measure of time-space complexity that is a more accurate measure of cost for algorithms with infrequent spikes in memory usage in the context of technologies such as cloud computing that allow dynamic allocation and de-allocation of resources during their execution. We give the first lower bounds on cumulative memory complexity that apply to general sequential classical algorithms.  We also prove the first such bounds for bounded-error quantum circuits.  Among many possible applications, we show that any classical sorting algorithm with success probability at least $1/\text{poly}(n)$ requires cumulative memory $\tilde \Omega(n^2)$, any classical matrix multiplication algorithm requires cumulative memory $\Omega(n^6/T)$, any quantum sorting circuit requires cumulative memory $\Omega(n^3/T)$, and any quantum circuit that finds $k$ disjoint collisions in a random function requires cumulative memory $\Omega(k^3n/T^2)$.  More generally, we present theorems that can be used to convert a wide class of existing time-space tradeoff lower bounds to matching lower bounds on cumulative memory complexity.
  </description>
  <pubDate>2023-01-13 07:30:49 UTC</pubDate>
  <author>ECCC Papers</author>
</item>

<item>
  <title>TR23-004 |  On linear-algebraic notions of expansion | 

	Yinan Li, 

	Youming Qiao, 

	Avi Wigderson, 

	Yuval Wigderson, 

	Chuanqi Zhang</title>
  <guid>https://eccc.weizmann.ac.il/report/2023/004</guid>
  <link>https://eccc.weizmann.ac.il/report/2023/004</link>
  <description>
    A fundamental fact about bounded-degree graph expanders is that three notions of expansion---vertex expansion, edge expansion, and spectral expansion---are all equivalent. In this paper, we study to what extent such a statement is true for linear-algebraic notions of expansion.

There are two well-studied notions of linear-algebraic expansion, namely dimension expansion (defined in analogy to graph vertex expansion) and quantum expansion (defined in analogy to graph spectral expansion). Lubotzky and Zelmanov proved that the latter implies the former. We prove that the converse is false: there are dimension expanders which are not quantum expanders.

Moreover, this asymmetry is explained by the fact that there are two distinct linear-algebraic analogues of graph edge expansion. The first of these is quantum edge expansion, which was introduced by Hastings, and which he proved to be equivalent to quantum expansion. We introduce a new notion, termed dimension edge expansion, which we prove is equivalent to dimension expansion and which is implied by quantum edge expansion. Thus, the separation above is implied by a finer one: dimension edge expansion is strictly weaker than quantum edge expansion. This new notion also leads to a new, more modular proof of the Lubotzky--Zelmanov result that quantum expanders are dimension expanders.
  </description>
  <pubDate>2023-01-13 07:27:38 UTC</pubDate>
  <author>ECCC Papers</author>
</item>

<item>
  <title>TR23-003 |  Streaming Lower Bounds and Asymmetric Set-Disjointness | 

	Jiapeng Zhang, 

	Shachar Lovett</title>
  <guid>https://eccc.weizmann.ac.il/report/2023/003</guid>
  <link>https://eccc.weizmann.ac.il/report/2023/003</link>
  <description>
    Frequency estimation in data streams is one of the classical problems in streaming algorithms. Following much research, there are now almost matching upper and lower bounds for the trade-off needed between the number of samples and the space complexity of the algorithm, when the data streams are adversarial. However, in the case where the data stream is given in a random order, or is stochastic, only weaker lower bounds exist. In this work we close this gap, up to logarithmic factors. 

In order to do so we consider the needle problem, which is a natural hard problem for frequency estimation studied in (Andoni et al. 2008, Crouch et al. 2016). Here, the goal is to distinguish between two distributions over data streams with $t$ samples. The first is uniform over a large enough domain. The second is a planted model; a secret &amp;#39;&amp;#39;needle&amp;#39;&amp;#39; is uniformly chosen, and then each element in the stream equals the needle with probability $p$, and otherwise is uniformly chosen from the domain. It is simple to design streaming algorithms that distinguish the distributions using space $s \approx 1/(p^2 t)$. It was unclear if this is tight, as the existing lower bounds are weaker. We close this gap and show that the trade-off is near optimal, up to a logarithmic factor. 

Our proof builds and extends classical connections between streaming algorithms and communication complexity, concretely multi-party unique set-disjointness. We introduce two new ingredients that allow us to prove sharp bounds. The first is a lower bound for an asymmetric version of multi-party unique set-disjointness, where players receive input sets of different sizes, and where the communication of each player is normalized relative to their input length. The second is a combinatorial technique that allows to sample needles in the planted model by first sampling intervals, and then sampling a uniform needle in each interval.
  </description>
  <pubDate>2023-01-13 07:25:30 UTC</pubDate>
  <author>ECCC Papers</author>
</item>

<item>
  <title>A Functorial Perspective on (Multi)computational Irreducibility</title>
  <guid>http://arxiv.org/abs/2301.04690</guid>
  <link>http://arxiv.org/abs/2301.04690</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorard_J/0/1/0/all/0/1&quot;&gt;Jonathan Gorard&lt;/a&gt;&lt;/p&gt;&lt;p&gt;This article aims to provide a novel formalization of the concept of
computational irreducibility in terms of the exactness of functorial
correspondence between a category of data structures and elementary
computations and a corresponding category of (1-dimensional) cobordisms. We
proceed to demonstrate that, by equipping both categories with a symmetric
monoidal structure and considering the case of higher-dimensional cobordism
categories, we obtain a natural extension of this formalism that serves also to
encompass non-deterministic or ``multiway&#39;&#39; computations, in which one
quantifies not only the irreducibility in the behavior of a single
(deterministic) computation path, but in the branching and merging behavior of
an entire ``multiway system&#39;&#39; of such paths too. We finally outline how, in the
most general case, the resulting symmetric monoidal functor may be considered
to be adjoint to the functor characterizing the Atiyah-Segal axiomatization of
a functorial quantum field theory. Thus, we conclude by arguing that the
irreducibility of (multi)computations may be thought of as being dual to the
locality of time evolution in functorial approaches to quantum mechanics and
quantum field theory. In the process, we propose an extension of the methods of
standard (monoidal) category theory, in which morphisms are effectively
equipped with intrinsic computational complexity data, together with an algebra
for how those complexities compose (both in sequence and in parallel, subject
to the monoidal structure).
&lt;/p&gt;
  </description>
  <pubDate>2023-01-13 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Isomorphisms Between Impossible and Hard Tasks</title>
  <guid>http://arxiv.org/abs/2301.04789</guid>
  <link>http://arxiv.org/abs/2301.04789</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monroe_H/0/1/0/all/0/1&quot;&gt;Hunter Monroe&lt;/a&gt;&lt;/p&gt;&lt;p&gt;If no efficient proof shows that an unprovable arithmetic sentence &#39;$x$ is
Kolmogorov random&#39; (&#39;$x{\in}R$&#39;) lacks a length $t$ proof, an isomorphism
associates for each $x$ impossible and hard tasks: ruling out any proof and
length $t$ proofs respectively. This resembles Pudl\&#39;ak&#39;s feasible
incompleteness. This possible isomorphism implies widely-believed complexity
theoretic conjectures hold -- in effect, translating theorems from
noncomputability about proof speedup and average-case hardness directly to
complexity.
&lt;/p&gt;
&lt;p&gt;Formally, we conjecture: sentence &quot;Peano arithmetic (PA) lacks any length $t$
proof of &#39;$x{\in}R$&#39;&quot; lacks $t^{\mathcal{O}(1)}$ length proofs in any
consistent extension $\mathcal{T}$ of PA if and only if $\mathcal{T}$ cannot
prove &#39;$x{\in}R$&#39;. If so, tautologies encoding the sentence lack
$t^{\mathcal{O}(1)}$ length proofs in any proof system $P$ for $x{\in}R$
sufficiently long (relative to the description of a program enumerating
theorems of a theory $\mathcal{T}$ proving &#39;$P$ is sound&#39;). $R$&#39;s density
implies: $\texttt{TAUT}{\notin}\textbf{AvgP}$, Feige&#39;s hypothesis holds, and, a
new conjecture, $P$&#39;s nonoptimality has dense witnesses. If the isomorphism
holds for any $\Pi^0$ sentence, $\textbf{PH}$ does not collapse, because the
arithmetic hierarchy does not collapse.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-13 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Local consistency as a reduction between constraint satisfaction problems</title>
  <guid>http://arxiv.org/abs/2301.05084</guid>
  <link>http://arxiv.org/abs/2301.05084</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalmau_V/0/1/0/all/0/1&quot;&gt;Victor Dalmau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oprsal_J/0/1/0/all/0/1&quot;&gt;Jakub Opr&amp;#x161;al&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the use of local consistency methods as reductions between
constraint satisfaction problems (CSPs), and promise version thereof, with the
aim to classify these reductions in similar way as the algebraic approach
classifies gadget reductions between CSPs. We classify a use of arc-consistency
in this way, provide first steps into classification of general
$k$-consistency, and ask whether every tractable finite template CSP is
reducible by such a reduction to solving systems of affine Diophantine
equations.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-13 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Fast persistent homology computation for functions on $\mathbb{R}$</title>
  <guid>http://arxiv.org/abs/2301.04745</guid>
  <link>http://arxiv.org/abs/2301.04745</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glisse_M/0/1/0/all/0/1&quot;&gt;Marc Glisse&lt;/a&gt;&lt;/p&gt;&lt;p&gt;0-dimensional persistent homology is known, from a computational point of
view, as the easy case. Indeed, given a list of $n$ edges in non-decreasing
order of filtration value, one only needs a union-find data structure to keep
track of the connected components and we get the persistence diagram in time
$O(n\alpha(n))$. The running time is thus usually dominated by sorting the
edges in $\Theta(n\log(n))$. A little-known fact is that, in the particularly
simple case of studying the sublevel sets of a piecewise-linear function on
$\mathbb{R}$ or $\mathbb{S}^1$, persistence can actually be computed in linear
time. This note presents a simple algorithm that achieves this complexity. An
implementation will soon be available in Gudhi.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-13 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>On Voronoi visibility maps of 1.5D terrains with multiple viewpoints</title>
  <guid>http://arxiv.org/abs/2301.05049</guid>
  <link>http://arxiv.org/abs/2301.05049</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keikha_V/0/1/0/all/0/1&quot;&gt;Vahideh Keikha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saumell_M/0/1/0/all/0/1&quot;&gt;Maria Saumell&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Given an $n$-vertex 1.5D terrain $\T$ and a set $\A$ of $m&amp;lt;n$ viewpoints, the
Voronoi visibility map $\vorvis(\T,\A)$ is a partitioning of $\T$ into regions
such that each region is assigned to the closest (in Euclidean distance)
visible viewpoint. The colored visibility map $\colvis(\T,\A)$ is a
partitioning of $\T$ into regions that have the same set of visible viewpoints.
In this paper, we propose an algorithm to compute $\vorvis(\T,\A)$ that runs in
$O(n+(m^2+k_c)\log n)$ time, where $k_c$ and $k_v$ denote the total complexity
of $\colvis(\T,\A)$ and $\vorvis(\T,\A)$, respectively. This improves upon a
previous algorithm for this problem. We also generalize our algorithm to higher
order Voronoi visibility maps, and to Voronoi visibility maps with respect to
other distances. Finally, we prove bounds relating $k_v$ to $k_c$, and we show
an application of our algorithm to a problem on limited range of sight.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-13 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Density functions of periodic sequences of continuous events</title>
  <guid>http://arxiv.org/abs/2301.05137</guid>
  <link>http://arxiv.org/abs/2301.05137</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anosova_O/0/1/0/all/0/1&quot;&gt;Olga Anosova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurlin_V/0/1/0/all/0/1&quot;&gt;Vitaliy Kurlin&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Periodic Geometry studies isometry invariants of periodic point sets that are
also continuous under perturbations. The motivations come from periodic
crystals whose structures are determined in a rigid form but any minimal cells
can discontinuously change due to small noise in measurements. For any integer
k&amp;gt;=0, the density function of a periodic set S was previously defined as the
fractional volume of all k-fold intersections (within a minimal cell) of balls
that have a variable radius t and centers at all points of S. This paper
introduces the density functions for periodic sets of points with different
initial radii motivated by atomic radii of chemical elements and by continuous
events occupying disjoint intervals in time series. The contributions are
explicit descriptions of the densities for periodic sequences of intervals. The
new densities are strictly stronger and distinguish periodic sequences that
have identical densities in the case of zero radii.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-13 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Quantum algorithm for finding minimum values in a Quantum Random Access Memory</title>
  <guid>http://arxiv.org/abs/2301.05122</guid>
  <link>http://arxiv.org/abs/2301.05122</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Albino_A/0/1/0/all/0/1&quot;&gt;Anton S. Albino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Galvao_L/0/1/0/all/0/1&quot;&gt;Lucas Q. Galv&amp;#xe3;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Hansen_E/0/1/0/all/0/1&quot;&gt;Ethan Hansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Neto_M/0/1/0/all/0/1&quot;&gt;Mauro Q. Nooblath Neto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Cruz_C/0/1/0/all/0/1&quot;&gt;Clebson Cruz&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Finding the minimum value in an unordered database is a common and
fundamental task in computer science. However, the optimal classical
deterministic algorithm can find the minimum value with a time complexity that
grows linearly with the number of elements in the database. In this paper, we
present the proposal of a quantum algorithm for finding the minimum value of a
database, which is quadratically faster than its best classical analogs. We
assume a Quantum Random Access Memory (QRAM) that stores values from a database
and perform an iterative search based on an oracle whose role is to limit the
searched values by controlling the states of the most significant qubits. A
complexity analysis was performed in order to demonstrate the advantage of this
quantum algorithm over its classical counterparts. Furthermore, we demonstrate
how the proposed algorithm would be used in an unsupervised machine learning
task through a quantum version of the K-means algorithm.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-13 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Private estimation algorithms for stochastic block models and mixture models</title>
  <guid>http://arxiv.org/abs/2301.04822</guid>
  <link>http://arxiv.org/abs/2301.04822</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongjie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_Addad_V/0/1/0/all/0/1&quot;&gt;Vincent Cohen-Addad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+dOrsi_T/0/1/0/all/0/1&quot;&gt;Tommaso d&amp;#x27;Orsi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Epasto_A/0/1/0/all/0/1&quot;&gt;Alessandro Epasto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imola_J/0/1/0/all/0/1&quot;&gt;Jacob Imola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steurer_D/0/1/0/all/0/1&quot;&gt;David Steurer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiegel_S/0/1/0/all/0/1&quot;&gt;Stefan Tiegel&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We introduce general tools for designing efficient private estimation
algorithms, in the high-dimensional settings, whose statistical guarantees
almost match those of the best known non-private algorithms. To illustrate our
techniques, we consider two problems: recovery of stochastic block models and
learning mixtures of spherical Gaussians. For the former, we present the first
efficient $(\epsilon, \delta)$-differentially private algorithm for both weak
recovery and exact recovery. Previously known algorithms achieving comparable
guarantees required quasi-polynomial time. For the latter, we design an
$(\epsilon, \delta)$-differentially private algorithm that recovers the centers
of the $k$-mixture when the minimum separation is at least $
O(k^{1/t}\sqrt{t})$. For all choices of $t$, this algorithm requires sample
complexity $n\geq k^{O(1)}d^{O(t)}$ and time complexity $(nd)^{O(t)}$. Prior
work required minimum separation at least $O(\sqrt{k})$ as well as an explicit
upper bound on the Euclidean norm of the centers.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-13 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Distance-2-Dispersion: Dispersion with Further Constraints</title>
  <guid>http://arxiv.org/abs/2301.04938</guid>
  <link>http://arxiv.org/abs/2301.04938</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaur_T/0/1/0/all/0/1&quot;&gt;Tanvir Kaur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_K/0/1/0/all/0/1&quot;&gt;Kaushik Mondal&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The aim of the dispersion problem is to place a set of $k(\leq n)$ mobile
robots in the nodes of an unknown graph consisting of $n$ nodes such that in
the final configuration each node contains at most one robot, starting from any
arbitrary initial configuration of the robots on the graph. In this work we
propose a variant of the dispersion problem where we start with any number of
robots, and put an additional constraint that no two adjacent nodes contain
robots in the final configuration. We name this problem as
Distance-2-Dispersion (D-2-D). However, even if the number of robots $k$ is
less than $n$, it may not possible for each robot to find a distinct node to
reside, maintaining our added constraint. Specifically, if a maximal
independent set is already formed by the nodes which contain a robot each, then
other robots, if any, who are searching for a node to seat, will not find one.
Hence we allow multiple robots to seat on some nodes only if there is no place
to seat. If $k\geq n$, it is guaranteed that the nodes with robots form a
maximal independent set of the underlying network.
&lt;/p&gt;
&lt;p&gt;The graph $G=(V, E)$ has $n$ nodes and $m$ edges, where nodes are anonymous.
It is a port labelled graph, i.e., each node $u$ assigns a distinct port number
to each of its incident edges from a range $[0,\delta-1]$ where $\delta$ is the
degree of the node $u$. The robots have unique ids in the range $[1, L]$, where
$L \ge k$. Co-located robots can communicate among themselves. We provide an
algorithm that solves D-2-D starting from a rooted configuration (i.e.,
initially all the robots are co-located) and terminate after $2\Delta(8m-3n+3)$
synchronous rounds using $O(log \Delta)$ memory per robot without using any
global knowledge of the graph parameters $m$, $n$ and $\Delta$, the maximum
degree of the graph. We also provide $\Omega(m\Delta)$ lower bound on the
number of rounds for the D-2-D problem.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-13 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Computing m-Eternal Domination Number of Cactus Graphs in Linear Time</title>
  <guid>http://arxiv.org/abs/2301.05155</guid>
  <link>http://arxiv.org/abs/2301.05155</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blazej_V/0/1/0/all/0/1&quot;&gt;V&amp;#xe1;clav Bla&amp;#x17e;ej&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kristan_J/0/1/0/all/0/1&quot;&gt;Jan Maty&amp;#xe1;&amp;#x161; K&amp;#x159;i&amp;#x161;&amp;#x165;an&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valla_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Valla&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In m-eternal domination attacker and defender play on a graph. Initially, the
defender places guards on vertices. In each round, the attacker chooses a
vertex to attack. Then, the defender can move each guard to a neighboring
vertex and must move a guard to the attacked vertex. The m-eternal domination
number is the minimum number of guards such that the graph can be defended
indefinitely. In this paper, we study the m-eternal domination number of cactus
graphs. We consider two variants of the m-eternal domination number: one allows
multiple guards to occupy a single vertex, the second variant requires the
guards to occupy distinct vertices. We develop several tools for obtaining
lower and upper bounds on these problems and we use them to obtain an algorithm
which computes the minimum number of required guards of cactus graphs for both
variants of the problem.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-13 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>1 Postdoc+2 PhD students at IDSIA (apply by April 30, 2023)</title>
  <guid>http://cstheory-jobs.org/2023/01/12/1-postdoc2-phd-students-at-idsia-apply-by-april-30-2023/</guid>
  <link>https://cstheory-jobs.org/2023/01/12/1-postdoc2-phd-students-at-idsia-apply-by-april-30-2023/</link>
  <description>
    &lt;p&gt;The Algorithms and Complexity Group at IDSIA Lugano (Switzerland), opens 2 Ph.D. positions (4 years) and 1 Postdoc position (up to 4 years).&lt;br /&gt;
These positions are supported by the SNSF project &amp;#8220;Computational methods for integrality gaps analysis&amp;#8221;. There is no specific deadline; the positions will be filled as soon as eligible candidates with an appropriate background apply.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://people.idsia.ch/~monaldo/positions/positions.html&quot;&gt;https://people.idsia.ch/~monaldo/positions/positions.html&lt;/a&gt;&lt;br /&gt;
Email: monaldo@idsia.ch&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </description>
  <pubDate>2023-01-12 16:08:46 UTC</pubDate>
  <author>CCI: jobs</author>
</item>

<item>
  <title>Semantic Search for the Blog</title>
  <guid>tag:blogger.com,1999:blog-3722233.post-8422659110181940895</guid>
  <link>http://blog.computationalcomplexity.org/2023/01/semantic-search-for-blog.html</link>
  <description>
    &lt;p&gt;&amp;nbsp;As Google started to &lt;a href=&quot;https://support.google.com/a/answer/10403871&quot;&gt;limit academic storage&lt;/a&gt;, I started looking at Google Takeout and started wondering what I could do with all that data. I downloaded all the posts from the blog, since we use Google&#39;s blogger, and ran them through &lt;a href=&quot;https://openai.com/blog/new-and-improved-embedding-model/&quot;&gt;OpenAI&#39;s Ada Embedding&lt;/a&gt;. The Ada embedding maps text up to 8192 words into a point on the 1536-dimensional unit sphere. You can measure the similarity between two embeddings via a simple dot product, giving you the cosine of the angle between them.&lt;/p&gt;&lt;p&gt;So I created a semantic search for the blog. Go ahead and &lt;a href=&quot;https://fortnow.pythonanywhere.com/&quot;&gt;try it out&lt;/a&gt;.&lt;/p&gt;
&lt;form action=&quot;https://fortnow.pythonanywhere.com/&quot; method=&quot;POST&quot;&gt;
    &lt;p&gt;Search for&amp;nbsp;&amp;nbsp;&lt;input name=&quot;search&quot; type=&quot;text&quot; /&gt;&lt;/p&gt;&lt;p&gt;You can enter a search term, phrase, or the full URL (including https) of a blog post. It will return a list of the 5 closest posts, with the percentage match, computed as the square of the cosine. I don&#39;t have a mechanism for automatically updating the files, so you&#39;ll only see posts from 2022 and earlier.&lt;/p&gt;&lt;p&gt;This was an Open AI-assisted affair, as I used ChatGPT and GitHub co-pilot to help with the python and pandas data frames. It took me longer to figure out how to create a web application so you can try the search. Similarity match doesn&#39;t work like normal searches, for example if you search for a city like &quot;Detroit&quot;, you&#39;ll get posts that mention other cities. Some other oddities, like &quot;mad&quot; seems to match &quot;Madhu&quot;. It probably says something about me that my most happy post is not about some great new theorem but about &lt;a href=&quot;https://blog.computationalcomplexity.org/2005/10/joy-on-south-side-of-chicago.html&quot;&gt;baseball&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;/form&gt;
&lt;p class=&quot;authors&quot;&gt;By Lance Fortnow&lt;/p&gt;
  </description>
  <pubDate>2023-01-12 14:41:00 UTC</pubDate>
  <author>Computational Complexity</author>
</item>

<item>
  <title>Resources on how to apply for a CS job in academia/industry</title>
  <guid>tag:blogger.com,1999:blog-27705661.post-2113255095239595644</guid>
  <link>http://processalgebra.blogspot.com/2023/01/resources-on-how-to-apply-for-cs-job-in.html</link>
  <description>
    &lt;p&gt;The PhD students in my department asked for advice on how to apply for jobs in academia and industry. I&#39;ll share whatever I might have to say with them this coming Tuesday and I am going through some material I collected.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Do you have any favourite resources on how to apply for a CS job in academia or industry such as &lt;a href=&quot;https://matt.might.net/articles/advice-for-academic-job-hunt/&quot; target=&quot;_blank&quot;&gt;this advice &lt;/a&gt;by Matt Might? If so, I&#39;d be grateful if you could share it with me as comments to this post. I&#39;ll collect the material and make it available somewhere.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Thanks in advance!&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By Luca Aceto&lt;/p&gt;
  </description>
  <pubDate>2023-01-12 09:12:00 UTC</pubDate>
  <author>Luca Aceto</author>
</item>

<item>
  <title>What are Blockchains Useful for, Really?</title>
  <guid>https://decentralizedthoughts.github.io/2023-01-12-what-are-blockchains-useful-for-really/</guid>
  <link>https://decentralizedthoughts.github.io/2023-01-12-what-are-blockchains-useful-for-really/</link>
  <description>
    Blockchains, or the decentralized ledger, are touted as the next big disruptive technology, as big as the Internet was in the 90s. What are these blockchains useful for, really? While there are relevant use cases, many examples people use that are either far too academic to be useful or are...
  </description>
  <pubDate>2023-01-12 05:00:00 UTC</pubDate>
  <author>Decentralized Thoughts</author>
</item>

<item>
  <title>Maximum Centre-Disjoint Mergeable Disks</title>
  <guid>http://arxiv.org/abs/2301.04350</guid>
  <link>http://arxiv.org/abs/2301.04350</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1&quot;&gt;Ali Gholami Rudi&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Given a set of disks on the plane, the goal of the problem studied in this
paper is to choose a subset of these disks such that none of its members
contains the centre of any other. Each disk not in this subset must be merged
with one of its nearby disks that is, increasing the latter&#39;s radius. We prove
that this problem is NP-hard. We also present polynomial-time algorithms for
the special case in which the centres of all disks are on a line.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-12 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>The Basis of Design Tools for Quantum Computing: Arrays, Decision Diagrams, Tensor Networks, and ZX-Calculus</title>
  <guid>http://arxiv.org/abs/2301.04147</guid>
  <link>http://arxiv.org/abs/2301.04147</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wille_R/0/1/0/all/0/1&quot;&gt;Robert Wille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Burgholzer_L/0/1/0/all/0/1&quot;&gt;Lukas Burgholzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Hillmich_S/0/1/0/all/0/1&quot;&gt;Stefan Hillmich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Grurl_T/0/1/0/all/0/1&quot;&gt;Thomas Grurl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ploier_A/0/1/0/all/0/1&quot;&gt;Alexander Ploier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Peham_T/0/1/0/all/0/1&quot;&gt;Tom Peham&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Quantum computers promise to efficiently solve important problems classical
computers never will. However, in order to capitalize on these prospects, a
fully automated quantum software stack needs to be developed. This involves a
multitude of complex tasks from the classical simulation of quantum circuits,
over their compilation to specific devices, to the verification of the circuits
to be executed as well as the obtained results. All of these tasks are highly
non-trivial and necessitate efficient data structures to tackle the inherent
complexity. Starting from rather straight-forward arrays over decision diagrams
(inspired by the design automation community) to tensor networks and the
ZX-calculus, various complementary approaches have been proposed. This work
provides a look &quot;under the hood&quot; of today&#39;s tools and showcases how these means
are utilized in them, e.g., for simulation, compilation, and verification of
quantum circuits.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-12 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Linear Time Online Algorithms for Constructing Linear-size Suffix Trie</title>
  <guid>http://arxiv.org/abs/2301.04295</guid>
  <link>http://arxiv.org/abs/2301.04295</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendrian_D/0/1/0/all/0/1&quot;&gt;Diptarama Hendrian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takagi_T/0/1/0/all/0/1&quot;&gt;Takuya Takagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inenaga_S/0/1/0/all/0/1&quot;&gt;Shunsuke Inenaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goto_K/0/1/0/all/0/1&quot;&gt;Keisuke Goto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Funakoshi_M/0/1/0/all/0/1&quot;&gt;Mitsuru Funakoshi&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The suffix trees are fundamental data structures for various kinds of string
processing. The suffix tree of a text string $T$ of length $n$ has $O(n)$ nodes
and edges, and the string label of each edge is encoded by a pair of positions
in $T$. Thus, even after the tree is built, the input string $T$ needs to be
kept stored and random access to $T$ is still needed. The \emph{linear-size
suffix tries} (\emph{LSTs}), proposed by Crochemore et al. [Linear-size suffix
tries, TCS 638:171-178, 2016], are a &quot;stand-alone&quot; alternative to the suffix
trees. Namely, the LST of an input text string $T$ of length $n$ occupies
$O(n)$ total space, and supports pattern matching and other tasks with the same
efficiency as the suffix tree without the need to store the input text string
$T$. Crochemore et al. proposed an \emph{offline} algorithm which transforms
the suffix tree of $T$ into the LST of $T$ in $O(n \log \sigma)$ time and
$O(n)$ space, where $\sigma$ is the alphabet size. In this paper, we present
two types of \emph{online} algorithms which &quot;directly&quot; construct the LST, from
right to left, and from left to right, without constructing the suffix tree as
an intermediate structure. Both algorithms construct the LST incrementally when
a new symbol is read, and do not access the previously read symbols. Both of
the right-to-left construction algorithm and the left-to-right construction
algorithm work in $O(n \log \sigma)$ time and $O(n)$ space. The main feature of
our algorithms is that the input text string does not need to be stored.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-12 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>A Note on Property Testing of the Binary Rank</title>
  <guid>http://arxiv.org/abs/2301.04406</guid>
  <link>http://arxiv.org/abs/2301.04406</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bshouty_N/0/1/0/all/0/1&quot;&gt;Nader H. Bshouty&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Let $M$ be a $n\times m$ $(0,1)$-matrix. We define the $s$-binary rank,
$br_s(M)$, of $M$ to be the minimal integer $d$ such that there are $d$
monochromatic rectangles that cover all the $1$-entries in the matrix, and each
$1$-entry is covered by at most $s$ rectangles. When $s=1$, this is the binary
rank,~$br(M)$, known from the literature. Let $R(M)$ and $C(M)$ be the set of
rows and columns of~$M$, respectively. We use the result of Sgall (Comb. 1999)
to prove that if $M$ has $s$-binary rank at most~$d$, then $|R(M)|\cdot
|C(M)|\le {d\choose \le s}2^{d}$ where ${d\choose \le s}=\sum_{i=0}^s{d\choose
i}$. This bound is tight; that is, there exists a matrix $M&#39;$ of $s$-binary
rank $d$ such that $|R(M&#39;)|\cdot |C(M&#39;)|= {d\choose \le s}2^{d}$. Using this
result, we give a new one-sided adaptive and non-adaptive testers for
$(0,1)$-matrices of $s$-binary rank at most $d$ (and exactly $d$) that makes
$\tilde O\left({d\choose \le s}2^d/\epsilon\right)$ and $\tilde
O\left({d\choose \le s}2^d/\epsilon^2\right)$ queries, respectively. For a
fixed $s$, this improves the query complexity of the tester of Parnas et al.
(Theory Comput. Syst. 2021) by a factor of $\tilde \Theta (2^d)$.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-12 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Stream-K: Work-centric Parallel Decomposition for Dense Matrix-Matrix Multiplication on the GPU</title>
  <guid>http://arxiv.org/abs/2301.03598</guid>
  <link>http://arxiv.org/abs/2301.03598</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osama_M/0/1/0/all/0/1&quot;&gt;Muhammad Osama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merrill_D/0/1/0/all/0/1&quot;&gt;Duane Merrill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cecka_C/0/1/0/all/0/1&quot;&gt;Cris Cecka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garland_M/0/1/0/all/0/1&quot;&gt;Michael Garland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Owens_J/0/1/0/all/0/1&quot;&gt;John D. Owens&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We introduce Stream-K, a work-centric parallelization of matrix
multiplication (GEMM) and related computations in dense linear algebra. Whereas
contemporary decompositions are primarily tile-based, our method operates by
partitioning an even share of the aggregate inner loop iterations among
physical processing elements. This provides a near-perfect utilization of
computing resources, regardless of how efficiently the output tiling for any
given problem quantizes across the underlying processing elements.
&lt;/p&gt;
&lt;p&gt;On GPU processors, our Stream-K parallelization of GEMM produces a peak
speedup of up to 14$\times$ and 6.7$\times$, and an average performance
response that is both higher and more consistent across 32,824 GEMM problem
geometries than state-of-the-art math libraries such as CUTLASS and cuBLAS.
Furthermore, we achieve this performance from a single tile size configuration
per floating-point precision, whereas today&#39;s math libraries employ complex
kernel-selection heuristics to select from a large ensemble of kernel variants.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-11 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Improved Approximation Algorithms for the Expanding Search Problem</title>
  <guid>http://arxiv.org/abs/2301.03638</guid>
  <link>http://arxiv.org/abs/2301.03638</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griesbach_S/0/1/0/all/0/1&quot;&gt;Svenja M. Griesbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hommelsheim_F/0/1/0/all/0/1&quot;&gt;Felix Hommelsheim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klimm_M/0/1/0/all/0/1&quot;&gt;Max Klimm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schewior_K/0/1/0/all/0/1&quot;&gt;Kevin Schewior&lt;/a&gt;&lt;/p&gt;&lt;p&gt;A searcher faces a graph with edge lengths and vertex weights, initially
having explored only a given starting vertex. In each step, the searcher adds
an edge to the solution that connects an unexplored vertex to an explored
vertex. This requires an amount of time equal to the edge length. The goal is
to minimize the vertex-weighted sum of the exploration times over all vertices.
We show that this problem is hard to approximate and provide algorithms with
improved approximation guarantees. For the case that all vertices have unit
weight, we provide a $2e$-approximation. For the general case, we give a
$(5e/2+\varepsilon)$-approximation for any $\varepsilon &amp;gt; 0$. Previously, for
both cases only an $8$-approximation was known. Finally, we provide a PTAS for
the case of a Euclidean graph.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-11 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Quantum Speedups for Zero-Sum Games via Improved Dynamic Gibbs Sampling</title>
  <guid>http://arxiv.org/abs/2301.03763</guid>
  <link>http://arxiv.org/abs/2301.03763</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Bouland_A/0/1/0/all/0/1&quot;&gt;Adam Bouland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Getachew_Y/0/1/0/all/0/1&quot;&gt;Yosheb Getachew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yujia Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Sidford_A/0/1/0/all/0/1&quot;&gt;Aaron Sidford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Tian_K/0/1/0/all/0/1&quot;&gt;Kevin Tian&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We give a quantum algorithm for computing an $\epsilon$-approximate Nash
equilibrium of a zero-sum game in a $m \times n$ payoff matrix with bounded
entries. Given a standard quantum oracle for accessing the payoff matrix our
algorithm runs in time $\widetilde{O}(\sqrt{m + n}\cdot \epsilon^{-2.5} +
\epsilon^{-3})$ and outputs a classical representation of the
$\epsilon$-approximate Nash equilibrium. This improves upon the best prior
quantum runtime of $\widetilde{O}(\sqrt{m + n} \cdot \epsilon^{-3})$ obtained
by [vAG19] and the classic $\widetilde{O}((m + n) \cdot \epsilon^{-2})$ runtime
due to [GK95] whenever $\epsilon = \Omega((m +n)^{-1})$. We obtain this result
by designing new quantum data structures for efficiently sampling from a
slowly-changing Gibbs distribution.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-11 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Proportionally Fair Matching with Multiple Groups</title>
  <guid>http://arxiv.org/abs/2301.03862</guid>
  <link>http://arxiv.org/abs/2301.03862</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bandyapadhyay_S/0/1/0/all/0/1&quot;&gt;Sayan Bandyapadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fomin_F/0/1/0/all/0/1&quot;&gt;Fedor V. Fomin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inamdar_T/0/1/0/all/0/1&quot;&gt;Tanmay Inamdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simonov_K/0/1/0/all/0/1&quot;&gt;Kirill Simonov&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The study of fair algorithms has become mainstream in machine learning and
artificial intelligence due to its increasing demand in dealing with biases and
discrimination. Along this line, researchers have considered fair versions of
traditional optimization problems including clustering, regression, ranking and
voting. However, most of the efforts have been channeled into designing
heuristic algorithms, which often do not provide any guarantees on the quality
of the solution. In this work, we study matching problems with the notion of
proportional fairness. Proportional fairness is one of the most popular notions
of group fairness where every group is represented up to an extent proportional
to the final selection size. Matching with proportional fairness or more
commonly, proportionally fair matching, was introduced in [Chierichetti et al.,
AISTATS, 2019], where the problem was studied with only two groups. However, in
many practical applications, the number of groups -- although often a small
constant -- is larger than two. In this work, we make the first step towards
understanding the computational complexity of proportionally fair matching with
more than two groups. We design exact and approximation algorithms achieving
reasonable guarantees on the quality of the matching as well as on the time
complexity. Our algorithms are also supported by suitable hardness bounds.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-11 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Maintaining Triconnected Components under Node Expansion</title>
  <guid>http://arxiv.org/abs/2301.03972</guid>
  <link>http://arxiv.org/abs/2301.03972</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fink_S/0/1/0/all/0/1&quot;&gt;Simon D. Fink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rutter_I/0/1/0/all/0/1&quot;&gt;Ignaz Rutter&lt;/a&gt;&lt;/p&gt;&lt;p&gt;SPQR-trees are a central component of graph drawing and are also important in
many further areas of computer science. From their inception onwards, they have
always had a strong relation to dynamic algorithms maintaining information,
e.g., on planarity and triconnectivity, under edge insertion and, later on,
also deletion. In this paper, we focus on a special kind of dynamic update, the
expansion of vertices into arbitrary biconnected graphs, while maintaining the
SPQR-tree and further information. This will also allow us to efficiently merge
two SPQR-trees by identifying the edges incident to two vertices with each
other. We do this working along an axiomatic definition lifting the SPQR-tree
to a stand-alone data structure that can be modified independently from the
graph it might have been derived from. Making changes to this structure, we can
now observe how the graph represented by the SPQR-tree changes, instead of
having to reason which updates to the SPQR-tree are necessary after a change to
the represented graph.
&lt;/p&gt;
&lt;p&gt;Using efficient expansions and merges allows us to improve the runtime of the
Synchronized Planarity algorithm by Bl\&quot;asius et al. [ESA 2021] from $O(m^2)$
to $O(m\cdot \Delta)$, where $\Delta$ is the maximum pipe degree. This also
reduces the time for solving several constrained planarity problems, e.g. for
Clustered Planarity from $O((n+d)^2)$ to $O(n+d\cdot \Delta)$, where $d$ is the
total number of crossings between cluster borders and edges and $\Delta$ is the
maximum number of edge crossings on a single cluster border.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-11 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Change Propagation Without Joins</title>
  <guid>http://arxiv.org/abs/2301.04003</guid>
  <link>http://arxiv.org/abs/2301.04003</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qichen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Binyang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1&quot;&gt;Ke Yi&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We revisit the classical change propagation framework for query evaluation
under updates. The standard framework takes a query plan and materializes the
intermediate views, which incurs high polynomial costs in both space and time,
with the join operator being the culprit. In this paper, we propose a new
change propagation framework without joins, thus naturally avoiding this
polynomial blowup. Meanwhile, we show that the new framework still supports
constant-delay enumeration of both the deltas and the full query results, the
same as in the standard framework. Furthermore, we provide a quantitative
analysis of its update cost, which not only recovers many recent theoretical
results on the problem, but also yields an effective approach to optimizing the
query plan. The new framework is also easy to be integrated into an existing
streaming database system. Experimental results show that our system prototype,
implemented using Flink DataStream API, significantly outperforms other systems
in terms of space, time, and latency.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-11 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>On the Complexity of the Two-Stage Majority Rule</title>
  <guid>http://arxiv.org/abs/2301.04009</guid>
  <link>http://arxiv.org/abs/2301.04009</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yongjie Yang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Sequential voting rules have been extensively used in parliamentary and
legislative decision making. After observing that the prevalent successive and
the amendment rules fail several fundamental axioms, Horan and Sprumont [2021]
proposed very recently a two-stage sequential rule which satisfies a variety of
desirable properties. This paper examines this rule by investigating the
complexity of Agenda Control, Coalition Manipulation, Possible Winner,
Necessary Winner, and eight standard election control problems. Our study
offers a comprehensive understanding of the complexity landscape of these
problems.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-11 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>On Knuth&#39;s conjecture for back and forward arcs in Depth First Search in a random digraph with geometric outdegree distribution</title>
  <guid>http://arxiv.org/abs/2301.04131</guid>
  <link>http://arxiv.org/abs/2301.04131</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Janson_S/0/1/0/all/0/1&quot;&gt;Svante Janson&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Donald Knuth, in a draft of a coming volume of The Art of Computer
Programming, has recently conjectured that in Depth-First Search of a random
digraph with geometric outdegree distribution, the numbers of back and forward
arcs have the same distribution.
&lt;/p&gt;
&lt;p&gt;We show that this conjecture is equivalent to an equality between two
generating functions defined by different recursions.
&lt;/p&gt;
&lt;p&gt;Unfortunately, we have not been able so use this to prove the conjecture,
which still is open, but we hope that this note will inspire others to succeed
with the conjecture.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-11 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Representing Matroids over the Reals is $\exists \mathbb R$-complete</title>
  <guid>http://arxiv.org/abs/2301.03221</guid>
  <link>http://arxiv.org/abs/2301.03221</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1&quot;&gt;Eunjung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mesmay_A/0/1/0/all/0/1&quot;&gt;Arnaud de Mesmay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miltzow_T/0/1/0/all/0/1&quot;&gt;Tillmann Miltzow&lt;/a&gt;&lt;/p&gt;&lt;p&gt;A matroid $M$ is an ordered pair $(E,I)$, where $E$ is a finite set called
the ground set and a collection $I\subset 2^{E}$ called the independent sets
which satisfy the conditions: (I1) $\emptyset \in I$, (I2) $I&#39;\subset I \in I$
implies $I&#39;\in I$, and (I3) $I_1,I_2 \in I$ and $|I_1| &amp;lt; |I_2|$ implies that
there is an $e\in I_2$ such that $I_1\cup \{e\} \in I$. The rank $rank(M)$ of a
matroid $M$ is the maximum size of an independent set. We say that a matroid
$M=(E,I)$ is representable over the reals if there is a map $\varphi : E
\rightarrow \mathbb{R}^{rank(M)}$ such that $I\in I$ if and only if
$\varphi(I)$ forms a linearly independent set.
&lt;/p&gt;
&lt;p&gt;We study the problem of matroid realizability over the reals. Given a matroid
$M$, we ask whether there is a set of points in the Euclidean space
representing $M$. We show that matroid realizability is $\exists \mathbb
R$-complete, already for matroids of rank 3. The complexity class $\exists
\mathbb R$ can be defined as the family of algorithmic problems that is
polynomial-time is equivalent to determining if a multivariate polynomial with
integers coefficients has a real root.
&lt;/p&gt;
&lt;p&gt;Our methods are similar to previous methods from the literature. Yet, the
result itself was never pointed out and there is no proof readily available in
the language of computer science.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Exceeding Computational Complexity Trial-and-Error Dynamic Action and Intelligence</title>
  <guid>http://arxiv.org/abs/2301.03384</guid>
  <link>http://arxiv.org/abs/2301.03384</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Chuyu Xiong&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Computational complexity is a core theory of computer science, which dictates
the degree of difficulty of computation. There are many problems with high
complexity that we have to deal, which is especially true for AI. This raises a
big question: Is there a better way to deal with these highly complex problems
other than bounded by computational complexity? We believe that ideas and
methods from intelligence science can be applied to these problems and help us
to exceed computational complexity. In this paper, we try to clarify concepts,
and we propose definitions such as unparticularized computing, particularized
computing, computing agents, and dynamic search. We also propose and discuss a
framework, i.e., trial-and-error + dynamic search. Number Partition Problem is
a well-known NP-complete problem, and we use this problem as an example to
illustrate the ideas discussed.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>A Critique of Sopin&#39;s &quot;${\rm PH} = {\rm PSPACE}$&quot;</title>
  <guid>http://arxiv.org/abs/2301.03487</guid>
  <link>http://arxiv.org/abs/2301.03487</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chavrimootoo_M/0/1/0/all/0/1&quot;&gt;Michael C. Chavrimootoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clingerman_I/0/1/0/all/0/1&quot;&gt;Ian Clingerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luu_Q/0/1/0/all/0/1&quot;&gt;Quan Luu&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We critique Valerii Sopin&#39;s paper &quot;${\rm PH} = {\rm PSPACE}$&quot; [Sop14]. The
paper claims to resolve one of the major open problems of theoretical computer
science by leveraging the Skolemization of existential quantifiers of
quantified boolean formulas to show that ${\rm QBF}$ (a well-known ${\rm
PSPACE}$-complete problem) is in $\Pi_4^p$, and thus ${\rm PH} = {\rm PSPACE}$.
In this critique, we highlight problems in that paper and conclude that it
fails to establish that ${\rm PH} = {\rm PSPACE}$.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Maximum overlap area of a convex polyhedron and a convex polygon under translation</title>
  <guid>http://arxiv.org/abs/2301.02949</guid>
  <link>http://arxiv.org/abs/2301.02949</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kweon_H/0/1/0/all/0/1&quot;&gt;Hyuk Jun Kweon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Honglin Zhu&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Let $P$ be a convex polyhedron and $Q$ be a convex polygon with $n$ vertices
in total in three-dimensional space. We present a deterministic algorithm that
finds a translation vector $v \in \mathbb{R}^3$ maximizing the overlap area $|P
\cap (Q + v)|$ in $O(n \log^2 n)$ time. We then apply our algorithm to solve
two related problems. We give an $O(n \log^3 n)$ time algorithm that finds the
maximum overlap area of three convex polygons with $n$ vertices in total. We
also give an $O(n \log^2 n)$ time algorithm that minimizes the symmetric
difference of two convex polygons under scaling and translation.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Variational Direct Modeling: A framework towards integration of parametric modeling and direct modeling in CAD</title>
  <guid>http://arxiv.org/abs/2301.02999</guid>
  <link>http://arxiv.org/abs/2301.02999</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1&quot;&gt;Qiang Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Hsi-Yung Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shuming Gao&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Feature-based parametric modeling is the de facto standard in CAD. Boundary
representation-based direct modeling is another CAD paradigm developed
recently. They have complementary advantages and limitations, thereby offering
huge potential for improvement towards an integrated CAD modeling scheme. Most
existing integration methods are developed by industry and typically treat
direct edits as pseudo-features, where little can be said about seamless
integration. This paper presents an alternative method for seamless
parametric/direct integration, which allows parametric and direct edits to work
in a unified way. The fundamental issues and challenges of parametric/direct
integration are first explained. A framework is then proposed to handle those
information inconsistencies, based on a detection-then-resolution strategy.
Algorithms that can systematically detect and resolve all possible types of
information inconsistencies are also given to implement the framework. With
them, model validity can be maintained during the whole model editing process,
and then the discrepancy between direct edits and parametric edits can be
resolved. The effectiveness of the proposed approach has been shown with a
series of case studies and comparisons, based on a preliminary prototype.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Machining feature recognition using descriptors with range constraints for mechanical 3D models</title>
  <guid>http://arxiv.org/abs/2301.03167</guid>
  <link>http://arxiv.org/abs/2301.03167</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Seungeun Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeo_C/0/1/0/all/0/1&quot;&gt;Changmo Yeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1&quot;&gt;Fazhi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jinwon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mun_D/0/1/0/all/0/1&quot;&gt;Duhwan Mun&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In machining feature recognition, geometric elements generated in a
three-dimensional computer-aided design model are identified. This technique is
used in manufacturability evaluation, process planning, and tool path
generation. Here, we propose a method of recognizing 16 types of machining
features using descriptors, often used in shape-based part retrieval studies.
The base face is selected for each feature type, and descriptors express the
base face&#39;s minimum, maximum, and equal conditions. Furthermore, the similarity
in the three conditions between the descriptors extracted from the target face
and those from the base face is calculated. If the similarity is greater than
or equal to the threshold, the target face is determined as the base face of
the feature. Machining feature recognition tests were conducted for two test
cases using the proposed method, and all machining features included in the
test cases were successfully recognized. Also, it was confirmed through an
additional test that the proposed method in this study showed better feature
recognition performance than the latest artificial neural network.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Combinatorial Properties for a Class of Simplicial Complexes Extended from Pseudo-fractal Scale-free Web</title>
  <guid>http://arxiv.org/abs/2301.03230</guid>
  <link>http://arxiv.org/abs/2301.03230</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zixuan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yucheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wanyue Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Liwang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhongzhi Zhang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Simplicial complexes are a popular tool used to model higher-order
interactions between elements of complex social and biological systems. In this
paper, we study some combinatorial aspects of a class of simplicial complexes
created by a graph product, which is an extension of the pseudo-fractal
scale-free web. We determine explicitly the independence number, the domination
number, and the chromatic number. Moreover, we derive closed-form expressions
for the number of acyclic orientations, the number of root-connected acyclic
orientations, the number of spanning trees, as well as the number of perfect
matchings for some particular cases.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Dimensionality Reduction for Persistent Homology with Gaussian Kernels</title>
  <guid>http://arxiv.org/abs/2301.03321</guid>
  <link>http://arxiv.org/abs/2301.03321</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boissonnat_J/0/1/0/all/0/1&quot;&gt;Jean-Daniel Boissonnat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_K/0/1/0/all/0/1&quot;&gt;Kunal Dutta&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Computing persistent homology using Gaussian kernels is useful in the domains
of topological data analysis and machine learning as shown by Phillips, Wang
and Zheng [SoCG 2015]. However, contrary to the case of computing persistent
homology using the Euclidean distance or even the $k$-distance, it is not known
how to compute the persistent homology of high dimensional data using Gaussian
kernels. In this paper, we consider a power distance version of the Gaussian
kernel distance (GKPD) given by Phillips, Wang and Zheng, and show that the
persistent homology of the \v{C}ech filtration of $P$ computed using the GKPD
is approximately preserved. For datasets in $d$-dimensional Euclidean space,
under a relative error bound of $\varepsilon \in [0,1]$, we obtain a
dimensionality of $(i)$ $O(\varepsilon^{-2}\log^2 n)$ for $n$-point datasets
and $(ii)$ $O(D\varepsilon^{-2}\log (Dr/\varepsilon))$ for datasets having
diameter $r$ (up to a scaling factor).
&lt;/p&gt;
&lt;p&gt;We use two main ingredients. The first one is a new decomposition of the
squared radii of \v{C}ech simplices using the kernel power distance, in terms
of the pairwise GKPDs between the vertices, which we state and prove. The
second one is the Random Fourier Features (RFF) map of Rahimi and Recht
[NeurIPS 2007], as used by Chen and Phillips [ALT 2017].
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Sparse Geometric Set Systems and the Beck-Fiala Conjecture</title>
  <guid>http://arxiv.org/abs/2301.03329</guid>
  <link>http://arxiv.org/abs/2301.03329</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_K/0/1/0/all/0/1&quot;&gt;Kunal Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Arijit Ghosh&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We investigate the combinatorial discrepancy of geometric set systems having
bounded shallow cell complexity in the \emph{Beck-Fiala} setting, where each
point belongs to at most $t$ ranges. For set systems with shallow cell
complexity $\psi(m,k)=g(m)k^{c}$, where $(i)$ $g(m) = o(m^{\varepsilon})$ for
any $\varepsilon\in (0,1],$ $(ii)$ $\psi$ is non-decreasing in $m$, and $(iii)$
$c&amp;gt;0$ is independent of $m$ and $k$, we get a discrepancy bound of
&lt;/p&gt;
&lt;p&gt;\[ O\left(\sqrt{\left(\log
n+\left(t^{c}g(n)\right)^{\frac{1}{1+c}}\right)\log n}\right).\]
&lt;/p&gt;
&lt;p&gt;For $t=\omega(\log^2 n)$, in several cases, such as for set systems of points
and half-planes / disks / pseudo-disks in $\mathbb{R}^2$, points and orthants
in $\mathbb{R}^3$ etc., these bounds are $o(\sqrt{t})$, which verifies (and
improves upon) the conjectured bound of Beck and Fiala~\emph{(Disc. Appl.
Math., 1981)}.
&lt;/p&gt;
&lt;p&gt;Our bounds are obtained by showing the existence of \emph{matchings with low
crossing number}, using the multiplicative weights update method of Welzl
\emph{(SoCG, 1988)}, together with the recent bound of Mustafa \emph{(Disc.
Comp. Geom., 2015)} on \emph{shallow packings} of set systems in terms of their
shallow cell complexity. For set systems of shallow cell complexity
$\psi(m,k)=m^{c_1}g(m)k^{c}$, we obtain matchings with crossing number at most
&lt;/p&gt;
&lt;p&gt;\[ O\left(\left(n^{c_1}g(n)t^{c}\right)^{\frac{1}{1+c_1+c}}\right).\]
&lt;/p&gt;
&lt;p&gt;These are of independent interest.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Strong Collapse of Random Simplicial Complexes</title>
  <guid>http://arxiv.org/abs/2301.03514</guid>
  <link>http://arxiv.org/abs/2301.03514</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boissonnat_J/0/1/0/all/0/1&quot;&gt;Jean-Daniel Boissonnat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_K/0/1/0/all/0/1&quot;&gt;Kunal Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1&quot;&gt;Soumik Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pritam_S/0/1/0/all/0/1&quot;&gt;Siddharth Pritam&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The \emph{strong collapse} of a simplicial complex, proposed by Barmak and
Minian (\emph{Disc. Comp. Geom. 2012}), is a combinatorial collapse of a
complex onto its sub-complex. Recently, it has received attention from
computational topology researchers, owing to its empirically observed
usefulness in simplification and size-reduction
&lt;/p&gt;
&lt;p&gt;of the size of simplicial complexes while preserving the homotopy class. We
consider the strong collapse process on random simplicial complexes. For the
&lt;/p&gt;
&lt;p&gt;Erd\H{o}s-R\&#39;enyi random clique complex $X(n,c/n)$ on $n$ vertices with edge
probability $c/n$ with $c&amp;gt;1$, we show that after any maximal sequence of strong
collapses
&lt;/p&gt;
&lt;p&gt;the remaining subcomplex, or \emph{core} must have $(1-\gamma)(1-c\gamma)
n+o(n)$ vertices asymptotically almost surely (a.a.s.), where $\gamma$ is the
least non-negative fixed
&lt;/p&gt;
&lt;p&gt;point of the function $f(x) = \exp\left(-c(1-x)\right)$ in the range $(0,1)$.
&lt;/p&gt;
&lt;p&gt;These are the first theoretical results proved for strong collapses on random
(or non-random) simplicial complexes.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Sublinear Time Algorithms for Several Geometric Optimization (With Outliers) Problems In Machine Learning</title>
  <guid>http://arxiv.org/abs/2301.02870</guid>
  <link>http://arxiv.org/abs/2301.02870</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hu Ding&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this paper, we study several important geometric optimization problems
arising in machine learning. First, we revisit the Minimum Enclosing Ball (MEB)
problem in Euclidean space $\mathbb{R}^d$. The problem has been extensively
studied before, but real-world machine learning tasks often need to handle
large-scale datasets so that we cannot even afford linear time algorithms.
Motivated by the recent studies on {\em beyond worst-case analysis}, we
introduce the notion of stability for MEB, which is natural and easy to
understand. Roughly speaking, an instance of MEB is stable, if the radius of
the resulting ball cannot be significantly reduced by removing a small fraction
of the input points. Under the stability assumption, we present two sampling
algorithms for computing radius-approximate MEB with sample complexities
independent of the number of input points $n$. In particular, the second
algorithm has the sample complexity even independent of the dimensionality $d$.
We also consider the general case without the stability assumption. We present
a hybrid algorithm that can output either a radius-approximate MEB or a
covering-approximate MEB. Our algorithm improves the running time and the
number of passes for the previous sublinear MEB algorithms. Our method relies
on two novel techniques, the Uniform-Adaptive Sampling method and Sandwich
Lemma. Furthermore, we observe that these two techniques can be generalized to
design sublinear time algorithms for a broader range of geometric optimization
problems with outliers in high dimensions, including MEB with outliers,
one-class and two-class linear SVMs with outliers, $k$-center clustering with
outliers, and flat fitting with outliers. Our proposed algorithms also work
fine for kernels.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Randomized Greedy Algorithms and Composable Coreset for k-Center Clustering with Outliers</title>
  <guid>http://arxiv.org/abs/2301.02814</guid>
  <link>http://arxiv.org/abs/2301.02814</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Ruomin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haikuo Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zixiu Wang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this paper, we study the problem of {\em $k$-center clustering with
outliers}. The problem has many important applications in real world, but the
presence of outliers can significantly increase the computational complexity.
Though a number of methods have been developed in the past decades, it is still
quite challenging to design quality guaranteed algorithm with low complexity
for this problem. Our idea is inspired by the greedy method, Gonzalez&#39;s
algorithm, that was developed for solving the ordinary $k$-center clustering
problem. Based on some novel observations, we show that a simple randomized
version of this greedy strategy actually can handle outliers efficiently. We
further show that this randomized greedy approach also yields small coreset for
the problem in doubling metrics (even if the doubling dimension is not given),
which can greatly reduce the computational complexity. Moreover, together with
the partial clustering framework proposed in &lt;a href=&quot;/abs/1703.01539&quot;&gt;arXiv:1703.01539&lt;/a&gt; , we prove that
our coreset method can be applied to distributed data with a low communication
complexity. The experimental results suggest that our algorithms can achieve
near optimal solutions and yield lower complexities comparing with the existing
methods.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Assigning Agents to Increase Network-Based Neighborhood Diversity</title>
  <guid>http://arxiv.org/abs/2301.02876</guid>
  <link>http://arxiv.org/abs/2301.02876</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1&quot;&gt;Zirou Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1&quot;&gt;Andrew Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marathe_M/0/1/0/all/0/1&quot;&gt;Madhav V. Marathe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1&quot;&gt;S. S. Ravi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenkrantz_D/0/1/0/all/0/1&quot;&gt;Daniel J. Rosenkrantz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stearns_R/0/1/0/all/0/1&quot;&gt;Richard E. Stearns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vullikanti_A/0/1/0/all/0/1&quot;&gt;Anil Vullikanti&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Social segregation is a persistent problem in society. Despite of the
existing strategic plans to advance diversity, we continue to witness spatial
segregation of people by demographic features. Motivated by real-world
applications, such as public-housing allocation for low-income individuals, we
examine the problem of assigning a group of agents to vertices in a graph that
represents spatial locations. Agents are of two types (subgroups) characterized
by certain sensitive features. The goal is to construct an assignment that
maximizes the level of diversity. Specifically, we quantify the diversity by
the number of well-integrated agents, that is, the agents who have at least one
neighbor of a different type in the network. Given the intractable nature of
this maximization problem, we focus on developing approximation algorithms with
provable performance guarantees. We first propose a local-improvement algorithm
for general graphs with a constant factor 1/2 approximation. Further, for a
special case where the sizes of two subgroups are similar, we present a
semidefinite programming approach that yields an approximation factor better
than 1/2. We also show that the problem can be solved efficiently when the
underlying graph is treewidth-bounded, and then use this result to obtain a
polynomial time approximation scheme (PTAS) for the problem on planar graphs.
Lastly, we conduct experiments to evaluate the performance of the proposed
algorithms on realistic networks.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Abstract Huffman Coding and PIFO Tree Embeddings</title>
  <guid>http://arxiv.org/abs/2301.02878</guid>
  <link>http://arxiv.org/abs/2301.02878</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DAngelo_K/0/1/0/all/0/1&quot;&gt;Keri D&amp;#x27;Angelo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kozen_D/0/1/0/all/0/1&quot;&gt;Dexter Kozen&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Algorithms for deriving Huffman codes and the recently developed algorithm
for compiling PIFO trees to trees of fixed shape (Mohan et al. 2022) are
similar, but work with different underlying algebraic operations. In this
paper, we exploit the monadic structure of prefix codes to create a generalized
Huffman algorithm that has these two applications as special cases.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Quantum Honest Byzantine Agreement as a Distributed Quantum Algorithm</title>
  <guid>http://arxiv.org/abs/2301.02944</guid>
  <link>http://arxiv.org/abs/2301.02944</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Edwards_M/0/1/0/all/0/1&quot;&gt;Marcus Edwards&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Blockchain technology has three main components: the network, consensus
algorithm and distributed data structure. Each of these brings with it
particular issues of scalability and efficiency. By recasting the network and
consensus algorithm components of blockchain to a quantum algorithm, we show
that the efficiency and scalability of blockchain technology can be improved in
the near-term without requiring powerful quantum computers to be available.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>SeedTree: A Dynamically Optimal and Local Self-Adjusting Tree</title>
  <guid>http://arxiv.org/abs/2301.03074</guid>
  <link>http://arxiv.org/abs/2301.03074</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pourdamghani_A/0/1/0/all/0/1&quot;&gt;Arash Pourdamghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avin_C/0/1/0/all/0/1&quot;&gt;Chen Avin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sama_R/0/1/0/all/0/1&quot;&gt;Robert Sama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmid_S/0/1/0/all/0/1&quot;&gt;Stefan Schmid&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We consider the fundamental problem of designing a self-adjusting tree, which
efficiently and locally adapts itself towards the demand it serves (namely
accesses to the items stored by the tree nodes), striking a balance between the
benefits of such adjustments (enabling faster access) and their costs
(reconfigurations). This problem finds applications, among others, in the
context of emerging demand-aware and reconfigurable datacenter networks and
features connections to self-adjusting data structures. Our main contribution
is SeedTree, a dynamically optimal self-adjusting tree which supports local
(i.e., greedy) routing, which is particularly attractive under highly dynamic
demands. SeedTree relies on an innovative approach which defines a set of
unique paths based on randomized item addresses, and uses a small constant
number of items per node. We complement our analytical results by showing the
benefits of SeedTree empirically, evaluating it on various synthetic and
real-world communication traces.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Dynamic Binary Search Trees: Improved Lower Bounds for the Greedy-Future Algorithm</title>
  <guid>http://arxiv.org/abs/2301.03084</guid>
  <link>http://arxiv.org/abs/2301.03084</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadeh_Y/0/1/0/all/0/1&quot;&gt;Yaniv Sadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaplan_H/0/1/0/all/0/1&quot;&gt;Haim Kaplan&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Binary search trees (BSTs) are one of the most basic and widely used data
structures. The best static tree for serving a sequence of queries (searches)
can be computed by dynamic programming. In contrast, when the BSTs are allowed
to be dynamic (i.e. change by rotations between searches), we still do not know
how to compute the optimal algorithm (OPT) for a given sequence. One of the
candidate algorithms whose serving cost is suspected to be optimal up-to a
(multiplicative) constant factor is known by the name Greedy Future (GF). In an
equivalent geometric way of representing queries on BSTs, GF is in fact
equivalent to another algorithm called Geometric Greedy (GG). Most of the
results on GF are obtained using the geometric model and the study of GG.
Despite this intensive recent fruitful research, the best lower bound we have
on the competitive ratio of GF is $\frac{4}{3}$. Furthermore, it has been
conjectured that the additive gap between the cost of GF and OPT is only linear
in the number of queries. In this paper we prove a lower bound of $2$ on the
competitive ratio of GF, and we prove that the additive gap between the cost of
GF and OPT can be $\Omega(m \cdot \log\log n)$ where $n$ is the number of items
in the tree and $m$ is the number of queries.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Structural Equivalence in Subgraph Matching</title>
  <guid>http://arxiv.org/abs/2301.03161</guid>
  <link>http://arxiv.org/abs/2301.03161</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dominic Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yurun Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thien Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moorman_J/0/1/0/all/0/1&quot;&gt;Jacob Moorman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molitor_D/0/1/0/all/0/1&quot;&gt;Denali Molitor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertozzi_A/0/1/0/all/0/1&quot;&gt;Andrea Bertozzi&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Symmetry plays a major role in subgraph matching both in the description of
the graphs in question and in how it confounds the search process. This work
addresses how to quantify these effects and how to use symmetries to increase
the efficiency of subgraph isomorphism algorithms. We introduce rigorous
definitions of structural equivalence and establish conditions for when it can
be safely used to generate more solutions. We illustrate how to adapt standard
search routines to utilize these symmetries to accelerate search and compactly
describe the solution space. We then adapt a state-of-the-art solver and
perform a comprehensive series of tests to demonstrate these methods&#39; efficacy
on a standard benchmark set. We extend these methods to multiplex graphs and
present results on large multiplex networks drawn from transportation systems,
social media, adversarial attacks, and knowledge graphs.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Subset verification and search algorithms for causal DAGs</title>
  <guid>http://arxiv.org/abs/2301.03180</guid>
  <link>http://arxiv.org/abs/2301.03180</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_D/0/1/0/all/0/1&quot;&gt;Davin Choo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shiragur_K/0/1/0/all/0/1&quot;&gt;Kirankumar Shiragur&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Learning causal relationships between variables is a fundamental task in
causal inference and directed acyclic graphs (DAGs) are a popular choice to
represent the causal relationships. As one can recover a causal graph only up
to its Markov equivalence class from observations, interventions are often used
for the recovery task. Interventions are costly in general and it is important
to design algorithms that minimize the number of interventions performed.
&lt;/p&gt;
&lt;p&gt;In this work, we study the problem of learning the causal relationships of a
subset of edges (target edges) in a graph with as few interventions as
possible. Under the assumptions of faithfulness, causal sufficiency, and ideal
interventions, we study this problem in two settings: when the underlying
ground truth causal graph is known (subset verification) and when it is unknown
(subset search).
&lt;/p&gt;
&lt;p&gt;For the subset verification problem, we provide an efficient algorithm to
compute a minimum sized interventional set; we further extend these results to
bounded size non-atomic interventions and node-dependent interventional costs.
For the subset search problem, in the worst case, we show that no algorithm
(even with adaptivity or randomization) can achieve an approximation ratio that
is asymptotically better than the vertex cover of the target edges when
compared with the subset verification number. This result is surprising as
there exists a logarithmic approximation algorithm for the search problem when
we wish to recover the whole causal graph. To obtain our results, we prove
several interesting structural properties of interventional causal graphs that
we believe have applications beyond the subset verification/search problems
studied here.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Space-Query Tradeoffs in Range Subgraph Counting and Listing</title>
  <guid>http://arxiv.org/abs/2301.03390</guid>
  <link>http://arxiv.org/abs/2301.03390</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Shiyuan Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shangqi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1&quot;&gt;Yufei Tao&lt;/a&gt;&lt;/p&gt;&lt;p&gt;This paper initializes the study of {\em range subgraph counting} and {\em
range subgraph listing}, both of which are motivated by the significant demands
in practice to perform graph analytics on subgraphs pertinent to only selected,
as opposed to all, vertices. In the first problem, there is an undirected graph
$G$ where each vertex carries a real-valued attribute. Given an interval $q$
and a pattern $Q$, a query counts the number of occurrences of $Q$ in the
subgraph of $G$ induced by the vertices whose attributes fall in $q$. The
second problem has the same setup except that a query needs to enumerate
(rather than count) those occurrences with a small delay. In both problems, our
goal is to understand the tradeoff between {\em space usage} and {\em query
cost}, or more specifically: (i) given a target on query efficiency, how much
pre-computed information about $G$ must we store? (ii) Or conversely, given a
budget on space usage, what is the best query time we can hope for? We
establish a suite of upper- and lower-bound results on such tradeoffs for
various query patterns.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Simple Binary Hypothesis Testing under Local Differential Privacy and Communication Constraints</title>
  <guid>http://arxiv.org/abs/2301.03566</guid>
  <link>http://arxiv.org/abs/2301.03566</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pensia_A/0/1/0/all/0/1&quot;&gt;Ankit Pensia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Asadi_A/0/1/0/all/0/1&quot;&gt;Amir R. Asadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jog_V/0/1/0/all/0/1&quot;&gt;Varun Jog&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Loh_P/0/1/0/all/0/1&quot;&gt;Po-Ling Loh&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study simple binary hypothesis testing under both local differential
privacy (LDP) and communication constraints. We qualify our results as either
minimax optimal or instance optimal: the former hold for the set of
distribution pairs with prescribed Hellinger divergence and total variation
distance, whereas the latter hold for specific distribution pairs. For the
sample complexity of simple hypothesis testing under pure LDP constraints, we
establish instance-optimal bounds for distributions with binary support;
minimax-optimal bounds for general distributions; and (approximately)
instance-optimal, computationally efficient algorithms for general
distributions. When both privacy and communication constraints are present, we
develop instance-optimal, computationally efficient algorithms that achieve the
minimum possible sample complexity (up to universal constants). Our results on
instance-optimal algorithms hinge on identifying the extreme points of the
joint range set $\mathcal A$ of two distributions $p$ and $q$, defined as
$\mathcal A := \{(\mathbf T p, \mathbf T q) | \mathbf T \in \mathcal C\}$,
where $\mathcal C$ is the set of channels characterizing the constraints.
&lt;/p&gt;
  </description>
  <pubDate>2023-01-10 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

</channel>
</rss>
