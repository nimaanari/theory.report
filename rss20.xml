<?xml version="1.0"?>
<rss version="2.0">

<channel>
  <title>Theory of Computing Report</title>
  <link></link>
  <language>en</language>
  <description></description>


<item>
  <title>A Quantum Outlier Theorem</title>
  <guid>http://arxiv.org/abs/2303.06256</guid>
  <link>http://arxiv.org/abs/2303.06256</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Epstein_S/0/1/0/all/0/1&quot;&gt;Samuel Epstein&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In recent results, it has been proven that all sampling methods produce
outliers. In this paper, we extend these results to quantum information theory.
Projectors of large rank must contain pure quantum states in their images that
are outlying states. Otherwise, the projectors are exotic, in that they have
high mutual information with the halting sequence. Thus quantum coding schemes
that use projections, such as Schumacher compression, must communicate using
outlier quantum states.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-14 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Finding large counterexamples by selectively exploring the Pachner graph</title>
  <guid>http://arxiv.org/abs/2303.06321</guid>
  <link>http://arxiv.org/abs/2303.06321</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Burton_B/0/1/0/all/0/1&quot;&gt;Benjamin A. Burton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+He_A/0/1/0/all/0/1&quot;&gt;Alexander He&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We often rely on censuses of triangulations to guide our intuition in
$3$-manifold topology. However, this can lead to misplaced faith in conjectures
if the smallest counterexamples are too large to appear in our census. Since
the number of triangulations increases super-exponentially with size, there is
no way to expand a census beyond relatively small triangulations; the current
census only goes up to $10$ tetrahedra. Here, we show that it is feasible to
search for large and hard-to-find counterexamples by using heuristics to
selectively (rather than exhaustively) enumerate triangulations. We use this
idea to find counterexamples to three conjectures which ask, for certain
$3$-manifolds, whether one-vertex triangulations always have a &quot;distinctive&quot;
edge that would allow us to recognise the $3$-manifold.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-14 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Betti Number for Point Sets</title>
  <guid>http://arxiv.org/abs/2303.06354</guid>
  <link>http://arxiv.org/abs/2303.06354</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Topology is the foundation for many industrial applications ranging from CAD
to simulation analysis. Computational topology mostly focuses on structured
data such as mesh, however unstructured dataset such as point set remains a
virgin land for topology scientists. The significance of point-based topology
can never be overemphasized, especially in the area of reverse engineering,
geometric modeling and algorithmic analysis. In this paper, we propose a novel
approach to compute the Betti number for point set data and illustrate its
usefulness in real world examples. To the best of our knowledge, our work is
pioneering and first of its kind in the fields of computational topology.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-14 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Generalizing Greenwald-Khanna Streaming Quantile Summaries for Weighted Inputs</title>
  <guid>http://arxiv.org/abs/2303.06288</guid>
  <link>http://arxiv.org/abs/2303.06288</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assadi_S/0/1/0/all/0/1&quot;&gt;Sepehr Assadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1&quot;&gt;Nirmit Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhu_M/0/1/0/all/0/1&quot;&gt;Milind Prabhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1&quot;&gt;Vihan Shah&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Estimating quantiles, like the median or percentiles, is a fundamental task
in data mining and data science. A (streaming) quantile summary is a data
structure that can process a set S of n elements in a streaming fashion and at
the end, for any phi in (0,1], return a phi-quantile of S up to an eps error,
i.e., return a phi&#39;-quantile with phi&#39;=phi +- eps. We are particularly
interested in comparison-based summaries that only compare elements of the
universe under a total ordering and are otherwise completely oblivious of the
universe. The best known deterministic quantile summary is the 20-year old
Greenwald-Khanna (GK) summary that uses O((1/eps) log(eps n)) space
[SIGMOD&#39;01]. This bound was recently proved to be optimal for all deterministic
comparison-based summaries by Cormode and Vesle\&#39;y [PODS&#39;20].
&lt;/p&gt;
&lt;p&gt;In this paper, we study weighted quantiles, a generalization of the quantiles
problem, where each element arrives with a positive integer weight which
denotes the number of copies of that element being inserted. The only known
method of handling weighted inputs via GK summaries is the naive approach of
breaking each weighted element into multiple unweighted items and feeding them
one by one to the summary, which results in a prohibitively large update time
(proportional to the maximum weight of input elements).
&lt;/p&gt;
&lt;p&gt;We give the first non-trivial extension of GK summaries for weighted inputs
and show that it takes O((1/eps) log(eps n)) space and O(log(1/eps)+ log
log(eps n)) update time per element to process a stream of length n (under some
quite mild assumptions on the range of weights and eps). En route to this, we
also simplify the original GK summaries for unweighted quantiles.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-14 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Binary Search with Distance-Dependent Costs</title>
  <guid>http://arxiv.org/abs/2303.06488</guid>
  <link>http://arxiv.org/abs/2303.06488</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leng_C/0/1/0/all/0/1&quot;&gt;Calvin Leng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kempe_D/0/1/0/all/0/1&quot;&gt;David Kempe&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We introduce a search problem generalizing the typical setting of Binary
Search on the line. Similar to the setting for Binary Search, a target is
chosen adversarially on the line, and in response to a query, the algorithm
learns whether the query was correct, too high, or too low. Different from the
Binary Search setting, the cost of a query is a monotone non-decreasing
function of the distance between the query and the correct answer; different
functions can be used for queries that are too high vs. those that are too low.
The algorithm&#39;s goal is to identify an adversarially chosen target with minimum
total cost. Note that the algorithm does not even know the cost it incurred
until the end, when the target is revealed. This abstraction captures many
natural settings in which a principal experiments by setting a quantity (such
as an item price, bandwidth, tax rate, medicine dosage, etc.) where the cost or
regret increases the further the chosen setting is from the optimal one.
&lt;/p&gt;
&lt;p&gt;First, we show that for arbitrary symmetric cost functions (i.e.,
overshooting vs. undershooting by the same amount leads to the same cost), the
standard Binary Search algorithm is a 4-approximation.
&lt;/p&gt;
&lt;p&gt;We then show that when the cost functions are bounded-degree polynomials of
the distance, the problem can be solved optimally using Dynamic Programming;
this relies on a careful encoding of the combined cost of past queries (which,
recall, will only be revealed in the future). We then generalize the setting to
finding a node on a tree; here, the response to a query is the direction on the
tree in which the target is located, and the cost is increasing in the distance
on the tree from the query to the target. Using the k-cut search tree framework
of Berendsohn and Kozma and the ideas we developed for the case of the line, we
give a PTAS when the cost function is a bounded-degree polynomial.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-14 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>PhD and Post-doctoral Positions  at Augusta University (apply by June 1, 2023)</title>
  <guid>http://cstheory-jobs.org/2023/03/13/phd-and-post-doctoral-positions-at-augusta-university-apply-by-june-1-2023/</guid>
  <link>https://cstheory-jobs.org/2023/03/13/phd-and-post-doctoral-positions-at-augusta-university-apply-by-june-1-2023/</link>
  <description>
    &lt;p&gt;The Concurrency In Reversible Computations (&lt;a href=&quot;https://github.com/CinRC&quot;&gt;https://github.com/CinRC&lt;/a&gt;) project is actively seeking a PhD student to fund starting Spring 2024. The funding for an admitted PhD student includes tuition waiver, stipend, health benefits, (international) conference travel and possibly equipment. Funding is currently available for the first three years of the appointment through a new NSF funded project.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://spots.augusta.edu/caubert/research/cinrc/phd_ad.html&quot;&gt;https://spots.augusta.edu/caubert/research/cinrc/phd_ad.html&lt;/a&gt;&lt;br /&gt;
Email: clement.aubert@math.cnrs.fr&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </description>
  <pubDate>2023-03-13 18:13:12 UTC</pubDate>
  <author>CCI: jobs</author>
</item>

<item>
  <title>Problems we assume are hard. Are they? ALSO- revised version of Demaine-Gasarch-Hajiaghayi is posted!</title>
  <guid>tag:blogger.com,1999:blog-3722233.post-6482721495747819843</guid>
  <link>https://blog.computationalcomplexity.org/2023/03/problems-we-assume-are-hard-are-they.html</link>
  <description>
    &lt;p&gt;&amp;nbsp;(The latest version of&amp;nbsp;&lt;/p&gt;&lt;p&gt;Computational Intractability: A Guide to Algorithmic Lower Bounds&lt;/p&gt;&lt;p&gt;by Demaine-Gasarch-Hajiaghayi is posted&amp;nbsp;&lt;a href=&quot;https://hardness.mit.edu/&quot;&gt;here&lt;/a&gt;. Its new and improved: (1) we have made all or most of the corrections send to us by proofreaders,&amp;nbsp; (2) there is a chapter on quantum computing, (3) there is an index.&amp;nbsp; Feel free to read it and send us corrections!&amp;nbsp;&lt;/p&gt;&lt;p&gt;This post is related to it in that most of the problems-assumed-hard mentioned in this post were the basis for chapters of the book.)&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;We think SAT is hard because (1) its NPC, and (2) many years of effort have failed to get it into P.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Imagine a world where we didn&#39;t have the Cook-Levin Theorem. We may still think SAT is hard. We may even take it as a hardness assumption to prove other things hard by showing SAT \le A. We may also be curious if they are equivalent and have lots of reductions A \le SAT. The reductions might be Karp or Cook.&amp;nbsp;&lt;/p&gt;&lt;p&gt;You do not have to imagine this world! We already have it- in different contexts. In other areas of complexity theory there are problems that are assumed hard, but for which there is no analog of Cook-Levin. Are they hard? They seem to be- but the evidence is empirical. Not that there&#39;s anything wrong with that.&amp;nbsp;&lt;/p&gt;&lt;p&gt;I will list out problems that&amp;nbsp;&lt;/p&gt;&lt;p&gt;a) we assume are hard, for some definition of&amp;nbsp; &lt;i&gt;we &lt;/i&gt;and &lt;i&gt;hard.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;b) we have NO proof of that and NO completeness or hardness result.&amp;nbsp; ADDED LATER- a commenter wanted me to clarify this.&lt;/p&gt;&lt;p&gt;&amp;nbsp;For SAT there is a well defined set of problems, NP, defined independent of any particular problem (that is, NP was NOT defined as all sets Karp-Red to TSP or anything of the sort) and by Cook-Levin we have&amp;nbsp;&lt;/p&gt;&lt;p&gt;if SAT is in P then NP is contained in P.&lt;/p&gt;&lt;p&gt;For FPT (the class the commenter was interested in) there IS the result&lt;/p&gt;&lt;p&gt;if weighed k-SAT is in FPT then W[1] is contained in FPT.&lt;/p&gt;&lt;p&gt;but W[1] is DEFINED as the set of problems FPT reducible to Weft-1 circuits (some books use a different basic problems) which IN MY OPINION is not a natural class. One may disagree with this of course.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;COUNTERAUGMENT: but W[1] Was defined as all problems FPT-reducible to Weft-1 circuits. And this is not the same&amp;nbsp;&lt;/p&gt;&lt;p&gt;(I do not include hardness assumptions for crypto because that&#39;s not quite the same thing. Also there are just so many of them!)&amp;nbsp;&lt;/p&gt;&lt;p&gt;I am sure I missed some- which is where YOU come in! Please leave comments with additional problems that I forgot (or perhaps did not know) to include.&amp;nbsp;&lt;/p&gt;&lt;p&gt;1) 1vs2 cycle: Given a graph that you are guaranteed is 1 cycle or the union of 2 cycles, determine which is the case. This is assumed to be hard to parallelize (we omit details&amp;nbsp;of defining that formally).&amp;nbsp; This has been used for lower bounds in parallelism. See&amp;nbsp;&lt;a href=&quot;https://doi.org/10.1109/FOCS.2019.00097&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;2) 3SUM: Given x1,..,xn an array of integers, are there 3 that add to 0? There is an O(n^2) algorithm. The hardness assumption is that, for all epsilon, there is no O(n^{2-\epsilon}) algorithm. This assumption has been used to get lower bounds in comp. geom. See the Wikipedia entry&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/3SUM&quot;&gt;here&lt;/a&gt;&amp;nbsp;or the introduction of this paper&amp;nbsp;&lt;a href=&quot;https://arxiv.org/pdf/2203.08356.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;p&gt;4) APSP (All Pairs Shortest Path) Given a graph G, find for each pair of vertices&amp;nbsp; the length of the shortest path. There is an O(n^3) algorithm. The hardness assumption is that, for all epsilon, there is no O(n^{3-epsilon}) algorithm. This assumption has been used to get lower bounds on graph problems. For more details see the introduction of this paper:&amp;nbsp;&lt;a href=&quot;https://arxiv.org/pdf/2203.08356.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;p&gt;5)&amp;nbsp; Weighted-SAT-k: Given a Boolean formula (it can be taken to be in 2CNF form) is there a satisfying assignment that has exactly k of the variables set to TRUE. This is assumed to not be fixed parameter tractable (that is no function f such that this problem is in&amp;nbsp; O(f(k)n^{O(1)}) time). Problems that are FPT-equiv to it are called W[1]-complete and are all thought to not be in FPT. W[2], W[t] are also defined but we omit this. W[1]-complete has also been defined in other ways, but I can&#39;t seem to find a Cook-Levin type theorem for them.&amp;nbsp;&lt;/p&gt;&lt;p&gt;6) Graph Isom.&amp;nbsp; One of he few problems that are in NP but thought to not be NP-complete and, at least for now, is not in P. Babai has shown its in quasi-poly time (n^{(log n)^{O(1)}). There is a notion of GI-hard: problems that, if they are in P then GI is in P. See he Wikipedia entry&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_isomorphism_problem&quot;&gt;here&lt;/a&gt;. Most of the GI-hard problems are variants of GI, e.g., graph isom. for directed graphs. GI could be in P without unbelievable consequences for complexity and without terrifying consequences for cryptography.&amp;nbsp;&lt;/p&gt;&lt;p&gt;7) Unique Games Conj: I won&#39;t define it formally here, see the Wikipedia entry&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Unique_games_conjecture&quot;&gt;here&lt;/a&gt;. From UGC you get several approximation results are optimal. Does that argue for UGC being true-- having consequences we believe? I would say YES since in some cases the algorithm that gives a good approx has an alpha-approx for some weird number alpha, and assuming UGC you get the SAME alpha as a lower bound.&amp;nbsp;&lt;/p&gt;&lt;p&gt;8) Existential&amp;nbsp; Theory of the Reals. Easier for you to read the Wikipedia entry&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Existential_theory_of_the_reals&quot;&gt;here&lt;/a&gt;. It is used for problems that are inbetween NP and PSPACE.&lt;/p&gt;&lt;p&gt;9) Orthogonal vector conjecture. See this paper:&amp;nbsp;&lt;a href=&quot;https://www.mit.edu/~lijieche/SODA_2019_OV-Equiv-Fullversion.pdf&quot;&gt;here&lt;/a&gt;. This is used to show problems are not in subquadratic time.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Possible research directions and thoughts&lt;/p&gt;&lt;p&gt;a) Try to prove a Cook-Levin type theorem for one of these problems.&lt;/p&gt;&lt;p&gt;b) Build classes analogous to the poly-hiearchy on one of these problems.&lt;/p&gt;&lt;p&gt;c) Ask bounded-query questions. For example: Are k queries to 3SUM more powerful than k-1 (This is a VERY Gasarchian question.)&amp;nbsp;&lt;/p&gt;&lt;p&gt;d) Try to prove that one of these problems is actually hard. That seems hard. Perhaps on a weaker model (thats prob already been done for at least one of them.)&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By gasarch&lt;/p&gt;
  </description>
  <pubDate>2023-03-13 15:39:00 UTC</pubDate>
  <author>Computational Complexity</author>
</item>

<item>
  <title>Revisiting the classics: Jensen’s inequality</title>
  <guid>https://francisbach.com/?p=8823</guid>
  <link>https://francisbach.com/jensen-inequality/</link>
  <description>
    &lt;p class=&quot;justify-text&quot;&gt;There are a few mathematical results that any researcher in applied mathematics uses on a daily basis. One of them is Jensen&amp;#8217;s inequality, which allows bounding expectations of functions of random variables. This really happens a lot in any probabilistic arguments but also as a tool to generate inequalities and optimization algorithms. In this blog post, I will present a collection of fun facts about the inequality, from very classical to more obscure. If you know other cool ones, please add them as comments.&lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;But before, &lt;em&gt;&lt;strong&gt;let me be very clear&lt;/strong&gt;&lt;/em&gt;: Jensen&amp;#8217;s inequality is often not in the direction that you would hope it to be. So, to avoid embarrassing mistakes, I always draw at least in my mind the figure below before using it.  &lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-full is-resized&quot;&gt;&lt;img src=&quot;https://francisbach.com/wp-content/uploads/2023/03/jensen-3.png&quot; alt=&quot;&quot; class=&quot;wp-image-8873&quot; width=&quot;546&quot; height=&quot;243&quot; srcset=&quot;https://francisbach.com/wp-content/uploads/2023/03/jensen-3.png 1616w, https://francisbach.com/wp-content/uploads/2023/03/jensen-3-300x134.png 300w, https://francisbach.com/wp-content/uploads/2023/03/jensen-3-1024x456.png 1024w, https://francisbach.com/wp-content/uploads/2023/03/jensen-3-768x342.png 768w, https://francisbach.com/wp-content/uploads/2023/03/jensen-3-1536x684.png 1536w, https://francisbach.com/wp-content/uploads/2023/03/jensen-3-850x379.png 850w&quot; sizes=&quot;(max-width: 546px) 100vw, 546px&quot; /&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;h2&gt;Simplest formulation and proof&lt;/h2&gt;



&lt;p class=&quot;justify-text&quot;&gt;Given a convex function defined on a convex subset \(C\) of \(\mathbb{R}^d\), and a random vector \(X\) with values in \(C\), then $$ f\big( \mathbb{E}[X] \big) \leqslant \mathbb{E} \big[ f(X) \big],$$ as soon as the expectations exist. For a strictly convex function, there is equality if and only if \(X\) is almost surely constant. This is often stated with \(\mu\) taking finitely many values, like in the plot below.&lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Starting with the standard definition of convexity that corresponds to random variables that take only two values in \(C\), this can be extended by recursion to all random variables taking finitely many values, and then by a density argument, to all random variables. See the &lt;a href=&quot;https://en.wikipedia.org/wiki/Jensen%27s_inequality&quot;&gt;Wikipedia&lt;/a&gt; page.&lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;&lt;strong&gt;Proof without words.&lt;/strong&gt; As nicely explained in this &lt;a href=&quot;https://mark.reid.name/blog/behold-jensens-inequality.html&quot;&gt;blog post&lt;/a&gt; by Mark Reid, a simple argument based on epigraphs leads to the inequality for discrete measures supported on \(x_1,\dots,x_n\), with non-negative weights \(\lambda_1, \dots,\lambda_n\) that sum to one, with an illustration below for \(n=4\): any convex combination of points \((x_i,f(x_i))\) has to be in the red convex polygon, which is above the function.&lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-large is-resized&quot;&gt;&lt;img loading=&quot;lazy&quot; src=&quot;https://francisbach.com/wp-content/uploads/2023/03/jensen_multi-3-1024x474.png&quot; alt=&quot;&quot; class=&quot;wp-image-8889&quot; width=&quot;542&quot; height=&quot;250&quot; srcset=&quot;https://francisbach.com/wp-content/uploads/2023/03/jensen_multi-3-1024x474.png 1024w, https://francisbach.com/wp-content/uploads/2023/03/jensen_multi-3-300x139.png 300w, https://francisbach.com/wp-content/uploads/2023/03/jensen_multi-3-768x355.png 768w, https://francisbach.com/wp-content/uploads/2023/03/jensen_multi-3-1536x711.png 1536w, https://francisbach.com/wp-content/uploads/2023/03/jensen_multi-3-850x393.png 850w, https://francisbach.com/wp-content/uploads/2023/03/jensen_multi-3.png 1549w&quot; sizes=&quot;(max-width: 542px) 100vw, 542px&quot; /&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;h2&gt;A bit of history&lt;/h2&gt;



&lt;p class=&quot;justify-text&quot;&gt;The result is typically attributed to the Danish mathematician &lt;a href=&quot;https://fr.wikipedia.org/wiki/Johan_Jensen&quot;&gt;Johan Jensen&lt;/a&gt; [&lt;a href=&quot;https://zenodo.org/record/2371297/files/article.pdf&quot;&gt;1&lt;/a&gt;, in French] who proved in 1906 the result for convex functions on the real line (in fact all continuous &lt;a href=&quot;https://en.wikipedia.org/wiki/Convex_function&quot;&gt;mid-point convex&lt;/a&gt; functions), but &lt;a href=&quot;https://en.wikipedia.org/wiki/Otto_H%C3%B6lder&quot;&gt;Otto Hölder&lt;/a&gt; had shown it earlier for twice differentiable functions [&lt;a href=&quot;http://gdz.sub.uni-goettingen.de/dms/resolveppn/?PPN=GDZPPN00252421X&quot;&gt;2&lt;/a&gt;, in German]. It turns out this was known thirty years earlier for uniform measures on finite sets, as shown by Jules Grolous [&lt;a href=&quot;https://books.google.fr/books?id=z_BFnsBAx18C&amp;amp;hl=fr&amp;amp;pg=PA401#v=onepage&amp;amp;q&amp;amp;f=true&quot;&gt;3&lt;/a&gt;], a relatively unknown former student from Ecole Polytechnique. See also [&lt;a href=&quot;https://www.jstor.org/stable/pdf/43667702.pdf?refreqid=excelsior%3A29d60497a1ddcba74004c27f25c39cd1&amp;amp;ab_segments=&amp;amp;origin=&amp;amp;initiator=&quot;&gt;4&lt;/a&gt;] for more details on the history of Jensen&amp;#8217;s inequality.&lt;/p&gt;



&lt;h2&gt;Classical applications&lt;/h2&gt;



&lt;p class=&quot;justify-text&quot;&gt;Jensen&amp;#8217;s inequality can be used to derive many other classical inequalities, typically applied to the exponential, logarithm or powers.&lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;&lt;strong&gt;Arithmetic, harmonic, and geometric means.&lt;/strong&gt; For \(X\) with positive real values, we have: $$ \mathbb{E}[X]\geqslant \exp \Big( \mathbb{E}\big[ \log(X)\big]\Big)  \ \mbox{ and } \  \mathbb{E}[X]  \geqslant \frac{1}{\mathbb{E}\big[\frac{1}{X}\big]},$$ which corresponds for empirical measures to classical &lt;a href=&quot;https://en.wikipedia.org/wiki/HM-GM-AM-QM_inequalities&quot;&gt;inequalities between means&lt;/a&gt;.&lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;&lt;strong&gt;Young&amp;#8217;s inequality.&lt;/strong&gt; For \(p,q&amp;gt;1\) such that \(\frac{1}{p}+\frac{1}{q}=1\), and two non-negative real numbers \(x,y\), we get by Jensen&amp;#8217;s inequality, $$ \log\big(\frac{1}{p} x^p + \frac{1}{q} y^q \big) \geqslant \frac{1}{p} \log(x^p) + \frac{1}{q} \log(y^q) = \log(xy),$$ leading to &lt;a href=&quot;https://en.wikipedia.org/wiki/Young%27s_inequality_for_products&quot;&gt;Young&amp;#8217;s inequality&lt;/a&gt; \(\displaystyle xy \leqslant \frac{1}{p} x^p + \frac{1}{q} y^q.\)&lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;&lt;strong&gt;Hölder&amp;#8217;s inequality.&lt;/strong&gt; For any positive \(x_1,\dots,x_n,y_1,\dots,y_n\), we can write $$\sum_{i=1}^n x_i y_i = \sum_{j=1}^n y_j^q \cdot \sum_{i=1}^n x_i y_i^{1-q} \frac{y_i^q}{\sum_{j=1}^n y_j^q} \leqslant \sum_{j=1}^n y_j^q \cdot \Big( \sum_{i=1}^n (x_i y_i^{1-q})^p \frac{y_i}{\sum_{j=1}^n y_j^q} \Big)^{1/p},$$ leading to &lt;a href=&quot;https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality&quot;&gt;Hölder&amp;#8217;s inequality&lt;/a&gt; (with the same relationship between \(p\) and \(q\) as above): $$\sum_{i=1}^n x_i y_i \leqslant \Big( \sum_{j=1}^n y_j^q \Big)^{1/q} \Big( \sum_{j=1}^n x_j^p \Big)^{1/p}.$$ This includes also &lt;a href=&quot;https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality&quot;&gt;Cauchy-Schwarz inequality&lt;/a&gt; if \(p=q=2\), and also multiple versions of the &amp;#8220;&lt;a href=&quot;https://francisbach.com/the-%ce%b7-trick-or-the-effectiveness-of-reweighted-least-squares/&quot;&gt;eta-trick&lt;/a&gt;&amp;#8220;.&lt;/p&gt;



&lt;h2&gt;Majorization-minimization&lt;/h2&gt;



&lt;p class=&quot;justify-text&quot;&gt;Within data science, Jensen&amp;#8217;s inequality is often used to derive auxiliary functions used in &lt;a href=&quot;https://en.wikipedia.org/wiki/MM_algorithm&quot;&gt;majorization-minimization&lt;/a&gt; algorithms, with two classical examples below.&lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;&lt;strong&gt;Non-negative matrix factorization (NMF).&lt;/strong&gt; Given a non-negative matrix \(V \in \mathbb{R}_+^{n \times d}\), the goal of NMF is to decompose it as \(V = WH\) with \(W \in \mathbb{R}_+^{n \times m}\) and \(H \in \mathbb{R}_+^{m \times d}\). This has many applications, in particular in source separation [&lt;a href=&quot;https://papers.nips.cc/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf&quot;&gt;5&lt;/a&gt;, &lt;a href=&quot;http://perso.ens-lyon.fr/patrice.abry/ENSEIGNEMENTS/14M2SCExam/Bertin.pdf&quot;&gt;6&lt;/a&gt;].&lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt; A classical cost function which is used to estimate \(W\) and \(H\) is the Kullback-Leibler divergence [&lt;a href=&quot;https://papers.nips.cc/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf&quot;&gt;5&lt;/a&gt;] $$ D(V \| WH) = \sum_{i=1}^n \sum_{j=1}^d \Big\{ V_{ij} \log \frac{ V_{ij} }{ ( WH)_{ij} }\  &amp;#8211; V_{ij} + (WH)_{ij} \Big\}.$$ To minimize the cost function above with respect to \(H\) only, the problematic term is \(\log  ( WH)_{ij} = \log \big( \sum_{k=1}^m W_{ik} H_{kj} \big)\), which is a &amp;#8220;log of a sum&amp;#8221;. To turn it into a &amp;#8220;sum of logs&amp;#8221;, we use Jensen&amp;#8217;s inequality for the logarithm, by introducing a probability vector \(q^{ij} \in \mathbb{R}_+^m\) (with non-negative values that sum to one), and lower-bounding $$ \log  ( WH)_{ij} = \log \Big( \sum_{k=1}^m q^{ij}_k \frac{W_{ik} H_{kj} }{q^{ij}_k} \Big) \geqslant \sum_{k=1}^n q^{ij}_{k} \log \frac{W_{ik} H_{kj}}{q^{ij}_k}.$$ For a fixed \(H\), the bound is tight for \(\displaystyle q^{ij}_k = \frac{W_{ik} H_{kj}}{(WH)_{ij}},\) and given all \(q\)&amp;#8217;s, we can minimize with respect to \(H_{ki}\) in closed form to get the update $$H_{kj} \leftarrow H_{kj}    \frac{\sum_{i=1}^n \! V_{ij} W_{ik}  \, /\,  (WH)_{ij} }{\sum_{i&amp;#8217;=1}^n \! W_{i&amp;#8217;k}}.$$ Because we had a tight upper bound at the current \(H\) (before the update), this is a descent algorithm. We can derive a similar update for \(W\). As shown in [&lt;a href=&quot;https://papers.nips.cc/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf&quot;&gt;5&lt;/a&gt;], this is a simple parameter-free descent algorithm that converges to a stationary point, often referred to as a multiplicative update algorithm. See a convergence analysis in [&lt;a href=&quot;https://perso.telecom-paristech.fr/rbadeau/assets/ieee-tnn-10.pdf&quot;&gt;7&lt;/a&gt;] and alternatives based on relative smoothness [&lt;a href=&quot;https://publications.ut-capitole.fr/id/eprint/25852/1/25852.pdf&quot;&gt;8&lt;/a&gt;] or on primal-dual formulations [&lt;a href=&quot;https://arxiv.org/pdf/1608.01264&quot;&gt;9&lt;/a&gt;, &lt;a href=&quot;https://hal.science/hal-01079229/document&quot;&gt;10&lt;/a&gt;].&lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;&lt;strong&gt;Expectation-maximization (EM).&lt;/strong&gt; The exact same technique of introducing a probability vector within the log and using Jensen&amp;#8217;s inequality is at the core of &lt;a href=&quot;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;EM for latent variable models&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Evidence_lower_bound&quot;&gt;variational inference&lt;/a&gt; (in fact NMF is simply a particular instance for a Poisson likelihood), which are two good topics for future posts (see &lt;a href=&quot;https://lips.cs.princeton.edu/the-elbo-without-jensen-or-kl/&quot;&gt;here&lt;/a&gt; for a simple derivation of the &amp;#8220;&lt;a href=&quot;https://en.wikipedia.org/wiki/Evidence_lower_bound&quot;&gt;evidence lower bound&lt;/a&gt;&amp;#8220;).&lt;/p&gt;



&lt;h2&gt;Information theory&lt;/h2&gt;



&lt;p class=&quot;justify-text&quot;&gt;Within &lt;a href=&quot;https://en.wikipedia.org/wiki/Information_theory&quot;&gt;information theory&lt;/a&gt;, the convexity of the logarithm and the use of Jensen&amp;#8217;s inequality play a major role in most classical results, e.g., positivity of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;&gt;Kullback-Leilbler divergence&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Data_processing_inequality&quot;&gt;data processing inequality&lt;/a&gt;. This also extends to all &lt;a href=&quot;https://en.wikipedia.org/wiki/F-divergence&quot;&gt;f-divergences&lt;/a&gt;.&lt;/p&gt;



&lt;h2&gt;Operator convexity&lt;/h2&gt;



&lt;p class=&quot;justify-text&quot;&gt;When considering convexity with respect to a generalized inequaility (such as based on the Löwner order), we can extend many of the classical formulas above (relationship between means, Young&amp;#8217;s and Hölder&amp;#8217;s inequality) to matrices. See &lt;a href=&quot;https://francisbach.com/matrix-monotony-and-convexity/&quot;&gt;earlier post&lt;/a&gt; for an introduction. For a certain set of functions (such as the square or the negative logarithm) \(f: \mathbb{R} \to \mathbb{R}\), then for a random symmetric matrix \(X\), we have (for the &lt;a href=&quot;https://en.wikipedia.org/wiki/Loewner_order&quot;&gt;Löwner order&lt;/a&gt;): $$ f\big( \mathbb{E}[X] \big) \preccurlyeq  \mathbb{E} \big[ f(X) \big].$$ An intriguing extension is the operator version of Jensen&amp;#8217;s inequality [&lt;a href=&quot;https://arxiv.org/pdf/math/0204049&quot;&gt;10&lt;/a&gt;], for potentially dependent random variables \((X,Y)\), where \(X\) is symmetric, and the sizes of \(X\) and \(Y\) are compatible: $$ f \Big( \mathbb{E} \big[ Y^\top X Y \big] \Big) \preccurlyeq \mathbb{E} \big[ Y^\top f(X) Y \big]  \ \mbox{ as soon as } \ \mathbb{E}[  Y^\top Y ] = I.$$&lt;/p&gt;



&lt;h2&gt;Exact expression of the remainder&lt;/h2&gt;



&lt;p class=&quot;justify-text&quot;&gt;There is a large literature on extensions, refinements on Jensen&amp;#8217;s inequality. I have a cute one of my own, which has probably been derived before. For twice differentiable functions \(f\), we can use &lt;a href=&quot;https://en.wikipedia.org/wiki/Taylor%27s_theorem&quot;&gt;Taylor formula with integral remainder&lt;/a&gt; on the segment between \(X\) and \(\mathbb{E}[X]\), leading to, with \(g(t) =  f\big( t X + (1-t) \mathbb{E}[X]\big)\), $$g(1) = g(0) + g&#39;(0) + \int_0^1 \! g^{\prime \prime}(t)(1-t)dt.$$ Taking expectations, this leads to $$\mathbb{E} \big[f(X)\big] &amp;#8211; f\big( \mathbb{E}[X]\big) = \mathbb{E} \bigg[ \int_0^1 \! ( X &amp;#8211; \mathbb{E}[X])^\top f^{\prime\prime}\big( t X + (1-t) \mathbb{E}[X]\big) ( X &amp;#8211; \mathbb{E}[X]) (1-t) dt \bigg].$$ From this expression, we recover traditional refinements or reversing of the order if \(f^{\prime\prime}\) has bounded eigenvalues. This can for example be used also for characterizing the equality cases in non-strictly convex situations [&lt;a href=&quot;https://arxiv.org/pdf/2202.08545.pdf&quot;&gt;11&lt;/a&gt;, page 31].&lt;/p&gt;



&lt;h2&gt;References&lt;/h2&gt;



&lt;p class=&quot;justify-text&quot;&gt;[1] &lt;a href=&quot;https://en.wikipedia.org/wiki/Johan_Jensen_(mathematician)&quot;&gt;&lt;/a&gt;Johan L. Jensen. &lt;a href=&quot;https://zenodo.org/record/2371297/files/article.pdf&quot;&gt;Sur les fonctions convexes et les inégalités entre les valeurs moyennes.&lt;/a&gt; &lt;em&gt;Acta Mathematica&lt;/em&gt;, 30(1): 175–193, 1906.&lt;br&gt;&lt;span style=&quot;font-size: revert; color: initial; font-family: -apple-system, BlinkMacSystemFont, &amp;quot;Segoe UI&amp;quot;, Roboto, Oxygen-Sans, Ubuntu, Cantarell, &amp;quot;Helvetica Neue&amp;quot;, sans-serif;&quot;&gt;[2] Otto Hölder, &lt;/span&gt;&lt;a style=&quot;font-size: revert; font-family: -apple-system, BlinkMacSystemFont, &amp;quot;Segoe UI&amp;quot;, Roboto, Oxygen-Sans, Ubuntu, Cantarell, &amp;quot;Helvetica Neue&amp;quot;, sans-serif;&quot; href=&quot;http://gdz.sub.uni-goettingen.de/dms/resolveppn/?PPN=GDZPPN00252421X&quot;&gt;Ueber einen Mittelwerthssatz&lt;/a&gt;&lt;span style=&quot;font-size: revert; color: initial; font-family: -apple-system, BlinkMacSystemFont, &amp;quot;Segoe UI&amp;quot;, Roboto, Oxygen-Sans, Ubuntu, Cantarell, &amp;quot;Helvetica Neue&amp;quot;, sans-serif;&quot;&gt;. &lt;/span&gt;Nachrichten von der Königl. Gesellschaft der Wissenschaften und der Georg-Augusts-Universität zu Göttingen&lt;span style=&quot;font-size: revert; color: initial; font-family: -apple-system, BlinkMacSystemFont, &amp;quot;Segoe UI&amp;quot;, Roboto, Oxygen-Sans, Ubuntu, Cantarell, &amp;quot;Helvetica Neue&amp;quot;, sans-serif;&quot;&gt;, (1):38-47&lt;/span&gt;, 1889.&lt;br&gt;&lt;span style=&quot;font-size: revert; color: initial; font-family: -apple-system, BlinkMacSystemFont, &amp;quot;Segoe UI&amp;quot;, Roboto, Oxygen-Sans, Ubuntu, Cantarell, &amp;quot;Helvetica Neue&amp;quot;, sans-serif;&quot;&gt;[3] Jules Grolous. &lt;/span&gt;&lt;a style=&quot;font-size: revert; font-family: -apple-system, BlinkMacSystemFont, &amp;quot;Segoe UI&amp;quot;, Roboto, Oxygen-Sans, Ubuntu, Cantarell, &amp;quot;Helvetica Neue&amp;quot;, sans-serif;&quot; href=&quot;https://books.google.fr/books?id=z_BFnsBAx18C&amp;amp;hl=fr&amp;amp;pg=PA401#v=onepage&amp;amp;q&amp;amp;f=true&quot;&gt;Un théorème sur les fonctions&lt;/a&gt;&lt;em style=&quot;font-size: revert; color: initial; font-family: -apple-system, BlinkMacSystemFont, &amp;quot;Segoe UI&amp;quot;, Roboto, Oxygen-Sans, Ubuntu, Cantarell, &amp;quot;Helvetica Neue&amp;quot;, sans-serif;&quot;&gt;. L’Institut, Journal Universel des Sciences et des Sociétés Savantes en France et à l’Etranger&lt;/em&gt;&lt;span style=&quot;font-size: revert; color: initial; font-family: -apple-system, BlinkMacSystemFont, &amp;quot;Segoe UI&amp;quot;, Roboto, Oxygen-Sans, Ubuntu, Cantarell, &amp;quot;Helvetica Neue&amp;quot;, sans-serif;&quot;&gt;, 3(153):401, 1875.&lt;/span&gt;&lt;br&gt;[4] D. S. Mitrinović and P. M. Vasić. &lt;a href=&quot;https://www.jstor.org/stable/pdf/43667702.pdf?refreqid=excelsior%3A29d60497a1ddcba74004c27f25c39cd1&amp;amp;ab_segments=&amp;amp;origin=&amp;amp;initiator=&quot;&gt;The centroid method in inequalities&lt;/a&gt;. &lt;em&gt;Publikacije Elektrotehničkog fakulteta. Serija Matematika i fizika&lt;/em&gt; 498/541:3-16, 1975.&lt;br&gt;[5] Daniel D. Lee and H. Sebastian Seung. &lt;a href=&quot;https://papers.nips.cc/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf&quot;&gt;Algorithms for non-negative matrix factorization&lt;/a&gt;. &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, 13, 2000.&lt;br&gt;[6] Cédric Févotte, Nancy Bertin, and Jean-Louis Durrieu. &lt;a href=&quot;http://perso.ens-lyon.fr/patrice.abry/ENSEIGNEMENTS/14M2SCExam/Bertin.pdf&quot;&gt;Nonnegative matrix factorization with the Itakura-Saito divergence: With application to music analysis&lt;/a&gt;. &lt;em&gt;Neural computation&lt;/em&gt; 21(3):793-830, 2009.&lt;br&gt;[6] Roland Badeau, Nancy Bertin, and Emmanuel Vincent. &lt;a href=&quot;https://perso.telecom-paristech.fr/rbadeau/assets/ieee-tnn-10.pdf&quot;&gt;Stability analysis of multiplicative update algorithms and application to nonnegative matrix factorization&lt;/a&gt;. &lt;em&gt;IEEE Transactions on Neural Networks&lt;/em&gt;, 21(12):1869-1881, 2010.&lt;br&gt;[7] Heinz H. Bauschke, Jérôme Bolte, and Marc Teboulle. &lt;a href=&quot;https://publications.ut-capitole.fr/id/eprint/25852/1/25852.pdf&quot;&gt;A descent lemma beyond Lipschitz gradient continuity: first-order methods revisited and applications&lt;/a&gt;. &lt;em&gt;Mathematics of Operations Research&lt;/em&gt; 42(2):330-348, 2017.&lt;br&gt;[8] Niao He, Zaid Harchaoui, Yichen Wang, and Le Song. &lt;a href=&quot;https://arxiv.org/pdf/1608.01264&quot;&gt;Fast and simple optimization for Poisson likelihood models&lt;/a&gt;. Technical report,&lt;em&gt; arXiv:1608.01264&lt;/em&gt;, 2016.&lt;br&gt;[9] Felipe Yanez and Francis Bach. &lt;a href=&quot;https://hal.science/hal-01079229/document&quot;&gt;Primal-dual algorithms for non-negative matrix factorization with the Kullback-Leibler divergence&lt;/a&gt;. &lt;em&gt;International Conference on Acoustics, Speech and Signal Processing (ICASSP)&lt;/em&gt;, 2017.&lt;br&gt;[10] Frank Hansen, Gert K. Pedersen. &lt;a href=&quot;https://arxiv.org/pdf/math/0204049&quot;&gt;Jensen&amp;#8217;s Operator Inequality&lt;/a&gt;. Technical report,&lt;em&gt; arXiv:0204049&lt;/em&gt;, 2002.&lt;br&gt;[11] Francis Bach. &lt;a href=&quot;https://arxiv.org/pdf/2202.08545.pdf&quot;&gt;Information theory with kernel methods&lt;/a&gt;. &lt;em&gt;IEEE Transactions in Information Theor&lt;/em&gt;y, 69(2):752-775, 2022.&lt;br&gt;&lt;/p&gt;



&lt;p&gt;&lt;a href=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Francis Bach&lt;/p&gt;
  </description>
  <pubDate>2023-03-13 13:47:14 UTC</pubDate>
  <author>Francis Bach</author>
</item>

<item>
  <title>PhD and Postdoc at Algorithms group in Bar-Ilan University  (Israel) (apply by June 1, 2023)</title>
  <guid>http://cstheory-jobs.org/2023/03/13/phd-and-postdoc-at-algorithms-group-in-bar-ilan-university-israel-apply-by-june-1-2023/</guid>
  <link>https://cstheory-jobs.org/2023/03/13/phd-and-postdoc-at-algorithms-group-in-bar-ilan-university-israel-apply-by-june-1-2023/</link>
  <description>
    &lt;p&gt;I am (Arnold Filtser) looking for both PhD and Postdoc applicants in the broad area of algorithms. We have a strong and thriving algorithms group here in BIU, and you will have many further options for collaboration. My research interest include (but not limited to): Metric Spaces, Low-Distortion Embeddings, Randomized Algorithms, approximation, and streaming algorithms.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://arnold.filtser.com/&quot;&gt;https://arnold.filtser.com/&lt;/a&gt;&lt;br /&gt;
Email: filtsea@cs.biu.ac.il&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </description>
  <pubDate>2023-03-13 10:00:02 UTC</pubDate>
  <author>CCI: jobs</author>
</item>

<item>
  <title>On the Existence of Anomalies, The Reals Case</title>
  <guid>http://arxiv.org/abs/2303.05614</guid>
  <link>http://arxiv.org/abs/2303.05614</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Epstein_S/0/1/0/all/0/1&quot;&gt;Samuel Epstein&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The Independence Postulate (IP) is a finitary Church-Turing Thesis, saying
mathematical sequences are independent from physical ones. Modelling
observations as infinite sequences of real numbers, IP implies the existence of
anomalies.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-13 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>How to Compress the Solution</title>
  <guid>http://arxiv.org/abs/2303.05616</guid>
  <link>http://arxiv.org/abs/2303.05616</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Epstein_S/0/1/0/all/0/1&quot;&gt;Samuel Epstein&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Using derandomization, we provide an upper bound on the compression size of
solutions to the graph coloring problem. In general, if solutions to a
combinatorial problem exist with high probability and the probability is
simple, then there exists a simple solution to the problem. Otherwise the
problem instance has high mutual information with the halting problem.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-13 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Uniform Tests and Algorithmic Thermodynamic Entropy</title>
  <guid>http://arxiv.org/abs/2303.05619</guid>
  <link>http://arxiv.org/abs/2303.05619</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Epstein_S/0/1/0/all/0/1&quot;&gt;Samuel Epstein&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We prove that given a computable metric space and two computable measures,
the set of points that have high universal uniform test scores with respect to
the first measure will have a lower bound with respect to the second measure.
This result is transferred to thermodynamics, showing that algorithmic
thermodynamic entropy must oscillate in the presence of dynamics. Another
application is that outliers will become emergent in computable dynamics of
computable metric spaces.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-13 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>DAG Scheduling in the BSP Model</title>
  <guid>http://arxiv.org/abs/2303.05989</guid>
  <link>http://arxiv.org/abs/2303.05989</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papp_P/0/1/0/all/0/1&quot;&gt;P&amp;#xe1;l Andr&amp;#xe1;s Papp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anegg_G/0/1/0/all/0/1&quot;&gt;Georg Anegg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yzelman_A/0/1/0/all/0/1&quot;&gt;A. N. Yzelman&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the problem of scheduling an arbitrary computational DAG on a fixed
number of processors while minimizing the makespan. While previous works have
mostly studied this problem in relatively restricted models, we define and
analyze DAG scheduling in the Bulk Synchronous Parallel (BSP) model, which is a
well-established parallel computing model that captures the communication cost
between processors much more accurately. We provide a detailed taxonomy of
simpler scheduling models that can be understood as variants or special cases
of BSP, and discuss the properties of the problem and the optimum cost in these
models, and how they differ from BSP. This essentially allows us to dissect the
different building blocks of the BSP model, and gain insight into how each of
these influences the scheduling problem.
&lt;/p&gt;
&lt;p&gt;We then analyze the hardness of DAG scheduling in BSP in detail. We show that
the problem is solvable in polynomial time for some very simple classes of
DAGs, but it is already NP-hard for in-trees or DAGs of height 2. We also
separately study the subproblem of scheduling communication steps, and we show
that the NP-hardness of this problem can depend on the problem parameters and
the communication rules within the BSP model. Finally, we present and analyze a
natural formulation of our scheduling task as an Integer Linear Program.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-13 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Multivariate to Bivariate Reduction for Noncommutative Polynomial Factorization</title>
  <guid>http://arxiv.org/abs/2303.06001</guid>
  <link>http://arxiv.org/abs/2303.06001</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arvind_V/0/1/0/all/0/1&quot;&gt;V. Arvind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joglekar_P/0/1/0/all/0/1&quot;&gt;Pushkar S. Joglekar&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Based on a theorem of Bergman we show that multivariate noncommutative
polynomial factorization is deterministic polynomial-time reducible to the
factorization of bivariate noncommutative polynomials. More precisely, we show
the following:
&lt;/p&gt;
&lt;p&gt;(1) In the white-box setting, given an n-variate noncommutative polynomial f
in F&amp;lt;X&amp;gt; over a field F (either a finite field or the rationals) as an
arithmetic circuit (or algebraic branching program), computing a complete
factorization of f is deterministic polynomial-time reducible to white-box
factorization of a noncommutative bivariate polynomial g in F&amp;lt;x,y&amp;gt;; the
reduction transforms f into a circuit for g (resp. ABP for g), and given a
complete factorization of g the reduction recovers a complete factorization of
f in polynomial time. We also obtain a similar deterministic polynomial-time
reduction in the black-box setting.
&lt;/p&gt;
&lt;p&gt;(2) Additionally, we show over the field of rationals that bivariate linear
matrix factorization of 4 x 4 matrices is at least as hard as factoring
square-free integers. This indicates that reducing noncommutative polynomial
factorization to linear matrix factorization (as done in our recent work
[AJ22]) is unlikely to succeed over the field of rationals even in the
bivariate case.
&lt;/p&gt;
&lt;p&gt;In contrast, multivariate linear matrix factorization for 3 x 3 matrices over
rationals is in polynomial time.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-13 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Scalable and Efficient Functional Map Computations on Dense Meshes</title>
  <guid>http://arxiv.org/abs/2303.05965</guid>
  <link>http://arxiv.org/abs/2303.05965</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magnet_R/0/1/0/all/0/1&quot;&gt;Robin Magnet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1&quot;&gt;Maks Ovsjanikov&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Spectral geometric methods have brought revolutionary changes to the field of
geometry processing. Of particular interest is the study of the Laplacian
spectrum as a compact, isometry and permutation-invariant representation of a
shape. Some recent works show how the intrinsic geometry of a full shape can be
recovered from its spectrum, but there are approaches that consider the more
challenging problem of recovering the geometry from the spectral information of
partial shapes. In this paper, we propose a possible way to fill this gap. We
introduce a learning-based method to estimate the Laplacian spectrum of the
union of partial non-rigid 3D shapes, without actually computing the 3D
geometry of the union or any correspondence between those partial shapes. We do
so by operating purely in the spectral domain and by defining the union
operation between short sequences of eigenvalues. We show that the approximated
union spectrum can be used as-is to reconstruct the complete geometry [MRC*19],
perform region localization on a template [RTO*19] and retrieve shapes from a
database, generalizing ShapeDNA [RWP06] to work with partialities. Working with
eigenvalues allows us to deal with unknown correspondence, different sampling,
and different discretizations (point clouds and meshes alike), making this
operation especially robust and general. Our approach is data-driven and can
generalize to isometric and non-isometric deformations of the surface, as long
as these stay within the same semantic class (e.g., human bodies or horses), as
well as to partiality artifacts not seen at training time.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-13 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Decomposition of zero-dimensional persistence modules via rooted subsets</title>
  <guid>http://arxiv.org/abs/2303.06118</guid>
  <link>http://arxiv.org/abs/2303.06118</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Alonso_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;ngel Javier Alonso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kerber_M/0/1/0/all/0/1&quot;&gt;Michael Kerber&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the decomposition of zero-dimensional persistence modules, viewed as
functors valued in the category of vector spaces factorizing through sets.
Instead of working directly at the level of vector spaces, we take a step back
and first study the decomposition problem at the level of sets.
&lt;/p&gt;
&lt;p&gt;This approach allows us to define the combinatorial notion of rooted subsets.
In the case of a filtered metric space $M$, rooted subsets relate the
clustering behavior of the points of $M$ with the decomposition of the
associated persistence module. In particular, we can identify intervals in such
a decomposition quickly. In addition, rooted subsets can be understood as a
generalization of the elder rule, and are also related to the notion of
constant conqueror of Cai, Kim, M\&#39;emoli and Wang. As an application, we give a
lower bound on the number of intervals that we can expect in the decomposition
of zero-dimensional persistence modules of a density-Rips filtration in
Euclidean space: in the limit, and under very general circumstances, we can
expect that at least 25% of the indecomposable summands are interval modules.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-13 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Optimal-Hash Exact String Matching Algorithms</title>
  <guid>http://arxiv.org/abs/2303.05799</guid>
  <link>http://arxiv.org/abs/2303.05799</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lecroq_T/0/1/0/all/0/1&quot;&gt;Thierry Lecroq&lt;/a&gt;&lt;/p&gt;&lt;p&gt;String matching is the problem of finding all the occurrences of a pattern in
a text. We propose improved versions of the fast family of string matching
algorithms based on hashing $q$-grams. The improvement consists of considering
minimal values $q$ such that each $q$-grams of the pattern has a unique hash
value. The new algorithms are fastest than algorithm of the HASH family for
short patterns on large size alphabets.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-13 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Faster Matroid Partition Algorithms</title>
  <guid>http://arxiv.org/abs/2303.05920</guid>
  <link>http://arxiv.org/abs/2303.05920</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terao_T/0/1/0/all/0/1&quot;&gt;Tatsuya Terao&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In the matroid partitioning problem, we are given $k$ matroids $\mathcal{M}_1
= (V, \mathcal{I}_1), \dots , \mathcal{M}_k = (V, \mathcal{I}_k)$ defined over
a common ground set $V$ of $n$ elements, and we need to find a partitionable
set $S \subseteq V$ of largest possible cardinality, denoted by $p$. Here, a
set $S \subseteq V$ is called partitionable if there exists a partition $(S_1,
\dots , S_k)$ of $S$ with $S_i \in \mathcal{I}_i$ for $i = 1, \ldots, k$. In
1986, Cunningham presented a matroid partition algorithm that uses $O(n p^{3/2}
+ k n)$ independence oracle queries, which was the previously known best
algorithm. This query complexity is $O(n^{5/2})$ when $k \leq n$.
&lt;/p&gt;
&lt;p&gt;Our main result is to present a matroid partition algorithm that uses
$\tilde{O}(k^{1/3} n p + k n)$ independence oracle queries, which is
$\tilde{O}(n^{7/3})$ when $k \leq n$. This improves upon previous Cunningham&#39;s
algorithm. To obtain this, we present a new approach \emph{edge recycling
augmentation}, which can be attained through new ideas: an efficient
utilization of the binary search technique by Nguyen and
Chakrabarty-Lee-Sidford-Singla-Wong and a careful analysis of the number of
independence oracle queries. Our analysis differs significantly from the one
for matroid intersection algorithms, because of the parameter $k$. We also
present a matroid partition algorithm that uses $\tilde{O}((n + k) \sqrt{p})$
rank oracle queries.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-13 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Simple and efficient four-cycle counting on sparse graphs</title>
  <guid>http://arxiv.org/abs/2303.06090</guid>
  <link>http://arxiv.org/abs/2303.06090</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burkhardt_P/0/1/0/all/0/1&quot;&gt;Paul Burkhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harris_D/0/1/0/all/0/1&quot;&gt;David G. Harris&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We consider the problem of counting 4-cycles ($C_4$) in a general undirected
graph $G$ of $n$ vertices and $m$ edges (in bipartite graphs, 4-cycles are also
often referred to as $\textit{butterflies}$). There have been a number of
previous algorithms for this problem; some of these are based on fast matrix
multiplication, which is attractive theoretically but not practical, and some
of these are based on randomized hash tables.
&lt;/p&gt;
&lt;p&gt;We develop a new simpler algorithm for counting $C_4$, which has several
practical improvements over previous algorithms; for example, it is fully
deterministic and avoids any expensive arithmetic in its inner loops. The
algorithm can also be adapted to count 4-cycles incident to each vertex and
edge. Our algorithm runs in $O(m\bar\delta(G))$ time and $O(n)$ space, where
$\bar \delta(G) \leq O(\sqrt{m})$ is the $\textit{average degeneracy}$
parameter introduced by Burkhardt, Faber &amp;amp; Harris (2020).
&lt;/p&gt;
  </description>
  <pubDate>2023-03-13 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Bootstrapping Dynamic Distance Oracles</title>
  <guid>http://arxiv.org/abs/2303.06102</guid>
  <link>http://arxiv.org/abs/2303.06102</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forster_S/0/1/0/all/0/1&quot;&gt;Sebastian Forster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goranci_G/0/1/0/all/0/1&quot;&gt;Gramoz Goranci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nazari_Y/0/1/0/all/0/1&quot;&gt;Yasamin Nazari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skarlatos_A/0/1/0/all/0/1&quot;&gt;Antonis Skarlatos&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Designing approximate all-pairs distance oracles in the fully dynamic setting
is one of the central problems in dynamic graph algorithms. Despite extensive
research on this topic, the first result breaking the $O(\sqrt{n})$ barrier on
the update time for any non-trivial approximation was introduced only recently
by Forster, Goranci and Henzinger [SODA&#39;21] who achieved $m^{1/\rho+o(1)}$
amortized update time with a $O(\log n)^{3\rho-2}$ factor in the approximation
ratio, for any parameter $\rho \geq 1$.
&lt;/p&gt;
&lt;p&gt;In this paper, we give the first constant-stretch fully dynamic distance
oracle with a small polynomial update and query time. Prior work required
either at least a poly-logarithmic approximation or much larger update time.
Our result gives a more fine-grained trade-off between stretch and update time,
for instance we can achieve constant stretch of $O(\frac{1}{\rho^2})^{4/\rho}$
in amortized update time $\tilde{O}(n^{\rho})$, and query time
$\tilde{O}(n^{\rho/8})$ for a constant parameter $\rho &amp;lt;1$. Our algorithm is
randomized and assumes an oblivious adversary.
&lt;/p&gt;
&lt;p&gt;A core technical idea underlying our construction is to design a black-box
reduction from decremental approximate hub-labeling schemes to fully dynamic
distance oracles, which may be of independent interest. We then apply this
reduction repeatedly to an existing decremental algorithm to bootstrap our
fully dynamic solution.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-13 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Any-Order Online Interval Selection</title>
  <guid>http://arxiv.org/abs/2303.06127</guid>
  <link>http://arxiv.org/abs/2303.06127</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borodin_A/0/1/0/all/0/1&quot;&gt;Allan Borodin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karavasilis_C/0/1/0/all/0/1&quot;&gt;Christodoulos Karavasilis&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We consider the problem of online interval scheduling on a single machine,
where intervals arrive online in an order chosen by an adversary, and the
algorithm must output a set of non-conflicting intervals. Traditionally in
scheduling theory, it is assumed that intervals arrive in order of increasing
start times. We drop that assumption and allow for intervals to arrive in any
possible order. We call this variant any-order interval selection (AOIS). We
assume that some online acceptances can be revoked, but a feasible solution
must always be maintained. For unweighted intervals and deterministic
algorithms, this problem is unbounded. Under the assumption that there are at
most $k$ different interval lengths, we give a simple algorithm that achieves a
competitive ratio of $2k$ and show that it is optimal amongst deterministic
algorithms, and a restricted class of randomized algorithms we call memoryless,
contributing to an open question by Adler and Azar 2003; namely whether a
randomized algorithm without access to history can achieve a constant
competitive ratio. We connect our model to the problem of call control on the
line, and show how the algorithms of Garay et al. 1997 can be applied to our
setting, resulting in an optimal algorithm for the case of proportional
weights. We also consider the case of intervals arriving in a random order, and
show that for single-lengthed instances, a one-directional algorithm (i.e.
replacing intervals in one direction), is the only deterministic memoryless
algorithm that can possibly benefit from random arrivals. Finally, we briefly
discuss the case of intervals with arbitrary weights.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-13 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>TR23-022 |  Unprovability of strong complexity lower bounds in bounded arithmetic | 

	Jiatu Li, 

	Igor Carboni Oliveira</title>
  <guid>https://eccc.weizmann.ac.il/report/2023/022</guid>
  <link>https://eccc.weizmann.ac.il/report/2023/022</link>
  <description>
    While there has been progress in establishing the unprovability of complexity statements in lower fragments of bounded arithmetic, understanding the limits of Jerabek&amp;#39;s theory $\textbf{APC}_1$ (2007) and of higher levels of Buss&amp;#39;s hierarchy $\textbf{S}^i_2$ (1986) has been a more elusive task. Even in the more restricted setting of Cook&amp;#39;s theory $\textbf{PV}$ (1975), known results often rely on a less natural formalization that encodes a complexity statement using a collection of sentences instead of a single sentence. This is done to reduce the quantifier complexity of the resulting sentences so that standard witnessing results can be invoked. 

In this work, we establish unprovability results for stronger theories and for sentences of higher quantifier complexity. In particular, we unconditionally show that $\textbf{APC}_1$ cannot prove strong complexity lower bounds separating the third level of the polynomial hierarchy. In more detail, we consider non-uniform average-case separations, and establish that $\textbf{APC}_1$ cannot prove a sentence stating that
* $\forall n \geq n_0\;\exists\,f_n \in \Pi_3\text{-}\textbf{SIZE}[n^d]$ that is $(1/n)$-far from every $\Sigma_3\text{-}\textbf{SIZE}[2^{n^{\delta}}]$ circuit. 
This is a consequence of a much more general result showing that, for every $i \geq 1$, strong separations for $\Pi_{i}\text{-}\textbf{SIZE}[\textrm{poly}(n)]$ versus $\Sigma_{i}\text{-}\textbf{SIZE}[2^{n^{\Omega(1)}}]$ cannot be proved in the theory $\textbf{T}_\textbf{PV}^i$ consisting of all true $\forall \Sigma^b_{i-1}$-sentences in the language of Cook&amp;#39;s theory $\textbf{PV}$. 

Our argument employs a convenient game-theoretic witnessing result that can be applied to sentences of arbitrary quantifier complexity. We combine it with extensions of a technique introduced by Krajicek (2011) that was recently employed by Pich and Santhanam (2021) to establish the unprovability of lower bounds in $\textbf{PV}$ (i.e., the case $i =1$ above, but under a weaker formalization) and in a fragment of $\textbf{APC}_1$.
  </description>
  <pubDate>2023-03-11 23:55:20 UTC</pubDate>
  <author>ECCC Papers</author>
</item>

<item>
  <title>Thursday, Mar 16th, 2023 — Vladimir Braverman from Rice University</title>
  <guid>http://dstheory.wordpress.com/?p=151</guid>
  <link>https://dstheory.wordpress.com/2023/03/10/thursday-mar-16th-2023-vladimir-braverman-from-rice-university/</link>
  <description>
    &lt;p class=&quot;has-text-align-justify&quot;&gt;The next &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://sites.google.com/view/dstheory/home&quot; target=&quot;_blank&quot;&gt;Foundations of Data Science&lt;/a&gt; virtual talk series on recent advances in &lt;em&gt;adversarially robust streaming&lt;/em&gt; will take place on &lt;strong&gt;Thursday, March 16th&lt;/strong&gt; at&lt;strong&gt; 1:00 PM Pacific Time&lt;/strong&gt; (16:00 Eastern Time, 22:00 Central European Time, 21:00 UTC). &lt;a href=&quot;https://profiles.rice.edu/faculty/vladimir-braverman&quot;&gt;Vladimir Braverman&lt;/a&gt; from&lt;strong&gt; Rice University&lt;/strong&gt; will talk about &lt;em&gt;“Adversarial Robustness of Streaming Algorithms through Importance Sampling”&lt;/em&gt;.&lt;/p&gt;



&lt;p&gt;&lt;a href=&quot;https://sites.google.com/view/dstheory&quot;&gt;Details of the talk (Zoom link) are available here.&lt;/a&gt;&lt;/p&gt;



&lt;p class=&quot;has-text-align-justify&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: Robustness against adversarial attacks has recently been at the forefront of algorithmic design for machine learning tasks. In the adversarial streaming model, an adversary gives an algorithm a sequence of adaptively chosen updates u1, &amp;#8230; ,un as a data stream. The goal of the algorithm is to compute or approximate some predetermined function for every prefix of the adversarial stream, but the adversary may generate future updates based on previous outputs of the algorithm. In particular, the adversary may gradually learn the random bits internally used by an algorithm to manipulate dependencies in the input. This is especially problematic as many important problems in the streaming model require randomized algorithms, as they are known to not admit any deterministic algorithms that use sublinear space. In this paper, we introduce adversarially robust streaming algorithms for central machine learning and algorithmic tasks, such as regression and clustering, as well as their more general counterparts, subspace embedding, low-rank approximation, and coreset construction. For regression and other numerical linear algebra related tasks, we consider the row arrival streaming model. Our results are based on a simple, but powerful, observation that many importance sampling-based algorithms give rise to adversarial robustness which is in contrast to sketching based algorithms, which are very prevalent in the streaming literature but suffer from adversarial attacks. In addition, we show that the well-known merge and reduce paradigm in streaming is adversarially robust. Since the merge and reduce paradigm allows coreset constructions in the streaming setting, we thus obtain robust algorithms for k-means, k-median, k-center, Bregman clustering, projective clustering, principal component analysis (PCA) and non-negative matrix factorization. To the best of our knowledge, these are the first adversarially robust results for these problems yet require no new algorithmic implementations. Finally, we empirically confirm the robustness of our algorithms on various adversarial attacks and demonstrate that by contrast, some common existing algorithms are not robust.&lt;/p&gt;



&lt;p&gt;This is a joint work with Avinatan Hassidim, Yossi Matias, Mariano Schain, Sandeep Silwal, Samson Zhou. This result has appeared in NeurIPS 2021.&lt;/p&gt;



&lt;p&gt;&amp;nbsp;The series is supported by the &lt;a href=&quot;https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;amp;HistoricalAwards=false&quot;&gt;NSF HDR TRIPODS Grant 1934846&lt;/a&gt;.&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By dstheory&lt;/p&gt;
  </description>
  <pubDate>2023-03-10 22:17:33 UTC</pubDate>
  <author>Foundation of Data Science - Virtual Talk Series</author>
</item>

<item>
  <title>Subspace Designs, Unit and Distinct Distances, and Piercing Standard Boxes.</title>
  <guid>http://gilkalai.wordpress.com/?p=23968</guid>
  <link>https://gilkalai.wordpress.com/2023/03/10/subspace-designs-unit-and-distinct-distances-and-piercing-standard-boxes/</link>
  <description>
    &lt;p&gt;A lot of things are happening and let me briefly report on three major advancements in combinatorics.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Peter Keevash, Ashwin Sah and Mehtaab Sawhney proved the existence of subspace designs with any given parameters, provided that the dimension of the underlying space is sufficiently large in terms of the other parameters of the design and satisfies the obvious necessary divisibility conditions.  Here is the link to the paper: &lt;a href=&quot;https://arxiv.org/abs/2212.00870&quot;&gt;The existence of subspace designs&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Noga Alon, Matija Bucić, and Lisa Sauermann made an important advance on the study of unit distances and distinct distances in arbitrary normed space. Here is a link to the paper: &lt;a href=&quot;https://arxiv.org/abs/2302.09058&quot;&gt;Unit and distinct distances in typical norms.&lt;/a&gt; The unit distance and distinct distances problems for Euclidean geometry are old and famous, and much attention has been given to the question of what happens to these problems if one considers normed spaces other than the Euclidean plane. Alon, Bucić, and Sauermann give an essentially tight answer to both questions for almost all norms on &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;mathbb R^d&quot; class=&quot;latex&quot; /&gt; , in a certain Baire categoric sense.&lt;/li&gt;
&lt;li&gt;István Tomon&amp;#8217;s paper: &lt;a href=&quot;https://arxiv.org/abs/2209.09887&quot;&gt;Lower bounds for piercing and coloring boxes&lt;/a&gt;. Here is Tomon&amp;#8217;s abstract from his recent Copenhagen-Jerusalem seminar: Configurations of axis-parallel boxes in &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;mathbb R^d&quot; class=&quot;latex&quot; /&gt; are extensively studied in combinatorial and computational geometry. Despite their innocent appearance, there are many old problems involving their structure that are still not well understood. I will talk about a construction, which addresses several of these problems, and shows that configurations of boxes may be more complex than people conjectured.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Subspace designs and q-analogs&lt;/h3&gt;
&lt;p&gt;We talked about subspace designs in &lt;a href=&quot;https://gilkalai.wordpress.com/2016/11/23/amazing-stefan-glock-daniela-kuhn-allan-lo-deryk-osthus-give-a-new-proof-for-keevashs-theorem-and-more-news-on-designs/&quot;&gt;this post&lt;/a&gt;  and in particular about the 2016 paper of Michael  Braun, Tuvi Etzion , Patric R. J. Östergard , Alexander Vardy,  and Alfred Wasserman. While preparing this post I learned the sad news that Alexander Vardy, a prominent coding theorist, had passed away a year ago at the young age of 58.&lt;/p&gt;
&lt;p&gt;The theme of finding &lt;img src=&quot;https://s0.wp.com/latex.php?latex=q&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=q&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;q&quot; class=&quot;latex&quot; /&gt;-analogs to combinatorial results and problems is important both in enumerative and algebraic combinatorics and in extremal combinatorics and it will be interesting to discuss it in some future post. While preparing for that I recalled the famous &lt;img src=&quot;https://s0.wp.com/latex.php?latex=q&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=q&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;q&quot; class=&quot;latex&quot; /&gt;&amp;#8211;&lt;a href=&quot;https://en.wikipedia.org/wiki/Dyson_conjecture&quot;&gt;Dyson conjecture&lt;/a&gt; that had been settled by Zeilberger and Bressoud in 1995. I learned that by now there are simpler proofs:  &amp;#8220;A shorter proof, using formal Laurent series, was given in 2004 by Ira Gessel and Guoce Xin, and an even shorter proof, using a quantitative form, due to Karasev and Petrov, and independently to Lason, of Noga Alon&amp;#8217;s Combinatorial Nullstellensatz, was given in 2012 by Gyula Karolyi and Zoltan Lorant Nagy. The latter method was extended, in 2013, by &lt;a href=&quot;http://www.math.rutgers.edu/~zeilberg/mamarim/mamarimhtml/qdyson.html&quot;&gt;Shalosh B. Ekhad and Doron Zeilberger&lt;/a&gt; to derive explicit expressions of any specific coefficient.&amp;#8221; (Wikipedia.)&lt;/p&gt;
&lt;h3&gt;Unit and distinct distances&lt;/h3&gt;
&lt;p&gt;Here is the abstract of the paper by Alon, Bucić, and Sauermann:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;Erdős&amp;#8217; unit distance problem and Erdős&amp;#8217; distinct distances problem are among the most classical and well-known open problems in all of discrete mathematics. They ask for the maximum number of unit distances, or the minimum number of distinct distances, respectively, determined by $latex &lt;span id=&quot;MathJax-Element-1-Frame&quot; class=&quot;MathJax&quot;&gt;&lt;span id=&quot;MathJax-Span-1&quot; class=&quot;math&quot;&gt;&lt;span id=&quot;MathJax-Span-2&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-3&quot; class=&quot;mi&quot;&gt;n$&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; points in the Euclidean plane. The question of what happens in these problems if one considers normed spaces other than the Euclidean plane has been raised in the 1980s by Ulam and Erdős and attracted a lot of attention over the years. We give an essentially tight answer to both questions for almost all norms on &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;mathbb R^d&quot; class=&quot;latex&quot; /&gt;, in a certain Baire categoric sense.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For the unit distance problem we prove that for almost all norms ||.|| on &lt;span id=&quot;MathJax-Element-3-Frame&quot; class=&quot;MathJax&quot;&gt;&lt;span id=&quot;MathJax-Span-11&quot; class=&quot;math&quot;&gt;&lt;span id=&quot;MathJax-Span-12&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-13&quot; class=&quot;msubsup&quot;&gt;&lt;span id=&quot;MathJax-Span-14&quot; class=&quot;texatom&quot;&gt;&lt;span id=&quot;MathJax-Span-15&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-16&quot; class=&quot;mi&quot;&gt;&lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;mathbb R^d&quot; class=&quot;latex&quot; /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, any set of &lt;img src=&quot;https://s0.wp.com/latex.php?latex=n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;n&quot; class=&quot;latex&quot; /&gt; points defines at most &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D+d+%5Ccdot+n+%5Clog_2+n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D+d+%5Ccdot+n+%5Clog_2+n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D+d+%5Ccdot+n+%5Clog_2+n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;frac{1}{2} d &amp;#92;cdot n &amp;#92;log_2 n&quot; class=&quot;latex&quot; /&gt; unit distances according to ||.||. We also show that this is essentially tight, by proving that for every norm ||.|| on &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;mathbb R^d&quot; class=&quot;latex&quot; /&gt;, for any large &lt;img src=&quot;https://s0.wp.com/latex.php?latex=n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;n&quot; class=&quot;latex&quot; /&gt;, we can find $latex &lt;span id=&quot;MathJax-Element-8-Frame&quot; class=&quot;MathJax&quot;&gt;&lt;span id=&quot;MathJax-Span-44&quot; class=&quot;math&quot;&gt;&lt;span id=&quot;MathJax-Span-45&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-46&quot; class=&quot;mi&quot;&gt;n$&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; points defining at least &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D%28d-1-o%281%29%29%5Ccdot+n+%5Clog_2+n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D%28d-1-o%281%29%29%5Ccdot+n+%5Clog_2+n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D%28d-1-o%281%29%29%5Ccdot+n+%5Clog_2+n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;frac{1}{2}(d-1-o(1))&amp;#92;cdot n &amp;#92;log_2 n&quot; class=&quot;latex&quot; /&gt; unit distances according to ||.||.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For the distinct distances problem, we prove that for almost all norms ||.|| on &lt;img src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;&amp;#92;mathbb R^d&quot; class=&quot;latex&quot; /&gt; any set of &lt;img src=&quot;https://s0.wp.com/latex.php?latex=n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n&amp;#038;bg=ffffff&amp;#038;fg=333333&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;n&quot; class=&quot;latex&quot; /&gt; &lt;span id=&quot;MathJax-Element-11-Frame&quot; class=&quot;MathJax&quot;&gt;&lt;span id=&quot;MathJax-Span-76&quot; class=&quot;math&quot;&gt;&lt;span id=&quot;MathJax-Span-77&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-78&quot; class=&quot;mi&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; points defines at least $latex &lt;span id=&quot;MathJax-Element-12-Frame&quot; class=&quot;MathJax&quot;&gt;&lt;span id=&quot;MathJax-Span-79&quot; class=&quot;math&quot;&gt;&lt;span id=&quot;MathJax-Span-80&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-81&quot; class=&quot;mo&quot;&gt;(&lt;/span&gt;&lt;span id=&quot;MathJax-Span-82&quot; class=&quot;mn&quot;&gt;1&lt;/span&gt;&lt;span id=&quot;MathJax-Span-83&quot; class=&quot;mo&quot;&gt;−&lt;/span&gt;&lt;span id=&quot;MathJax-Span-84&quot; class=&quot;mi&quot;&gt;o&lt;/span&gt;&lt;span id=&quot;MathJax-Span-85&quot; class=&quot;mo&quot;&gt;(&lt;/span&gt;&lt;span id=&quot;MathJax-Span-86&quot; class=&quot;mn&quot;&gt;1&lt;/span&gt;&lt;span id=&quot;MathJax-Span-87&quot; class=&quot;mo&quot;&gt;)&lt;/span&gt;&lt;span id=&quot;MathJax-Span-88&quot; class=&quot;mo&quot;&gt;)&lt;/span&gt;&lt;span id=&quot;MathJax-Span-89&quot; class=&quot;mi&quot;&gt;n$&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; distinct distances according to ||.||. This is clearly tight up to the $latex &lt;span id=&quot;MathJax-Element-13-Frame&quot; class=&quot;MathJax&quot;&gt;&lt;span id=&quot;MathJax-Span-90&quot; class=&quot;math&quot;&gt;&lt;span id=&quot;MathJax-Span-91&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-92&quot; class=&quot;mi&quot;&gt;o&lt;/span&gt;&lt;span id=&quot;MathJax-Span-93&quot; class=&quot;mo&quot;&gt;(&lt;/span&gt;&lt;span id=&quot;MathJax-Span-94&quot; class=&quot;mn&quot;&gt;1&lt;/span&gt;&lt;span id=&quot;MathJax-Span-95&quot; class=&quot;mo&quot;&gt;)$&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; term.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Our results settle, in a strong and somewhat surprising form, problems and conjectures of Brass, of Matoušek, and of Brass-Moser-Pach. The proofs combine combinatorial and geometric ideas with tools from Linear Algebra, Topology and Algebraic Geometry.&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We discussed the unit distances and distinct distances in many posts over here, and they are also related to problems around Borsuk&amp;#8217;s problem (see also my survey paper &lt;a href=&quot;https://arxiv.org/abs/1505.04952&quot;&gt;Some old and new problems in combinatorial geometry I: Around &lt;span class=&quot;search-hit mathjax&quot;&gt;Borsuk&amp;#8217;s&lt;/span&gt; problem&lt;/a&gt;).&lt;/p&gt;
&lt;h3&gt;Standard boxes&lt;/h3&gt;
&lt;p&gt;Here is the abstract of Tomon&amp;#8217;s paper.&lt;br /&gt;
&lt;img data-attachment-id=&quot;23978&quot; data-permalink=&quot;https://gilkalai.wordpress.com/2023/03/10/subspace-designs-unit-and-distinct-distances-and-piercing-standard-boxes/tomon/&quot; data-orig-file=&quot;https://gilkalai.files.wordpress.com/2023/03/tomon.png&quot; data-orig-size=&quot;663,481&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;Tomon&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://gilkalai.files.wordpress.com/2023/03/tomon.png?w=300&quot; data-large-file=&quot;https://gilkalai.files.wordpress.com/2023/03/tomon.png?w=640&quot; class=&quot;alignnone size-full wp-image-23978&quot; src=&quot;https://gilkalai.files.wordpress.com/2023/03/tomon.png?w=640&quot; alt=&quot;Tomon&quot; srcset=&quot;https://gilkalai.files.wordpress.com/2023/03/tomon.png?w=640 640w, https://gilkalai.files.wordpress.com/2023/03/tomon.png?w=150 150w, https://gilkalai.files.wordpress.com/2023/03/tomon.png?w=300 300w, https://gilkalai.files.wordpress.com/2023/03/tomon.png 663w&quot; sizes=&quot;(max-width: 640px) 100vw, 640px&quot;   /&gt;&lt;/p&gt;
&lt;p&gt;We discussed the intersection pattern of (standard) boxes on several occasions such as &lt;a href=&quot;https://gilkalai.wordpress.com/2014/07/03/my-mathematical-dialogue-with-jurgen-eckhoff/&quot;&gt;this post&lt;/a&gt; about Jurgen Eckhoff.&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Gil Kalai&lt;/p&gt;
  </description>
  <pubDate>2023-03-10 11:28:42 UTC</pubDate>
  <author>Gil Kalai</author>
</item>

<item>
  <title>Agnostic PAC Learning of k-juntas Using L2-Polynomial Regression</title>
  <guid>http://arxiv.org/abs/2303.04859</guid>
  <link>http://arxiv.org/abs/2303.04859</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heidari_M/0/1/0/all/0/1&quot;&gt;Mohsen Heidari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szpankowski_W/0/1/0/all/0/1&quot;&gt;Wojciech Szpankowski&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Many conventional learning algorithms rely on loss functions other than the
natural 0-1 loss for computational efficiency and theoretical tractability.
Among them are approaches based on absolute loss (L1 regression) and square
loss (L2 regression). The first is proved to be an \textit{agnostic} PAC
learner for various important concept classes such as \textit{juntas}, and
\textit{half-spaces}. On the other hand, the second is preferable because of
its computational efficiency, which is linear in the sample size. However, PAC
learnability is still unknown as guarantees have been proved only under
distributional restrictions. The question of whether L2 regression is an
agnostic PAC learner for 0-1 loss has been open since 1993 and yet has to be
answered.
&lt;/p&gt;
&lt;p&gt;This paper resolves this problem for the junta class on the Boolean cube --
proving agnostic PAC learning of k-juntas using L2 polynomial regression.
Moreover, we present a new PAC learning algorithm based on the Boolean Fourier
expansion with lower computational complexity. Fourier-based algorithms, such
as Linial et al. (1993), have been used under distributional restrictions, such
as uniform distribution. We show that with an appropriate change, one can apply
those algorithms in agnostic settings without any distributional assumption. We
prove our results by connecting the PAC learning with 0-1 loss to the minimum
mean square estimation (MMSE) problem. We derive an elegant upper bound on the
0-1 loss in terms of the MMSE error and show that the sign of the MMSE is a PAC
learner for any concept class containing it.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-10 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>A New Heuristic for Rectilinear Crossing Minimization</title>
  <guid>http://arxiv.org/abs/2303.05136</guid>
  <link>http://arxiv.org/abs/2303.05136</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dore_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Dor&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Formenti_E/0/1/0/all/0/1&quot;&gt;Enrico Formenti&lt;/a&gt;&lt;/p&gt;&lt;p&gt;A new heuristic for rectilinear crossing minimization is proposed. It is
based on the idea of iteratively repositioning nodes after a first initial
graph drawing. The new position of a node is computed by casting rays from the
node towards graph edges. Each ray receives a mark and the one with the best
mark determines the new position. The heuristic has interesting performances
when compared to the best competitors which can be found in classical graph
drawing libraries like OGDF.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-10 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Range Avoidance for Constant-Depth Circuits: Hardness and Algorithms</title>
  <guid>http://arxiv.org/abs/2303.05044</guid>
  <link>http://arxiv.org/abs/2303.05044</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gajulapalli_K/0/1/0/all/0/1&quot;&gt;Karthik Gajulapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golovnev_A/0/1/0/all/0/1&quot;&gt;Alexander Golovnev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagargoje_S/0/1/0/all/0/1&quot;&gt;Satyajeet Nagargoje&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saraogi_S/0/1/0/all/0/1&quot;&gt;Sidhant Saraogi&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Range Avoidance (AVOID) is a total search problem where, given a Boolean
circuit $C\colon\{0,1\}^n\to\{0,1\}^m$, $m&amp;gt;n$, the task is to find a
$y\in\{0,1\}^m$ outside the range of $C$. For an integer $k\geq 2$,
$NC^0_k$-AVOID is a special case of AVOID where each output bit of $C$ depends
on at most $k$ input bits. Ren, Santhanam, and Wang (FOCS 2022) and Guruswami,
Lyu, and Wang (RANDOM 2022) proved that explicit constructions of functions of
high circuit complexity, rigid matrices, optimal linear codes, Ramsey graphs,
and other combinatorial objects reduce to $NC^0_4$-AVOID, thus establishing
conditional hardness of the $NC^0_4$-AVOID problem. On the other hand,
$NC^0_2$-AVOID admits polynomial-time algorithms, leaving the question about
the complexity of $NC^0_3$-AVOID open. We give the first reduction of an
explicit construction question to $NC^0_3$-AVOID. Specifically, we prove that a
polynomial-time algorithm (with an $NP$ oracle) for $NC^0_3$-AVOID for the case
of $m=n+n^{2/3}$ would imply an explicit construction of a rigid matrix, and,
thus, a super-linear lower bound on the size of log-depth circuits. We also
give deterministic polynomial-time algorithms for all $NC^0_k$-AVOID problems
for ${m\geq n^{k-1}/\log(n)}$. Prior work required an $NP$ oracle, and required
larger stretch, $m \geq n^{k-1}$.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-10 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Smoothed Analysis of Sequential Probability Assignment</title>
  <guid>http://arxiv.org/abs/2303.04845</guid>
  <link>http://arxiv.org/abs/2303.04845</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_A/0/1/0/all/0/1&quot;&gt;Alankrita Bhatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haghtalab_N/0/1/0/all/0/1&quot;&gt;Nika Haghtalab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shetty_A/0/1/0/all/0/1&quot;&gt;Abhishek Shetty&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We initiate the study of smoothed analysis for the sequential probability
assignment problem with contexts. We study information-theoretically optimal
minmax rates as well as a framework for algorithmic reduction involving the
maximum likelihood estimator oracle. Our approach establishes a general-purpose
reduction from minimax rates for sequential probability assignment for smoothed
adversaries to minimax rates for transductive learning. This leads to optimal
(logarithmic) fast rates for parametric classes and classes with finite VC
dimension. On the algorithmic front, we develop an algorithm that efficiently
taps into the MLE oracle, for general classes of functions. We show that under
general conditions this algorithmic approach yields sublinear regret.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-10 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Parallel Strong Connectivity Based on Faster Reachability</title>
  <guid>http://arxiv.org/abs/2303.04934</guid>
  <link>http://arxiv.org/abs/2303.04934</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Letong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xiaojun Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yan Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yihan Sun&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Computing strongly connected components (SCC) is a fundamental problems in
graph processing. As today&#39;s real-world graphs are getting larger and larger,
parallel SCC is increasingly important. SCC is challenging in the parallel
setting and is particularly hard on large-diameter graphs. Many existing
parallel SCC implementations can be even slower than Tarjan&#39;s sequential
algorithm on large-diameter graphs.
&lt;/p&gt;
&lt;p&gt;To tackle this challenge, we propose an efficient parallel SCC implementation
using a new parallel reachability algorithm. Our solution is based on a novel
idea referred to as vertical granularity control (VGC). It breaks the
synchronization barriers to increase parallelism and hide scheduling overhead.
To use VGC in our SCC algorithm, we also design an efficient data structure
called the \emph{parallel hash bag}. It uses parallel dynamic resizing to avoid
redundant work in maintaining frontiers (vertices processed in a round).
&lt;/p&gt;
&lt;p&gt;We implement the parallel SCC algorithm by Blelloch et al.\ (J.\ ACM, 2020)
using our new parallel reachability algorithm. We compare our implementation to
the state-of-the-art systems, including GBBS, iSpan, Multi-step, and our highly
optimized Tarjan&#39;s (sequential) algorithm, on 18 graphs, including social, web,
$k$-NN, and lattice graphs. On a machine with 96 cores, our implementation is
the fastest on 16 out of 18 graphs. On average (geometric means) over all
graphs, our SCC is 6.0$\times$ faster than the best previous parallel code
(GBBS), 12.8$\times$ faster than Tarjan&#39;s sequential algorithms, and
2.7$\times$ faster than the \emph{best existing implementation on each graph}.
&lt;/p&gt;
&lt;p&gt;We believe that our techniques are of independent interest. We also apply our
parallel hash bag and VGC scheme to other graph problems, including
connectivity and least-element lists (LE-lists).
&lt;/p&gt;
  </description>
  <pubDate>2023-03-10 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>A Survey of Quantum Alternatives to Randomized Algorithms: Monte Carlo Integration and Beyond</title>
  <guid>http://arxiv.org/abs/2303.04945</guid>
  <link>http://arxiv.org/abs/2303.04945</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Intallura_P/0/1/0/all/0/1&quot;&gt;Philip Intallura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Korpas_G/0/1/0/all/0/1&quot;&gt;Georgios Korpas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Chakraborty_S/0/1/0/all/0/1&quot;&gt;Sudeepto Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kungurtsev_V/0/1/0/all/0/1&quot;&gt;Vyacheslav Kungurtsev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Marecek_J/0/1/0/all/0/1&quot;&gt;Jakub Marecek&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Monte Carlo sampling is a powerful toolbox of algorithmic techniques widely
used for a number of applications wherein some noisy quantity, or summary
statistic thereof, is sought to be estimated. In this paper, we survey the
literature for implementing Monte Carlo procedures using quantum circuits,
focusing on the potential to obtain a quantum advantage in the computational
speed of these procedures. We revisit the quantum algorithms that could replace
classical Monte Carlo and then consider both the existing quantum algorithms
and the potential quantum realizations that include adaptive enhancements as
alternatives to the classical procedure.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-10 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Spatio-Temporal Trajectory Similarity Measures: A Comprehensive Survey and Quantitative Study</title>
  <guid>http://arxiv.org/abs/2303.05012</guid>
  <link>http://arxiv.org/abs/2303.05012</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Danlei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1&quot;&gt;Hanxi Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Ziquan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yunjun Gao&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Spatio-temporal trajectory analytics is at the core of smart mobility
solutions, which offers unprecedented information for diversified applications
such as urban planning, infrastructure development, and vehicular networks.
Trajectory similarity measure, which aims to evaluate the distance between two
trajectories, is a fundamental functionality of trajectory analytics. In this
paper, we propose a comprehensive survey that investigates all the most common
and representative spatio-temporal trajectory measures. First, we provide an
overview of spatio-temporal trajectory measures in terms of three hierarchical
perspectives: Non-learning vs. Learning, Free Space vs. Road Network, and
Standalone vs. Distributed. Next, we present an evaluation benchmark by
designing five real-world transformation scenarios. Based on this benchmark,
extensive experiments are conducted to study the effectiveness,
robustness,nefficiency, and scalability of each measure, which offers
guidelines for trajectory measure selection among multiple techniques and
applications such as trajectory data mining, deep learning, and distributed
processing.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-10 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Robust optimization with belief functions</title>
  <guid>http://arxiv.org/abs/2303.05067</guid>
  <link>http://arxiv.org/abs/2303.05067</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goerigk_M/0/1/0/all/0/1&quot;&gt;Marc Goerigk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillaume_R/0/1/0/all/0/1&quot;&gt;Romain Guillaume&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasperski_A/0/1/0/all/0/1&quot;&gt;Adam Kasperski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zielinski_P/0/1/0/all/0/1&quot;&gt;Pawe&amp;#x142; Zieli&amp;#x144;ski&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this paper, an optimization problem with uncertain objective function
coefficients is considered. The uncertainty is specified by providing a
discrete scenario set, containing possible realizations of the objective
function coefficients. The concept of belief function in the traditional and
possibilistic setting is applied to define a set of admissible probability
distributions over the scenario set. The generalized Hurwicz criterion is then
used to compute a solution. In this paper, the complexity of the resulting
problem is explored. Some exact and approximation methods of solving it are
proposed.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-10 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Distributed Half-Integral Matching and Beyond</title>
  <guid>http://arxiv.org/abs/2303.05250</guid>
  <link>http://arxiv.org/abs/2303.05250</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahal_S/0/1/0/all/0/1&quot;&gt;Sameep Dahal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suomela_J/0/1/0/all/0/1&quot;&gt;Jukka Suomela&lt;/a&gt;&lt;/p&gt;&lt;p&gt;By prior work, it is known that any distributed graph algorithm that finds a
maximal matching requires $\Omega(\log^* n)$ communication rounds, while it is
possible to find a maximal fractional matching in $O(1)$ rounds in
bounded-degree graphs. However, all prior $O(1)$-round algorithms for maximal
fractional matching use arbitrarily fine-grained fractional values. In
particular, none of them is able to find a half-integral solution, using only
values from $\{0, \frac12, 1\}$. We show that the use of fine-grained
fractional values is necessary, and moreover we give a complete
characterization on exactly how small values are needed: if we consider maximal
fractional matching in graphs of maximum degree $\Delta = 2d$, and any
distributed graph algorithm with round complexity $T(\Delta)$ that only depends
on $\Delta$ and is independent of $n$, we show that the algorithm has to use
fractional values with a denominator at least $2^d$. We give a new algorithm
that shows that this is also sufficient.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-10 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Direct Access for Answers to Conjunctive Queries with Aggregation</title>
  <guid>http://arxiv.org/abs/2303.05327</guid>
  <link>http://arxiv.org/abs/2303.05327</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eldar_I/0/1/0/all/0/1&quot;&gt;Idan Eldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carmeli_N/0/1/0/all/0/1&quot;&gt;Nofar Carmeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kimelfeld_B/0/1/0/all/0/1&quot;&gt;Benny Kimelfeld&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the fine-grained complexity of conjunctive queries with grouping and
aggregation. For some common aggregate functions (e.g., min, max, count, sum),
such a query can be phrased as an ordinary conjunctive query over a database
annotated with a suitable commutative semiring. Specifically, we investigate
the ability to evaluate such queries by constructing in log-linear time a data
structure that provides logarithmic-time direct access to the answers ordered
by a given lexicographic order. This task is nontrivial since the number of
answers might be larger than log-linear in the size of the input, and so, the
data structure needs to provide a compact representation of the space of
answers.
&lt;/p&gt;
&lt;p&gt;In the absence of aggregation and annotation, past research provides a
sufficient tractability condition on queries and orders. For queries without
self-joins, this condition is not just sufficient, but also necessary (under
conventional lower-bound assumptions in fine-grained complexity). We show that
all past results continue to hold for annotated databases, assuming that the
annotation itself is not part of the lexicographic order. On the other hand, we
show infeasibility for the case of count-distinct that does not have any
efficient representation as a commutative semiring. We then investigate the
ability to include the aggregate and annotation outcome in the lexicographic
order. Among the hardness results, standing out as tractable is the case of a
semiring with an idempotent addition, such as those of min and max. Notably,
this case captures also count-distinct over a logarithmic-size domain.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-10 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Elastic Founder Graphs Improved and Enhanced</title>
  <guid>http://arxiv.org/abs/2303.05336</guid>
  <link>http://arxiv.org/abs/2303.05336</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizzo_N/0/1/0/all/0/1&quot;&gt;Nicola Rizzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Equi_M/0/1/0/all/0/1&quot;&gt;Massimo Equi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norri_T/0/1/0/all/0/1&quot;&gt;Tuukka Norri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makinen_V/0/1/0/all/0/1&quot;&gt;Veli M&amp;#xe4;kinen&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Indexing labeled graphs for pattern matching is a central challenge of
pangenomics. Equi et al. (Algorithmica, 2022) developed the Elastic Founder
Graph ($\mathsf{EFG}$) representing an alignment of $m$ sequences of length
$n$, drawn from alphabet $\Sigma$ plus the special gap character: the paths
spell the original sequences or their recombination. By enforcing the
semi-repeat-free property, the $\mathsf{EFG}$ admits a polynomial-space index
for linear-time pattern matching, breaking through the conditional lower bounds
on indexing labeled graphs (Equi et al., SOFSEM 2021). In this work we improve
the space of the $\mathsf{EFG}$ index answering pattern matching queries in
linear time, from linear in the length of all strings spelled by three
consecutive node labels, to linear in the size of the edge labels. Then, we
develop linear-time construction algorithms optimizing for different metrics:
we improve the existing linearithmic construction algorithms to $O(mn)$, by
solving the novel exclusive ancestor set problem on trees; we propose, for the
simplified gapless setting, an $O(mn)$-time solution minimizing the maximum
block height, that we generalize by substituting block height with prefix-aware
height. Finally, to show the versatility of the framework, we develop a
BWT-based $\mathsf{EFG}$ index and study how to encode and perform document
listing queries on a set of paths of the graphs, reporting which paths present
a given pattern as a substring. We propose the $\mathsf{EFG}$ framework as an
improved and enhanced version of the framework for the gapless setting, along
with construction methods that are valid in any setting concerned with the
segmentation of aligned sequences.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-10 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Fast algorithms for Vizing&#39;s theorem on bounded degree graphs</title>
  <guid>http://arxiv.org/abs/2303.05408</guid>
  <link>http://arxiv.org/abs/2303.05408</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernshteyn_A/0/1/0/all/0/1&quot;&gt;Anton Bernshteyn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhawan_A/0/1/0/all/0/1&quot;&gt;Abhishek Dhawan&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Vizing&#39;s theorem states that every graph $G$ of maximum degree $\Delta$ can
be properly edge-colored using $\Delta + 1$ colors. The fastest currently known
$(\Delta+1)$-edge-coloring algorithm for general graphs is due to Sinnamon and
runs in time $O(m\sqrt{n})$, where $n = |V(G)|$ and $m =|E(G)|$. Using the
bound $m \leq \Delta n/2$, the running time of Sinnamon&#39;s algorithm can be
expressed as $O(\Delta n^{3/2})$. In the regime when $\Delta$ is considerably
smaller than $n$ (for instance, when $\Delta$ is a constant), this can be
improved, as Gabow, Nishizeki, Kariv, Leven, and Terada designed an algorithm
with running time $O(\Delta m \log n) = O(\Delta^2 n \log n)$. Here we give an
algorithm whose running time is only linear in $n$ (which is obviously best
possible) and polynomial in $\Delta$. We also develop new algorithms for
$(\Delta+1)$-edge-coloring in the $\mathsf{LOCAL}$ model of distributed
computation. Namely, we design a deterministic $\mathsf{LOCAL}$ algorithm with
running time $\mathsf{poly}(\Delta, \log\log n) \log^5 n$ and a randomized
$\mathsf{LOCAL}$ algorithm with running time $\mathsf{poly}(\Delta) \log^2 n$.
The key new ingredient in our algorithms is a novel application of the entropy
compression method.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-10 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Ten fully-funded PhD positions in Computer Science at the Gran Sasso Science Institute</title>
  <guid>tag:blogger.com,1999:blog-27705661.post-3589839334453146314</guid>
  <link>http://processalgebra.blogspot.com/2023/03/ten-fully-funded-phd-positions-in.html</link>
  <description>
    &lt;p&gt;The Computer Science group at the GSSI has ten fully-funded PhD positions. See the &lt;a href=&quot;https://www.gssi.it/albo-ufficiale-online-gssi/item/download/4164_c550280ade939db61570a29ef700f63e&quot; target=&quot;_blank&quot;&gt;call for applications&lt;/a&gt; for details. The deadline for applications is 30 May 2023. &lt;br /&gt;&lt;/p&gt;&lt;p&gt;The &lt;a href=&quot;https://sites.google.com/gssi.it/csgssi&quot; target=&quot;_blank&quot;&gt;Computer Science group at the GSSI&lt;/a&gt; provides an excellent environment for PhD students and its group has been ranked as &lt;a href=&quot;https://processalgebra.blogspot.com/2022/12/computer-science-and-mathematics-at.html&quot; target=&quot;_blank&quot;&gt;&quot;excellent&quot;&lt;/a&gt; by a recent national research assessment exercise. In my, admittedly biased, opinion, it is one of the places to be for research in Computer Science in Italy. &lt;/p&gt;&lt;p&gt;Spread the news! &lt;br /&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;br /&gt;&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By Luca Aceto&lt;/p&gt;
  </description>
  <pubDate>2023-03-09 13:49:00 UTC</pubDate>
  <author>Luca Aceto</author>
</item>

<item>
  <title>The False Promise of Chomskyism</title>
  <guid>https://scottaaronson.blog/?p=7094</guid>
  <link>https://scottaaronson.blog/?p=7094</link>
  <description>
    &lt;p&gt;&lt;strong&gt;&lt;mark style=&quot;background-color:rgba(0, 0, 0, 0)&quot; class=&quot;has-inline-color has-vivid-red-color&quot;&gt;Important Update (March 10):&lt;/mark&gt;&lt;/strong&gt; On deeper reflection, I probably don&amp;#8217;t &lt;em&gt;need&lt;/em&gt; to spend emotional energy refuting people like Chomsky, who believe that Large Language Models are just a laughable fad rather than a step-change in how humans can and will use technology, any more than I would&amp;#8217;ve needed to spend it refuting those who said the same about the World Wide Web in 1993.  Yes, they&amp;#8217;re wrong, and yes, despite being wrong they&amp;#8217;re self-certain, hostile, and smug, and yes I can see this, and yes it angers me.  But the world is going to make the argument for me.  And if not the world, &lt;a href=&quot;https://twitter.com/SebastienBubeck/status/1634009568341622784/photo/1&quot;&gt;Bing already does a perfectly serviceable job&lt;/a&gt; at refuting Chomsky&amp;#8217;s points (h/t Sebastien Bubeck via Boaz Barak).&lt;/p&gt;



&lt;p&gt;Meanwhile, out there in reality, &lt;a href=&quot;https://southpark.cc.com/episodes/8byci4/south-park-deep-learning-season-26-ep-4?fbclid=IwAR0hxxqhxF3jsMu7EZvGqJYmFruseZTNL_7W_8i_Y7dfhDYhmMoc9KOYWco&quot;&gt;last night&amp;#8217;s &lt;em&gt;South Park&lt;/em&gt; episode&lt;/a&gt; does a &lt;em&gt;much&lt;/em&gt; better job than most academic thinkpieces at exploring how ordinary people are going to respond (and have already responded) to the availability of ChatGPT.  It will &lt;em&gt;not&lt;/em&gt;, to put it mildly, be with sneering Chomskyan disdain, whether the effects on the world are for good or ill or (most likely) both.  Among other things&amp;#8212;I don&amp;#8217;t want to give away too much!&amp;#8212;this episode prominently features a soothsayer accompanied by a bird that caws whenever it detects GPT-generated text.  Now why didn&amp;#8217;t I think of that in preference to cryptographic watermarking??&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;&lt;mark style=&quot;background-color:rgba(0, 0, 0, 0)&quot; class=&quot;has-inline-color has-vivid-red-color&quot;&gt;Another Update (March 11)&lt;/mark&gt;&lt;/strong&gt;: To my astonishment and delight, even many of the &lt;em&gt;anti&lt;/em&gt;-LLM AI experts are refusing to defend Chomsky’s attack-piece.  That’s the one important point about which I stand corrected!&lt;/p&gt;



&lt;p&gt;&lt;mark style=&quot;background-color:rgba(0, 0, 0, 0)&quot; class=&quot;has-inline-color has-vivid-red-color&quot;&gt;&lt;strong&gt;Another Update (March 12):&lt;/strong&gt;&lt;/mark&gt; “As a Professor of Linguistics myself, I find it a little sad that someone who while young was a profound innovator in linguistics and more is now conservatively trying to block exciting new approaches.“ —&lt;a href=&quot;https://mobile.twitter.com/chrmanning/status/1633873660221218816&quot;&gt;Christopher Manning&lt;/a&gt;&lt;/p&gt;



&lt;hr class=&quot;wp-block-separator has-alpha-channel-opacity&quot;/&gt;



&lt;p&gt;I was asked to respond to the &lt;em&gt;New York Times&lt;/em&gt; opinion piece entitled &lt;a href=&quot;https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html&quot;&gt;The False Promise of ChatGPT&lt;/a&gt;, by Noam Chomsky along with Ian Roberts and Jeffrey Watumull (who once took my class at MIT).  I&amp;#8217;ll be busy all day at the Harvard CS department, where I&amp;#8217;m &lt;a href=&quot;https://events.seas.harvard.edu/event/how_much_information_is_in_a_quantum_state&quot;&gt;giving a quantum talk&lt;/a&gt; this afternoon. &lt;strong&gt;[Added: Several commenters complained that they found this sentence &amp;#8220;condescending,&amp;#8221; but I&amp;#8217;m not sure what exactly they wanted me to say&amp;#8212;that I was visiting some school in Cambridge, MA, two T stops from the school where Chomsky works and I used to work?]&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;But for now:&lt;/p&gt;



&lt;p&gt;In this piece Chomsky, the intellectual godfather god of an effort that failed for 60 years to build machines that can converse in ordinary language, condemns the effort that succeeded.  &lt;strong&gt;[Added: Please, &lt;em&gt;please&lt;/em&gt; stop writing that I must be an ignoramus since I don&amp;#8217;t even know that Chomsky has never worked on AI.  I know perfectly well that he hasn&amp;#8217;t, and meant only that he tends to be regarded as authoritative &lt;em&gt;by&lt;/em&gt; the &amp;#8220;don&amp;#8217;t-look-through-the-telescope&amp;#8221; AI faction, the ones views he himself fully endorses in his attack-piece.  If you don&amp;#8217;t know the relevant history, &lt;a href=&quot;https://norvig.com/chomsky.html&quot;&gt;read Norvig&lt;/a&gt;.]&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Chomsky condemns ChatGPT for four reasons:&lt;/p&gt;



&lt;ol&gt;
&lt;li&gt;because it could, in principle, misinterpret sentences that could also be sentence fragments, like &amp;#8220;John is too stubborn to talk to&amp;#8221; (bizarrely, he never checks whether it &lt;em&gt;does&lt;/em&gt; misinterpret it&amp;#8212;I just tried it this morning and it seems to decide correctly based on context whether it&amp;#8217;s a sentence or a sentence fragment, much like I would!);&lt;/li&gt;



&lt;li&gt;because it doesn’t learn the way humans do (personally, I think ChatGPT and other large language models have &lt;em&gt;massively&lt;/em&gt; illuminated at least one component of the human language faculty, what you could call its &lt;a href=&quot;https://en.wikipedia.org/wiki/Predictive_coding&quot;&gt;predictive coding&lt;/a&gt; component, though clearly not all of it);&lt;/li&gt;



&lt;li&gt;because it could learn false facts or grammatical systems if fed false training data (how could it be otherwise?); and&lt;/li&gt;



&lt;li&gt;most of all because it’s “amoral,” refusing to take a stand on potentially controversial issues (he gives an example involving the ethics of terraforming Mars).&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;This last, of course, is a &lt;em&gt;choice&lt;/em&gt;, imposed by OpenAI using reinforcement learning.  The reason for it is simply that ChatGPT is a consumer product.  The same people who condemn it for not taking controversial stands would condemn it much more loudly if it did — just like the same people who condemn it for wrong answers and explanations, would condemn it equally for right ones (Chomsky promises as much in the essay).&lt;/p&gt;



&lt;p&gt;I submit that, like the Jesuit astronomers declining to look through Galileo’s telescope, what Chomsky and his followers are ultimately angry at is reality itself, for having the temerity to offer something up that they didn’t predict and that doesn’t fit their worldview.&lt;/p&gt;



&lt;p&gt;[&lt;em&gt;Note for people who might be visiting this blog for the first time:&lt;/em&gt; I&amp;#8217;m a CS professor at UT Austin, on leave for one year to work at OpenAI on the theoretical foundations of AI safety.  I accepted OpenAI&amp;#8217;s offer in part because I already held the views here, or something close to them; and given that I could see how large language models were poised to change the world for good and ill, I wanted to be part of the effort to help prevent their misuse.  No one at OpenAI asked me to write this or saw it beforehand, and I don&amp;#8217;t even know to what extent they agree with it.]&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Scott&lt;/p&gt;
  </description>
  <pubDate>2023-03-09 12:01:55 UTC</pubDate>
  <author>Scott Aaronson</author>
</item>

<item>
  <title>TR23-021 |  Range Avoidance for Constant-Depth Circuits: Hardness and Algorithms | 

	Sidhant Saraogi, 

	Alexander Golovnev, 

	Satyajeet Nagargoje, 

	Karthik Gajulapalli</title>
  <guid>https://eccc.weizmann.ac.il/report/2023/021</guid>
  <link>https://eccc.weizmann.ac.il/report/2023/021</link>
  <description>
    Range Avoidance (AVOID) is a total search problem where, given a Boolean circuit $C\colon\{0,1\}^n\to\{0,1\}^m$, $m&amp;gt;n$, the task is to find a $y\in\{0,1\}^m$ outside the range of $C$. For an integer $k\geq 2$, $NC^0_k$-AVOID is a special case of AVOID where each output bit of $C$ depends on at most $k$ input bits. Ren, Santhanam, and Wang (FOCS 2022) and Guruswami, Lyu, and Wang (RANDOM 2022) proved that explicit constructions of functions of high circuit complexity, rigid matrices, optimal linear codes, Ramsey graphs, and other combinatorial objects reduce to $NC^0_4$-AVOID, thus establishing conditional hardness of the $NC^0_4$-AVOID problem. On the other hand, $NC^0_2$-AVOID admits polynomial-time algorithms, leaving the question about the complexity of $NC^0_3$-AVOID open.

We give the first reduction of an explicit construction question to $NC^0_3$-AVOID. Specifically, we prove that a polynomial-time algorithm (with an $NP$ oracle) for $NC^0_3$-AVOID for the case of $m=n+n^{2/3}$ would imply an explicit construction of a rigid matrix, and, thus, a super-linear lower bound on the size of log-depth circuits.

We also give deterministic polynomial-time algorithms for all $NC^0_k$-AVOID problems for ${m\geq n^{k-1}/\log(n)}$. Prior work required an $NP$ oracle, and required larger stretch, $m \geq n^{k-1}$.
  </description>
  <pubDate>2023-03-09 09:26:46 UTC</pubDate>
  <author>ECCC Papers</author>
</item>

<item>
  <title>Classical vs Quantum Advice under Classically-Accessible Oracle</title>
  <guid>http://arxiv.org/abs/2303.04298</guid>
  <link>http://arxiv.org/abs/2303.04298</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xingjian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qipeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Pelecanos_A/0/1/0/all/0/1&quot;&gt;Angelos Pelecanos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Yamakawa_T/0/1/0/all/0/1&quot;&gt;Takashi Yamakawa&lt;/a&gt;&lt;/p&gt;&lt;p&gt;It is a long-standing open question to construct a classical oracle relative
to which BQP/qpoly $\neq$ BQP/poly or QMA $\neq$ QCMA. In this paper, we
construct classically-accessible classical oracles relative to which BQP/qpoly
$\neq$ BQP/poly. Here, classically-accessible classical oracles are oracles
that can be accessed only classically even for quantum algorithms. Based on a
similar technique, we also show an alternative proof for separation of QMA and
QCMA relative to a distributional quantumly-accessible classical oracles, which
was recently shown by Natarajan and Nirkhe.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-09 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>The Descriptive Complexity of Graph Neural Networks</title>
  <guid>http://arxiv.org/abs/2303.04613</guid>
  <link>http://arxiv.org/abs/2303.04613</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grohe_M/0/1/0/all/0/1&quot;&gt;Martin Grohe&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We analyse the power of graph neural networks (GNNs) in terms of Boolean
circuit complexity and descriptive complexity.
&lt;/p&gt;
&lt;p&gt;We prove that the graph queries that can be computed by a polynomial-size
bounded-depth family of GNNs are exactly those definable in the guarded
fragment GFO+C of first-order logic with counting and with built-in relations.
This puts GNNs in the circuit complexity class TC^0. Remarkably, the GNN
families may use arbitrary real weights and a wide class of activation
functions that includes the standard ReLU, logistic &quot;sigmoid&quot;, and hyperbolic
tangent functions. If the GNNs are allowed to use random initialisation and
global readout (both standard features of GNNs widely used in practice), they
can compute exactly the same queries as bounded depth Boolean circuits with
threshold gates, that is, exactly the queries in TC^0. Moreover, we show that
queries computable by a single GNN with piecewise linear activations and
rational weights are definable in GFO+C without built-in relations. Therefore,
they are contained in uniform TC^0.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-09 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Improved Bounds for Covering Paths and Trees in the Plane</title>
  <guid>http://arxiv.org/abs/2303.04350</guid>
  <link>http://arxiv.org/abs/2303.04350</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biniaz_A/0/1/0/all/0/1&quot;&gt;Ahmad Biniaz&lt;/a&gt;&lt;/p&gt;&lt;p&gt;A covering path for a planar point set is a path drawn in the plane with
straight-line edges such that every point lies at a vertex or on an edge of the
path. A covering tree is defined analogously. Let $\pi(n)$ be the minimum
number such that every set of $n$ points in the plane can be covered by a
noncrossing path with at most $\pi(n)$ edges. Let $\tau(n)$ be the analogous
number for noncrossing covering trees. Dumitrescu, Gerbner, Keszegh, and T\&#39;oth
(Discrete &amp;amp; Computational Geometry, 2014) established the following
inequalities: \[\frac{5n}{9} - O(1) &amp;lt; \pi(n) &amp;lt;
\left(1-\frac{1}{601080391}\right)n, \quad\text{and} \quad\frac{9n}{17} - O(1)
&amp;lt; \tau(n)\leqslant \left\lfloor\frac{5n}{6}\right\rfloor.\] We report the
following improved upper bounds: \[\pi(n)\leqslant
\left(1-\frac{1}{22}\right)n, \quad\text{and}\quad \tau(n)\leqslant
\left\lceil\frac{4n}{5}\right\rceil.\]
&lt;/p&gt;
&lt;p&gt;In the same context we study rainbow polygons. For a set of colored points in
the plane, a perfect rainbow polygon is a simple polygon that contains exactly
one point of each color in its interior or on its boundary. Let $\rho(k)$ be
the minimum number such that every $k$-colored point set in the plane admits a
perfect rainbow polygon of size $\rho(k)$. Flores-Pe\~naloza, Kano,
Mart\&#39;inez-Sandoval, Orden, Tejel, T\&#39;oth, Urrutia, and Vogtenhuber (Discrete
Mathematics, 2021) proved that $20k/19 - O(1) &amp;lt;\rho(k) &amp;lt; 10k/7 + O(1).$ We
report the improved upper bound $\rho(k)&amp;lt; 7k/5 + O(1)$.
&lt;/p&gt;
&lt;p&gt;To obtain the improved bounds we present simple $O(n\log n)$-time algorithms
that achieve paths, trees, and polygons with our desired number of edges.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-09 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>B-Treaps Revised: Write Efficient Randomized Block Search Trees with High Load</title>
  <guid>http://arxiv.org/abs/2303.04722</guid>
  <link>http://arxiv.org/abs/2303.04722</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safavi_R/0/1/0/all/0/1&quot;&gt;Roodabeh Safavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seybold_M/0/1/0/all/0/1&quot;&gt;Martin P. Seybold&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Uniquely represented data structures represent each logical state with a
unique storage state. We study the problem of maintaining a dynamic set of $n$
keys from a totally ordered universe in this context.
&lt;/p&gt;
&lt;p&gt;We introduce a two-layer data structure called
$(\alpha,\varepsilon)$-Randomized Block Search Tree (RBST) that is uniquely
represented and suitable for external memory. Though RBSTs naturally generalize
the well-known binary Treaps, several new ideas are needed to analyze the {\em
expected} search, update, and storage, efficiency in terms of block-reads,
block-writes, and blocks stored. We prove that searches have
$O(\varepsilon^{-1} + \log_\alpha n)$ block-reads, that $(\alpha,
\varepsilon)$-RBSTs have an asymptotic load-factor of at least
$(1-\varepsilon)$ for every $\varepsilon \in (0,1/2]$, and that dynamic updates
perform $O(\varepsilon^{-1} + \log_\alpha(n)/\alpha)$ block-writes, i.e.
$O(1/\varepsilon)$ writes if $\alpha=\Omega(\frac{\log n}{\log \log n} )$. Thus
$(\alpha, \varepsilon)$-RBSTs provide improved search, storage-, and
write-efficiency bounds in regard to the known, uniquely represented B-Treap
[Golovin; ICALP&#39;09].
&lt;/p&gt;
  </description>
  <pubDate>2023-03-09 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Diversity Embeddings and the Hypergraph Sparsest Cut</title>
  <guid>http://arxiv.org/abs/2303.04199</guid>
  <link>http://arxiv.org/abs/2303.04199</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jozefiak_A/0/1/0/all/0/1&quot;&gt;Adam D. Jozefiak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shepherd_F/0/1/0/all/0/1&quot;&gt;F. Bruce Shepherd&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Good approximations have been attained for the sparsest cut problem by
rounding solutions to convex relaxations via low-distortion metric embeddings.
Recently, Bryant and Tupper showed that this approach extends to the hypergraph
setting by formulating a linear program whose solutions are so-called
diversities which are rounded via diversity embeddings into $\ell_1$.
Diversities are a generalization of metric spaces in which the nonnegative
function is defined on all subsets as opposed to only on pairs of elements.
&lt;/p&gt;
&lt;p&gt;We show that this approach yields a polytime $O(\log{n})$-approximation when
either the supply or demands are given by a graph. This result improves upon
Plotkin et al.&#39;s $O(\log{(kn)}\log{n})$-approximation, where $k$ is the number
of demands, for the setting where the supply is given by a graph and the
demands are given by a hypergraph. Additionally, we provide a polytime
$O(\min{\{r_G,r_H\}}\log{r_H}\log{n})$-approximation for when the supply and
demands are given by hypergraphs whose hyperedges are bounded in cardinality by
$r_G$ and $r_H$ respectively.
&lt;/p&gt;
&lt;p&gt;To establish these results we provide an $O(\log{n})$-distortion $\ell_1$
embedding for the class of diversities known as diameter diversities. This
improves upon Bryant and Tupper&#39;s $O(\log\^2{n})$-distortion embedding. The
smallest known distortion with which an arbitrary diversity can be embedded
into $\ell_1$ is $O(n)$. We show that for any $\epsilon &amp;gt; 0$ and any $p&amp;gt;0$,
there is a family of diversities which cannot be embedded into $\ell_1$ in
polynomial time with distortion smaller than $O(n^{1-\epsilon})$ based on
querying the diversities on sets of cardinality at most $O(\log^p{n})$, unless
$P=NP$. This disproves (an algorithmic refinement of) Bryant and Tupper&#39;s
conjecture that there exists an $O(\sqrt{n})$-distortion $\ell_1$ embedding
based off a diversity&#39;s induced metric.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-09 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Investigating the complexity of the double distance problems</title>
  <guid>http://arxiv.org/abs/2303.04205</guid>
  <link>http://arxiv.org/abs/2303.04205</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braga_M/0/1/0/all/0/1&quot;&gt;Marilia D. V. Braga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brockmann_L/0/1/0/all/0/1&quot;&gt;Leonie R. Brockmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klerx_K/0/1/0/all/0/1&quot;&gt;Katharina Klerx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoye_J/0/1/0/all/0/1&quot;&gt;Jens Stoye&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Two genomes over the same set of gene families form a canonical pair when
each of them has exactly one gene from each family. Different distances of
canonical genomes can be derived from a structure called breakpoint graph,
which represents the relation between the two given genomes as a collection of
cycles of even length and paths. Then, the breakpoint distance is equal to n -
(c_2 + p_0/2), where n is the number of genes, c_2 is the number of cycles of
length 2 and p_0 is the number of paths of length 0. Similarly, when the
considered rearrangements are those modeled by the double-cut-and-join (DCJ)
operation, the rearrangement distance is n - (c + p_e/2), where c is the total
number of cycles and p_e is the total number of even paths.
&lt;/p&gt;
&lt;p&gt;The distance formulation is a basic unit for several other combinatorial
problems related to genome evolution and ancestral reconstruction, such as
median or double distance. Interestingly, both median and double distance
problems can be solved in polynomial time for the breakpoint distance, while
they are NP-hard for the rearrangement distance. One way of exploring the
complexity space between these two extremes is to consider the {\sigma}_k
distance, defined to be n - [c_2 + c_4 + ... + c_k + (p_0 + p_2 + ... +p_k)/2],
and increasingly investigate the complexities of median and double distance for
the {\sigma}_4 distance, then the {\sigma}_6 distance, and so on. While for the
median much effort was done in our and in other research groups but no progress
was obtained even for the {\sigma}_4 distance, for solving the double distance
under {\sigma}_4 and {\sigma}_6 distances we could devise linear time
algorithms, which we present here.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-09 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Polynomial Time and Private Learning of Unbounded Gaussian Mixture Models</title>
  <guid>http://arxiv.org/abs/2303.04288</guid>
  <link>http://arxiv.org/abs/2303.04288</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arbas_J/0/1/0/all/0/1&quot;&gt;Jamil Arbas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ashtiani_H/0/1/0/all/0/1&quot;&gt;Hassan Ashtiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liaw_C/0/1/0/all/0/1&quot;&gt;Christopher Liaw&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the problem of privately estimating the parameters of
$d$-dimensional Gaussian Mixture Models (GMMs) with $k$ components. For this,
we develop a technique to reduce the problem to its non-private counterpart.
This allows us to privatize existing non-private algorithms in a blackbox
manner, while incurring only a small overhead in the sample complexity and
running time. As the main application of our framework, we develop an
$(\varepsilon, \delta)$-differentially private algorithm to learn GMMs using
the non-private algorithm of Moitra and Valiant [MV10] as a blackbox.
Consequently, this gives the first sample complexity upper bound and first
polynomial time algorithm for privately learning GMMs without any boundedness
assumptions on the parameters.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-09 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Optimal Sparse Recovery with Decision Stumps</title>
  <guid>http://arxiv.org/abs/2303.04301</guid>
  <link>http://arxiv.org/abs/2303.04301</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Banihashem_K/0/1/0/all/0/1&quot;&gt;Kiarash Banihashem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hajiaghayi_M/0/1/0/all/0/1&quot;&gt;MohammadTaghi Hajiaghayi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Springer_M/0/1/0/all/0/1&quot;&gt;Max Springer&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Decision trees are widely used for their low computational cost, good
predictive performance, and ability to assess the importance of features.
Though often used in practice for feature selection, the theoretical guarantees
of these methods are not well understood. We here obtain a tight finite sample
bound for the feature selection problem in linear regression using single-depth
decision trees. We examine the statistical properties of these &quot;decision
stumps&quot; for the recovery of the $s$ active features from $p$ total features,
where $s \ll p$. Our analysis provides tight sample performance guarantees on
high-dimensional sparse systems which align with the finite sample bound of
$O(s \log p)$ as obtained by Lasso, improving upon previous bounds for both the
median and optimal splitting criteria. Our results extend to the non-linear
regime as well as arbitrary sub-Gaussian distributions, demonstrating that tree
based methods attain strong feature selection properties under a wide variety
of settings and further shedding light on the success of these methods in
practice. As a byproduct of our analysis, we show that we can provably
guarantee recovery even when the number of active features $s$ is unknown. We
further validate our theoretical results and proof methodology using
computational experiments.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-09 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Change a Bit to save Bytes: Compression for Floating Point Time-Series Data</title>
  <guid>http://arxiv.org/abs/2303.04478</guid>
  <link>http://arxiv.org/abs/2303.04478</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taurone_F/0/1/0/all/0/1&quot;&gt;Francesco Taurone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucani_D/0/1/0/all/0/1&quot;&gt;Daniel E. Lucani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feher_M/0/1/0/all/0/1&quot;&gt;Marcell Feh&amp;#xe9;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The number of IoT devices is expected to continue its dramatic growth in the
coming years and, with it, a growth in the amount of data to be transmitted,
processed and stored. Compression techniques that support analytics directly on
the compressed data could pave the way for systems to scale efficiently to
these growing demands. This paper proposes two novel methods for preprocessing
a stream of floating point data to improve the compression capabilities of
various IoT data compressors. In particular, these techniques are shown to be
helpful with recent compressors that allow for random access and analytics
while maintaining good compression. Our techniques improve compression with
reductions up to 80% when allowing for at most 1% of recovery error.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-09 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Streaming Kernel PCA Algorithm With Small Space</title>
  <guid>http://arxiv.org/abs/2303.04555</guid>
  <link>http://arxiv.org/abs/2303.04555</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yichuan Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Han Zhang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Principal Component Analysis (PCA) is a widely used technique in machine
learning, data analysis and signal processing. With the increase in the size
and complexity of datasets, it has become important to develop low-space usage
algorithms for PCA. Streaming PCA has gained significant attention in recent
years, as it can handle large datasets efficiently. The kernel method, which is
commonly used in learning algorithms such as Support Vector Machines (SVMs),
has also been applied in PCA algorithms.
&lt;/p&gt;
&lt;p&gt;We propose a streaming algorithm for Kernel PCA problems based on the
traditional scheme by Oja. Our algorithm addresses the challenge of reducing
the memory usage of PCA while maintaining its accuracy. We analyze the
performance of our algorithm by studying the conditions under which it
succeeds. Specifically, we show that, when the spectral ratio $R :=
\lambda_1/\lambda_2$ of the target covariance matrix is lower bounded by $C
\cdot \log n\cdot \log d$, the streaming PCA can be solved with $O(d)$ space
cost.
&lt;/p&gt;
&lt;p&gt;Our proposed algorithm has several advantages over existing methods. First,
it is a streaming algorithm that can handle large datasets efficiently. Second,
it employs the kernel method, which allows it to capture complex nonlinear
relationships among data points. Third, it has a low-space usage, making it
suitable for applications where memory is limited.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-09 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Interior-point methods on manifolds: theory and applications</title>
  <guid>http://arxiv.org/abs/2303.04771</guid>
  <link>http://arxiv.org/abs/2303.04771</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nieuwboer_H/0/1/0/all/0/1&quot;&gt;Harold Nieuwboer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Walter_M/0/1/0/all/0/1&quot;&gt;Michael Walter&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Interior-point methods offer a highly versatile framework for convex
optimization that is effective in theory and practice. A key notion in their
theory is that of a self-concordant barrier. We give a suitable generalization
of self-concordance to Riemannian manifolds and show that it gives the same
structural results and guarantees as in the Euclidean setting, in particular
local quadratic convergence of Newton&#39;s method. We then analyze a short-step
path-following method for optimizing compatible objectives over a convex domain
for which one has a self-concordant barrier, and obtain the standard complexity
guarantees as in the Euclidean setting. We show that on the positive-definite
matrices and other symmetric spaces, the squared distance to a point is a
self-concordant function. Our work is motivated by recent progress on scaling
problems and non-commutative optimization, and we show that these fit into our
framework, yielding algorithms with state-of-the-art complexity guarantees.
Furthermore, we show how to apply our methods to computing geometric medians on
spaces with constant negative curvature.
&lt;/p&gt;
  </description>
  <pubDate>2023-03-09 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

</channel>
</rss>
