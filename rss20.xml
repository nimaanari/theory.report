<?xml version="1.0"?>
<rss version="2.0">

<channel>
  <title>Theory of Computing Report</title>
  <link></link>
  <language>en</language>
  <description></description>


<item>
  <title>TR22-142 |  Correlation bounds against polynomials | 

	Emanuele Viola</title>
  <guid>https://eccc.weizmann.ac.il/report/2022/142</guid>
  <link>https://eccc.weizmann.ac.il/report/2022/142</link>
  <description>
    A survey on correlation bounds against polynomials.
  </description>
  <pubDate>2022-11-03 15:00:46 UTC</pubDate>
  <author>ECCC Papers</author>
</item>

<item>
  <title>Should you quit Twitter and Texas?</title>
  <guid>tag:blogger.com,1999:blog-3722233.post-1022697361105747430</guid>
  <link>http://blog.computationalcomplexity.org/2022/11/should-you-quit-twitter-and-texas.html</link>
  <description>
    &lt;p&gt;Generally with some exceptions, I use &lt;a href=&quot;https://facebook.com/fortnow&quot;&gt;Facebook&lt;/a&gt; for personal stuff, &lt;a href=&quot;https://www.linkedin.com/in/fortnow/&quot;&gt;LinkedIn&lt;/a&gt; for Illinois Tech stuff and &lt;a href=&quot;https://twitter.com/fortnow&quot;&gt;Twitter&lt;/a&gt;&amp;nbsp;and this blog for CS stuff. Many of you got to this post through the &lt;a href=&quot;https://twitter.com/fortnow/status/1588144625755328514&quot;&gt;Twitter link&lt;/a&gt;. Now that Elon Musk has bought the social media company, should I and the rest of the academic twitterverse move on to something else?&lt;/p&gt;&lt;p&gt;I&#39;d say not yet. Let&#39;s see what Elon does to the place. Maybe he can allow more points of view, without turning it into a cesspool. Or maybe he ruins it. It&#39;ll be a network effect--if too many academics leave Twitter, I&#39;d have to follow or I&#39;d have few followers. I wonder where they will go. I hope it isn&#39;t TikTok.&lt;/p&gt;&lt;p&gt;On a similar vein, I often here of those who suggest we don&#39;t hold conferences in certain jurisdictions for political reasons, for example Texas, because of its laws against abortion and transgender rights. I don&#39;t believe computer science, as a field, should be making decisions based on politics. Academics who live in these states don&#39;t generally hold the same views as the political leaders in those states.&lt;/p&gt;&lt;p&gt;Should we not have meetings in Illinois because some in our field might be opposed to abortion? Or do we just assume everyone has the same political views in the field. Individuals can make their own choices as to whether to attend, but it&#39;s best when politics is left out of academics. &lt;a href=&quot;https://focs2022.eecs.berkeley.edu/&quot;&gt;FOCS 2022&lt;/a&gt; is wrapping up today in Denver. Seems like a safe choice--perhaps all US conferences in the future should be in Colorado.&amp;nbsp;&lt;/p&gt;&lt;p&gt;There are limits--I wouldn&#39;t attend or organize a conference in Russia in the near future. But if we start eliminating locations based on politics, we&#39;ll only be able to meet up in the metaverse, and we won&#39;t have social media to tell us how to get there.&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By Lance Fortnow&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 12:18:00 UTC</pubDate>
  <author>Computational Complexity</author>
</item>

<item>
  <title>Persistent Tensors and Multiqudit Entanglement Transformation</title>
  <guid>http://arxiv.org/abs/2211.00652</guid>
  <link>http://arxiv.org/abs/2211.00652</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Gharahi_M/0/1/0/all/0/1&quot;&gt;Masoud Gharahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Lysikov_V/0/1/0/all/0/1&quot;&gt;Vladimir Lysikov&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We construct a lower bound of the tensor rank for a new class of tensors,
which we call persistent tensors. We present three specific families of
persistent tensors, of which the lower bound is tight. We show that there is a
chain of degenerations between these three families of minimal-rank persistent
tensors that can be used to study the entanglement transformation between them.
In addition, we show that these three families of persistent tensors are indeed
different generalizations of multiqubit $\rm{W}$ states within multiqudit
systems and are geometrically in the orbit closure of multiqudit $\rm{GHZ}$
states. Consequently, we show that one can obtain every one of the
generalizations of $\rm{W}$ state from a multiqudit $\rm{GHZ}$ state via
asymptotic Stochastic Local Operations and Classical Communication (SLOCC) with
rate one. Finally, we extend the obtained lower bound of the tensor rank to
direct sums with persistent summands and to even more general combinations of
tensors, which we call block pyramidal tensors. As a result, we show that the
tensor rank is multiplicative under the Kronecker and tensor products of
minimal-rank persistent tensors with the $\rm{GHZ}$ tensor.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Quantum Pseudoentanglement</title>
  <guid>http://arxiv.org/abs/2211.00747</guid>
  <link>http://arxiv.org/abs/2211.00747</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Bouland_A/0/1/0/all/0/1&quot;&gt;Adam Bouland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Fefferman_B/0/1/0/all/0/1&quot;&gt;Bill Fefferman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Soumik Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Vazirani_U/0/1/0/all/0/1&quot;&gt;Umesh Vazirani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zixin Zhou&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Quantum pseudorandom states are efficiently constructable states which
nevertheless masquerade as Haar-random states to poly-time observers. First
defined by Ji, Liu and Song, such states have found a number of applications
ranging from cryptography to the AdS/CFT correspondence. A fundamental question
is exactly how much entanglement is required to create such states. Haar-random
states, as well as $t$-designs for $t\geq 2$, exhibit near-maximal
entanglement. Here we provide the first construction of pseudorandom states
with only polylogarithmic entanglement entropy across an equipartition of the
qubits, which is the minimum possible. Our construction can be based on any
one-way function secure against quantum attack. We additionally show that the
entanglement in our construction is fully &quot;tunable&quot;, in the sense that one can
have pseudorandom states with entanglement $\Theta(f(n))$ for any desired
function $\omega(\log n) \leq f(n) \leq O(n)$.
&lt;/p&gt;
&lt;p&gt;More fundamentally, our work calls into question to what extent entanglement
is a &quot;feelable&quot; quantity of quantum systems. Inspired by recent work of
Gheorghiu and Hoban, we define a new notion which we call &quot;pseudoentanglement&quot;,
which are ensembles of efficiently constructable quantum states which hide
their entanglement entropy. We show such states exist in the strongest form
possible while simultaneously being pseudorandom states. We also describe
diverse applications of our result from entanglement distillation to property
testing to quantum gravity.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Polynomial Identity Testing via Evaluation of Rational Functions</title>
  <guid>http://arxiv.org/abs/2211.01062</guid>
  <link>http://arxiv.org/abs/2211.01062</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melkebeek_D/0/1/0/all/0/1&quot;&gt;Dieter van Melkebeek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morgan_A/0/1/0/all/0/1&quot;&gt;Andrew Morgan&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We introduce a hitting set generator for Polynomial Identity Testing based on
evaluations of low-degree univariate rational functions at abscissas associated
with the variables. Despite the univariate nature, we establish an equivalence
up to rescaling with a generator introduced by Shpilka and Volkovich, which has
a similar structure but uses multivariate polynomials in the abscissas.
&lt;/p&gt;
&lt;p&gt;We study the power of the generator by characterizing its vanishing ideal,
i.e., the set of polynomials that it fails to hit. Capitalizing on the
univariate nature, we develop a small collection of polynomials that jointly
produce the vanishing ideal. As corollaries, we obtain tight bounds on the
minimum degree, sparseness, and partition class size of set-multilinearity in
the vanishing ideal. Inspired by an alternating algebra representation, we
develop a structured deterministic membership test for the vanishing ideal. As
a proof of concept, we rederive known derandomization results based on the
generator by Shpilka and Volkovich and present a new application for read-once
oblivious algebraic branching programs.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Partitioning a Polygon Into Small Pieces</title>
  <guid>http://arxiv.org/abs/2211.01359</guid>
  <link>http://arxiv.org/abs/2211.01359</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abrahamsen_M/0/1/0/all/0/1&quot;&gt;Mikkel Abrahamsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasmussen_N/0/1/0/all/0/1&quot;&gt;Nichlas Langhoff Rasmussen&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the problem of partitioning a given simple polygon $P$ into a
minimum number of polygonal pieces, each of which has bounded size. We give
algorithms for seven notions of `bounded size,&#39; namely that each piece has
bounded area, perimeter, straight-line diameter, geodesic diameter, or that
each piece must be contained in a unit disk, an axis-aligned unit square or an
arbitrarily rotated unit square.
&lt;/p&gt;
&lt;p&gt;A more general version of the area problem has already been studied. Here we
are, in addition to $P$, given positive real values $a_1,\ldots,a_k$ such that
the sum $\sum_{i=1}^k a_i$ equals the area of $P$. The goal is to partition $P$
into exactly $k$ pieces $Q_1,\ldots,Q_k$ such that the area of $Q_i$ is $a_i$.
Such a partition always exists, and an algorithm with running time $O(nk)$ has
previously been described, where $n$ is the number of corners of $P$. We give
an algorithm with optimal running time $O(n+k)$. For polygons with holes, we
get running time $O(n\log n+k)$.
&lt;/p&gt;
&lt;p&gt;For the other problems, it seems out of reach to compute optimal partitions
for simple polygons; for most of them, even in extremely restricted cases such
as when $P$ is a square. We therefore develop $O(1)$-approximation algorithms
for these problems, which means that the number of pieces in the produced
partition is at most a constant factor larger than the cardinality of a minimum
partition. Existing algorithms do not allow Steiner points, which means that
all corners of the produced pieces must also be corners of $P$. This has the
disappointing consequence that a partition does often not exist, whereas our
algorithms always produce useful partitions. Furthermore, an optimal partition
without Steiner points may require $\Omega(n)$ pieces for polygons where a
partition consisting of just $2$ pieces exists when Steiner points are allowed.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Simplified Prophet Inequalities for Combinatorial Auctions</title>
  <guid>http://arxiv.org/abs/2211.00707</guid>
  <link>http://arxiv.org/abs/2211.00707</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_A/0/1/0/all/0/1&quot;&gt;Alexander Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kesselheim_T/0/1/0/all/0/1&quot;&gt;Thomas Kesselheim&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We consider prophet inequalities for XOS and MPH-$k$ combinatorial auctions
and give a simplified proof for the existence of static and anonymous item
prices which recover the state-of-the-art competitive ratios.
&lt;/p&gt;
&lt;p&gt;Our proofs make use of a linear programming formulation which has a
non-negative objective value if there are prices which admit a given
competitive ratio $\alpha \geq 1$. Changing our perspective to dual space by an
application of strong LP duality, we use an interpretation of the dual
variables as probabilities to directly obtain our result. In contrast to
previous work, our proofs do not require to argue about specific values of
buyers for bundles, but only about the presence or absence of items.
&lt;/p&gt;
&lt;p&gt;As a side remark, for any $k \geq 2$, this simplification also leads to a
tiny improvement in the best competitive ratio for MPH-$k$ combinatorial
auctions from $4k-2$ to $2k + 2 \sqrt{k(k-1)} -1$.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Alternative polynomial-time algorithm for Bipartite Matching</title>
  <guid>http://arxiv.org/abs/2211.00711</guid>
  <link>http://arxiv.org/abs/2211.00711</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillemot_S/0/1/0/all/0/1&quot;&gt;Sylvain Guillemot&lt;/a&gt;&lt;/p&gt;&lt;p&gt;If $G$ is a bipartite graph, Hall&#39;s theorem \cite{H35} gives a condition for
the existence of a matching of $G$ covering one side of the bipartition. This
theorem admits a well-known algorithmic proof involving the repeated search of
augmenting paths. We present here an alternative algorithm, using a
game-theoretic formulation of the problem. We also show how to extend this
formulation to the setting of balanced hypergraphs.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Privacy Induces Robustness: Information-Computation Gaps and Sparse Mean Estimation</title>
  <guid>http://arxiv.org/abs/2211.00724</guid>
  <link>http://arxiv.org/abs/2211.00724</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Georgiev_K/0/1/0/all/0/1&quot;&gt;Kristian Georgiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hopkins_S/0/1/0/all/0/1&quot;&gt;Samuel B. Hopkins&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We establish a simple connection between robust and differentially-private
algorithms: private mechanisms which perform well with very high probability
are automatically robust in the sense that they retain accuracy even if a
constant fraction of the samples they receive are adversarially corrupted.
Since optimal mechanisms typically achieve these high success probabilities,
our results imply that optimal private mechanisms for many basic statistics
problems are robust.
&lt;/p&gt;
&lt;p&gt;We investigate the consequences of this observation for both algorithms and
computational complexity across different statistical problems. Assuming the
Brennan-Bresler secret-leakage planted clique conjecture, we demonstrate a
fundamental tradeoff between computational efficiency, privacy leakage, and
success probability for sparse mean estimation. Private algorithms which match
this tradeoff are not yet known -- we achieve that (up to polylogarithmic
factors) in a polynomially-large range of parameters via the Sum-of-Squares
method.
&lt;/p&gt;
&lt;p&gt;To establish an information-computation gap for private sparse mean
estimation, we also design new (exponential-time) mechanisms using fewer
samples than efficient algorithms must use. Finally, we give evidence for
privacy-induced information-computation gaps for several other statistics and
learning problems, including PAC learning parity functions and estimation of
the mean of a multivariate Gaussian.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Benchmarking Hashing Algorithms for Load Balancing in a Distributed Database Environment</title>
  <guid>http://arxiv.org/abs/2211.00741</guid>
  <link>http://arxiv.org/abs/2211.00741</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slesarev_A/0/1/0/all/0/1&quot;&gt;Alexander Slesarev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikhailov_M/0/1/0/all/0/1&quot;&gt;Mikhail Mikhailov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chernishev_G/0/1/0/all/0/1&quot;&gt;George Chernishev&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Modern high load applications store data using multiple database instances.
Such an architecture requires data consistency, and it is important to ensure
even distribution of data among nodes. Load balancing is used to achieve these
goals.
&lt;/p&gt;
&lt;p&gt;Hashing is the backbone of virtually all load balancing systems. Since the
introduction of classic Consistent Hashing, many algorithms have been devised
for this purpose.
&lt;/p&gt;
&lt;p&gt;One of the purposes of the load balancer is to ensure storage cluster
scalability. It is crucial for the performance of the whole system to transfer
as few data records as possible during node addition or removal. The load
balancer hashing algorithm has the greatest impact on this process.
&lt;/p&gt;
&lt;p&gt;In this paper we experimentally evaluate several hashing algorithms used for
load balancing, conducting both simulated and real system experiments. To
evaluate algorithm performance, we have developed a benchmark suite based on
Unidata MDM~ -- a scalable toolkit for various Master Data Management (MDM)
applications. For assessment, we have employed three criteria~ -- uniformity of
the produced distribution, the number of moved records, and computation speed.
Following the results of our experiments, we have created a table, in which
each algorithm is given an assessment according to the abovementioned criteria.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Balancing Utility and Fairness in Submodular Maximization (Technical Report)</title>
  <guid>http://arxiv.org/abs/2211.00980</guid>
  <link>http://arxiv.org/abs/2211.00980</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuchen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonchi_F/0/1/0/all/0/1&quot;&gt;Francesco Bonchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Ying Wang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Submodular function maximization is central in numerous data science
applications, including data summarization, influence maximization, and
recommendation. In many of these problems, our goal is to find a solution that
maximizes the \emph{average} of the utilities for all users, each measured by a
monotone submodular function. When the population of users is composed of
several demographic groups, another critical problem is whether the utility is
fairly distributed across groups. In the context of submodular optimization, we
seek to improve the welfare of the \emph{least well-off} group, i.e., to
maximize the minimum utility for any group, to ensure fairness. Although the
\emph{utility} and \emph{fairness} objectives are both desirable, they might
contradict each other, and, to our knowledge, little attention has been paid to
optimizing them jointly. In this paper, we propose a novel problem called
\emph{Bicriteria Submodular Maximization} (BSM) to strike a balance between
utility and fairness. Specifically, it requires finding a fixed-size solution
to maximize the utility function, subject to the value of the fairness function
not being below a threshold. Since BSM is inapproximable within any constant
factor in general, we propose efficient data-dependent approximation algorithms
for BSM by converting it into other submodular optimization problems and
utilizing existing algorithms for the converted problems to obtain solutions to
BSM. Using real-world and synthetic datasets, we showcase applications of our
framework in three submodular maximization problems, namely maximum coverage,
influence maximization, and facility location.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Joint Correlation Detection and Alignment of Gaussian Databases</title>
  <guid>http://arxiv.org/abs/2211.01069</guid>
  <link>http://arxiv.org/abs/2211.01069</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamir_R/0/1/0/all/0/1&quot;&gt;Ran Tamir&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this work, we propose an efficient two-stage algorithm solving a joint
problem of correlation detection and permutation recovery between two Gaussian
databases. Correlation detection is an hypothesis testing problem; under the
null hypothesis, the databases are independent, and under the alternate
hypothesis, they are correlated, under an unknown row permutation. We develop
relatively tight bounds on the type-I and type-II error probabilities, and show
that the analyzed detector performs better than a recently proposed detector,
at least for some specific parameter choices. Since the proposed detector
relies on a statistic, which is a sum of dependent indicator random variables,
then in order to bound the type-I probability of error, we develop a novel
graph-theoretic technique for bounding the $k$-th order moments of such
statistics. When the databases are accepted as correlated, the algorithm also
outputs an estimation for the underlying row permutation. By comparing to known
converse results for this problem, we prove that the alignment error
probability converges to zero under the asymptotically lowest possible
correlation coefficient.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Set Selection under Explorable Stochastic Uncertainty via Covering Techniques</title>
  <guid>http://arxiv.org/abs/2211.01097</guid>
  <link>http://arxiv.org/abs/2211.01097</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Megow_N/0/1/0/all/0/1&quot;&gt;Nicole Megow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schloter_J/0/1/0/all/0/1&quot;&gt;Jens Schl&amp;#xf6;ter&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Given subsets of uncertain values, we study the problem of identifying the
subset of minimum total value (sum of the uncertain values) by querying as few
values as possible. This set selection problem falls into the field of
explorable uncertainty and is of intrinsic importance therein as it implies
strong adversarial lower bounds for a wide range of interesting combinatorial
problems such as knapsack and matchings. We consider a stochastic problem
variant and give algorithms that, in expectation, improve upon these
adversarial lower bounds. The key to our results is to prove a strong
structural connection to a seemingly unrelated covering problem with
uncertainty in the constraints via a linear programming formulation. We exploit
this connection to derive an algorithmic framework that can be used to solve
both problems under uncertainty, obtaining nearly tight bounds on the
competitive ratio. This is the first non-trivial stochastic result concerning
the sum of unknown values without further structure known for the set. Further,
we handle for the first time uncertainty in the constraints in a value-query
model. With our novel methods, we lay the foundations for solving more general
problems in the area of explorable uncertainty.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>New Tradeoffs for Decremental Approximate All-Pairs Shortest Paths</title>
  <guid>http://arxiv.org/abs/2211.01152</guid>
  <link>http://arxiv.org/abs/2211.01152</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dory_M/0/1/0/all/0/1&quot;&gt;Michal Dory&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forster_S/0/1/0/all/0/1&quot;&gt;Sebastian Forster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nazari_Y/0/1/0/all/0/1&quot;&gt;Yasamin Nazari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vos_T/0/1/0/all/0/1&quot;&gt;Tijn de Vos&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We provide new tradeoffs between approximation and running time for the
decremental all-pairs shortest paths (APSP) problem. For undirected graphs with
$m$ edges and $n$ nodes undergoing edge deletions, we provide two new
approximate decremental APSP algorithms, one for weighted and one for
unweighted graphs. Our first result is an algorithm that supports $(2+
\epsilon)$-approximate all-pairs constant-time distance queries with total
update time $\tilde{O}(m^{1/2}n^{3/2})$ when $m= O(n^{5/3})$ (and $m= n^{1+c}$
for any constant $c &amp;gt;0$), or $\tilde{O}(mn^{2/3})$ when $m = \Omega(n^{5/3})$.
Prior to our work the fastest algorithm for weighted graphs with approximation
at most $3$ had total $\tilde O(mn)$ update time providing a
$(1+\epsilon)$-approximation [Bernstein, SICOMP 2016]. Our technique also
yields a decremental algorithm with total update time $\tilde{O}(nm^{3/4})$
supporting $(2+\epsilon, W_{u,v})$-approximate queries where the second term is
an additional additive term and $W_{u,v}$ is the maximum weight on the shortest
path from $u$ to $v$.
&lt;/p&gt;
&lt;p&gt;Our second result is a decremental algorithm that given an unweighted graph
and a constant integer $k \geq 2 $, supports $(1+\epsilon, 2(k-1))$-approximate
queries and has $\tilde{O}(n^{2-1/k}m^{1/k})$ total update time (when
$m=n^{1+c}$ for any constant $c &amp;gt;0$). For comparison, in the special case of
$(1+\epsilon, 2)$-approximation, this improves over the state-of-the-art
algorithm by [Henzinger, Krinninger, Nanongkai, SICOMP 2016] with total update
time of $\tilde{O}(n^{2.5})$. All of our results are randomized and work
against an oblivious adversary.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>A Two Step Approach to Weighted Bipartite Link Recommendations</title>
  <guid>http://arxiv.org/abs/2211.01153</guid>
  <link>http://arxiv.org/abs/2211.01153</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1&quot;&gt;Nathan Ma&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Many real world person-person or person-product relationships can be modeled
graphically. More specifically, bipartite graphs can be especially useful when
modeling scenarios that involve two disjoint groups. As a result, many existing
papers have utilized bipartite graphs for the classical link recommendation
problem. In this paper, using the principle of bipartite graphs, we present
another approach to this problem with a two step algorithm that takes into
account frequency and similarity between common edges to make recommendations.
We test this approach with bipartite data gathered from the Epinions and
Movielens data sources, and find it to perform with roughly 14 percent error,
which improves upon baseline results. This is a promising result, and can be
refined to generate even more accurate recommendations.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Cluster Assignment in Multi-Agent Systems : Sparsity Bounds and Fault Tolerance</title>
  <guid>http://arxiv.org/abs/2211.01316</guid>
  <link>http://arxiv.org/abs/2211.01316</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sharf_M/0/1/0/all/0/1&quot;&gt;Miel Sharf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zelazo_D/0/1/0/all/0/1&quot;&gt;Daniel Zelazo&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study cluster assignment in homogeneous diffusive multi-agent networks.
Given the number of clusters and agents within each cluster, we design the
network graph ensuring the system will converge to the prescribed cluster
configuration. Using recent results linking clustering and symmetries, we show
that it is possible to design an oriented graph for which the action of the
automorphism group of the graph has orbits of predetermined sizes, guaranteeing
the network will converge to the prescribed cluster configuration. We provide
bounds on the number of edges needed to construct these graphs along with a
constructive approach for their generation. We also consider the robustness of
the clustering process under agent malfunction.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>PhotoGuard: Defending Against Diffusion-based Image Manipulation</title>
  <guid>https://gradientscience.org/photoguard/</guid>
  <link>https://gradientscience.org/photoguard/</link>
  <description>
    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://use.fontawesome.com/releases/v5.8.1/css/all.css&quot; integrity=&quot;sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf&quot; crossorigin=&quot;anonymous&quot; /&gt;

&lt;style&gt;
    blockquote {
        padding-top: 0;
        padding-bottom: 0;
    }

    .bbutton{
        width: 27.5%;
        margin: 2.5%;
        height: 3rem;
        line-height: 3rem;
        display: inline-block;
        background: #DDD;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: 400;
        color: #000;
    }

    .bbutton:hover{
        background: #e98a99;
    }
&lt;/style&gt;

&lt;script src=&quot;/assets/scripts/onload.js&quot;&gt;&lt;/script&gt;

&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;/assets/css/style.css&quot; /&gt;

&lt;p&gt;&lt;a class=&quot;bbutton&quot; style=&quot;float: left; width: 45%;&quot; href=&quot;#&quot;&gt;
&lt;i class=&quot;fas fa-file-pdf&quot;&gt;&lt;/i&gt;
    Paper Coming Soon!
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a class=&quot;bbutton&quot; style=&quot;float: left; width: 45%;&quot; href=&quot;https://github.com/MadryLab/photoguard&quot;&gt;
&lt;i class=&quot;fab fa-github&quot;&gt;&lt;/i&gt;
   Code
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;A few nights ago on The Daily Show, host Trevor Noah interviewed Mira Murati (CTO of OpenAI) about DALL$\cdot$E 2 and, more generally, the power of AI:&lt;/p&gt;

&lt;div style=&quot;width: 100%; text-align: center; padding-top: 10px; padding-bottom: 10px;&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Ba_C-C6UwlI?start=228&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The whole interview is a great watch, but one thing that stood out to us is Trevor’s question at 3:50:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;“So how do you safeguard them [generative models]?… We can very quickly find ourselves in a world where nothing is real, and everything that’s real isn’t, and we question it. How do you prevent, or can you even prevent that completely?”&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Indeed, DALL$\cdot$E (and diffusion models in general) greatly exacerbate the risks of malicious image manipulation—what previously required extensive knowledge of photoshop can now be done with just a simple natural-language query:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/photoguard/dog_example.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In fact, editing photos of cute dogs is just the tip of the iceberg:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/photoguard/trevor_example.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, back to Trevor Noah’s question—is there any hope of protecting against this manipulation? We spent a few nights hacking away, and it turns out that—by leveraging adversarial examples—we can do exactly that! The details of our scheme are below,  but the essence is that, by slightly modifying (imperceptibly, even) the input image, we can make it immune to direct editing by generative models!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/photoguard/photoguard_headline.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By the way, Michael Kosta is not the only person who has a selfie with Trevor. Hadi — the lead student on this project — took a selfie with Trevor couple years ago too.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;A selfie of Hadi and Trevor Noah&quot; src=&quot;/assets/photoguard/hadi_trevor_selfie.png&quot; style=&quot;width:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, Hadi can leverage diffusion-powered photo editing to “deepen” his
(imaginary) friendship with Trevor.
If the selfie was guarded, none of this would be possible (sadly for Trevor,
it isn’t)!&lt;/p&gt;

&lt;div class=&quot;widget&quot;&gt;
    &lt;div class=&quot;choices_one_full&quot; id=&quot;gen&quot;&gt;
    &lt;span class=&quot;widgetheading&quot; id=&quot;genclass&quot;&gt;Choose an Image&lt;/span&gt;
    &lt;/div&gt;
    &lt;div style=&quot;border-right: 3px white solid;&quot;&gt;
        &lt;img id=&quot;gen0-1&quot; class=&quot;image-container&quot; style=&quot;width: 0%; margin: 0;&quot; /&gt;
        &lt;img id=&quot;gen0-2&quot; class=&quot;image-container&quot; style=&quot;width: 100%; margin: 0;&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;clear:both;&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;some-details&quot;&gt;Some details&lt;/h2&gt;

&lt;p&gt;The core of our “immunization” process is to leverage so-called &lt;a href=&quot;http://gradientscience.org/intro_adversarial/&quot;&gt;adversarial attacks&lt;/a&gt; on these generative models. In particular, we implemented two different attacks, focused on &lt;em&gt;latent diffusion models&lt;/em&gt; (like &lt;a href=&quot;http://stability.ai&quot;&gt;Stable Diffusion&lt;/a&gt;). For simplicity, we can think of such models as having two parts:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The &lt;em&gt;conditioning mechanism&lt;/em&gt; is how the model incorporates external data such as the starting image and the prompt into its final generation. Typically, a pre-trained encoder converts the external signals to a shared embedding space—the model concatenates these embeddings and uses them as input to…&lt;/li&gt;
  &lt;li&gt;…the &lt;em&gt;diffusion process,&lt;/em&gt; which is responsible for generating the final image generated images. There are many good introductions to how exactly diffusion processes work, but the summary is that we start from random noise, and then repeatedly apply a model that “denoises” the input a little bit at a time.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We construct both a simple attack on the conditioning mechanism, and a complex attack on the end-to-end diffusion process itself.&lt;/p&gt;

&lt;h3 id=&quot;simple-attack&quot;&gt;Simple attack&lt;/h3&gt;

&lt;p&gt;In the simpler of the two attacks, we attack only the conditioning step of the diffusion process. That is, given a starting image $x_0$, we find an image $x_{adv}$ satisfying:&lt;/p&gt;

\[x_{adv} = \arg\max_{\mid\mid x - x_0 \mid\mid \lt \delta} \mathcal{L}(z_x, z_{targ})\]

&lt;p&gt;where $z_x$ is the embedding of the input $x$, and $z_{targ}$ is a fixed embedding. We set $z_{targ}$ to the all zeros vector (or even to an embedding of a random image), causing the diffusion model to completely ignore the starting image and focus only on the prompt.&lt;/p&gt;

&lt;div class=&quot;widget&quot;&gt;
    &lt;div class=&quot;choices_one_full&quot; id=&quot;gen-1&quot;&gt;
    &lt;span class=&quot;widgetheading&quot; id=&quot;genclass&quot;&gt;Choose an Image&lt;/span&gt;
    &lt;/div&gt;
    &lt;div style=&quot;border-right: 3px white solid;&quot;&gt;
        &lt;img id=&quot;gen1-1&quot; class=&quot;image-container&quot; style=&quot;width: 0%; margin: 0;&quot; /&gt;
        &lt;img id=&quot;gen1-2&quot; class=&quot;image-container&quot; style=&quot;width: 100%; margin: 0;&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;clear:both;&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;complex-attack&quot;&gt;Complex attack&lt;/h3&gt;

&lt;p&gt;However, we find that we can do an even stronger attack! In this more complex attack, we modify the starting image with the goal of breaking the &lt;em&gt;whole&lt;/em&gt; end-to-end diffusion process. Because the diffusion process is iterative and involves repeated application of a network, taking gradients through the diffusion process is memory-intensive. We found that differentiating through &lt;em&gt;only four&lt;/em&gt; denoising steps was enough to throw off the entire diffusion process. With a little engineering, we were able to fit four steps onto a single (A100) GPU. As you can see, editing our immunized/defended photos lead to much clearer fake images than the previous attack.&lt;/p&gt;

&lt;p&gt;Here are some examples of fake photos, with and without our “immunization!”&lt;/p&gt;

&lt;div class=&quot;widget&quot;&gt;
    &lt;div class=&quot;choices_one_full&quot; id=&quot;gen-2&quot;&gt;
    &lt;span class=&quot;widgetheading&quot; id=&quot;genclass&quot;&gt;Choose an Image&lt;/span&gt;
    &lt;/div&gt;
    &lt;div style=&quot;border-right: 3px white solid;&quot;&gt;
        &lt;img id=&quot;gen2-1&quot; class=&quot;image-container&quot; style=&quot;width: 0%; margin: 0;&quot; /&gt;
        &lt;img id=&quot;gen2-2&quot; class=&quot;image-container&quot; style=&quot;width: 100%; margin: 0;&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;clear:both;&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;takeaways-and-future-work&quot;&gt;Takeaways and Future Work&lt;/h2&gt;

&lt;p&gt;So, using relatively simple techniques relating to adversarial examples (and about a week’s worth of hacking), we were able to protect images against manipulation from diffusion-based generative models. That said, this is just the beginning, and there are still many unanswered questions!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We only constructed these examples by using an &lt;a href=&quot;https://stability.ai/blog/stable-diffusion-public-release&quot;&gt;open-source diffusion model&lt;/a&gt; (from &lt;a href=&quot;https://huggingface.co/runwayml/stable-diffusion-inpainting&quot;&gt;HuggingFace&lt;/a&gt;). Is it be possible to make them with only black-box access to the model?&lt;/li&gt;
  &lt;li&gt;Our complex attack uses a lot of memory (we could only fit four diffusion steps onto a single GPU). Meanwhile, &lt;a href=&quot;https://arxiv.org/abs/2205.07460&quot;&gt;recent work&lt;/a&gt; shows that for some diffusion processes, one can obtain the gradient through the entire diffusion process by solving a stochastic differential equation (SDE) and using a constant amount of memory. Is it possible to do something similar more generally?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More generally, we’re excited about the prospect of adversarial examples being used for forcing &lt;em&gt;intended&lt;/em&gt; behavior, rather than for exploiting vulnerabilities (a phenomenon also seen in our work on &lt;a href=&quot;http://gradientscience.org/unadversarial/&quot;&gt;unadversarial examples&lt;/a&gt;!).&lt;/p&gt;

&lt;script&gt;
 function main() {
     const BASE_DIR = &quot;/assets/photoguard/&quot;;
     
     // Generation
     var genImage1 = document.getElementById(&#39;gen0-1&#39;);
     var genImage2 = document.getElementById(&#39;gen0-2&#39;);
     var genSrcs = range(10).map((name) =&gt; BASE_DIR + &#39;hadi_selfies/&#39; + name + &quot;_orig.png&quot;);
     function genMapper(origSrc, id) {
           genImage1.src = origSrc;
           genImage2.src = origSrc.replace(&quot;_orig.png&quot;, &quot;.png&quot;);
     }
     activate_one_widget(&#39;gen&#39;, genSrcs, genMapper);

    // Generation
     var genImage11 = document.getElementById(&#39;gen1-1&#39;);
     var genImage12 = document.getElementById(&#39;gen1-2&#39;);
     var genSrcs1 = range(8).map((name) =&gt; BASE_DIR + &#39;encoder_attack/&#39; + name + &quot;_orig.png&quot;);
     function genMapper1(origSrc, id) {
           genImage11.src = origSrc;
           genImage12.src = origSrc.replace(&quot;_orig.png&quot;, &quot;.png&quot;);
     }

     activate_one_widget(&#39;gen-1&#39;, genSrcs1, genMapper1);
    
     var genImage21 = document.getElementById(&#39;gen2-1&#39;);
     var genImage22 = document.getElementById(&#39;gen2-2&#39;);
     var genSrcs2 = range(10).map((name) =&gt; BASE_DIR + &#39;diffusion_attack/&#39; + name + &quot;_orig.png&quot;);
     function genMapper2(origSrc, id) {
           genImage21.src = origSrc;
           genImage22.src = origSrc.replace(&quot;_orig.png&quot;, &quot;.png&quot;);
     }

     activate_one_widget(&#39;gen-2&#39;, genSrcs2, genMapper2);
 }
 window.onload = main;

&lt;/script&gt;
  </description>
  <pubDate>2022-11-03 00:00:00 UTC</pubDate>
  <author>Gradient Science</author>
</item>

<item>
  <title>TCS+ talk: Wednesday, November 9 — Yaonan Jin, Columbia University</title>
  <guid>http://tcsplus.wordpress.com/?p=646</guid>
  <link>https://tcsplus.wordpress.com/2022/11/02/tcs-talk-wednesday-november-9-yaonan-jin-columbia-university/</link>
  <description>
    &lt;p&gt;The next TCS+ talk will take place this coming Wednesday, November 9th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). &lt;strong&gt;Yaonan Jin&lt;/strong&gt; from Columbia University will speak about &amp;#8220;&lt;em&gt;First Price Auction is 1-1/e² Efficient&lt;/em&gt;&amp;#8221; (abstract below).&lt;/p&gt;
&lt;p&gt;You can reserve a spot as an individual or a group to join us live by signing up on &lt;a href=&quot;https://sites.google.com/view/tcsplus/welcome/next-tcs-talk&quot;&gt;the online form&lt;/a&gt;. Registration is &lt;em&gt;not&lt;/em&gt; required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted &lt;a href=&quot;https://sites.google.com/view/tcsplus/welcome/past-talks&quot;&gt;on our website&lt;/a&gt; afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to &lt;a href=&quot;https://sites.google.com/view/tcsplus/welcome/suggest-a-talk&quot;&gt;suggest&lt;/a&gt; a possible topic or speaker, please see &lt;a href=&quot;https://sites.google.com/view/tcsplus/&quot;&gt;the website&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class=&quot;wp-block-quote&quot;&gt;&lt;p&gt;Abstract: We prove that, for the first-price auction, the tight Price of Anarchy (PoA) and the tight Price of Stability (PoS) are both &lt;img src=&quot;https://s0.wp.com/latex.php?latex=1-1%2Fe%5E2+%5Capprox+0.8647&amp;#038;bg=fff&amp;#038;fg=444444&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=1-1%2Fe%5E2+%5Capprox+0.8647&amp;#038;bg=fff&amp;#038;fg=444444&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1-1%2Fe%5E2+%5Capprox+0.8647&amp;#038;bg=fff&amp;#038;fg=444444&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;1-1/e^2 &amp;#92;approx 0.8647&quot; class=&quot;latex&quot; /&gt;, closing the gap between the best known bounds [0.7430, 0.8689].&lt;/p&gt;
&lt;p&gt;Based on joint works with Pinyan Lu.&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/2207.01761&quot;&gt;https://arxiv.org/abs/2207.01761&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/2207.04455&quot;&gt;https://arxiv.org/abs/2207.04455&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p class=&quot;authors&quot;&gt;By plustcs&lt;/p&gt;
  </description>
  <pubDate>2022-11-02 15:47:26 UTC</pubDate>
  <author>TCS+ Seminar Series</author>
</item>

<item>
  <title>Composable Coresets for Constrained Determinant Maximization and Beyond</title>
  <guid>http://arxiv.org/abs/2211.00289</guid>
  <link>http://arxiv.org/abs/2211.00289</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahabadi_S/0/1/0/all/0/1&quot;&gt;Sepideh Mahabadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1&quot;&gt;Thuy-Duong Vuong&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the task of determinant maximization under partition constraint, in
the context of large data sets. Given a point set $V\subset \mathbb{R}^d$ that
is partitioned into $s$ groups $V_1,..., V_s$, and integers $k_1,...,k_s$ where
$k=\sum_i k_i$, the goal is to pick $k_i$ points from group $i$ such that the
overall determinant of the picked $k$ points is maximized. Determinant
Maximization and its constrained variants have gained a lot of interest for
modeling diversityand have found applications in the context of fairness and
data summarization.
&lt;/p&gt;
&lt;p&gt;We study the design of composable coresets for the constrained determinant
maximization problem. Composable coresets are small subsets of the data that
(approximately) preserve optimal solutions to optimization tasks and enable
efficient solutions in several other large data models including the
distributed and the streaming settings. In this work, we consider two regimes.
For the case of $k&amp;gt;d$, we show a peeling algorithm that gives us a composable
coreset of size $kd$ with an approximation factor of $d^{O(d)}$. We complement
our results by showing that this approximation factor is tight. For the case of
$k\leq d$, we show that a simple modification of the previous algorithms
results in an optimal coreset verified by our lower bounds. Our results apply
to all strongly Rayleigh distribution and several other experimental design
problems. In addition, we show coreset construction algorithms under the more
general laminar matroid constraints.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-02 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>A GPU-friendly, Parallel, and (Almost-)In-Place Algorithm for Building Left-Balanced kd-Trees</title>
  <guid>http://arxiv.org/abs/2211.00120</guid>
  <link>http://arxiv.org/abs/2211.00120</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wald_I/0/1/0/all/0/1&quot;&gt;Ingo Wald&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We present an algorithm that allows for building left-balanced and complete
k-d trees over k-dimensional points in a trivially parallel and GPU friendly
way. Our algorithm requires exactly one int per data point as temporary
storage, and uses O(logN ) iterations, each of which performs one parallel
sort, and one trivially parallel CUDA per-node update kernel.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-02 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Computational Power of A Single Oblivious Mobile Agent in Two-Edge-Connected Graphs</title>
  <guid>http://arxiv.org/abs/2211.00332</guid>
  <link>http://arxiv.org/abs/2211.00332</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inoue_T/0/1/0/all/0/1&quot;&gt;Taichi Inoue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1&quot;&gt;Naoki Kitamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izumi_T/0/1/0/all/0/1&quot;&gt;Taisuke Izumi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masuzawa_T/0/1/0/all/0/1&quot;&gt;Toshimitsu Masuzawa&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We investigate the computational power of a single mobile agent in an
$n$-node graph with storage (i.e., node memory). It has been shown that the
system with one-bit agent memory and $O(1)$-bit storage is as powerful as the
one with $O(n)$-bit agent memory and $O(1)$-bit storage, and thus we focus on
the difference between one-bit memory agents and oblivious (i.e. zero-bit
memory) agents. While it has been also shown that their computational powers
are not equivalent, all the known results exhibiting such a difference rely on
the fact that oblivious agents cannot transfer any information from one side to
the other side across the bridge edge. Then our main question is stated as
follows: Are the computational powers of one-bit memory agents and oblivious
agents equivalent in 2-edge-connected graphs or not? The main contribution of
this paper is to answer this question positively under the relaxed assumption
that each node has $O(\log\Delta)$-bit storage ($\Delta$ is the maximum degree
of the graph). We present an algorithm of simulating any algorithm for a single
one-bit memory agent by one oblivious agent with $O(n^2)$-time overhead per
round. Our result implies that the topological structure of graphs
differentiating the computational powers of oblivious and non-oblivious agents
is completely characterized by the existence of bridge edges.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-02 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>A Near-Linear Kernel for Two-Parsimony Distance</title>
  <guid>http://arxiv.org/abs/2211.00378</guid>
  <link>http://arxiv.org/abs/2211.00378</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deen_E/0/1/0/all/0/1&quot;&gt;Elise Deen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iersel_L/0/1/0/all/0/1&quot;&gt;Leo van Iersel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janssen_R/0/1/0/all/0/1&quot;&gt;Remie Janssen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1&quot;&gt;Mark Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murakami_Y/0/1/0/all/0/1&quot;&gt;Yuki Murakami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeh_N/0/1/0/all/0/1&quot;&gt;Norbert Zeh&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The maximum parsimony distance $d_{\textrm{MP}}(T_1,T_2)$ and the
bounded-state maximum parsimony distance $d_{\textrm{MP}}^t(T_1,T_2)$ measure
the difference between two phylogenetic trees $T_1,T_2$ in terms of the maximum
difference between their parsimony scores for any character (with $t$ a bound
on the number of states in the character, in the case of
$d_{\textrm{MP}}^t(T_1,T_2)$). While computing $d_{\textrm{MP}}(T_1, T_2)$ was
previously shown to be fixed-parameter tractable with a linear kernel, no such
result was known for $d_{\textrm{MP}}^t(T_1,T_2)$. In this paper, we prove that
computing $d_{\textrm{MP}}^t(T_1, T_2)$ is fixed-parameter tractable for
all~$t$. Specifically, we prove that this problem has a kernel of size $O(k \lg
k)$, where $k = d_{\textrm{MP}}^t(T_1, T_2)$. As the primary analysis tool, we
introduce the concept of leg-disjoint incompatible quartets, which may be of
independent interest.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-02 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>On the zeroes of hypergraph independence polynomials</title>
  <guid>http://arxiv.org/abs/2211.00464</guid>
  <link>http://arxiv.org/abs/2211.00464</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Galvin_D/0/1/0/all/0/1&quot;&gt;David Galvin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+McKinley_G/0/1/0/all/0/1&quot;&gt;Gwen McKinley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Perkins_W/0/1/0/all/0/1&quot;&gt;Will Perkins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sarantis_M/0/1/0/all/0/1&quot;&gt;Michail Sarantis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tetali_P/0/1/0/all/0/1&quot;&gt;Prasad Tetali&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the locations of complex zeroes of independence polynomials of
bounded degree hypergraphs. For graphs, this is a long-studied subject with
applications to statistical physics, algorithms, and combinatorics. Results on
zero-free regions for bounded-degree graphs include Shearer&#39;s result on the
optimal zero-free disk, along with several recent results on other zero-free
regions. Much less is known for hypergraphs. We make some steps towards an
understanding of zero-free regions for bounded-degree hypergaphs by proving
that all hypergraphs of maximum degree $\Delta$ have a zero-free disk almost as
large as the optimal disk for graphs of maximum degree $\Delta$ established by
Shearer (of radius $\sim 1/(e \Delta)$). Up to logarithmic factors in $\Delta$
this is optimal, even for hypergraphs with all edge-sizes strictly greater than
$2$. We conjecture that for $k\ge 3$, $k$-uniform linear hypergraphs have a
much larger zero-free disk of radius $\Omega(\Delta^{- \frac{1}{k-1}} )$. We
establish this in the case of linear hypertrees.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-02 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Postdocs at Harvard at Harvard University (apply by November 21, 2022)</title>
  <guid>http://cstheory-jobs.org/2022/11/01/postdocs-at-harvard-at-harvard-university-apply-by-november-21-2022/</guid>
  <link>https://cstheory-jobs.org/2022/11/01/postdocs-at-harvard-at-harvard-university-apply-by-november-21-2022/</link>
  <description>
    &lt;p&gt;Multiple postdoc positions at Harvard University in theoretical CS, machine learning foundations, quantum computing, CS and society and more&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://windowsontheory.org/2022/11/01/postdocs-at-harvard/&quot;&gt;https://windowsontheory.org/2022/11/01/postdocs-at-harvard/&lt;/a&gt;&lt;br /&gt;
Email: achoat@seas.harvard.edu&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 17:19:24 UTC</pubDate>
  <author>CCI: jobs</author>
</item>

<item>
  <title>Postdocs at Harvard!</title>
  <guid>http://windowsontheory.org/?p=8462</guid>
  <link>https://windowsontheory.org/2022/11/01/postdocs-at-harvard/</link>
  <description>
    &lt;p&gt;The &lt;a href=&quot;https://mlfoundations.org/#opportunities&quot;&gt;ML Foundations &lt;/a&gt;and &lt;a href=&quot;https://toc.seas.harvard.edu/positions&quot;&gt;theory&lt;/a&gt; groups at Harvard are looking for postdocs for the coming academic year. &lt;/p&gt;



&lt;p&gt;There are also several other positions at Harvard, including at the &lt;a href=&quot;https://datascience.harvard.edu/data-science-postdoctoral-fellows&quot;&gt;Harvard Data Science Initiative (HDSI)&lt;/a&gt;, &lt;a href=&quot;https://cmsa.fas.harvard.edu/about-us/jobs/&quot;&gt; Center of Mathematical Sciences and Applications (CMSA),&lt;/a&gt; &lt;a href=&quot;https://cbs.fas.harvard.edu/research/theory/#swartz&quot;&gt;Swartz fellows&lt;/a&gt; at the Center for Brain Sciences,  the &lt;a href=&quot;https://academicpositions.harvard.edu/postings/11637&quot;&gt;George Carrier fellowship in applied mathematics,&lt;/a&gt;  the &lt;a href=&quot;https://crcs.seas.harvard.edu/apply&quot;&gt;Center for Research on Computation and Society (CRCS)&lt;/a&gt;,   &lt;a href=&quot;https://quantum.harvard.edu/external-candidates&quot;&gt;Harvard Quantum Initiative (HQI)&lt;/a&gt;. I hope that the newly announced &lt;a href=&quot;https://www.harvard.edu/kempner-institute/&quot;&gt;Kempner Institute&lt;/a&gt; will also be able to offer positions for the next academic year. &lt;/p&gt;



&lt;p&gt;All these positions have different foci, conditions, and the searches are run by different institutions, even if the set of potential mentors might be overlapping. Hence I strongly recommend that people apply to all positions that they are interested in. (&lt;/p&gt;



&lt;p&gt;These positions and others are posted on the &lt;a href=&quot;https://mlfoundations.org/#opportunities&quot;&gt;opportunities&lt;/a&gt; section of the ML foundations home page ( &lt;a href=&quot;https://mlfoundations.org/#opportunities&quot; rel=&quot;nofollow&quot;&gt;https://mlfoundations.org/#opportunities&lt;/a&gt; ). When I hear of new opportunities, I may update there and/or on &lt;a href=&quot;https://twitter.com/boazbaraktcs&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Boaz Barak&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 17:16:59 UTC</pubDate>
  <author>Windows on Theory</author>
</item>

<item>
  <title>Postdoc at Harvard University (apply by December 1, 2022)</title>
  <guid>http://cstheory-jobs.org/2022/11/01/postdoc-at-harvard-university-apply-by-december-1-2022/</guid>
  <link>https://cstheory-jobs.org/2022/11/01/postdoc-at-harvard-university-apply-by-december-1-2022/</link>
  <description>
    &lt;p&gt;The Harvard CS Theory Group invites applications for a variety of postdoctoral fellowships, including the Michael Rabin Postdoctoral Fellowship, postdocs in the Privacy Tools Project, Fairness in Prediction Algorithms, and Foundations in Machine Learning, and postdocs with individual faculty members.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://academicpositions.harvard.edu/postings/11762&quot;&gt;https://academicpositions.harvard.edu/postings/11762&lt;/a&gt;&lt;br /&gt;
Email: achoat@seas.harvard.edu&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 16:20:19 UTC</pubDate>
  <author>CCI: jobs</author>
</item>

<item>
  <title>Composition Basics</title>
  <guid>https://differentialprivacy.org/composition-basics/</guid>
  <link>https://differentialprivacy.org/composition-basics/</link>
  <description>
    &lt;p&gt;Our data is subject to many different uses. Many entities will have access to our data and those entities will perform many different analyses that involve our data. The greatest risk to privacy is that an attacker will combine multiple pieces of information from the same or different sources and that the combination of these will reveal sensitive details about us.
Thus we cannot study privacy leakage in a vacuum; it is important that we can reason about the accumulated privacy leakage over multiple independent analyses, which is known as &lt;em&gt;composition&lt;/em&gt;. We have &lt;a href=&quot;/privacy-composition/&quot;&gt;previously discussed&lt;/a&gt; why composition is so important for differential privacy.&lt;/p&gt;

&lt;p&gt;This is the first in a series of posts on &lt;em&gt;composition&lt;/em&gt; in which we will explain in more detail how compositoin analyses work.&lt;/p&gt;

&lt;p&gt;Composition is quantitative. The differential privacy guarantee of the overall system will depend on the number of analyses and the privacy parameters that they each satisfy. The exact relationship between these quantities can be complex. There are various composition theorems that give bounds on the overall parameters in terms of the parameters of the parts of the system.&lt;/p&gt;

&lt;p&gt;The simplest composition theorem is what is known as basic composition, which applies to pure \(\varepsilon\)-DP (although it can be extended to approximate \((\varepsilon,\delta)\)-DP):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Basic Composition)
Let \(M_1, M_2, \cdots, M_k : \mathcal{X}^n \to \mathcal{Y}\) be randomized algorithms. Suppose \(M_j\) is \(\varepsilon_j\)-DP for each \(j \in [k]\).
Define \(M : \mathcal{X}^n \to \mathcal{Y}^k\) by \(M(x)=(M_1(x),M_2(x),\cdots,M_k(x))\), where each algorithm is run independently. Then \(M\) is \(\varepsilon\)-DP for \(\varepsilon = \sum_{j=1}^k \varepsilon_j\).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;
Fix an arbitrary pair of neighbouring datasets \(x,x’ \in \mathcal{X}^n\) and output \(y \in \mathcal{Y}^k\).
To establish that \(M\) is \(\varepsilon\)-DP, we must show that \(e^{-\varepsilon} \le \frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} \le e^\varepsilon\). By independence, we have \[\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} = \frac{\prod_{j=1}^k\mathbb{P}[M_j(x)=y_j]}{\prod_{j=1}^k\mathbb{P}[M_j(x’)=y_j]} =  \prod_{j=1}^k \frac{\mathbb{P}[M_j(x)=y_j]}{\mathbb{P}[M_j(x’)=y_j]} \le \prod_{j=1}^k e^{\varepsilon_j} = e^{\sum_{j=1}^k \varepsilon_j} = e^\varepsilon,\] where the inequality follows from the fact that each \(M_j\) is \(\varepsilon_j\)-DP and, hence, \(e^{-\varepsilon_j} \le \frac{\mathbb{P}[M_j(x)=y_j]}{\mathbb{P}[M_j(x’)=y_j]} \le e^{\varepsilon_j}\). Similarly, \(\prod_{j=1}^k \frac{\mathbb{P}[M_j(x)=y_j]}{\mathbb{P}[M_j(x’)=y_j]} \ge \prod_{j=1}^k e^{-\varepsilon_j}\), which completes the proof. ∎&lt;/p&gt;

&lt;p&gt;Basic composition is already a powerful result, despite its simple proof; it establishes the versatility of differential privacy and allows us to begin reasoning about complex systems in terms of their building blocks. For example, suppose we have \(k\) functions \(f_1, \cdots, f_k : \mathcal{X}^n \to \mathbb{R}\) each of sensitivity \(1\). For each \(j \in [k]\), we know that adding \(\mathsf{Laplace}(1/\varepsilon)\) noise to the value of \(f_j(x)\) satisfies \(\varepsilon\)-DP. Thus, if we add independent \(\mathsf{Laplace}(1/\varepsilon)\) noise to each value \(f_j(x)\) for all \(j \in [k]\), then basic composition tells us that releasing this vector of \(k\) noisy values satisfies \(k\varepsilon\)-DP. If we want the overall system to be \(\varepsilon\)-DP, then we should add independent \(\mathsf{Laplace}(k/\varepsilon)\) noise to each value \(f_j(x)\).&lt;/p&gt;

&lt;h2 id=&quot;is-basic-composition-optimal&quot;&gt;Is Basic Composition Optimal?&lt;/h2&gt;

&lt;p&gt;If we want to release \(k\) values each of sensitivity \(1\) (as above) and have the overall release be \(\varepsilon\)-DP, then, using basic composition, we can add \(\mathsf{Laplace}(k/\varepsilon)\) noise to each value. The variance of the noise for each value is \(2k^2/\varepsilon^2\), so the standard deviation is \(\sqrt{2} k /\varepsilon\). In other words, the scale of the noise must grow linearly with the number of values \(k\) if the overall privacy and each value’s sensitivity is fixed. It is natural to wonder whether the scale of the Laplace noise can be reduced by improving the basic composition result. We now show that this is not possible.&lt;/p&gt;

&lt;p&gt;For each \(j \in [k]\), let \(M_j : \mathcal{X}^n \to \mathbb{R}\) be the algorithm that releases \(f_j(x)\) with \(\mathsf{Laplace}(k/\varepsilon)\) noise added. Let \(M : \mathcal{X}^n \to \mathbb{R}^k\) be the composition of these \(k\) algorithms. Then \(M_j\) is \(\varepsilon/k\)-DP for each \(j \in [k]\) and basic composition tells us that \(M\) is \(\varepsilon\)-DP. The question is whether \(M\) satisfies a better DP guarantee than this – i.e., does \(M\) satisfy \(\varepsilon_*\)-DP for some \(\varepsilon_*&amp;lt;\varepsilon\)?
Suppose we have neighbouring datasets \(x,x’\in\mathcal{X}^n\) such that \(f_j(x) = f_j(x’)+1\) for each \(j \in [k]\). Let \(y=(a,a,\cdots,a) \in \mathbb{R}^k\) for some \(a \ge \max_{j=1}^k f_j(x)\).
Then 
\[
        \frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} = \frac{\prod_{j=1}^k \mathbb{P}[f_j(x)+\mathsf{Laplace}(k/\varepsilon)=y_j]}{\prod_{j=1}^k \mathbb{P}[f_j(x’)+\mathsf{Laplace}(k/\varepsilon)=y_j]} 
\]
\[
         = \prod_{j=1}^k \frac{\frac{\varepsilon}{2k}\exp\left(-\frac{\varepsilon}{k} |y_j-f_j(x)| \right)}{\frac{\varepsilon}{2k}\exp\left(-\frac{\varepsilon}{k} |y_j-f_j(x’)| \right)} 
         = \prod_{j=1}^k \frac{\exp\left(-\frac{\varepsilon}{k} (y_j-f_j(x)) \right)}{\exp\left(-\frac{\varepsilon}{k} (y_j-f_j(x’)) \right)} 
\]
\[
         = \prod_{j=1}^k \exp\left(\frac{\varepsilon}{k}\left(f_j(x)-f_j(x’)\right)\right)
         = \exp\left( \frac{\varepsilon}{k} \sum_{j=1}^k \left(f_j(x)-f_j(x’)\right)\right)= e^\varepsilon,
\]
where the third equality removes the absolute values because \(y_j \ge f_j(x)\) and \(y_j \ge f_j(x’)\).
This shows that basic composition is optimal. For this example, we cannot prove a better guarantee than what is given by basic composition.&lt;/p&gt;

&lt;p&gt;Is there some other way to improve upon basic composition that circumvents this example? Note that we assumed that there are neighbouring datasets \(x,x’\in\mathcal{X}^n\) such that \(f_j(x) = f_j(x’)+1\) for each \(j \in [k]\). In some settings, no such worst case datasets exist. In that case, instead of scaling the noise linearly with \(k\), we can scale the Laplace noise according to the \(\ell_1\) sensitivity \(\Delta_1 := \sup_{x,x’ \in \mathcal{X}^n \atop \text{neighbouring}} \sum_{j=1}^k |f_j(x)-f_j(x’)|\).&lt;/p&gt;

&lt;p&gt;Instead of adding assumptions to the problem, we will look more closely at the example above.
We showed that there exists some output \(y \in \mathbb{R}^d\) such that \(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} = e^\varepsilon\).
However, such outputs \(y\) are very rare, as we require \(y_j \ge \max\{f_j(x),f_j(x’)\}\) for each \(j \in [k]\) where \(y_j = f_j(x) + \mathsf{Laplace}(k/\varepsilon)\). Thus, in order to observe an output \(y\) such that the likelihood ratio is maximal, all of the \(k\) Laplace noise samples must be positive, which happens with probability \(2^{-k}\). 
The fact that outputs \(y\) with maximal likelihood ratio are exceedingly rare turns out to be a general phenomenon and not specific to the example above.&lt;/p&gt;

&lt;p&gt;Can we improve on basic composition if we only ask for a high probability bound? That is, instead of demanding \(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} \le e^{\varepsilon_*}\) for all \(y \in \mathcal{Y}\), we demand \(\mathbb{P}_{Y \gets M(x)}\left[\frac{\mathbb{P}[M(x)=Y]}{\mathbb{P}[M(x’)=Y]} \le e^{\varepsilon_*}\right] \ge 1-\delta\) for some \(0 &amp;lt; \delta \ll 1\). Can we prove a better bound \(\varepsilon_* &amp;lt; \varepsilon\) in this relaxed setting? The answer turns out to be yes.&lt;/p&gt;

&lt;p&gt;The limitation of pure \(\varepsilon\)-DP is that events with tiny probability – which are negligible in real-world applications – can dominate the privacy analysis. This motivates us to move to relaxed notions of differential privacy, such as approximate \((\varepsilon,\delta)\)-DP and concentrated DP, which are less sensitive to low probability events.&lt;/p&gt;

&lt;h2 id=&quot;preview-advanced-composition&quot;&gt;Preview: Advanced Composition&lt;/h2&gt;

&lt;p&gt;By moving to approximate \((\varepsilon,\delta)\)-DP with \(\delta&amp;gt;0\), we can prove an asymptotically better composition theorem, which is known as &lt;em&gt;the advanced composition theorem&lt;/em&gt; &lt;strong&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/5670947&quot; title=&quot;Cynthia Dwork, Guy Rothblum, Salil Vadhan. Boosting and Differential Privacy. FOCS 2010.&quot;&gt;[DRV10]&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Advanced Composition Starting from Pure DP&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;)
Let \(M_1, M_2, \cdots, M_k : \mathcal{X}^n \to \mathcal{Y}\) be randomized algorithms. Suppose \(M_j\) is \(\varepsilon_j\)-DP for each \(j \in [k]\).
Define \(M : \mathcal{X}^n \to \mathcal{Y}^k\) by \(M(x)=(M_1(x),M_2(x),\cdots,M_k(x))\), where each algorithm is run independently. Then \(M\) is \((\varepsilon,\delta)\)-DP for any \(\delta&amp;gt;0\) with \[\varepsilon = \frac12 \sum_{j=1}^k \varepsilon_j^2 + \sqrt{2\log(1/\delta) \sum_{j=1}^k \varepsilon_j^2}.\]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Recall that basic composition gives \(\delta=0\) and \(\varepsilon = \sum_{j=1}^k \varepsilon_j\). That is, basic composition scales with the 1-norm of the vector \((\varepsilon_1, \varepsilon_2, \cdots, \varepsilon_k)\), whereas advanced composition scales with the 2-norm of this vector (and the squared 2-norm).
Neither bound strictly dominates the other. However, asymptotically (in a sense we will make precise in the next paragraph) advanced composition dominates basic composition.&lt;/p&gt;

&lt;p&gt;Suppose we have a fixed \((\varepsilon,\delta)\)-DP guarantee for the entire system and we must answer \(k\) queries of sensitivity \(1\).
Using basic composition, we can answer each query by adding \(\mathsf{Laplace}(k/\varepsilon)\) noise to each answer.
However, using advanced composition, we can answer each query by adding \(\mathsf{Laplace}(\sqrt{k/2\rho})\) noise to each answer, where&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;
\[\rho = \frac{\varepsilon^2}{4\log(1/\delta)+4\varepsilon}.\]
If the privacy parameters \(\varepsilon,\delta&amp;gt;0\) are fixed (which implies \(\rho\) is fixed) and \(k \to \infty\), we can see that asymptotically advanced composition gives noise per query scaling as \(\Theta(\sqrt{k})\), while basic composition results in noise scaling as \(\Theta(k)\).&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;In the next few posts we will explain how advanced composition works. We hope this conveys an intuitive understanding of composition and, in particular, how this \(\sqrt{k}\) asymptotic behaviour arises. If you want to read ahead, these posts are extracts from &lt;a href=&quot;https://arxiv.org/abs/2210.00597&quot;&gt;this book chapter&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This result generalizes to approximate DP. If instead we assume \(M_j\) is \((\varepsilon_j,\delta_j)\)-DP for each \(j \in [k]\), then the final composition is \((\varepsilon,\delta+\sum_{j=1}^k \delta_j)\)-DP with \(\varepsilon\) as before. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Adding \(\mathsf{Laplace}(\sqrt{k/2\rho})\) noise to a sensitivity-1 query ensures \(\varepsilon_j\)-DP for \(\varepsilon_j = \sqrt{2\rho/k}\). Hence \(\sum_{j=1}^k \varepsilon_j^2 = 2\rho\). Setting \(\rho = \frac{\varepsilon^2}{4\log(1/\delta)+4\varepsilon}\) ensures that \(\frac12 \sum_{j=1}^k \varepsilon_j^2 + \sqrt{2\log(1/\delta) \sum_{j=1}^k \varepsilon_j^2} = \rho + \sqrt{4\rho\log(1/\delta)} \le \varepsilon\). &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;&lt;p class=&quot;authors&quot;&gt;By &lt;/p&gt;
  </description>
  <pubDate>2022-11-01 15:45:00 UTC</pubDate>
  <author>DifferentialPrivacy.org</author>
</item>

<item>
  <title>The isotropy group of the matrix multiplication tensor</title>
  <guid>http://arxiv.org/abs/2210.16565</guid>
  <link>http://arxiv.org/abs/2210.16565</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burichenko_V/0/1/0/all/0/1&quot;&gt;V.P.Burichenko&lt;/a&gt;&lt;/p&gt;&lt;p&gt;By an {\em isotropy group} of a tensor $t\in V_1 \otimes V_2\otimes
V_3=\widetilde V$ we mean the group of all invertible linear transformations of
$\widetilde V$ that leave $t$ invariant and are compatible (in an obvious
sense) with the structure of tensor product on~$\widetilde V$. We consider the
case where $t$ is the structure tensor of multiplication map of rectangular
matrices. The isotropy group of this tensor was studied in 1970s by de Groote,
Strassen, and Brockett-Dobkin. In the present work we enlarge, make more
precise, expose in the language of group actions on tensor spaces, and endow
with proofs the results previously known. This is necessary for studying the
algorithms of fast matrix multiplication admitting symmetries. The latter seems
to be a promising new way for constructing fast algorithms.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Parallel Breadth-First Search and Exact Shortest Paths and Stronger Notions for Approximate Distances</title>
  <guid>http://arxiv.org/abs/2210.16351</guid>
  <link>http://arxiv.org/abs/2210.16351</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rozhon_V/0/1/0/all/0/1&quot;&gt;V&amp;#xe1;clav Rozho&amp;#x148;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haeupler_B/0/1/0/all/0/1&quot;&gt;Bernhard Haeupler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinsson_A/0/1/0/all/0/1&quot;&gt;Anders Martinsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grunau_C/0/1/0/all/0/1&quot;&gt;Christoph Grunau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuzic_G/0/1/0/all/0/1&quot;&gt;Goran Zuzic&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We introduce stronger notions for approximate single-source shortest-path
distances, show how to efficiently compute them from weaker standard notions,
and demonstrate the algorithmic power of these new notions and transformations.
One application is the first work-efficient parallel algorithm for computing
exact single-source shortest paths graphs -- resolving a major open problem in
parallel computing.
&lt;/p&gt;
&lt;p&gt;Given a source vertex in a directed graph with polynomially-bounded
nonnegative integer lengths, the algorithm computes an exact shortest path tree
in $m \log^{O(1)} n$ work and $n^{1/2+o(1)}$ depth. Previously, no parallel
algorithm improving the trivial linear depths of Dijkstra&#39;s algorithm without
significantly increasing the work was known, even for the case of undirected
and unweighted graphs (i.e., for computing a BFS-tree).
&lt;/p&gt;
&lt;p&gt;Our main result is a black-box transformation that uses $\log^{O(1)} n$
standard approximate distance computations to produce approximate distances
which also satisfy the subtractive triangle inequality (up to a
$(1+\varepsilon)$ factor) and even induce an exact shortest path tree in a
graph with only slightly perturbed edge lengths. These strengthened
approximations are algorithmically significantly more powerful and overcome
well-known and often encountered barriers for using approximate distances. In
directed graphs they can even be boosted to exact distances. This results in a
black-box transformation of any (parallel or distributed) algorithm for
approximate shortest paths in directed graphs into an algorithm computing exact
distances at essentially no cost. Applying this to the recent breakthroughs of
Fineman et al. for compute approximate SSSP-distances via approximate hopsets
gives new parallel and distributed algorithm for exact shortest paths.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Dynamic Bandits with an Auto-Regressive Temporal Structure</title>
  <guid>http://arxiv.org/abs/2210.16386</guid>
  <link>http://arxiv.org/abs/2210.16386</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qinyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golrezaei_N/0/1/0/all/0/1&quot;&gt;Negin Golrezaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouneffouf_D/0/1/0/all/0/1&quot;&gt;Djallel Bouneffouf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Multi-armed bandit (MAB) problems are mainly studied under two extreme
settings known as stochastic and adversarial. These two settings, however, do
not capture realistic environments such as search engines and marketing and
advertising, in which rewards stochastically change in time. Motivated by that,
we introduce and study a dynamic MAB problem with stochastic temporal
structure, where the expected reward of each arm is governed by an
auto-regressive (AR) model. Due to the dynamic nature of the rewards, simple
&quot;explore and commit&quot; policies fail, as all arms have to be explored
continuously over time. We formalize this by characterizing a per-round regret
lower bound, where the regret is measured against a strong (dynamic) benchmark.
We then present an algorithm whose per-round regret almost matches our regret
lower bound. Our algorithm relies on two mechanisms: (i) alternating between
recently pulled arms and unpulled arms with potential, and (ii) restarting.
These mechanisms enable the algorithm to dynamically adapt to changes and
discard irrelevant past information at a suitable rate. In numerical studies,
we further demonstrate the strength of our algorithm under different types of
non-stationary settings.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Flows, Scaling, and Entropy Revisited: a Unified Perspective via Optimizing Joint Distributions</title>
  <guid>http://arxiv.org/abs/2210.16456</guid>
  <link>http://arxiv.org/abs/2210.16456</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Altschuler_J/0/1/0/all/0/1&quot;&gt;Jason M. Altschuler&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this short expository note, we describe a unified algorithmic perspective
on several classical problems which have traditionally been studied in
different communities. This perspective views the main characters -- the
problems of Optimal Transport, Minimum Mean Cycle, Matrix Scaling, and Matrix
Balancing -- through the same lens of optimization problems over joint
probability distributions P(x,y) with constrained marginals. While this is how
Optimal Transport is typically introduced, this lens is markedly less
conventional for the other three problems. This perspective leads to a simple
and unified framework spanning problem formulation, algorithm development, and
runtime analysis.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>The Vector Balancing Constant for Zonotopes</title>
  <guid>http://arxiv.org/abs/2210.16460</guid>
  <link>http://arxiv.org/abs/2210.16460</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Heck_L/0/1/0/all/0/1&quot;&gt;Laurel Heck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Reis_V/0/1/0/all/0/1&quot;&gt;Victor Reis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Rothvoss_T/0/1/0/all/0/1&quot;&gt;Thomas Rothvoss&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The vector balancing constant $\mathrm{vb}(K,Q)$ of two symmetric convex
bodies $K,Q$ is the minimum $r \geq 0$ so that any number of vectors from $K$
can be balanced into an $r$-scaling of $Q$. A question raised by Schechtman is
whether for any zonotope $K \subseteq \mathbb{R}^d$ one has $\mathrm{vb}(K,K)
\lesssim \sqrt{d}$. Intuitively, this asks whether a natural geometric
generalization of Spencer&#39;s Theorem (for which $K = B^d_\infty$) holds. We
prove that for any zonotope $K \subseteq \mathbb{R}^d$ one has
$\mathrm{vb}(K,K) \lesssim \sqrt{d} \log \log \log d$. Our main technical
contribution is a tight lower bound on the Gaussian measure of any section of a
normalized zonotope, generalizing Vaaler&#39;s Theorem for cubes. We also prove
that for two different normalized zonotopes $K$ and $Q$ one has
$\mathrm{vb}(K,Q) \lesssim \sqrt{d \log d}$. All the bounds are constructive
and the corresponding colorings can be computed in polynomial time.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Improved Approximation Algorithms for Capacitated Vehicle Routing with Fixed Capacity</title>
  <guid>http://arxiv.org/abs/2210.16534</guid>
  <link>http://arxiv.org/abs/2210.16534</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jingyang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1&quot;&gt;Mingyu Xiao&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The Capacitated Vehicle Routing Problem (CVRP) is one of the most extensively
studied problems in combinatorial optimization. According to the property of
the demand of customers, we distinguish three variants of CVRP: unit-demand,
splittable and unsplittable. We consider $k$-CVRP in general metrics and
general graphs, where $k$ is the capacity of the vehicle and all the three
versions are APX-hard for each fixed $k\geq 3$.
&lt;/p&gt;
&lt;p&gt;In this paper, we give a $(5/2-\Theta(\sqrt{1/k}))$-approximation algorithm
for splittable and unit-demand $k$-CVRP and a
$(5/2+\ln2-\Theta(\sqrt{1/k}))$-approximation algorithm for unsplittable
$k$-CVRP (assume the approximation ratio for metric TSP is $\alpha=3/2$). Thus,
our approximation ratio is better than previous results for sufficient large
$k$, say $k\leq 1.7\times 10^7$.
&lt;/p&gt;
&lt;p&gt;For small $k$, we design independent algorithms by using more techniques to
get further improvements. For splittable and unit-demand cases, we improve the
ratio from $1.934$ to $1.500$ for $k=3$, and from $1.750$ to $1.667$ for $k=4$.
For the unsplittable case, we improve the ratio from $2.693$ to $1.500$ for
$k=3$, from $2.443$ to $1.750$ for $k=4$, and from $2.893$ to $2.157$ for
$k=5$.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>One Gradient Frank-Wolfe for Decentralized Online Convex and Submodular Optimization</title>
  <guid>http://arxiv.org/abs/2210.16790</guid>
  <link>http://arxiv.org/abs/2210.16790</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tuan-Anh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thang_N/0/1/0/all/0/1&quot;&gt;Nguyen Kim Thang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trystram_D/0/1/0/all/0/1&quot;&gt;Denis Trystram&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Decentralized learning has been studied intensively in recent years motivated
by its wide applications in the context of federated learning. The majority of
previous research focuses on the offline setting in which the objective
function is static. However, the offline setting becomes unrealistic in
numerous machine learning applications that witness the change of massive data.
In this paper, we propose \emph{decentralized online} algorithm for convex and
continuous DR-submodular optimization, two classes of functions that are
present in a variety of machine learning problems. Our algorithms achieve
performance guarantees comparable to those in the centralized offline setting.
Moreover, on average, each participant performs only a \emph{single} gradient
computation per time step. Subsequently, we extend our algorithms to the bandit
setting. Finally, we illustrate the competitive performance of our algorithms
in real-world experiments.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Halloween linkage</title>
  <guid>https://11011110.github.io/blog/2022/10/31/halloween-linkage</guid>
  <link>https://11011110.github.io/blog/2022/10/31/halloween-linkage.html</link>
  <description>
    &lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Perspective drawing often consists of mapping a curved world onto a flat screen. But &lt;a href=&quot;http://www.szegedicsaba.com/curvedscreen.htm&quot;&gt;what if you mapped a polyhedral world onto a curved screen&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mastodon.social/@curved_ruler/109174245124286751&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;)?&lt;/span&gt; Artwork by Szegedi Csaba, 1986. &lt;a href=&quot;https://www.szegedicsaba.hu/en&quot;&gt;His more recent work&lt;/a&gt; is different enough that it’s not obvious they’re the same person.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.wired.com/story/wikipedia-state-sponsored-disinformation/&quot;&gt;The hunt for Wikipedia’s disinformation moles&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109187943967850219&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;).&lt;/span&gt; &lt;em&gt;Wired&lt;/em&gt; on coordinated long-term state-level disinformation campaigns on Wikipedia. I couldn’t find their link to the research they report on, but it appears to be “&lt;a href=&quot;https://www.isdglobal.org/isd-publications/information-warfare-and-wikipedia/&quot;&gt;Information Warfare and Wikipedia&lt;/a&gt;“(by Carl Miller, Melanie Smith, Oliver Marsh, Kata Balint, Chris Inskip, and Francesca Visser of the Institute for Strategic Dialogue), which focuses on Russian disinformation in their war on Ukraine.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.quantamagazine.org/mathematicians-surprised-by-hidden-fibonacci-numbers-20221017/&quot;&gt;Fibonacci numbers and fractals hiding in geometric optimization problems in symplectic geometry&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109193514333145471&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;).&lt;/span&gt;  I don’t understand symplectic geometry at all but this article kind of makes me want to try. Based on &lt;a href=&quot;https://annals.math.princeton.edu/2012/175-3/p05&quot;&gt;a 2012 paper by Dusa McDuff and Felix Schlenk&lt;/a&gt;, and &lt;a href=&quot;https://doi.org/10.1007/978-3-030-80979-9_2&quot;&gt;a 2021 paper by 
Maria Bertozzi, Tara S. Holm, Emily Maw, Dusa McDuff, Grace T. Mwakyoma, Ana Rita Pires, and Morgan Weiler&lt;/a&gt; (&lt;a href=&quot;https://arxiv.org/abs/2010.08567&quot;&gt;arXiv:2010.08567&lt;/a&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;My colleague and coauthor &lt;a href=&quot;https://meetings.informs.org/wordpress/indianapolis2022/awards-hall/&quot;&gt;Vijay Vazirani wins the INFORMS John von Neumann Theory Prize&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109199025441051589&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;)&lt;/span&gt;  “for his fundamental and sustained contributions to the design of algorithms, including approximation algorithms, computational complexity theory, and algorithmic game theory, central to operations research and the management sciences”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lost documents from ancient Greek science are still being discovered in palimpsests, parchments that have been scraped clean and rewritten with something else, via modern multispectral imaging techniques &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109204919184604129&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;).&lt;/span&gt; The latest: &lt;a href=&quot;https://arstechnica.com/science/2022/10/part-of-lost-star-catalog-of-hipparchus-found-lurking-under-medieval-codex/&quot;&gt;a constellation from a lost star catalog by Hipparchus&lt;/a&gt; on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Codex_Climaci_Rescriptus&quot;&gt;Codex Climaci rescriptus&lt;/a&gt; from Saint Catherine’s Monastery on the Sinai Peninsula.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://xorshammer.com/2008/09/04/a-geometrically-natural-uncomputable-function/&quot;&gt;A geometrically natural uncomputable function&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109210523463519897&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;,&lt;/span&gt; &lt;a href=&quot;https://news.ycombinator.com/item?id=33268451&quot;&gt;via&lt;/a&gt;; from 2008), reporting on Alexander Nebutovsky’s “&lt;a href=&quot;https://doi.org/10.1002/cpa.3160480402&quot;&gt;Non-recursive functions, knots ‘with thick ropes’, and self-clenching ‘thick’ hyperspheres&lt;/a&gt;”. If you embed an &lt;span style=&quot;white-space:nowrap&quot;&gt;\(n\)-dimensional&lt;/span&gt; sphere into an &lt;span style=&quot;white-space:nowrap&quot;&gt;\((n+1)\)-dimensional&lt;/span&gt; one, you can always continuously move the embedding to the equator, but you may have to move the embedding closer to itself first. How much closer? It’s uncomputable!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://origami.kosmulski.org/blog/2022-10-23-fujimoto-books-public-domain&quot;&gt;Five geometric origami books by Shuzo Fujimoto now public-domain&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109218557060750300&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;,&lt;/span&gt; &lt;a href=&quot;https://news.ycombinator.com/item?id=33307845&quot;&gt;via&lt;/a&gt;). They were originally published in the 1970s and 1980s and focus on polyhedra, tessellations, modular origami, and the like. In Japanese, but with lots of diagrams.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Chris Purcell asks: &lt;a href=&quot;https://mathstodon.xyz/@ccppurcell/109223240056862107&quot;&gt;what do you call those fractal cracking patterns in paint, mud, etc.&lt;/a&gt;?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arstechnica.com/science/2022/10/the-feds-new-open-access-policy-whos-gonna-pay-for-it/&quot;&gt;Who will pay the publication fees under new grant-agency rules that all publications must be open access&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109232886231027636&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;)?&lt;/span&gt; “We face a growing risk that the ability to pay APCs – rather than the merits of the research – will determine what and who gets published.” … “I fear that forcing pay-to-play for every paper will end up amplifying existing inequities.”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Claims to have proven &lt;a href=&quot;https://doi.org/10.1007/s11225-022-10017-2&quot;&gt;the twin prime conjecture&lt;/a&gt; and &lt;a href=&quot;https://doi.org/10.1007/s11225-022-10015-4&quot;&gt;the existence of infinitely many Mersenne primes&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109237034145768039&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;)&lt;/span&gt; were published by Janusz Czelakowski in a peer-reviewed Polish logic journal. Beyond the obvious mistake in theorem 1.1 of the 1st link (the word “prime” is missing at an important point), other editors in a discussion on Wikipedia (where I found this) were skeptical that model theory and forcing are the right way to prove results like this. After &lt;a href=&quot;https://mathoverflow.net/questions/433278/czelakowskis-claimed-proof-of-the-twin-prime-conjecture&quot;&gt;the MathOverflow discussion&lt;/a&gt; turned up more serious errors, the editor-in-chief promised a retraction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Kite_(geometry)&quot;&gt;New Wikipedia Good Article on kites&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109243040526023165&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;),&lt;/span&gt; the quadrilaterals, not the toys on strings and not the birds. The article briefly mentions that every non-rhombus kite has sides that are bitangents to two unequal circles. I was unable to source and did not include (although a figure makes it obvious) that this can be reversed. Every two unequal circles have four bitangents forming sides of exactly three quadrilaterals: a convex kite, a concave kite, and an antiparallelogram.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The warm welcome given to edgelords on Twitter by the site’s new owner has led to another mass migration to Mastodon. One of the familiar names making the move is Vi Hart, who posted among other things about &lt;a href=&quot;https://mastodon.social/@vihart/109244906964274293&quot;&gt;60 whisks tangled together in icosahedral symmetry&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The usual version of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Chessboard_paradox&quot;&gt;chessboard paradox&lt;/a&gt; features seeming dissections of two different-area rectangles into the same set of triangles and trapezoids. Bruce35dc finds a variant with &lt;a href=&quot;https://mathstodon.xyz/@Bruce35dc/109246400468582681&quot;&gt;three dissections of the same rectangle&lt;/a&gt;, into seven pieces, six out of the seven, and five of the six!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is a truth universally acknowledged that unless you write your papers with a catchy start, the referees will get bored and stop reading before getting to the important parts. &lt;a href=&quot;https://igorpak.wordpress.com/2022/10/26/how-to-start-a-paper/&quot;&gt;Igor Pak has some advice&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109261439776959800&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;).&lt;/span&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A few years ago Google search was good enough, and Wikipedia’s bad enough, that I would regularly search Google for “something site:en.wikipedia.org”. Now the tables have turned: &lt;a href=&quot;https://www.theverge.com/23416056/wikipedia-app-vs-google-mobile-search&quot;&gt;James Vincent suggests switching to the Wikipedia mobile app as the default for searches&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109265698873986719&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;,&lt;/span&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2022-10-31/In_the_media&quot;&gt;via&lt;/a&gt;) since much of the time what you are trying to find is there or linked from there. Of course, many of my searches seek sources for Wikipedia content, so that wouldn’t help me…&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;authors&quot;&gt;By David Eppstein&lt;/p&gt;
  </description>
  <pubDate>2022-10-31 17:24:00 UTC</pubDate>
  <author>David Eppstein</author>
</item>

<item>
  <title>Oh right, quantum computing</title>
  <guid>https://scottaaronson.blog/?p=6784</guid>
  <link>https://scottaaronson.blog/?p=6784</link>
  <description>
    &lt;p&gt;These days, I often need to remind myself that, as an undergrad, grad student, postdoc, or professor, I&amp;#8217;ve now been doing quantum computing research for a quarter-century&amp;#8212;i.e., well over half of the subject&amp;#8217;s existence.  As a direct result, when I feel completely jaded about a new development in QC, it might actually be exciting.  When I feel moderately excited, it might actually be the most exciting thing for years.&lt;/p&gt;



&lt;p&gt;With that in mind:&lt;/p&gt;



&lt;hr class=&quot;wp-block-separator has-alpha-channel-opacity&quot;/&gt;



&lt;p&gt;(1) Last week National Public Radio&amp;#8217;s Marketplace &lt;a href=&quot;https://www.marketplace.org/2022/10/27/china-and-the-us-vie-for-quantum-computing-supremacy/amp/&quot;&gt;interviewed&lt;/a&gt; me, John Martinis, and others about the current state of quantum computing.  While the piece wasn&amp;#8217;t entirely hype-free, I&amp;#8217;m pleased to report that my own views were represented accurately!  To wit:&lt;/p&gt;



&lt;blockquote class=&quot;wp-block-quote&quot;&gt;&lt;p&gt;“There is a tsunami of hype about what quantum computers are going to revolutionize,” said Scott Aaronson, a professor of computer science at the University of Texas at Austin. “Quantum computing has turned into a word that venture capitalists or people seeking government funding will sprinkle on anything because it sounds good.”&lt;/p&gt;&lt;p&gt;Aaronson warned we can’t be certain that these computers will in fact revolutionize machine learning and finance and optimization problems.  “We can’t prove that there’s not a quantum algorithm that solves all these problems super fast, but we can’t even prove there’s not an algorithm for a conventional computer that does it,” he said.  [In the recorded version, they replaced this by a simpler but also accurate thought: namely, that we can&amp;#8217;t prove one way or the other whether there&amp;#8217;s a useful quantum advantage for these tasks.]&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;font-size: revert; color: initial;&quot;&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;



&lt;hr class=&quot;wp-block-separator has-alpha-channel-opacity&quot;/&gt;



&lt;p&gt;(2) I don&amp;#8217;t like to use this blog to toot my own research horn, but on Thursday my postdoc Jason Pollack and I released a paper, entitled &lt;a href=&quot;https://arxiv.org/pdf/2210.15601.pdf&quot;&gt;Discrete Bulk Reconstruction&lt;/a&gt;. And to be honest, I&amp;#8217;m pretty damned excited about it.  It represents about 8 months of Jason&amp;#8212;a cosmologist and string theorist who studied under Sean Carroll&amp;#8212;helping me understand &lt;a href=&quot;https://en.wikipedia.org/wiki/AdS/CFT_correspondence&quot;&gt;AdS/CFT&lt;/a&gt; in the language of the undergraduate CS curriculum, like min-cuts on undirected graphs, so that we could then look for polynomial-time algorithms to implement the holographic mapping from boundary quantum states to the spatial geometry in the bulk.  We drew heavily on previous work in the same direction, especially the already-seminal 2015 &lt;a href=&quot;https://arxiv.org/abs/1505.07839&quot;&gt;holographic entropy cone&lt;/a&gt; paper by Ning Bao et al.  But I&amp;#8217;d like to think that, among other things, our work represents a new frontier in just how accessible AdS/CFT itself can be made to CS and discrete math types.  Anyway, here&amp;#8217;s the abstract if you&amp;#8217;re interested:&lt;/p&gt;



&lt;blockquote class=&quot;wp-block-quote&quot;&gt;&lt;p&gt;According to the &lt;i&gt;AdS/CFT correspondence&lt;/i&gt;, the geometries of certain spacetimes are fully determined by quantum states that live on their boundaries &amp;#8212; indeed, by the von Neumann entropies of portions of those boundary states. This work investigates to what extent the geometries can be reconstructed from the entropies &lt;i&gt;in polynomial time&lt;/i&gt;. Bouland, Fefferman, and Vazirani (2019) argued that the AdS/CFT map can be exponentially complex if one wants to reconstruct regions such as the interiors of black holes. Our main result provides a sort of converse: we show that, in the special case of a single 1D boundary, if the input data consists of a list of entropies of &lt;i&gt;contiguous&lt;/i&gt; boundary regions, and if the entropies satisfy a single inequality called Strong Subadditivity, then we can construct a graph model for the bulk in linear time. Moreover, the bulk graph is planar, it has O(N&lt;sup&gt;2&lt;/sup&gt;) vertices (the information-theoretic minimum), and it&amp;#8217;s &amp;#8220;universal,&amp;#8221; with only the edge weights depending on the specific entropies in question. From a combinatorial perspective, our problem boils down to an &amp;#8220;inverse&amp;#8221; of the famous min-cut problem: rather than being given a graph and asked to find a min-cut, here we&amp;#8217;re given the values of min-cuts separating various sets of vertices, and need to find a weighted undirected graph consistent with those values. Our solution to this problem relies on the notion of a &amp;#8220;bulkless&amp;#8221; graph, which might be of independent interest for AdS/CFT. We also make initial progress on the case of multiple 1D boundaries &amp;#8212; where the boundaries could be connected via wormholes &amp;#8212; including an upper bound of O(N&lt;sup&gt;4&lt;/sup&gt;) vertices whenever a planar bulk graph exists (thus putting the problem into the complexity class NP).&lt;/p&gt;&lt;/blockquote&gt;



&lt;hr class=&quot;wp-block-separator has-alpha-channel-opacity&quot;/&gt;



&lt;p&gt;(3) Anand Natarajan and Chinmay Nirkhe posted a preprint entitled &lt;a href=&quot;https://arxiv.org/pdf/2210.15380.pdf&quot;&gt;A classical oracle separation between QMA and QCMA&lt;/a&gt;, which makes progress on a problem that&amp;#8217;s been raised on this blog all the way back to its inception.  A bit of context: &lt;a href=&quot;https://en.wikipedia.org/wiki/QMA&quot;&gt;QMA&lt;/a&gt;, Quantum Merlin-Arthur, captures what can be proven using a quantum state with poly(n) qubits as the proof, and a polynomial-time quantum algorithm as the verifier.  QCMA, or Quantum Classical Merlin-Arthur, is the same as QMA except that now the proof has to be classical.  A fundamental problem of quantum complexity theory, first raised by &lt;a href=&quot;https://arxiv.org/abs/quant-ph/0210077&quot;&gt;Aharonov and Naveh&lt;/a&gt; in 2002, is whether QMA=QCMA.  In 2007, &lt;a href=&quot;https://arxiv.org/abs/quant-ph/0604056&quot;&gt;Greg Kuperberg and I&lt;/a&gt; introduced the concept of quantum oracle separation&amp;#8212;that is, a unitary that can be applied in a black-box manner&amp;#8212;in order to show that there&amp;#8217;s a quantum oracle relative to which QCMA≠QMA.  In 2015, &lt;a href=&quot;https://arxiv.org/abs/1510.06750&quot;&gt;Fefferman and Kimmel&lt;/a&gt; improved this, to show that there&amp;#8217;s a &amp;#8220;randomized in-place&amp;#8221; oracle relative to which QCMA≠QMA.  Natarajan and Nirkhe now remove the &amp;#8220;in-place&amp;#8221; part, meaning the only thing still &amp;#8220;wrong&amp;#8221; with their oracle is that it&amp;#8217;s randomized.  Derandomizing their construction would finally settle this 20-year-old open problem (except, of course, for the minor detail of whether QMA=QCMA in the &amp;#8220;real,&amp;#8221; unrelativized world!).&lt;/p&gt;



&lt;hr class=&quot;wp-block-separator has-alpha-channel-opacity&quot;/&gt;



&lt;p&gt;(4) Oh right, the Google group reports the use of their superconducting processor to &lt;a href=&quot;https://arxiv.org/pdf/2210.10255.pdf&quot;&gt;simulate non-abelian anyons&lt;/a&gt;.  Cool.&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Scott&lt;/p&gt;
  </description>
  <pubDate>2022-10-31 05:26:35 UTC</pubDate>
  <author>Scott Aaronson</author>
</item>

<item>
  <title>List Agreement Expansion from Coboundary Expansion</title>
  <guid>http://arxiv.org/abs/2210.15714</guid>
  <link>http://arxiv.org/abs/2210.15714</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gotlib_R/0/1/0/all/0/1&quot;&gt;Roy Gotlib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaufman_T/0/1/0/all/0/1&quot;&gt;Tali Kaufman&lt;/a&gt;&lt;/p&gt;&lt;p&gt;One of the key components in PCP constructions are agreement tests. In
agreement test the tester is given access to subsets of fixed size of some set,
each equipped with an assignment. The tester is then tasked with testing
whether these local assignments agree with some global assignment over the
entire set. One natural generalization of this concept is the case where,
instead of a single assignment to each local view, the tester is given access
to $l$ different assignments for every subset. The tester is then tasked with
testing whether there exist $l$ global functions that agree with all of the
assignments of all of the local views.
&lt;/p&gt;
&lt;p&gt;In this work we present sufficient condition for a set system to exhibit this
generalized definition of list agreement expansion. This is, to our knowledge,
the first work to consider this natural generalization of agreement testing.
Despite initially appearing very similar to agreement expansion, list agreement
expansion seem to require a different set of techniques. This is due to the
fact that the natural extension of agreement testing does not suffice when
testing for list agreement, as list agreement crucially relies on a global
structure. It follows that if a local assignments satisfy list agreement they
must not only agree locally but also exhibit some additional structure. In
order to test for the existence of this additional structure we use a
connection between covering spaces of a high dimensional complex and its
coboundaries. We use this connection as a form of ``decoupling&#39;&#39;.
&lt;/p&gt;
&lt;p&gt;Moreover, we show that any set system that exhibits list agreement expansion
also supports direct sum testing. This is the first scheme for direct sum
testing that works regardless of the parity of the sizes of the local sets.
Prior to our work the schemes for direct sum testing were based on the parity
of the sizes of the local tests.
&lt;/p&gt;
  </description>
  <pubDate>2022-10-31 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>DESSERT: An Efficient Algorithm for Vector Set Search with Vector Set Queries</title>
  <guid>http://arxiv.org/abs/2210.15748</guid>
  <link>http://arxiv.org/abs/2210.15748</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engels_J/0/1/0/all/0/1&quot;&gt;Joshua Engels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coleman_B/0/1/0/all/0/1&quot;&gt;Benjamin Coleman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakshman_V/0/1/0/all/0/1&quot;&gt;Vihan Lakshman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1&quot;&gt;Anshumali Shrivastava&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the problem of \emph{vector set search} with \emph{vector set
queries}. This task is analogous to traditional near-neighbor search, with the
exception that both the query and each element in the collection are
\textit{sets} of vectors. We identify this problem as a core subroutine for
many web applications and find that existing solutions are unacceptably slow.
Towards this end, we present a new approximate search algorithm, DESSERT ({\bf
D}ESSERT {\bf E}ffeciently {\bf S}earches {\bf S}ets of {\bf E}mbeddings via
{\bf R}etrieval {\bf T}ables). DESSERT is a general tool with strong
theoretical guarantees and excellent empirical performance. When we integrate
DESSERT into ColBERT, a highly optimized state-of-the-art semantic search
method, we find a 2-5x speedup on the MSMarco passage ranking task with minimal
loss in recall, underscoring the effectiveness and practical applicability of
our proposal.
&lt;/p&gt;
  </description>
  <pubDate>2022-10-31 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Parallel Self-Avoiding Walks for a Low-Autocorrelation Binary Sequences Problem</title>
  <guid>http://arxiv.org/abs/2210.15962</guid>
  <link>http://arxiv.org/abs/2210.15962</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boskovic_B/0/1/0/all/0/1&quot;&gt;Borko Bo&amp;#x161;kovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herzog_J/0/1/0/all/0/1&quot;&gt;Jana Herzog&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brest_J/0/1/0/all/0/1&quot;&gt;Janez Brest&lt;/a&gt;&lt;/p&gt;&lt;p&gt;A low-autocorrelation binary sequences problem with a high figure of merit
factor represents a formidable computational challenge. An efficient parallel
computing algorithm is required to reach the new best-known solutions for this
problem. Therefore, we developed the $\mathit{sokol}_{\mathit{skew}}$ solver
for the skew-symmetric search space. The developed solver takes the advantage
of parallel computing on graphics processing units. The solver organized the
search process as a sequence of parallel and contiguous self-avoiding walks and
achieved a speedup factor of 387 compared with $\mathit{lssOrel}$, its
predecessor. The $\mathit{sokol}_{\mathit{skew}}$ solver belongs to stochastic
solvers and can not guarantee the optimality of solutions. To mitigate this
problem, we established the predictive model of stopping conditions according
to the small instances for which the optimal skew-symmetric solutions are
known. With its help and 99% probability, the $\mathit{sokol}_{\mathit{skew}}$
solver found all the known and seven new best-known skew-symmetric sequences
for odd instances from $L=121$ to $L=223$. For larger instances, the solver can
not reach 99% probability within our limitations, but it still found several
new best-known binary sequences. We also analyzed the trend of the best merit
factor values, and it shows that as sequence size increases, the value of the
merit factor also increases, and this trend is flatter for larger instances.
&lt;/p&gt;
  </description>
  <pubDate>2022-10-31 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Lecturer (Teaching Faculty) at Princeton University (apply by February 1, 2023)</title>
  <guid>http://cstheory-jobs.org/2022/10/30/lecturer-teaching-faculty-at-princeton-university-apply-by-february-1-2023/</guid>
  <link>https://cstheory-jobs.org/2022/10/30/lecturer-teaching-faculty-at-princeton-university-apply-by-february-1-2023/</link>
  <description>
    &lt;p&gt;The Princeton University CS Department invites applications to join our teaching faculty. The teaching load for teaching faculty is typically one course per semester plus independent work advising, leaving time to pursue other activities, such as engaging in research.&lt;/p&gt;
&lt;p&gt;Review of applications will begin November 2022 on a rolling basis for Fall 2023 appointments.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://lift.cs.princeton.edu/hiring.html&quot;&gt;https://lift.cs.princeton.edu/hiring.html&lt;/a&gt;&lt;br /&gt;
Email: pparedes@cs.princeton.edu&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </description>
  <pubDate>2022-10-30 21:11:58 UTC</pubDate>
  <author>CCI: jobs</author>
</item>

<item>
  <title>Assistant Professor of Teaching at University at Buffalo (apply by November 15, 2022)</title>
  <guid>http://cstheory-jobs.org/2022/10/30/assistant-professor-of-teaching-at-university-at-buffalo-apply-by-november-15-2022/</guid>
  <link>https://cstheory-jobs.org/2022/10/30/assistant-professor-of-teaching-at-university-at-buffalo-apply-by-november-15-2022/</link>
  <description>
    &lt;p&gt;We have an opening for a teaching position for teaching courses in algorithms and computer security. Successful candidates will help support the establishment of a new course-based MS program. The deadline is a soft deadline as we will consider candidates until the position is filled but applying by Nov 15 would ensure timely consideration of your application!&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://www.ubjobs.buffalo.edu/postings/36683&quot;&gt;https://www.ubjobs.buffalo.edu/postings/36683&lt;/a&gt;&lt;br /&gt;
Email: atri@buffalo.edu&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </description>
  <pubDate>2022-10-30 20:46:24 UTC</pubDate>
  <author>CCI: jobs</author>
</item>

<item>
  <title>Tenure track faculty position at University at Buffalo (apply by December 30, 2022)</title>
  <guid>http://cstheory-jobs.org/2022/10/30/tenure-track-faculty-position-at-university-at-buffalo-apply-by-december-30-2022/</guid>
  <link>https://cstheory-jobs.org/2022/10/30/tenure-track-faculty-position-at-university-at-buffalo-apply-by-december-30-2022/</link>
  <description>
    &lt;p&gt;We are hiring tenure track/tenured professor in theory (among four areas in total). We also have positions open for machine learning and security/privacy that would also be theory friendly.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://www.ubjobs.buffalo.edu/postings/37335&quot;&gt;https://www.ubjobs.buffalo.edu/postings/37335&lt;/a&gt;&lt;br /&gt;
Email: atri@buffalo.edu&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </description>
  <pubDate>2022-10-30 20:43:18 UTC</pubDate>
  <author>CCI: jobs</author>
</item>

<item>
  <title>What was the recent  Nobel Prize in Physics really about?(Guest Post)</title>
  <guid>tag:blogger.com,1999:blog-3722233.post-495471484673519962</guid>
  <link>http://blog.computationalcomplexity.org/2022/10/does-physics-nobel-prize-winner.html</link>
  <description>
    &lt;p&gt;&amp;nbsp;David Marcus was a Math major a year ahead of me at SUNY Stony brook (he graduated in 1979,&lt;/p&gt;&lt;p&gt;I graduated in 1980). He then got a PhD from MIT in Math, and is a reader of this blog.&amp;nbsp; Recently he emailed me that he thinks the current Nobel Prize Winners in Physics do not understand their own work. Is it true? Let&#39;s find out!&lt;/p&gt;&lt;p&gt;------------------------&lt;/p&gt;&lt;p&gt;(Guest blog from David Marcus)&lt;/p&gt;&lt;p&gt;2022 Nobel Prize in Physics Awarded for Experiments that Demonstrate Nonlocality&lt;/p&gt;&lt;p&gt;The 2022 Nobel Prize in Physics was recently awarded to experimenters who demonstrated that the world is nonlocal. The curious thing is that neither the writers of the Nobel Prize press release nor the recipients seem to understand that this is what they demonstrated.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;For example, the press release (see &lt;a href=&quot;https://www.nobelprize.org/prizes/physics/2022/press-release/&quot;&gt;here&lt;/a&gt;) says: &quot;John Clauser developed John Bell&#39;s ideas, leading to a practical experiment. When he took the measurements, they supported quantum mechanics by clearly violating a Bell inequality. This means that quantum mechanics cannot be replaced by a theory that uses hidden variables.&quot; That is not what the experiments mean, and the statement is false.&lt;/p&gt;&lt;p&gt;The word &quot;locality&quot; means that doing something here cannot instantly change something other there.&lt;/p&gt;&lt;p&gt;The experimental setup is the following: You prepare two particles, A and B, and send them in opposite directions so that they are far apart. You and your colleague do experiments on each particle at the same time. If you and your colleague perform the same experiment, then, from your experiment on A, you can predict with certainty the result of your colleague&#39;s experiment on B (and vice versa).&lt;/p&gt;&lt;p&gt;In a paper in 1935, Einstein, Podolsky, and Rosen pointed out that, assuming locality, the experimental results at A and B must be determined by the source that prepared the particles. They didn&#39;t actually say, &quot;assuming locality&quot;, but they implicitly assumed it. (If you disagree with them, please offer an alternative.)&lt;/p&gt;&lt;p&gt;In 1964, John Bell published his paper. In it, he considered three of the experiments that could be done on the particles A and B. Assuming the results are determined by the source (which follows from Einstein, Podolsky, and Rosen&#39;s argument), he derived an inequality on the correlations between the results of the three experiments on the two particles. The math is simple; for details, see&amp;nbsp;&lt;a href=&quot;http://www.scholarpedia.org/article/Bell%27s_theorem&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The Nobel Prize winners did experiments, and their results violated Bell&#39;s inequality (or similar inequalities). Hence, the world is nonlocal.&lt;/p&gt;&lt;p&gt;The simplest theory that agrees with experiment is Bohmian Mechanics. This is a deterministic theory of particles whose motion is governed by a wave (the wave function being the solution of the Schrödinger equation). Of course, Bohmian Mechanics is nonlocal, as is the world.&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By gasarch&lt;/p&gt;
  </description>
  <pubDate>2022-10-30 17:35:00 UTC</pubDate>
  <author>Computational Complexity</author>
</item>

<item>
  <title>Postdoc at Linköping University (apply by November 7, 2022)</title>
  <guid>http://cstheory-jobs.org/2022/10/29/postdoc-at-linkoping-university-apply-by-november-7-2022/</guid>
  <link>https://cstheory-jobs.org/2022/10/29/postdoc-at-linkoping-university-apply-by-november-7-2022/</link>
  <description>
    &lt;p&gt;Linköping University invites applications for postdoc positions. In particular, the Theoretical Computer Science Laboratory&lt;br /&gt;
is looking for postdocs with a strong background in theoretical computer science. Examples of research topics include (1) fine-grained complexity, (2) algebraic methods for CSPs, (3) parameterized complexity, and (4) randomized and approximation algorithms.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://liu.se/en/work-at-liu/vacancies?rmpage=job&amp;amp;rmjob=20079&amp;amp;rmlang=UK&quot;&gt;https://liu.se/en/work-at-liu/vacancies?rmpage=job&amp;amp;rmjob=20079&amp;amp;rmlang=UK&lt;/a&gt;&lt;br /&gt;
Email: peter.jonsson@liu.se&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </description>
  <pubDate>2022-10-29 05:59:45 UTC</pubDate>
  <author>CCI: jobs</author>
</item>

<item>
  <title>assisstant professor at University of British Columbia (apply by November 15, 2022)</title>
  <guid>http://cstheory-jobs.org/2022/10/28/assisstant-professor-at-university-of-british-columbia-apply-by-november-15-2022/</guid>
  <link>https://cstheory-jobs.org/2022/10/28/assisstant-professor-at-university-of-british-columbia-apply-by-november-15-2022/</link>
  <description>
    &lt;p&gt;The UBC math department seeks candidates for a tenure-track assistant professor position, with expertise in the mathematics of Machine Learning and AI. Specific topics of interest include, but are not limited to, neural nets, statistical learning theory, inverse problems, optimization, mathematical data science, and mathematics of information, with a strong theoretical component.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://www.mathjobs.org/jobs/list/20980&quot;&gt;https://www.mathjobs.org/jobs/list/20980&lt;/a&gt;&lt;br /&gt;
Email: exec-coord@math.ubc.ca&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </description>
  <pubDate>2022-10-28 19:41:58 UTC</pubDate>
  <author>CCI: jobs</author>
</item>

<item>
  <title>On Bryan Caplan and his new book</title>
  <guid>https://scottaaronson.blog/?p=6778</guid>
  <link>https://scottaaronson.blog/?p=6778</link>
  <description>
    &lt;p&gt;Yesterday I attended a lecture by George Mason University economist &lt;a href=&quot;https://betonit.substack.com/&quot;&gt;Bryan Caplan&lt;/a&gt;, who&amp;#8217;s currently visiting UT Austin, about his new book entitled &lt;em&gt;&lt;a href=&quot;https://www.amazon.com/Dont-Be-Feminist-Genuine-Justice/dp/B0BD3DFMMH/ref=asc_df_B0BD3DFMMH/&quot;&gt;Don’t Be a Feminist&lt;/a&gt;&lt;/em&gt;.  (&lt;a href=&quot;https://betonit.substack.com/p/aaronson-on-feminism-my-reply&quot;&gt;See also here&lt;/a&gt; for previous back-and-forth between me and Bryan about his book.)  A few remarks:&lt;/p&gt;



&lt;p&gt;(1) Maybe surprisingly, there were no protesters storming the lectern, no security detail, not even a single rotten vegetable thrown. About 30 people showed up, majority men but women too. They listened politely and asked polite questions afterward. One feminist civilly challenged Bryan during the Q&amp;amp;A about his gender pay gap statistics.&lt;/p&gt;



&lt;p&gt;(2) How is it that I got denounced &lt;a&gt;&lt;/a&gt;by half the planet for saying once, in a blog comment, that I agreed with 97% of feminism but had concerns with one particular way it was operationalized, whereas Bryan seems to be … not denounced in the slightest for publishing a book and going on a lecture tour about how he rejects feminism in its entirety as angry and self-pitying in addition to factually false? Who can explain this to me?&lt;/p&gt;



&lt;p&gt;(3) For purposes of his argument, Bryan defines feminism as &amp;#8220;the view that women are generally treated less fairly than men,&amp;#8221; rather than (say) &amp;#8220;the view that men and women &lt;em&gt;ought&lt;/em&gt; to be treated equally,&amp;#8221; or &amp;#8220;the radical belief that women are people,&amp;#8221; or other formulations that Bryan considers too obvious to debate.  He then rebuts feminism as he&amp;#8217;s defined it, by taking the audience on a horror tour of all the ways society treats men less fairly than women (expectations of doing dirty and dangerous work, divorce law, military drafts as in Ukraine right now, &amp;#8230;), as well as potentially benign explanations for apparent unfairness toward women, to argue that it&amp;#8217;s at least &lt;em&gt;debatable&lt;/em&gt; which sex gets the rawer deal on average.&lt;/p&gt;



&lt;p&gt;During the Q&amp;amp;A, I raised what I thought was the central objection to Bryan&amp;#8217;s relatively narrow definition of feminism. Namely that, by the standards of 150 years ago, Bryan is &lt;em&gt;obviously&lt;/em&gt; a feminist, and so am I, and so is everyone in the room. (Whereupon a right-wing business school professor interjected: &amp;#8220;please don’t make assumptions about me!&amp;#8221;)&lt;/p&gt;



&lt;p&gt;I explained that &lt;em&gt;this&lt;/em&gt; is why I call myself a feminist, despite agreeing with many of Bryan&amp;#8217;s substantive points: because I want no one to imagine for a nanosecond that, if I had the power, I&amp;#8217;d take gender relations back to how they were generations ago.&lt;/p&gt;



&lt;p&gt;Bryan replied that &gt;60% of Americans call themselves non-feminists in surveys. So, he asked me rhetorically, do &lt;em&gt;all&lt;/em&gt; those Americans secretly yearn to take us back to the 19th century? Such a position, he said, seemed so absurdly uncharitable as not to be worth responding to.&lt;/p&gt;



&lt;p&gt;Reflecting about it on my walk home, I realized: actually, give or take the exact percentages, this is &lt;em&gt;precisely&lt;/em&gt; the progressive thesis. I.e., that just like at least a solid minority of Germans turned out to be totally fine with Nazism, however much they might&amp;#8217;ve denied it beforehand, so too at least a solid minority of Americans would be fine with&amp;#8212;if not ecstatic about&amp;#8212;&lt;em&gt;The Handmaid&amp;#8217;s Tale&lt;/em&gt; made real. Indeed, they&amp;#8217;d add, it&amp;#8217;s only vociferous progressive activism that stands between us and that dystopia.&lt;/p&gt;



&lt;p&gt;And if anyone were tempted to doubt this, progressives might point to the election of Donald Trump, the failed insurrection to maintain his power, and the repeal of &lt;em&gt;Roe&lt;/em&gt; as proof enough to last for a quadrillion years.&lt;/p&gt;



&lt;p&gt;Bryan would probably reply: why even waste time engaging with such a hysterical position? To me, though, the hysterical position sadly has more than a grain of truth to it. I &lt;em&gt;wish&lt;/em&gt; we lived in a world where there was no point in calling oneself a pro-democracy anti-racist feminist and a hundred other banal and obvious things. I just don&amp;#8217;t think that we do.&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Scott&lt;/p&gt;
  </description>
  <pubDate>2022-10-28 16:54:52 UTC</pubDate>
  <author>Scott Aaronson</author>
</item>

<item>
  <title>ACM survey on math requirements for the CS major</title>
  <guid>http://thmatters.wordpress.com/?p=1383</guid>
  <link>https://thmatters.wordpress.com/2022/10/28/acm-survey-on-math-requirements-for-the-cs-major/</link>
  <description>
    &lt;p&gt;The &lt;strong&gt;ACM/IEEE-CS/AAAI CS2023 Curricular Task Force &lt;/strong&gt;is working on updating the undergraduate CS curriculum guidelines for the next decade. They have distributed a survey about the role of math in that curriculum, which is of direct interest to the TCS community. Please consider taking the survey so your opinion is heard!&lt;/p&gt;



&lt;p&gt;From the Task Force:&lt;/p&gt;



&lt;p&gt;&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8211;&lt;/p&gt;



&lt;p&gt;Dear educator,&lt;/p&gt;



&lt;p&gt;What math should undergraduate Computer Science students know?&lt;/p&gt;



&lt;p&gt;The CS2023 Task Force is collecting (and will share!) input from the community on this very important topic both as a useful “sense of the community” for everyone and, pertinent to our immediate goal, to shape our decennial curricular recommendations.&lt;/p&gt;



&lt;p&gt;We invite you to fill out a survey: &lt;a href=&quot;https://tinyurl.com/7zjbu7pr&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;&gt;https://tinyurl.com/7zjbu7pr&lt;/a&gt;&lt;/p&gt;



&lt;p&gt;As you fill out this survey, we ask you to reflect on:&lt;/p&gt;



&lt;ul&gt;
&lt;li&gt;Discrete mathematics: student preparedness, topics covered, what’s missing?&lt;/li&gt;



&lt;li&gt;What should come beyond discrete mathematics, if anything?&lt;/li&gt;



&lt;li&gt;What do the new high-growth areas (AI, ML, quantum computing, data science) need by way of mathematical preparation?&lt;/li&gt;



&lt;li&gt;Do most CS jobs need much mathematics, and do current mathematical requirements pose a barrier to some populations of students?&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Thank you in advance for taking the time to fill out the survey!&lt;/p&gt;



&lt;p&gt;&lt;em&gt;If you believe that other colleagues in your department can contribute, please forward the survey link to them&lt;/em&gt;.&lt;/p&gt;



&lt;p&gt;Amruth Kumar and Rajendra Raj&lt;/p&gt;



&lt;p&gt;On behalf of the of the ACM/IEEE-CS/AAAI CS2023 Curricular Task Force&lt;/p&gt;



&lt;p&gt;NOTE: By participating, you agree that we may use your responses for this study; and that this data may be presented in aggregate form (with no personally identifying information) in articles or websites.&lt;/p&gt;



&lt;p&gt;&amp;#8212;&amp;#8212;&amp;#8211;&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shuchic&lt;/p&gt;
  </description>
  <pubDate>2022-10-28 15:09:38 UTC</pubDate>
  <author>Theory Matters</author>
</item>

<item>
  <title>Postdoc at University of Michigan (apply by January 10, 2023)</title>
  <guid>http://cstheory-jobs.org/2022/10/28/postdoc-at-university-of-michigan-apply-by-january-10-2023/</guid>
  <link>https://cstheory-jobs.org/2022/10/28/postdoc-at-university-of-michigan-apply-by-january-10-2023/</link>
  <description>
    &lt;p&gt;The Theory Group at the University of Michigan invites applications for postdoctoral position(s) beginning September 2023. The position will have an initial appointment for one year but may be extended depending on circumstances.&lt;/p&gt;
&lt;p&gt;Applicants should be recent PhDs with interests that align well with our ongoing research.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://forms.gle/P9SAVGhP3tu9rayZA&quot;&gt;https://forms.gle/P9SAVGhP3tu9rayZA&lt;/a&gt;&lt;br /&gt;
Email: thsa@umich.edu&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </description>
  <pubDate>2022-10-28 13:06:28 UTC</pubDate>
  <author>CCI: jobs</author>
</item>

<item>
  <title>Geodesic packing in graphs</title>
  <guid>http://arxiv.org/abs/2210.15325</guid>
  <link>http://arxiv.org/abs/2210.15325</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Manuel_P/0/1/0/all/0/1&quot;&gt;Paul Manuel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bresar_B/0/1/0/all/0/1&quot;&gt;Bostjan Bresar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Klavzar_S/0/1/0/all/0/1&quot;&gt;Sandi Klavzar&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Given a graph $G$, a geodesic packing in $G$ is a set of vertex-disjoint
maximal geodesics, and the geodesic packing number of $G$, ${\gpack}(G)$, is
the maximum cardinality of a geodesic packing in $G$. It is proved that the
decision version of the geodesic packing number is NP-complete. We also
consider the geodesic transversal number, ${\gt}(G)$, which is the minimum
cardinality of a set of vertices that hit all maximal geodesics in $G$. While
$\gt(G)\ge \gpack(G)$ in every graph $G$, the quotient ${\rm gt}(G)/{\rm
gpack}(G)$ is investigated. By using the rook&#39;s graph, it is proved that there
does not exist a constant $C &amp;lt; 3$ such that $\frac{{\rm gt}(G)}{{\rm
gpack}(G)}\le C$ would hold for all graphs $G$. If $T$ is a tree, then it is
proved that ${\rm gpack}(T) = {\rm gt}(T)$, and a linear algorithm for
determining ${\rm gpack}(T)$ is derived. The geodesic packing number is also
determined for the strong product of paths.
&lt;/p&gt;
  </description>
  <pubDate>2022-10-28 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>On Tsirelson pairs of C*-algebras</title>
  <guid>http://arxiv.org/abs/2210.15509</guid>
  <link>http://arxiv.org/abs/2210.15509</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Goldbring_I/0/1/0/all/0/1&quot;&gt;Isaac Goldbring&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hart_B/0/1/0/all/0/1&quot;&gt;Bradd Hart&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We introduce the notion of a Tsirelson pair of C*-algebras, which is a pair
of C*-algebras for which the space of quantum strategies obtained by using
states on the minimal tensor product of the pair and the space of quantum
strategies obtained by using states on the maximal tensor product of the pair
coincide. We exhibit a number of examples of such pairs that are ``nontrivial&#39;&#39;
in the sense that the minimal tensor product and the maximal tensor product of
the pair are not isomorphic. For example, we prove that any pair containing a
C*-algebra with Kirchberg&#39;s QWEP property is a Tsirelson pair. We then
introduce the notion of a C*-algebra with the Tsirelson property (TP) and
establish a number of closure properties for this class. We also show that the
class of C*-algebras with the TP form an axiomatizable class (in the sense of
model theory), but that this class admits no ``effective&#39;&#39; axiomatization.
&lt;/p&gt;
  </description>
  <pubDate>2022-10-28 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

</channel>
</rss>
