<?xml version="1.0"?>
<rss version="2.0">

<channel>
  <title>Theory of Computing Report</title>
  <link></link>
  <language>en</language>
  <description></description>


<item>
  <title>TR22-179 |  Round-vs-Resilience Tradeoffs for Binary Feedback Channels | 

	Raghuvansh Saxena, 

	Gillat Kol, 

	Zhijun Zhang, 

	Klim Efremenko, 

	Mark Braverman</title>
  <guid>https://eccc.weizmann.ac.il/report/2022/179</guid>
  <link>https://eccc.weizmann.ac.il/report/2022/179</link>
  <description>
    In a celebrated result from the $60$&amp;#39;s, Berlekamp showed that feedback can be used to increase the maximum fraction of adversarial noise that can be tolerated by binary error correcting codes from $1/4$ to $1/3$. However, his result relies on the assumption that feedback is &amp;quot;continuous&amp;quot;, i.e., after every utilization of the channel, the sender gets the symbol received by the receiver. While this assumption is natural in some settings, in other settings it may be unreasonable or too costly to maintain.

In this work, we initiate the study of round-restricted feedback channels, where the number $r$ of feedback rounds is possibly much smaller than the number of utilizations of the channel. Error correcting codes for such channels are protocols where the sender can ask for feedback at most $r$ times, and, upon a feedback request, it obtains all the symbols received since its last feedback request. 

We design such error correcting protocols for both the adversarial binary erasure channel and for the adversarial binary corruption (bit flip) channel. For the erasure channel, we give an exact characterization of the round-vs-resilience tradeoff by designing a (constant rate) protocol with $r$ feedback rounds, for every $r$, and proving that the noise resilience it achieves is optimal. For the corruption channel, we give a protocol with one feedback round and prove that its optimality hinges on a &amp;quot;clean&amp;quot; combinatorial conjecture about the maximum cut in weighted graphs.
  </description>
  <pubDate>2022-12-16 04:08:19 UTC</pubDate>
  <author>ECCC Papers</author>
</item>

<item>
  <title>A Lower Bound on the Constant in the Fourier Min-Entropy/Influence Conjecture</title>
  <guid>http://arxiv.org/abs/2212.07713</guid>
  <link>http://arxiv.org/abs/2212.07713</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_A/0/1/0/all/0/1&quot;&gt;Aniruddha Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_P/0/1/0/all/0/1&quot;&gt;Palash Sarkar&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We describe a new construction of Boolean functions. A specific instance of
our construction provides a 30-variable Boolean function having
min-entropy/influence ratio to be $128/45 \approx 2.8444$ which is presently
the highest known value of this ratio that is achieved by any Boolean function.
Correspondingly, $128/45$ is also presently the best known lower bound on the
universal constant of the Fourier min-entropy/influence conjecture.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-16 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>A Graphical #SAT Algorithm for Formulae with Small Clause Density</title>
  <guid>http://arxiv.org/abs/2212.08048</guid>
  <link>http://arxiv.org/abs/2212.08048</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laakkonen_T/0/1/0/all/0/1&quot;&gt;Tuomas Laakkonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meichanetzidis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Meichanetzidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wetering_J/0/1/0/all/0/1&quot;&gt;John van de Wetering&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the counting version of the Boolean satisfiability problem #SAT
using the ZH-calculus, a graphical language originally introduced to reason
about quantum circuits. Using this we find a natural extension of #SAT which we
call $\#SAT_\pm$, where variables are additionally labeled by phases, which is
GapP-complete. Using graphical reasoning, we find a reduction from #SAT to
$\#2SAT_\pm$ in the ZH-calculus. We observe that the DPLL algorithm for #2SAT
can be adapted to $\#2SAT_\pm$ directly and hence that Wahlstrom&#39;s
$O^*(1.2377^n)$ upper bound applies to $\#2SAT_\pm$ as well. Combining this
with our reduction from #SAT to $\#2SAT_\pm$ gives us novel upper bounds in
terms of clauses and variables that are better than $O^*(2^n)$ for small clause
densities of $\frac{m}{n} &amp;lt; 2.25$. This is to our knowledge the first
non-trivial upper bound for #SAT that is independent of clause size. Our
algorithm improves on Dubois&#39; upper bound for $\#kSAT$ whenever $\frac{m}{n} &amp;lt;
1.85$ and $k \geq 4$, and the Williams&#39; average-case analysis whenever
$\frac{m}{n} &amp;lt; 1.21$ and $k \geq 6$. We also obtain an unconditional upper
bound of $O^*(1.88^m)$ for $\#4SAT$ in terms of clauses only, and find an
improved bound on $\#3SAT$ for $1.2577 &amp;lt; \frac{m}{n} \leq \frac{7}{3}$. Our
results demonstrate that graphical reasoning can lead to new algorithmic
insights, even outside the domain of quantum computing that the calculus was
intended for.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-16 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>MABSplit: Faster Forest Training Using Multi-Armed Bandits</title>
  <guid>http://arxiv.org/abs/2212.07473</guid>
  <link>http://arxiv.org/abs/2212.07473</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiwari_M/0/1/0/all/0/1&quot;&gt;Mo Tiwari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_R/0/1/0/all/0/1&quot;&gt;Ryan Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Je-Yong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thrun_S/0/1/0/all/0/1&quot;&gt;Sebastian Thrun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piech_C/0/1/0/all/0/1&quot;&gt;Chris Piech&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shomorony_I/0/1/0/all/0/1&quot;&gt;Ilan Shomorony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Martin Jinye Zhang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Random forests are some of the most widely used machine learning models
today, especially in domains that necessitate interpretability. We present an
algorithm that accelerates the training of random forests and other popular
tree-based learning methods. At the core of our algorithm is a novel
node-splitting subroutine, dubbed MABSplit, used to efficiently find split
points when constructing decision trees. Our algorithm borrows techniques from
the multi-armed bandit literature to judiciously determine how to allocate
samples and computational power across candidate split points. We provide
theoretical guarantees that MABSplit improves the sample complexity of each
node split from linear to logarithmic in the number of data points. In some
settings, MABSplit leads to 100x faster training (an 99% reduction in training
time) without any decrease in generalization performance. We demonstrate
similar speedups when MABSplit is used across a variety of forest-based
variants, such as Extremely Random Forests and Random Patches. We also show our
algorithm can be used in both classification and regression tasks. Finally, we
show that MABSplit outperforms existing methods in generalization performance
and feature importance calculations under a fixed computational budget. All of
our experimental results are reproducible via a one-line script at
https://github.com/ThrunGroup/FastForest.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-16 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Performance Enhancement Strategies for Sparse Matrix-Vector Multiplication (SpMV) and Iterative Linear Solvers</title>
  <guid>http://arxiv.org/abs/2212.07490</guid>
  <link>http://arxiv.org/abs/2212.07490</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammed_T/0/1/0/all/0/1&quot;&gt;Thaha Mohammed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehmood_R/0/1/0/all/0/1&quot;&gt;Rashid Mehmood&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Iterative solutions of sparse linear systems and sparse eigenvalue problems
have a fundamental role in vital fields of scientific research and engineering.
The crucial computing kernel for such iterative solutions is the multiplication
of a sparse matrix by a dense vector. Efficient implementation of sparse
matrix-vector multiplication (SpMV) and linear solvers are therefore essential
and has been subjected to extensive research across a variety of computing
architectures and accelerators such as central processing units (CPUs),
graphical processing units (GPUs), many integrated cores (MICs), and field
programmable gate arrays (FPGAs). Unleashing the full potential of an
architecture/accelerator requires determining the factors that affect an
efficient implementation of SpMV. This article presents the first of its kind,
in-depth survey covering over two hundred state-of-the-art optimization schemes
for solving sparse iterative linear systems with a focus on computing SpMV. A
new taxonomy for iterative solutions and SpMV techniques common to all
architectures is proposed. This article includes reviews of SpMV techniques for
all architectures to consolidate a single taxonomy to encourage
cross-architectural and heterogeneous-architecture developments. However, the
primary focus is on GPUs. The major contributions as well as the primary,
secondary, and tertiary contributions of the SpMV techniques are first
highlighted utilizing the taxonomy and then qualitatively compared. A summary
of the current state of the research for each architecture is discussed
separately. Finally, several open problems and key challenges for future
research directions are outlined.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-16 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Correlating Theory and Practice in Finding Clubs and Plexes</title>
  <guid>http://arxiv.org/abs/2212.07533</guid>
  <link>http://arxiv.org/abs/2212.07533</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Figiel_A/0/1/0/all/0/1&quot;&gt;Aleksander Figiel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koana_T/0/1/0/all/0/1&quot;&gt;Tomohiro Koana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nichterlein_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Nichterlein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wunsche_N/0/1/0/all/0/1&quot;&gt;Niklas W&amp;#xfc;nsche&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Finding large &quot;cliquish&quot; subgraphs is a classic NP-hard graph problem. In
this work, we focus on finding maximum $s$-clubs and $s$-plexes, i.e., graphs
of diameter $s$ and graphs where each vertex is adjacent to all but $s$
vertices. Preprocessing based on Turing kernelization is a standard tool to
tackle these problems, especially on sparse graphs. We provide a new
parameterized analysis for the Turing kernelization and demonstrate their
usefulness in practice. Moreover, we provide evidence that the new theoretical
bounds indeed better explain the observed running times than the existing
theoretical running time bounds. To this end, we suggest a general method to
compare how well theoretical running time bounds fit to measured running times.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-16 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Exact fixed-radius nearest neighbor search with an application to clustering</title>
  <guid>http://arxiv.org/abs/2212.07679</guid>
  <link>http://arxiv.org/abs/2212.07679</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinye Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guttel_S/0/1/0/all/0/1&quot;&gt;Stefan G&amp;#xfc;ttel&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Fixed-radius nearest-neighbor search is a common database operation that
retrieves all data points within a user-specified distance to a query point.
There are efficient approximate nearest neighbor search algorithms that provide
fast query responses but they often have a very compute-intensive indexing
phase and require parameter tuning. Therefore, exact brute force and tree-based
search methods are still widely used. Here we propose a new fixed-radius
nearest neighbor search method that significantly improves over brute force and
tree-based methods in terms of index and query time, returns exact results, and
requires no parameter tuning. The method exploits a sorting of the data points
by their first principal component, thereby facilitating a reduction in query
search space. Further speedup is gained from an efficient implementation using
high-level Basic Linear Algebra Subprograms (BLAS). We provide theoretical
analysis of our method and demonstrate its practical performance when used
stand-alone and when applied within a clustering algorithm.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-16 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Min-max Submodular Ranking for Multiple Agents</title>
  <guid>http://arxiv.org/abs/2212.07682</guid>
  <link>http://arxiv.org/abs/2212.07682</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qingyun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1&quot;&gt;Sungjin Im&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moseley_B/0/1/0/all/0/1&quot;&gt;Benjamin Moseley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruilong Zhang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In the submodular ranking (SR) problem, the input consists of a set of
submodular functions defined on a ground set of elements. The goal is to order
elements for all the functions to have value above a certain threshold as soon
on average as possible, assuming we choose one element per time. The problem is
flexible enough to capture various applications in machine learning, including
decision trees.
&lt;/p&gt;
&lt;p&gt;This paper considers the min-max version of SR where multiple instances share
the ground set. With the view of each instance being associated with an agent,
the min-max problem is to order the common elements to minimize the maximum
objective of all agents -- thus, finding a fair solution for all agents. We
give approximation algorithms for this problem and demonstrate their
effectiveness in the application of finding a decision tree for multiple
agents.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-16 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Parameterized Algorithms for String Matching to DAGs: Funnels and Beyond</title>
  <guid>http://arxiv.org/abs/2212.07870</guid>
  <link>http://arxiv.org/abs/2212.07870</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caceres_M/0/1/0/all/0/1&quot;&gt;Manuel Caceres&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The problem of String Matching to Labeled Graphs (SMLG) asks to find all the
paths in a labeled graph $G = (V, E)$ whose spellings match that of an input
string $S \in \Sigma^m$. SMLG can be solved in quadratic $O(m|E|)$ time [Amir
et al., JALG], which was proven to be optimal by a recent lower bound
conditioned on SETH [Equi et al., ICALP 2019]. The lower bound states that no
strongly subquadratic time algorithm exists, even if restricted to directed
acyclic graphs (DAGs).
&lt;/p&gt;
&lt;p&gt;In this work we present the first parameterized algorithms for SMLG in DAGs.
Our parameters capture the topological structure of $G$. All our results are
derived from a generalization of the Knuth-Morris-Pratt algorithm [Park and
Kim, CPM 1995] optimized to work in time proportional to the number of
prefix-incomparable matches.
&lt;/p&gt;
&lt;p&gt;To obtain the parameterization in the topological structure of $G$, we first
study a special class of DAGs called funnels [Millani et al., JCO] and
generalize them to $k$-funnels and the class $ST_k$. We present several novel
characterizations and algorithmic contributions on both funnels and their
generalizations.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-16 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Automatic vehicle trajectory data reconstruction at scale</title>
  <guid>http://arxiv.org/abs/2212.07907</guid>
  <link>http://arxiv.org/abs/2212.07907</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanbing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gloudemans_D/0/1/0/all/0/1&quot;&gt;Derek Gloudemans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teoh_Z/0/1/0/all/0/1&quot;&gt;Zi Nean Teoh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lisa Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zachar_G/0/1/0/all/0/1&quot;&gt;Gergely Zach&amp;#xe1;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbour_W/0/1/0/all/0/1&quot;&gt;William Barbour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Work_D/0/1/0/all/0/1&quot;&gt;Daniel Work&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Vehicle trajectory data has received increasing research attention over the
past decades. With the technological sensing improvements such as
high-resolution video cameras, in-vehicle radars and lidars, abundant
individual and contextual traffic data is now available. However, though the
data quantity is massive, it is by itself of limited utility for traffic
research because of noise and systematic sensing errors, thus necessitates
proper processing to ensure data quality. We draw particular attention to
extracting high-resolution vehicle trajectory data from video cameras as
traffic monitoring cameras are becoming increasingly ubiquitous. We explore
methods for automatic trajectory data reconciliation, given &quot;raw&quot; vehicle
detection and tracking information from automatic video processing algorithms.
We propose a pipeline including a) an online data association algorithm to
match fragments that are associated to the same object (vehicle), which is
formulated as a min-cost network flow problem of a graph, and b) a trajectory
reconciliation method formulated as a quadratic program to enhance raw
detection data. The pipeline leverages vehicle dynamics and physical
constraints to associate tracked objects when they become fragmented, remove
measurement noise on trajectories and impute missing data due to
fragmentations. The accuracy is benchmarked on a sample of manually-labeled
data, which shows that the reconciled trajectories improve the accuracy on all
the tested input data for a wide range of measures. An online version of the
reconciliation pipeline is implemented and will be applied in a continuous
video processing system running on a camera network covering a 4-mile stretch
of Interstate-24 near Nashville, Tennessee.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-16 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Beyond Square-root Error: New Algorithms for Differentially Private All Pairs Shortest Distances</title>
  <guid>http://arxiv.org/abs/2212.07997</guid>
  <link>http://arxiv.org/abs/2212.07997</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1&quot;&gt;Chengyuan Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jie Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upadhyay_J/0/1/0/all/0/1&quot;&gt;Jalaj Upadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We consider the problem of releasing all pairs shortest path distances (APSD)
in a weighted undirected graph with differential privacy (DP) guarantee. We
consider the weight-level privacy introduced by Sealfon [PODS&#39;16], where the
graph topology is public but the edge weight is considered sensitive and
protected from inference via the released all pairs shortest distances. The
privacy guarantee ensures that the probability of differentiating two sets of
edge weights on the same graph differing by an $\ell_1$ norm of $1$ is bounded.
The goal is to minimize the additive error introduced to the released APSD
while meeting the privacy guarantee. The best bounds known (Chen et al.
[SODA&#39;23]; Fan et al. [Arxiv&#39;22]) is an $\tilde{O}(n^{2/3})$ additive error for
$\varepsilon$-DP and an $\tilde{O}(n^{1/2})$ additive error for $(\varepsilon,
\delta)$-DP.
&lt;/p&gt;
&lt;p&gt;In this paper, we present new algorithms with improved additive error bounds:
$\tilde{O}(n^{1/3})$ for $\varepsilon$-DP and $\tilde{O}(n^{1/4})$ for
$(\varepsilon, \delta)$-DP, narrowing the gap with the current lower bound of
$\tilde{\Omega}(n^{1/6})$ for $(\varepsilon, \delta)$-DP. The algorithms use
new ideas to carefully inject noises to a selective subset of shortest path
distances so as to control both `sensitivity&#39; (the maximum number of times an
edge is involved) and the number of these perturbed values needed to produce
each of the APSD output. In addition, we also obtain, for $(\varepsilon,
\delta)$-DP shortest distances on lines, trees and cycles, a lower bound of
$\Omega(\log{n})$ for the additive error through a formulation by the matrix
mechanism.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-16 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Privately Estimating a Gaussian: Efficient, Robust and Optimal</title>
  <guid>http://arxiv.org/abs/2212.08018</guid>
  <link>http://arxiv.org/abs/2212.08018</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alabi_D/0/1/0/all/0/1&quot;&gt;Daniel Alabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kothari_P/0/1/0/all/0/1&quot;&gt;Pravesh K. Kothari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tankala_P/0/1/0/all/0/1&quot;&gt;Pranay Tankala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkat_P/0/1/0/all/0/1&quot;&gt;Prayaag Venkat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fred Zhang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this work, we give efficient algorithms for privately estimating a
Gaussian distribution in both pure and approximate differential privacy (DP)
models with optimal dependence on the dimension in the sample complexity. In
the pure DP setting, we give an efficient algorithm that estimates an unknown
$d$-dimensional Gaussian distribution up to an arbitrary tiny total variation
error using $\widetilde{O}(d^2 \log \kappa)$ samples while tolerating a
constant fraction of adversarial outliers. Here, $\kappa$ is the condition
number of the target covariance matrix. The sample bound matches best
non-private estimators in the dependence on the dimension (up to a
polylogarithmic factor). We prove a new lower bound on differentially private
covariance estimation to show that the dependence on the condition number
$\kappa$ in the above sample bound is also tight. Prior to our work, only
identifiability results (yielding inefficient super-polynomial time algorithms)
were known for the problem. In the approximate DP setting, we give an efficient
algorithm to estimate an unknown Gaussian distribution up to an arbitrarily
tiny total variation error using $\widetilde{O}(d^2)$ samples while tolerating
a constant fraction of adversarial outliers. Prior to our work, all efficient
approximate DP algorithms incurred a super-quadratic sample cost or were not
outlier-robust. For the special case of mean estimation, our algorithm achieves
the optimal sample complexity of $\widetilde O(d)$, improving on a $\widetilde
O(d^{1.5})$ bound from prior work. Our pure DP algorithm relies on a recursive
private preconditioning subroutine that utilizes the recent work on private
mean estimation [Hopkins et al., 2022]. Our approximate DP algorithms are based
on a substantial upgrade of the method of stabilizing convex relaxations
introduced in [Kothari et al., 2022].
&lt;/p&gt;
  </description>
  <pubDate>2022-12-16 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>A Mutation Carol 2</title>
  <guid>https://rjlipton.wpcomstaging.com/?p=20626</guid>
  <link>https://rjlipton.wpcomstaging.com/2022/12/15/a-mutation-carol-2/</link>
  <description>
    &lt;p&gt;&lt;font color=&quot;#0044cc&quot;&gt;&lt;br /&gt;
&lt;em&gt;Ghosts of creations past and citations not present&lt;/em&gt;&lt;br /&gt;
&lt;font color=&quot;#000000&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://rjlipton.wpcomstaging.com/2022/12/15/a-mutation-carol-2/apipfj/&quot; rel=&quot;attachment wp-att-20628&quot;&gt;&lt;img data-attachment-id=&quot;20628&quot; data-permalink=&quot;https://rjlipton.wpcomstaging.com/2022/12/15/a-mutation-carol-2/apipfj/&quot; data-orig-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?fit=3264%2C3264&amp;amp;ssl=1&quot; data-orig-size=&quot;3264,3264&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;APIPFJ&quot; data-image-description=&quot;&quot; data-image-caption=&quot;&quot; data-medium-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?fit=300%2C300&amp;amp;ssl=1&quot; data-large-file=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?fit=600%2C600&amp;amp;ssl=1&quot; decoding=&quot;async&quot; loading=&quot;lazy&quot; src=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ-150x150.jpg?resize=150%2C150&amp;#038;ssl=1&quot; alt=&quot;&quot; width=&quot;150&quot; height=&quot;150&quot; class=&quot;alignright size-thumbnail wp-image-20628&quot; srcset=&quot;https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=150%2C150&amp;amp;ssl=1 150w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=300%2C300&amp;amp;ssl=1 300w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=1024%2C1024&amp;amp;ssl=1 1024w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=768%2C768&amp;amp;ssl=1 768w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=1536%2C1536&amp;amp;ssl=1 1536w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=2048%2C2048&amp;amp;ssl=1 2048w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=1200%2C1200&amp;amp;ssl=1 1200w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=800%2C800&amp;amp;ssl=1 800w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=400%2C400&amp;amp;ssl=1 400w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?resize=200%2C200&amp;amp;ssl=1 200w, https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/12/APIPFJ.jpg?w=1800&amp;amp;ssl=1 1800w&quot; sizes=&quot;(max-width: 150px) 100vw, 150px&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
Domenico Amalfitano, Ana Paiva, Alexis Inquel, Luis Pinto, Anna Rita Fasolino, and Ren&amp;eacute; Just are the authors of an article in this month&amp;#8217;s Communications of the &lt;a href=&quot;https://cacm.acm.org/magazines/2022/12&quot;&gt;ACM&lt;/a&gt;. Their &lt;a href=&quot;https://cacm.acm.org/magazines/2022/12/266928-how-do-java-mutation-tools-differ/fulltext&quot;&gt;article&lt;/a&gt; is on the program testing method called &lt;a href=&quot;https://en.wikipedia.org/wiki/Mutation_testing&quot;&gt;mutation&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;
Today we discuss how far back citations should go.&lt;/p&gt;
&lt;p&gt;
When I opened this month&amp;#8217;s CACM hardcopy and saw the title &amp;#8220;How do Java Mutation Tools Differ?,&amp;#8221; I looked at the article&amp;#8217;s references. Like most researchers, I am proud and enjoy seeing my work cited. Their article leads with the following as a major reference:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
[33] Jeff Offutt. &amp;#8220;A Mutation Carol: Past, Present, and Future.&amp;#8221; &lt;em&gt;Information and Software Technology&lt;/em&gt; &lt;b&gt;53&lt;/b&gt;, 10 (2011), 1098&amp;#8211;1107.
&lt;/ul&gt;
&lt;p&gt;
This paper is cited twice, sandwiched around a mention of a 2019 survey. The second time is for a definition of &lt;em&gt;mutation analysis&lt;/em&gt; as, &amp;#8220;the use of well-defined rules defined on syntactic descriptions to make systematic changes to the syntax or to objects developed from the syntax.&amp;#8221; There is only one citation dated before 2001, a 1992 paper by Offutt. &lt;/p&gt;
&lt;p&gt;
What isn&amp;#8217;t cited is anything from the more distant past, before the Internet, before &lt;a href=&quot;https://www.indystar.com/story/entertainment/2021/12/09/seinfeld-festivus-episode-netflix-christmas-holiday-show-larry-david/8842569002/&quot;&gt;Seinfeld&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Simpsons_Roasting_on_an_Open_Fire&quot;&gt;The Simpsons&lt;/a&gt;. In particular, not this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
Richard A. DeMillo, Richard J. Lipton, and Fred G. Sayward. &amp;#8220;Hints on Test Data Selection: Help for the Practicing Programmer.&amp;#8221; &lt;em&gt;IEEE Computer&lt;/em&gt; &lt;b&gt;11&lt;/b&gt;, 4 (1978), 34&amp;#8211;41.
&lt;/ul&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; Bringing Past to Present &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
I was shocked, then upset, and then amazed. This isn&amp;#8217;t &lt;em&gt;plagiarizing&lt;/em&gt;, but there is still a sense of using someone else&amp;#8217;s ideas as currency without giving credit. Or maybe mutation testing is now coin-of-the-realm? Whatever, I felt &lt;a href=&quot;https://www.rogerebert.com/reviews/scrooged-1988&quot;&gt;scrooged&lt;/a&gt;&amp;#8212;or rather, &amp;#8220;ghosted.&amp;#8221; &lt;/p&gt;
&lt;p&gt;
My disorientation was alleviated upon looking at the &amp;#8220;Mutation Carol&amp;#8221; paper after a prompt from Ken. It not only cites the 1978 paper but, with echoes of the Charles Dickens story, takes me all the way back to my school days:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt; &lt;/b&gt; &lt;em&gt; &amp;#8220;Legend has it that the first ideas of mutation analysis were postulated in 1971 in a class term paper by Richard Lipton [2]. Depending on who we ask, his professor, Dave Parnas, either thought mutation was a bad idea or a reasonably clever idea that was not worthy of a PhD dissertation. The first research project was started in the late 1970s by DeMillo (Georgia Tech), Lipton (Princeton), and Sayward (Yale).&amp;#8221; &lt;/em&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;p&gt;
The term paper that Offutt referred to is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
Richard J. Lipton. &amp;#8220;Fault diagnosis of computer programs.&amp;#8221; Technical Report, Student Report, Carnegie Mellon University, 1971.
&lt;/ul&gt;
&lt;p&gt;
I am not suggesting people should cite that. But a second kind of source to cite in an applied survey is the &lt;em&gt;first implementation&lt;/em&gt;. This source was far from tiny: Tim Budd&amp;#8217;s PhD dissertation titled &lt;a href=&quot;https://www.google.com/books/edition/_/V1b1vgEACAAJ?hl=en&quot;&gt;Mutation Analysis&lt;/a&gt; in 1980 from Yale University. Well, Offutt references Budd copiously in his next paragraphs, besides citing papers by him and others. And Offutt should know&amp;#8212;he was a PhD student of DeMillo later in the 1980s. &lt;/p&gt;
&lt;p&gt;
I guess&amp;#8212;letting my heart soften a little here&amp;#8212;the authors of the CACM paper figured that their major reference [33] sufficed for the record, all the more since its author was in the originators&amp;#8217; circle. But readers may not look at paper 33. Holding hardcopy, one cannot. This leads to a wider question.&lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; Citation Proprieties &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
The question is, (when) &lt;em&gt;should one cite a paper that one hasn&amp;#8217;t actually consulted?&lt;/em&gt; Ken calls this &amp;#8220;Transitive Citation.&amp;#8221; Is it a vice? Here are some considerations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
The citation could be based on memory of having read the paper in the past. Even if you didn&amp;#8217;t look at the paper while doing your current project, it may be material to your knowledge. &lt;/p&gt;
&lt;li&gt;
Supposing one never read the stem paper&amp;#8212;that a survey or monograph sufficed&amp;#8212;the stem paper may still be more accessible or concisely informative for readers. &lt;/p&gt;
&lt;li&gt;
The transitively cited papers may be used to set a context or tell a story. This is one reason many in computer science, especially for conference papers, use the &amp;#8220;alpha&amp;#8221; style of citation, like [DLS78] for the above paper. It takes less space that writing out the author names and spares readers who recognize the tag the interruption of going to the references. &lt;/p&gt;
&lt;li&gt;
On the other hand, it may be that the contents of the stem paper have attained the status of &lt;a href=&quot;https://libguides.sjsu.edu/plagiarism/what-does-not-need-to-be-cited&quot;&gt;common knowledge&lt;/a&gt; that does not need to be cited.
&lt;/ul&gt;
&lt;p&gt;
What is common knowledge can be tricky because it depends on the scope of the audience. It is not just what they know but how readily they can find sources. For instance, the above link from the San Jos&amp;eacute; State University Library lists the following as examples that need not be cited:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
Abraham Lincoln was the 16th President of the United States. &lt;/p&gt;
&lt;li&gt;
Sacramento is the capital of California. &lt;/p&gt;
&lt;li&gt;
A genome is all the DNA in an organism, including its genes.
&lt;/ul&gt;
&lt;p&gt;
The second item might not be known by a non-American (or even by an American) and the third presupposes memory of secondary school science. Yet the point is that they were established long ago and can be &amp;#8220;found in many sources&amp;#8221; as the link states. But in our case, the presence of many sources&amp;#8212;when they have a unique and agreed lower bound&amp;#8212;comes back to our original questions.&lt;/p&gt;
&lt;p&gt;
&lt;p&gt;&lt;H2&gt; Open Problems &lt;/H2&gt;&lt;/p&gt;
&lt;p&gt;&lt;p&gt;
What should we do about this paper? Did they violate basic citation rules? Or is it okay since our initial creation of the mutation method is well known to all those who work in the area&amp;#8212;and the impressive panoply of tools covered in their article go well beyond origins? &lt;/p&gt;
&lt;p&gt;
What do you think?  &lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By RJLipton+KWRegan&lt;/p&gt;
  </description>
  <pubDate>2022-12-15 23:18:00 UTC</pubDate>
  <author>Richard Lipton</author>
</item>

<item>
  <title>Linkage for the end of the Fall term</title>
  <guid>https://11011110.github.io/blog/2022/12/15/linkage-end-fall</guid>
  <link>https://11011110.github.io/blog/2022/12/15/linkage-end-fall.html</link>
  <description>
    &lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.math.columbia.edu/~woit/wordpress/?p=13181&quot;&gt;This week’s hype&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@johncarlosbaez/109438052685084970&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;,&lt;/span&gt; &lt;a href=&quot;https://www.math.columbia.edu/~woit/wordpress/?p=13209&quot;&gt;more&lt;/a&gt;, &lt;a href=&quot;https://www.math.columbia.edu/~woit/wordpress/?p=13229&quot;&gt;still more&lt;/a&gt;, &lt;a href=&quot;https://www.math.columbia.edu/~woit/wordpress/?p=13251&quot;&gt;even more&lt;/a&gt;, &lt;a href=&quot;https://www.math.columbia.edu/~woit/wordpress/?p=13256&quot;&gt;one more&lt;/a&gt;). And that’s just the &lt;em&gt;Not Even Wrong&lt;/em&gt; posts; see them for a more thorough link and media roundup. Short summary: physicists ran a simulation of a quantum physics model that is unrelated to the conventional notion of wormholes from general relativity, sharing only a name with it. They used a quantum computer for the simulation even though it could as well have been run on a classical computer. And then they screamed from the rooftops that they had created the world’s first wormhole, apparently deliberately misleading everyone who didn’t read the fine print (including many major media outlets and research administrators) into thinking that they had brought into existence a physical wormhole.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When I viewed &lt;a href=&quot;https://www.techradar.com/news/youre-not-wrong-websites-have-way-more-trackers-now&quot;&gt;a recent blog post complaining about the increasing number of trackers embedded on social media websites&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109448435744739337&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;),&lt;/span&gt; including stats obtained using uBlock origin, I found that uBlock origin blocked 17 items from the post. On the social media websites I frequent (&lt;a href=&quot;https://mathstodon.xyz/@11011110&quot;&gt;mathstodon.xyz&lt;/a&gt; and this blog), it blocks 0 items. Hmm.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Certain phrases, once so commonplace that you could use them in analogies to explain more abstruse mathematical concepts and be instantly understood, have fallen by the wayside &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109451664672868335&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;).&lt;/span&gt; If you try to use them in the same way now, you will be met by blank stares instead of understanding. They are archaic and need to be retired from this sort of use. Today’s example: “telephone line”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://homepages.gac.edu/~jsiehler/NoThree/noThree.html&quot;&gt;Playable puzzles based on the no-three-in-line problem&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@jsiehler/109439179003169135&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;).&lt;/span&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jOTTZtVPrgo&quot;&gt;Can the same net fold into two shapes&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@robinhouston/109449485664418535&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;)?&lt;/span&gt; Matt Parker explores what’s known about polyominoes that fold into more than one cuboid, with a nice shoutout to Demaine and O’Rourke’s &lt;em&gt;Geometric Folding Algorithms&lt;/em&gt;. Still unknown: can you find one that folds into more than three cuboids?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another batch of three new mathematical Wikipedia Good Articles &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109468237232647886&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;):&lt;/span&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;How many \(k\)-element subsets of \([1,n]\) can you find so that all pairs intersect? The answer is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Ko%E2%80%93Rado_theorem&quot;&gt;Erdős–Ko–Rado theorem&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;How many patterns of pairwise connections can the subscribers to a telephone system form? The answer is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Telephone_number_(mathematics)&quot;&gt;telephone number&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Which graphs can you draw so that all vertices are one unit apart? The answer is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Unit_distance_graph&quot;&gt;unit distance graph&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://mathstodon.xyz/@Danpiker/109467207392971380&quot;&gt;Helicoidal decomposition of a tetrahedron into four identical curvy shapes&lt;/a&gt;, closely related to the &lt;a href=&quot;https://incoherency.co.uk/blog/stories/hanayama-cast-marble.html&quot;&gt;Hanayama cast “Marble” puzzle&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109468194107944510&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;).&lt;/span&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://mathstodon.xyz/@stecks/109477453878139223&quot;&gt;Katie Steckles enjoyes the presence of a table of polar plots of real spherical harmonic amplitude from Wikipedia in an image search for “Twelve Days of Christmas”&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I recently learned, from &lt;a href=&quot;https://retractionwatch.com/2022/12/05/a-paper-used-capital-ts-instead-of-error-bars-but-wait-theres-more/&quot;&gt;an article about the entertainingly amateurish fake statistics in a bad paper they published&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109492742952418728&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;),&lt;/span&gt; that dubious journal publisher Hindawi was recently taken over by somewhat more reputable journal publisher Wiley, and that Wiley is rightly worried about the bad reputation Hindawi is casting on them.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As a &lt;a href=&quot;https://en.wikipedia.org/wiki/Wigner_crystal&quot;&gt;Wigner crystal&lt;/a&gt; is &lt;a href=&quot;https://mathstodon.xyz/@lisyarus@mastodon.gamedev.place/109485736245660088&quot;&gt;dynamically deformed&lt;/a&gt;, its grid defects move around “in a kind of cosmic way”. Simulations by Nikita Lisitsa and Ricky Reusser.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://techintersections.org/&quot;&gt;Tech Intersections: Women of Color in Computing&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@ellenspertus@mastodon.lol/109484842652745472&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;),&lt;/span&gt; upcoming conference at Mills College in Oakland, California, January 28.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://roganbrown.com/home.html&quot;&gt;Rogan Brown – Paper Sculptures&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@Mndah@mastodon.art/109460158785990968&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;).&lt;/span&gt; Delicately cut traceries resembling microfauna.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://mathstodon.xyz/@Danpiker/109512464477416714&quot;&gt;What aspect ratios of rectangles can be used to tile a square?&lt;/a&gt; &lt;a href=&quot;https://mathstodon.xyz/@johncarlosbaez/109517010782719784&quot;&gt;More&lt;/a&gt;. Two long Mastodon threads. A couple of pointers into the middle of them: For recursive \(1\)-to-\((n-1)\) guillotine partitions, there’s &lt;a href=&quot;https://mathstodon.xyz/@11011110/109514032375078374&quot;&gt;a nice recursive construction for the polynomials&lt;/a&gt; whose roots give you the possible aspect ratios. And &lt;a href=&quot;https://mathstodon.xyz/@11011110/109519145856607390&quot;&gt;one way to form subdivisions of a square into equally-oriented similar rectangles&lt;/a&gt; is to start with a &lt;a href=&quot;https://en.wikipedia.org/wiki/Squaring_the_square&quot;&gt;squared square&lt;/a&gt; or &lt;a href=&quot;http://www.squaring.net/sq/sr/sr.html&quot;&gt;squared rectangle&lt;/a&gt; and then scale the axes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Even those so old-fashioned as to become monks and build stone Gothic cathedrals in the wilderness &lt;a href=&quot;https://carmelitegothic.com/cnc-stone-carving/&quot;&gt;use 3d modeling and CNC machining instead of painstaking hand carving&lt;/a&gt; &lt;span style=&quot;white-space:nowrap&quot;&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109519963114960008&quot;&gt;\(\mathbb{M}\)&lt;/a&gt;,&lt;/span&gt; &lt;a href=&quot;https://news.ycombinator.com/item?id=33940043&quot;&gt;via&lt;/a&gt;). As they write, “Medieval builders were always on the forefront of the technology of their day … To build Gothic today, it must become a reality, not a romantic idea locked up forever in one’s head.” See also &lt;a href=&quot;https://www.arup.com/projects/sagrada-familia&quot;&gt;a related article on digital modeling for the Sagrada Familia&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;authors&quot;&gt;By David Eppstein&lt;/p&gt;
  </description>
  <pubDate>2022-12-15 17:45:00 UTC</pubDate>
  <author>David Eppstein</author>
</item>

<item>
  <title>FinTech is Dead, Long Live FinTech</title>
  <guid>tag:blogger.com,1999:blog-3722233.post-3824700556781513642</guid>
  <link>http://blog.computationalcomplexity.org/2022/12/fintech-is-dead-long-live-fintech.html</link>
  <description>
    &lt;p&gt;Bill &lt;a href=&quot;https://blog.computationalcomplexity.org/2022/12/commercials-are-not-logical-ftx-edition.html&quot;&gt;didn&#39;t feel&lt;/a&gt; he had the expertise to share new insights on the FTX affair. Never stopped me.&lt;/p&gt;&lt;p&gt;FTX is nothing short of corporate malfeasance in a poorly regulated industry, and since Bill had posted, Sam Bankman-Fried was arrested and charged with fraud and conspiracy. The whole affair has rippled through the cryptocurrency ecosphere and will likely lead to a much higher regulatory framework than the industry was hoping for.&lt;/p&gt;&lt;p&gt;Cryptocurrencies have had an &lt;a href=&quot;https://en.wikipedia.org/wiki/Cryptocurrency_bubble#2021%E2%80%932022_crash&quot;&gt;interesting 2022&lt;/a&gt; starting with the &lt;a href=&quot;https://www.youtube.com/watch?v=gI0oRtEu7s0&quot;&gt;Superbowl&lt;/a&gt; &lt;a href=&quot;https://youtu.be/hWMnbJJpeZc&quot;&gt;ads&lt;/a&gt; Bill was rallying against. FTX even sponsored &lt;a href=&quot;https://news.sportslogos.net/2021/10/09/explaining-the-ftx-patch-worn-by-mlb-umpires/baseball/&quot;&gt;baseball umpires&lt;/a&gt;, &lt;a href=&quot;https://frontofficesports.com/ftx-remains-attached-to-miami-heat-arena-month-after-bankruptcy/&quot;&gt;sports arenas&lt;/a&gt;&amp;nbsp;and fortune cookies including this prescient one&lt;/p&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaSJFlLuWRhcO_BHTa4HDGrDftf95SK4jBdpAbodo1hn2XhcSrJ01-HJuEpuut8tjd55UgMU_VGtKxTUv2XayM1VbEkkeWLyfZKQ1odpKo9mMXI0M4yuAHvEVNgroslbnrzZofIO14OmXLATmrwI9OnmGvV0NAqrc_S2ZAQpV2u8qAljo8sA/s767/PXL_20220621_174821537.jpg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;219&quot; data-original-width=&quot;767&quot; height=&quot;91&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaSJFlLuWRhcO_BHTa4HDGrDftf95SK4jBdpAbodo1hn2XhcSrJ01-HJuEpuut8tjd55UgMU_VGtKxTUv2XayM1VbEkkeWLyfZKQ1odpKo9mMXI0M4yuAHvEVNgroslbnrzZofIO14OmXLATmrwI9OnmGvV0NAqrc_S2ZAQpV2u8qAljo8sA/s320/PXL_20220621_174821537.jpg&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;p&gt;Those were in the glory times of FinTech, i.e. early 2022. Even before FTX had its problems, we had the Three Arrows Capital bankruptcy and the &lt;a href=&quot;https://en.wikipedia.org/wiki/Terra_(blockchain)#Collapse&quot;&gt;TerraUSD stablecoin collapse&lt;/a&gt;. Bitcoin lost over 60% of its value during the year.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Of all the problems we&#39;ve seen with cryptocurrency, it&#39;s never because of the crypto. I&#39;m still shocked how the person or people called Satoshi Nakamoto got it mostly right on the &lt;a href=&quot;https://bitcoin.org/bitcoin.pdf&quot;&gt;first try&lt;/a&gt;. They did fail to account for the energy needed to mine bitcoin once bitcoin became valuable, but on the other hand who could have predicted bitcoin would become so valuable. Certainly &lt;a href=&quot;https://blog.computationalcomplexity.org/2011/11/making-money-computationally-hard-way.html&quot;&gt;not me&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;I still don&#39;t see much of a market for most cryptocurrencies beyond speculation and illegal activities, though we have much of both. Nevertheless it feels weird in 2022 that we still use physical bills and coins. I am hopeful for a CBDC (Central Bank Digital Currency), not a stablecoin tied to a dollar, but US currency itself just in digital form.&lt;/p&gt;&lt;p&gt;We now use digital tickets for most events and transport. Even driver&#39;s licenses&amp;nbsp;and passports are moving digital--they only gets scanned anyway. There are a few remaining uses of paper--property titles, legal wills, social security cards, birth, death, marriage and divorce certificates. Perhaps we could use blockchain or even centralized databases to eliminate these as well.&lt;/p&gt;&lt;p&gt;Blockchain could also be helpful to track our digital goods, so we can use the music, movies, books, games, virtual clothes and accoutrements on different platforms without having to buy them twice. (HT &lt;a href=&quot;https://twitter.com/dsivakumar/status/1602484986845990914&quot;&gt;Siva&lt;/a&gt;)&lt;/p&gt;&lt;p&gt;The most important use of FinTech will be for those who have the least, those who pay large fees to have a bank account, cash a paycheck or send money to family overseas. FinTech can be a great democratizing tool, if we use it that way and not just as another get rich scheme.&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By Lance Fortnow&lt;/p&gt;
  </description>
  <pubDate>2022-12-15 16:57:00 UTC</pubDate>
  <author>Computational Complexity</author>
</item>

<item>
  <title>Discrete, continuous and continuized accelerations</title>
  <guid>https://francisbach.com/?p=7834</guid>
  <link>https://francisbach.com/continuized-acceleration/</link>
  <description>
    &lt;p class=&quot;has-text-align-justify justify-text&quot;&gt;In optimization, acceleration is the art of modifying an algorithm in order to obtain faster convergence. Building accelerations and explaining their performance have been the subject of a countless number of publications, see [2] for a review. In this blog post, we give a vignette of these discussions on a minimal but challenging example, Nesterov acceleration. We introduce and illustrate different frameworks to understand and design acceleration techniques: the discrete framework based on iterations, the continuous-time framework, and the recent continuized framework that we proposed in [1].&lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;First, let us describe the setting we use. Let \(f:\mathbb{R}^d \to \mathbb{R}\) be a convex and differentiable function, that we seek to minimize. We assume that \(f\) is \(L\)-smooth, i.e., $$\forall x,y \in \mathbb{R}^d, \qquad f(y) \leq f(x) + \langle \nabla f(x), y-x \rangle + \frac{L}{2} \Vert y-x \Vert^2 \, , $$ and that it is \(\mu\)-strongly convex for some \(\mu &amp;gt; 0\), i.e., $$\forall x,y \in \mathbb{R}^d, \qquad f(y) \geq f(x) + \langle \nabla f(x), y-x \rangle + \frac{\mu}{2} \Vert y-x \Vert^2 \, .$$We had already made these assumptions in &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://francisbach.com/computer-aided-analyses/&quot; target=&quot;_blank&quot;&gt;a previous blog post on computer-aided analyses in optimization&lt;/a&gt;. In this setting, the function \(f\) has a unique minimizer that we denote \(x_*\). &lt;/p&gt;



&lt;figure class=&quot;wp-block-image size-full is-resized&quot;&gt;&lt;img src=&quot;https://francisbach.com/wp-content/uploads/2022/12/SmoothStronglyConvex.png&quot; alt=&quot;&quot; class=&quot;wp-image-8604&quot; width=&quot;840&quot; height=&quot;363&quot; srcset=&quot;https://francisbach.com/wp-content/uploads/2022/12/SmoothStronglyConvex.png 1002w, https://francisbach.com/wp-content/uploads/2022/12/SmoothStronglyConvex-300x130.png 300w, https://francisbach.com/wp-content/uploads/2022/12/SmoothStronglyConvex-768x333.png 768w, https://francisbach.com/wp-content/uploads/2022/12/SmoothStronglyConvex-850x368.png 850w&quot; sizes=&quot;(max-width: 840px) 100vw, 840px&quot; /&gt;&lt;/figure&gt;



&lt;p class=&quot;has-text-align-justify&quot;&gt;In this figure, the blue function \(f\) is \(L\)-smooth and \(\mu\)-strongly convex: it is possible to create global upper and lower quadratic bounds from every \(x \in \mathbb{R}^d\) with respective curvatures \(L\) and \(\mu\).&lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;Of course, smoothness and strong convexity are not the only assumptions under which acceleration is studied (see [2] for a larger variety of accelerations). In particular, in [1], these is a discussion similar to the one of this blog post in the case where strong convexity is not assumed. &lt;/p&gt;



&lt;h2&gt;Discrete-time acceleration&lt;/h2&gt;



&lt;p class=&quot;justify-text&quot;&gt;The basic algorithm for minimizing \(f\) is gradient descent: we start from an initial guess \(x_0 \in \mathbb{R}^d\) and iterate $$x_{k+1} = x_k \, &amp;#8211; \gamma \nabla f(x_k) \, ,$$where \(\gamma\) is a step size. &lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;Let us show how this performs on an example in dimension \(d=2\). In the plot below, the thin curves represent the level sets of the function \(f\). With a slight abuse of notation, we denote \(x_1\) and \(x_2\) the coordinates of the vector \(x \in \mathbb{R}^2\).&lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-full is-resized&quot;&gt;&lt;img loading=&quot;lazy&quot; src=&quot;https://francisbach.com/wp-content/uploads/2022/12/gd-4.gif&quot; alt=&quot;&quot; class=&quot;wp-image-8662&quot; width=&quot;600&quot; height=&quot;450&quot;/&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p class=&quot;justify-text&quot;&gt;When \(\gamma \leq \frac{1}{L}\), we have the linear convergence rate $$f(x_k) \, &amp;#8211; f(x_*) \leq \frac{L}{2} \Vert x_0 \, &amp;#8211; x_* \Vert^2 (1-\gamma \mu)^k \, ,$$see [3, Theorem 2.1.15] for instance. This upper bound is minimized at \(\gamma = \frac{1}{L}\), leading to a \(O\big( \left( 1\, &amp;#8211; \frac{\mu}{L} \right)^k \big)\) convergence rate. Although this convergence rate is linear, it can be slow when the condition number \(\frac{L}{\mu} \) is large. &lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;To tackle this issue, Nesterov [3] proposed an alternative algorithm: $$\begin{aligned}&lt;br&gt;&amp;amp;y_k = x_k + \frac{\sqrt{\gamma\mu}}{1 + \sqrt{\gamma\mu}}(z_k-x_k) \, , \\ &amp;amp;x_{k+1} = y_k \, &amp;#8211; \gamma \nabla f(y_k) \, , \\ &amp;amp;z_{k+1} = z_k + \sqrt{\gamma\mu}(y_k-z_k)  \, &amp;#8211;  \sqrt{\frac{\gamma}{\mu}}\nabla f(y_k ) \, . \end{aligned}$$&lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-full is-resized&quot;&gt;&lt;img loading=&quot;lazy&quot; src=&quot;https://francisbach.com/wp-content/uploads/2022/12/nest-4.gif&quot; alt=&quot;&quot; class=&quot;wp-image-8663&quot; width=&quot;600&quot; height=&quot;450&quot;/&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p class=&quot;justify-text&quot;&gt;Nesterov proved the following convergence bound for his algorithm: if \(\gamma \leq \frac{1}{L}\), $$f (x_k) \, &amp;#8211; f(x_*) \leq \left(f(x_0) \, &amp;#8211; f(x_*) + \frac{\mu}{2} \Vert z_0 \, &amp;#8211; x_* \Vert^2 \right) \left(1\,  &amp;#8211; \sqrt{\gamma\mu}\right)^k \, .$$ Again, this upper bound is minimized at \(\gamma = \frac{1}{L}\); this leads to a \(O\left( \left( 1 \, &amp;#8211; \sqrt{\frac{\mu}{L}} \right)^k \right)\) convergence rate. The acceleration lies in this new square root dependence in the condition number, that enables significant speed-ups in practice. To show this, let us compare the sequence \(x_k\) of Nesterov acceleration with gradient descent.  &lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-full is-resized&quot;&gt;&lt;img loading=&quot;lazy&quot; src=&quot;https://francisbach.com/wp-content/uploads/2022/12/gd-nest-2.gif&quot; alt=&quot;&quot; class=&quot;wp-image-8664&quot; width=&quot;600&quot; height=&quot;450&quot;/&gt;&lt;/figure&gt;&lt;/div&gt;

&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-full is-resized&quot;&gt;&lt;img loading=&quot;lazy&quot; src=&quot;https://francisbach.com/wp-content/uploads/2022/12/nest-dg-performances-1.png&quot; alt=&quot;&quot; class=&quot;wp-image-8665&quot; width=&quot;600&quot; height=&quot;450&quot; srcset=&quot;https://francisbach.com/wp-content/uploads/2022/12/nest-dg-performances-1.png 1200w, https://francisbach.com/wp-content/uploads/2022/12/nest-dg-performances-1-300x225.png 300w, https://francisbach.com/wp-content/uploads/2022/12/nest-dg-performances-1-1024x768.png 1024w, https://francisbach.com/wp-content/uploads/2022/12/nest-dg-performances-1-768x576.png 768w, https://francisbach.com/wp-content/uploads/2022/12/nest-dg-performances-1-850x638.png 850w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot; /&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p class=&quot;justify-text&quot;&gt;We observe that the asymptotic performance of Nesterov acceleration is much better. Of course, comparing the two algorithms in terms of the iteration number \(k\) is unfair to gradient descent, as the complexity per iteration of Nesterov acceleration is higher. However, as suggested by the theoretical convergence bound, the speed-up provided by Nesterov acceleration is largely worth the additional complexity per iteration on badly conditioned problems. &lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;From a high-level perspective, Nesterov acceleration iterates over several variables, alternating between gradient steps (always with respect to the gradient at \(y_k\)) and mixing steps, where the running value of a variable is replaced by a linear combination of the other variables. However, the precise way gradient and mixing steps are coupled is rather mysterious, and the improved convergence bound relies heavily on the detailed structure of the iterations.&lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;Only an inspection of the proof of the convergence bound [2,3] can provide a rigorous understanding of Nesterov acceleration and its performance. Many works contributed to interpret and motivate the iteration [11-15]. In this blog post, we try to give some high-level intuitions through the continuous and continuized points of view on Nesterov acceleration. &lt;/p&gt;



&lt;h2&gt;Continuous-time acceleration&lt;/h2&gt;



&lt;p class=&quot;justify-text&quot;&gt;Continuous-time approaches propose to gain insights on the above algorithms through their limit as the stepsize \(\gamma\) vanishes [4,10]. This was already the subject of a &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://francisbach.com/gradient-flows/&quot; target=&quot;_blank&quot;&gt;previous blog post&lt;/a&gt;. In this approach, one needs to rescale the iterate number \(k\) as \(\gamma \to 0\) in order to obtain a non-degenerate limit. &lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;For instance, to study gradient descent, we define a rescaled time variable \(t = \gamma k \in \mathbb{R}_{\geq 0}\) and the reparametrized iterates \(X(t) = X(\gamma k) = x_k\). Then for small \(\gamma\), $$\frac{dX}{dt}(t) \approx \frac{X(t+\gamma)-X(t)}{\gamma} = \frac{x_{t/\gamma+1}-x_{t/\gamma}}{\gamma} = \, &amp;#8211; \nabla f(x_{t/\gamma}) = \, &amp;#8211; \nabla f(X(t)) \, .$$In the limit \(\gamma \to 0\), the approximation of the derivative becomes exact and this gives the gradient flow equation $$\frac{dX}{dt}(t) = \, &amp;#8211; \nabla f(X(t)) \, .$$&lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-full is-resized&quot;&gt;&lt;img loading=&quot;lazy&quot; src=&quot;https://francisbach.com/wp-content/uploads/2022/12/gd-gf-2.gif&quot; alt=&quot;&quot; class=&quot;wp-image-8666&quot; width=&quot;600&quot; height=&quot;450&quot;/&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p class=&quot;justify-text&quot;&gt;Note that in the simulation above, the gradient descents are aligned with the gradient flow in terms of their common time parameter \(t = \gamma k\). However, as the stepsize \(\gamma\) is not the same in the two gradient descents, the number of iterations is also not the same. &lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;In [4,5], a similar limit is taken for the iterates \(x_k\) of Nesterov acceleration. As the computations are a bit lengthy, let us simply state the result. Define a rescaled time variable \(t = \sqrt{\gamma} k \in \mathbb{R}_{\geq 0}\) and the reparametrized iterates \(X(t) = X(\sqrt{\gamma} k ) = x_k\). As \(\gamma \to 0\), \(X\) satisfies the ordinary differential equation (ODE) $$ \frac{d^2X}{dt^2}(t) + 2\sqrt{\mu} \frac{dX}{dt}(t) = \, &amp;#8211; \nabla f(X(t)) \, .$$&lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-full is-resized&quot;&gt;&lt;img loading=&quot;lazy&quot; src=&quot;https://francisbach.com/wp-content/uploads/2022/12/nest-nf-2.gif&quot; alt=&quot;&quot; class=&quot;wp-image-8678&quot; width=&quot;600&quot; height=&quot;450&quot;/&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p class=&quot;justify-text&quot;&gt;At this point, it is tempting to try to overlap the last two plots in order to compare the gradient flow with the limiting ODE for Nesterov acceleration. However, this can not be done as the notion of time \(t\) is not the same in both cases. The gradient flow is obtained at the limit of gradient descent with \(t_1 = \gamma k\); meanwhile Nesterov acceleration has a continuous limit in the scaling \(t_2 = \sqrt{\gamma} k\). For a small stepsize \(\gamma\), \(t_2\) is an order of magnitude larger than \(t_1\); this confirms that Nesterov acceleration is indeed an acceleration. &lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;The precise ODE obtained in the limit of Nesterov acceleration gives insights on the mechanism underlying acceleration. It is a second-order ODE, while the gradient flow is a first-order ODE. The gradient flow represents the movement of a particle rolling on the graph of \(f\), with no inertia. Meanwhile, the ODE for Nesterov acceleration also represents the movement of a particle rolling on the graph of \(f\), but with inertia and with a friction coefficient proportional to \(\sqrt{\mu}\). This comforts the high-level idea that acceleration is achieved by &amp;#8221;giving inertia to the iterates&amp;#8221;. &lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;However, the continuous perspective on acceleration comes with important limitations. First, as illustrated in the plots, the continuous limits appear in a computationally inefficient limit of small stepsizes. However, the rates of convergence of the discrete algorithms depend on their ability to be stable while using large stepsizes; this aspect can not be apprehended in the continuous limit. &lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;Relatedly, the continuous limits are not algorithms by themselves; they need to be discretized to be implemented. The discretization of the accelerated ODE could lead to Nesterov acceleration, but also to multiple other algorithms including Polyak&amp;#8217;s heavy ball method (see [6] or [2, Section 2.3.3]), another algorithm that is not as stable on non-quadratic strongly convex functions. Said differently, the continuous limit is unable to discriminate two discrete algorithms with different performances. However, it should be noted that this limitation was adressed in [5] by considering other ODEs that approximate the discrete algorithms at a higher-order when \(\gamma \to 0\). &lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;In the next section, we present the so-called &amp;#8220;continuized&amp;#8221; version of Nesterov acceleration, a joint work [1] with Mathieu Even, Francis Bach, Nicolas Flammarion, Hadrien Hendrikx, Pierre Gaillard, Laurent Massoulié, Adrien Taylor and myself. It is defined in continuous time, but it does not correspond to a limit where \(\gamma \to 0\). Furthermore, it does not need to be approximated to be implemented in discrete time. &lt;/p&gt;



&lt;h2&gt;Continuized acceleration&lt;/h2&gt;



&lt;p class=&quot;justify-text&quot;&gt;The continuized acceleration is composed of two variables \(x_t\), \(z_t\) indexed by a continuous time \(t \geq 0\), that are continuously mixing and that take gradient steps at random times. More precisely, let \(T_1, T_2, T_3, \dots \geq 0\) be random times such that \(T_1, T_2-T_1, T_3-T_2, \dots\) are independent identically distributed (i.i.d.), of distribution exponential with rate \(1\). The exponential distribution is a classical distribution which has a special &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://en.wikipedia.org/wiki/Exponential_distribution&quot; target=&quot;_blank&quot;&gt;memoryless property&lt;/a&gt;; in our case of rate \(1\), it is the distribution with density \(\exp(-t)\). Below, we show five realizations of the random times \(T_1, T_2, T_3, \dots\), between \(t=0\) and \(t = 10\):&lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-large&quot;&gt;&lt;img loading=&quot;lazy&quot; width=&quot;1024&quot; height=&quot;205&quot; src=&quot;https://francisbach.com/wp-content/uploads/2022/12/poisson-process0-1-1024x205.png&quot; alt=&quot;&quot; class=&quot;wp-image-8650&quot; srcset=&quot;https://francisbach.com/wp-content/uploads/2022/12/poisson-process0-1-1024x205.png 1024w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process0-1-300x60.png 300w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process0-1-768x154.png 768w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process0-1-850x170.png 850w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process0-1.png 1500w&quot; sizes=&quot;(max-width: 1024px) 100vw, 1024px&quot; /&gt;&lt;/figure&gt;&lt;/div&gt;

&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-large&quot;&gt;&lt;img loading=&quot;lazy&quot; width=&quot;1024&quot; height=&quot;205&quot; src=&quot;https://francisbach.com/wp-content/uploads/2022/12/poisson-process1-1-1024x205.png&quot; alt=&quot;&quot; class=&quot;wp-image-8651&quot; srcset=&quot;https://francisbach.com/wp-content/uploads/2022/12/poisson-process1-1-1024x205.png 1024w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process1-1-300x60.png 300w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process1-1-768x154.png 768w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process1-1-850x170.png 850w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process1-1.png 1500w&quot; sizes=&quot;(max-width: 1024px) 100vw, 1024px&quot; /&gt;&lt;/figure&gt;&lt;/div&gt;

&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-large&quot;&gt;&lt;img loading=&quot;lazy&quot; width=&quot;1024&quot; height=&quot;205&quot; src=&quot;https://francisbach.com/wp-content/uploads/2022/12/poisson-process2-1-1024x205.png&quot; alt=&quot;&quot; class=&quot;wp-image-8652&quot; srcset=&quot;https://francisbach.com/wp-content/uploads/2022/12/poisson-process2-1-1024x205.png 1024w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process2-1-300x60.png 300w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process2-1-768x154.png 768w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process2-1-850x170.png 850w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process2-1.png 1500w&quot; sizes=&quot;(max-width: 1024px) 100vw, 1024px&quot; /&gt;&lt;/figure&gt;&lt;/div&gt;

&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-large&quot;&gt;&lt;img loading=&quot;lazy&quot; width=&quot;1024&quot; height=&quot;205&quot; src=&quot;https://francisbach.com/wp-content/uploads/2022/12/poisson-process3-1-1024x205.png&quot; alt=&quot;&quot; class=&quot;wp-image-8653&quot; srcset=&quot;https://francisbach.com/wp-content/uploads/2022/12/poisson-process3-1-1024x205.png 1024w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process3-1-300x60.png 300w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process3-1-768x154.png 768w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process3-1-850x170.png 850w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process3-1.png 1500w&quot; sizes=&quot;(max-width: 1024px) 100vw, 1024px&quot; /&gt;&lt;/figure&gt;&lt;/div&gt;

&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-large&quot;&gt;&lt;img loading=&quot;lazy&quot; width=&quot;1024&quot; height=&quot;205&quot; src=&quot;https://francisbach.com/wp-content/uploads/2022/12/poisson-process4-1-1024x205.png&quot; alt=&quot;&quot; class=&quot;wp-image-8654&quot; srcset=&quot;https://francisbach.com/wp-content/uploads/2022/12/poisson-process4-1-1024x205.png 1024w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process4-1-300x60.png 300w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process4-1-768x154.png 768w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process4-1-850x170.png 850w, https://francisbach.com/wp-content/uploads/2022/12/poisson-process4-1.png 1500w&quot; sizes=&quot;(max-width: 1024px) 100vw, 1024px&quot; /&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p class=&quot;justify-text&quot;&gt;By convention, we choose that our stochastic processes \(t \mapsto x_t\), \(t \mapsto z_t\) are &amp;#8220;&lt;a href=&quot;https://en.wikipedia.org/wiki/C%C3%A0dl%C3%A0g&quot; target=&quot;_blank&quot; rel=&quot;noreferrer noopener&quot;&gt;càdlàg&lt;/a&gt;&amp;#8221; almost surely, i.e., right continuous with well-defined left-limits \(x_{t-}\), \(z_{t-}\) at all points \(t\)&lt;em&gt;. &lt;/em&gt;At random times \(T_1, T_2, \dots\), our sequences take gradient steps $$\begin{aligned} x_{T_k} &amp;amp;= x_{T_k-} &amp;#8211; \gamma \nabla f (x_{T_k-}) \, , \\z_{T_k} &amp;amp;= z_{T_k-} &amp;#8211; \sqrt{\frac{\gamma}{\mu}} \nabla f (x_{T_k-}) \, . \end{aligned}$$Because of the memoryless property of the exponential distribution, in a infinitesimal time interval \([t, t+dt]\), the variables take gradients steps with probability \(dt\), independently of the past.&lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;Between these random times, the variables mix through a linear, translation-invariant, ODE $$\begin{aligned}&amp;amp;dx_t = \sqrt{\gamma\mu} (z_t \, &amp;#8211; x_t) dt \, , \\&amp;amp;dz_t = \sqrt{\gamma\mu} (x_t \, &amp;#8211; z_t) dt \, .\end{aligned}$$Following the notation of stochastic calculus, we can write the process more compactly. Denote \(dN(t) = \sum_{k\geq 1} \delta_{T_k}(dt)\) the Poisson point measure; it is the sum of Dirac masses at the random times \(T_1, T_2, \dots\). Then we write&lt;br&gt;$$\begin{aligned}&lt;br&gt;dx_t &amp;amp;= \sqrt{\gamma\mu} (z_t \, &amp;#8211; x_t) dt \, &amp;#8211; \gamma \nabla f(x_t) dN(t) \, , \\&lt;br&gt;dz_t &amp;amp;= \sqrt{\gamma\mu} (x_t \, &amp;#8211; z_t) dt-  \sqrt{\frac{\gamma}{\mu}} \nabla f(x_t) dN(t) \, . &lt;br&gt;\end{aligned}$$This should be understood as detailed above: the first component of these equations is an evolution with respect to the Lebesgue measure, thus a continuous flow; the second component is an evolution with respect to a discrete measure; when the time \(t\) hits a Dirac mass of the measure \(dN(t)\), then we take discrete jumps. This gives a continuous-time random process; we show one realization below. &lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-full is-resized&quot;&gt;&lt;img loading=&quot;lazy&quot; src=&quot;https://francisbach.com/wp-content/uploads/2022/12/cont-3.gif&quot; alt=&quot;&quot; class=&quot;wp-image-8668&quot; width=&quot;600&quot; height=&quot;450&quot;/&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p class=&quot;justify-text&quot;&gt;In this simulation, we see similarities with the discrete acceleration of Nesterov. However, the process is now random and in continuous time. We comment this comparison in detail in the rest of the blog post. &lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;In [1], we proved the following convergence bound for this process: if \(\gamma \leq \frac{1}{L}\), $$\mathbb{E} f(x_t) \, &amp;#8211; f(x_*) \leq \left(f(x_0) \, &amp;#8211; f(x_*) + \frac{\mu}{2} \Vert z_0 \,  &amp;#8211; x_* \Vert^2 \right) \exp\left(  &amp;#8211; \sqrt{\gamma \mu} t \right) \, .$$Note the similarity with the bound proved by Nesterov for the discrete acceleration. In fact, the proof through Lyapunov techniques is essentially the same. In the continuized acceleration, the main difference is that we have a statement in expectation over the sampling of the Poisson point measure.  &lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;In summary, we observe from the bounds and the simulation that the discrete and continuized accelerations behave similarly. The continuized acceleration is more involved as it is a random process with ODE components and jumps. Why would one prefer the continuized acceleration? Can we discretize the continuized acceleration easily? &lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;In the next section, we explain why the discretization of the continuized acceleration is not a problem. But before that, let us propose high-level answers to the first question. &lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;First, from a Markov chain indexed by a discrete time index \(k\), one can associate the so-called &lt;em&gt;continuized&lt;/em&gt; Markov chain, indexed by a continuous time \(t\), that makes transition with the same Markov kernel, but at random times, with independent exponential time intervals [8]. This terminology motivated the name of the continuized acceleration in [1]. The continuized Markov chain is appreciated for its continuous time parameter \(t\), for instance because it enables to use differential calculus. Still, the continuized Markov chain keeps many properties of the original Markov chain; similarly the continuized acceleration is arguably simpler to analyze, while performing similarly to Nesterov acceleration.&lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;Second, processes that alternate randomly are generally simpler to analyze that those that alternate in cycles. For instance, stochastic gradient descent is easier to analyze when components are selected randomly rather than in an ordered way. Coordinate gradient descent is easier to analyze when coordinates are selected randomly rather than in an ordered way [9]. Similarly, the continuized acceleration is simpler to analyze because the gradient steps and the mixing steps alternate randomly, due to the randomness of \(T_k\).&lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;Third, thanks to this random alternation, the continuized framework enables to build accelerations in asynchronous distributed optimization. In fact, this application was the original motivation to build the continuized acceleration in [1]. The interested reader can consult [1, Sections 6-7] to learn how accelerated distributed algorithms were built in asynchronous settings where acceleration was previously unknown. &lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;After these high-level remarks, let us return to a concrete question that we left: how can we implement the continuized acceleration? &lt;/p&gt;



&lt;h2&gt;The discrete implementation of the continuized acceleration&lt;/h2&gt;



&lt;p class=&quot;justify-text&quot;&gt;The continuized acceleration can be implemented exactly as a discrete algorithm. Indeed, between the times of the jumps \(T_k\), the dynamics of the continuized acceleration are governed by an ODE that is integrable in closed form. It is thus wise to discretize at the times \(T_k\) of the jumps. More precisely, define $$\begin{aligned} &amp;amp;\tilde{x}_k := x_{T_{k}} \, , &amp;amp;&amp;amp;\tilde{y}_k := x_{T_{k+1}-} \, , &amp;amp;&amp;amp;\tilde{z}_k := z_{T_{k}} \, .&lt;br&gt;\end{aligned}$$In [1], we proved that the three sequences \(\tilde{x}_k\), \(\tilde{y}_k\), \(\tilde{z}_k\), \(k \geq 0\), satisfy a recurrence relation: $$\begin{aligned}&lt;br&gt;&amp;amp;\tilde{y}_k = \tilde{x}_k + \tau_k(\tilde{z}_k-\tilde{x}_k) \, , \\ &amp;amp;\tilde{x}_{k+1} = \tilde{y}_k \, &amp;#8211; \gamma \nabla f(\tilde{y}_k) \, , \\ &amp;amp;\tilde{z}_{k+1} = \tilde{z}_k + \tau_k&#39;(\tilde{y}_k-\tilde{z}_k) \, &amp;#8211; \sqrt{\frac{\gamma}{\mu}} \nabla f(\tilde{y}_k ) \, ,&lt;br&gt;\end{aligned}$$ with $$\begin{aligned} &amp;amp;{ \tau_k = \frac{1}{2}\left(1 &amp;#8211; \exp\left(-2\sqrt{\gamma\mu}(T_{k+1}-T_k)\right)\right)} \, , &amp;amp;&amp;amp;\tau_k&amp;#8217; = \tanh\left(\sqrt{\gamma\mu}(T_{k+1}-T_k)\right) \, .\end{aligned}$$Note that this recurrence relation has the same structure as Nesterov&amp;#8217;s original acceleration: in fact, only the two coefficients \(\tau_k\) and \(\tau_k&amp;#8217;\) have been randomized. In short, the continuized acceleration can be implemented as a randomized version of the discrete acceleration. &lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;In this discrete implementation, the continuized acceleration performs similarly to Nesterov acceleration. &lt;/p&gt;


&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-full is-resized&quot;&gt;&lt;img loading=&quot;lazy&quot; src=&quot;https://francisbach.com/wp-content/uploads/2022/12/gd-nest-cont-3.gif&quot; alt=&quot;&quot; class=&quot;wp-image-8669&quot; width=&quot;600&quot; height=&quot;450&quot;/&gt;&lt;/figure&gt;&lt;/div&gt;

&lt;div class=&quot;wp-block-image&quot;&gt;
&lt;figure class=&quot;aligncenter size-full is-resized&quot;&gt;&lt;img loading=&quot;lazy&quot; src=&quot;https://francisbach.com/wp-content/uploads/2022/12/cont-nest-gd-performances-3.png&quot; alt=&quot;&quot; class=&quot;wp-image-8670&quot; width=&quot;600&quot; height=&quot;450&quot; srcset=&quot;https://francisbach.com/wp-content/uploads/2022/12/cont-nest-gd-performances-3.png 1200w, https://francisbach.com/wp-content/uploads/2022/12/cont-nest-gd-performances-3-300x225.png 300w, https://francisbach.com/wp-content/uploads/2022/12/cont-nest-gd-performances-3-1024x768.png 1024w, https://francisbach.com/wp-content/uploads/2022/12/cont-nest-gd-performances-3-768x576.png 768w, https://francisbach.com/wp-content/uploads/2022/12/cont-nest-gd-performances-3-850x638.png 850w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot; /&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;h2&gt;Conclusion&lt;/h2&gt;



&lt;p class=&quot;justify-text&quot;&gt;For practical purposes, Nesterov&amp;#8217;s discrete acceleration wins. The iteration is elementary to implement and it enjoys a great performance. (There are exceptions in distributed optimization for which Nesterov acceleration would not enable asynchrony but the continuized acceleration does [1]).&lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;For conceptual purposes, the continuous perspective on acceleration gives a crude intuition on the mechanism at play. However, there are also important aspects lost in the limit of small stepsizes, related to the choice of the discretization. &lt;/p&gt;



&lt;p class=&quot;justify-text&quot;&gt;The continuized acceleration has two faces, continuous and discrete. As a discrete iteration, it is a randomized version of Nesterov&amp;#8217;s original iteration. As a continuous process, it is more sophisticated that a simple ODE, as it involves random jumps. However, it gives a continuous-time perspective on acceleration without any conceptual loss. &lt;/p&gt;



&lt;h2&gt;References&lt;/h2&gt;



&lt;p class=&quot;justify-text&quot;&gt;[1] Mathieu Even, Raphaël Berthier, Francis Bach, Nicolas Flammarion, Hadrien Hendrikx, Pierre Gaillard, Laurent Massoulié, and Adrien Taylor. &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://proceedings.neurips.cc/paper/2021/hash/ec26fc2eb2b75aece19c70392dc744c2-Abstract.html&quot; target=&quot;_blank&quot;&gt;Continuized accelerations of deterministic and stochastic gradient descents, and of gossip algorithms&lt;/a&gt;. &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 34 (2021): 28054-28066.&lt;br&gt;[2] Alexandre d&amp;#8217;Aspremont, Damien Scieur, and Adrien Taylor. &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://www.nowpublishers.com/article/Details/OPT-036&quot; target=&quot;_blank&quot;&gt;Acceleration methods&lt;/a&gt;. &lt;em&gt;Foundations and Trends® in Optimization&lt;/em&gt; 5, no. 1-2 (2021): 1-245.&lt;br&gt;[3] Yurii Nesterov. &lt;em&gt;&lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://link.springer.com/book/10.1007/978-1-4419-8853-9&quot; target=&quot;_blank&quot;&gt;Introductory lectures on convex optimization: A basic course&lt;/a&gt;&lt;/em&gt;. Vol. 87. Springer Science &amp;amp; Business Media, 2003.&lt;br&gt;[4] Weijie Su, Stephen Boyd, and Emmanuel Candes. &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://jmlr.org/papers/v17/15-084.html&quot; target=&quot;_blank&quot;&gt;A differential equation for modeling Nesterov’s accelerated gradient method: theory and insights&lt;/a&gt;. &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 27 (2014).&lt;br&gt;[5] Bin Shi, Simon Du, Michael Jordan, and Weijie Su. &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://link.springer.com/article/10.1007/s10107-021-01681-8&quot; target=&quot;_blank&quot;&gt;Understanding the acceleration phenomenon via high-resolution differential equations&lt;/a&gt;. &lt;em&gt;Mathematical Programming&lt;/em&gt; 195, no. 1 (2022): 79-148.&lt;br&gt;[6] Boris Polyak. &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://www.sciencedirect.com/science/article/abs/pii/0041555364901375&quot; target=&quot;_blank&quot;&gt;Some methods of speeding up the convergence of iteration methods&lt;/a&gt;. &lt;em&gt;USSR computational mathematics and mathematical physics&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(5), pp.1-17 (1964).&lt;br&gt;[7] Lessard, Laurent, Benjamin Recht, and Andrew Packard. &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://epubs.siam.org/doi/abs/10.1137/15M1009597?casa_token=eux_zMBz0-QAAAAA:s32oBPHvN0hyzdt9LQLJYhggXmncsC66SAFXiIhztJQ8Zdu7avLKCMhkfZaND2Jdd_WZcl-B5jKS&quot; target=&quot;_blank&quot;&gt;Analysis and design of optimization algorithms via integral quadratic constraints&lt;/a&gt;. &lt;em&gt;SIAM Journal on Optimization&lt;/em&gt; 26, no. 1 (2016): 57-95.&lt;br&gt;[8] David Aldous and James Fill. &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;http://www.stat.berkeley.edu/$\sim$aldous/RWG/book.html&quot; target=&quot;_blank&quot;&gt;Reversible Markov chains and random walks on graphs&lt;/a&gt;. Unpublished monograph (1995).&lt;br&gt;[9] Stephen Wright. &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://link.springer.com/article/10.1007/s10107-015-0892-3&quot; target=&quot;_blank&quot;&gt;Coordinate descent algorithms&lt;/a&gt;. &lt;em&gt;Mathematical Programming&lt;/em&gt; 151, no. 1 (2015): 3-34.&lt;br&gt;[10] Hedy Attouch, Zaki Chbani, Juan Peypouquet, and Patrick Redont.&lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://link.springer.com/article/10.1007/s10107-016-0992-8&quot; target=&quot;_blank&quot;&gt; Fast convergence of inertial dynamics and algorithms with asymptotic vanishing viscosity&lt;/a&gt;. &lt;em&gt;Mathematical Programming&lt;/em&gt; 168, no. 1 (2018): 123-175.&lt;br&gt;[11] Sébastien Bubeck, Yin Tat Lee, and Mohit Singh. &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://arxiv.org/abs/1506.08187&quot; target=&quot;_blank&quot;&gt;A geometric alternative to Nesterov&amp;#8217;s accelerated gradient descent&lt;/a&gt;. &lt;em&gt;arXiv preprint arXiv:1506.08187&lt;/em&gt; (2015).&lt;br&gt;[12] Nicolas Flammarion and Francis Bach. &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://proceedings.mlr.press/v40/Flammarion15.html&quot; target=&quot;_blank&quot;&gt;From averaging to acceleration, there is only a step-size&lt;/a&gt;. In &lt;em&gt;Conference on Learning Theory&lt;/em&gt;, pp. 658-695. PMLR (2015).&lt;br&gt;[13] Yossi Arjevani, Shai Shalev-Shwartz, and Ohad Shamir. &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://jmlr.org/papers/v17/15-106.html&quot; target=&quot;_blank&quot;&gt;On lower and upper bounds in smooth and strongly convex optimization&lt;/a&gt;. &lt;em&gt;The Journal of Machine Learning Research&lt;/em&gt; 17, no. 1 (2016): 4303-4353.&lt;br&gt;[14] Donghwan Kim and Jeffrey Fessler. &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://link.springer.com/article/10.1007/s10107-015-0949-3&quot; target=&quot;_blank&quot;&gt;Optimized first-order methods for smooth convex minimization&lt;/a&gt;. &lt;em&gt;Mathematical programming&lt;/em&gt; 159, no. 1 (2016): 81-107.&lt;br&gt;[15] Zeyuan Allen-Zhu and Lorenzo Orecchia. &lt;a rel=&quot;noreferrer noopener&quot; href=&quot;https://drops.dagstuhl.de/opus/volltexte/2017/8185/pdf/LIPIcs-ITCS-2017-3.pdf&quot; target=&quot;_blank&quot;&gt;Linear coupling: An ultimate unification of gradient and mirror descent&lt;/a&gt;. In &lt;em&gt;Proceedings of the 8th Innovations in Theoretical Computer Science ITCS&amp;#8217;17&lt;/em&gt; (2017).&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Raphael Berthier&lt;/p&gt;
  </description>
  <pubDate>2022-12-15 15:25:08 UTC</pubDate>
  <author>Francis Bach</author>
</item>

<item>
  <title>A Pedagogical reference to kick off the New Year</title>
  <guid>https://ptreview.sublinear.info/?p=1813</guid>
  <link>https://ptreview.sublinear.info/2022/12/a-pedagogical-reference-to-kick-off-the-new-year/</link>
  <description>
    &lt;p&gt;Dear Readers,&lt;/p&gt;



&lt;p&gt;Our own Clément Canonne has written a beautiful survey which is now available in &lt;a href=&quot;https://www.nowpublishers.com/article/Details/CIT-114&quot;&gt;FnT book format from now publishers&lt;/a&gt;. This appears to be a very promising read &amp;#8212; especially for the Distribution Testers among you. Today&amp;#8217;s post is a mere advertisement for this beautiful survey/book which is clearly the result of a dedicated pursuit.&lt;/p&gt;



&lt;p&gt;Let me now dig into this survey a teeny tiny bit. One among the many cool features of this survey is that it uses one central example (testing goodness-of-fit) to give a unified treatment to the diverse tools and techniques used in distribution testing. Another plus for me is the historical notes section that accompanies every chapter. In particular, I really liked jumping into the informative history section at the end of Chapter 2 which has an almost story like feel to it. If the above points do not catch your fancy, then please try opening the survey. You will be hardpressed to find a book that is typeset in such an aesthetically pleasing way with colored fonts to emphasize various parameters in several intricate proofs. Happy Reading!&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Akash&lt;/p&gt;
  </description>
  <pubDate>2022-12-15 12:31:32 UTC</pubDate>
  <author>Property Testing Review</author>
</item>

<item>
  <title>3D Neuron Morphology Analysis</title>
  <guid>http://arxiv.org/abs/2212.07044</guid>
  <link>http://arxiv.org/abs/2212.07044</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jiaxiang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goebel_M/0/1/0/all/0/1&quot;&gt;Michael Goebel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borba_C/0/1/0/all/0/1&quot;&gt;Cezar Borba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_W/0/1/0/all/0/1&quot;&gt;William Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manjunath_B/0/1/0/all/0/1&quot;&gt;B.S. Manjunath&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We consider the problem of finding an accurate representation of neuron
shapes, extracting sub-cellular features, and classifying neurons based on
neuron shapes. In neuroscience research, the skeleton representation is often
used as a compact and abstract representation of neuron shapes. However,
existing methods are limited to getting and analyzing &quot;curve&quot; skeletons which
can only be applied for tubular shapes. This paper presents a 3D neuron
morphology analysis method for more general and complex neuron shapes. First,
we introduce the concept of skeleton mesh to represent general neuron shapes
and propose a novel method for computing mesh representations from 3D surface
point clouds. A skeleton graph is then obtained from skeleton mesh and is used
to extract sub-cellular features. Finally, an unsupervised learning method is
used to embed the skeleton graph for neuron classification. Extensive
experiment results are provided and demonstrate the robustness of our method to
analyze neuron morphology.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-15 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Toroidal Coordinates: Decorrelating Circular Coordinates With Lattice Reduction</title>
  <guid>http://arxiv.org/abs/2212.07201</guid>
  <link>http://arxiv.org/abs/2212.07201</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scoccola_L/0/1/0/all/0/1&quot;&gt;Luis Scoccola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gakhar_H/0/1/0/all/0/1&quot;&gt;Hitesh Gakhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bush_J/0/1/0/all/0/1&quot;&gt;Johnathan Bush&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schonsheck_N/0/1/0/all/0/1&quot;&gt;Nikolas Schonsheck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rask_T/0/1/0/all/0/1&quot;&gt;Tatum Rask&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Ling Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perea_J/0/1/0/all/0/1&quot;&gt;Jose A. Perea&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The circular coordinates algorithm of de Silva, Morozov, and
Vejdemo-Johansson takes as input a dataset together with a cohomology class
representing a $1$-dimensional hole in the data; the output is a map from the
data into the circle that captures this hole, and that is of minimum energy in
a suitable sense. However, when applied to several cohomology classes, the
output circle-valued maps can be &quot;geometrically correlated&quot; even if the chosen
cohomology classes are linearly independent. It is shown in the original work
that less correlated maps can be obtained with suitable integer linear
combinations of the cohomology classes, with the linear combinations being
chosen by inspection. In this paper, we identify a formal notion of geometric
correlation between circle-valued maps which, in the Riemannian manifold case,
corresponds to the Dirichlet form, a bilinear form derived from the Dirichlet
energy. We describe a systematic procedure for constructing low energy
torus-valued maps on data, starting from a set of linearly independent
cohomology classes. We showcase our procedure with computational examples. Our
main algorithm is based on the Lenstra--Lenstra--Lov\&#39;asz algorithm from
computational number theory.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-15 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Approximate Discrete Fr\&#39;echet distance: simplified, extended and structured</title>
  <guid>http://arxiv.org/abs/2212.07124</guid>
  <link>http://arxiv.org/abs/2212.07124</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoog_I/0/1/0/all/0/1&quot;&gt;Ivor van der Hoog&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rotenberg_E/0/1/0/all/0/1&quot;&gt;Eva Rotenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_S/0/1/0/all/0/1&quot;&gt;Sampsong Wong&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The Fr\&#39;{e}chet distance is one of the most studied distance measures between
curves $P$ and $Q$. The data structure variant of the problem is a longstanding
open problem: Efficiently preprocess $P$, so that for any $Q$ given at query
time, one can efficiently approximate their Fr\&#39;{e}chet distance. There exist
conditional lower bounds that prohibit $(1 + \varepsilon)$-approximate
Fr\&#39;{e}chet distance computations in subquadratic time, even when preprocessing
$P$ using any polynomial amount of time and space. As a consequence, the
problem has been studied under various restrictions: restricting $Q$ to be a
(horizontal) segment, or requiring $P$ and $Q$ to be so-called \emph{realistic}
input curves.
&lt;/p&gt;
&lt;p&gt;We give a data structure for $(1+\varepsilon)$-approximate discrete
Fr\&#39;{e}chet distance in any metric space $\mathcal{X}$ between a realistic
input curve $P$ and any query curve $Q$. After preprocessing the input curve
$P$ (of length $|P|=n$) in $O(n \log n)$ time, we may answer queries specifying
a query curve $Q$ and an $\varepsilon$, and output a value $d(P,Q)$ which is at
most a $(1+\varepsilon)$-factor away from the true Fr\&#39;{e}chet distance between
$Q$ and $P$. Thus, we give the first data structure that adapts to
$\varepsilon$-values specified at query time, and the first data structure to
handle query curves with arbitrarily many vertices. Our query time is
asymptotically linear in $|Q|=m$, $\frac{1}{\varepsilon}$, $\log n$, and the
realism parameter $c$ or $\kappa$.
&lt;/p&gt;
&lt;p&gt;The method presented in this paper simplifies and generalizes previous
contributions to the static problem variant. We obtain efficient queries (and
therefore static algorithms) for Fr\&#39;{e}chet distance computation in
high-dimensional spaces and other metric spaces (e.g., when $\mathcal{X}$ is a
graph under the shortest path metric). Our method supports subcurve queries at
no additional cost.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-15 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Interweaving Real-Time Jobs with Energy Harvesting to Maximize Throughput</title>
  <guid>http://arxiv.org/abs/2212.07002</guid>
  <link>http://arxiv.org/abs/2212.07002</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schieber_B/0/1/0/all/0/1&quot;&gt;Baruch Schieber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samineni_B/0/1/0/all/0/1&quot;&gt;Bhargav Samineni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vahidi_S/0/1/0/all/0/1&quot;&gt;Soroush Vahidi&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Motivated by baterryless IoT devices, we consider the following scheduling
problem. The input includes $n$ unit time jobs $\mathcal{J} = \{J_1, \ldots,
J_n\}$, where each job $J_i$ has a release time $r_i$, due date $d_i$, energy
requirement $e_i$, and weight $w_i$. We consider time to be slotted; hence, all
time related job values refer to slots. Let $T=\max_i\{d_i\}$. The input also
includes an $h_t$ value for every time slot $t$ ($1 \leq t \leq T$), which is
the energy harvestable on that slot. Energy is harvested at time slots when no
job is executed. The objective is to find a feasible schedule that maximizes
the weight of the scheduled jobs. A schedule is feasible if for every job $J_j$
in the schedule and its corresponding slot $t_j$, $t_{j} \neq t_{j&#39;}$ if ${j}
\neq {j&#39;}$, $r_j \leq t_j \leq d_j$, and the available energy before $t_j$ is
at least $e_j$. To the best of our knowledge, we are the first to consider the
theoretical aspects of this problem.
&lt;/p&gt;
&lt;p&gt;In this work we show the following. (1) A polynomial time algorithm when all
jobs have identical $r_i, d_i$ and $w_i$. (2) A $\frac{1}{2}$-approximation
algorithm when all jobs have identical $w_i$ but arbitrary $r_i$ and $d_i$. (3)
An FPTAS when all jobs have identical $r_i$ and $d_i$ but arbitrary $w_i$. (4)
Reductions showing that all the variants of the problem in which at least one
of the attributes $r_i$, $d_i$, or $w_i$ are not identical for all jobs are
NP-Hard.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-15 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Efficient Non-isomorphic Graph Enumeration Algorithms for Subclasses of Perfect Graphs</title>
  <guid>http://arxiv.org/abs/2212.07119</guid>
  <link>http://arxiv.org/abs/2212.07119</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawahara_J/0/1/0/all/0/1&quot;&gt;Jun Kawahara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saitoh_T/0/1/0/all/0/1&quot;&gt;Toshiki Saitoh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeda_H/0/1/0/all/0/1&quot;&gt;Hirokazu Takeda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshinaka_R/0/1/0/all/0/1&quot;&gt;Ryo Yoshinaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshioka_Y/0/1/0/all/0/1&quot;&gt;Yui Yoshioka&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Intersection graphs are well-studied in the area of graph algorithms. Some
intersection graph classes are known to have algorithms enumerating all
unlabeled graphs by reverse search. Since these algorithms output graphs one by
one and the numbers of graphs in these classes are vast, they work only for a
small number of vertices. Binary decision diagrams (BDDs) are compact data
structures for various types of data and useful for solving optimization and
enumeration problems. This study proposes enumeration algorithms for five
intersection graph classes, which admit $\mathrm{O}(n)$-bit string
representations for their member graphs. Our algorithm for each class
enumerates all unlabeled graphs with $n$ vertices over BDDs representing the
binary strings in time polynomial in $n$. Moreover, our algorithms are extended
to enumerate those with constraints on the maximum (bi)clique size and/or the
number of edges.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-15 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Postdoc at Northeastern</title>
  <guid>http://emanueleviola.wordpress.com/?p=1110</guid>
  <link>https://emanueleviola.wordpress.com/2022/12/14/postdoc-at-northeastern/</link>
  <description>
    &lt;p&gt;&lt;a href=&quot;https://www.khoury.northeastern.edu/research/postdoctoral-fellowship/&quot;&gt;Apply here&lt;/a&gt;.  Exciting opportunity.  Two years, and you can also write grants.  All areas are being considered.&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Manu&lt;/p&gt;
  </description>
  <pubDate>2022-12-14 17:56:44 UTC</pubDate>
  <author>Emanuele Viola</author>
</item>

<item>
  <title>Postdoc and research fellow at HIIT (Aalto University and University of Helsinki) (apply by January 8, 2023)</title>
  <guid>http://cstheory-jobs.org/2022/12/14/postdoc-and-research-fellow-at-hiit-aalto-university-and-university-of-helsinki-apply-by-january-8-2023/</guid>
  <link>https://cstheory-jobs.org/2022/12/14/postdoc-and-research-fellow-at-hiit-aalto-university-and-university-of-helsinki-apply-by-january-8-2023/</link>
  <description>
    &lt;p&gt;The Helsinki Institute for Information Technology invites applications for Research Fellow (&amp;lt;=5 years) and Postdoctoral Fellow (&amp;lt;=3 years) positions. In the foundations of computing focus area we welcome applicants working in all areas of TCS, e.g. algorithms, complexity, comp. logic, optimization, cryptography, comp. geometry, natural computation, and distributed, parallel, and quantum computing.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://www.hiit.fi/hiit-postdoctoral-and-research-fellow-positions/&quot;&gt;https://www.hiit.fi/hiit-postdoctoral-and-research-fellow-positions/&lt;/a&gt;&lt;br /&gt;
Email: coordinator@hiit.fi&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </description>
  <pubDate>2022-12-14 15:27:54 UTC</pubDate>
  <author>CCI: jobs</author>
</item>

<item>
  <title>On computing the vertex connectivity of 1-plane graphs</title>
  <guid>http://arxiv.org/abs/2212.06782</guid>
  <link>http://arxiv.org/abs/2212.06782</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biedl_T/0/1/0/all/0/1&quot;&gt;Therese Biedl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murali_K/0/1/0/all/0/1&quot;&gt;Karthik Murali&lt;/a&gt;&lt;/p&gt;&lt;p&gt;A graph is called 1-plane if it has an embedding in the plane where each edge
is crossed at most once by another edge. A crossing of a 1-plane graph is
called an $\times$-crossing if the endpoints of the crossing pair of edges
induce a matching. In this paper, we show how to compute the vertex
connectivity of a 1-plane graph $G$ without $\times$-crossings in linear time.
To do so, we show that for any two vertices $u,v$ in a minimum separating set
$S$, the distance between $u$ and $v$ in an auxiliary graph $\Lambda(G)$
(obtained by planarizing $G$ and then inserting into each face a new vertex
adjacent to all vertices of the face) is small. It hence suffices to search for
a minimum separating set in various subgraphs $\Lambda_i$ of $\Lambda(G)$ with
small diameter. Since $\Lambda(G)$ is planar, the subgraphs $\Lambda_i$ have
small treewidth. Each minimum separating set $S$ then gives rise to a partition
of $\Lambda_i$ into three vertex sets with special properties; such a partition
can be found via Courcelle&#39;s theorem in linear time.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-14 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>A (Slightly) Improved Deterministic Approximation Algorithm for Metric TSP</title>
  <guid>http://arxiv.org/abs/2212.06296</guid>
  <link>http://arxiv.org/abs/2212.06296</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karlin_A/0/1/0/all/0/1&quot;&gt;Anna R. Karlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_N/0/1/0/all/0/1&quot;&gt;Nathan Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gharan_S/0/1/0/all/0/1&quot;&gt;Shayan Oveis Gharan&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We show that the max entropy algorithm can be derandomized (with respect to a
particular objective function) to give a deterministic $3/2-\epsilon$
approximation algorithm for metric TSP for some $\epsilon &amp;gt; 10^{-36}$.
&lt;/p&gt;
&lt;p&gt;To obtain our result, we apply the method of conditional expectation to an
objective function constructed in prior work which was used to certify that the
expected cost of the algorithm is at most $3/2-\epsilon$ times the cost of an
optimal solution to the subtour elimination LP. The proof in this work involves
showing that the expected value of this objective function can be computed in
polynomial time (at all stages of the algorithm&#39;s execution).
&lt;/p&gt;
  </description>
  <pubDate>2022-12-14 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Dynamic Maxflow via Dynamic Interior Point Methods</title>
  <guid>http://arxiv.org/abs/2212.06315</guid>
  <link>http://arxiv.org/abs/2212.06315</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brand_J/0/1/0/all/0/1&quot;&gt;Jan van den Brand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang P. Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sidford_A/0/1/0/all/0/1&quot;&gt;Aaron Sidford&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this paper we provide an algorithm for maintaining a
$(1-\epsilon)$-approximate maximum flow in a dynamic, capacitated graph
undergoing edge additions. Over a sequence of $m$-additions to an $n$-node
graph where every edge has capacity $O(\mathrm{poly}(m))$ our algorithm runs in
time $\widehat{O}(m \sqrt{n} \cdot \epsilon^{-1})$. To obtain this result we
design dynamic data structures for the more general problem of detecting when
the value of the minimum cost circulation in a dynamic graph undergoing edge
additions obtains value at most $F$ (exactly) for a given threshold $F$. Over a
sequence $m$-additions to an $n$-node graph where every edge has capacity
$O(\mathrm{poly}(m))$ and cost $O(\mathrm{poly}(m))$ we solve this thresholded
minimum cost flow problem in $\widehat{O}(m \sqrt{n})$. Both of our algorithms
succeed with high probability against an adaptive adversary. We obtain these
results by dynamizing the recent interior point method used to obtain an almost
linear time algorithm for minimum cost flow (Chen, Kyng, Liu, Peng, Probst
Gutenberg, Sachdeva 2022), and introducing a new dynamic data structure for
maintaining minimum ratio cycles in an undirected graph that succeeds with high
probability against adaptive adversaries.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-14 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>How Does Independence Help Generalization? Sample Complexity of ERM on Product Distributions</title>
  <guid>http://arxiv.org/abs/2212.06422</guid>
  <link>http://arxiv.org/abs/2212.06422</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tao Lin&lt;/a&gt;&lt;/p&gt;&lt;p&gt;While many classical notions of learnability (e.g., PAC learnability) are
distribution-free, utilizing the specific structures of an input distribution
may improve learning performance. For example, a product distribution on a
multi-dimensional input space has a much simpler structure than a correlated
distribution. A recent paper [GHTZ21] shows that the sample complexity of a
general learning problem on product distributions is polynomial in the input
dimension, which is exponentially smaller than that on correlated
distributions. However, the learning algorithm they use is not the standard
Empirical Risk Minimization (ERM) algorithm. In this note, we characterize the
sample complexity of ERM in a general learning problem on product
distributions. We show that, even though product distributions are simpler than
correlated distributions, ERM still needs an exponential number of samples to
learn on product distributions, instead of a polynomial. This leads to the
conclusion that a product distribution by itself does not make a learning
problem easier -- an algorithm designed specifically for product distributions
is needed.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-14 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Streaming Euclidean MST to a Constant Factor</title>
  <guid>http://arxiv.org/abs/2212.06546</guid>
  <link>http://arxiv.org/abs/2212.06546</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_Addad_V/0/1/0/all/0/1&quot;&gt;Vincent Cohen-Addad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayaram_R/0/1/0/all/0/1&quot;&gt;Rajesh Jayaram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levi_A/0/1/0/all/0/1&quot;&gt;Amit Levi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waingarten_E/0/1/0/all/0/1&quot;&gt;Erik Waingarten&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study streaming algorithms for the fundamental geometric problem of
computing the cost of the Euclidean Minimum Spanning Tree (MST) on an $n$-point
set $X \subset \mathbb{R}^d$. In the streaming model, the points in $X$ can be
added and removed arbitrarily, and the goal is to maintain an approximation in
small space. In low dimensions, $(1+\epsilon)$ approximations are possible in
sublinear space [Frahling, Indyk, Sohler, SoCG &#39;05]. However, for high
dimensional spaces the best known approximation for this problem was
$\tilde{O}(\log n)$, due to [Chen, Jayaram, Levi, Waingarten, STOC &#39;22],
improving on the prior $O(\log^2 n)$ bound due to [Indyk, STOC &#39;04] and
[Andoni, Indyk, Krauthgamer, SODA &#39;08]. In this paper, we break the logarithmic
barrier, and give the first constant factor sublinear space approximation to
Euclidean MST. For any $\epsilon\geq 1$, our algorithm achieves an
$\tilde{O}(\epsilon^{-2})$ approximation in $n^{O(\epsilon)}$ space.
&lt;/p&gt;
&lt;p&gt;We complement this by proving that any single pass algorithm which obtains a
better than $1.10$-approximation must use $\Omega(\sqrt{n})$ space,
demonstrating that $(1+\epsilon)$ approximations are not possible in
high-dimensions, and that our algorithm is tight up to a constant.
Nevertheless, we demonstrate that $(1+\epsilon)$ approximations are possible in
sublinear space with $O(1/\epsilon)$ passes over the stream. More generally,
for any $\alpha \geq 2$, we give a $\alpha$-pass streaming algorithm which
achieves a $(1+O(\frac{\log \alpha + 1}{ \alpha \epsilon}))$ approximation in
$n^{O(\epsilon)} d^{O(1)}$ space. Our streaming algorithms are linear sketches,
and therefore extend to the massively-parallel computation model (MPC). Thus,
our results imply the first $(1+\epsilon)$-approximation to Euclidean MST in a
constant number of rounds in the MPC model.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-14 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Dimensionality reduction on complex vector spaces for dynamic weighted Euclidean distance</title>
  <guid>http://arxiv.org/abs/2212.06605</guid>
  <link>http://arxiv.org/abs/2212.06605</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellizzoni_P/0/1/0/all/0/1&quot;&gt;Paolo Pellizzoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1&quot;&gt;Francesco Silvestri&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The weighted Euclidean distance between two vectors is a Euclidean distance
where the contribution of each dimension is scaled by a given non-negative
weight. The Johnson-Lindenstrauss (JL) lemma can be easily adapted to the
weighted Euclidean distance if weights are known at construction time. Given a
set of $n$ vectors with dimension $d$, it suffices to scale each dimension of
the input vectors according to the weights, and then apply any standard JL
reduction: the weighted Euclidean distance between pairs of vectors is
preserved within a multiplicative factor $\epsilon$ with high probability.
&lt;/p&gt;
&lt;p&gt;However, this is not the case when weights are provided after the
dimensionality reduction. In this paper, we show that by applying a linear map
from real vectors to a complex vector space, it is possible to update the
compressed vectors so that the weighted Euclidean distances between pairs of
points can be computed within a multiplicative factor $\epsilon$, even when
weights are provided after the dimensionality reduction. Finally, we consider
the estimation of weighted Euclidean norms in streaming settings: we show how
to estimate the weighted norm when the weights are provided either after or
concurrently with the input vector.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-14 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Fast Number Parsing Without Fallback</title>
  <guid>http://arxiv.org/abs/2212.06644</guid>
  <link>http://arxiv.org/abs/2212.06644</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mushtak_N/0/1/0/all/0/1&quot;&gt;Noble Mushtak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemire_D/0/1/0/all/0/1&quot;&gt;Daniel Lemire&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In recent work, Lemire (2021) presented a fast algorithm to convert number
strings into binary floating-point numbers. The algorithm has been adopted by
several important systems: e.g., it is part of the runtime libraries of GCC 12,
Rust 1.55, and Go 1.16. The algorithm parses any number string with a
significand containing no more than 19 digits into an IEEE floating-point
number. However, there is a check leading to a fallback function to ensure
correctness. This fallback function is never called in practice. We prove that
the fallback is unnecessary. Thus we can slightly simplify the algorithm and
its implementation.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-14 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Profit Maximization in Social Networks and Non-monotone DR-submodular Maximization</title>
  <guid>http://arxiv.org/abs/2212.06646</guid>
  <link>http://arxiv.org/abs/2212.06646</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1&quot;&gt;Shuyang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chuangen Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Weili Wu&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this paper, we study the non-monotone DR-submodular function maximization
over integer lattice. Functions over integer lattice have been defined
submodular property that is similar to submodularity of set functions.
DR-submodular is a further extended submodular concept for functions over the
integer lattice, which captures the diminishing return property. Such functions
find many applications in machine learning, social networks, wireless networks,
etc. The techniques for submodular set function maximization can be applied to
DR-submodular function maximization, e.g., the double greedy algorithm has a
$1/2$-approximation ratio, whose running time is $O(nB)$, where $n$ is the size
of the ground set, $B$ is the integer bound of a coordinate. In our study, we
design a $1/2$-approximate binary search double greedy algorithm, and we prove
that its time complexity is $O(n\log B)$, which significantly improves the
running time. Specifically, we consider its application to the profit
maximization problem in social networks with a bipartite model, the goal of
this problem is to maximize the net profit gained from a product promoting
activity, which is the difference of the influence gain and the promoting cost.
We prove that the objective function is DR-submodular over integer lattice. We
apply binary search double greedy algorithm to this problem and verify the
effectiveness.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-14 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Tailored Data Augmentation to Mitigate Model Failures</title>
  <guid>https://gradientscience.org/failure-directions2/</guid>
  <link>https://gradientscience.org/failure-directions2/</link>
  <description>
    &lt;meta charset=&quot;utf-8&quot; /&gt;

&lt;!-- &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1, shrink-to-fit=no&quot;&gt; --&gt;
&lt;!-- &lt;link rel=&quot;stylesheet&quot; href=&quot;https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css&quot; integrity=&quot;sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T&quot; crossorigin=&quot;anonymous&quot;&gt; --&gt;

&lt;link rel=&quot;stylesheet&quot; href=&quot;https://use.fontawesome.com/releases/v5.8.1/css/all.css&quot; integrity=&quot;sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf&quot; crossorigin=&quot;anonymous&quot; /&gt;

&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://gradientscience.org/assets/css/style.css&quot; /&gt;

&lt;link rel=&quot;stylesheet&quot; href=&quot;https://gradientscience.org/assets/multilabel/style.css&quot; /&gt;

&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://gradientscience.org/assets/failure-directions2/css/style.css&quot; /&gt;

&lt;!-- &lt;link rel=&quot;stylesheet&quot; href=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css&quot;&gt; --&gt;
&lt;!-- &lt;script src=&quot;https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js&quot;&gt;&lt;/script&gt; --&gt;
&lt;!-- &lt;script src=&quot;https://code.jquery.com/jquery-3.3.1.slim.min.js&quot; integrity=&quot;sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; --&gt;
&lt;script src=&quot;https://code.jquery.com/jquery-3.3.1.min.js&quot; integrity=&quot;sha384-tsQFqpEReu7ZLhBV2VZlAu7zcOV+rXbYlF2cqB8txI/8aZajjp4Bqd+V6D5IgvKT&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;link rel=&quot;stylesheet&quot; href=&quot;https://www.w3schools.com/w3css/4/w3.css&quot;&gt; --&gt;
&lt;!-- chart.js --&gt;
&lt;script src=&quot;https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a class=&quot;bbutton&quot; style=&quot;float: left; width: 45%;&quot; href=&quot;https://arxiv.org/abs/2206.14754&quot;&gt;
&lt;i class=&quot;fas fa-file-pdf&quot;&gt;&lt;/i&gt;
    Paper
&lt;/a&gt;
&lt;a class=&quot;bbutton&quot; style=&quot;float: left; width: 45%;&quot; href=&quot;https://github.com/MadryLab/failure-directions&quot;&gt;
&lt;i class=&quot;fab fa-github&quot;&gt;&lt;/i&gt;
   Code
&lt;/a&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;i&gt;
In our &lt;a href=&quot;https://gradientscience.org/failure-directions/&quot;&gt;previous blogpost&lt;/a&gt; we described a method for automatically distilling a model’s failure modes as directions in a latent space. In this post, we’ll discuss how we can combine this method with off-the-shelf text-to-image models to perform synthetic data augmentation that specifically targets (and thus helps mitigate) a model’s classification failures. 
&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;Machine learning models are prone to failures —especially on subpopulations which are ambiguous, corrupted, or underrepresented in the training data. For example, a CIFAR-10 classification model might fail to identify cats on grass if its training dataset contained mostly examples of cats indoors.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;EasyHardCats&quot; src=&quot;/assets/failure-directions2/images/easy_hard_cats.png&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our &lt;a href=&quot;https://gradientscience.org/failure-directions/&quot;&gt;previous blog post&lt;/a&gt; described a method for automatically distilling such meaningful error patterns as directions in a latent space. In the context of our example above, this would correspond to identifying an axis such that the easy examples (e.g., cats inside) lie in one of its directions, and the hard ones (e.g., cats on grass) lie in the opposite direction.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;EasyHardCats&quot; src=&quot;/assets/failure-directions2/images/cat_axis.png&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, coming up with such a consistent “hardness rating” is only half the battle. That is, once we’ve identified relevant failure modes, how can we improve the underlying model’s reliability on them? One approach (as we discuss in the &lt;a href=&quot;https://arxiv.org/abs/2206.14754&quot;&gt;paper&lt;/a&gt;) is to simply add more data that belongs to the corresponding subpopulations (e.g., add more examples of cats on grass). For example, we can use our method to filter examples from an external pool of data (if we do not wish to use the entire external pool due to e.g., computational constraints) that would be useful to add to the training dataset (see Figure 6 in our paper).&lt;/p&gt;

&lt;p&gt;But what if we don’t have access to such an external pool of data?  It turns out that we can directly generate the images we need! Specifically, as we show in the (revised) version of our &lt;a href=&quot;https://arxiv.org/abs/2206.14754&quot;&gt;paper&lt;/a&gt;, we can leverage the recent stunning progress in text-to-image generation, as exemplified by diffusion models such as &lt;a href=&quot;https://huggingface.co/blog/stable_diffusion&quot;&gt;Stable Diffusion&lt;/a&gt;, &lt;a href=&quot;https://imagen.research.google/&quot;&gt;Imagen&lt;/a&gt;, and &lt;a href=&quot;https://openai.com/dall-e-2/&quot;&gt;DALL-E 2&lt;/a&gt;. That is, by combining our method with off-the-shelf diffusion models, we can perform “synthetic data augmentation” to specifically target a model’s failure modes.&lt;/p&gt;

&lt;h2 id=&quot;recap-identifying-and-captioning-model-failure-modes&quot;&gt;Recap: Identifying and Captioning Model Failure Modes&lt;/h2&gt;
&lt;p&gt;Our &lt;a href=&quot;https://gradientscience.org/failure-directions/&quot;&gt;previous blogpost&lt;/a&gt; and corresponding &lt;a href=&quot;https://arxiv.org/pdf/2206.14754.pdf&quot;&gt;paper&lt;/a&gt; presented a method for automatically identifying and captioning failure modes in a dataset. The key idea here was that, when exposed to a challenging subpopulation, the errors that a model makes are consistent, and share common features. (In our running example of cats, the hard examples tend to have outdoor backgrounds.) We can thus leverage that consistency and feature sharing by training a simple linear classifier to predict whether the original model is likely to make a mistake. Specifically, by training a linear SVM (per-class) to separate the incorrect and correct examples given their (normalized) feature embeddings.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;EasyHardCats&quot; src=&quot;/assets/failure-directions2/images/cat_megafig.png&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In our running example of a cat, the SVM (the black line above) learns to separate cats indoors from cats outside. The vector orthogonal to the SVM’s decision boundary (in gray) thus encapsulates the direction of the captured failure mode: the images that are most aligned (or anti-aligned) with this direction represent the most prototypically “hard” or “easy” examples.&lt;/p&gt;

&lt;p&gt;Moreover, by leveraging a joint vision/language space for our underlying featurization (e.g., &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;&gt;CLIP&lt;/a&gt;) our method provides a convenient way to caption the underlying failure modes with natural language descriptions. That is, since CLIP embeds both images and text in the same space, we can surface sentences whose text embedding is most aligned with our captured direction. In our example of cats from CIFAR-10, this method extracts “a photo of a white cat on the grass” as a hard subpopulation and “a photo of a cat inside” as an easy one.&lt;/p&gt;

&lt;h2 id=&quot;targeted-data-augmentation-with-stable-diffusion&quot;&gt;Targeted Data Augmentation with Stable Diffusion&lt;/h2&gt;

&lt;p&gt;How can we use text-to-image diffusion models, such as Stable Diffusion or DALL-E 2, to generate synthetic images on which to fine-tune our model (thus “hardening” it against the corresponding distribution shift)? One straightforward approach is to simply use the name of the class to generate new images. For example, we can generate new examples of cats for CIFAR-10 by plugging in the sentence “a photo of a cat.”&lt;/p&gt;

&lt;p&gt;However, just inputting “a photo of a cat” as a caption results in fairly generic examples of cats. But we wanted to perform targeted data augmentation, in order to improve the model’s reliability on our particular identified challenging subpopulations (e.g., cats on grass). Conveniently, we already discussed how  to automatically surface natural language captions for the extracted failure modes. So, we can just plug these captions into our text-to-image model! Below are examples of such generated images.&lt;/p&gt;

&lt;div id=&quot;sd_widget&quot; style=&quot;overflow:auto; text-align: center&quot;&gt;&lt;/div&gt;

&lt;p&gt;How well does fine-tuning on such synthetic images fare? As one might hope, fine-tuning the original model’s final layer indeed improves model performance on the hard subpopulation (see below).&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;HardFinetune&quot; src=&quot;/assets/failure-directions2/images/hard_finetune.png&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;directly-synthesizing-hard-examples&quot;&gt;Directly synthesizing hard examples&lt;/h2&gt;

&lt;p&gt;However, the above approach, although successful, feels somewhat suboptimal. After all, our initial SVM “hard” direction already lives in a joint vision/language space—and so does the Stable Diffusion model we use. Can we thus skip the intermediate captioning step and directly decode our extracted SVM direction into synthetic images?&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Direct&quot; src=&quot;/assets/failure-directions2/images/direct.png&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Yes, we can! We just need to interpolate between a reference class caption (e.g., “a photo of a cat”) and our extracted SVM direction to generate either harder or easier images that correspond to our captured failure mode.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Circle&quot; src=&quot;/assets/failure-directions2/images/circle.png&quot; style=&quot;width:40%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Below, we display some examples of hard (and easy) images generated for each CIFAR-10 class. Notice that the encoded directions include rich information such as background, pose, and distance to the object, which would not be conveyed by the captions alone. As our revised &lt;a href=&quot;https://arxiv.org/abs/2206.14754&quot;&gt;paper&lt;/a&gt; shows, the original model performs worst on the “hard” images and best on the “easy” ones.&lt;/p&gt;

&lt;div id=&quot;rdm_widget&quot; style=&quot;overflow:auto; text-align: center&quot;&gt;&lt;/div&gt;

&lt;p&gt;So, indeed, by directly decoding the extracted SVM direction, we can capture the failure mode itself as a collection of new (synthetic) images, without relying on the proxy of captions!&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this blog post, we demonstrated how to use off-the-shelf diffusion models to perform &lt;strong&gt;targeted “data augmentation”&lt;/strong&gt;, improving test accuracy on hard subpopulations to which a model might be vulnerable. This put forward a fully automated pipeline for identifying, interpreting, and intervening on challenging subpopulations. We believe that as the power of text-to-image models increases, such targeted “data augmentation” will become an even more powerful and versatile tool for improving model reliability.&lt;/p&gt;

&lt;script src=&quot;https://gradientscience.org/assets/scripts/onload.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://cdn.jsdelivr.net/gh/nicolaspanel/numjs@0.15.1/dist/numjs.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://cdn.jsdelivr.net/npm/mathjs@6.6.0/dist/math.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://code.jquery.com/jquery-3.3.1.slim.min.js&quot; integrity=&quot;sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js&quot; integrity=&quot;sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js&quot; integrity=&quot;sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://gradientscience.org/assets/failure-directions2/thumbnail_plot.js&quot;&gt;&lt;/script&gt;

&lt;script&gt;
    var img_root = &quot;https://gradientscience.org/assets/failure-directions2/images/sd_figs/&quot;
    var thumb_root = &quot;https://gradientscience.org/assets/failure-directions2/images/sd_figs_thumbnails/&quot;
    var bias_base = [[&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;]];

    var thumbnail_imgs = [[], []]
    var main_imgs = [[], []]
    bias_base.forEach(function(base, idx){
        base.forEach(function(b, sec_idx) {
            thumbnail_imgs[idx].push(thumb_root + b + &quot;.png&quot;);
            main_imgs[idx].push(img_root + b + &quot;.png&quot;);
        });
    });
    make_thumbnail_plot(&#39;sd_widget&#39;, thumbnail_imgs, main_imgs)
&lt;/script&gt;

&lt;script&gt;
    var img_root = &quot;https://gradientscience.org/assets/failure-directions2/images/rdm_figs/&quot;
    var thumb_root = &quot;https://gradientscience.org/assets/failure-directions2/images/rdm_figs_thumbnails/&quot;
    var bias_base = [[&quot;1&quot;, &quot;2&quot;, &quot;3&quot;], [ &quot;4&quot;, &quot;5&quot;, &quot;6&quot;]];

    var thumbnail_imgs = [[], []]
    var main_imgs = [[], []]
    bias_base.forEach(function(base, idx){
        base.forEach(function(b, sec_idx) {
            thumbnail_imgs[idx].push(thumb_root + b + &quot;.png&quot;);
            main_imgs[idx].push(img_root + b + &quot;.png&quot;);
        });
    });
    make_thumbnail_plot(&#39;rdm_widget&#39;, thumbnail_imgs, main_imgs)
&lt;/script&gt;
  </description>
  <pubDate>2022-12-14 00:00:00 UTC</pubDate>
  <author>Gradient Science</author>
</item>

<item>
  <title>PhD Student at King’s College London (apply by February 28, 2023)</title>
  <guid>http://cstheory-jobs.org/2022/12/13/phd-student-at-kings-college-london-apply-by-february-28-2023/</guid>
  <link>https://cstheory-jobs.org/2022/12/13/phd-student-at-kings-college-london-apply-by-february-28-2023/</link>
  <description>
    &lt;p&gt;PhD positions are available in the areas of multi-agent systems, game dynamics, game theory, machine learning, reinforcement learning and/or cryptoeconomics (mechanism design/incentives). Funding is available for UK home students and options can be discussed for international students.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;http://stefanosleonardos.com/&quot;&gt;http://stefanosleonardos.com/&lt;/a&gt; (This is funding coming from my start-up package and there is no explicit job advert website. Hope this is acceptable)&lt;br /&gt;
Email: stefanos.leonardos@kcl.ac.uk&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </description>
  <pubDate>2022-12-13 21:03:18 UTC</pubDate>
  <author>CCI: jobs</author>
</item>

<item>
  <title>Randomly traceable graphs</title>
  <guid>https://11011110.github.io/blog/2022/12/13/randomly-traceable-graphs</guid>
  <link>https://11011110.github.io/blog/2022/12/13/randomly-traceable-graphs.html</link>
  <description>
    &lt;p&gt;In my recently-concluded graph algorithms course, one of my early homework assignments asked about undirected graphs with the following property: any oriented path that does not cover all vertices can be extended to form a Hamiltonian path. I phrased it in terms of depth-first search: which graphs have the property that, no matter where you start and no matter what order you explore the neighbors of each node, a depth first search tree will automatically produce a Hamiltonian path? The intent was to reinforce the idea that the same graph can have multiple depth first search trees depending on contingent issues of how the graph is represented. I called these “unicursal graphs”.&lt;/p&gt;

&lt;p&gt;This is also closely related to &lt;a href=&quot;/blog/2019/05/02/playing-model-trains.html&quot;&gt;some research I published in WADS 2019 on reconfiguring paths in graphs&lt;/a&gt;, although in that work I was focusing on whether all paths of a certain length could be connected to each other by local moves and here the question is merely whether any path could be a dead end without any possible moves.&lt;/p&gt;

&lt;p&gt;The homework asked only for an example of a graph that was unicursal but not complete. I had in mind two possibilities, the cycle graphs and the balanced complete bipartite graphs. One of the students in the graduate section of the class, Alvin Chiu, took the problem and ran with it, eventually proving that these are the only examples. Unfortunately, he also discovered that the result was in the literature already: Gary Chartrand and Hudson V. Kronk (1968), “Randomly traceable graphs”, &lt;em&gt;SIAM J. Appl. Math.&lt;/em&gt; 16 (4): 696–700, &lt;a href=&quot;https://doi.org/10.1137/0116056&quot;&gt;doi:10.1137/0116056&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I think it’s a cute result that deserves to be more widely known, so I thought I’d outline a proof here.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The defining property is that any path can be extended to a Hamiltonian path. If you remove the first edge of a Hamiltonian path and extend the rest, the only possibility is to return to the first vertex of the initial path, by an edge that completes it to form a Hamiltonian cycle. So every path can be extended one step farther, to a Hamiltonian cycle.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Find any cycle in your graph. If that is the whole graph, you have one of the three possibilities in the classification of these graphs; otherwise, it has at least one diagonal. By drawing an S-shaped path through this diagonal and around your starting cycle, and then completing it to another cycle, you can show that the graph must include a rotated copy of the diagonal. By repeating this operation you can show that it includes every rotated copy of every diagonal, and therefore that it forms a &lt;a href=&quot;https://en.wikipedia.org/wiki/Circulant_graph&quot;&gt;circulant graph&lt;/a&gt;, a graph whose symmetries include a cyclic rotation of all the vertices.&lt;/p&gt;

    &lt;p style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/blog/assets/2022/unicursal-cases.svg&quot; alt=&quot;Case analysis for randomly traceable graphs. Left: completion of an S-shaped path through the diagonal of a Hamiltonian cycle shows the existence of a rotated copy of the diagonal. Right: completion of a C-shaped path skipping the apex of a triangle shows that all vertices are adjacent to the apex.&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If the graph contains a triangle, then (by extending the path through two sides of the triangle) you can arrive at a situation where the triangle is formed by three consecutive vertices of your Hamiltonian cycle, and includes a diagonal skipping the center vertex of the three. By drawing C-shaped paths around the cycle that use this diagonal to skip the center vertex, and then completing these paths to cycles, you can show that this center vertex is &lt;a href=&quot;https://en.wikipedia.org/wiki/Universal_vertex&quot;&gt;universal&lt;/a&gt;: it has edges to everything else. By the circulant symmetry of the graph, it must be complete, the second of the three possibilities in the classification of these graphs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the remaining case, any diagonal of a Hamiltonian cycle and its rotated copy form a four-vertex cycle. As in the previous case, by extending a path through three sides of the cycle you can arrive at a situation where some four-vertex cycle is formed by four consecutive vertices of a Hamiltonian cycle. A similar argument involving C-shaped paths shows that the two central vertices of the 4-cycle are adjacent to all other vertices, and by symmetry every two consecutive vertices of the Hamiltonian cycle are adjacent to all other vertices. This implies that there are at least \(n^2/4\) edges, and by &lt;a href=&quot;https://en.wikipedia.org/wiki/Mantel&#39;s_theorem&quot;&gt;Mantel’s theorem&lt;/a&gt; the graph can only be a balanced complete bipartite graph, the third of the three possibilities.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(&lt;a href=&quot;https://mathstodon.xyz/@11011110/109508139457197936&quot;&gt;Discuss on Mastodon&lt;/a&gt;)&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By David Eppstein&lt;/p&gt;
  </description>
  <pubDate>2022-12-13 11:41:00 UTC</pubDate>
  <author>David Eppstein</author>
</item>

<item>
  <title>An approach to robust ICP initialization</title>
  <guid>http://arxiv.org/abs/2212.05332</guid>
  <link>http://arxiv.org/abs/2212.05332</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolpakov_A/0/1/0/all/0/1&quot;&gt;Alexander Kolpakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werman_M/0/1/0/all/0/1&quot;&gt;Michael Werman&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this note, we propose an approach for initializing the Iterative Closest
Point (ICP) algorithm that allows us to apply ICP to unlabelled point clouds
that are related by rigid transformations. We also give bounds on the
robustness of our approach to noise. Numerical experiments confirm our
theoretical findings.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-13 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Isometric deformable cones and cylinders carrying planar curves</title>
  <guid>http://arxiv.org/abs/2212.05484</guid>
  <link>http://arxiv.org/abs/2212.05484</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nawratil_G/0/1/0/all/0/1&quot;&gt;Georg Nawratil&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study cones and cylinders with a 1-parametric isometric deformation
carrying at least two planar curves, which remain planar during this continuous
flexion and are located in non-parallel planes. We investigate this
geometric/kinematic problem in the smooth and the discrete setting, as it is
the base for a generalized construction of so-called T-hedral zipper tubes. In
contrast to the cylindrical case, which can be solved easily, the conical one
is more tricky, but we succeed to give a closed form solution for the discrete
case, which is used to prove that these cones correspond to caps of Bricard
octahedra of the plane-symmetric type. For the smooth case we are able to
reduce the problem by means of symbolic computation to an ordinary differential
equation, but its solution remains an open problem.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-13 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Transcoding Unicode Characters with AVX-512 Instructions</title>
  <guid>http://arxiv.org/abs/2212.05098</guid>
  <link>http://arxiv.org/abs/2212.05098</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clausecker_R/0/1/0/all/0/1&quot;&gt;Robert Clausecker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemire_D/0/1/0/all/0/1&quot;&gt;Daniel Lemire&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Intel includes on its recent processors a powerful set of instructions
capable of processing 512-bit registers with a single instruction (AVX-512).
Some of these instructions have no equivalent in earlier instruction sets. We
leverage these instructions to efficiently transcode strings between the most
common formats: UTF-8 and UTF-16. With our novel algorithms, we are often twice
as fast as the previous best solutions. For example, we transcode Chinese text
from UTF-8 to UTF-16 at more than 5 GiB/s using fewer than 2 CPU instructions
per character. To ensure reproducibility, we make our software freely available
as an open source library.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-13 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Efficient and Generic Algorithms for Quantitative Attack Tree Analysis</title>
  <guid>http://arxiv.org/abs/2212.05358</guid>
  <link>http://arxiv.org/abs/2212.05358</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopuhaa_Zwakenberg_M/0/1/0/all/0/1&quot;&gt;Milan Lopuha&amp;#xe4;-Zwakenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budde_C/0/1/0/all/0/1&quot;&gt;Carlos E. Budde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoelinga_M/0/1/0/all/0/1&quot;&gt;Mari&amp;#xeb;lle Stoelinga&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Numerous analysis methods for quantitative attack tree analysis have been
proposed. These algorithms compute relevant security metrics, i.e. performance
indicators that quantify how good the security of a system is; typical metrics
being the most likely attack, the cheapest, or the most damaging one. However,
existing methods are only geared towards specific metrics or do not work on
general attack trees. This paper classifies attack trees in two dimensions:
proper trees vs. directed acyclic graphs (i.e. with shared subtrees); and
static vs. dynamic gates. For three out of these four classes, we propose novel
algorithms that work over a generic attribute domain, encompassing a large
number of concrete security metrics defined on the attack tree semantics;
dynamic attack trees with directed acyclic graph structure are left as an open
problem. We also analyse the computational complexity of our methods.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-13 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Beyond circular-arc graphs -- recognizing lollipop graphs and medusa graphs</title>
  <guid>http://arxiv.org/abs/2212.05433</guid>
  <link>http://arxiv.org/abs/2212.05433</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cagirici_D/0/1/0/all/0/1&quot;&gt;Deniz A&amp;#x11f;ao&amp;#x11f;lu &amp;#xc7;a&amp;#x11f;&amp;#x131;r&amp;#x131;c&amp;#x131;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cagirici_O/0/1/0/all/0/1&quot;&gt;Onur &amp;#xc7;a&amp;#x11f;&amp;#x131;r&amp;#x131;c&amp;#x131;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derbisz_J/0/1/0/all/0/1&quot;&gt;Jan Derbisz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartmann_T/0/1/0/all/0/1&quot;&gt;Tim A. Hartmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hlineny_P/0/1/0/all/0/1&quot;&gt;Petr Hlin&amp;#x11b;n&amp;#xfd;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kratochvil_J/0/1/0/all/0/1&quot;&gt;Jan Kratochv&amp;#xed;l&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krawczyk_T/0/1/0/all/0/1&quot;&gt;Tomasz Krawczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeman_P/0/1/0/all/0/1&quot;&gt;Peter Zeman&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In 1992 Bir\&#39;{o}, Hujter and Tuza introduced, for every fixed connected graph
$H$, the class of $H$-graphs, defined as the intersection graphs of connected
subgraphs of some subdivision of $H$. Recently, quite a lot of research has
been devoted to understanding the tractability border for various computational
problems, such as recognition or isomorphism testing, in classes of $H$-graphs
for different graphs $H$. In this work we undertake this research topic,
focusing on the recognition problem. Chaplick, T\&quot;{o}pfer, Voborn\&#39;{\i}k, and
Zeman showed, for every fixed tree $T$, a polynomial-time algorithm recognizing
$T$-graphs. Tucker showed a polynomial time algorithm recognizing $K_3$-graphs
(circular-arc graphs). On the other hand, Chaplick at al. showed that
recognition of $H$-graphs is $NP$-hard if $H$ contains two different cycles
sharing an edge.
&lt;/p&gt;
&lt;p&gt;The main two results of this work narrow the gap between the $NP$-hard and
$P$ cases of $H$-graphs recognition. First, we show that recognition of
$H$-graphs is $NP$-hard when $H$ contains two different cycles. On the other
hand, we show a polynomial-time algorithm recognizing $L$-graphs, where $L$ is
a graph containing a cycle and an edge attached to it ($L$-graphs are called
lollipop graphs). Our work leaves open the recognition problems of $M$-graphs
for every unicyclic graph $M$ different from a cycle and a lollipop. Other
results of this work, which shed some light on the cases that remain open, are
as follows. Firstly, the recognition of $M$-graphs, where $M$ is a fixed
unicyclic graph, admits a polynomial time algorithm if we restrict the input to
graphs containing particular holes (hence recognition of $M$-graphs is probably
most difficult for chordal graphs). Secondly, the recognition of medusa graphs,
which are defined as the union of $M$-graphs, where $M$ runs over all unicyclic
graphs, is $NP$-complete.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-13 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Algorithms approaching the threshold for semi-random planted clique</title>
  <guid>http://arxiv.org/abs/2212.05619</guid>
  <link>http://arxiv.org/abs/2212.05619</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buhai_R/0/1/0/all/0/1&quot;&gt;Rares-Darius Buhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kothari_P/0/1/0/all/0/1&quot;&gt;Pravesh K. Kothari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steurer_D/0/1/0/all/0/1&quot;&gt;David Steurer&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We design new polynomial-time algorithms for recovering planted cliques in
the semi-random graph model introduced by Feige and Kilian~\cite{FK01}. The
previous best algorithms for this model succeed if the planted clique has size
at least \(n^{2/3}\) in a graph with \(n\) vertices (Mehta, Mckenzie, Trevisan,
2019 and Charikar, Steinhardt, Valiant 2017). Our algorithms work for
planted-clique sizes approaching \(n^{1/2}\) -- the information-theoretic
threshold in the semi-random model~\cite{steinhardt2017does} and a conjectured
computational threshold even in the easier fully-random model. This result
comes close to resolving open questions by Feige and Steinhardt.
&lt;/p&gt;
&lt;p&gt;Our algorithms are based on higher constant degree sum-of-squares relaxation
and rely on a new conceptual connection that translates certificates of upper
bounds on biclique numbers in \emph{unbalanced} bipartite Erd\H{o}s--R\&#39;enyi
random graphs into algorithms for semi-random planted clique. The use of a
higher-constant degree sum-of-squares is essential in our setting: we prove a
lower bound on the basic SDP for certifying bicliques that shows that the basic
SDP cannot succeed for planted cliques of size $k =o(n^{2/3})$. We also provide
some evidence that the information-computation trade-off of our current
algorithms may be inherent by proving an average-case lower bound for
unbalanced bicliques in the low-degree-polynomials model.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-13 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Binary Error-Correcting Codes with Minimal Noiseless Feedback</title>
  <guid>http://arxiv.org/abs/2212.05673</guid>
  <link>http://arxiv.org/abs/2212.05673</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1&quot;&gt;Meghal Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guruswami_V/0/1/0/all/0/1&quot;&gt;Venkatesan Guruswami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rachel Yun Zhang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In the setting of error-correcting codes with feedback, Alice wishes to
communicate a $k$-bit message $x$ to Bob by sending a sequence of bits over a
channel while noiselessly receiving feedback from Bob. It has been long known
(Berlekamp, 1964) that in this model, Bob can still correctly determine $x$
even if $\approx \frac13$ of Alice&#39;s bits are flipped adversarially. This
improves upon the classical setting without feedback, where recovery is not
possible for error fractions exceeding $\frac14$.
&lt;/p&gt;
&lt;p&gt;The original feedback setting assumes that after transmitting each bit, Alice
knows (via feedback) what bit Bob received. In this work, our focus in on the
limited feedback model, where Bob is only allowed to send a few bits at a small
number of pre-designated points in the protocol. For any desired $\epsilon &amp;gt;
0$, we construct a coding scheme that tolerates a fraction $ 1/3-\epsilon$ of
bit flips relying only on $O_\epsilon(\log k)$ bits of feedback from Bob sent
in a fixed $O_\epsilon(1)$ number of rounds. We complement this with a matching
lower bound showing that $\Omega(\log k)$ bits of feedback are necessary to
recover from an error fraction exceeding $1/4$ (the threshold without any
feedback), and for schemes resilient to a fraction $1/3-\epsilon$ of bit flips,
the number of rounds must grow as $\epsilon \to 0$.
&lt;/p&gt;
&lt;p&gt;We also study (and resolve) the question for the simpler model of erasures.
We show that $O_\epsilon(\log k)$ bits of feedback spread over $O_\epsilon(1)$
rounds suffice to tolerate a fraction $(1-\epsilon)$ of erasures. Likewise, our
$\Omega(\log k)$ lower bound applies for erasure fractions exceeding $1/2$, and
an increasing number of rounds are required as the erasure fraction approaches
$1$.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-13 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Minimum-weight partitioning of a set with associated subsets</title>
  <guid>http://arxiv.org/abs/2212.05823</guid>
  <link>http://arxiv.org/abs/2212.05823</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zinder_Y/0/1/0/all/0/1&quot;&gt;Yakov Zinder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bertrand M.T. Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berlinska_J/0/1/0/all/0/1&quot;&gt;Joanna Berli&amp;#x144;ska&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The paper presents complexity results and performance guaranties for a family
of approximation algorithms for an optimisation problem arising in software
testing and manufacturing. The problem is formulated as a partitioning of a set
where each element has an associated subset in another set, but can also be
viewed as a scheduling problem with infinitely large communication delay,
precedence constraints in the form of a bipartite graph, and duplication.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-13 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Assistant Professor in Computer Science at HSE University (apply by January 15, 2023)</title>
  <guid>http://cstheory-jobs.org/2022/12/12/assistant-professor-in-computer-science-at-hse-university-apply-by-january-15-2023/</guid>
  <link>https://cstheory-jobs.org/2022/12/12/assistant-professor-in-computer-science-at-hse-university-apply-by-january-15-2023/</link>
  <description>
    &lt;p&gt;The Faculty of Computer Science, HSE University invites applications for positions of assistant [professor in all areas of computer science including theoretical CS.&lt;br /&gt;
Candidates must hold a recent PhD in computer science, mathematics or related fields, posess teaching experience and speak fluent English.&lt;br /&gt;
Tenure track positions are only available on a full-time, residential basis in Moscow, Russia.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://iri.hse.ru/ru/TTCS2223&quot;&gt;https://iri.hse.ru/ru/TTCS2223&lt;/a&gt;&lt;br /&gt;
Email: iri@hse.ru&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </description>
  <pubDate>2022-12-12 11:11:39 UTC</pubDate>
  <author>CCI: jobs</author>
</item>

<item>
  <title>What about Validity?</title>
  <guid>https://decentralizedthoughts.github.io/2022-12-12-what-about-validity/</guid>
  <link>https://decentralizedthoughts.github.io/2022-12-12-what-about-validity/</link>
  <description>
    Perhaps the architipical trilemma is consensus - it requires three properties: agreement, liveness, and validity. Getting any two is easy, but all three together is what makes consensus such a facinating problem that continues to create new challenges even after 40 years of research. A lot of research focuses on...
  </description>
  <pubDate>2022-12-12 09:00:00 UTC</pubDate>
  <author>Decentralized Thoughts</author>
</item>

<item>
  <title>Commercials are not logical.  FTX edition.</title>
  <guid>tag:blogger.com,1999:blog-3722233.post-1729382940974134289</guid>
  <link>http://blog.computationalcomplexity.org/2022/12/commercials-are-not-logical-ftx-edition.html</link>
  <description>
    &lt;p&gt;Some people asked me to comment on FTX since I teach Crypto. My insights are no better than anyone else; however, I have wanted to do a blog post about the illogic of commercials, so I will do that with FTX as an example.&amp;nbsp;&lt;/p&gt;&lt;p&gt;ALL conversations in the post are fictional.&lt;/p&gt;&lt;p&gt;------------------------------------&lt;/p&gt;&lt;p&gt;Alice: Bob,&amp;nbsp; why did you invest $1,000,000 with FTX?&lt;/p&gt;&lt;p&gt;Bob: Because Tom Brady endorsed it. (See&amp;nbsp;&lt;a href=&quot;https://nypost.com/2022/11/22/tom-brady-stephen-curry-larry-david-probed-over-ftx-endorsements/&quot;&gt;here&lt;/a&gt;&amp;nbsp;for an article about that and&amp;nbsp;&lt;a href=&quot;https://www.youtube.com/watch?v=gI0oRtEu7s0&quot;&gt;here&lt;/a&gt;&amp;nbsp;for a commercial Tom Brady did for FTX.)&amp;nbsp;&lt;/p&gt;&lt;p&gt;Alice: But Tom Brady is a football player, not a finance person.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Bob: Well... I know that now.&amp;nbsp;&lt;/p&gt;&lt;p&gt;(For an absolutely fantastic and now ironic commercial for FTX see&amp;nbsp;&lt;a href=&quot;https://www.google.com/search?q=you+tube+FTX+superbowl+commercial&amp;amp;rlz=1C1EJFC_enUS884US884&amp;amp;oq=you+tube+FTX+&amp;amp;aqs=chrome.0.69i59j0i13i512j69i57j0i13i512l7.4938j1j15&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8#fpstate=ive&amp;amp;vld=cid:b7e5529a,vid:_-FQqo46CJQ&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;p&gt;------------------------------------------------------------&lt;/p&gt;&lt;p&gt;The people in commercials are&amp;nbsp; paid to hawk the product.&amp;nbsp;&lt;/p&gt;&lt;p&gt;a) Are they experts? In the above case about about FTX&amp;nbsp; NO, they are not. So this should not work.&amp;nbsp;&lt;/p&gt;&lt;p&gt;b) There are commercials where the people ARE experts. For example, a basketball player endorsing Sneaker brand (not quite an expert, but they DO use the product). But even this should not work since the viewers KNOW that the person is being PAID to tell us it&#39;s a good product.&amp;nbsp;&lt;/p&gt;&lt;p&gt;c) Commercial spokesman (Geico-Gecko, Progressive-Flo, Dos&amp;nbsp; XX- the worlds most interesting man, see his commercial&amp;nbsp;&lt;a href=&quot;https://www.youtube.com/watch?v=n5HX7y1yDi4&quot;&gt;here&lt;/a&gt;&amp;nbsp;and a Ramsey Meme based on it&amp;nbsp;&lt;a href=&quot;https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/idont.jpg&quot;&gt;here&lt;/a&gt;) are even stranger- they are fictional characters who are urging me to buy something.&amp;nbsp; So the question &lt;i&gt;are they experts? &lt;/i&gt;doesn&#39;t even make sense. Someone pretending to be someone they are not is pretending to like a product they do not use.&amp;nbsp;&lt;/p&gt;&lt;p&gt;d) Some commercials pretend to be&amp;nbsp; informative but are not. For example, the Insurance Companies seem to brag about something that ALL of the insurance companies do (bundling, not-paying-for-what-you-don&#39;t-need).&amp;nbsp;&lt;/p&gt;&lt;p&gt;e) A truly new product may have an informative commercial, just to tell me that its out there. For example. &lt;i&gt;Stephen Colbert&#39;s Americone Dream Ice Cream&lt;/i&gt; which is awesome. (My spellchecker thinks &lt;i&gt;Americone&lt;/i&gt; is not a word. This time they are probably right.)&amp;nbsp;&lt;/p&gt;&lt;p&gt;f) SO, if I did a commercial for &lt;i&gt;Stephen Colbert&#39;s Americone Dream Ice Cream&lt;/i&gt; would you try it? Lets say I wasn&#39;t being paid and I truly did it for my love of that ice cream. STILL doesn&#39;t make sense- my tastes and yours may differ. However, you can try it and decide, and&amp;nbsp; it costs far less than $1,000,000.&lt;/p&gt;&lt;p&gt;---------------------------------------&lt;/p&gt;&lt;p&gt;So what are we left with? The only commercials that make logical sense would be those where&amp;nbsp;&lt;/p&gt;&lt;p&gt;a) the person is an expert&lt;/p&gt;&lt;p&gt;b) the person is not being paid&lt;/p&gt;&lt;p&gt;c) the person is telling you something about the product that distinguishes it from its competitors OR p its a new product&lt;/p&gt;&lt;p&gt;d) Its not a matter of taste- there is an absolute standard that is easily understood.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Do you know of ANY commercials like that? Influencers who are NOT paid by the people whose product they are hawking (are there such influencers?) might begin to qualify.&amp;nbsp;&lt;/p&gt;&lt;p&gt;------------------------------------------------------&lt;/p&gt;&lt;p&gt;So LOGICALLY commercials should not work. So why do they?&lt;/p&gt;&lt;p&gt;a) They don&#39;t. Its possible they just are not that effective. See&amp;nbsp;&lt;a href=&quot;https://insight.kellogg.northwestern.edu/article/tv-advertising-is-usually-not-worth-it&quot;&gt;here&lt;/a&gt;&amp;nbsp;for DON&quot;T WORK and&amp;nbsp;&lt;a href=&quot;https://www.investopedia.com/financial-edge/1111/8-of-the-most-successful-ad-campaigns-of-all-time.aspx&quot;&gt;here&lt;/a&gt;&amp;nbsp;for 8 times when it did work. I think&amp;nbsp;&lt;a href=&quot;https://www.google.com/search?q=you+tube+soup+commercial+stan+freberg&amp;amp;rlz=1C1EJFC_enUS884US884&amp;amp;ei=gaiWY7iVAqae5NoPgum46Ac&amp;amp;ved=0ahUKEwj42avlmfP7AhUmD1kFHYI0Dn0Q4dUDCA8&amp;amp;uact=5&amp;amp;oq=you+tube+soup+commercial+stan+freberg&amp;amp;gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIFCCEQqwI6CggAEEcQ1gQQsAM6BggAEBYQHjoFCAAQhgM6BwghEKABEAo6CAghEBYQHhAdSgQIQRgASgQIRhgAUNkDWOoWYOgXaAFwAXgAgAFyiAH7CJIBBDEwLjOYAQCgAQHIAQi4AQPAAQE&amp;amp;sclient=gws-wiz-serp#fpstate=ive&amp;amp;vld=cid:cf8fea70,vid:lZN86IOv79k&quot;&gt;this&lt;/a&gt;&amp;nbsp;is a great commercial (watch it to the end) but it does not make me want to out and buy soup. (Stan Freberg wrote some GREAT commercials. They are on You Tube and I recommend them highly for entertainment. He also has several great novelty-song albums.)&amp;nbsp;&lt;/p&gt;&lt;p&gt;b) Indirectly. They build brand awareness.&lt;/p&gt;&lt;p&gt;c) Because people are stupid. This is not an interesting answer since I still want to know what their reasoning is, whether or not its faulty.&lt;/p&gt;&lt;p&gt;d) Because you are part of a movement. Drink the Uncola (7UP) to be rebellious! Or the 1984-Apple commercial (see&amp;nbsp;&lt;a href=&quot;https://www.youtube.com/watch?v=zIE-5hg7FoA&quot;&gt;here&lt;/a&gt;). These are both odd since drinking 7UP or having an Apple Computer seem to me to be the opposite of rebellious.&amp;nbsp;&lt;/p&gt;&lt;p&gt;e) Buying the product to express your philosophy:&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ted: Carol, why did you invest in $1,000,000 in FTX?&amp;nbsp;&lt;/p&gt;&lt;p&gt;Carol: Because I believe in effective altruism.&lt;/p&gt;&lt;p&gt;Ted: Do you still?&lt;/p&gt;&lt;p&gt;Carol: This might need a rethink. But for now I&#39;ll&amp;nbsp; invest by getting a Freedom Unlimited credit card that gets cash back since Kevin Hart says I should (see&amp;nbsp;&lt;a href=&quot;https://www.google.com/search?q=you+tube+kevin+hart+credit+card&amp;amp;rlz=1C1EJFC_enUS884US884&amp;amp;oq=you+tube+kevin+hart+credit+card&amp;amp;aqs=chrome..69i57j69i64.12160j1j4&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8#fpstate=ive&amp;amp;vld=cid:e977cbbb,vid:m3TSpuaQvDo&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By gasarch&lt;/p&gt;
  </description>
  <pubDate>2022-12-12 04:03:00 UTC</pubDate>
  <author>Computational Complexity</author>
</item>

<item>
  <title>Implementing Neural Network-Based Equalizers in a Coherent Optical Transmission System Using Field-Programmable Gate Arrays</title>
  <guid>http://arxiv.org/abs/2212.04703</guid>
  <link>http://arxiv.org/abs/2212.04703</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Freire_P/0/1/0/all/0/1&quot;&gt;Pedro J. Freire&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Srivallapanondh_S/0/1/0/all/0/1&quot;&gt;Sasipim Srivallapanondh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anderson_M/0/1/0/all/0/1&quot;&gt;Michael Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Spinnler_B/0/1/0/all/0/1&quot;&gt;Bernhard Spinnler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bex_T/0/1/0/all/0/1&quot;&gt;Thomas Bex&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eriksson_T/0/1/0/all/0/1&quot;&gt;Tobias A. Eriksson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Napoli_A/0/1/0/all/0/1&quot;&gt;Antonio Napoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schairer_W/0/1/0/all/0/1&quot;&gt;Wolfgang Schairer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Costa_N/0/1/0/all/0/1&quot;&gt;Nelson Costa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Blott_M/0/1/0/all/0/1&quot;&gt;Michaela Blott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Turitsyn_S/0/1/0/all/0/1&quot;&gt;Sergei K. Turitsyn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prilepsky_J/0/1/0/all/0/1&quot;&gt;Jaroslaw E. Prilepsky&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this work, we demonstrate the offline FPGA realization of both recurrent
and feedforward neural network (NN)-based equalizers for nonlinearity
compensation in coherent optical transmission systems. First, we present a
realization pipeline showing the conversion of the models from Python libraries
to the FPGA chip synthesis and implementation. Then, we review the main
alternatives for the hardware implementation of nonlinear activation functions.
The main results are divided into three parts: a performance comparison, an
analysis of how activation functions are implemented, and a report on the
complexity of the hardware. The performance in Q-factor is presented for the
cases of bidirectional long-short-term memory coupled with convolutional NN
(biLSTM + CNN) equalizer, CNN equalizer, and standard 1-StpS digital
back-propagation (DBP) for the simulation and experiment propagation of a
single channel dual-polarization (SC-DP) 16QAM at 34 GBd along 17x70km of LEAF.
The biLSTM+CNN equalizer provides a similar result to DBP and a 1.7 dB Q-factor
gain compared with the chromatic dispersion compensation baseline in the
experimental dataset. After that, we assess the Q-factor and the impact of
hardware utilization when approximating the activation functions of NN using
Taylor series, piecewise linear, and look-up table (LUT) approximations. We
also show how to mitigate the approximation errors with extra training and
provide some insights into possible gradient problems in the LUT approximation.
Finally, to evaluate the complexity of hardware implementation to achieve 400G
throughput, fixed-point NN-based equalizers with approximated activation
functions are developed and implemented in an FPGA.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-12 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Controllability of complex networks: input node placement restricting the longest control chain</title>
  <guid>http://arxiv.org/abs/2212.04718</guid>
  <link>http://arxiv.org/abs/2212.04718</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alizadeh_S/0/1/0/all/0/1&quot;&gt;Samie Alizadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Posfai_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe1;rton P&amp;#xf3;sfai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghasemi_A/0/1/0/all/0/1&quot;&gt;Abdorasoul Ghasemi&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The minimum number of inputs needed to control a network is frequently used
to quantify its controllability. Control of linear dynamics through a minimum
set of inputs, however, often has prohibitively large energy requirements and
there is an inherent trade-off between minimizing the number of inputs and
control energy. To better understand this trade-off, we study the problem of
identifying a minimum set of input nodes such that controllabililty is ensured
while restricting the length of the longest control chain. The longest control
chain is the maximum distance from input nodes to any network node, and recent
work found that reducing its length significantly reduces control energy. We
map the longest control chain-constraint minimum input problem to finding a
joint maximum matching and minimum dominating set. We show that this graph
combinatorial problem is NP-complete, and we introduce and validate a heuristic
approximation. Applying this algorithm to a collection of real and model
networks, we investigate how network structure affects the minimum number of
inputs, revealing, for example, that for many real networks reducing the
longest control chain requires only few or no additional inputs, only the
rearrangement of the input nodes.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-12 01:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>A refinement on the structure of vertex-critical ($P_5$, gem)-free graphs</title>
  <guid>http://arxiv.org/abs/2212.04659</guid>
  <link>http://arxiv.org/abs/2212.04659</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cameron_B/0/1/0/all/0/1&quot;&gt;Ben Cameron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hoang_C/0/1/0/all/0/1&quot;&gt;Ch&amp;#xed;nh T. Ho&amp;#xe0;ng&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We give a new, stronger proof that there are only finitely many
$k$-vertex-critical ($P_5$,~gem)-free graphs for all $k$. Our proof further
refines the structure of these graphs and allows for the implementation of a
simple exhaustive computer search to completely list all $6$- and
$7$-vertex-critical $(P_5$, gem)-free graphs. Our results imply the existence
of polynomial-time certifying algorithms to decide the $k$-colourability of
$(P_5$, gem)-free graphs for all $k$ where the certificate is either a
$k$-colouring or a $(k+1)$-vertex-critical induced subgraph. Our complete lists
for $k\le 7$ allow for the implementation of these algorithms for all $k\le 6$.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-12 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Breaking the Barrier $2^k$ for Subset Feedback Vertex Set in Chordal Graphs</title>
  <guid>http://arxiv.org/abs/2212.04726</guid>
  <link>http://arxiv.org/abs/2212.04726</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1&quot;&gt;Tian Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1&quot;&gt;Mingyu Xiao&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The Subset Feedback Vertex Set problem (SFVS), to delete $k$ vertices from a
given graph such that any vertex in a vertex subset (called a terminal set) is
not in a cycle in the remaining graph, generalizes the famous Feedback Vertex
Set problem and Multiway Cut problem. SFVS remains $\mathrm{NP}$-hard even in
split and chordal graphs, and SFVS in Chordal Graphs can be considered as a
special case of the 3-Hitting Set problem. However, it is not easy to solve
SFVS in Chordal Graphs faster than 3-Hitting Set. In 2019, Philip, Rajan,
Saurabh, and Tale (Algorithmica 2019) proved that SFVS in Chordal Graphs can be
solved in $2^k n^{\mathcal{O}(1)}$, slightly improving the best result $2.076^k
n^{\mathcal{O}(1)}$ for 3-Hitting Set. In this paper, we break the
&quot;$2^k$-barrier&quot; for SFVS in Chordal Graphs by giving a $1.619^k
n^{\mathcal{O}(1)}$-time algorithm. Our algorithm uses reduction and branching
rules based on the Dulmage-Mendelsohn decomposition and a divide-and-conquer
method.
&lt;/p&gt;
  </description>
  <pubDate>2022-12-12 01:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

</channel>
</rss>
