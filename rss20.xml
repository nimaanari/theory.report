<?xml version="1.0"?>
<rss version="2.0">

<channel>
  <title>Theory of Computing Report</title>
  <link></link>
  <language>en</language>
  <description></description>


<item>
  <title>Improved Inapproximability of VC Dimension and Littlestone&#39;s Dimension via (Unbalanced) Biclique</title>
  <guid>http://arxiv.org/abs/2211.01443</guid>
  <link>http://arxiv.org/abs/2211.01443</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manurangsi_P/0/1/0/all/0/1&quot;&gt;Pasin Manurangsi&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the complexity of computing (and approximating) VC Dimension and
Littlestone&#39;s Dimension when we are given the concept class explicitly. We give
a simple reduction from Maximum (Unbalanced) Biclique problem to approximating
VC Dimension and Littlestone&#39;s Dimension. With this connection, we derive a
range of hardness of approximation results and running time lower bounds. For
example, under the (randomized) Gap-Exponential Time Hypothesis or the
Strongish Planted Clique Hypothesis, we show a tight inapproximability result:
both dimensions are hard to approximate to within a factor of $o(\log n)$ in
polynomial-time. These improve upon constant-factor inapproximability results
from [Manurangsi and Rubinstein, COLT 2017].
&lt;/p&gt;
  </description>
  <pubDate>2022-11-04 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Pseudorandom (Function-Like) Quantum State Generators: New Definitions and Applications</title>
  <guid>http://arxiv.org/abs/2211.01444</guid>
  <link>http://arxiv.org/abs/2211.01444</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ananth_P/0/1/0/all/0/1&quot;&gt;Prabhanjan Ananth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Gulati_A/0/1/0/all/0/1&quot;&gt;Aditya Gulati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Qian_L/0/1/0/all/0/1&quot;&gt;Lower Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Yuen_H/0/1/0/all/0/1&quot;&gt;Henry Yuen&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Pseudorandom quantum states (PRS) are efficiently constructible states that
are computationally indistinguishable from being Haar-random, and have recently
found cryptographic applications. We explore new definitions, new properties
and applications of pseudorandom states, and present the following
contributions:
&lt;/p&gt;
&lt;p&gt;1. New Definitions: We study variants of pseudorandom function-like state
(PRFS) generators, introduced by Ananth, Qian, and Yuen (CRYPTO&#39;22), where the
pseudorandomness property holds even when the generator can be queried
adaptively or in superposition. We show feasibility of these variants assuming
the existence of post-quantum one-way functions.
&lt;/p&gt;
&lt;p&gt;2. Classical Communication: We show that PRS generators with logarithmic
output length imply commitment and encryption schemes with classical
communication. Previous constructions of such schemes from PRS generators
required quantum communication.
&lt;/p&gt;
&lt;p&gt;3. Simplified Proof: We give a simpler proof of the Brakerski--Shmueli
(TCC&#39;19) result that polynomially-many copies of uniform superposition states
with random binary phases are indistinguishable from Haar-random states.
&lt;/p&gt;
&lt;p&gt;4. Necessity of Computational Assumptions: We also show that a secure PRS
with output length logarithmic, or larger, in the key length necessarily
requires computational assumptions.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-04 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>The Complexity of Pattern Counting in Directed Graphs, Parameterised by the Outdegree</title>
  <guid>http://arxiv.org/abs/2211.01905</guid>
  <link>http://arxiv.org/abs/2211.01905</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bressan_M/0/1/0/all/0/1&quot;&gt;Marco Bressan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanzinger_M/0/1/0/all/0/1&quot;&gt;Matthias Lanzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_M/0/1/0/all/0/1&quot;&gt;Marc Roth&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the fixed-parameter tractability of the following fundamental
problem: given two directed graphs $\vec H$ and $\vec G$, count the number of
copies of $\vec H$ in $\vec G$. The standard setting, where the tractability is
well understood, uses only $|\vec H|$ as a parameter. In this paper we take a
step forward, and adopt as a parameter $|\vec H|+d(\vec G)$, where $d(\vec G)$
is the maximum outdegree of $|\vec G|$. Under this parameterization, we
completely characterize the fixed-parameter tractability of the problem in both
its non-induced and induced versions through two novel structural parameters,
the fractional cover number $\rho^*$ and the source number $\alpha_s$. On the
one hand we give algorithms with running time $f(|\vec H|,d(\vec G)) \cdot
|\vec G|^{\rho^*\!(\vec H)+O(1)}$ and $f(|\vec H|,d(\vec G)) \cdot |\vec
G|^{\alpha_s(\vec H)+O(1)}$ for counting respectively the copies and induced
copies of $\vec H$ in $\vec G$; on the other hand we show that, unless the
Exponential Time Hypothesis fails, for any class $\vec C$ of directed graphs
the (induced) counting problem is fixed-parameter tractable if and only if
$\rho^*(\vec C)$ ($\alpha_s(\vec C)$) is bounded. These results explain how the
orientation of the pattern can make counting easy or hard, and prove that a
classic algorithm by Chiba and Nishizeki and its extensions (Chiba, Nishizeki
SICOMP 85; Bressan Algorithmica 21) are optimal unless ETH fails.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-04 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Membership in moment cones, quiver semi-invariants, and generic semi-stability for bipartite quivers</title>
  <guid>http://arxiv.org/abs/2211.01990</guid>
  <link>http://arxiv.org/abs/2211.01990</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chindris_C/0/1/0/all/0/1&quot;&gt;Calin Chindris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Collins_B/0/1/0/all/0/1&quot;&gt;Brett Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kline_D/0/1/0/all/0/1&quot;&gt;Daniel Kline&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Let $Q$ be a bipartite quiver with vertex set $Q_0$ such that the number of
arrows between any two source and sink vertices is constant. Let
$\beta=(\beta(x))_{x \in Q_0}$ be a dimension vector of $Q$ with positive
integer coordinates, and let $\Delta(Q, \beta)$ be the moment cone associated
to $(Q, \beta)$. We show that the membership problem for $\Delta(Q, \beta)$ can
be solved in strongly polynomial time.
&lt;/p&gt;
&lt;p&gt;As a key step in our approach, we first solve the polytopal problem for
semi-invariants of $Q$ and its flag-extensions. Specifically, let $Q_{\beta}$
be the flag-extension of $Q$ obtained by attaching a flag $\mathcal{F}(x)$ of
length $\beta(x)-1$ at every vertex $x$ of $Q$, and let $\widetilde{\beta}$ be
the extension of $\beta$ to $Q_{\beta}$ that takes values $1, \ldots, \beta(x)$
along the vertices of the flag $\mathcal{F}(x)$ for every vertex $x$ of $Q$.
For an integral weight $\widetilde{\sigma}$ of $Q_{\beta}$, let
$K_{\widetilde{\sigma}}$ be the dimension of the space of semi-invariants of
weight $\widetilde{\sigma}$ on the representation space of
$\widetilde{\beta}$-dimensional complex representations of $Q_{\beta}$.
&lt;/p&gt;
&lt;p&gt;We show that $K_{\widetilde{\sigma}}$ can be expressed as the number of
lattice points of a certain hive-type polytope. This polytopal description
together with Derksen-Weyman&#39;s Saturation Theorem for quiver semi-invariants
allows us to use Tardos&#39;s algorithm to solve the membership problem for
$\Delta(Q,\beta)$ in strongly polynomial time. In particular, this yields a
strongly polynomial time algorithm for solving the generic semi-stability
problem for representations of $Q$ and $Q_\beta$.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-04 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Complexity of Simon&#39;s problem in classical sense</title>
  <guid>http://arxiv.org/abs/2211.01776</guid>
  <link>http://arxiv.org/abs/2211.01776</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zantema_H/0/1/0/all/0/1&quot;&gt;Hans Zantema&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Simon&#39;s problem is a standard example of a problem that is exponential in
classical sense, while it admits a polynomial solution in quantum computing. It
is about a function $f$ for which it is given that a unique non-zero vector $s$
exists for which $f(x) = f(x \oplus s)$ for all $x$, where $\oplus$ is the
exclusive or operator. The goal is to find $s$. The exponential lower bound for
the classical sense assumes that $f$ only admits black box access. In this
paper we investigate classical complexity when $f$ is given by a standard
representation like a circuit. We focus on finding the vector space of all
vectors $s$ for which $f(x) = f(x \oplus s)$ for all $x$, for any given $f$.
Two main results are: (1) if $f$ is given by any circuit, then checking whether
this vector space contains a non-zero element is NP-hard, and (2) if $f$ is
given by any ordered BDD, then a basis of this vector space can be computed in
polynomial time.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-04 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>A New Approach to Estimating Effective Resistances and Counting Spanning Trees in Expander Graphs</title>
  <guid>http://arxiv.org/abs/2211.01468</guid>
  <link>http://arxiv.org/abs/2211.01468</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lawrence Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachdeva_S/0/1/0/all/0/1&quot;&gt;Sushant Sachdeva&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We demonstrate that for expander graphs, for all $\epsilon &amp;gt; 0,$ there exists
a data structure of size $\widetilde{O}(n\epsilon^{-1})$ which can be used to
return $(1 + \epsilon)$-approximations to effective resistances in
$\widetilde{O}(1)$ time per query. Short of storing all effective resistances,
previous best approaches could achieve $\widetilde{O}(n\epsilon^{-2})$ size and
$\widetilde{O}(\epsilon^{-2})$ time per query by storing Johnson-Lindenstrauss
vectors for each vertex, or $\widetilde{O}(n\epsilon^{-1})$ size and
$\widetilde{O}(n\epsilon^{-1})$ time per query by storing a spectral sketch.
&lt;/p&gt;
&lt;p&gt;Our construction is based on two key ideas: 1) $\epsilon^{-1}$-sparse,
$\epsilon$-additive approximations to $DL^+1_u$ for all $u,$ can be used to
recover $(1 + \epsilon)$-approximations to the effective resistances, 2) In
expander graphs, only $\widetilde{O}(\epsilon^{-1})$ coordinates of a vector
similar to $DL^+1_u$ are larger than $\epsilon.$ We give an efficient
construction for such a data structure in $\widetilde{O}(m + n\epsilon^{-2})$
time via random walks. This results in an algorithm for computing
$(1+\epsilon)$-approximate effective resistances for $s$ vertex pairs in
expanders that runs in $\widetilde{O}(m + n\epsilon^{-2} + s)$ time, improving
over the previously best known running time of $m^{1 + o(1)} + (n +
s)n^{o(1)}\epsilon^{-1.5}$ for $s = \omega(n\epsilon^{-0.5}).$
&lt;/p&gt;
&lt;p&gt;We employ the above algorithm to compute a $(1+\delta)$-approximation to the
number of spanning trees in an expander graph, or equivalently, approximating
the (pseudo)determinant of its Laplacian in $\widetilde{O}(m +
n^{1.5}\delta^{-1})$ time. This improves on the previously best known result of
$m^{1+o(1)} + n^{1.875+o(1)}\delta^{-1.75}$ time, and matches the best known
size of determinant sparsifiers.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-04 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Computing a many-to-many matching with demands and capacities between two sets using the Hungarian algorithm</title>
  <guid>http://arxiv.org/abs/2211.01612</guid>
  <link>http://arxiv.org/abs/2211.01612</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajabi_Alni_F/0/1/0/all/0/1&quot;&gt;Fatemeh Rajabi-Alni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagheri_A/0/1/0/all/0/1&quot;&gt;Alireza Bagheri&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Given two sets A={a_1,a_2,...,a_s} and {b_1,b_2,...,b_t}, a many-to-many
matching with demands and capacities (MMDC) between A and B matches each
element a_i in A to at least \alpha_i and at most \alpha&#39;_i elements in B, and
each element b_j in B to at least \beta_j and at most \beta&#39;_j elements in A
for all 1=&amp;lt;i&amp;lt;=s and 1=&amp;lt;j&amp;lt;=t. In this paper, we present an algorithm for finding
a minimum-cost MMDC between A and B using the well-known Hungarian algorithm.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-04 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Pairing optimization via statistics: Algebraic structure in pairing problems and its application to performance enhancement</title>
  <guid>http://arxiv.org/abs/2211.01661</guid>
  <link>http://arxiv.org/abs/2211.01661</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujita_N/0/1/0/all/0/1&quot;&gt;Naoki Fujita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rohm_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; R&amp;#xf6;hm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mihana_T/0/1/0/all/0/1&quot;&gt;Takatomo Mihana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horisaki_R/0/1/0/all/0/1&quot;&gt;Ryoichi Horisaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Aohan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasegawa_M/0/1/0/all/0/1&quot;&gt;Mikio Hasegawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naruse_M/0/1/0/all/0/1&quot;&gt;Makoto Naruse&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Fully pairing all elements of a set while attempting to maximize the total
benefit is a combinatorically difficult problem. Such pairing problems
naturally appear in various situations in science, technology, economics, and
other fields. In our previous study, we proposed an efficient method to infer
the underlying compatibilities among the entities, under the constraint that
only the total compatibility is observable. Furthermore, by transforming the
pairing problem into a traveling salesman problem with a multi-layer
architecture, a pairing optimization algorithm was successfully demonstrated to
derive a high-total-compatibility pairing. However, there is substantial room
for further performance enhancement by further exploiting the underlying
mathematical properties. In this study, we prove the existence of algebraic
structures in the pairing problem. We transform the initially estimated
compatibility information into an equivalent form where the variance of the
individual compatibilities is minimized. We then demonstrate that the total
compatibility obtained when using the heuristic pairing algorithm on the
transformed problem is significantly higher compared to the previous method.
With this improved perspective on the pairing problem using fundamental
mathematical properties, we can contribute to practical applications such as
wireless communications beyond 5G, where efficient pairing is of critical
importance.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-04 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>A Round and Bipartize Approximation Algorithm for Vertex Cover</title>
  <guid>http://arxiv.org/abs/2211.01699</guid>
  <link>http://arxiv.org/abs/2211.01699</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kashaev_D/0/1/0/all/0/1&quot;&gt;Danish Kashaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schafer_G/0/1/0/all/0/1&quot;&gt;Guido Sch&amp;#xe4;fer&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The vertex cover problem is a fundamental and widely studied combinatorial
optimization problem. It is known that its standard linear programming
relaxation is integral for bipartite graphs and half-integral for general
graphs. As a consequence, the natural rounding algorithm based on this
relaxation computes an optimal solution for bipartite graphs and a
$2$-approximation for general graphs. This raises the question of whether one
can obtain improved bounds on the approximation ratio, depending on how close
the graph is to being bipartite.
&lt;/p&gt;
&lt;p&gt;In this paper, we consider a round-and-bipartize algorithm that exploits the
knowledge of an induced bipartite subgraph to attain improved approximation
ratios. Equivalently, we suppose that we have access to a subset of vertices
$S$ whose removal bipartizes the graph.
&lt;/p&gt;
&lt;p&gt;If $S$ is an independent set, we prove an approximation ratio of $1 +
1/\rho$, where $2\rho -1$ denotes the odd girth of the contracted graph
$\tilde{\mathcal{G}} := \mathcal{G} /S$ and thus satisfies $\rho \geq 2$. We
show that this is tight for any graph and independent set by providing a family
of weight functions for which this bound is attained. In addition, we give
tight upper bounds for the fractional chromatic number and the integrality gap
of such graphs, both of which also depend on the odd girth.
&lt;/p&gt;
&lt;p&gt;If $S$ is an arbitrary set, we prove a tight approximation ratio of
$\left(1+1/\rho \right) (1 - \alpha) + 2 \alpha$, where $\alpha \in [0,1]$
denotes the total normalized dual sum of the edges lying inside of the set $S$.
As an algorithmic application, we show that for any efficiently $k$-colorable
graph with $k \geq 4$ we can find a bipartizing set satisfying $\alpha \leq 1 -
4/k$. This provides an approximation algorithm recovering the bound of $2 -
2/k$ in the worst case (i.e., when $\rho = 2$), which is best possible for this
setting when using the standard relaxation.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-04 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Efficient Branch-and-Bound Algorithms for Finding Triangle-Constrained 2-Clubs</title>
  <guid>http://arxiv.org/abs/2211.01701</guid>
  <link>http://arxiv.org/abs/2211.01701</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gruttemeier_N/0/1/0/all/0/1&quot;&gt;Niels Gr&amp;#xfc;ttemeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komusiewicz_C/0/1/0/all/0/1&quot;&gt;Christian Komusiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kessler_P/0/1/0/all/0/1&quot;&gt;Philipp Heinrich Ke&amp;#xdf;ler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sommer_F/0/1/0/all/0/1&quot;&gt;Frank Sommer&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In the Vertex Triangle 2-Club problem, we are given an undirected graph $G$
and aim to find a maximum-vertex subgraph of $G$ that has diameter at most 2
and in which every vertex is contained in at least $\ell$ triangles in the
subgraph. So far, the only algorithm for solving Vertex Triangle 2-Club relies
on an ILP formulation [Almeida and Br\&#39;as, Comput. Oper. Res. 2019]. In this
work, we develop a combinatorial branch-and-bound algorithm that, coupled with
a set of data reduction rules, outperforms the existing implementation and is
able to find optimal solutions on sparse real-world graphs with more than
100,000 vertices in a few minutes. We also extend our algorithm to the Edge
Triangle 2-Club problem where the triangle constraint is imposed on all edges
of the subgraph.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-04 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Model-Checking for First-Order Logic with Disjoint Paths Predicates in Proper Minor-Closed Graph Classes</title>
  <guid>http://arxiv.org/abs/2211.01723</guid>
  <link>http://arxiv.org/abs/2211.01723</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golovach_P/0/1/0/all/0/1&quot;&gt;Petr A. Golovach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stamoulis_G/0/1/0/all/0/1&quot;&gt;Giannos Stamoulis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thilikos_D/0/1/0/all/0/1&quot;&gt;Dimitrios M. Thilikos&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The disjoint paths logic, FOL+DP, is an extension of First-Order Logic (FOL)
with the extra atomic predicate ${\sf dp}_k(x_1,y_1,\ldots,x_k,y_k),$
expressing the existence of internally vertex-disjoint paths between $x_i$ and
$y_i,$ for $i\in\{1,\ldots, k\}$. This logic can express a wide variety of
problems that escape the expressibility potential of FOL. We prove that for
every proper minor-closed graph class, model-checking for FOL+DP can be done in
quadratic time. We also introduce an extension of FOL+DP, namely the scattered
disjoint paths logic, FOL+SDP, where we further consider the atomic predicate
$s{\sf -sdp}_k(x_1,y_1,\ldots,x_k,y_k),$ demanding that the disjoint paths are
within distance bigger than some fixed value $s$. Using the same technique we
prove that model-checking for FOL+SDP can be done in quadratic time on classes
of graphs with bounded Euler genus.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-04 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Distributed Reconfiguration of Spanning Trees</title>
  <guid>http://arxiv.org/abs/2211.01725</guid>
  <link>http://arxiv.org/abs/2211.01725</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Siddharth Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1&quot;&gt;Manish Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pai_S/0/1/0/all/0/1&quot;&gt;Shreyas Pai&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In a reconfiguration problem, given a problem and two feasible solutions of
the problem, the task is to find a sequence of transformations to reach from
one solution to the other such that every intermediate state is also a feasible
solution to the problem. In this paper, we study the distributed spanning tree
reconfiguration problem and we define a new reconfiguration step, called
$k$-simultaneous add and delete, in which every node is allowed to add at most
$k$ edges and delete at most $k$ edges such that multiple nodes do not add or
delete the same edge.
&lt;/p&gt;
&lt;p&gt;We first observe that, if the two input spanning trees are rooted, then we
can do the reconfiguration using a single $1$-simultaneous add and delete step
in one round in the CONGEST model. Therefore, we focus our attention towards
unrooted spanning trees and show that transforming an unrooted spanning tree
into another using a single $1$-simultaneous add and delete step requires
$\Omega(n)$ rounds in the LOCAL model. We additionally show that transforming
an unrooted spanning tree into another using a single $2$-simultaneous add and
delete step can be done in $O(\log n)$ rounds in the CONGEST model.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-04 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>SQUID: Faster Analytics via Sampled Quantiles Data-structure</title>
  <guid>http://arxiv.org/abs/2211.01726</guid>
  <link>http://arxiv.org/abs/2211.01726</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Basat_R/0/1/0/all/0/1&quot;&gt;Ran Ben-Basat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Einziger_G/0/1/0/all/0/1&quot;&gt;Gil Einziger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;Wenchen Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tayh_B/0/1/0/all/0/1&quot;&gt;Bilal Tayh&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Measurement is a fundamental enabler of network applications such as load
balancing, attack detection and mitigation, and traffic engineering. A key
building block in many critical measurement tasks is \emph{q-MAX}, where we
wish to find the largest $q$ values in a number stream. A standard approach of
maintaining a heap of the largest $q$ values ordered results in logarithmic
runtime, which is too slow for large measurements. Modern approaches attain a
constant runtime by removing small items in bulk and retaining the largest $q$
items at all times. Yet, these approaches are bottlenecked by an expensive
quantile calculation method.
&lt;/p&gt;
&lt;p&gt;We propose SQUID, a method that redesigns q-MAX to allow the use of
\emph{approximate quantiles}, which we can compute efficiently, thereby
accelerating the solution and, subsequently, many measurement tasks. We
demonstrate the benefit of our approach by designing a novel weighted heavy
hitters data structure that is faster and more accurate than the existing
alternatives. Here, we combine our previous techniques with a lazy deletion of
small entries, which expiates the maintenance process and increases the
accuracy. We also demonstrate the applicability of our algorithmic approach in
a general algorithmic scope by implementing the LRFU cache policy with a
constant update time. Furthermore, we also show the practicality of SQUID for
improving real-world networked systems, by implementing a P4 prototype of SQUID
for in-network caching and demonstrating how SQUID enables a wide spectrum of
score-based caching policies directly on a P4 switch.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-04 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Matching Augmentation via Simultaneous Contractions</title>
  <guid>http://arxiv.org/abs/2211.01912</guid>
  <link>http://arxiv.org/abs/2211.01912</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1&quot;&gt;Mohit Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hommelsheim_F/0/1/0/all/0/1&quot;&gt;Felix Hommelsheim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Megow_N/0/1/0/all/0/1&quot;&gt;Nicole Megow&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We consider the matching augmentation problem (MAP), where a matching of a
graph needs to be extended into a $2$-edge-connected spanning subgraph by
adding the minimum number of edges to it. We present a polynomial-time
algorithm with an approximation ratio of $13/8 = 1.625$ improving upon an
earlier $5/3$-approximation. The improvement builds on a new
$\alpha$-approximation preserving reduction for any $\alpha\geq 3/2$ from
arbitrary MAP instances to well-structured instances that do not contain
certain forbidden structures like parallel edges, small separators, and
contractible subgraphs. We further introduce, as key ingredients, the technique
of repeated simultaneous contractions and provide improved lower bounds for
instances that cannot be contracted.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-04 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Distributed Maximal Matching and Maximal Independent Set on Hypergraphs</title>
  <guid>http://arxiv.org/abs/2211.01945</guid>
  <link>http://arxiv.org/abs/2211.01945</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balliu_A/0/1/0/all/0/1&quot;&gt;Alkida Balliu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brandt_S/0/1/0/all/0/1&quot;&gt;Sebastian Brandt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhn_F/0/1/0/all/0/1&quot;&gt;Fabian Kuhn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olivetti_D/0/1/0/all/0/1&quot;&gt;Dennis Olivetti&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We investigate the distributed complexity of maximal matching and maximal
independent set (MIS) in hypergraphs in the LOCAL model. A maximal matching of
a hypergraph $H=(V_H,E_H)$ is a maximal disjoint set $M\subseteq E_H$ of
hyperedges and an MIS $S\subseteq V_H$ is a maximal set of nodes such that no
hyperedge is fully contained in $S$. Both problems can be solved by a simple
sequential greedy algorithm, which can be implemented naively in $O(\Delta r +
\log^* n)$ rounds, where $\Delta$ is the maximum degree, $r$ is the rank, and
$n$ is the number of nodes.
&lt;/p&gt;
&lt;p&gt;We show that for maximal matching, this naive algorithm is optimal in the
following sense. Any deterministic algorithm for solving the problem requires
$\Omega(\min\{\Delta r, \log_{\Delta r} n\})$ rounds, and any randomized one
requires $\Omega(\min\{\Delta r, \log_{\Delta r} \log n\})$ rounds. Hence, for
any algorithm with a complexity of the form $O(f(\Delta, r) + g(n))$, we have
$f(\Delta, r) \in \Omega(\Delta r)$ if $g(n)$ is not too large, and in
particular if $g(n) = \log^* n$ (which is the optimal asymptotic dependency on
$n$ due to Linial&#39;s lower bound [FOCS&#39;87]). Our lower bound proof is based on
the round elimination framework, and its structure is inspired by a new round
elimination fixed point that we give for the $\Delta$-vertex coloring problem
in hypergraphs.
&lt;/p&gt;
&lt;p&gt;For the MIS problem on hypergraphs, we show that for $\Delta\ll r$, there are
significant improvements over the naive $O(\Delta r + \log^* n)$-round
algorithm. We give two deterministic algorithms for the problem. We show that a
hypergraph MIS can be computed in $O(\Delta^2\cdot\log r + \Delta\cdot\log
r\cdot \log^* r + \log^* n)$ rounds. We further show that at the cost of a
worse dependency on $\Delta$, the dependency on $r$ can be removed almost
entirely, by giving an algorithm with complexity $\Delta^{O(\Delta)}\cdot\log^*
r + O(\log^* n)$.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-04 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Truthful Matching with Online Items and Offline Agents</title>
  <guid>http://arxiv.org/abs/2211.02004</guid>
  <link>http://arxiv.org/abs/2211.02004</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldman_M/0/1/0/all/0/1&quot;&gt;Michal Feldman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fusco_F/0/1/0/all/0/1&quot;&gt;Federico Fusco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leonardi_S/0/1/0/all/0/1&quot;&gt;Stefano Leonardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mauras_S/0/1/0/all/0/1&quot;&gt;Simon Mauras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reiffenhauser_R/0/1/0/all/0/1&quot;&gt;Rebecca Reiffenh&amp;#xe4;user&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study truthful mechanisms for welfare maximization in online bipartite
matching. In our (multi-parameter) setting, every buyer is associated with a
(possibly private) desired set of items, and has a private value for being
assigned an item in her desired set. Unlike most online matching settings,
where agents arrive online, in our setting the items arrive online in an
adversarial order while the buyers are present for the entire duration of the
process. This poses a significant challenge to the design of truthful
mechanisms, due to the ability of buyers to strategize over future rounds. We
provide an almost full picture of the competitive ratios in different
scenarios, including myopic vs. non-myopic agents, tardy vs. prompt payments,
and private vs. public desired sets. Among other results, we identify the
frontier for which the celebrated $e/(e-1)$ competitive ratio for the
vertex-weighted online matching of Karp, Vazirani and Vazirani extends to
truthful agents and online items.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-04 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Competitive Kill-and-Restart Strategies for Non-Clairvoyant Scheduling</title>
  <guid>http://arxiv.org/abs/2211.02044</guid>
  <link>http://arxiv.org/abs/2211.02044</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jager_S/0/1/0/all/0/1&quot;&gt;Sven J&amp;#xe4;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sagnol_G/0/1/0/all/0/1&quot;&gt;Guillaume Sagnol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waldschmidt_D/0/1/0/all/0/1&quot;&gt;Daniel Schmidt genannt Waldschmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warode_P/0/1/0/all/0/1&quot;&gt;Philipp Warode&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We consider the fundamental scheduling problem of minimizing the sum of
weighted completion times on a single machine in the non-clairvoyant setting.
While no non-preemptive algorithm is constant competitive, Motwani, Phillips,
and Torng (SODA &#39;93) proved that the simple preemptive round robin procedure is
$2$-competitive and that no better competitive ratio is possible, initiating a
long line of research focused on preemptive algorithms for generalized variants
of the problem. As an alternative model, Shmoys, Wein, and Williamson (FOCS
&#39;91) introduced kill-and-restart schedules, where running jobs may be killed
and restarted from scratch later, and analyzed then for the makespan objective.
However, to the best of our knowledge, this concept has never been considered
for the total completion time objective in the non-clairvoyant model.
&lt;/p&gt;
&lt;p&gt;We contribute to both models: First we give for any $b &amp;gt; 1$ a tight analysis
for the natural $b$-scaling kill-and-restart strategy for scheduling jobs
without release dates, as well as for a randomized variant of it. This implies
a performance guarantee of $(1+3\sqrt{3})\approx 6.197$ for the deterministic
algorithm and of $\approx 3.032$ for the randomized version. Second, we show
that the preemptive Weighted Shortest Elapsed Time First (WSETF) rule is
$2$-competitive for jobs released in an online fashion over time, matching the
lower bound by Motwani et al. Using this result as well as the competitiveness
of round robin for multiple machines, we prove performance guarantees of
adaptions of the $b$-scaling algorithm to online release dates and unweighted
jobs on identical parallel machines.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-04 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>TR22-142 |  Correlation bounds against polynomials | 

	Emanuele Viola</title>
  <guid>https://eccc.weizmann.ac.il/report/2022/142</guid>
  <link>https://eccc.weizmann.ac.il/report/2022/142</link>
  <description>
    A survey on correlation bounds against polynomials.
  </description>
  <pubDate>2022-11-03 15:00:46 UTC</pubDate>
  <author>ECCC Papers</author>
</item>

<item>
  <title>Should you quit Twitter and Texas?</title>
  <guid>tag:blogger.com,1999:blog-3722233.post-1022697361105747430</guid>
  <link>http://blog.computationalcomplexity.org/2022/11/should-you-quit-twitter-and-texas.html</link>
  <description>
    &lt;p&gt;Generally with some exceptions, I use &lt;a href=&quot;https://facebook.com/fortnow&quot;&gt;Facebook&lt;/a&gt; for personal stuff, &lt;a href=&quot;https://www.linkedin.com/in/fortnow/&quot;&gt;LinkedIn&lt;/a&gt; for Illinois Tech stuff and &lt;a href=&quot;https://twitter.com/fortnow&quot;&gt;Twitter&lt;/a&gt;&amp;nbsp;and this blog for CS stuff. Many of you got to this post through the &lt;a href=&quot;https://twitter.com/fortnow/status/1588144625755328514&quot;&gt;Twitter link&lt;/a&gt;. Now that Elon Musk has bought the social media company, should I and the rest of the academic twitterverse move on to something else?&lt;/p&gt;&lt;p&gt;I&#39;d say not yet. Let&#39;s see what Elon does to the place. Maybe he can allow more points of view, without turning it into a cesspool. Or maybe he ruins it. It&#39;ll be a network effect--if too many academics leave Twitter, I&#39;d have to follow or I&#39;d have few followers. I wonder where they will go. I hope it isn&#39;t TikTok.&lt;/p&gt;&lt;p&gt;On a similar vein, I often here of those who suggest we don&#39;t hold conferences in certain jurisdictions for political reasons, for example Texas, because of its laws against abortion and transgender rights. I don&#39;t believe computer science, as a field, should be making decisions based on politics. Academics who live in these states don&#39;t generally hold the same views as the political leaders in those states.&lt;/p&gt;&lt;p&gt;Should we not have meetings in Illinois because some in our field might be opposed to abortion? Or do we just assume everyone has the same political views in the field. Individuals can make their own choices as to whether to attend, but it&#39;s best when politics is left out of academics. &lt;a href=&quot;https://focs2022.eecs.berkeley.edu/&quot;&gt;FOCS 2022&lt;/a&gt; is wrapping up today in Denver. Seems like a safe choice--perhaps all US conferences in the future should be in Colorado.&amp;nbsp;&lt;/p&gt;&lt;p&gt;There are limits--I wouldn&#39;t attend or organize a conference in Russia in the near future. But if we start eliminating locations based on politics, we&#39;ll only be able to meet up in the metaverse, and we won&#39;t have social media to tell us how to get there.&lt;/p&gt;&lt;p class=&quot;authors&quot;&gt;By Lance Fortnow&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 12:18:00 UTC</pubDate>
  <author>Computational Complexity</author>
</item>

<item>
  <title>Persistent Tensors and Multiqudit Entanglement Transformation</title>
  <guid>http://arxiv.org/abs/2211.00652</guid>
  <link>http://arxiv.org/abs/2211.00652</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Gharahi_M/0/1/0/all/0/1&quot;&gt;Masoud Gharahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Lysikov_V/0/1/0/all/0/1&quot;&gt;Vladimir Lysikov&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We construct a lower bound of the tensor rank for a new class of tensors,
which we call persistent tensors. We present three specific families of
persistent tensors, of which the lower bound is tight. We show that there is a
chain of degenerations between these three families of minimal-rank persistent
tensors that can be used to study the entanglement transformation between them.
In addition, we show that these three families of persistent tensors are indeed
different generalizations of multiqubit $\rm{W}$ states within multiqudit
systems and are geometrically in the orbit closure of multiqudit $\rm{GHZ}$
states. Consequently, we show that one can obtain every one of the
generalizations of $\rm{W}$ state from a multiqudit $\rm{GHZ}$ state via
asymptotic Stochastic Local Operations and Classical Communication (SLOCC) with
rate one. Finally, we extend the obtained lower bound of the tensor rank to
direct sums with persistent summands and to even more general combinations of
tensors, which we call block pyramidal tensors. As a result, we show that the
tensor rank is multiplicative under the Kronecker and tensor products of
minimal-rank persistent tensors with the $\rm{GHZ}$ tensor.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Quantum Pseudoentanglement</title>
  <guid>http://arxiv.org/abs/2211.00747</guid>
  <link>http://arxiv.org/abs/2211.00747</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Bouland_A/0/1/0/all/0/1&quot;&gt;Adam Bouland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Fefferman_B/0/1/0/all/0/1&quot;&gt;Bill Fefferman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Soumik Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Vazirani_U/0/1/0/all/0/1&quot;&gt;Umesh Vazirani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zixin Zhou&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Quantum pseudorandom states are efficiently constructable states which
nevertheless masquerade as Haar-random states to poly-time observers. First
defined by Ji, Liu and Song, such states have found a number of applications
ranging from cryptography to the AdS/CFT correspondence. A fundamental question
is exactly how much entanglement is required to create such states. Haar-random
states, as well as $t$-designs for $t\geq 2$, exhibit near-maximal
entanglement. Here we provide the first construction of pseudorandom states
with only polylogarithmic entanglement entropy across an equipartition of the
qubits, which is the minimum possible. Our construction can be based on any
one-way function secure against quantum attack. We additionally show that the
entanglement in our construction is fully &quot;tunable&quot;, in the sense that one can
have pseudorandom states with entanglement $\Theta(f(n))$ for any desired
function $\omega(\log n) \leq f(n) \leq O(n)$.
&lt;/p&gt;
&lt;p&gt;More fundamentally, our work calls into question to what extent entanglement
is a &quot;feelable&quot; quantity of quantum systems. Inspired by recent work of
Gheorghiu and Hoban, we define a new notion which we call &quot;pseudoentanglement&quot;,
which are ensembles of efficiently constructable quantum states which hide
their entanglement entropy. We show such states exist in the strongest form
possible while simultaneously being pseudorandom states. We also describe
diverse applications of our result from entanglement distillation to property
testing to quantum gravity.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Polynomial Identity Testing via Evaluation of Rational Functions</title>
  <guid>http://arxiv.org/abs/2211.01062</guid>
  <link>http://arxiv.org/abs/2211.01062</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melkebeek_D/0/1/0/all/0/1&quot;&gt;Dieter van Melkebeek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morgan_A/0/1/0/all/0/1&quot;&gt;Andrew Morgan&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We introduce a hitting set generator for Polynomial Identity Testing based on
evaluations of low-degree univariate rational functions at abscissas associated
with the variables. Despite the univariate nature, we establish an equivalence
up to rescaling with a generator introduced by Shpilka and Volkovich, which has
a similar structure but uses multivariate polynomials in the abscissas.
&lt;/p&gt;
&lt;p&gt;We study the power of the generator by characterizing its vanishing ideal,
i.e., the set of polynomials that it fails to hit. Capitalizing on the
univariate nature, we develop a small collection of polynomials that jointly
produce the vanishing ideal. As corollaries, we obtain tight bounds on the
minimum degree, sparseness, and partition class size of set-multilinearity in
the vanishing ideal. Inspired by an alternating algebra representation, we
develop a structured deterministic membership test for the vanishing ideal. As
a proof of concept, we rederive known derandomization results based on the
generator by Shpilka and Volkovich and present a new application for read-once
oblivious algebraic branching programs.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Partitioning a Polygon Into Small Pieces</title>
  <guid>http://arxiv.org/abs/2211.01359</guid>
  <link>http://arxiv.org/abs/2211.01359</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abrahamsen_M/0/1/0/all/0/1&quot;&gt;Mikkel Abrahamsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasmussen_N/0/1/0/all/0/1&quot;&gt;Nichlas Langhoff Rasmussen&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the problem of partitioning a given simple polygon $P$ into a
minimum number of polygonal pieces, each of which has bounded size. We give
algorithms for seven notions of `bounded size,&#39; namely that each piece has
bounded area, perimeter, straight-line diameter, geodesic diameter, or that
each piece must be contained in a unit disk, an axis-aligned unit square or an
arbitrarily rotated unit square.
&lt;/p&gt;
&lt;p&gt;A more general version of the area problem has already been studied. Here we
are, in addition to $P$, given positive real values $a_1,\ldots,a_k$ such that
the sum $\sum_{i=1}^k a_i$ equals the area of $P$. The goal is to partition $P$
into exactly $k$ pieces $Q_1,\ldots,Q_k$ such that the area of $Q_i$ is $a_i$.
Such a partition always exists, and an algorithm with running time $O(nk)$ has
previously been described, where $n$ is the number of corners of $P$. We give
an algorithm with optimal running time $O(n+k)$. For polygons with holes, we
get running time $O(n\log n+k)$.
&lt;/p&gt;
&lt;p&gt;For the other problems, it seems out of reach to compute optimal partitions
for simple polygons; for most of them, even in extremely restricted cases such
as when $P$ is a square. We therefore develop $O(1)$-approximation algorithms
for these problems, which means that the number of pieces in the produced
partition is at most a constant factor larger than the cardinality of a minimum
partition. Existing algorithms do not allow Steiner points, which means that
all corners of the produced pieces must also be corners of $P$. This has the
disappointing consequence that a partition does often not exist, whereas our
algorithms always produce useful partitions. Furthermore, an optimal partition
without Steiner points may require $\Omega(n)$ pieces for polygons where a
partition consisting of just $2$ pieces exists when Steiner points are allowed.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Geometry</author>
</item>

<item>
  <title>Simplified Prophet Inequalities for Combinatorial Auctions</title>
  <guid>http://arxiv.org/abs/2211.00707</guid>
  <link>http://arxiv.org/abs/2211.00707</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_A/0/1/0/all/0/1&quot;&gt;Alexander Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kesselheim_T/0/1/0/all/0/1&quot;&gt;Thomas Kesselheim&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We consider prophet inequalities for XOS and MPH-$k$ combinatorial auctions
and give a simplified proof for the existence of static and anonymous item
prices which recover the state-of-the-art competitive ratios.
&lt;/p&gt;
&lt;p&gt;Our proofs make use of a linear programming formulation which has a
non-negative objective value if there are prices which admit a given
competitive ratio $\alpha \geq 1$. Changing our perspective to dual space by an
application of strong LP duality, we use an interpretation of the dual
variables as probabilities to directly obtain our result. In contrast to
previous work, our proofs do not require to argue about specific values of
buyers for bundles, but only about the presence or absence of items.
&lt;/p&gt;
&lt;p&gt;As a side remark, for any $k \geq 2$, this simplification also leads to a
tiny improvement in the best competitive ratio for MPH-$k$ combinatorial
auctions from $4k-2$ to $2k + 2 \sqrt{k(k-1)} -1$.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Alternative polynomial-time algorithm for Bipartite Matching</title>
  <guid>http://arxiv.org/abs/2211.00711</guid>
  <link>http://arxiv.org/abs/2211.00711</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillemot_S/0/1/0/all/0/1&quot;&gt;Sylvain Guillemot&lt;/a&gt;&lt;/p&gt;&lt;p&gt;If $G$ is a bipartite graph, Hall&#39;s theorem \cite{H35} gives a condition for
the existence of a matching of $G$ covering one side of the bipartition. This
theorem admits a well-known algorithmic proof involving the repeated search of
augmenting paths. We present here an alternative algorithm, using a
game-theoretic formulation of the problem. We also show how to extend this
formulation to the setting of balanced hypergraphs.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Privacy Induces Robustness: Information-Computation Gaps and Sparse Mean Estimation</title>
  <guid>http://arxiv.org/abs/2211.00724</guid>
  <link>http://arxiv.org/abs/2211.00724</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Georgiev_K/0/1/0/all/0/1&quot;&gt;Kristian Georgiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hopkins_S/0/1/0/all/0/1&quot;&gt;Samuel B. Hopkins&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We establish a simple connection between robust and differentially-private
algorithms: private mechanisms which perform well with very high probability
are automatically robust in the sense that they retain accuracy even if a
constant fraction of the samples they receive are adversarially corrupted.
Since optimal mechanisms typically achieve these high success probabilities,
our results imply that optimal private mechanisms for many basic statistics
problems are robust.
&lt;/p&gt;
&lt;p&gt;We investigate the consequences of this observation for both algorithms and
computational complexity across different statistical problems. Assuming the
Brennan-Bresler secret-leakage planted clique conjecture, we demonstrate a
fundamental tradeoff between computational efficiency, privacy leakage, and
success probability for sparse mean estimation. Private algorithms which match
this tradeoff are not yet known -- we achieve that (up to polylogarithmic
factors) in a polynomially-large range of parameters via the Sum-of-Squares
method.
&lt;/p&gt;
&lt;p&gt;To establish an information-computation gap for private sparse mean
estimation, we also design new (exponential-time) mechanisms using fewer
samples than efficient algorithms must use. Finally, we give evidence for
privacy-induced information-computation gaps for several other statistics and
learning problems, including PAC learning parity functions and estimation of
the mean of a multivariate Gaussian.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Benchmarking Hashing Algorithms for Load Balancing in a Distributed Database Environment</title>
  <guid>http://arxiv.org/abs/2211.00741</guid>
  <link>http://arxiv.org/abs/2211.00741</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slesarev_A/0/1/0/all/0/1&quot;&gt;Alexander Slesarev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikhailov_M/0/1/0/all/0/1&quot;&gt;Mikhail Mikhailov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chernishev_G/0/1/0/all/0/1&quot;&gt;George Chernishev&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Modern high load applications store data using multiple database instances.
Such an architecture requires data consistency, and it is important to ensure
even distribution of data among nodes. Load balancing is used to achieve these
goals.
&lt;/p&gt;
&lt;p&gt;Hashing is the backbone of virtually all load balancing systems. Since the
introduction of classic Consistent Hashing, many algorithms have been devised
for this purpose.
&lt;/p&gt;
&lt;p&gt;One of the purposes of the load balancer is to ensure storage cluster
scalability. It is crucial for the performance of the whole system to transfer
as few data records as possible during node addition or removal. The load
balancer hashing algorithm has the greatest impact on this process.
&lt;/p&gt;
&lt;p&gt;In this paper we experimentally evaluate several hashing algorithms used for
load balancing, conducting both simulated and real system experiments. To
evaluate algorithm performance, we have developed a benchmark suite based on
Unidata MDM~ -- a scalable toolkit for various Master Data Management (MDM)
applications. For assessment, we have employed three criteria~ -- uniformity of
the produced distribution, the number of moved records, and computation speed.
Following the results of our experiments, we have created a table, in which
each algorithm is given an assessment according to the abovementioned criteria.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Balancing Utility and Fairness in Submodular Maximization (Technical Report)</title>
  <guid>http://arxiv.org/abs/2211.00980</guid>
  <link>http://arxiv.org/abs/2211.00980</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuchen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonchi_F/0/1/0/all/0/1&quot;&gt;Francesco Bonchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Ying Wang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Submodular function maximization is central in numerous data science
applications, including data summarization, influence maximization, and
recommendation. In many of these problems, our goal is to find a solution that
maximizes the \emph{average} of the utilities for all users, each measured by a
monotone submodular function. When the population of users is composed of
several demographic groups, another critical problem is whether the utility is
fairly distributed across groups. In the context of submodular optimization, we
seek to improve the welfare of the \emph{least well-off} group, i.e., to
maximize the minimum utility for any group, to ensure fairness. Although the
\emph{utility} and \emph{fairness} objectives are both desirable, they might
contradict each other, and, to our knowledge, little attention has been paid to
optimizing them jointly. In this paper, we propose a novel problem called
\emph{Bicriteria Submodular Maximization} (BSM) to strike a balance between
utility and fairness. Specifically, it requires finding a fixed-size solution
to maximize the utility function, subject to the value of the fairness function
not being below a threshold. Since BSM is inapproximable within any constant
factor in general, we propose efficient data-dependent approximation algorithms
for BSM by converting it into other submodular optimization problems and
utilizing existing algorithms for the converted problems to obtain solutions to
BSM. Using real-world and synthetic datasets, we showcase applications of our
framework in three submodular maximization problems, namely maximum coverage,
influence maximization, and facility location.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Joint Correlation Detection and Alignment of Gaussian Databases</title>
  <guid>http://arxiv.org/abs/2211.01069</guid>
  <link>http://arxiv.org/abs/2211.01069</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamir_R/0/1/0/all/0/1&quot;&gt;Ran Tamir&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this work, we propose an efficient two-stage algorithm solving a joint
problem of correlation detection and permutation recovery between two Gaussian
databases. Correlation detection is an hypothesis testing problem; under the
null hypothesis, the databases are independent, and under the alternate
hypothesis, they are correlated, under an unknown row permutation. We develop
relatively tight bounds on the type-I and type-II error probabilities, and show
that the analyzed detector performs better than a recently proposed detector,
at least for some specific parameter choices. Since the proposed detector
relies on a statistic, which is a sum of dependent indicator random variables,
then in order to bound the type-I probability of error, we develop a novel
graph-theoretic technique for bounding the $k$-th order moments of such
statistics. When the databases are accepted as correlated, the algorithm also
outputs an estimation for the underlying row permutation. By comparing to known
converse results for this problem, we prove that the alignment error
probability converges to zero under the asymptotically lowest possible
correlation coefficient.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Set Selection under Explorable Stochastic Uncertainty via Covering Techniques</title>
  <guid>http://arxiv.org/abs/2211.01097</guid>
  <link>http://arxiv.org/abs/2211.01097</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Megow_N/0/1/0/all/0/1&quot;&gt;Nicole Megow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schloter_J/0/1/0/all/0/1&quot;&gt;Jens Schl&amp;#xf6;ter&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Given subsets of uncertain values, we study the problem of identifying the
subset of minimum total value (sum of the uncertain values) by querying as few
values as possible. This set selection problem falls into the field of
explorable uncertainty and is of intrinsic importance therein as it implies
strong adversarial lower bounds for a wide range of interesting combinatorial
problems such as knapsack and matchings. We consider a stochastic problem
variant and give algorithms that, in expectation, improve upon these
adversarial lower bounds. The key to our results is to prove a strong
structural connection to a seemingly unrelated covering problem with
uncertainty in the constraints via a linear programming formulation. We exploit
this connection to derive an algorithmic framework that can be used to solve
both problems under uncertainty, obtaining nearly tight bounds on the
competitive ratio. This is the first non-trivial stochastic result concerning
the sum of unknown values without further structure known for the set. Further,
we handle for the first time uncertainty in the constraints in a value-query
model. With our novel methods, we lay the foundations for solving more general
problems in the area of explorable uncertainty.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>New Tradeoffs for Decremental Approximate All-Pairs Shortest Paths</title>
  <guid>http://arxiv.org/abs/2211.01152</guid>
  <link>http://arxiv.org/abs/2211.01152</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dory_M/0/1/0/all/0/1&quot;&gt;Michal Dory&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forster_S/0/1/0/all/0/1&quot;&gt;Sebastian Forster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nazari_Y/0/1/0/all/0/1&quot;&gt;Yasamin Nazari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vos_T/0/1/0/all/0/1&quot;&gt;Tijn de Vos&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We provide new tradeoffs between approximation and running time for the
decremental all-pairs shortest paths (APSP) problem. For undirected graphs with
$m$ edges and $n$ nodes undergoing edge deletions, we provide two new
approximate decremental APSP algorithms, one for weighted and one for
unweighted graphs. Our first result is an algorithm that supports $(2+
\epsilon)$-approximate all-pairs constant-time distance queries with total
update time $\tilde{O}(m^{1/2}n^{3/2})$ when $m= O(n^{5/3})$ (and $m= n^{1+c}$
for any constant $c &amp;gt;0$), or $\tilde{O}(mn^{2/3})$ when $m = \Omega(n^{5/3})$.
Prior to our work the fastest algorithm for weighted graphs with approximation
at most $3$ had total $\tilde O(mn)$ update time providing a
$(1+\epsilon)$-approximation [Bernstein, SICOMP 2016]. Our technique also
yields a decremental algorithm with total update time $\tilde{O}(nm^{3/4})$
supporting $(2+\epsilon, W_{u,v})$-approximate queries where the second term is
an additional additive term and $W_{u,v}$ is the maximum weight on the shortest
path from $u$ to $v$.
&lt;/p&gt;
&lt;p&gt;Our second result is a decremental algorithm that given an unweighted graph
and a constant integer $k \geq 2 $, supports $(1+\epsilon, 2(k-1))$-approximate
queries and has $\tilde{O}(n^{2-1/k}m^{1/k})$ total update time (when
$m=n^{1+c}$ for any constant $c &amp;gt;0$). For comparison, in the special case of
$(1+\epsilon, 2)$-approximation, this improves over the state-of-the-art
algorithm by [Henzinger, Krinninger, Nanongkai, SICOMP 2016] with total update
time of $\tilde{O}(n^{2.5})$. All of our results are randomized and work
against an oblivious adversary.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>A Two Step Approach to Weighted Bipartite Link Recommendations</title>
  <guid>http://arxiv.org/abs/2211.01153</guid>
  <link>http://arxiv.org/abs/2211.01153</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1&quot;&gt;Nathan Ma&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Many real world person-person or person-product relationships can be modeled
graphically. More specifically, bipartite graphs can be especially useful when
modeling scenarios that involve two disjoint groups. As a result, many existing
papers have utilized bipartite graphs for the classical link recommendation
problem. In this paper, using the principle of bipartite graphs, we present
another approach to this problem with a two step algorithm that takes into
account frequency and similarity between common edges to make recommendations.
We test this approach with bipartite data gathered from the Epinions and
Movielens data sources, and find it to perform with roughly 14 percent error,
which improves upon baseline results. This is a promising result, and can be
refined to generate even more accurate recommendations.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Cluster Assignment in Multi-Agent Systems : Sparsity Bounds and Fault Tolerance</title>
  <guid>http://arxiv.org/abs/2211.01316</guid>
  <link>http://arxiv.org/abs/2211.01316</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sharf_M/0/1/0/all/0/1&quot;&gt;Miel Sharf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zelazo_D/0/1/0/all/0/1&quot;&gt;Daniel Zelazo&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study cluster assignment in homogeneous diffusive multi-agent networks.
Given the number of clusters and agents within each cluster, we design the
network graph ensuring the system will converge to the prescribed cluster
configuration. Using recent results linking clustering and symmetries, we show
that it is possible to design an oriented graph for which the action of the
automorphism group of the graph has orbits of predetermined sizes, guaranteeing
the network will converge to the prescribed cluster configuration. We provide
bounds on the number of edges needed to construct these graphs along with a
constructive approach for their generation. We also consider the robustness of
the clustering process under agent malfunction.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-03 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>PhotoGuard: Defending Against Diffusion-based Image Manipulation</title>
  <guid>https://gradientscience.org/photoguard/</guid>
  <link>https://gradientscience.org/photoguard/</link>
  <description>
    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://use.fontawesome.com/releases/v5.8.1/css/all.css&quot; integrity=&quot;sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf&quot; crossorigin=&quot;anonymous&quot; /&gt;

&lt;style&gt;
    blockquote {
        padding-top: 0;
        padding-bottom: 0;
    }

    .bbutton{
        width: 27.5%;
        margin: 2.5%;
        height: 3rem;
        line-height: 3rem;
        display: inline-block;
        background: #DDD;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: 400;
        color: #000;
    }

    .bbutton:hover{
        background: #e98a99;
    }
&lt;/style&gt;

&lt;script src=&quot;/assets/scripts/onload.js&quot;&gt;&lt;/script&gt;

&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;/assets/css/style.css&quot; /&gt;

&lt;p&gt;&lt;a class=&quot;bbutton&quot; style=&quot;float: left; width: 45%;&quot; href=&quot;#&quot;&gt;
&lt;i class=&quot;fas fa-file-pdf&quot;&gt;&lt;/i&gt;
  Paper Coming Soon!
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a class=&quot;bbutton&quot; style=&quot;float: left; width: 45%;&quot; href=&quot;https://github.com/MadryLab/photoguard&quot;&gt;
&lt;i class=&quot;fab fa-github&quot;&gt;&lt;/i&gt;
 Code
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;A few nights ago on The Daily Show, host Trevor Noah interviewed Mira Murati (CTO of OpenAI) about DALL$\cdot$E 2 and, more generally, the power of AI:&lt;/p&gt;

&lt;div style=&quot;width: 100%; text-align: center; padding-top: 10px; padding-bottom: 10px;&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Ba_C-C6UwlI?start=228&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The whole interview is a great watch, but one thing that stood out to us is Trevors question at 3:50:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;So how do you safeguard them [generative models]? We can very quickly find ourselves in a world where nothing is real, and everything thats real isnt, and we question it. How do you prevent, or can you even prevent that completely?&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Indeed, DALL$\cdot$E (and diffusion models in general) greatly exacerbate the risks of malicious image manipulationwhat previously required extensive knowledge of photoshop can now be done with just a simple natural-language query:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/photoguard/dog_example.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In fact, editing photos of cute dogs is just the tip of the iceberg:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/photoguard/trevor_example.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, back to Trevor Noahs questionis there any hope of protecting against this manipulation? We spent a few nights hacking away, and it turns out thatby leveraging adversarial exampleswe can do exactly that! The details of our scheme are below,  but the essence is that, by slightly modifying (imperceptibly, even) the input image, we can make it immune to direct editing by generative models!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/photoguard/photoguard_headline.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By the way, Michael Kosta is not the only person who has a selfie with Trevor. Hadi  the lead student on this project  took a selfie with Trevor couple years ago too.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;A selfie of Hadi and Trevor Noah&quot; src=&quot;/assets/photoguard/hadi_trevor_selfie.png&quot; style=&quot;width:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, Hadi can leverage diffusion-powered photo editing to deepen his
(imaginary) friendship with Trevor.
If the selfie was guarded, none of this would be possible (sadly for Trevor,
it isnt)!&lt;/p&gt;

&lt;div class=&quot;widget&quot;&gt;
    &lt;div class=&quot;choices_one_full&quot; id=&quot;gen&quot;&gt;
    &lt;span class=&quot;widgetheading&quot; id=&quot;genclass&quot;&gt;Choose an Image&lt;/span&gt;
    &lt;/div&gt;
    &lt;div style=&quot;border-right: 3px white solid;&quot;&gt;
        &lt;img id=&quot;gen0-1&quot; class=&quot;image-container&quot; style=&quot;width: 0%; margin: 0;&quot; /&gt;
        &lt;img id=&quot;gen0-2&quot; class=&quot;image-container&quot; style=&quot;width: 100%; margin: 0;&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;clear:both;&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;some-details&quot;&gt;Some details&lt;/h2&gt;

&lt;p&gt;The core of our immunization process is to leverage so-called &lt;a href=&quot;http://gradientscience.org/intro_adversarial/&quot;&gt;adversarial attacks&lt;/a&gt; on these generative models. In particular, we implemented two different attacks, focused on &lt;em&gt;latent diffusion models&lt;/em&gt; (like &lt;a href=&quot;http://stability.ai&quot;&gt;Stable Diffusion&lt;/a&gt;). For simplicity, we can think of such models as having two parts:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The &lt;em&gt;conditioning mechanism&lt;/em&gt; is how the model incorporates external data such as the starting image and the prompt into its final generation. Typically, a pre-trained encoder converts the external signals to a shared embedding spacethe model concatenates these embeddings and uses them as input to&lt;/li&gt;
  &lt;li&gt;the &lt;em&gt;diffusion process,&lt;/em&gt; which is responsible for generating the final image generated images. There are many good introductions to how exactly diffusion processes work, but the summary is that we start from random noise, and then repeatedly apply a model that denoises the input a little bit at a time.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We construct both a simple attack on the conditioning mechanism, and a complex attack on the end-to-end diffusion process itself.&lt;/p&gt;

&lt;h3 id=&quot;simple-attack&quot;&gt;Simple attack&lt;/h3&gt;

&lt;p&gt;In the simpler of the two attacks, we attack only the conditioning step of the diffusion process. That is, given a starting image $x_0$, we find an image $x_{adv}$ satisfying:&lt;/p&gt;

\[x_{adv} = \arg\max_{\mid\mid x - x_0 \mid\mid \lt \delta} \mathcal{L}(z_x, z_{targ})\]

&lt;p&gt;where $z_x$ is the embedding of the input $x$, and $z_{targ}$ is a fixed embedding. We set $z_{targ}$ to the all zeros vector (or even to an embedding of a random image), causing the diffusion model to completely ignore the starting image and focus only on the prompt.&lt;/p&gt;

&lt;div class=&quot;widget&quot;&gt;
    &lt;div class=&quot;choices_one_full&quot; id=&quot;gen-1&quot;&gt;
    &lt;span class=&quot;widgetheading&quot; id=&quot;genclass&quot;&gt;Choose an Image&lt;/span&gt;
    &lt;/div&gt;
    &lt;div style=&quot;border-right: 3px white solid;&quot;&gt;
        &lt;img id=&quot;gen1-1&quot; class=&quot;image-container&quot; style=&quot;width: 0%; margin: 0;&quot; /&gt;
        &lt;img id=&quot;gen1-2&quot; class=&quot;image-container&quot; style=&quot;width: 100%; margin: 0;&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;clear:both;&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;complex-attack&quot;&gt;Complex attack&lt;/h3&gt;

&lt;p&gt;However, we find that we can do an even stronger attack! In this more complex attack, we modify the starting image with the goal of breaking the &lt;em&gt;whole&lt;/em&gt; end-to-end diffusion process. Because the diffusion process is iterative and involves repeated application of a network, taking gradients through the diffusion process is memory-intensive. We found that differentiating through &lt;em&gt;only four&lt;/em&gt; denoising steps was enough to throw off the entire diffusion process. With a little engineering, we were able to fit four steps onto a single (A100) GPU. As you can see, editing our immunized/defended photos lead to much clearer fake images than the previous attack.&lt;/p&gt;

&lt;p&gt;Here are some examples of fake photos, with and without our immunization!&lt;/p&gt;

&lt;div class=&quot;widget&quot;&gt;
    &lt;div class=&quot;choices_one_full&quot; id=&quot;gen-2&quot;&gt;
    &lt;span class=&quot;widgetheading&quot; id=&quot;genclass&quot;&gt;Choose an Image&lt;/span&gt;
    &lt;/div&gt;
    &lt;div style=&quot;border-right: 3px white solid;&quot;&gt;
        &lt;img id=&quot;gen2-1&quot; class=&quot;image-container&quot; style=&quot;width: 0%; margin: 0;&quot; /&gt;
        &lt;img id=&quot;gen2-2&quot; class=&quot;image-container&quot; style=&quot;width: 100%; margin: 0;&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;clear:both;&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;takeaways-and-future-work&quot;&gt;Takeaways and Future Work&lt;/h2&gt;

&lt;p&gt;So, using relatively simple techniques relating to adversarial examples (and about a weeks worth of hacking), we were able to protect images against manipulation from diffusion-based generative models. That said, this is just the beginning, and there are still many unanswered questions!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We only constructed these examples by using an &lt;a href=&quot;https://stability.ai/blog/stable-diffusion-public-release&quot;&gt;open-source diffusion model&lt;/a&gt; (from &lt;a href=&quot;https://huggingface.co/runwayml/stable-diffusion-inpainting&quot;&gt;HuggingFace&lt;/a&gt;). Is it be possible to make them with only black-box access to the model?&lt;/li&gt;
  &lt;li&gt;Our complex attack uses a lot of memory (we could only fit four diffusion steps onto a single GPU). Meanwhile, &lt;a href=&quot;https://arxiv.org/abs/2205.07460&quot;&gt;recent work&lt;/a&gt; shows that for some diffusion processes, one can obtain the gradient through the entire diffusion process by solving a stochastic differential equation (SDE) and using a constant amount of memory. Is it possible to do something similar more generally?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More generally, were excited about the prospect of adversarial examples being used for forcing &lt;em&gt;intended&lt;/em&gt; behavior, rather than for exploiting vulnerabilities (a phenomenon also seen in our work on &lt;a href=&quot;http://gradientscience.org/unadversarial/&quot;&gt;unadversarial examples&lt;/a&gt;!).&lt;/p&gt;

&lt;script&gt;
 function main() {
     const BASE_DIR = &quot;/assets/photoguard/&quot;;
     
     // Generation
     var genImage1 = document.getElementById(&#39;gen0-1&#39;);
     var genImage2 = document.getElementById(&#39;gen0-2&#39;);
     var genSrcs = range(10).map((name) =&gt; BASE_DIR + &#39;hadi_selfies/&#39; + name + &quot;_orig.png&quot;);
     function genMapper(origSrc, id) {
           genImage1.src = origSrc;
           genImage2.src = origSrc.replace(&quot;_orig.png&quot;, &quot;.png&quot;);
     }
     activate_one_widget(&#39;gen&#39;, genSrcs, genMapper);

    // Generation
     var genImage11 = document.getElementById(&#39;gen1-1&#39;);
     var genImage12 = document.getElementById(&#39;gen1-2&#39;);
     var genSrcs1 = range(8).map((name) =&gt; BASE_DIR + &#39;encoder_attack/&#39; + name + &quot;_orig.png&quot;);
     function genMapper1(origSrc, id) {
           genImage11.src = origSrc;
           genImage12.src = origSrc.replace(&quot;_orig.png&quot;, &quot;.png&quot;);
     }

     activate_one_widget(&#39;gen-1&#39;, genSrcs1, genMapper1);
    
     var genImage21 = document.getElementById(&#39;gen2-1&#39;);
     var genImage22 = document.getElementById(&#39;gen2-2&#39;);
     var genSrcs2 = range(10).map((name) =&gt; BASE_DIR + &#39;diffusion_attack/&#39; + name + &quot;_orig.png&quot;);
     function genMapper2(origSrc, id) {
           genImage21.src = origSrc;
           genImage22.src = origSrc.replace(&quot;_orig.png&quot;, &quot;.png&quot;);
     }

     activate_one_widget(&#39;gen-2&#39;, genSrcs2, genMapper2);
 }
 window.onload = main;

&lt;/script&gt;
  </description>
  <pubDate>2022-11-03 00:00:00 UTC</pubDate>
  <author>Gradient Science</author>
</item>

<item>
  <title>TCS+ talk: Wednesday, November 9  Yaonan Jin, Columbia University</title>
  <guid>http://tcsplus.wordpress.com/?p=646</guid>
  <link>https://tcsplus.wordpress.com/2022/11/02/tcs-talk-wednesday-november-9-yaonan-jin-columbia-university/</link>
  <description>
    &lt;p&gt;The next TCS+ talk will take place this coming Wednesday, November 9th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). &lt;strong&gt;Yaonan Jin&lt;/strong&gt; from Columbia University will speak about &amp;#8220;&lt;em&gt;First Price Auction is 1-1/e Efficient&lt;/em&gt;&amp;#8221; (abstract below).&lt;/p&gt;
&lt;p&gt;You can reserve a spot as an individual or a group to join us live by signing up on &lt;a href=&quot;https://sites.google.com/view/tcsplus/welcome/next-tcs-talk&quot;&gt;the online form&lt;/a&gt;. Registration is &lt;em&gt;not&lt;/em&gt; required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted &lt;a href=&quot;https://sites.google.com/view/tcsplus/welcome/past-talks&quot;&gt;on our website&lt;/a&gt; afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to &lt;a href=&quot;https://sites.google.com/view/tcsplus/welcome/suggest-a-talk&quot;&gt;suggest&lt;/a&gt; a possible topic or speaker, please see &lt;a href=&quot;https://sites.google.com/view/tcsplus/&quot;&gt;the website&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class=&quot;wp-block-quote&quot;&gt;&lt;p&gt;Abstract: We prove that, for the first-price auction, the tight Price of Anarchy (PoA) and the tight Price of Stability (PoS) are both &lt;img src=&quot;https://s0.wp.com/latex.php?latex=1-1%2Fe%5E2+%5Capprox+0.8647&amp;#038;bg=fff&amp;#038;fg=444444&amp;#038;s=0&amp;#038;c=20201002&quot; srcset=&quot;https://s0.wp.com/latex.php?latex=1-1%2Fe%5E2+%5Capprox+0.8647&amp;#038;bg=fff&amp;#038;fg=444444&amp;#038;s=0&amp;#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1-1%2Fe%5E2+%5Capprox+0.8647&amp;#038;bg=fff&amp;#038;fg=444444&amp;#038;s=0&amp;#038;c=20201002&amp;#038;zoom=4.5 4x&quot; alt=&quot;1-1/e^2 &amp;#92;approx 0.8647&quot; class=&quot;latex&quot; /&gt;, closing the gap between the best known bounds [0.7430, 0.8689].&lt;/p&gt;
&lt;p&gt;Based on joint works with Pinyan Lu.&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/2207.01761&quot;&gt;https://arxiv.org/abs/2207.01761&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/2207.04455&quot;&gt;https://arxiv.org/abs/2207.04455&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p class=&quot;authors&quot;&gt;By plustcs&lt;/p&gt;
  </description>
  <pubDate>2022-11-02 15:47:26 UTC</pubDate>
  <author>TCS+ Seminar Series</author>
</item>

<item>
  <title>Composable Coresets for Constrained Determinant Maximization and Beyond</title>
  <guid>http://arxiv.org/abs/2211.00289</guid>
  <link>http://arxiv.org/abs/2211.00289</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahabadi_S/0/1/0/all/0/1&quot;&gt;Sepideh Mahabadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1&quot;&gt;Thuy-Duong Vuong&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the task of determinant maximization under partition constraint, in
the context of large data sets. Given a point set $V\subset \mathbb{R}^d$ that
is partitioned into $s$ groups $V_1,..., V_s$, and integers $k_1,...,k_s$ where
$k=\sum_i k_i$, the goal is to pick $k_i$ points from group $i$ such that the
overall determinant of the picked $k$ points is maximized. Determinant
Maximization and its constrained variants have gained a lot of interest for
modeling diversityand have found applications in the context of fairness and
data summarization.
&lt;/p&gt;
&lt;p&gt;We study the design of composable coresets for the constrained determinant
maximization problem. Composable coresets are small subsets of the data that
(approximately) preserve optimal solutions to optimization tasks and enable
efficient solutions in several other large data models including the
distributed and the streaming settings. In this work, we consider two regimes.
For the case of $k&amp;gt;d$, we show a peeling algorithm that gives us a composable
coreset of size $kd$ with an approximation factor of $d^{O(d)}$. We complement
our results by showing that this approximation factor is tight. For the case of
$k\leq d$, we show that a simple modification of the previous algorithms
results in an optimal coreset verified by our lower bounds. Our results apply
to all strongly Rayleigh distribution and several other experimental design
problems. In addition, we show coreset construction algorithms under the more
general laminar matroid constraints.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-02 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>A GPU-friendly, Parallel, and (Almost-)In-Place Algorithm for Building Left-Balanced kd-Trees</title>
  <guid>http://arxiv.org/abs/2211.00120</guid>
  <link>http://arxiv.org/abs/2211.00120</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wald_I/0/1/0/all/0/1&quot;&gt;Ingo Wald&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We present an algorithm that allows for building left-balanced and complete
k-d trees over k-dimensional points in a trivially parallel and GPU friendly
way. Our algorithm requires exactly one int per data point as temporary
storage, and uses O(logN ) iterations, each of which performs one parallel
sort, and one trivially parallel CUDA per-node update kernel.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-02 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Computational Power of A Single Oblivious Mobile Agent in Two-Edge-Connected Graphs</title>
  <guid>http://arxiv.org/abs/2211.00332</guid>
  <link>http://arxiv.org/abs/2211.00332</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inoue_T/0/1/0/all/0/1&quot;&gt;Taichi Inoue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1&quot;&gt;Naoki Kitamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izumi_T/0/1/0/all/0/1&quot;&gt;Taisuke Izumi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masuzawa_T/0/1/0/all/0/1&quot;&gt;Toshimitsu Masuzawa&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We investigate the computational power of a single mobile agent in an
$n$-node graph with storage (i.e., node memory). It has been shown that the
system with one-bit agent memory and $O(1)$-bit storage is as powerful as the
one with $O(n)$-bit agent memory and $O(1)$-bit storage, and thus we focus on
the difference between one-bit memory agents and oblivious (i.e. zero-bit
memory) agents. While it has been also shown that their computational powers
are not equivalent, all the known results exhibiting such a difference rely on
the fact that oblivious agents cannot transfer any information from one side to
the other side across the bridge edge. Then our main question is stated as
follows: Are the computational powers of one-bit memory agents and oblivious
agents equivalent in 2-edge-connected graphs or not? The main contribution of
this paper is to answer this question positively under the relaxed assumption
that each node has $O(\log\Delta)$-bit storage ($\Delta$ is the maximum degree
of the graph). We present an algorithm of simulating any algorithm for a single
one-bit memory agent by one oblivious agent with $O(n^2)$-time overhead per
round. Our result implies that the topological structure of graphs
differentiating the computational powers of oblivious and non-oblivious agents
is completely characterized by the existence of bridge edges.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-02 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>A Near-Linear Kernel for Two-Parsimony Distance</title>
  <guid>http://arxiv.org/abs/2211.00378</guid>
  <link>http://arxiv.org/abs/2211.00378</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deen_E/0/1/0/all/0/1&quot;&gt;Elise Deen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iersel_L/0/1/0/all/0/1&quot;&gt;Leo van Iersel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janssen_R/0/1/0/all/0/1&quot;&gt;Remie Janssen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1&quot;&gt;Mark Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murakami_Y/0/1/0/all/0/1&quot;&gt;Yuki Murakami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeh_N/0/1/0/all/0/1&quot;&gt;Norbert Zeh&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The maximum parsimony distance $d_{\textrm{MP}}(T_1,T_2)$ and the
bounded-state maximum parsimony distance $d_{\textrm{MP}}^t(T_1,T_2)$ measure
the difference between two phylogenetic trees $T_1,T_2$ in terms of the maximum
difference between their parsimony scores for any character (with $t$ a bound
on the number of states in the character, in the case of
$d_{\textrm{MP}}^t(T_1,T_2)$). While computing $d_{\textrm{MP}}(T_1, T_2)$ was
previously shown to be fixed-parameter tractable with a linear kernel, no such
result was known for $d_{\textrm{MP}}^t(T_1,T_2)$. In this paper, we prove that
computing $d_{\textrm{MP}}^t(T_1, T_2)$ is fixed-parameter tractable for
all~$t$. Specifically, we prove that this problem has a kernel of size $O(k \lg
k)$, where $k = d_{\textrm{MP}}^t(T_1, T_2)$. As the primary analysis tool, we
introduce the concept of leg-disjoint incompatible quartets, which may be of
independent interest.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-02 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>On the zeroes of hypergraph independence polynomials</title>
  <guid>http://arxiv.org/abs/2211.00464</guid>
  <link>http://arxiv.org/abs/2211.00464</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Galvin_D/0/1/0/all/0/1&quot;&gt;David Galvin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+McKinley_G/0/1/0/all/0/1&quot;&gt;Gwen McKinley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Perkins_W/0/1/0/all/0/1&quot;&gt;Will Perkins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sarantis_M/0/1/0/all/0/1&quot;&gt;Michail Sarantis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tetali_P/0/1/0/all/0/1&quot;&gt;Prasad Tetali&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We study the locations of complex zeroes of independence polynomials of
bounded degree hypergraphs. For graphs, this is a long-studied subject with
applications to statistical physics, algorithms, and combinatorics. Results on
zero-free regions for bounded-degree graphs include Shearer&#39;s result on the
optimal zero-free disk, along with several recent results on other zero-free
regions. Much less is known for hypergraphs. We make some steps towards an
understanding of zero-free regions for bounded-degree hypergaphs by proving
that all hypergraphs of maximum degree $\Delta$ have a zero-free disk almost as
large as the optimal disk for graphs of maximum degree $\Delta$ established by
Shearer (of radius $\sim 1/(e \Delta)$). Up to logarithmic factors in $\Delta$
this is optimal, even for hypergraphs with all edge-sizes strictly greater than
$2$. We conjecture that for $k\ge 3$, $k$-uniform linear hypergraphs have a
much larger zero-free disk of radius $\Omega(\Delta^{- \frac{1}{k-1}} )$. We
establish this in the case of linear hypertrees.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-02 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Postdocs at Harvard at Harvard University (apply by November 21, 2022)</title>
  <guid>http://cstheory-jobs.org/2022/11/01/postdocs-at-harvard-at-harvard-university-apply-by-november-21-2022/</guid>
  <link>https://cstheory-jobs.org/2022/11/01/postdocs-at-harvard-at-harvard-university-apply-by-november-21-2022/</link>
  <description>
    &lt;p&gt;Multiple postdoc positions at Harvard University in theoretical CS, machine learning foundations, quantum computing, CS and society and more&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://windowsontheory.org/2022/11/01/postdocs-at-harvard/&quot;&gt;https://windowsontheory.org/2022/11/01/postdocs-at-harvard/&lt;/a&gt;&lt;br /&gt;
Email: achoat@seas.harvard.edu&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 17:19:24 UTC</pubDate>
  <author>CCI: jobs</author>
</item>

<item>
  <title>Postdocs at Harvard!</title>
  <guid>http://windowsontheory.org/?p=8462</guid>
  <link>https://windowsontheory.org/2022/11/01/postdocs-at-harvard/</link>
  <description>
    &lt;p&gt;The &lt;a href=&quot;https://mlfoundations.org/#opportunities&quot;&gt;ML Foundations &lt;/a&gt;and &lt;a href=&quot;https://toc.seas.harvard.edu/positions&quot;&gt;theory&lt;/a&gt; groups at Harvard are looking for postdocs for the coming academic year. &lt;/p&gt;



&lt;p&gt;There are also several other positions at Harvard, including at the &lt;a href=&quot;https://datascience.harvard.edu/data-science-postdoctoral-fellows&quot;&gt;Harvard Data Science Initiative (HDSI)&lt;/a&gt;, &lt;a href=&quot;https://cmsa.fas.harvard.edu/about-us/jobs/&quot;&gt; Center of Mathematical Sciences and Applications (CMSA),&lt;/a&gt; &lt;a href=&quot;https://cbs.fas.harvard.edu/research/theory/#swartz&quot;&gt;Swartz fellows&lt;/a&gt; at the Center for Brain Sciences,  the &lt;a href=&quot;https://academicpositions.harvard.edu/postings/11637&quot;&gt;George Carrier fellowship in applied mathematics,&lt;/a&gt;  the &lt;a href=&quot;https://crcs.seas.harvard.edu/apply&quot;&gt;Center for Research on Computation and Society (CRCS)&lt;/a&gt;,   &lt;a href=&quot;https://quantum.harvard.edu/external-candidates&quot;&gt;Harvard Quantum Initiative (HQI)&lt;/a&gt;. I hope that the newly announced &lt;a href=&quot;https://www.harvard.edu/kempner-institute/&quot;&gt;Kempner Institute&lt;/a&gt; will also be able to offer positions for the next academic year. &lt;/p&gt;



&lt;p&gt;All these positions have different foci, conditions, and the searches are run by different institutions, even if the set of potential mentors might be overlapping. Hence I strongly recommend that people apply to all positions that they are interested in. (&lt;/p&gt;



&lt;p&gt;These positions and others are posted on the &lt;a href=&quot;https://mlfoundations.org/#opportunities&quot;&gt;opportunities&lt;/a&gt; section of the ML foundations home page ( &lt;a href=&quot;https://mlfoundations.org/#opportunities&quot; rel=&quot;nofollow&quot;&gt;https://mlfoundations.org/#opportunities&lt;/a&gt; ). When I hear of new opportunities, I may update there and/or on &lt;a href=&quot;https://twitter.com/boazbaraktcs&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By Boaz Barak&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 17:16:59 UTC</pubDate>
  <author>Windows on Theory</author>
</item>

<item>
  <title>Postdoc at Harvard University (apply by December 1, 2022)</title>
  <guid>http://cstheory-jobs.org/2022/11/01/postdoc-at-harvard-university-apply-by-december-1-2022/</guid>
  <link>https://cstheory-jobs.org/2022/11/01/postdoc-at-harvard-university-apply-by-december-1-2022/</link>
  <description>
    &lt;p&gt;The Harvard CS Theory Group invites applications for a variety of postdoctoral fellowships, including the Michael Rabin Postdoctoral Fellowship, postdocs in the Privacy Tools Project, Fairness in Prediction Algorithms, and Foundations in Machine Learning, and postdocs with individual faculty members.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href=&quot;https://academicpositions.harvard.edu/postings/11762&quot;&gt;https://academicpositions.harvard.edu/postings/11762&lt;/a&gt;&lt;br /&gt;
Email: achoat@seas.harvard.edu&lt;/p&gt;
&lt;p class=&quot;authors&quot;&gt;By shacharlovett&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 16:20:19 UTC</pubDate>
  <author>CCI: jobs</author>
</item>

<item>
  <title>Composition Basics</title>
  <guid>https://differentialprivacy.org/composition-basics/</guid>
  <link>https://differentialprivacy.org/composition-basics/</link>
  <description>
    &lt;p&gt;Our data is subject to many different uses. Many entities will have access to our data and those entities will perform many different analyses that involve our data. The greatest risk to privacy is that an attacker will combine multiple pieces of information from the same or different sources and that the combination of these will reveal sensitive details about us.
Thus we cannot study privacy leakage in a vacuum; it is important that we can reason about the accumulated privacy leakage over multiple independent analyses, which is known as &lt;em&gt;composition&lt;/em&gt;. We have &lt;a href=&quot;/privacy-composition/&quot;&gt;previously discussed&lt;/a&gt; why composition is so important for differential privacy.&lt;/p&gt;

&lt;p&gt;This is the first in a series of posts on &lt;em&gt;composition&lt;/em&gt; in which we will explain in more detail how compositoin analyses work.&lt;/p&gt;

&lt;p&gt;Composition is quantitative. The differential privacy guarantee of the overall system will depend on the number of analyses and the privacy parameters that they each satisfy. The exact relationship between these quantities can be complex. There are various composition theorems that give bounds on the overall parameters in terms of the parameters of the parts of the system.&lt;/p&gt;

&lt;p&gt;The simplest composition theorem is what is known as basic composition, which applies to pure \(\varepsilon\)-DP (although it can be extended to approximate \((\varepsilon,\delta)\)-DP):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Basic Composition)
Let \(M_1, M_2, \cdots, M_k : \mathcal{X}^n \to \mathcal{Y}\) be randomized algorithms. Suppose \(M_j\) is \(\varepsilon_j\)-DP for each \(j \in [k]\).
Define \(M : \mathcal{X}^n \to \mathcal{Y}^k\) by \(M(x)=(M_1(x),M_2(x),\cdots,M_k(x))\), where each algorithm is run independently. Then \(M\) is \(\varepsilon\)-DP for \(\varepsilon = \sum_{j=1}^k \varepsilon_j\).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;
Fix an arbitrary pair of neighbouring datasets \(x,x \in \mathcal{X}^n\) and output \(y \in \mathcal{Y}^k\).
To establish that \(M\) is \(\varepsilon\)-DP, we must show that \(e^{-\varepsilon} \le \frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x)=y]} \le e^\varepsilon\). By independence, we have \[\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x)=y]} = \frac{\prod_{j=1}^k\mathbb{P}[M_j(x)=y_j]}{\prod_{j=1}^k\mathbb{P}[M_j(x)=y_j]} =  \prod_{j=1}^k \frac{\mathbb{P}[M_j(x)=y_j]}{\mathbb{P}[M_j(x)=y_j]} \le \prod_{j=1}^k e^{\varepsilon_j} = e^{\sum_{j=1}^k \varepsilon_j} = e^\varepsilon,\] where the inequality follows from the fact that each \(M_j\) is \(\varepsilon_j\)-DP and, hence, \(e^{-\varepsilon_j} \le \frac{\mathbb{P}[M_j(x)=y_j]}{\mathbb{P}[M_j(x)=y_j]} \le e^{\varepsilon_j}\). Similarly, \(\prod_{j=1}^k \frac{\mathbb{P}[M_j(x)=y_j]}{\mathbb{P}[M_j(x)=y_j]} \ge \prod_{j=1}^k e^{-\varepsilon_j}\), which completes the proof. &lt;/p&gt;

&lt;p&gt;Basic composition is already a powerful result, despite its simple proof; it establishes the versatility of differential privacy and allows us to begin reasoning about complex systems in terms of their building blocks. For example, suppose we have \(k\) functions \(f_1, \cdots, f_k : \mathcal{X}^n \to \mathbb{R}\) each of sensitivity \(1\). For each \(j \in [k]\), we know that adding \(\mathsf{Laplace}(1/\varepsilon)\) noise to the value of \(f_j(x)\) satisfies \(\varepsilon\)-DP. Thus, if we add independent \(\mathsf{Laplace}(1/\varepsilon)\) noise to each value \(f_j(x)\) for all \(j \in [k]\), then basic composition tells us that releasing this vector of \(k\) noisy values satisfies \(k\varepsilon\)-DP. If we want the overall system to be \(\varepsilon\)-DP, then we should add independent \(\mathsf{Laplace}(k/\varepsilon)\) noise to each value \(f_j(x)\).&lt;/p&gt;

&lt;h2 id=&quot;is-basic-composition-optimal&quot;&gt;Is Basic Composition Optimal?&lt;/h2&gt;

&lt;p&gt;If we want to release \(k\) values each of sensitivity \(1\) (as above) and have the overall release be \(\varepsilon\)-DP, then, using basic composition, we can add \(\mathsf{Laplace}(k/\varepsilon)\) noise to each value. The variance of the noise for each value is \(2k^2/\varepsilon^2\), so the standard deviation is \(\sqrt{2} k /\varepsilon\). In other words, the scale of the noise must grow linearly with the number of values \(k\) if the overall privacy and each values sensitivity is fixed. It is natural to wonder whether the scale of the Laplace noise can be reduced by improving the basic composition result. We now show that this is not possible.&lt;/p&gt;

&lt;p&gt;For each \(j \in [k]\), let \(M_j : \mathcal{X}^n \to \mathbb{R}\) be the algorithm that releases \(f_j(x)\) with \(\mathsf{Laplace}(k/\varepsilon)\) noise added. Let \(M : \mathcal{X}^n \to \mathbb{R}^k\) be the composition of these \(k\) algorithms. Then \(M_j\) is \(\varepsilon/k\)-DP for each \(j \in [k]\) and basic composition tells us that \(M\) is \(\varepsilon\)-DP. The question is whether \(M\) satisfies a better DP guarantee than this  i.e., does \(M\) satisfy \(\varepsilon_*\)-DP for some \(\varepsilon_*&amp;lt;\varepsilon\)?
Suppose we have neighbouring datasets \(x,x\in\mathcal{X}^n\) such that \(f_j(x) = f_j(x)+1\) for each \(j \in [k]\). Let \(y=(a,a,\cdots,a) \in \mathbb{R}^k\) for some \(a \ge \max_{j=1}^k f_j(x)\).
Then 
\[
        \frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x)=y]} = \frac{\prod_{j=1}^k \mathbb{P}[f_j(x)+\mathsf{Laplace}(k/\varepsilon)=y_j]}{\prod_{j=1}^k \mathbb{P}[f_j(x)+\mathsf{Laplace}(k/\varepsilon)=y_j]} 
\]
\[
         = \prod_{j=1}^k \frac{\frac{\varepsilon}{2k}\exp\left(-\frac{\varepsilon}{k} |y_j-f_j(x)| \right)}{\frac{\varepsilon}{2k}\exp\left(-\frac{\varepsilon}{k} |y_j-f_j(x)| \right)} 
         = \prod_{j=1}^k \frac{\exp\left(-\frac{\varepsilon}{k} (y_j-f_j(x)) \right)}{\exp\left(-\frac{\varepsilon}{k} (y_j-f_j(x)) \right)} 
\]
\[
         = \prod_{j=1}^k \exp\left(\frac{\varepsilon}{k}\left(f_j(x)-f_j(x)\right)\right)
         = \exp\left( \frac{\varepsilon}{k} \sum_{j=1}^k \left(f_j(x)-f_j(x)\right)\right)= e^\varepsilon,
\]
where the third equality removes the absolute values because \(y_j \ge f_j(x)\) and \(y_j \ge f_j(x)\).
This shows that basic composition is optimal. For this example, we cannot prove a better guarantee than what is given by basic composition.&lt;/p&gt;

&lt;p&gt;Is there some other way to improve upon basic composition that circumvents this example? Note that we assumed that there are neighbouring datasets \(x,x\in\mathcal{X}^n\) such that \(f_j(x) = f_j(x)+1\) for each \(j \in [k]\). In some settings, no such worst case datasets exist. In that case, instead of scaling the noise linearly with \(k\), we can scale the Laplace noise according to the \(\ell_1\) sensitivity \(\Delta_1 := \sup_{x,x \in \mathcal{X}^n \atop \text{neighbouring}} \sum_{j=1}^k |f_j(x)-f_j(x)|\).&lt;/p&gt;

&lt;p&gt;Instead of adding assumptions to the problem, we will look more closely at the example above.
We showed that there exists some output \(y \in \mathbb{R}^d\) such that \(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x)=y]} = e^\varepsilon\).
However, such outputs \(y\) are very rare, as we require \(y_j \ge \max\{f_j(x),f_j(x)\}\) for each \(j \in [k]\) where \(y_j = f_j(x) + \mathsf{Laplace}(k/\varepsilon)\). Thus, in order to observe an output \(y\) such that the likelihood ratio is maximal, all of the \(k\) Laplace noise samples must be positive, which happens with probability \(2^{-k}\). 
The fact that outputs \(y\) with maximal likelihood ratio are exceedingly rare turns out to be a general phenomenon and not specific to the example above.&lt;/p&gt;

&lt;p&gt;Can we improve on basic composition if we only ask for a high probability bound? That is, instead of demanding \(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x)=y]} \le e^{\varepsilon_*}\) for all \(y \in \mathcal{Y}\), we demand \(\mathbb{P}_{Y \gets M(x)}\left[\frac{\mathbb{P}[M(x)=Y]}{\mathbb{P}[M(x)=Y]} \le e^{\varepsilon_*}\right] \ge 1-\delta\) for some \(0 &amp;lt; \delta \ll 1\). Can we prove a better bound \(\varepsilon_* &amp;lt; \varepsilon\) in this relaxed setting? The answer turns out to be yes.&lt;/p&gt;

&lt;p&gt;The limitation of pure \(\varepsilon\)-DP is that events with tiny probability  which are negligible in real-world applications  can dominate the privacy analysis. This motivates us to move to relaxed notions of differential privacy, such as approximate \((\varepsilon,\delta)\)-DP and concentrated DP, which are less sensitive to low probability events.&lt;/p&gt;

&lt;h2 id=&quot;preview-advanced-composition&quot;&gt;Preview: Advanced Composition&lt;/h2&gt;

&lt;p&gt;By moving to approximate \((\varepsilon,\delta)\)-DP with \(\delta&amp;gt;0\), we can prove an asymptotically better composition theorem, which is known as &lt;em&gt;the advanced composition theorem&lt;/em&gt; &lt;strong&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/5670947&quot; title=&quot;Cynthia Dwork, Guy Rothblum, Salil Vadhan. Boosting and Differential Privacy. FOCS 2010.&quot;&gt;[DRV10]&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Advanced Composition Starting from Pure DP&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;)
Let \(M_1, M_2, \cdots, M_k : \mathcal{X}^n \to \mathcal{Y}\) be randomized algorithms. Suppose \(M_j\) is \(\varepsilon_j\)-DP for each \(j \in [k]\).
Define \(M : \mathcal{X}^n \to \mathcal{Y}^k\) by \(M(x)=(M_1(x),M_2(x),\cdots,M_k(x))\), where each algorithm is run independently. Then \(M\) is \((\varepsilon,\delta)\)-DP for any \(\delta&amp;gt;0\) with \[\varepsilon = \frac12 \sum_{j=1}^k \varepsilon_j^2 + \sqrt{2\log(1/\delta) \sum_{j=1}^k \varepsilon_j^2}.\]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Recall that basic composition gives \(\delta=0\) and \(\varepsilon = \sum_{j=1}^k \varepsilon_j\). That is, basic composition scales with the 1-norm of the vector \((\varepsilon_1, \varepsilon_2, \cdots, \varepsilon_k)\), whereas advanced composition scales with the 2-norm of this vector (and the squared 2-norm).
Neither bound strictly dominates the other. However, asymptotically (in a sense we will make precise in the next paragraph) advanced composition dominates basic composition.&lt;/p&gt;

&lt;p&gt;Suppose we have a fixed \((\varepsilon,\delta)\)-DP guarantee for the entire system and we must answer \(k\) queries of sensitivity \(1\).
Using basic composition, we can answer each query by adding \(\mathsf{Laplace}(k/\varepsilon)\) noise to each answer.
However, using advanced composition, we can answer each query by adding \(\mathsf{Laplace}(\sqrt{k/2\rho})\) noise to each answer, where&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;
\[\rho = \frac{\varepsilon^2}{4\log(1/\delta)+4\varepsilon}.\]
If the privacy parameters \(\varepsilon,\delta&amp;gt;0\) are fixed (which implies \(\rho\) is fixed) and \(k \to \infty\), we can see that asymptotically advanced composition gives noise per query scaling as \(\Theta(\sqrt{k})\), while basic composition results in noise scaling as \(\Theta(k)\).&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;In the next few posts we will explain how advanced composition works. We hope this conveys an intuitive understanding of composition and, in particular, how this \(\sqrt{k}\) asymptotic behaviour arises. If you want to read ahead, these posts are extracts from &lt;a href=&quot;https://arxiv.org/abs/2210.00597&quot;&gt;this book chapter&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This result generalizes to approximate DP. If instead we assume \(M_j\) is \((\varepsilon_j,\delta_j)\)-DP for each \(j \in [k]\), then the final composition is \((\varepsilon,\delta+\sum_{j=1}^k \delta_j)\)-DP with \(\varepsilon\) as before.&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Adding \(\mathsf{Laplace}(\sqrt{k/2\rho})\) noise to a sensitivity-1 query ensures \(\varepsilon_j\)-DP for \(\varepsilon_j = \sqrt{2\rho/k}\). Hence \(\sum_{j=1}^k \varepsilon_j^2 = 2\rho\). Setting \(\rho = \frac{\varepsilon^2}{4\log(1/\delta)+4\varepsilon}\) ensures that \(\frac12 \sum_{j=1}^k \varepsilon_j^2 + \sqrt{2\log(1/\delta) \sum_{j=1}^k \varepsilon_j^2} = \rho + \sqrt{4\rho\log(1/\delta)} \le \varepsilon\).&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;&lt;p class=&quot;authors&quot;&gt;By &lt;/p&gt;
  </description>
  <pubDate>2022-11-01 15:45:00 UTC</pubDate>
  <author>DifferentialPrivacy.org</author>
</item>

<item>
  <title>The isotropy group of the matrix multiplication tensor</title>
  <guid>http://arxiv.org/abs/2210.16565</guid>
  <link>http://arxiv.org/abs/2210.16565</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burichenko_V/0/1/0/all/0/1&quot;&gt;V.P.Burichenko&lt;/a&gt;&lt;/p&gt;&lt;p&gt;By an {\em isotropy group} of a tensor $t\in V_1 \otimes V_2\otimes
V_3=\widetilde V$ we mean the group of all invertible linear transformations of
$\widetilde V$ that leave $t$ invariant and are compatible (in an obvious
sense) with the structure of tensor product on~$\widetilde V$. We consider the
case where $t$ is the structure tensor of multiplication map of rectangular
matrices. The isotropy group of this tensor was studied in 1970s by de Groote,
Strassen, and Brockett-Dobkin. In the present work we enlarge, make more
precise, expose in the language of group actions on tensor spaces, and endow
with proofs the results previously known. This is necessary for studying the
algorithms of fast matrix multiplication admitting symmetries. The latter seems
to be a promising new way for constructing fast algorithms.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 00:30:00 UTC</pubDate>
  <author>arXiv: Computational Complexity</author>
</item>

<item>
  <title>Parallel Breadth-First Search and Exact Shortest Paths and Stronger Notions for Approximate Distances</title>
  <guid>http://arxiv.org/abs/2210.16351</guid>
  <link>http://arxiv.org/abs/2210.16351</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rozhon_V/0/1/0/all/0/1&quot;&gt;V&amp;#xe1;clav Rozho&amp;#x148;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haeupler_B/0/1/0/all/0/1&quot;&gt;Bernhard Haeupler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinsson_A/0/1/0/all/0/1&quot;&gt;Anders Martinsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grunau_C/0/1/0/all/0/1&quot;&gt;Christoph Grunau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuzic_G/0/1/0/all/0/1&quot;&gt;Goran Zuzic&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We introduce stronger notions for approximate single-source shortest-path
distances, show how to efficiently compute them from weaker standard notions,
and demonstrate the algorithmic power of these new notions and transformations.
One application is the first work-efficient parallel algorithm for computing
exact single-source shortest paths graphs -- resolving a major open problem in
parallel computing.
&lt;/p&gt;
&lt;p&gt;Given a source vertex in a directed graph with polynomially-bounded
nonnegative integer lengths, the algorithm computes an exact shortest path tree
in $m \log^{O(1)} n$ work and $n^{1/2+o(1)}$ depth. Previously, no parallel
algorithm improving the trivial linear depths of Dijkstra&#39;s algorithm without
significantly increasing the work was known, even for the case of undirected
and unweighted graphs (i.e., for computing a BFS-tree).
&lt;/p&gt;
&lt;p&gt;Our main result is a black-box transformation that uses $\log^{O(1)} n$
standard approximate distance computations to produce approximate distances
which also satisfy the subtractive triangle inequality (up to a
$(1+\varepsilon)$ factor) and even induce an exact shortest path tree in a
graph with only slightly perturbed edge lengths. These strengthened
approximations are algorithmically significantly more powerful and overcome
well-known and often encountered barriers for using approximate distances. In
directed graphs they can even be boosted to exact distances. This results in a
black-box transformation of any (parallel or distributed) algorithm for
approximate shortest paths in directed graphs into an algorithm computing exact
distances at essentially no cost. Applying this to the recent breakthroughs of
Fineman et al. for compute approximate SSSP-distances via approximate hopsets
gives new parallel and distributed algorithm for exact shortest paths.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Dynamic Bandits with an Auto-Regressive Temporal Structure</title>
  <guid>http://arxiv.org/abs/2210.16386</guid>
  <link>http://arxiv.org/abs/2210.16386</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qinyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golrezaei_N/0/1/0/all/0/1&quot;&gt;Negin Golrezaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouneffouf_D/0/1/0/all/0/1&quot;&gt;Djallel Bouneffouf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Multi-armed bandit (MAB) problems are mainly studied under two extreme
settings known as stochastic and adversarial. These two settings, however, do
not capture realistic environments such as search engines and marketing and
advertising, in which rewards stochastically change in time. Motivated by that,
we introduce and study a dynamic MAB problem with stochastic temporal
structure, where the expected reward of each arm is governed by an
auto-regressive (AR) model. Due to the dynamic nature of the rewards, simple
&quot;explore and commit&quot; policies fail, as all arms have to be explored
continuously over time. We formalize this by characterizing a per-round regret
lower bound, where the regret is measured against a strong (dynamic) benchmark.
We then present an algorithm whose per-round regret almost matches our regret
lower bound. Our algorithm relies on two mechanisms: (i) alternating between
recently pulled arms and unpulled arms with potential, and (ii) restarting.
These mechanisms enable the algorithm to dynamically adapt to changes and
discard irrelevant past information at a suitable rate. In numerical studies,
we further demonstrate the strength of our algorithm under different types of
non-stationary settings.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Flows, Scaling, and Entropy Revisited: a Unified Perspective via Optimizing Joint Distributions</title>
  <guid>http://arxiv.org/abs/2210.16456</guid>
  <link>http://arxiv.org/abs/2210.16456</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Altschuler_J/0/1/0/all/0/1&quot;&gt;Jason M. Altschuler&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this short expository note, we describe a unified algorithmic perspective
on several classical problems which have traditionally been studied in
different communities. This perspective views the main characters -- the
problems of Optimal Transport, Minimum Mean Cycle, Matrix Scaling, and Matrix
Balancing -- through the same lens of optimization problems over joint
probability distributions P(x,y) with constrained marginals. While this is how
Optimal Transport is typically introduced, this lens is markedly less
conventional for the other three problems. This perspective leads to a simple
and unified framework spanning problem formulation, algorithm development, and
runtime analysis.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>The Vector Balancing Constant for Zonotopes</title>
  <guid>http://arxiv.org/abs/2210.16460</guid>
  <link>http://arxiv.org/abs/2210.16460</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Heck_L/0/1/0/all/0/1&quot;&gt;Laurel Heck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Reis_V/0/1/0/all/0/1&quot;&gt;Victor Reis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Rothvoss_T/0/1/0/all/0/1&quot;&gt;Thomas Rothvoss&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The vector balancing constant $\mathrm{vb}(K,Q)$ of two symmetric convex
bodies $K,Q$ is the minimum $r \geq 0$ so that any number of vectors from $K$
can be balanced into an $r$-scaling of $Q$. A question raised by Schechtman is
whether for any zonotope $K \subseteq \mathbb{R}^d$ one has $\mathrm{vb}(K,K)
\lesssim \sqrt{d}$. Intuitively, this asks whether a natural geometric
generalization of Spencer&#39;s Theorem (for which $K = B^d_\infty$) holds. We
prove that for any zonotope $K \subseteq \mathbb{R}^d$ one has
$\mathrm{vb}(K,K) \lesssim \sqrt{d} \log \log \log d$. Our main technical
contribution is a tight lower bound on the Gaussian measure of any section of a
normalized zonotope, generalizing Vaaler&#39;s Theorem for cubes. We also prove
that for two different normalized zonotopes $K$ and $Q$ one has
$\mathrm{vb}(K,Q) \lesssim \sqrt{d \log d}$. All the bounds are constructive
and the corresponding colorings can be computed in polynomial time.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

<item>
  <title>Improved Approximation Algorithms for Capacitated Vehicle Routing with Fixed Capacity</title>
  <guid>http://arxiv.org/abs/2210.16534</guid>
  <link>http://arxiv.org/abs/2210.16534</link>
  <description>
    &lt;p class=&quot;arxiv-authors&quot;&gt;&lt;b&gt;Authors:&lt;/b&gt; &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jingyang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1&quot;&gt;Mingyu Xiao&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The Capacitated Vehicle Routing Problem (CVRP) is one of the most extensively
studied problems in combinatorial optimization. According to the property of
the demand of customers, we distinguish three variants of CVRP: unit-demand,
splittable and unsplittable. We consider $k$-CVRP in general metrics and
general graphs, where $k$ is the capacity of the vehicle and all the three
versions are APX-hard for each fixed $k\geq 3$.
&lt;/p&gt;
&lt;p&gt;In this paper, we give a $(5/2-\Theta(\sqrt{1/k}))$-approximation algorithm
for splittable and unit-demand $k$-CVRP and a
$(5/2+\ln2-\Theta(\sqrt{1/k}))$-approximation algorithm for unsplittable
$k$-CVRP (assume the approximation ratio for metric TSP is $\alpha=3/2$). Thus,
our approximation ratio is better than previous results for sufficient large
$k$, say $k\leq 1.7\times 10^7$.
&lt;/p&gt;
&lt;p&gt;For small $k$, we design independent algorithms by using more techniques to
get further improvements. For splittable and unit-demand cases, we improve the
ratio from $1.934$ to $1.500$ for $k=3$, and from $1.750$ to $1.667$ for $k=4$.
For the unsplittable case, we improve the ratio from $2.693$ to $1.500$ for
$k=3$, from $2.443$ to $1.750$ for $k=4$, and from $2.893$ to $2.157$ for
$k=5$.
&lt;/p&gt;
  </description>
  <pubDate>2022-11-01 00:30:00 UTC</pubDate>
  <author>arXiv: Data Structures and Algorithms</author>
</item>

</channel>
</rss>
